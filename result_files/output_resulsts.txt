{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'learning_rate': 0.0030541937159358503,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1537904159,
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.392418282278356}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.670
[3,     1] loss: 0.646
[4,     1] loss: 0.627
[5,     1] loss: 0.609
[6,     1] loss: 0.589
[7,     1] loss: 0.570
[8,     1] loss: 0.547
[9,     1] loss: 0.528
[10,     1] loss: 0.511
[11,     1] loss: 0.496
[12,     1] loss: 0.481
[13,     1] loss: 0.463
[14,     1] loss: 0.449
[15,     1] loss: 0.439
[16,     1] loss: 0.438
[17,     1] loss: 0.410
[18,     1] loss: 0.425
[19,     1] loss: 0.390
[20,     1] loss: 0.391
[21,     1] loss: 0.377
[22,     1] loss: 0.364
[23,     1] loss: 0.354
[24,     1] loss: 0.346
[25,     1] loss: 0.334
[26,     1] loss: 0.325
[27,     1] loss: 0.319
[28,     1] loss: 0.313
[29,     1] loss: 0.306
[30,     1] loss: 0.300
[31,     1] loss: 0.295
[32,     1] loss: 0.290
[33,     1] loss: 0.285
[34,     1] loss: 0.279
[35,     1] loss: 0.274
[36,     1] loss: 0.269
[37,     1] loss: 0.262
[38,     1] loss: 0.254
[39,     1] loss: 0.249
[40,     1] loss: 0.241
[41,     1] loss: 0.235
[42,     1] loss: 0.226
[43,     1] loss: 0.223
[44,     1] loss: 0.216
[45,     1] loss: 0.212
[46,     1] loss: 0.230
[47,     1] loss: 0.265
[48,     1] loss: 0.234
[49,     1] loss: 0.235
[50,     1] loss: 0.226
[51,     1] loss: 0.219
[52,     1] loss: 0.209
[53,     1] loss: 0.206
[54,     1] loss: 0.196
[55,     1] loss: 0.188
[56,     1] loss: 0.174
[57,     1] loss: 0.166
[58,     1] loss: 0.157
[59,     1] loss: 0.149
[60,     1] loss: 0.141
[61,     1] loss: 0.135
[62,     1] loss: 0.128
[63,     1] loss: 0.123
[64,     1] loss: 0.118
[65,     1] loss: 0.114
[66,     1] loss: 0.111
[67,     1] loss: 0.109
[68,     1] loss: 0.107
[69,     1] loss: 0.106
[70,     1] loss: 0.104
[71,     1] loss: 0.104
[72,     1] loss: 0.101
[73,     1] loss: 0.101
[74,     1] loss: 0.100
[75,     1] loss: 0.101
[76,     1] loss: 0.098
[77,     1] loss: 0.099
[78,     1] loss: 0.096
[79,     1] loss: 0.098
[80,     1] loss: 0.098
[81,     1] loss: 0.096
[82,     1] loss: 0.097
[83,     1] loss: 0.095
[84,     1] loss: 0.097
[85,     1] loss: 0.095
[86,     1] loss: 0.095
[87,     1] loss: 0.095
[88,     1] loss: 0.095
[89,     1] loss: 0.093
[90,     1] loss: 0.097
[91,     1] loss: 0.209
[92,     1] loss: 0.638
[93,     1] loss: 0.734
[94,     1] loss: 0.738
[95,     1] loss: 0.554
[96,     1] loss: 0.542
[97,     1] loss: 0.586
[98,     1] loss: 0.600
[99,     1] loss: 0.608
[100,     1] loss: 0.611
[101,     1] loss: 0.612
[102,     1] loss: 0.619
[103,     1] loss: 0.625
[104,     1] loss: 0.631
[105,     1] loss: 0.637
Early stopping applied (best metric=0.42567914724349976)
Finished Training
Total time taken: 24.711501836776733
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.675
[3,     1] loss: 0.646
[4,     1] loss: 0.627
[5,     1] loss: 0.607
[6,     1] loss: 0.591
[7,     1] loss: 0.567
[8,     1] loss: 0.548
[9,     1] loss: 0.523
[10,     1] loss: 0.502
[11,     1] loss: 0.477
[12,     1] loss: 0.453
[13,     1] loss: 0.427
[14,     1] loss: 0.403
[15,     1] loss: 0.379
[16,     1] loss: 0.358
[17,     1] loss: 0.334
[18,     1] loss: 0.315
[19,     1] loss: 0.299
[20,     1] loss: 0.278
[21,     1] loss: 0.266
[22,     1] loss: 0.249
[23,     1] loss: 0.234
[24,     1] loss: 0.223
[25,     1] loss: 0.208
[26,     1] loss: 0.200
[27,     1] loss: 0.190
[28,     1] loss: 0.178
[29,     1] loss: 0.175
[30,     1] loss: 0.167
[31,     1] loss: 0.160
[32,     1] loss: 0.157
[33,     1] loss: 0.152
[34,     1] loss: 0.148
[35,     1] loss: 0.145
[36,     1] loss: 0.142
[37,     1] loss: 0.143
[38,     1] loss: 0.140
[39,     1] loss: 0.140
[40,     1] loss: 0.137
[41,     1] loss: 0.137
[42,     1] loss: 0.133
[43,     1] loss: 0.135
[44,     1] loss: 0.136
[45,     1] loss: 0.136
[46,     1] loss: 0.135
[47,     1] loss: 0.133
[48,     1] loss: 0.134
[49,     1] loss: 0.135
[50,     1] loss: 0.136
[51,     1] loss: 0.163
[52,     1] loss: 0.249
[53,     1] loss: 0.519
[54,     1] loss: 0.435
[55,     1] loss: 0.419
[56,     1] loss: 0.372
[57,     1] loss: 0.409
[58,     1] loss: 0.358
[59,     1] loss: 0.346
[60,     1] loss: 0.318
[61,     1] loss: 0.309
[62,     1] loss: 0.291
[63,     1] loss: 0.279
[64,     1] loss: 0.266
[65,     1] loss: 0.250
[66,     1] loss: 0.241
[67,     1] loss: 0.227
[68,     1] loss: 0.217
[69,     1] loss: 0.208
[70,     1] loss: 0.196
[71,     1] loss: 0.186
[72,     1] loss: 0.176
[73,     1] loss: 0.168
[74,     1] loss: 0.162
[75,     1] loss: 0.154
[76,     1] loss: 0.149
[77,     1] loss: 0.143
[78,     1] loss: 0.137
[79,     1] loss: 0.134
[80,     1] loss: 0.132
[81,     1] loss: 0.127
[82,     1] loss: 0.127
[83,     1] loss: 0.127
[84,     1] loss: 0.123
[85,     1] loss: 0.124
[86,     1] loss: 0.122
[87,     1] loss: 0.123
[88,     1] loss: 0.123
[89,     1] loss: 0.121
[90,     1] loss: 0.121
[91,     1] loss: 0.123
[92,     1] loss: 0.123
[93,     1] loss: 0.124
[94,     1] loss: 0.121
[95,     1] loss: 0.122
[96,     1] loss: 0.125
[97,     1] loss: 0.124
[98,     1] loss: 0.122
[99,     1] loss: 0.124
[100,     1] loss: 0.123
[101,     1] loss: 0.122
[102,     1] loss: 0.125
[103,     1] loss: 0.124
[104,     1] loss: 0.123
[105,     1] loss: 0.123
[106,     1] loss: 0.126
[107,     1] loss: 0.134
[108,     1] loss: 0.418
Early stopping applied (best metric=0.4030125141143799)
Finished Training
Total time taken: 20.274346828460693
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.678
[4,     1] loss: 0.669
[5,     1] loss: 0.658
[6,     1] loss: 0.644
[7,     1] loss: 0.631
[8,     1] loss: 0.615
[9,     1] loss: 0.599
[10,     1] loss: 0.579
[11,     1] loss: 0.561
[12,     1] loss: 0.543
[13,     1] loss: 0.522
[14,     1] loss: 0.500
[15,     1] loss: 0.481
[16,     1] loss: 0.458
[17,     1] loss: 0.436
[18,     1] loss: 0.416
[19,     1] loss: 0.394
[20,     1] loss: 0.372
[21,     1] loss: 0.349
[22,     1] loss: 0.331
[23,     1] loss: 0.309
[24,     1] loss: 0.291
[25,     1] loss: 0.275
[26,     1] loss: 0.258
[27,     1] loss: 0.242
[28,     1] loss: 0.228
[29,     1] loss: 0.218
[30,     1] loss: 0.211
[31,     1] loss: 0.196
[32,     1] loss: 0.190
[33,     1] loss: 0.181
[34,     1] loss: 0.175
[35,     1] loss: 0.172
[36,     1] loss: 0.164
[37,     1] loss: 0.162
[38,     1] loss: 0.157
[39,     1] loss: 0.155
[40,     1] loss: 0.151
[41,     1] loss: 0.149
[42,     1] loss: 0.148
[43,     1] loss: 0.150
[44,     1] loss: 0.152
[45,     1] loss: 0.154
[46,     1] loss: 0.151
[47,     1] loss: 0.149
[48,     1] loss: 0.148
[49,     1] loss: 0.142
[50,     1] loss: 0.178
[51,     1] loss: 0.379
[52,     1] loss: 0.263
[53,     1] loss: 0.296
[54,     1] loss: 0.281
[55,     1] loss: 0.258
[56,     1] loss: 0.250
[57,     1] loss: 0.251
[58,     1] loss: 0.237
[59,     1] loss: 0.229
[60,     1] loss: 0.215
[61,     1] loss: 0.204
[62,     1] loss: 0.197
[63,     1] loss: 0.194
[64,     1] loss: 0.180
[65,     1] loss: 0.171
[66,     1] loss: 0.161
[67,     1] loss: 0.153
[68,     1] loss: 0.145
[69,     1] loss: 0.139
[70,     1] loss: 0.133
[71,     1] loss: 0.128
[72,     1] loss: 0.125
[73,     1] loss: 0.122
[74,     1] loss: 0.119
[75,     1] loss: 0.116
[76,     1] loss: 0.115
[77,     1] loss: 0.115
[78,     1] loss: 0.115
[79,     1] loss: 0.114
[80,     1] loss: 0.114
[81,     1] loss: 0.115
[82,     1] loss: 0.114
[83,     1] loss: 0.113
[84,     1] loss: 0.116
[85,     1] loss: 0.116
[86,     1] loss: 0.117
[87,     1] loss: 0.118
[88,     1] loss: 0.117
[89,     1] loss: 0.119
[90,     1] loss: 0.117
[91,     1] loss: 0.119
[92,     1] loss: 0.117
[93,     1] loss: 0.120
[94,     1] loss: 0.121
[95,     1] loss: 0.120
[96,     1] loss: 0.123
[97,     1] loss: 0.121
[98,     1] loss: 0.119
[99,     1] loss: 0.119
[100,     1] loss: 0.120
[101,     1] loss: 0.117
[102,     1] loss: 0.121
[103,     1] loss: 0.119
[104,     1] loss: 0.121
[105,     1] loss: 0.119
[106,     1] loss: 0.118
[107,     1] loss: 0.118
[108,     1] loss: 0.138
[109,     1] loss: 0.460
[110,     1] loss: 0.975
[111,     1] loss: 0.907
[112,     1] loss: 0.799
[113,     1] loss: 0.743
[114,     1] loss: 0.703
[115,     1] loss: 0.684
[116,     1] loss: 0.677
[117,     1] loss: 0.678
[118,     1] loss: 0.679
[119,     1] loss: 0.681
[120,     1] loss: 0.682
[121,     1] loss: 0.684
[122,     1] loss: 0.685
[123,     1] loss: 0.686
[124,     1] loss: 0.687
[125,     1] loss: 0.688
[126,     1] loss: 0.688
[127,     1] loss: 0.689
Early stopping applied (best metric=0.35137662291526794)
Finished Training
Total time taken: 24.511861562728882
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.666
[5,     1] loss: 0.656
[6,     1] loss: 0.647
[7,     1] loss: 0.635
[8,     1] loss: 0.621
[9,     1] loss: 0.608
[10,     1] loss: 0.591
[11,     1] loss: 0.575
[12,     1] loss: 0.559
[13,     1] loss: 0.536
[14,     1] loss: 0.517
[15,     1] loss: 0.493
[16,     1] loss: 0.472
[17,     1] loss: 0.447
[18,     1] loss: 0.426
[19,     1] loss: 0.404
[20,     1] loss: 0.382
[21,     1] loss: 0.356
[22,     1] loss: 0.337
[23,     1] loss: 0.315
[24,     1] loss: 0.300
[25,     1] loss: 0.281
[26,     1] loss: 0.267
[27,     1] loss: 0.253
[28,     1] loss: 0.238
[29,     1] loss: 0.224
[30,     1] loss: 0.213
[31,     1] loss: 0.204
[32,     1] loss: 0.196
[33,     1] loss: 0.186
[34,     1] loss: 0.181
[35,     1] loss: 0.174
[36,     1] loss: 0.168
[37,     1] loss: 0.162
[38,     1] loss: 0.160
[39,     1] loss: 0.157
[40,     1] loss: 0.153
[41,     1] loss: 0.148
[42,     1] loss: 0.147
[43,     1] loss: 0.146
[44,     1] loss: 0.145
[45,     1] loss: 0.141
[46,     1] loss: 0.144
[47,     1] loss: 0.178
[48,     1] loss: 0.181
[49,     1] loss: 0.358
[50,     1] loss: 0.326
[51,     1] loss: 0.286
[52,     1] loss: 0.286
[53,     1] loss: 0.255
[54,     1] loss: 0.263
[55,     1] loss: 0.259
[56,     1] loss: 0.252
[57,     1] loss: 0.244
[58,     1] loss: 0.235
[59,     1] loss: 0.227
[60,     1] loss: 0.218
[61,     1] loss: 0.208
[62,     1] loss: 0.199
[63,     1] loss: 0.189
[64,     1] loss: 0.181
[65,     1] loss: 0.173
[66,     1] loss: 0.165
[67,     1] loss: 0.160
[68,     1] loss: 0.153
[69,     1] loss: 0.148
[70,     1] loss: 0.145
[71,     1] loss: 0.139
[72,     1] loss: 0.137
[73,     1] loss: 0.137
[74,     1] loss: 0.136
[75,     1] loss: 0.133
[76,     1] loss: 0.133
[77,     1] loss: 0.133
[78,     1] loss: 0.131
[79,     1] loss: 0.131
[80,     1] loss: 0.132
[81,     1] loss: 0.133
[82,     1] loss: 0.129
[83,     1] loss: 0.132
[84,     1] loss: 0.132
[85,     1] loss: 0.132
[86,     1] loss: 0.131
[87,     1] loss: 0.132
[88,     1] loss: 0.131
[89,     1] loss: 0.133
[90,     1] loss: 0.131
[91,     1] loss: 0.131
[92,     1] loss: 0.140
[93,     1] loss: 0.329
[94,     1] loss: 0.388
[95,     1] loss: 0.832
[96,     1] loss: 0.615
[97,     1] loss: 0.504
[98,     1] loss: 0.525
[99,     1] loss: 0.542
[100,     1] loss: 0.552
[101,     1] loss: 0.557
[102,     1] loss: 0.557
[103,     1] loss: 0.561
[104,     1] loss: 0.566
[105,     1] loss: 0.571
[106,     1] loss: 0.574
[107,     1] loss: 0.576
[108,     1] loss: 0.578
[109,     1] loss: 0.578
[110,     1] loss: 0.579
[111,     1] loss: 0.579
[112,     1] loss: 0.579
[113,     1] loss: 0.579
[114,     1] loss: 0.579
[115,     1] loss: 0.578
Early stopping applied (best metric=0.36053410172462463)
Finished Training
Total time taken: 22.674399614334106
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.654
[4,     1] loss: 0.634
[5,     1] loss: 0.616
[6,     1] loss: 0.598
[7,     1] loss: 0.579
[8,     1] loss: 0.557
[9,     1] loss: 0.534
[10,     1] loss: 0.513
[11,     1] loss: 0.484
[12,     1] loss: 0.460
[13,     1] loss: 0.440
[14,     1] loss: 0.420
[15,     1] loss: 0.400
[16,     1] loss: 0.386
[17,     1] loss: 0.381
[18,     1] loss: 0.360
[19,     1] loss: 0.352
[20,     1] loss: 0.335
[21,     1] loss: 0.331
[22,     1] loss: 0.321
[23,     1] loss: 0.317
[24,     1] loss: 0.309
[25,     1] loss: 0.307
[26,     1] loss: 0.300
[27,     1] loss: 0.297
[28,     1] loss: 0.288
[29,     1] loss: 0.282
[30,     1] loss: 0.278
[31,     1] loss: 0.274
[32,     1] loss: 0.269
[33,     1] loss: 0.261
[34,     1] loss: 0.256
[35,     1] loss: 0.249
[36,     1] loss: 0.243
[37,     1] loss: 0.238
[38,     1] loss: 0.230
[39,     1] loss: 0.225
[40,     1] loss: 0.218
[41,     1] loss: 0.211
[42,     1] loss: 0.204
[43,     1] loss: 0.197
[44,     1] loss: 0.194
[45,     1] loss: 0.187
[46,     1] loss: 0.185
[47,     1] loss: 0.180
[48,     1] loss: 0.174
[49,     1] loss: 0.170
[50,     1] loss: 0.165
[51,     1] loss: 0.160
[52,     1] loss: 0.154
[53,     1] loss: 0.148
[54,     1] loss: 0.144
[55,     1] loss: 0.138
[56,     1] loss: 0.133
[57,     1] loss: 0.192
[58,     1] loss: 0.495
[59,     1] loss: 0.485
[60,     1] loss: 0.376
[61,     1] loss: 0.381
[62,     1] loss: 0.387
[63,     1] loss: 0.367
[64,     1] loss: 0.351
[65,     1] loss: 0.341
[66,     1] loss: 0.323
[67,     1] loss: 0.316
[68,     1] loss: 0.312
[69,     1] loss: 0.302
[70,     1] loss: 0.295
[71,     1] loss: 0.285
[72,     1] loss: 0.278
[73,     1] loss: 0.267
[74,     1] loss: 0.259
[75,     1] loss: 0.252
[76,     1] loss: 0.241
[77,     1] loss: 0.236
[78,     1] loss: 0.228
[79,     1] loss: 0.223
[80,     1] loss: 0.216
[81,     1] loss: 0.210
[82,     1] loss: 0.205
[83,     1] loss: 0.202
[84,     1] loss: 0.197
[85,     1] loss: 0.192
[86,     1] loss: 0.189
[87,     1] loss: 0.183
[88,     1] loss: 0.181
Early stopping applied (best metric=0.37618547677993774)
Finished Training
Total time taken: 18.049310445785522
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.682
[4,     1] loss: 0.674
[5,     1] loss: 0.665
[6,     1] loss: 0.655
[7,     1] loss: 0.645
[8,     1] loss: 0.634
[9,     1] loss: 0.620
[10,     1] loss: 0.605
[11,     1] loss: 0.589
[12,     1] loss: 0.570
[13,     1] loss: 0.552
[14,     1] loss: 0.529
[15,     1] loss: 0.508
[16,     1] loss: 0.491
[17,     1] loss: 0.470
[18,     1] loss: 0.447
[19,     1] loss: 0.430
[20,     1] loss: 0.412
[21,     1] loss: 0.396
[22,     1] loss: 0.378
[23,     1] loss: 0.364
[24,     1] loss: 0.351
[25,     1] loss: 0.340
[26,     1] loss: 0.326
[27,     1] loss: 0.315
[28,     1] loss: 0.303
[29,     1] loss: 0.296
[30,     1] loss: 0.286
[31,     1] loss: 0.275
[32,     1] loss: 0.267
[33,     1] loss: 0.259
[34,     1] loss: 0.250
[35,     1] loss: 0.240
[36,     1] loss: 0.233
[37,     1] loss: 0.228
[38,     1] loss: 0.217
[39,     1] loss: 0.211
[40,     1] loss: 0.206
[41,     1] loss: 0.198
[42,     1] loss: 0.192
[43,     1] loss: 0.184
[44,     1] loss: 0.179
[45,     1] loss: 0.174
[46,     1] loss: 0.171
[47,     1] loss: 0.179
[48,     1] loss: 0.249
[49,     1] loss: 0.301
[50,     1] loss: 0.296
[51,     1] loss: 0.235
[52,     1] loss: 0.271
[53,     1] loss: 0.269
[54,     1] loss: 0.256
[55,     1] loss: 0.258
[56,     1] loss: 0.250
[57,     1] loss: 0.236
[58,     1] loss: 0.231
[59,     1] loss: 0.228
[60,     1] loss: 0.220
[61,     1] loss: 0.212
[62,     1] loss: 0.205
[63,     1] loss: 0.195
[64,     1] loss: 0.188
[65,     1] loss: 0.180
[66,     1] loss: 0.173
[67,     1] loss: 0.166
[68,     1] loss: 0.161
[69,     1] loss: 0.161
[70,     1] loss: 0.176
[71,     1] loss: 0.199
[72,     1] loss: 0.179
[73,     1] loss: 0.159
[74,     1] loss: 0.162
[75,     1] loss: 0.166
[76,     1] loss: 0.161
[77,     1] loss: 0.159
[78,     1] loss: 0.157
[79,     1] loss: 0.155
[80,     1] loss: 0.153
[81,     1] loss: 0.150
[82,     1] loss: 0.148
[83,     1] loss: 0.145
[84,     1] loss: 0.145
[85,     1] loss: 0.144
[86,     1] loss: 0.142
[87,     1] loss: 0.142
[88,     1] loss: 0.141
[89,     1] loss: 0.139
[90,     1] loss: 0.137
[91,     1] loss: 0.139
[92,     1] loss: 0.138
[93,     1] loss: 0.136
[94,     1] loss: 0.139
[95,     1] loss: 0.137
[96,     1] loss: 0.138
[97,     1] loss: 0.136
[98,     1] loss: 0.137
[99,     1] loss: 0.138
[100,     1] loss: 0.138
[101,     1] loss: 0.138
Early stopping applied (best metric=0.3955767750740051)
Finished Training
Total time taken: 22.93839430809021
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.680
[3,     1] loss: 0.667
[4,     1] loss: 0.653
[5,     1] loss: 0.641
[6,     1] loss: 0.624
[7,     1] loss: 0.607
[8,     1] loss: 0.593
[9,     1] loss: 0.574
[10,     1] loss: 0.557
[11,     1] loss: 0.542
[12,     1] loss: 0.526
[13,     1] loss: 0.511
[14,     1] loss: 0.493
[15,     1] loss: 0.488
[16,     1] loss: 0.476
[17,     1] loss: 0.459
[18,     1] loss: 0.453
[19,     1] loss: 0.438
[20,     1] loss: 0.429
[21,     1] loss: 0.416
[22,     1] loss: 0.406
[23,     1] loss: 0.400
[24,     1] loss: 0.395
[25,     1] loss: 0.388
[26,     1] loss: 0.386
[27,     1] loss: 0.380
[28,     1] loss: 0.375
[29,     1] loss: 0.370
[30,     1] loss: 0.366
[31,     1] loss: 0.365
[32,     1] loss: 0.358
[33,     1] loss: 0.350
[34,     1] loss: 0.348
[35,     1] loss: 0.341
[36,     1] loss: 0.336
[37,     1] loss: 0.328
[38,     1] loss: 0.321
[39,     1] loss: 0.312
[40,     1] loss: 0.305
[41,     1] loss: 0.296
[42,     1] loss: 0.284
[43,     1] loss: 0.275
[44,     1] loss: 0.262
[45,     1] loss: 0.246
[46,     1] loss: 0.233
[47,     1] loss: 0.219
[48,     1] loss: 0.203
[49,     1] loss: 0.186
[50,     1] loss: 0.171
[51,     1] loss: 0.155
[52,     1] loss: 0.139
[53,     1] loss: 0.124
[54,     1] loss: 0.112
[55,     1] loss: 0.100
[56,     1] loss: 0.091
[57,     1] loss: 0.081
[58,     1] loss: 0.074
[59,     1] loss: 0.066
[60,     1] loss: 0.064
[61,     1] loss: 0.060
[62,     1] loss: 0.058
[63,     1] loss: 0.055
[64,     1] loss: 0.054
[65,     1] loss: 0.053
[66,     1] loss: 0.052
[67,     1] loss: 0.052
[68,     1] loss: 0.053
[69,     1] loss: 0.054
[70,     1] loss: 0.054
[71,     1] loss: 0.056
[72,     1] loss: 0.058
[73,     1] loss: 0.059
Early stopping applied (best metric=0.4066323935985565)
Finished Training
Total time taken: 16.707296133041382
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.681
[3,     1] loss: 0.663
[4,     1] loss: 0.648
[5,     1] loss: 0.635
[6,     1] loss: 0.622
[7,     1] loss: 0.607
[8,     1] loss: 0.592
[9,     1] loss: 0.573
[10,     1] loss: 0.558
[11,     1] loss: 0.538
[12,     1] loss: 0.517
[13,     1] loss: 0.499
[14,     1] loss: 0.477
[15,     1] loss: 0.459
[16,     1] loss: 0.448
[17,     1] loss: 0.444
[18,     1] loss: 0.427
[19,     1] loss: 0.425
[20,     1] loss: 0.405
[21,     1] loss: 0.404
[22,     1] loss: 0.394
[23,     1] loss: 0.390
[24,     1] loss: 0.381
[25,     1] loss: 0.375
[26,     1] loss: 0.370
[27,     1] loss: 0.367
[28,     1] loss: 0.363
[29,     1] loss: 0.362
[30,     1] loss: 0.358
[31,     1] loss: 0.356
[32,     1] loss: 0.356
[33,     1] loss: 0.355
[34,     1] loss: 0.357
[35,     1] loss: 0.354
[36,     1] loss: 0.360
[37,     1] loss: 0.399
[38,     1] loss: 0.401
[39,     1] loss: 0.371
[40,     1] loss: 0.376
[41,     1] loss: 0.374
[42,     1] loss: 0.371
[43,     1] loss: 0.364
[44,     1] loss: 0.362
[45,     1] loss: 0.357
[46,     1] loss: 0.354
[47,     1] loss: 0.353
[48,     1] loss: 0.351
[49,     1] loss: 0.348
[50,     1] loss: 0.348
[51,     1] loss: 0.345
[52,     1] loss: 0.346
[53,     1] loss: 0.351
[54,     1] loss: 0.348
[55,     1] loss: 0.349
[56,     1] loss: 0.345
[57,     1] loss: 0.340
[58,     1] loss: 0.336
[59,     1] loss: 0.336
[60,     1] loss: 0.331
[61,     1] loss: 0.330
[62,     1] loss: 0.327
[63,     1] loss: 0.324
[64,     1] loss: 0.320
[65,     1] loss: 0.316
[66,     1] loss: 0.313
[67,     1] loss: 0.324
[68,     1] loss: 0.424
[69,     1] loss: 0.542
[70,     1] loss: 0.405
[71,     1] loss: 0.406
[72,     1] loss: 0.398
[73,     1] loss: 0.385
[74,     1] loss: 0.376
[75,     1] loss: 0.361
[76,     1] loss: 0.353
[77,     1] loss: 0.341
[78,     1] loss: 0.326
[79,     1] loss: 0.313
[80,     1] loss: 0.299
[81,     1] loss: 0.285
[82,     1] loss: 0.274
[83,     1] loss: 0.262
[84,     1] loss: 0.254
[85,     1] loss: 0.234
[86,     1] loss: 0.236
[87,     1] loss: 0.225
[88,     1] loss: 0.201
[89,     1] loss: 0.189
[90,     1] loss: 0.172
[91,     1] loss: 0.163
[92,     1] loss: 0.152
[93,     1] loss: 0.144
[94,     1] loss: 0.137
[95,     1] loss: 0.127
[96,     1] loss: 0.122
[97,     1] loss: 0.115
[98,     1] loss: 0.111
[99,     1] loss: 0.105
[100,     1] loss: 0.103
[101,     1] loss: 0.101
[102,     1] loss: 0.099
[103,     1] loss: 0.097
[104,     1] loss: 0.098
[105,     1] loss: 0.096
[106,     1] loss: 0.096
[107,     1] loss: 0.097
[108,     1] loss: 0.098
[109,     1] loss: 0.098
[110,     1] loss: 0.101
[111,     1] loss: 0.105
[112,     1] loss: 0.146
[113,     1] loss: 0.205
[114,     1] loss: 0.357
[115,     1] loss: 0.803
[116,     1] loss: 0.384
[117,     1] loss: 0.456
[118,     1] loss: 0.499
[119,     1] loss: 0.494
[120,     1] loss: 0.483
[121,     1] loss: 0.481
[122,     1] loss: 0.484
[123,     1] loss: 0.482
[124,     1] loss: 0.470
[125,     1] loss: 0.457
[126,     1] loss: 0.441
[127,     1] loss: 0.426
[128,     1] loss: 0.413
[129,     1] loss: 0.395
[130,     1] loss: 0.386
[131,     1] loss: 0.369
[132,     1] loss: 0.357
[133,     1] loss: 0.344
[134,     1] loss: 0.331
[135,     1] loss: 0.320
[136,     1] loss: 0.308
[137,     1] loss: 0.296
Early stopping applied (best metric=0.3960251212120056)
Finished Training
Total time taken: 30.610522270202637
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.659
[4,     1] loss: 0.644
[5,     1] loss: 0.629
[6,     1] loss: 0.613
[7,     1] loss: 0.596
[8,     1] loss: 0.579
[9,     1] loss: 0.560
[10,     1] loss: 0.539
[11,     1] loss: 0.517
[12,     1] loss: 0.493
[13,     1] loss: 0.470
[14,     1] loss: 0.445
[15,     1] loss: 0.423
[16,     1] loss: 0.400
[17,     1] loss: 0.379
[18,     1] loss: 0.363
[19,     1] loss: 0.342
[20,     1] loss: 0.326
[21,     1] loss: 0.309
[22,     1] loss: 0.292
[23,     1] loss: 0.278
[24,     1] loss: 0.265
[25,     1] loss: 0.252
[26,     1] loss: 0.240
[27,     1] loss: 0.231
[28,     1] loss: 0.221
[29,     1] loss: 0.213
[30,     1] loss: 0.206
[31,     1] loss: 0.199
[32,     1] loss: 0.195
[33,     1] loss: 0.190
[34,     1] loss: 0.185
[35,     1] loss: 0.180
[36,     1] loss: 0.178
[37,     1] loss: 0.174
[38,     1] loss: 0.172
[39,     1] loss: 0.168
[40,     1] loss: 0.167
[41,     1] loss: 0.165
[42,     1] loss: 0.164
[43,     1] loss: 0.166
[44,     1] loss: 0.173
[45,     1] loss: 0.172
[46,     1] loss: 0.173
[47,     1] loss: 0.170
[48,     1] loss: 0.167
[49,     1] loss: 0.165
[50,     1] loss: 0.164
[51,     1] loss: 0.159
[52,     1] loss: 0.152
[53,     1] loss: 0.210
[54,     1] loss: 0.380
[55,     1] loss: 0.324
[56,     1] loss: 0.270
[57,     1] loss: 0.285
[58,     1] loss: 0.292
[59,     1] loss: 0.262
[60,     1] loss: 0.253
[61,     1] loss: 0.243
[62,     1] loss: 0.231
[63,     1] loss: 0.222
[64,     1] loss: 0.213
[65,     1] loss: 0.204
[66,     1] loss: 0.196
[67,     1] loss: 0.182
[68,     1] loss: 0.171
[69,     1] loss: 0.165
[70,     1] loss: 0.162
[71,     1] loss: 0.156
[72,     1] loss: 0.147
[73,     1] loss: 0.140
[74,     1] loss: 0.134
[75,     1] loss: 0.134
[76,     1] loss: 0.128
[77,     1] loss: 0.128
[78,     1] loss: 0.125
[79,     1] loss: 0.121
[80,     1] loss: 0.120
[81,     1] loss: 0.119
[82,     1] loss: 0.117
[83,     1] loss: 0.117
[84,     1] loss: 0.116
[85,     1] loss: 0.118
[86,     1] loss: 0.117
[87,     1] loss: 0.118
[88,     1] loss: 0.119
[89,     1] loss: 0.120
[90,     1] loss: 0.118
[91,     1] loss: 0.119
[92,     1] loss: 0.119
[93,     1] loss: 0.120
[94,     1] loss: 0.118
[95,     1] loss: 0.119
[96,     1] loss: 0.122
[97,     1] loss: 0.122
[98,     1] loss: 0.121
[99,     1] loss: 0.121
[100,     1] loss: 0.119
[101,     1] loss: 0.119
[102,     1] loss: 0.122
[103,     1] loss: 0.122
[104,     1] loss: 0.119
[105,     1] loss: 0.121
[106,     1] loss: 0.123
[107,     1] loss: 0.122
[108,     1] loss: 0.140
[109,     1] loss: 0.237
[110,     1] loss: 0.746
[111,     1] loss: 0.751
[112,     1] loss: 0.641
[113,     1] loss: 0.652
[114,     1] loss: 0.661
[115,     1] loss: 0.669
[116,     1] loss: 0.671
[117,     1] loss: 0.665
[118,     1] loss: 0.657
[119,     1] loss: 0.655
[120,     1] loss: 0.651
[121,     1] loss: 0.648
[122,     1] loss: 0.649
[123,     1] loss: 0.649
[124,     1] loss: 0.649
[125,     1] loss: 0.648
[126,     1] loss: 0.650
[127,     1] loss: 0.651
[128,     1] loss: 0.652
[129,     1] loss: 0.653
[130,     1] loss: 0.653
[131,     1] loss: 0.653
[132,     1] loss: 0.654
[133,     1] loss: 0.654
[134,     1] loss: 0.655
[135,     1] loss: 0.655
[136,     1] loss: 0.654
[137,     1] loss: 0.654
[138,     1] loss: 0.654
Early stopping applied (best metric=0.3564454913139343)
Finished Training
Total time taken: 32.050548791885376
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.694
[3,     1] loss: 0.676
[4,     1] loss: 0.663
[5,     1] loss: 0.648
[6,     1] loss: 0.636
[7,     1] loss: 0.622
[8,     1] loss: 0.610
[9,     1] loss: 0.594
[10,     1] loss: 0.579
[11,     1] loss: 0.564
[12,     1] loss: 0.546
[13,     1] loss: 0.529
[14,     1] loss: 0.509
[15,     1] loss: 0.493
[16,     1] loss: 0.475
[17,     1] loss: 0.456
[18,     1] loss: 0.435
[19,     1] loss: 0.420
[20,     1] loss: 0.402
[21,     1] loss: 0.382
[22,     1] loss: 0.357
[23,     1] loss: 0.341
[24,     1] loss: 0.321
[25,     1] loss: 0.302
[26,     1] loss: 0.288
[27,     1] loss: 0.271
[28,     1] loss: 0.252
[29,     1] loss: 0.242
[30,     1] loss: 0.230
[31,     1] loss: 0.221
[32,     1] loss: 0.214
[33,     1] loss: 0.207
[34,     1] loss: 0.202
[35,     1] loss: 0.195
[36,     1] loss: 0.190
[37,     1] loss: 0.185
[38,     1] loss: 0.181
[39,     1] loss: 0.181
[40,     1] loss: 0.177
[41,     1] loss: 0.175
[42,     1] loss: 0.174
[43,     1] loss: 0.173
[44,     1] loss: 0.172
[45,     1] loss: 0.170
[46,     1] loss: 0.169
[47,     1] loss: 0.166
[48,     1] loss: 0.169
[49,     1] loss: 0.180
[50,     1] loss: 0.198
[51,     1] loss: 0.621
[52,     1] loss: 0.441
[53,     1] loss: 0.456
[54,     1] loss: 0.415
[55,     1] loss: 0.402
[56,     1] loss: 0.394
[57,     1] loss: 0.391
[58,     1] loss: 0.378
[59,     1] loss: 0.369
[60,     1] loss: 0.367
[61,     1] loss: 0.366
[62,     1] loss: 0.359
[63,     1] loss: 0.347
[64,     1] loss: 0.336
[65,     1] loss: 0.318
[66,     1] loss: 0.311
[67,     1] loss: 0.301
[68,     1] loss: 0.291
[69,     1] loss: 0.280
[70,     1] loss: 0.269
[71,     1] loss: 0.257
[72,     1] loss: 0.251
[73,     1] loss: 0.240
[74,     1] loss: 0.231
[75,     1] loss: 0.225
[76,     1] loss: 0.216
[77,     1] loss: 0.211
[78,     1] loss: 0.205
[79,     1] loss: 0.197
[80,     1] loss: 0.195
[81,     1] loss: 0.191
[82,     1] loss: 0.189
[83,     1] loss: 0.185
[84,     1] loss: 0.183
[85,     1] loss: 0.182
[86,     1] loss: 0.178
[87,     1] loss: 0.175
[88,     1] loss: 0.174
[89,     1] loss: 0.171
[90,     1] loss: 0.172
[91,     1] loss: 0.169
[92,     1] loss: 0.167
[93,     1] loss: 0.168
[94,     1] loss: 0.166
[95,     1] loss: 0.167
[96,     1] loss: 0.166
[97,     1] loss: 0.167
[98,     1] loss: 0.222
[99,     1] loss: 0.324
[100,     1] loss: 0.423
[101,     1] loss: 0.669
Early stopping applied (best metric=0.3922387361526489)
Finished Training
Total time taken: 24.20143222808838
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.667
[4,     1] loss: 0.655
[5,     1] loss: 0.639
[6,     1] loss: 0.623
[7,     1] loss: 0.608
[8,     1] loss: 0.590
[9,     1] loss: 0.571
[10,     1] loss: 0.551
[11,     1] loss: 0.531
[12,     1] loss: 0.505
[13,     1] loss: 0.481
[14,     1] loss: 0.454
[15,     1] loss: 0.426
[16,     1] loss: 0.400
[17,     1] loss: 0.374
[18,     1] loss: 0.344
[19,     1] loss: 0.323
[20,     1] loss: 0.304
[21,     1] loss: 0.285
[22,     1] loss: 0.270
[23,     1] loss: 0.254
[24,     1] loss: 0.237
[25,     1] loss: 0.225
[26,     1] loss: 0.213
[27,     1] loss: 0.207
[28,     1] loss: 0.195
[29,     1] loss: 0.193
[30,     1] loss: 0.182
[31,     1] loss: 0.176
[32,     1] loss: 0.171
[33,     1] loss: 0.169
[34,     1] loss: 0.165
[35,     1] loss: 0.161
[36,     1] loss: 0.157
[37,     1] loss: 0.156
[38,     1] loss: 0.153
[39,     1] loss: 0.151
[40,     1] loss: 0.148
[41,     1] loss: 0.146
[42,     1] loss: 0.147
[43,     1] loss: 0.147
[44,     1] loss: 0.145
[45,     1] loss: 0.145
[46,     1] loss: 0.143
[47,     1] loss: 0.143
[48,     1] loss: 0.141
[49,     1] loss: 0.138
[50,     1] loss: 0.134
[51,     1] loss: 0.130
[52,     1] loss: 0.130
[53,     1] loss: 0.246
[54,     1] loss: 0.610
[55,     1] loss: 0.511
[56,     1] loss: 0.412
[57,     1] loss: 0.372
[58,     1] loss: 0.365
[59,     1] loss: 0.349
[60,     1] loss: 0.340
[61,     1] loss: 0.327
[62,     1] loss: 0.315
[63,     1] loss: 0.304
[64,     1] loss: 0.293
[65,     1] loss: 0.284
[66,     1] loss: 0.270
[67,     1] loss: 0.262
[68,     1] loss: 0.250
[69,     1] loss: 0.239
[70,     1] loss: 0.227
[71,     1] loss: 0.215
[72,     1] loss: 0.200
[73,     1] loss: 0.192
[74,     1] loss: 0.185
[75,     1] loss: 0.175
[76,     1] loss: 0.169
[77,     1] loss: 0.163
[78,     1] loss: 0.157
[79,     1] loss: 0.151
[80,     1] loss: 0.145
[81,     1] loss: 0.142
[82,     1] loss: 0.140
[83,     1] loss: 0.136
[84,     1] loss: 0.133
[85,     1] loss: 0.132
[86,     1] loss: 0.130
[87,     1] loss: 0.132
[88,     1] loss: 0.128
[89,     1] loss: 0.126
[90,     1] loss: 0.125
[91,     1] loss: 0.128
[92,     1] loss: 0.126
[93,     1] loss: 0.126
[94,     1] loss: 0.134
[95,     1] loss: 0.203
[96,     1] loss: 0.190
[97,     1] loss: 0.171
[98,     1] loss: 0.342
[99,     1] loss: 0.304
[100,     1] loss: 0.314
[101,     1] loss: 0.359
[102,     1] loss: 0.338
[103,     1] loss: 0.295
[104,     1] loss: 0.269
[105,     1] loss: 0.278
[106,     1] loss: 0.258
[107,     1] loss: 0.255
[108,     1] loss: 0.254
[109,     1] loss: 0.247
[110,     1] loss: 0.235
[111,     1] loss: 0.227
[112,     1] loss: 0.223
[113,     1] loss: 0.213
[114,     1] loss: 0.205
[115,     1] loss: 0.202
[116,     1] loss: 0.208
[117,     1] loss: 0.201
[118,     1] loss: 0.189
[119,     1] loss: 0.208
[120,     1] loss: 0.213
[121,     1] loss: 0.220
[122,     1] loss: 0.217
[123,     1] loss: 0.208
[124,     1] loss: 0.203
[125,     1] loss: 0.201
[126,     1] loss: 0.198
[127,     1] loss: 0.187
[128,     1] loss: 0.182
[129,     1] loss: 0.173
[130,     1] loss: 0.167
[131,     1] loss: 0.164
[132,     1] loss: 0.161
[133,     1] loss: 0.159
[134,     1] loss: 0.156
[135,     1] loss: 0.152
[136,     1] loss: 0.149
[137,     1] loss: 0.151
[138,     1] loss: 0.151
[139,     1] loss: 0.149
[140,     1] loss: 0.147
[141,     1] loss: 0.145
[142,     1] loss: 0.148
[143,     1] loss: 0.146
[144,     1] loss: 0.147
[145,     1] loss: 0.144
Early stopping applied (best metric=0.3625241816043854)
Finished Training
Total time taken: 36.84263896942139
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.675
[3,     1] loss: 0.660
[4,     1] loss: 0.647
[5,     1] loss: 0.635
[6,     1] loss: 0.619
[7,     1] loss: 0.604
[8,     1] loss: 0.588
[9,     1] loss: 0.568
[10,     1] loss: 0.550
[11,     1] loss: 0.532
[12,     1] loss: 0.510
[13,     1] loss: 0.494
[14,     1] loss: 0.484
[15,     1] loss: 0.491
[16,     1] loss: 0.469
[17,     1] loss: 0.480
[18,     1] loss: 0.446
[19,     1] loss: 0.442
[20,     1] loss: 0.434
[21,     1] loss: 0.419
[22,     1] loss: 0.419
[23,     1] loss: 0.402
[24,     1] loss: 0.400
[25,     1] loss: 0.391
[26,     1] loss: 0.386
[27,     1] loss: 0.381
[28,     1] loss: 0.377
[29,     1] loss: 0.373
[30,     1] loss: 0.370
[31,     1] loss: 0.367
[32,     1] loss: 0.365
[33,     1] loss: 0.364
[34,     1] loss: 0.364
[35,     1] loss: 0.363
[36,     1] loss: 0.362
[37,     1] loss: 0.363
[38,     1] loss: 0.361
[39,     1] loss: 0.365
[40,     1] loss: 0.381
[41,     1] loss: 0.418
[42,     1] loss: 0.370
[43,     1] loss: 0.384
[44,     1] loss: 0.384
[45,     1] loss: 0.378
[46,     1] loss: 0.375
[47,     1] loss: 0.370
[48,     1] loss: 0.366
[49,     1] loss: 0.366
[50,     1] loss: 0.365
[51,     1] loss: 0.360
[52,     1] loss: 0.361
[53,     1] loss: 0.358
[54,     1] loss: 0.356
[55,     1] loss: 0.357
[56,     1] loss: 0.358
[57,     1] loss: 0.357
[58,     1] loss: 0.359
[59,     1] loss: 0.359
[60,     1] loss: 0.359
[61,     1] loss: 0.357
[62,     1] loss: 0.356
[63,     1] loss: 0.358
Early stopping applied (best metric=0.4194321036338806)
Finished Training
Total time taken: 17.343225240707397
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.678
[3,     1] loss: 0.662
[4,     1] loss: 0.649
[5,     1] loss: 0.635
[6,     1] loss: 0.622
[7,     1] loss: 0.608
[8,     1] loss: 0.589
[9,     1] loss: 0.572
[10,     1] loss: 0.552
[11,     1] loss: 0.530
[12,     1] loss: 0.506
[13,     1] loss: 0.480
[14,     1] loss: 0.457
[15,     1] loss: 0.431
[16,     1] loss: 0.404
[17,     1] loss: 0.378
[18,     1] loss: 0.352
[19,     1] loss: 0.332
[20,     1] loss: 0.304
[21,     1] loss: 0.287
[22,     1] loss: 0.268
[23,     1] loss: 0.271
[24,     1] loss: 0.248
[25,     1] loss: 0.229
[26,     1] loss: 0.218
[27,     1] loss: 0.206
[28,     1] loss: 0.194
[29,     1] loss: 0.185
[30,     1] loss: 0.180
[31,     1] loss: 0.171
[32,     1] loss: 0.163
[33,     1] loss: 0.158
[34,     1] loss: 0.152
[35,     1] loss: 0.149
[36,     1] loss: 0.147
[37,     1] loss: 0.144
[38,     1] loss: 0.142
[39,     1] loss: 0.139
[40,     1] loss: 0.138
[41,     1] loss: 0.138
[42,     1] loss: 0.135
[43,     1] loss: 0.134
[44,     1] loss: 0.132
[45,     1] loss: 0.133
[46,     1] loss: 0.133
[47,     1] loss: 0.130
[48,     1] loss: 0.128
[49,     1] loss: 0.128
[50,     1] loss: 0.126
[51,     1] loss: 0.122
[52,     1] loss: 0.126
[53,     1] loss: 0.214
[54,     1] loss: 0.739
[55,     1] loss: 0.577
[56,     1] loss: 0.380
[57,     1] loss: 0.403
[58,     1] loss: 0.416
[59,     1] loss: 0.410
[60,     1] loss: 0.403
[61,     1] loss: 0.403
[62,     1] loss: 0.400
[63,     1] loss: 0.389
[64,     1] loss: 0.377
[65,     1] loss: 0.367
[66,     1] loss: 0.356
[67,     1] loss: 0.341
[68,     1] loss: 0.329
[69,     1] loss: 0.317
[70,     1] loss: 0.309
[71,     1] loss: 0.294
[72,     1] loss: 0.280
[73,     1] loss: 0.269
[74,     1] loss: 0.255
[75,     1] loss: 0.242
[76,     1] loss: 0.231
[77,     1] loss: 0.213
[78,     1] loss: 0.231
[79,     1] loss: 0.237
[80,     1] loss: 0.241
[81,     1] loss: 0.242
[82,     1] loss: 0.237
[83,     1] loss: 0.235
[84,     1] loss: 0.224
[85,     1] loss: 0.221
[86,     1] loss: 0.217
[87,     1] loss: 0.208
[88,     1] loss: 0.205
[89,     1] loss: 0.199
[90,     1] loss: 0.195
Early stopping applied (best metric=0.3183465600013733)
Finished Training
Total time taken: 23.78805184364319
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.651
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.611
[8,     1] loss: 0.598
[9,     1] loss: 0.583
[10,     1] loss: 0.568
[11,     1] loss: 0.549
[12,     1] loss: 0.532
[13,     1] loss: 0.512
[14,     1] loss: 0.494
[15,     1] loss: 0.475
[16,     1] loss: 0.459
[17,     1] loss: 0.447
[18,     1] loss: 0.422
[19,     1] loss: 0.407
[20,     1] loss: 0.394
[21,     1] loss: 0.378
[22,     1] loss: 0.370
[23,     1] loss: 0.361
[24,     1] loss: 0.347
[25,     1] loss: 0.332
[26,     1] loss: 0.324
[27,     1] loss: 0.314
[28,     1] loss: 0.306
[29,     1] loss: 0.296
[30,     1] loss: 0.287
[31,     1] loss: 0.277
[32,     1] loss: 0.265
[33,     1] loss: 0.257
[34,     1] loss: 0.245
[35,     1] loss: 0.234
[36,     1] loss: 0.225
[37,     1] loss: 0.214
[38,     1] loss: 0.207
[39,     1] loss: 0.198
[40,     1] loss: 0.189
[41,     1] loss: 0.179
[42,     1] loss: 0.173
[43,     1] loss: 0.163
[44,     1] loss: 0.153
[45,     1] loss: 0.150
[46,     1] loss: 0.143
[47,     1] loss: 0.137
[48,     1] loss: 0.135
[49,     1] loss: 0.128
[50,     1] loss: 0.124
[51,     1] loss: 0.119
[52,     1] loss: 0.114
[53,     1] loss: 0.111
[54,     1] loss: 0.104
[55,     1] loss: 0.101
[56,     1] loss: 0.099
[57,     1] loss: 0.097
[58,     1] loss: 0.193
[59,     1] loss: 0.729
[60,     1] loss: 0.352
[61,     1] loss: 0.517
[62,     1] loss: 0.441
[63,     1] loss: 0.469
[64,     1] loss: 0.442
[65,     1] loss: 0.436
[66,     1] loss: 0.440
[67,     1] loss: 0.434
[68,     1] loss: 0.433
[69,     1] loss: 0.433
[70,     1] loss: 0.426
[71,     1] loss: 0.418
[72,     1] loss: 0.408
[73,     1] loss: 0.396
[74,     1] loss: 0.384
[75,     1] loss: 0.371
[76,     1] loss: 0.358
[77,     1] loss: 0.340
[78,     1] loss: 0.325
[79,     1] loss: 0.307
[80,     1] loss: 0.290
[81,     1] loss: 0.273
[82,     1] loss: 0.258
[83,     1] loss: 0.235
[84,     1] loss: 0.220
[85,     1] loss: 0.213
[86,     1] loss: 0.194
Early stopping applied (best metric=0.4096550941467285)
Finished Training
Total time taken: 23.074241638183594
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.696
[3,     1] loss: 0.684
[4,     1] loss: 0.673
[5,     1] loss: 0.660
[6,     1] loss: 0.649
[7,     1] loss: 0.632
[8,     1] loss: 0.616
[9,     1] loss: 0.597
[10,     1] loss: 0.578
[11,     1] loss: 0.558
[12,     1] loss: 0.537
[13,     1] loss: 0.519
[14,     1] loss: 0.502
[15,     1] loss: 0.486
[16,     1] loss: 0.471
[17,     1] loss: 0.458
[18,     1] loss: 0.448
[19,     1] loss: 0.438
[20,     1] loss: 0.429
[21,     1] loss: 0.420
[22,     1] loss: 0.413
[23,     1] loss: 0.408
[24,     1] loss: 0.402
[25,     1] loss: 0.395
[26,     1] loss: 0.390
[27,     1] loss: 0.386
[28,     1] loss: 0.379
[29,     1] loss: 0.374
[30,     1] loss: 0.368
[31,     1] loss: 0.361
[32,     1] loss: 0.354
[33,     1] loss: 0.347
[34,     1] loss: 0.339
[35,     1] loss: 0.332
[36,     1] loss: 0.324
[37,     1] loss: 0.314
[38,     1] loss: 0.308
[39,     1] loss: 0.299
[40,     1] loss: 0.290
[41,     1] loss: 0.283
[42,     1] loss: 0.275
[43,     1] loss: 0.265
[44,     1] loss: 0.257
[45,     1] loss: 0.247
[46,     1] loss: 0.241
[47,     1] loss: 0.231
[48,     1] loss: 0.267
[49,     1] loss: 0.268
[50,     1] loss: 0.329
[51,     1] loss: 0.536
[52,     1] loss: 0.467
[53,     1] loss: 0.423
[54,     1] loss: 0.378
[55,     1] loss: 0.372
[56,     1] loss: 0.404
[57,     1] loss: 0.371
[58,     1] loss: 0.338
[59,     1] loss: 0.329
[60,     1] loss: 0.315
[61,     1] loss: 0.306
[62,     1] loss: 0.299
[63,     1] loss: 0.287
[64,     1] loss: 0.286
[65,     1] loss: 0.280
[66,     1] loss: 0.276
[67,     1] loss: 0.271
[68,     1] loss: 0.265
[69,     1] loss: 0.260
[70,     1] loss: 0.257
[71,     1] loss: 0.249
[72,     1] loss: 0.248
[73,     1] loss: 0.245
[74,     1] loss: 0.242
[75,     1] loss: 0.240
[76,     1] loss: 0.237
[77,     1] loss: 0.235
[78,     1] loss: 0.233
[79,     1] loss: 0.230
[80,     1] loss: 0.230
[81,     1] loss: 0.228
[82,     1] loss: 0.227
[83,     1] loss: 0.228
[84,     1] loss: 0.223
[85,     1] loss: 0.223
[86,     1] loss: 0.221
[87,     1] loss: 0.220
[88,     1] loss: 0.220
[89,     1] loss: 0.218
[90,     1] loss: 0.217
[91,     1] loss: 0.217
[92,     1] loss: 0.215
[93,     1] loss: 0.213
[94,     1] loss: 0.213
[95,     1] loss: 0.212
[96,     1] loss: 0.212
[97,     1] loss: 0.211
[98,     1] loss: 0.212
[99,     1] loss: 0.210
[100,     1] loss: 0.211
[101,     1] loss: 0.210
[102,     1] loss: 0.208
[103,     1] loss: 0.208
[104,     1] loss: 0.211
[105,     1] loss: 0.214
[106,     1] loss: 0.253
[107,     1] loss: 0.648
[108,     1] loss: 0.560
[109,     1] loss: 0.615
[110,     1] loss: 0.533
[111,     1] loss: 0.554
[112,     1] loss: 0.505
[113,     1] loss: 0.486
[114,     1] loss: 0.489
[115,     1] loss: 0.490
[116,     1] loss: 0.481
[117,     1] loss: 0.477
[118,     1] loss: 0.476
[119,     1] loss: 0.463
[120,     1] loss: 0.454
[121,     1] loss: 0.447
[122,     1] loss: 0.436
Early stopping applied (best metric=0.3952624797821045)
Finished Training
Total time taken: 34.20058822631836
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.661
[5,     1] loss: 0.648
[6,     1] loss: 0.635
[7,     1] loss: 0.622
[8,     1] loss: 0.609
[9,     1] loss: 0.595
[10,     1] loss: 0.578
[11,     1] loss: 0.563
[12,     1] loss: 0.545
[13,     1] loss: 0.526
[14,     1] loss: 0.509
[15,     1] loss: 0.487
[16,     1] loss: 0.468
[17,     1] loss: 0.447
[18,     1] loss: 0.426
[19,     1] loss: 0.404
[20,     1] loss: 0.386
[21,     1] loss: 0.366
[22,     1] loss: 0.345
[23,     1] loss: 0.325
[24,     1] loss: 0.304
[25,     1] loss: 0.289
[26,     1] loss: 0.272
[27,     1] loss: 0.259
[28,     1] loss: 0.244
[29,     1] loss: 0.235
[30,     1] loss: 0.234
[31,     1] loss: 0.221
[32,     1] loss: 0.210
[33,     1] loss: 0.208
[34,     1] loss: 0.196
[35,     1] loss: 0.191
[36,     1] loss: 0.185
[37,     1] loss: 0.182
[38,     1] loss: 0.177
[39,     1] loss: 0.174
[40,     1] loss: 0.168
[41,     1] loss: 0.169
[42,     1] loss: 0.167
[43,     1] loss: 0.163
[44,     1] loss: 0.162
[45,     1] loss: 0.162
[46,     1] loss: 0.161
[47,     1] loss: 0.159
[48,     1] loss: 0.157
[49,     1] loss: 0.158
[50,     1] loss: 0.155
[51,     1] loss: 0.154
[52,     1] loss: 0.151
[53,     1] loss: 0.145
[54,     1] loss: 0.141
[55,     1] loss: 0.137
[56,     1] loss: 0.220
[57,     1] loss: 0.358
[58,     1] loss: 0.416
[59,     1] loss: 0.281
[60,     1] loss: 0.316
[61,     1] loss: 0.302
[62,     1] loss: 0.299
[63,     1] loss: 0.300
[64,     1] loss: 0.301
[65,     1] loss: 0.291
[66,     1] loss: 0.278
[67,     1] loss: 0.267
[68,     1] loss: 0.257
[69,     1] loss: 0.250
[70,     1] loss: 0.238
[71,     1] loss: 0.230
[72,     1] loss: 0.223
[73,     1] loss: 0.213
[74,     1] loss: 0.208
[75,     1] loss: 0.202
[76,     1] loss: 0.198
[77,     1] loss: 0.193
[78,     1] loss: 0.190
[79,     1] loss: 0.187
[80,     1] loss: 0.184
Early stopping applied (best metric=0.40081706643104553)
Finished Training
Total time taken: 22.853389739990234
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.681
[4,     1] loss: 0.669
[5,     1] loss: 0.657
[6,     1] loss: 0.644
[7,     1] loss: 0.629
[8,     1] loss: 0.613
[9,     1] loss: 0.596
[10,     1] loss: 0.576
[11,     1] loss: 0.557
[12,     1] loss: 0.535
[13,     1] loss: 0.514
[14,     1] loss: 0.486
[15,     1] loss: 0.463
[16,     1] loss: 0.438
[17,     1] loss: 0.412
[18,     1] loss: 0.391
[19,     1] loss: 0.366
[20,     1] loss: 0.347
[21,     1] loss: 0.325
[22,     1] loss: 0.306
[23,     1] loss: 0.287
[24,     1] loss: 0.271
[25,     1] loss: 0.257
[26,     1] loss: 0.241
[27,     1] loss: 0.230
[28,     1] loss: 0.219
[29,     1] loss: 0.209
[30,     1] loss: 0.200
[31,     1] loss: 0.192
[32,     1] loss: 0.191
[33,     1] loss: 0.186
[34,     1] loss: 0.177
[35,     1] loss: 0.171
[36,     1] loss: 0.167
[37,     1] loss: 0.165
[38,     1] loss: 0.158
[39,     1] loss: 0.157
[40,     1] loss: 0.153
[41,     1] loss: 0.149
[42,     1] loss: 0.146
[43,     1] loss: 0.144
[44,     1] loss: 0.141
[45,     1] loss: 0.141
[46,     1] loss: 0.163
[47,     1] loss: 0.149
[48,     1] loss: 0.147
[49,     1] loss: 0.143
[50,     1] loss: 0.135
[51,     1] loss: 0.130
[52,     1] loss: 0.123
[53,     1] loss: 0.115
[54,     1] loss: 0.116
[55,     1] loss: 0.316
[56,     1] loss: 0.444
[57,     1] loss: 0.272
[58,     1] loss: 0.412
[59,     1] loss: 0.305
[60,     1] loss: 0.323
[61,     1] loss: 0.284
[62,     1] loss: 0.300
[63,     1] loss: 0.270
[64,     1] loss: 0.272
[65,     1] loss: 0.265
[66,     1] loss: 0.255
[67,     1] loss: 0.243
[68,     1] loss: 0.230
[69,     1] loss: 0.213
[70,     1] loss: 0.202
[71,     1] loss: 0.190
[72,     1] loss: 0.175
[73,     1] loss: 0.170
[74,     1] loss: 0.159
[75,     1] loss: 0.149
[76,     1] loss: 0.143
[77,     1] loss: 0.133
[78,     1] loss: 0.130
[79,     1] loss: 0.123
[80,     1] loss: 0.122
[81,     1] loss: 0.121
[82,     1] loss: 0.116
[83,     1] loss: 0.117
[84,     1] loss: 0.114
[85,     1] loss: 0.114
[86,     1] loss: 0.115
[87,     1] loss: 0.114
[88,     1] loss: 0.116
[89,     1] loss: 0.116
[90,     1] loss: 0.117
[91,     1] loss: 0.118
[92,     1] loss: 0.118
[93,     1] loss: 0.119
[94,     1] loss: 0.122
[95,     1] loss: 0.122
[96,     1] loss: 0.123
[97,     1] loss: 0.121
[98,     1] loss: 0.122
[99,     1] loss: 0.122
[100,     1] loss: 0.124
[101,     1] loss: 0.121
[102,     1] loss: 0.121
[103,     1] loss: 0.120
[104,     1] loss: 0.120
[105,     1] loss: 0.122
[106,     1] loss: 0.122
[107,     1] loss: 0.123
[108,     1] loss: 0.129
[109,     1] loss: 0.261
[110,     1] loss: 0.757
[111,     1] loss: 0.506
[112,     1] loss: 0.544
[113,     1] loss: 0.529
[114,     1] loss: 0.497
[115,     1] loss: 0.467
[116,     1] loss: 0.443
[117,     1] loss: 0.426
[118,     1] loss: 0.413
[119,     1] loss: 0.402
[120,     1] loss: 0.396
[121,     1] loss: 0.388
[122,     1] loss: 0.378
Early stopping applied (best metric=0.3450374901294708)
Finished Training
Total time taken: 36.17161726951599
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.667
[5,     1] loss: 0.657
[6,     1] loss: 0.645
[7,     1] loss: 0.633
[8,     1] loss: 0.619
[9,     1] loss: 0.609
[10,     1] loss: 0.589
[11,     1] loss: 0.575
[12,     1] loss: 0.558
[13,     1] loss: 0.537
[14,     1] loss: 0.518
[15,     1] loss: 0.496
[16,     1] loss: 0.474
[17,     1] loss: 0.452
[18,     1] loss: 0.429
[19,     1] loss: 0.406
[20,     1] loss: 0.388
[21,     1] loss: 0.365
[22,     1] loss: 0.346
[23,     1] loss: 0.329
[24,     1] loss: 0.309
[25,     1] loss: 0.290
[26,     1] loss: 0.276
[27,     1] loss: 0.261
[28,     1] loss: 0.245
[29,     1] loss: 0.233
[30,     1] loss: 0.222
[31,     1] loss: 0.211
[32,     1] loss: 0.199
[33,     1] loss: 0.192
[34,     1] loss: 0.182
[35,     1] loss: 0.177
[36,     1] loss: 0.170
[37,     1] loss: 0.164
[38,     1] loss: 0.159
[39,     1] loss: 0.156
[40,     1] loss: 0.152
[41,     1] loss: 0.149
[42,     1] loss: 0.146
[43,     1] loss: 0.145
[44,     1] loss: 0.143
[45,     1] loss: 0.140
[46,     1] loss: 0.140
[47,     1] loss: 0.138
[48,     1] loss: 0.135
[49,     1] loss: 0.133
[50,     1] loss: 0.171
[51,     1] loss: 0.242
[52,     1] loss: 0.473
[53,     1] loss: 0.420
[54,     1] loss: 0.396
[55,     1] loss: 0.362
[56,     1] loss: 0.344
[57,     1] loss: 0.335
[58,     1] loss: 0.315
[59,     1] loss: 0.296
[60,     1] loss: 0.283
[61,     1] loss: 0.270
[62,     1] loss: 0.256
[63,     1] loss: 0.244
[64,     1] loss: 0.252
[65,     1] loss: 0.226
[66,     1] loss: 0.219
[67,     1] loss: 0.208
[68,     1] loss: 0.195
[69,     1] loss: 0.188
[70,     1] loss: 0.177
[71,     1] loss: 0.171
[72,     1] loss: 0.163
[73,     1] loss: 0.158
[74,     1] loss: 0.151
[75,     1] loss: 0.146
[76,     1] loss: 0.138
[77,     1] loss: 0.138
[78,     1] loss: 0.135
[79,     1] loss: 0.130
[80,     1] loss: 0.131
[81,     1] loss: 0.128
[82,     1] loss: 0.125
[83,     1] loss: 0.125
[84,     1] loss: 0.123
[85,     1] loss: 0.124
[86,     1] loss: 0.124
[87,     1] loss: 0.123
[88,     1] loss: 0.120
[89,     1] loss: 0.121
[90,     1] loss: 0.123
[91,     1] loss: 0.121
[92,     1] loss: 0.122
[93,     1] loss: 0.123
[94,     1] loss: 0.123
[95,     1] loss: 0.123
[96,     1] loss: 0.124
[97,     1] loss: 0.122
[98,     1] loss: 0.123
[99,     1] loss: 0.149
[100,     1] loss: 0.373
[101,     1] loss: 0.365
[102,     1] loss: 0.891
[103,     1] loss: 0.468
[104,     1] loss: 0.458
[105,     1] loss: 0.493
[106,     1] loss: 0.523
[107,     1] loss: 0.513
[108,     1] loss: 0.497
[109,     1] loss: 0.488
[110,     1] loss: 0.492
Early stopping applied (best metric=0.3972121775150299)
Finished Training
Total time taken: 34.03257918357849
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.702
[3,     1] loss: 0.693
[4,     1] loss: 0.684
[5,     1] loss: 0.675
[6,     1] loss: 0.664
[7,     1] loss: 0.652
[8,     1] loss: 0.637
[9,     1] loss: 0.620
[10,     1] loss: 0.602
[11,     1] loss: 0.581
[12,     1] loss: 0.563
[13,     1] loss: 0.542
[14,     1] loss: 0.523
[15,     1] loss: 0.503
[16,     1] loss: 0.488
[17,     1] loss: 0.471
[18,     1] loss: 0.459
[19,     1] loss: 0.447
[20,     1] loss: 0.436
[21,     1] loss: 0.426
[22,     1] loss: 0.417
[23,     1] loss: 0.407
[24,     1] loss: 0.399
[25,     1] loss: 0.392
[26,     1] loss: 0.384
[27,     1] loss: 0.376
[28,     1] loss: 0.367
[29,     1] loss: 0.357
[30,     1] loss: 0.349
[31,     1] loss: 0.339
[32,     1] loss: 0.328
[33,     1] loss: 0.316
[34,     1] loss: 0.303
[35,     1] loss: 0.290
[36,     1] loss: 0.277
[37,     1] loss: 0.263
[38,     1] loss: 0.254
[39,     1] loss: 0.240
[40,     1] loss: 0.230
[41,     1] loss: 0.216
[42,     1] loss: 0.206
[43,     1] loss: 0.197
[44,     1] loss: 0.189
[45,     1] loss: 0.180
[46,     1] loss: 0.172
[47,     1] loss: 0.165
[48,     1] loss: 0.160
[49,     1] loss: 0.155
[50,     1] loss: 0.148
[51,     1] loss: 0.144
[52,     1] loss: 0.141
[53,     1] loss: 0.140
[54,     1] loss: 0.144
[55,     1] loss: 0.155
[56,     1] loss: 0.420
[57,     1] loss: 0.403
[58,     1] loss: 0.417
[59,     1] loss: 0.367
[60,     1] loss: 0.326
[61,     1] loss: 0.305
[62,     1] loss: 0.292
[63,     1] loss: 0.278
[64,     1] loss: 0.268
[65,     1] loss: 0.261
[66,     1] loss: 0.249
[67,     1] loss: 0.232
[68,     1] loss: 0.219
[69,     1] loss: 0.207
[70,     1] loss: 0.192
[71,     1] loss: 0.178
[72,     1] loss: 0.173
[73,     1] loss: 0.183
[74,     1] loss: 0.158
[75,     1] loss: 0.159
[76,     1] loss: 0.138
[77,     1] loss: 0.137
[78,     1] loss: 0.134
[79,     1] loss: 0.126
[80,     1] loss: 0.121
[81,     1] loss: 0.119
[82,     1] loss: 0.112
[83,     1] loss: 0.112
[84,     1] loss: 0.111
[85,     1] loss: 0.108
[86,     1] loss: 0.107
[87,     1] loss: 0.105
[88,     1] loss: 0.106
[89,     1] loss: 0.104
[90,     1] loss: 0.104
[91,     1] loss: 0.107
[92,     1] loss: 0.105
[93,     1] loss: 0.107
[94,     1] loss: 0.108
[95,     1] loss: 0.105
[96,     1] loss: 0.107
[97,     1] loss: 0.109
[98,     1] loss: 0.109
[99,     1] loss: 0.109
[100,     1] loss: 0.111
[101,     1] loss: 0.111
[102,     1] loss: 0.110
[103,     1] loss: 0.110
[104,     1] loss: 0.109
[105,     1] loss: 0.109
[106,     1] loss: 0.110
[107,     1] loss: 0.112
[108,     1] loss: 0.111
[109,     1] loss: 0.108
[110,     1] loss: 0.109
[111,     1] loss: 0.110
[112,     1] loss: 0.111
[113,     1] loss: 0.112
[114,     1] loss: 0.131
[115,     1] loss: 0.277
[116,     1] loss: 0.403
[117,     1] loss: 0.499
[118,     1] loss: 0.561
[119,     1] loss: 0.376
[120,     1] loss: 0.442
[121,     1] loss: 0.371
[122,     1] loss: 0.381
[123,     1] loss: 0.363
[124,     1] loss: 0.332
[125,     1] loss: 0.321
[126,     1] loss: 0.313
[127,     1] loss: 0.308
[128,     1] loss: 0.300
[129,     1] loss: 0.297
[130,     1] loss: 0.288
[131,     1] loss: 0.276
[132,     1] loss: 0.269
[133,     1] loss: 0.259
[134,     1] loss: 0.247
[135,     1] loss: 0.238
[136,     1] loss: 0.231
[137,     1] loss: 0.221
[138,     1] loss: 0.213
[139,     1] loss: 0.204
[140,     1] loss: 0.196
[141,     1] loss: 0.185
[142,     1] loss: 0.180
[143,     1] loss: 0.175
[144,     1] loss: 0.170
[145,     1] loss: 0.164
[146,     1] loss: 0.157
[147,     1] loss: 0.155
[148,     1] loss: 0.151
[149,     1] loss: 0.148
[150,     1] loss: 0.145
[151,     1] loss: 0.144
[152,     1] loss: 0.142
[153,     1] loss: 0.141
[154,     1] loss: 0.139
[155,     1] loss: 0.138
[156,     1] loss: 0.134
[157,     1] loss: 0.133
[158,     1] loss: 0.134
[159,     1] loss: 0.133
Early stopping applied (best metric=0.3158678114414215)
Finished Training
Total time taken: 50.30986022949219
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.682
[3,     1] loss: 0.661
[4,     1] loss: 0.646
[5,     1] loss: 0.633
[6,     1] loss: 0.618
[7,     1] loss: 0.602
[8,     1] loss: 0.587
[9,     1] loss: 0.567
[10,     1] loss: 0.550
[11,     1] loss: 0.530
[12,     1] loss: 0.508
[13,     1] loss: 0.482
[14,     1] loss: 0.457
[15,     1] loss: 0.434
[16,     1] loss: 0.411
[17,     1] loss: 0.390
[18,     1] loss: 0.370
[19,     1] loss: 0.347
[20,     1] loss: 0.324
[21,     1] loss: 0.307
[22,     1] loss: 0.290
[23,     1] loss: 0.271
[24,     1] loss: 0.255
[25,     1] loss: 0.239
[26,     1] loss: 0.224
[27,     1] loss: 0.210
[28,     1] loss: 0.199
[29,     1] loss: 0.188
[30,     1] loss: 0.176
[31,     1] loss: 0.169
[32,     1] loss: 0.158
[33,     1] loss: 0.154
[34,     1] loss: 0.144
[35,     1] loss: 0.138
[36,     1] loss: 0.135
[37,     1] loss: 0.140
[38,     1] loss: 0.150
[39,     1] loss: 0.139
[40,     1] loss: 0.137
[41,     1] loss: 0.137
[42,     1] loss: 0.133
[43,     1] loss: 0.131
[44,     1] loss: 0.128
[45,     1] loss: 0.124
[46,     1] loss: 0.122
[47,     1] loss: 0.119
[48,     1] loss: 0.115
[49,     1] loss: 0.113
[50,     1] loss: 0.108
[51,     1] loss: 0.105
[52,     1] loss: 0.100
[53,     1] loss: 0.096
[54,     1] loss: 0.100
[55,     1] loss: 0.177
[56,     1] loss: 0.699
[57,     1] loss: 0.389
[58,     1] loss: 0.385
[59,     1] loss: 0.380
[60,     1] loss: 0.380
[61,     1] loss: 0.369
[62,     1] loss: 0.361
[63,     1] loss: 0.354
[64,     1] loss: 0.347
[65,     1] loss: 0.339
[66,     1] loss: 0.327
[67,     1] loss: 0.312
[68,     1] loss: 0.299
[69,     1] loss: 0.283
[70,     1] loss: 0.277
[71,     1] loss: 0.261
[72,     1] loss: 0.250
[73,     1] loss: 0.235
[74,     1] loss: 0.220
[75,     1] loss: 0.209
[76,     1] loss: 0.196
[77,     1] loss: 0.185
[78,     1] loss: 0.177
[79,     1] loss: 0.170
[80,     1] loss: 0.158
[81,     1] loss: 0.153
[82,     1] loss: 0.147
[83,     1] loss: 0.141
[84,     1] loss: 0.136
[85,     1] loss: 0.131
[86,     1] loss: 0.129
[87,     1] loss: 0.127
[88,     1] loss: 0.123
[89,     1] loss: 0.123
[90,     1] loss: 0.120
[91,     1] loss: 0.122
[92,     1] loss: 0.121
[93,     1] loss: 0.121
[94,     1] loss: 0.125
[95,     1] loss: 0.165
[96,     1] loss: 0.395
[97,     1] loss: 0.728
[98,     1] loss: 0.385
Early stopping applied (best metric=0.3644305169582367)
Finished Training
Total time taken: 29.79150938987732
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.689
[3,     1] loss: 0.676
[4,     1] loss: 0.664
[5,     1] loss: 0.656
[6,     1] loss: 0.644
[7,     1] loss: 0.635
[8,     1] loss: 0.624
[9,     1] loss: 0.612
[10,     1] loss: 0.600
[11,     1] loss: 0.585
[12,     1] loss: 0.568
[13,     1] loss: 0.553
[14,     1] loss: 0.535
[15,     1] loss: 0.513
[16,     1] loss: 0.495
[17,     1] loss: 0.474
[18,     1] loss: 0.453
[19,     1] loss: 0.432
[20,     1] loss: 0.414
[21,     1] loss: 0.391
[22,     1] loss: 0.373
[23,     1] loss: 0.350
[24,     1] loss: 0.330
[25,     1] loss: 0.314
[26,     1] loss: 0.300
[27,     1] loss: 0.294
[28,     1] loss: 0.285
[29,     1] loss: 0.274
[30,     1] loss: 0.259
[31,     1] loss: 0.250
[32,     1] loss: 0.235
[33,     1] loss: 0.228
[34,     1] loss: 0.218
[35,     1] loss: 0.205
[36,     1] loss: 0.201
[37,     1] loss: 0.194
[38,     1] loss: 0.189
[39,     1] loss: 0.184
[40,     1] loss: 0.180
[41,     1] loss: 0.176
[42,     1] loss: 0.171
[43,     1] loss: 0.169
[44,     1] loss: 0.166
[45,     1] loss: 0.160
[46,     1] loss: 0.159
[47,     1] loss: 0.153
[48,     1] loss: 0.149
[49,     1] loss: 0.143
[50,     1] loss: 0.143
[51,     1] loss: 0.289
[52,     1] loss: 0.361
[53,     1] loss: 0.464
[54,     1] loss: 0.434
[55,     1] loss: 0.406
[56,     1] loss: 0.365
[57,     1] loss: 0.352
[58,     1] loss: 0.347
[59,     1] loss: 0.333
[60,     1] loss: 0.324
[61,     1] loss: 0.324
[62,     1] loss: 0.317
[63,     1] loss: 0.312
[64,     1] loss: 0.303
[65,     1] loss: 0.294
[66,     1] loss: 0.284
[67,     1] loss: 0.273
[68,     1] loss: 0.264
[69,     1] loss: 0.256
Early stopping applied (best metric=0.4060448408126831)
Finished Training
Total time taken: 22.523385286331177
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.698
[3,     1] loss: 0.685
[4,     1] loss: 0.674
[5,     1] loss: 0.664
[6,     1] loss: 0.653
[7,     1] loss: 0.642
[8,     1] loss: 0.631
[9,     1] loss: 0.621
[10,     1] loss: 0.609
[11,     1] loss: 0.597
[12,     1] loss: 0.584
[13,     1] loss: 0.572
[14,     1] loss: 0.558
[15,     1] loss: 0.543
[16,     1] loss: 0.529
[17,     1] loss: 0.512
[18,     1] loss: 0.496
[19,     1] loss: 0.478
[20,     1] loss: 0.466
[21,     1] loss: 0.459
[22,     1] loss: 0.433
[23,     1] loss: 0.433
[24,     1] loss: 0.405
[25,     1] loss: 0.408
[26,     1] loss: 0.379
[27,     1] loss: 0.367
[28,     1] loss: 0.356
[29,     1] loss: 0.344
[30,     1] loss: 0.333
[31,     1] loss: 0.322
[32,     1] loss: 0.313
[33,     1] loss: 0.305
[34,     1] loss: 0.298
[35,     1] loss: 0.291
[36,     1] loss: 0.284
[37,     1] loss: 0.280
[38,     1] loss: 0.276
[39,     1] loss: 0.269
[40,     1] loss: 0.264
[41,     1] loss: 0.261
[42,     1] loss: 0.257
[43,     1] loss: 0.256
[44,     1] loss: 0.250
[45,     1] loss: 0.250
[46,     1] loss: 0.251
[47,     1] loss: 0.252
[48,     1] loss: 0.262
[49,     1] loss: 0.249
[50,     1] loss: 0.245
[51,     1] loss: 0.240
[52,     1] loss: 0.233
[53,     1] loss: 0.235
[54,     1] loss: 0.233
[55,     1] loss: 0.239
[56,     1] loss: 0.225
[57,     1] loss: 0.224
[58,     1] loss: 0.218
[59,     1] loss: 0.213
[60,     1] loss: 0.208
[61,     1] loss: 0.206
[62,     1] loss: 0.203
[63,     1] loss: 0.199
[64,     1] loss: 0.195
[65,     1] loss: 0.192
[66,     1] loss: 0.196
[67,     1] loss: 0.269
[68,     1] loss: 0.341
[69,     1] loss: 0.409
[70,     1] loss: 0.413
[71,     1] loss: 0.422
[72,     1] loss: 0.408
[73,     1] loss: 0.412
[74,     1] loss: 0.401
[75,     1] loss: 0.399
[76,     1] loss: 0.385
[77,     1] loss: 0.380
[78,     1] loss: 0.370
[79,     1] loss: 0.361
[80,     1] loss: 0.352
[81,     1] loss: 0.338
[82,     1] loss: 0.331
[83,     1] loss: 0.320
[84,     1] loss: 0.312
[85,     1] loss: 0.309
[86,     1] loss: 0.301
[87,     1] loss: 0.296
[88,     1] loss: 0.291
[89,     1] loss: 0.287
[90,     1] loss: 0.281
[91,     1] loss: 0.277
[92,     1] loss: 0.276
[93,     1] loss: 0.275
[94,     1] loss: 0.270
[95,     1] loss: 0.269
[96,     1] loss: 0.267
[97,     1] loss: 0.267
[98,     1] loss: 0.265
[99,     1] loss: 0.262
[100,     1] loss: 0.263
[101,     1] loss: 0.263
[102,     1] loss: 0.261
[103,     1] loss: 0.260
[104,     1] loss: 0.258
[105,     1] loss: 0.256
[106,     1] loss: 0.254
[107,     1] loss: 0.251
[108,     1] loss: 0.251
[109,     1] loss: 0.262
[110,     1] loss: 0.368
[111,     1] loss: 0.310
[112,     1] loss: 0.319
[113,     1] loss: 0.349
[114,     1] loss: 0.318
[115,     1] loss: 0.308
[116,     1] loss: 0.294
[117,     1] loss: 0.290
[118,     1] loss: 0.283
[119,     1] loss: 0.263
[120,     1] loss: 0.291
[121,     1] loss: 0.304
[122,     1] loss: 0.270
[123,     1] loss: 0.270
[124,     1] loss: 0.264
[125,     1] loss: 0.253
[126,     1] loss: 0.262
[127,     1] loss: 0.250
[128,     1] loss: 0.247
[129,     1] loss: 0.248
Early stopping applied (best metric=0.3879182040691376)
Finished Training
Total time taken: 42.16172194480896
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.678
[3,     1] loss: 0.658
[4,     1] loss: 0.640
[5,     1] loss: 0.624
[6,     1] loss: 0.604
[7,     1] loss: 0.587
[8,     1] loss: 0.568
[9,     1] loss: 0.551
[10,     1] loss: 0.530
[11,     1] loss: 0.510
[12,     1] loss: 0.491
[13,     1] loss: 0.470
[14,     1] loss: 0.448
[15,     1] loss: 0.424
[16,     1] loss: 0.400
[17,     1] loss: 0.384
[18,     1] loss: 0.383
[19,     1] loss: 0.370
[20,     1] loss: 0.345
[21,     1] loss: 0.334
[22,     1] loss: 0.313
[23,     1] loss: 0.304
[24,     1] loss: 0.294
[25,     1] loss: 0.283
[26,     1] loss: 0.276
[27,     1] loss: 0.268
[28,     1] loss: 0.260
[29,     1] loss: 0.256
[30,     1] loss: 0.250
[31,     1] loss: 0.245
[32,     1] loss: 0.244
[33,     1] loss: 0.238
[34,     1] loss: 0.232
[35,     1] loss: 0.232
[36,     1] loss: 0.230
[37,     1] loss: 0.227
[38,     1] loss: 0.223
[39,     1] loss: 0.222
[40,     1] loss: 0.222
[41,     1] loss: 0.222
[42,     1] loss: 0.219
[43,     1] loss: 0.226
[44,     1] loss: 0.224
[45,     1] loss: 0.221
[46,     1] loss: 0.219
[47,     1] loss: 0.220
[48,     1] loss: 0.219
[49,     1] loss: 0.215
[50,     1] loss: 0.215
[51,     1] loss: 0.214
[52,     1] loss: 0.212
[53,     1] loss: 0.212
[54,     1] loss: 0.207
[55,     1] loss: 0.205
[56,     1] loss: 0.204
[57,     1] loss: 0.295
[58,     1] loss: 0.299
[59,     1] loss: 0.935
[60,     1] loss: 0.645
[61,     1] loss: 0.446
[62,     1] loss: 0.490
[63,     1] loss: 0.495
[64,     1] loss: 0.506
[65,     1] loss: 0.518
[66,     1] loss: 0.531
[67,     1] loss: 0.536
[68,     1] loss: 0.536
Early stopping applied (best metric=0.38457638025283813)
Finished Training
Total time taken: 22.84239101409912
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.648
[6,     1] loss: 0.632
[7,     1] loss: 0.617
[8,     1] loss: 0.599
[9,     1] loss: 0.581
[10,     1] loss: 0.563
[11,     1] loss: 0.539
[12,     1] loss: 0.518
[13,     1] loss: 0.495
[14,     1] loss: 0.473
[15,     1] loss: 0.452
[16,     1] loss: 0.436
[17,     1] loss: 0.413
[18,     1] loss: 0.404
[19,     1] loss: 0.383
[20,     1] loss: 0.374
[21,     1] loss: 0.356
[22,     1] loss: 0.346
[23,     1] loss: 0.333
[24,     1] loss: 0.325
[25,     1] loss: 0.317
[26,     1] loss: 0.308
[27,     1] loss: 0.301
[28,     1] loss: 0.295
[29,     1] loss: 0.289
[30,     1] loss: 0.284
[31,     1] loss: 0.279
[32,     1] loss: 0.275
[33,     1] loss: 0.271
[34,     1] loss: 0.266
[35,     1] loss: 0.261
[36,     1] loss: 0.256
[37,     1] loss: 0.253
[38,     1] loss: 0.250
[39,     1] loss: 0.246
[40,     1] loss: 0.240
[41,     1] loss: 0.246
[42,     1] loss: 0.244
[43,     1] loss: 0.237
[44,     1] loss: 0.235
[45,     1] loss: 0.230
[46,     1] loss: 0.223
[47,     1] loss: 0.217
[48,     1] loss: 0.212
[49,     1] loss: 0.206
[50,     1] loss: 0.202
[51,     1] loss: 0.195
[52,     1] loss: 0.191
[53,     1] loss: 0.242
[54,     1] loss: 0.439
[55,     1] loss: 0.292
[56,     1] loss: 0.604
[57,     1] loss: 0.372
[58,     1] loss: 0.428
[59,     1] loss: 0.452
[60,     1] loss: 0.463
[61,     1] loss: 0.469
[62,     1] loss: 0.471
[63,     1] loss: 0.475
[64,     1] loss: 0.474
[65,     1] loss: 0.475
[66,     1] loss: 0.470
[67,     1] loss: 0.469
[68,     1] loss: 0.464
[69,     1] loss: 0.461
[70,     1] loss: 0.453
[71,     1] loss: 0.446
[72,     1] loss: 0.437
[73,     1] loss: 0.426
[74,     1] loss: 0.416
[75,     1] loss: 0.406
[76,     1] loss: 0.397
[77,     1] loss: 0.382
[78,     1] loss: 0.372
[79,     1] loss: 0.360
[80,     1] loss: 0.352
[81,     1] loss: 0.342
[82,     1] loss: 0.333
[83,     1] loss: 0.329
[84,     1] loss: 0.324
[85,     1] loss: 0.321
[86,     1] loss: 0.316
[87,     1] loss: 0.311
[88,     1] loss: 0.309
[89,     1] loss: 0.309
[90,     1] loss: 0.309
[91,     1] loss: 0.310
[92,     1] loss: 0.308
[93,     1] loss: 0.307
Early stopping applied (best metric=0.33517977595329285)
Finished Training
Total time taken: 32.40998196601868
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.652
[5,     1] loss: 0.639
[6,     1] loss: 0.627
[7,     1] loss: 0.611
[8,     1] loss: 0.596
[9,     1] loss: 0.578
[10,     1] loss: 0.558
[11,     1] loss: 0.540
[12,     1] loss: 0.521
[13,     1] loss: 0.501
[14,     1] loss: 0.483
[15,     1] loss: 0.462
[16,     1] loss: 0.440
[17,     1] loss: 0.422
[18,     1] loss: 0.401
[19,     1] loss: 0.383
[20,     1] loss: 0.366
[21,     1] loss: 0.345
[22,     1] loss: 0.325
[23,     1] loss: 0.312
[24,     1] loss: 0.296
[25,     1] loss: 0.281
[26,     1] loss: 0.264
[27,     1] loss: 0.251
[28,     1] loss: 0.240
[29,     1] loss: 0.228
[30,     1] loss: 0.219
[31,     1] loss: 0.210
[32,     1] loss: 0.199
[33,     1] loss: 0.194
[34,     1] loss: 0.184
[35,     1] loss: 0.177
[36,     1] loss: 0.173
[37,     1] loss: 0.168
[38,     1] loss: 0.166
[39,     1] loss: 0.159
[40,     1] loss: 0.160
[41,     1] loss: 0.157
[42,     1] loss: 0.153
[43,     1] loss: 0.151
[44,     1] loss: 0.149
[45,     1] loss: 0.148
[46,     1] loss: 0.148
[47,     1] loss: 0.147
[48,     1] loss: 0.149
[49,     1] loss: 0.160
[50,     1] loss: 0.187
[51,     1] loss: 0.681
[52,     1] loss: 0.376
[53,     1] loss: 0.347
[54,     1] loss: 0.352
[55,     1] loss: 0.358
[56,     1] loss: 0.359
[57,     1] loss: 0.357
[58,     1] loss: 0.347
[59,     1] loss: 0.338
[60,     1] loss: 0.330
[61,     1] loss: 0.318
[62,     1] loss: 0.310
[63,     1] loss: 0.301
[64,     1] loss: 0.287
[65,     1] loss: 0.279
[66,     1] loss: 0.269
[67,     1] loss: 0.258
[68,     1] loss: 0.248
[69,     1] loss: 0.239
[70,     1] loss: 0.231
[71,     1] loss: 0.218
[72,     1] loss: 0.209
[73,     1] loss: 0.200
[74,     1] loss: 0.190
[75,     1] loss: 0.184
[76,     1] loss: 0.177
[77,     1] loss: 0.209
[78,     1] loss: 0.204
[79,     1] loss: 0.202
[80,     1] loss: 0.198
[81,     1] loss: 0.189
[82,     1] loss: 0.174
[83,     1] loss: 0.174
[84,     1] loss: 0.188
[85,     1] loss: 0.178
[86,     1] loss: 0.191
[87,     1] loss: 0.167
[88,     1] loss: 0.181
[89,     1] loss: 0.162
[90,     1] loss: 0.177
[91,     1] loss: 0.174
[92,     1] loss: 0.161
[93,     1] loss: 0.158
[94,     1] loss: 0.160
[95,     1] loss: 0.154
[96,     1] loss: 0.162
[97,     1] loss: 0.158
[98,     1] loss: 0.156
[99,     1] loss: 0.160
[100,     1] loss: 0.155
[101,     1] loss: 0.156
[102,     1] loss: 0.151
[103,     1] loss: 0.146
[104,     1] loss: 0.148
[105,     1] loss: 0.144
[106,     1] loss: 0.144
[107,     1] loss: 0.142
[108,     1] loss: 0.143
[109,     1] loss: 0.141
[110,     1] loss: 0.140
[111,     1] loss: 0.140
[112,     1] loss: 0.136
[113,     1] loss: 0.142
[114,     1] loss: 0.139
[115,     1] loss: 0.138
[116,     1] loss: 0.138
[117,     1] loss: 0.138
[118,     1] loss: 0.139
[119,     1] loss: 0.138
[120,     1] loss: 0.141
[121,     1] loss: 0.140
[122,     1] loss: 0.140
[123,     1] loss: 0.143
[124,     1] loss: 0.142
[125,     1] loss: 0.141
[126,     1] loss: 0.142
[127,     1] loss: 0.140
[128,     1] loss: 0.143
[129,     1] loss: 0.165
[130,     1] loss: 0.418
[131,     1] loss: 0.488
[132,     1] loss: 0.654
[133,     1] loss: 0.569
[134,     1] loss: 0.597
[135,     1] loss: 0.585
[136,     1] loss: 0.573
[137,     1] loss: 0.556
[138,     1] loss: 0.535
[139,     1] loss: 0.525
[140,     1] loss: 0.521
[141,     1] loss: 0.517
[142,     1] loss: 0.515
Early stopping applied (best metric=0.41931968927383423)
Finished Training
Total time taken: 50.1818585395813
{'Hydroxylation-P Validation Accuracy': 0.7917808740673062, 'Hydroxylation-P Validation Sensitivity': 0.7873333333333333, 'Hydroxylation-P Validation Specificity': 0.7926993865030675, 'Hydroxylation-P Validation Precision': 0.46016284651971734, 'Hydroxylation-P AUC ROC': 0.84792596115707, 'Hydroxylation-P AUC PR': 0.604270680401729, 'Hydroxylation-P MCC': 0.48502817134453574, 'Hydroxylation-P F1': 0.575647024771092, 'Validation Loss (Hydroxylation-P)': 0.3810132300853729, 'Validation Loss (total)': 0.3810132300853729}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006278204647157831,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4027689720,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.558259997084949}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.646
[4,     1] loss: 0.606
[5,     1] loss: 0.558
[6,     1] loss: 0.502
[7,     1] loss: 0.444
[8,     1] loss: 0.384
[9,     1] loss: 0.317
[10,     1] loss: 0.264
[11,     1] loss: 0.215
[12,     1] loss: 0.171
[13,     1] loss: 0.129
[14,     1] loss: 0.099
[15,     1] loss: 0.084
[16,     1] loss: 0.099
[17,     1] loss: 0.082
[18,     1] loss: 0.065
[19,     1] loss: 0.049
[20,     1] loss: 0.040
[21,     1] loss: 0.031
[22,     1] loss: 0.026
[23,     1] loss: 0.020
[24,     1] loss: 0.015
[25,     1] loss: 0.012
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.010
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.008
[32,     1] loss: 0.007
[33,     1] loss: 0.007
[34,     1] loss: 0.008
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.008
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.005
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
Early stopping applied (best metric=0.41671043634414673)
Finished Training
Total time taken: 19.61933422088623
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.647
[4,     1] loss: 0.613
[5,     1] loss: 0.572
[6,     1] loss: 0.527
[7,     1] loss: 0.493
[8,     1] loss: 0.449
[9,     1] loss: 0.408
[10,     1] loss: 0.375
[11,     1] loss: 0.346
[12,     1] loss: 0.316
[13,     1] loss: 0.262
[14,     1] loss: 0.227
[15,     1] loss: 0.201
[16,     1] loss: 0.165
[17,     1] loss: 0.143
[18,     1] loss: 0.118
[19,     1] loss: 0.097
[20,     1] loss: 0.082
[21,     1] loss: 0.064
[22,     1] loss: 0.048
[23,     1] loss: 0.040
[24,     1] loss: 0.029
[25,     1] loss: 0.028
[26,     1] loss: 0.025
[27,     1] loss: 0.019
[28,     1] loss: 0.016
[29,     1] loss: 0.014
[30,     1] loss: 0.013
[31,     1] loss: 0.013
[32,     1] loss: 0.012
[33,     1] loss: 0.011
[34,     1] loss: 0.011
[35,     1] loss: 0.011
[36,     1] loss: 0.011
[37,     1] loss: 0.011
[38,     1] loss: 0.011
[39,     1] loss: 0.010
[40,     1] loss: 0.011
[41,     1] loss: 0.010
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.008
Early stopping applied (best metric=0.43006882071495056)
Finished Training
Total time taken: 19.666334629058838
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.685
[3,     1] loss: 0.645
[4,     1] loss: 0.605
[5,     1] loss: 0.559
[6,     1] loss: 0.505
[7,     1] loss: 0.448
[8,     1] loss: 0.379
[9,     1] loss: 0.325
[10,     1] loss: 0.275
[11,     1] loss: 0.226
[12,     1] loss: 0.194
[13,     1] loss: 0.154
[14,     1] loss: 0.118
[15,     1] loss: 0.088
[16,     1] loss: 0.070
[17,     1] loss: 0.055
[18,     1] loss: 0.053
[19,     1] loss: 0.075
[20,     1] loss: 0.061
[21,     1] loss: 0.038
[22,     1] loss: 0.057
[23,     1] loss: 0.038
[24,     1] loss: 0.045
[25,     1] loss: 0.041
[26,     1] loss: 0.047
[27,     1] loss: 0.041
[28,     1] loss: 0.037
[29,     1] loss: 0.031
[30,     1] loss: 0.024
[31,     1] loss: 0.031
[32,     1] loss: 0.024
[33,     1] loss: 0.024
[34,     1] loss: 0.022
[35,     1] loss: 0.020
[36,     1] loss: 0.019
[37,     1] loss: 0.018
[38,     1] loss: 0.017
[39,     1] loss: 0.016
[40,     1] loss: 0.014
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.009
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.009
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.010
[59,     1] loss: 0.009
[60,     1] loss: 0.009
Early stopping applied (best metric=0.36670541763305664)
Finished Training
Total time taken: 20.06134057044983
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.678
[3,     1] loss: 0.638
[4,     1] loss: 0.594
[5,     1] loss: 0.544
[6,     1] loss: 0.484
[7,     1] loss: 0.419
[8,     1] loss: 0.365
[9,     1] loss: 0.310
[10,     1] loss: 0.259
[11,     1] loss: 0.211
[12,     1] loss: 0.197
[13,     1] loss: 0.165
[14,     1] loss: 0.134
[15,     1] loss: 0.107
[16,     1] loss: 0.077
[17,     1] loss: 0.058
[18,     1] loss: 0.053
[19,     1] loss: 0.052
[20,     1] loss: 0.037
[21,     1] loss: 0.028
[22,     1] loss: 0.023
[23,     1] loss: 0.021
[24,     1] loss: 0.016
[25,     1] loss: 0.014
[26,     1] loss: 0.012
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.007
[31,     1] loss: 0.007
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
Early stopping applied (best metric=0.4463023245334625)
Finished Training
Total time taken: 19.916340827941895
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.692
[3,     1] loss: 0.660
[4,     1] loss: 0.625
[5,     1] loss: 0.580
[6,     1] loss: 0.531
[7,     1] loss: 0.462
[8,     1] loss: 0.393
[9,     1] loss: 0.321
[10,     1] loss: 0.259
[11,     1] loss: 0.213
[12,     1] loss: 0.190
[13,     1] loss: 0.144
[14,     1] loss: 0.120
[15,     1] loss: 0.097
[16,     1] loss: 0.070
[17,     1] loss: 0.056
[18,     1] loss: 0.051
[19,     1] loss: 0.033
[20,     1] loss: 0.040
[21,     1] loss: 0.039
[22,     1] loss: 0.022
[23,     1] loss: 0.033
[24,     1] loss: 0.019
[25,     1] loss: 0.044
[26,     1] loss: 0.024
[27,     1] loss: 0.050
[28,     1] loss: 0.022
[29,     1] loss: 0.019
[30,     1] loss: 0.021
[31,     1] loss: 0.020
[32,     1] loss: 0.019
[33,     1] loss: 0.019
[34,     1] loss: 0.018
[35,     1] loss: 0.018
[36,     1] loss: 0.017
[37,     1] loss: 0.015
[38,     1] loss: 0.014
[39,     1] loss: 0.012
[40,     1] loss: 0.011
[41,     1] loss: 0.012
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.009
[49,     1] loss: 0.010
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.010
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.008
Early stopping applied (best metric=0.406143456697464)
Finished Training
Total time taken: 20.85435390472412
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.672
[4,     1] loss: 0.644
[5,     1] loss: 0.596
[6,     1] loss: 0.546
[7,     1] loss: 0.482
[8,     1] loss: 0.422
[9,     1] loss: 0.366
[10,     1] loss: 0.303
[11,     1] loss: 0.251
[12,     1] loss: 0.199
[13,     1] loss: 0.158
[14,     1] loss: 0.127
[15,     1] loss: 0.104
[16,     1] loss: 0.089
[17,     1] loss: 0.079
[18,     1] loss: 0.069
[19,     1] loss: 0.063
[20,     1] loss: 0.052
[21,     1] loss: 0.045
[22,     1] loss: 0.040
[23,     1] loss: 0.032
[24,     1] loss: 0.031
[25,     1] loss: 0.021
[26,     1] loss: 0.020
[27,     1] loss: 0.016
[28,     1] loss: 0.018
[29,     1] loss: 0.033
[30,     1] loss: 0.020
[31,     1] loss: 0.013
[32,     1] loss: 0.014
[33,     1] loss: 0.043
[34,     1] loss: 0.073
[35,     1] loss: 0.025
[36,     1] loss: 0.036
[37,     1] loss: 0.025
[38,     1] loss: 0.038
[39,     1] loss: 0.020
[40,     1] loss: 0.025
[41,     1] loss: 0.024
[42,     1] loss: 0.020
[43,     1] loss: 0.021
[44,     1] loss: 0.018
[45,     1] loss: 0.016
[46,     1] loss: 0.015
[47,     1] loss: 0.014
[48,     1] loss: 0.013
[49,     1] loss: 0.013
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.010
Early stopping applied (best metric=0.4881770610809326)
Finished Training
Total time taken: 20.59835195541382
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.679
[3,     1] loss: 0.646
[4,     1] loss: 0.611
[5,     1] loss: 0.577
[6,     1] loss: 0.542
[7,     1] loss: 0.509
[8,     1] loss: 0.485
[9,     1] loss: 0.458
[10,     1] loss: 0.437
[11,     1] loss: 0.413
[12,     1] loss: 0.384
[13,     1] loss: 0.350
[14,     1] loss: 0.320
[15,     1] loss: 0.262
[16,     1] loss: 0.227
[17,     1] loss: 0.179
[18,     1] loss: 0.160
[19,     1] loss: 0.146
[20,     1] loss: 0.122
[21,     1] loss: 0.120
[22,     1] loss: 0.096
[23,     1] loss: 0.144
[24,     1] loss: 0.122
[25,     1] loss: 0.107
[26,     1] loss: 0.104
[27,     1] loss: 0.065
[28,     1] loss: 0.061
[29,     1] loss: 0.060
[30,     1] loss: 0.053
[31,     1] loss: 0.045
[32,     1] loss: 0.036
[33,     1] loss: 0.029
[34,     1] loss: 0.024
[35,     1] loss: 0.021
[36,     1] loss: 0.017
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.009
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.007
[59,     1] loss: 0.007
[60,     1] loss: 0.007
[61,     1] loss: 0.006
[62,     1] loss: 0.006
[63,     1] loss: 0.006
[64,     1] loss: 0.006
[65,     1] loss: 0.006
[66,     1] loss: 0.006
[67,     1] loss: 0.005
[68,     1] loss: 0.005
[69,     1] loss: 0.005
[70,     1] loss: 0.005
[71,     1] loss: 0.005
[72,     1] loss: 0.005
[73,     1] loss: 0.005
[74,     1] loss: 0.005
[75,     1] loss: 0.004
[76,     1] loss: 0.005
[77,     1] loss: 0.004
[78,     1] loss: 0.004
[79,     1] loss: 0.004
[80,     1] loss: 0.004
[81,     1] loss: 0.004
[82,     1] loss: 0.004
[83,     1] loss: 0.004
[84,     1] loss: 0.004
[85,     1] loss: 0.004
[86,     1] loss: 0.004
Early stopping applied (best metric=0.4270447790622711)
Finished Training
Total time taken: 31.790555000305176
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.649
[4,     1] loss: 0.608
[5,     1] loss: 0.558
[6,     1] loss: 0.499
[7,     1] loss: 0.423
[8,     1] loss: 0.351
[9,     1] loss: 0.281
[10,     1] loss: 0.216
[11,     1] loss: 0.173
[12,     1] loss: 0.141
[13,     1] loss: 0.108
[14,     1] loss: 0.079
[15,     1] loss: 0.067
[16,     1] loss: 0.043
[17,     1] loss: 0.043
[18,     1] loss: 0.062
[19,     1] loss: 0.076
[20,     1] loss: 0.182
[21,     1] loss: 0.021
[22,     1] loss: 0.196
[23,     1] loss: 0.112
[24,     1] loss: 0.045
[25,     1] loss: 0.058
[26,     1] loss: 0.051
[27,     1] loss: 0.049
[28,     1] loss: 0.041
[29,     1] loss: 0.041
[30,     1] loss: 0.041
[31,     1] loss: 0.035
[32,     1] loss: 0.032
[33,     1] loss: 0.029
[34,     1] loss: 0.025
[35,     1] loss: 0.022
[36,     1] loss: 0.018
[37,     1] loss: 0.015
[38,     1] loss: 0.014
[39,     1] loss: 0.013
[40,     1] loss: 0.011
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.010
[59,     1] loss: 0.010
Early stopping applied (best metric=0.3595852851867676)
Finished Training
Total time taken: 22.440393447875977
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.687
[3,     1] loss: 0.655
[4,     1] loss: 0.615
[5,     1] loss: 0.570
[6,     1] loss: 0.521
[7,     1] loss: 0.463
[8,     1] loss: 0.404
[9,     1] loss: 0.343
[10,     1] loss: 0.272
[11,     1] loss: 0.222
[12,     1] loss: 0.178
[13,     1] loss: 0.178
[14,     1] loss: 0.134
[15,     1] loss: 0.099
[16,     1] loss: 0.062
[17,     1] loss: 0.061
[18,     1] loss: 0.040
[19,     1] loss: 0.030
[20,     1] loss: 0.024
[21,     1] loss: 0.018
[22,     1] loss: 0.016
[23,     1] loss: 0.014
[24,     1] loss: 0.012
[25,     1] loss: 0.011
[26,     1] loss: 0.010
[27,     1] loss: 0.009
[28,     1] loss: 0.009
[29,     1] loss: 0.009
[30,     1] loss: 0.008
[31,     1] loss: 0.009
[32,     1] loss: 0.008
[33,     1] loss: 0.009
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.009
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.006
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
[60,     1] loss: 0.006
[61,     1] loss: 0.006
Early stopping applied (best metric=0.3205544948577881)
Finished Training
Total time taken: 23.853407621383667
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.692
[3,     1] loss: 0.657
[4,     1] loss: 0.615
[5,     1] loss: 0.559
[6,     1] loss: 0.497
[7,     1] loss: 0.427
[8,     1] loss: 0.355
[9,     1] loss: 0.287
[10,     1] loss: 0.226
[11,     1] loss: 0.181
[12,     1] loss: 0.153
[13,     1] loss: 0.133
[14,     1] loss: 0.113
[15,     1] loss: 0.099
[16,     1] loss: 0.079
[17,     1] loss: 0.058
[18,     1] loss: 0.053
[19,     1] loss: 0.050
[20,     1] loss: 0.218
[21,     1] loss: 0.194
[22,     1] loss: 0.098
[23,     1] loss: 0.061
[24,     1] loss: 0.094
[25,     1] loss: 0.070
[26,     1] loss: 0.066
[27,     1] loss: 0.063
[28,     1] loss: 0.054
[29,     1] loss: 0.051
[30,     1] loss: 0.053
[31,     1] loss: 0.047
[32,     1] loss: 0.042
[33,     1] loss: 0.040
[34,     1] loss: 0.040
[35,     1] loss: 0.034
[36,     1] loss: 0.032
[37,     1] loss: 0.029
[38,     1] loss: 0.027
[39,     1] loss: 0.024
[40,     1] loss: 0.022
[41,     1] loss: 0.020
[42,     1] loss: 0.019
[43,     1] loss: 0.017
[44,     1] loss: 0.016
[45,     1] loss: 0.017
[46,     1] loss: 0.015
[47,     1] loss: 0.015
[48,     1] loss: 0.015
[49,     1] loss: 0.015
[50,     1] loss: 0.014
[51,     1] loss: 0.015
[52,     1] loss: 0.015
[53,     1] loss: 0.015
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.014
[57,     1] loss: 0.014
[58,     1] loss: 0.014
[59,     1] loss: 0.014
[60,     1] loss: 0.014
[61,     1] loss: 0.013
[62,     1] loss: 0.013
[63,     1] loss: 0.013
[64,     1] loss: 0.013
[65,     1] loss: 0.013
[66,     1] loss: 0.013
[67,     1] loss: 0.012
[68,     1] loss: 0.012
[69,     1] loss: 0.011
[70,     1] loss: 0.011
[71,     1] loss: 0.012
[72,     1] loss: 0.011
Early stopping applied (best metric=0.41183197498321533)
Finished Training
Total time taken: 27.661474466323853
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.665
[4,     1] loss: 0.628
[5,     1] loss: 0.582
[6,     1] loss: 0.529
[7,     1] loss: 0.475
[8,     1] loss: 0.423
[9,     1] loss: 0.359
[10,     1] loss: 0.304
[11,     1] loss: 0.265
[12,     1] loss: 0.212
[13,     1] loss: 0.205
[14,     1] loss: 0.151
[15,     1] loss: 0.175
[16,     1] loss: 0.132
[17,     1] loss: 0.102
[18,     1] loss: 0.102
[19,     1] loss: 0.086
[20,     1] loss: 0.076
[21,     1] loss: 0.065
[22,     1] loss: 0.054
[23,     1] loss: 0.047
[24,     1] loss: 0.040
[25,     1] loss: 0.034
[26,     1] loss: 0.029
[27,     1] loss: 0.026
[28,     1] loss: 0.024
[29,     1] loss: 0.020
[30,     1] loss: 0.019
[31,     1] loss: 0.018
[32,     1] loss: 0.016
[33,     1] loss: 0.016
[34,     1] loss: 0.015
[35,     1] loss: 0.015
[36,     1] loss: 0.014
[37,     1] loss: 0.014
[38,     1] loss: 0.013
[39,     1] loss: 0.013
[40,     1] loss: 0.013
[41,     1] loss: 0.013
[42,     1] loss: 0.013
[43,     1] loss: 0.012
[44,     1] loss: 0.012
[45,     1] loss: 0.012
[46,     1] loss: 0.013
[47,     1] loss: 0.011
[48,     1] loss: 0.012
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.009
Early stopping applied (best metric=0.44830989837646484)
Finished Training
Total time taken: 22.178378105163574
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.669
[3,     1] loss: 0.625
[4,     1] loss: 0.580
[5,     1] loss: 0.531
[6,     1] loss: 0.485
[7,     1] loss: 0.429
[8,     1] loss: 0.358
[9,     1] loss: 0.291
[10,     1] loss: 0.235
[11,     1] loss: 0.183
[12,     1] loss: 0.146
[13,     1] loss: 0.115
[14,     1] loss: 0.098
[15,     1] loss: 0.084
[16,     1] loss: 0.061
[17,     1] loss: 0.054
[18,     1] loss: 0.049
[19,     1] loss: 0.051
[20,     1] loss: 0.039
[21,     1] loss: 0.026
[22,     1] loss: 0.029
[23,     1] loss: 0.017
[24,     1] loss: 0.021
[25,     1] loss: 0.032
[26,     1] loss: 0.013
[27,     1] loss: 0.021
[28,     1] loss: 0.032
[29,     1] loss: 0.155
[30,     1] loss: 0.021
[31,     1] loss: 0.263
[32,     1] loss: 0.054
[33,     1] loss: 0.144
[34,     1] loss: 0.096
[35,     1] loss: 0.086
[36,     1] loss: 0.101
[37,     1] loss: 0.094
[38,     1] loss: 0.082
[39,     1] loss: 0.066
[40,     1] loss: 0.056
[41,     1] loss: 0.045
[42,     1] loss: 0.038
[43,     1] loss: 0.032
[44,     1] loss: 0.027
[45,     1] loss: 0.023
[46,     1] loss: 0.019
[47,     1] loss: 0.017
[48,     1] loss: 0.015
[49,     1] loss: 0.013
[50,     1] loss: 0.012
[51,     1] loss: 0.012
[52,     1] loss: 0.012
[53,     1] loss: 0.011
[54,     1] loss: 0.012
[55,     1] loss: 0.012
[56,     1] loss: 0.012
[57,     1] loss: 0.013
[58,     1] loss: 0.014
[59,     1] loss: 0.014
[60,     1] loss: 0.015
[61,     1] loss: 0.015
[62,     1] loss: 0.016
[63,     1] loss: 0.016
[64,     1] loss: 0.016
[65,     1] loss: 0.016
[66,     1] loss: 0.016
[67,     1] loss: 0.015
[68,     1] loss: 0.015
[69,     1] loss: 0.015
[70,     1] loss: 0.014
[71,     1] loss: 0.014
[72,     1] loss: 0.013
[73,     1] loss: 0.013
[74,     1] loss: 0.013
[75,     1] loss: 0.012
[76,     1] loss: 0.012
[77,     1] loss: 0.011
[78,     1] loss: 0.011
[79,     1] loss: 0.011
[80,     1] loss: 0.010
[81,     1] loss: 0.010
[82,     1] loss: 0.010
[83,     1] loss: 0.010
[84,     1] loss: 0.010
[85,     1] loss: 0.010
Early stopping applied (best metric=0.49383002519607544)
Finished Training
Total time taken: 33.38357162475586
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.680
[3,     1] loss: 0.648
[4,     1] loss: 0.614
[5,     1] loss: 0.569
[6,     1] loss: 0.530
[7,     1] loss: 0.471
[8,     1] loss: 0.422
[9,     1] loss: 0.350
[10,     1] loss: 0.290
[11,     1] loss: 0.235
[12,     1] loss: 0.178
[13,     1] loss: 0.143
[14,     1] loss: 0.119
[15,     1] loss: 0.097
[16,     1] loss: 0.077
[17,     1] loss: 0.062
[18,     1] loss: 0.101
[19,     1] loss: 0.040
[20,     1] loss: 0.375
[21,     1] loss: 0.056
[22,     1] loss: 0.343
[23,     1] loss: 0.138
[24,     1] loss: 0.062
[25,     1] loss: 0.113
[26,     1] loss: 0.119
[27,     1] loss: 0.113
[28,     1] loss: 0.117
[29,     1] loss: 0.099
[30,     1] loss: 0.082
[31,     1] loss: 0.083
[32,     1] loss: 0.065
[33,     1] loss: 0.061
[34,     1] loss: 0.052
[35,     1] loss: 0.041
[36,     1] loss: 0.035
[37,     1] loss: 0.029
[38,     1] loss: 0.024
[39,     1] loss: 0.019
[40,     1] loss: 0.016
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.010
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.012
[51,     1] loss: 0.012
[52,     1] loss: 0.012
[53,     1] loss: 0.013
[54,     1] loss: 0.014
[55,     1] loss: 0.014
[56,     1] loss: 0.014
[57,     1] loss: 0.015
[58,     1] loss: 0.015
[59,     1] loss: 0.015
Early stopping applied (best metric=0.35626304149627686)
Finished Training
Total time taken: 23.027394771575928
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.641
[4,     1] loss: 0.598
[5,     1] loss: 0.553
[6,     1] loss: 0.512
[7,     1] loss: 0.468
[8,     1] loss: 0.408
[9,     1] loss: 0.348
[10,     1] loss: 0.288
[11,     1] loss: 0.236
[12,     1] loss: 0.190
[13,     1] loss: 0.157
[14,     1] loss: 0.125
[15,     1] loss: 0.095
[16,     1] loss: 0.083
[17,     1] loss: 0.090
[18,     1] loss: 0.085
[19,     1] loss: 0.075
[20,     1] loss: 0.066
[21,     1] loss: 0.056
[22,     1] loss: 0.056
[23,     1] loss: 0.052
[24,     1] loss: 0.053
[25,     1] loss: 0.054
[26,     1] loss: 0.052
[27,     1] loss: 0.047
[28,     1] loss: 0.049
[29,     1] loss: 0.048
[30,     1] loss: 0.046
[31,     1] loss: 0.042
[32,     1] loss: 0.043
[33,     1] loss: 0.041
[34,     1] loss: 0.037
[35,     1] loss: 0.034
[36,     1] loss: 0.029
[37,     1] loss: 0.028
[38,     1] loss: 0.024
[39,     1] loss: 0.021
[40,     1] loss: 0.019
[41,     1] loss: 0.016
[42,     1] loss: 0.014
[43,     1] loss: 0.012
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.004
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
Early stopping applied (best metric=0.43755850195884705)
Finished Training
Total time taken: 22.982391595840454
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.685
[3,     1] loss: 0.660
[4,     1] loss: 0.631
[5,     1] loss: 0.596
[6,     1] loss: 0.551
[7,     1] loss: 0.510
[8,     1] loss: 0.466
[9,     1] loss: 0.425
[10,     1] loss: 0.382
[11,     1] loss: 0.338
[12,     1] loss: 0.292
[13,     1] loss: 0.242
[14,     1] loss: 0.198
[15,     1] loss: 0.158
[16,     1] loss: 0.132
[17,     1] loss: 0.129
[18,     1] loss: 0.089
[19,     1] loss: 0.104
[20,     1] loss: 0.052
[21,     1] loss: 0.073
[22,     1] loss: 0.039
[23,     1] loss: 0.036
[24,     1] loss: 0.030
[25,     1] loss: 0.024
[26,     1] loss: 0.019
[27,     1] loss: 0.017
[28,     1] loss: 0.014
[29,     1] loss: 0.012
[30,     1] loss: 0.010
[31,     1] loss: 0.009
[32,     1] loss: 0.008
[33,     1] loss: 0.008
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
[60,     1] loss: 0.004
[61,     1] loss: 0.004
[62,     1] loss: 0.004
[63,     1] loss: 0.004
Early stopping applied (best metric=0.4296252429485321)
Finished Training
Total time taken: 25.77444338798523
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.666
[4,     1] loss: 0.633
[5,     1] loss: 0.590
[6,     1] loss: 0.534
[7,     1] loss: 0.463
[8,     1] loss: 0.388
[9,     1] loss: 0.318
[10,     1] loss: 0.262
[11,     1] loss: 0.231
[12,     1] loss: 0.205
[13,     1] loss: 0.184
[14,     1] loss: 0.198
[15,     1] loss: 0.128
[16,     1] loss: 0.219
[17,     1] loss: 0.115
[18,     1] loss: 0.265
[19,     1] loss: 0.117
[20,     1] loss: 0.115
[21,     1] loss: 0.169
[22,     1] loss: 0.135
[23,     1] loss: 0.107
[24,     1] loss: 0.100
[25,     1] loss: 0.093
[26,     1] loss: 0.091
[27,     1] loss: 0.084
[28,     1] loss: 0.074
[29,     1] loss: 0.066
[30,     1] loss: 0.057
[31,     1] loss: 0.053
[32,     1] loss: 0.047
[33,     1] loss: 0.040
[34,     1] loss: 0.033
[35,     1] loss: 0.029
[36,     1] loss: 0.025
[37,     1] loss: 0.021
[38,     1] loss: 0.020
[39,     1] loss: 0.017
[40,     1] loss: 0.017
[41,     1] loss: 0.015
[42,     1] loss: 0.014
[43,     1] loss: 0.014
[44,     1] loss: 0.014
[45,     1] loss: 0.014
[46,     1] loss: 0.013
[47,     1] loss: 0.013
[48,     1] loss: 0.013
[49,     1] loss: 0.012
[50,     1] loss: 0.012
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.013
[54,     1] loss: 0.012
[55,     1] loss: 0.012
[56,     1] loss: 0.012
[57,     1] loss: 0.012
[58,     1] loss: 0.012
Early stopping applied (best metric=0.4812576174736023)
Finished Training
Total time taken: 24.815826654434204
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.711
[2,     1] loss: 0.700
[3,     1] loss: 0.677
[4,     1] loss: 0.645
[5,     1] loss: 0.608
[6,     1] loss: 0.560
[7,     1] loss: 0.517
[8,     1] loss: 0.488
[9,     1] loss: 0.442
[10,     1] loss: 0.411
[11,     1] loss: 0.375
[12,     1] loss: 0.348
[13,     1] loss: 0.316
[14,     1] loss: 0.292
[15,     1] loss: 0.257
[16,     1] loss: 0.227
[17,     1] loss: 0.203
[18,     1] loss: 0.182
[19,     1] loss: 0.173
[20,     1] loss: 0.144
[21,     1] loss: 0.136
[22,     1] loss: 0.122
[23,     1] loss: 0.113
[24,     1] loss: 0.101
[25,     1] loss: 0.093
[26,     1] loss: 0.084
[27,     1] loss: 0.070
[28,     1] loss: 0.111
[29,     1] loss: 0.115
[30,     1] loss: 0.081
[31,     1] loss: 0.175
[32,     1] loss: 0.100
[33,     1] loss: 0.124
[34,     1] loss: 0.073
[35,     1] loss: 0.079
[36,     1] loss: 0.086
[37,     1] loss: 0.062
[38,     1] loss: 0.059
[39,     1] loss: 0.053
[40,     1] loss: 0.047
[41,     1] loss: 0.043
[42,     1] loss: 0.038
[43,     1] loss: 0.034
[44,     1] loss: 0.030
[45,     1] loss: 0.025
[46,     1] loss: 0.024
[47,     1] loss: 0.021
[48,     1] loss: 0.019
[49,     1] loss: 0.017
[50,     1] loss: 0.017
[51,     1] loss: 0.017
[52,     1] loss: 0.016
[53,     1] loss: 0.016
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.015
[57,     1] loss: 0.015
[58,     1] loss: 0.016
[59,     1] loss: 0.016
[60,     1] loss: 0.016
[61,     1] loss: 0.016
[62,     1] loss: 0.017
[63,     1] loss: 0.017
[64,     1] loss: 0.017
[65,     1] loss: 0.017
[66,     1] loss: 0.017
[67,     1] loss: 0.017
[68,     1] loss: 0.016
[69,     1] loss: 0.016
[70,     1] loss: 0.016
[71,     1] loss: 0.016
[72,     1] loss: 0.015
[73,     1] loss: 0.014
[74,     1] loss: 0.013
[75,     1] loss: 0.013
[76,     1] loss: 0.014
[77,     1] loss: 0.013
[78,     1] loss: 0.012
[79,     1] loss: 0.012
[80,     1] loss: 0.012
[81,     1] loss: 0.011
[82,     1] loss: 0.010
[83,     1] loss: 0.009
[84,     1] loss: 0.010
[85,     1] loss: 0.009
[86,     1] loss: 0.009
[87,     1] loss: 0.009
Early stopping applied (best metric=0.44767600297927856)
Finished Training
Total time taken: 35.52261471748352
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.658
[4,     1] loss: 0.622
[5,     1] loss: 0.578
[6,     1] loss: 0.525
[7,     1] loss: 0.473
[8,     1] loss: 0.421
[9,     1] loss: 0.384
[10,     1] loss: 0.319
[11,     1] loss: 0.272
[12,     1] loss: 0.218
[13,     1] loss: 0.188
[14,     1] loss: 0.160
[15,     1] loss: 0.137
[16,     1] loss: 0.110
[17,     1] loss: 0.104
[18,     1] loss: 0.087
[19,     1] loss: 0.088
[20,     1] loss: 0.068
[21,     1] loss: 0.086
[22,     1] loss: 0.070
[23,     1] loss: 0.084
[24,     1] loss: 0.050
[25,     1] loss: 0.054
[26,     1] loss: 0.043
[27,     1] loss: 0.033
[28,     1] loss: 0.047
[29,     1] loss: 0.031
[30,     1] loss: 0.030
[31,     1] loss: 0.024
[32,     1] loss: 0.024
[33,     1] loss: 0.022
[34,     1] loss: 0.019
[35,     1] loss: 0.017
[36,     1] loss: 0.016
[37,     1] loss: 0.015
[38,     1] loss: 0.013
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.010
[43,     1] loss: 0.009
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.008
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.008
[58,     1] loss: 0.009
[59,     1] loss: 0.009
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.008
Early stopping applied (best metric=0.438447505235672)
Finished Training
Total time taken: 25.28803062438965
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.698
[3,     1] loss: 0.683
[4,     1] loss: 0.651
[5,     1] loss: 0.609
[6,     1] loss: 0.551
[7,     1] loss: 0.483
[8,     1] loss: 0.405
[9,     1] loss: 0.350
[10,     1] loss: 0.302
[11,     1] loss: 0.221
[12,     1] loss: 0.200
[13,     1] loss: 0.144
[14,     1] loss: 0.131
[15,     1] loss: 0.108
[16,     1] loss: 0.092
[17,     1] loss: 0.079
[18,     1] loss: 0.063
[19,     1] loss: 0.056
[20,     1] loss: 0.045
[21,     1] loss: 0.036
[22,     1] loss: 0.030
[23,     1] loss: 0.023
[24,     1] loss: 0.018
[25,     1] loss: 0.016
[26,     1] loss: 0.012
[27,     1] loss: 0.011
[28,     1] loss: 0.010
[29,     1] loss: 0.009
[30,     1] loss: 0.008
[31,     1] loss: 0.008
[32,     1] loss: 0.008
[33,     1] loss: 0.007
[34,     1] loss: 0.008
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.007
[48,     1] loss: 0.008
[49,     1] loss: 0.007
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
[60,     1] loss: 0.006
Early stopping applied (best metric=0.3408145606517792)
Finished Training
Total time taken: 25.478435754776
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.686
[3,     1] loss: 0.648
[4,     1] loss: 0.611
[5,     1] loss: 0.572
[6,     1] loss: 0.537
[7,     1] loss: 0.495
[8,     1] loss: 0.450
[9,     1] loss: 0.394
[10,     1] loss: 0.331
[11,     1] loss: 0.277
[12,     1] loss: 0.212
[13,     1] loss: 0.161
[14,     1] loss: 0.129
[15,     1] loss: 0.100
[16,     1] loss: 0.082
[17,     1] loss: 0.089
[18,     1] loss: 0.063
[19,     1] loss: 0.048
[20,     1] loss: 0.040
[21,     1] loss: 0.032
[22,     1] loss: 0.036
[23,     1] loss: 0.089
[24,     1] loss: 0.249
[25,     1] loss: 0.041
[26,     1] loss: 0.067
[27,     1] loss: 0.051
[28,     1] loss: 0.041
[29,     1] loss: 0.048
[30,     1] loss: 0.042
[31,     1] loss: 0.042
[32,     1] loss: 0.039
[33,     1] loss: 0.036
[34,     1] loss: 0.034
[35,     1] loss: 0.029
[36,     1] loss: 0.026
[37,     1] loss: 0.023
[38,     1] loss: 0.021
[39,     1] loss: 0.018
[40,     1] loss: 0.015
[41,     1] loss: 0.013
[42,     1] loss: 0.012
[43,     1] loss: 0.012
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.010
[47,     1] loss: 0.009
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.010
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.011
[57,     1] loss: 0.011
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.010
[61,     1] loss: 0.010
Early stopping applied (best metric=0.3269543945789337)
Finished Training
Total time taken: 25.86144471168518
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.668
[3,     1] loss: 0.617
[4,     1] loss: 0.564
[5,     1] loss: 0.503
[6,     1] loss: 0.438
[7,     1] loss: 0.370
[8,     1] loss: 0.304
[9,     1] loss: 0.246
[10,     1] loss: 0.201
[11,     1] loss: 0.162
[12,     1] loss: 0.119
[13,     1] loss: 0.098
[14,     1] loss: 0.065
[15,     1] loss: 0.057
[16,     1] loss: 0.032
[17,     1] loss: 0.023
[18,     1] loss: 0.022
[19,     1] loss: 0.015
[20,     1] loss: 0.011
[21,     1] loss: 0.010
[22,     1] loss: 0.008
[23,     1] loss: 0.006
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.005
[27,     1] loss: 0.005
[28,     1] loss: 0.005
[29,     1] loss: 0.005
[30,     1] loss: 0.005
[31,     1] loss: 0.005
[32,     1] loss: 0.005
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.006
[36,     1] loss: 0.006
[37,     1] loss: 0.006
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.005
[44,     1] loss: 0.005
[45,     1] loss: 0.005
[46,     1] loss: 0.005
[47,     1] loss: 0.005
[48,     1] loss: 0.004
[49,     1] loss: 0.004
[50,     1] loss: 0.004
[51,     1] loss: 0.004
[52,     1] loss: 0.004
[53,     1] loss: 0.004
[54,     1] loss: 0.003
[55,     1] loss: 0.003
[56,     1] loss: 0.003
Early stopping applied (best metric=0.44656768441200256)
Finished Training
Total time taken: 24.235224962234497
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.683
[3,     1] loss: 0.649
[4,     1] loss: 0.609
[5,     1] loss: 0.556
[6,     1] loss: 0.495
[7,     1] loss: 0.423
[8,     1] loss: 0.349
[9,     1] loss: 0.281
[10,     1] loss: 0.222
[11,     1] loss: 0.171
[12,     1] loss: 0.147
[13,     1] loss: 0.116
[14,     1] loss: 0.089
[15,     1] loss: 0.069
[16,     1] loss: 0.053
[17,     1] loss: 0.041
[18,     1] loss: 0.033
[19,     1] loss: 0.026
[20,     1] loss: 0.022
[21,     1] loss: 0.017
[22,     1] loss: 0.015
[23,     1] loss: 0.012
[24,     1] loss: 0.011
[25,     1] loss: 0.009
[26,     1] loss: 0.008
[27,     1] loss: 0.008
[28,     1] loss: 0.007
[29,     1] loss: 0.007
[30,     1] loss: 0.007
[31,     1] loss: 0.008
[32,     1] loss: 0.007
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.009
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.010
[40,     1] loss: 0.009
[41,     1] loss: 0.009
[42,     1] loss: 0.009
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
Early stopping applied (best metric=0.4377277195453644)
Finished Training
Total time taken: 25.418434858322144
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.645
[4,     1] loss: 0.599
[5,     1] loss: 0.550
[6,     1] loss: 0.491
[7,     1] loss: 0.427
[8,     1] loss: 0.350
[9,     1] loss: 0.284
[10,     1] loss: 0.208
[11,     1] loss: 0.148
[12,     1] loss: 0.109
[13,     1] loss: 0.086
[14,     1] loss: 0.059
[15,     1] loss: 0.038
[16,     1] loss: 0.039
[17,     1] loss: 0.035
[18,     1] loss: 0.029
[19,     1] loss: 0.014
[20,     1] loss: 0.010
[21,     1] loss: 0.010
[22,     1] loss: 0.009
[23,     1] loss: 0.008
[24,     1] loss: 0.007
[25,     1] loss: 0.006
[26,     1] loss: 0.006
[27,     1] loss: 0.007
[28,     1] loss: 0.006
[29,     1] loss: 0.006
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.007
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
[61,     1] loss: 0.005
Early stopping applied (best metric=0.35359662771224976)
Finished Training
Total time taken: 27.110459804534912
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.677
[3,     1] loss: 0.639
[4,     1] loss: 0.599
[5,     1] loss: 0.558
[6,     1] loss: 0.511
[7,     1] loss: 0.458
[8,     1] loss: 0.403
[9,     1] loss: 0.351
[10,     1] loss: 0.295
[11,     1] loss: 0.246
[12,     1] loss: 0.237
[13,     1] loss: 0.196
[14,     1] loss: 0.168
[15,     1] loss: 0.144
[16,     1] loss: 0.112
[17,     1] loss: 0.116
[18,     1] loss: 0.094
[19,     1] loss: 0.081
[20,     1] loss: 0.068
[21,     1] loss: 0.054
[22,     1] loss: 0.043
[23,     1] loss: 0.034
[24,     1] loss: 0.029
[25,     1] loss: 0.027
[26,     1] loss: 0.025
[27,     1] loss: 0.020
[28,     1] loss: 0.017
[29,     1] loss: 0.014
[30,     1] loss: 0.012
[31,     1] loss: 0.012
[32,     1] loss: 0.010
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.007
[36,     1] loss: 0.006
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.005
[56,     1] loss: 0.005
Early stopping applied (best metric=0.44496798515319824)
Finished Training
Total time taken: 25.076431035995483
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.684
[3,     1] loss: 0.652
[4,     1] loss: 0.619
[5,     1] loss: 0.576
[6,     1] loss: 0.527
[7,     1] loss: 0.466
[8,     1] loss: 0.406
[9,     1] loss: 0.338
[10,     1] loss: 0.264
[11,     1] loss: 0.206
[12,     1] loss: 0.156
[13,     1] loss: 0.133
[14,     1] loss: 0.115
[15,     1] loss: 0.091
[16,     1] loss: 0.077
[17,     1] loss: 0.064
[18,     1] loss: 0.054
[19,     1] loss: 0.057
[20,     1] loss: 0.036
[21,     1] loss: 0.024
[22,     1] loss: 0.028
[23,     1] loss: 0.016
[24,     1] loss: 0.018
[25,     1] loss: 0.011
[26,     1] loss: 0.012
[27,     1] loss: 0.008
[28,     1] loss: 0.014
[29,     1] loss: 0.007
[30,     1] loss: 0.008
[31,     1] loss: 0.010
[32,     1] loss: 0.009
[33,     1] loss: 0.009
[34,     1] loss: 0.010
[35,     1] loss: 0.009
[36,     1] loss: 0.009
[37,     1] loss: 0.010
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.007
[48,     1] loss: 0.006
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.007
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.006
[59,     1] loss: 0.006
[60,     1] loss: 0.006
[61,     1] loss: 0.005
Early stopping applied (best metric=0.38471561670303345)
Finished Training
Total time taken: 26.904457330703735
{'Hydroxylation-P Validation Accuracy': 0.6829238109740622, 'Hydroxylation-P Validation Sensitivity': 0.8683174603174603, 'Hydroxylation-P Validation Specificity': 0.6430300763130331, 'Hydroxylation-P Validation Precision': 0.3590287650649034, 'Hydroxylation-P AUC ROC': 0.8385007256032244, 'Hydroxylation-P AUC PR': 0.5676605725094801, 'Hydroxylation-P MCC': 0.4028441155414755, 'Hydroxylation-P F1': 0.5017081618880735, 'Validation Loss (Hydroxylation-P)': 0.4136574590206146, 'Validation Loss (total)': 0.4136574590206146}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006161546129372214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1499528480,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.725891927337331}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.645
[4,     1] loss: 0.608
[5,     1] loss: 0.566
[6,     1] loss: 0.518
[7,     1] loss: 0.465
[8,     1] loss: 0.413
[9,     1] loss: 0.342
[10,     1] loss: 0.278
[11,     1] loss: 0.226
[12,     1] loss: 0.189
[13,     1] loss: 0.160
[14,     1] loss: 0.109
[15,     1] loss: 0.102
[16,     1] loss: 0.091
[17,     1] loss: 0.064
[18,     1] loss: 0.047
[19,     1] loss: 0.046
[20,     1] loss: 0.031
[21,     1] loss: 0.028
[22,     1] loss: 0.020
[23,     1] loss: 0.018
[24,     1] loss: 0.012
[25,     1] loss: 0.010
[26,     1] loss: 0.010
[27,     1] loss: 0.008
[28,     1] loss: 0.007
[29,     1] loss: 0.007
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.007
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.006
[47,     1] loss: 0.007
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.004
[59,     1] loss: 0.005
Early stopping applied (best metric=0.38580211997032166)
Finished Training
Total time taken: 24.52942132949829
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.640
[4,     1] loss: 0.599
[5,     1] loss: 0.552
[6,     1] loss: 0.495
[7,     1] loss: 0.428
[8,     1] loss: 0.359
[9,     1] loss: 0.287
[10,     1] loss: 0.222
[11,     1] loss: 0.171
[12,     1] loss: 0.141
[13,     1] loss: 0.094
[14,     1] loss: 0.053
[15,     1] loss: 0.037
[16,     1] loss: 0.025
[17,     1] loss: 0.018
[18,     1] loss: 0.014
[19,     1] loss: 0.011
[20,     1] loss: 0.009
[21,     1] loss: 0.007
[22,     1] loss: 0.006
[23,     1] loss: 0.005
[24,     1] loss: 0.004
[25,     1] loss: 0.004
[26,     1] loss: 0.004
[27,     1] loss: 0.004
[28,     1] loss: 0.004
[29,     1] loss: 0.005
[30,     1] loss: 0.005
[31,     1] loss: 0.005
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.004
[55,     1] loss: 0.005
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
Early stopping applied (best metric=0.34382790327072144)
Finished Training
Total time taken: 25.71343970298767
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.666
[4,     1] loss: 0.636
[5,     1] loss: 0.600
[6,     1] loss: 0.560
[7,     1] loss: 0.520
[8,     1] loss: 0.483
[9,     1] loss: 0.447
[10,     1] loss: 0.419
[11,     1] loss: 0.389
[12,     1] loss: 0.354
[13,     1] loss: 0.326
[14,     1] loss: 0.279
[15,     1] loss: 0.243
[16,     1] loss: 0.252
[17,     1] loss: 0.227
[18,     1] loss: 0.207
[19,     1] loss: 0.172
[20,     1] loss: 0.159
[21,     1] loss: 0.161
[22,     1] loss: 0.136
[23,     1] loss: 0.132
[24,     1] loss: 0.105
[25,     1] loss: 0.109
[26,     1] loss: 0.090
[27,     1] loss: 0.067
[28,     1] loss: 0.061
[29,     1] loss: 0.054
[30,     1] loss: 0.048
[31,     1] loss: 0.041
[32,     1] loss: 0.034
[33,     1] loss: 0.031
[34,     1] loss: 0.025
[35,     1] loss: 0.027
[36,     1] loss: 0.018
[37,     1] loss: 0.045
[38,     1] loss: 0.014
[39,     1] loss: 0.065
[40,     1] loss: 0.019
[41,     1] loss: 0.066
[42,     1] loss: 0.014
[43,     1] loss: 0.014
[44,     1] loss: 0.024
[45,     1] loss: 0.015
[46,     1] loss: 0.016
[47,     1] loss: 0.023
[48,     1] loss: 0.077
[49,     1] loss: 0.444
[50,     1] loss: 0.133
[51,     1] loss: 0.215
[52,     1] loss: 0.091
[53,     1] loss: 0.125
[54,     1] loss: 0.170
[55,     1] loss: 0.169
[56,     1] loss: 0.136
[57,     1] loss: 0.112
[58,     1] loss: 0.085
[59,     1] loss: 0.065
[60,     1] loss: 0.053
[61,     1] loss: 0.042
[62,     1] loss: 0.032
[63,     1] loss: 0.025
[64,     1] loss: 0.021
[65,     1] loss: 0.018
[66,     1] loss: 0.015
[67,     1] loss: 0.013
[68,     1] loss: 0.013
[69,     1] loss: 0.013
[70,     1] loss: 0.013
[71,     1] loss: 0.013
[72,     1] loss: 0.013
[73,     1] loss: 0.014
[74,     1] loss: 0.014
[75,     1] loss: 0.016
[76,     1] loss: 0.016
[77,     1] loss: 0.017
[78,     1] loss: 0.018
[79,     1] loss: 0.018
[80,     1] loss: 0.019
[81,     1] loss: 0.019
[82,     1] loss: 0.019
[83,     1] loss: 0.019
[84,     1] loss: 0.018
[85,     1] loss: 0.017
[86,     1] loss: 0.017
[87,     1] loss: 0.015
[88,     1] loss: 0.014
[89,     1] loss: 0.014
[90,     1] loss: 0.013
[91,     1] loss: 0.012
[92,     1] loss: 0.013
[93,     1] loss: 0.011
[94,     1] loss: 0.012
[95,     1] loss: 0.012
[96,     1] loss: 0.011
[97,     1] loss: 0.012
[98,     1] loss: 0.013
[99,     1] loss: 0.012
[100,     1] loss: 0.012
[101,     1] loss: 0.013
[102,     1] loss: 0.013
[103,     1] loss: 0.014
[104,     1] loss: 0.014
[105,     1] loss: 0.014
[106,     1] loss: 0.014
[107,     1] loss: 0.014
Early stopping applied (best metric=0.4037272334098816)
Finished Training
Total time taken: 45.34477424621582
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.671
[3,     1] loss: 0.621
[4,     1] loss: 0.575
[5,     1] loss: 0.531
[6,     1] loss: 0.485
[7,     1] loss: 0.446
[8,     1] loss: 0.402
[9,     1] loss: 0.363
[10,     1] loss: 0.330
[11,     1] loss: 0.299
[12,     1] loss: 0.266
[13,     1] loss: 0.230
[14,     1] loss: 0.196
[15,     1] loss: 0.165
[16,     1] loss: 0.139
[17,     1] loss: 0.115
[18,     1] loss: 0.099
[19,     1] loss: 0.087
[20,     1] loss: 0.074
[21,     1] loss: 0.092
[22,     1] loss: 0.060
[23,     1] loss: 0.069
[24,     1] loss: 0.050
[25,     1] loss: 0.047
[26,     1] loss: 0.037
[27,     1] loss: 0.033
[28,     1] loss: 0.026
[29,     1] loss: 0.026
[30,     1] loss: 0.021
[31,     1] loss: 0.022
[32,     1] loss: 0.071
[33,     1] loss: 0.173
[34,     1] loss: 0.048
[35,     1] loss: 0.266
[36,     1] loss: 0.048
[37,     1] loss: 0.132
[38,     1] loss: 0.127
[39,     1] loss: 0.082
[40,     1] loss: 0.069
[41,     1] loss: 0.071
[42,     1] loss: 0.067
[43,     1] loss: 0.054
[44,     1] loss: 0.044
[45,     1] loss: 0.035
[46,     1] loss: 0.029
[47,     1] loss: 0.024
[48,     1] loss: 0.021
[49,     1] loss: 0.018
[50,     1] loss: 0.015
[51,     1] loss: 0.014
[52,     1] loss: 0.013
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.011
[58,     1] loss: 0.010
Early stopping applied (best metric=0.4168030321598053)
Finished Training
Total time taken: 24.82042694091797
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.666
[4,     1] loss: 0.634
[5,     1] loss: 0.596
[6,     1] loss: 0.551
[7,     1] loss: 0.495
[8,     1] loss: 0.455
[9,     1] loss: 0.412
[10,     1] loss: 0.378
[11,     1] loss: 0.342
[12,     1] loss: 0.295
[13,     1] loss: 0.267
[14,     1] loss: 0.231
[15,     1] loss: 0.195
[16,     1] loss: 0.160
[17,     1] loss: 0.132
[18,     1] loss: 0.117
[19,     1] loss: 0.134
[20,     1] loss: 0.093
[21,     1] loss: 0.266
[22,     1] loss: 0.077
[23,     1] loss: 0.097
[24,     1] loss: 0.076
[25,     1] loss: 0.069
[26,     1] loss: 0.072
[27,     1] loss: 0.066
[28,     1] loss: 0.056
[29,     1] loss: 0.052
[30,     1] loss: 0.045
[31,     1] loss: 0.041
[32,     1] loss: 0.039
[33,     1] loss: 0.035
[34,     1] loss: 0.030
[35,     1] loss: 0.028
[36,     1] loss: 0.024
[37,     1] loss: 0.021
[38,     1] loss: 0.019
[39,     1] loss: 0.017
[40,     1] loss: 0.016
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.010
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.011
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.010
Early stopping applied (best metric=0.44203829765319824)
Finished Training
Total time taken: 25.540438652038574
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.639
[4,     1] loss: 0.588
[5,     1] loss: 0.543
[6,     1] loss: 0.494
[7,     1] loss: 0.453
[8,     1] loss: 0.409
[9,     1] loss: 0.377
[10,     1] loss: 0.334
[11,     1] loss: 0.288
[12,     1] loss: 0.258
[13,     1] loss: 0.226
[14,     1] loss: 0.198
[15,     1] loss: 0.170
[16,     1] loss: 0.146
[17,     1] loss: 0.125
[18,     1] loss: 0.108
[19,     1] loss: 0.090
[20,     1] loss: 0.082
[21,     1] loss: 0.079
[22,     1] loss: 0.069
[23,     1] loss: 0.062
[24,     1] loss: 0.054
[25,     1] loss: 0.047
[26,     1] loss: 0.042
[27,     1] loss: 0.039
[28,     1] loss: 0.035
[29,     1] loss: 0.031
[30,     1] loss: 0.026
[31,     1] loss: 0.024
[32,     1] loss: 0.021
[33,     1] loss: 0.020
[34,     1] loss: 0.019
[35,     1] loss: 0.018
[36,     1] loss: 0.017
[37,     1] loss: 0.016
[38,     1] loss: 0.015
[39,     1] loss: 0.014
[40,     1] loss: 0.015
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.013
[46,     1] loss: 0.013
[47,     1] loss: 0.013
[48,     1] loss: 0.012
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.009
[56,     1] loss: 0.008
Early stopping applied (best metric=0.48960748314857483)
Finished Training
Total time taken: 23.993409633636475
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.691
[3,     1] loss: 0.666
[4,     1] loss: 0.637
[5,     1] loss: 0.602
[6,     1] loss: 0.558
[7,     1] loss: 0.501
[8,     1] loss: 0.450
[9,     1] loss: 0.402
[10,     1] loss: 0.360
[11,     1] loss: 0.305
[12,     1] loss: 0.273
[13,     1] loss: 0.232
[14,     1] loss: 0.184
[15,     1] loss: 0.152
[16,     1] loss: 0.133
[17,     1] loss: 0.110
[18,     1] loss: 0.080
[19,     1] loss: 0.078
[20,     1] loss: 0.066
[21,     1] loss: 0.036
[22,     1] loss: 0.031
[23,     1] loss: 0.021
[24,     1] loss: 0.015
[25,     1] loss: 0.013
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.008
[30,     1] loss: 0.007
[31,     1] loss: 0.007
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.006
[36,     1] loss: 0.006
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.005
[52,     1] loss: 0.006
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.004
[59,     1] loss: 0.005
[60,     1] loss: 0.004
Early stopping applied (best metric=0.3348855674266815)
Finished Training
Total time taken: 26.911460638046265
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.688
[3,     1] loss: 0.662
[4,     1] loss: 0.632
[5,     1] loss: 0.594
[6,     1] loss: 0.542
[7,     1] loss: 0.483
[8,     1] loss: 0.422
[9,     1] loss: 0.357
[10,     1] loss: 0.283
[11,     1] loss: 0.230
[12,     1] loss: 0.174
[13,     1] loss: 0.131
[14,     1] loss: 0.109
[15,     1] loss: 0.086
[16,     1] loss: 0.067
[17,     1] loss: 0.039
[18,     1] loss: 0.033
[19,     1] loss: 0.022
[20,     1] loss: 0.026
[21,     1] loss: 0.017
[22,     1] loss: 0.029
[23,     1] loss: 0.018
[24,     1] loss: 0.032
[25,     1] loss: 0.133
[26,     1] loss: 0.035
[27,     1] loss: 0.100
[28,     1] loss: 0.044
[29,     1] loss: 0.031
[30,     1] loss: 0.026
[31,     1] loss: 0.030
[32,     1] loss: 0.031
[33,     1] loss: 0.029
[34,     1] loss: 0.027
[35,     1] loss: 0.025
[36,     1] loss: 0.023
[37,     1] loss: 0.022
[38,     1] loss: 0.018
[39,     1] loss: 0.016
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.009
[54,     1] loss: 0.008
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.008
[59,     1] loss: 0.008
Early stopping applied (best metric=0.37051770091056824)
Finished Training
Total time taken: 26.941463232040405
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.642
[4,     1] loss: 0.596
[5,     1] loss: 0.536
[6,     1] loss: 0.473
[7,     1] loss: 0.398
[8,     1] loss: 0.329
[9,     1] loss: 0.267
[10,     1] loss: 0.203
[11,     1] loss: 0.181
[12,     1] loss: 0.157
[13,     1] loss: 0.127
[14,     1] loss: 0.128
[15,     1] loss: 0.100
[16,     1] loss: 0.093
[17,     1] loss: 0.078
[18,     1] loss: 0.053
[19,     1] loss: 0.058
[20,     1] loss: 0.040
[21,     1] loss: 0.033
[22,     1] loss: 0.028
[23,     1] loss: 0.025
[24,     1] loss: 0.021
[25,     1] loss: 0.017
[26,     1] loss: 0.015
[27,     1] loss: 0.013
[28,     1] loss: 0.012
[29,     1] loss: 0.011
[30,     1] loss: 0.010
[31,     1] loss: 0.009
[32,     1] loss: 0.009
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.007
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.006
Early stopping applied (best metric=0.4178544878959656)
Finished Training
Total time taken: 25.955446243286133
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.668
[3,     1] loss: 0.625
[4,     1] loss: 0.586
[5,     1] loss: 0.546
[6,     1] loss: 0.506
[7,     1] loss: 0.459
[8,     1] loss: 0.419
[9,     1] loss: 0.376
[10,     1] loss: 0.316
[11,     1] loss: 0.255
[12,     1] loss: 0.218
[13,     1] loss: 0.195
[14,     1] loss: 0.152
[15,     1] loss: 0.139
[16,     1] loss: 0.161
[17,     1] loss: 0.095
[18,     1] loss: 0.152
[19,     1] loss: 0.091
[20,     1] loss: 0.105
[21,     1] loss: 0.113
[22,     1] loss: 0.057
[23,     1] loss: 0.064
[24,     1] loss: 0.053
[25,     1] loss: 0.041
[26,     1] loss: 0.037
[27,     1] loss: 0.029
[28,     1] loss: 0.022
[29,     1] loss: 0.018
[30,     1] loss: 0.017
[31,     1] loss: 0.015
[32,     1] loss: 0.012
[33,     1] loss: 0.010
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.008
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.008
[46,     1] loss: 0.007
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
Early stopping applied (best metric=0.45486965775489807)
Finished Training
Total time taken: 24.173416137695312
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.654
[4,     1] loss: 0.613
[5,     1] loss: 0.559
[6,     1] loss: 0.494
[7,     1] loss: 0.421
[8,     1] loss: 0.343
[9,     1] loss: 0.280
[10,     1] loss: 0.231
[11,     1] loss: 0.185
[12,     1] loss: 0.157
[13,     1] loss: 0.137
[14,     1] loss: 0.109
[15,     1] loss: 0.099
[16,     1] loss: 0.185
[17,     1] loss: 0.072
[18,     1] loss: 0.176
[19,     1] loss: 0.070
[20,     1] loss: 0.168
[21,     1] loss: 0.068
[22,     1] loss: 0.063
[23,     1] loss: 0.074
[24,     1] loss: 0.070
[25,     1] loss: 0.061
[26,     1] loss: 0.052
[27,     1] loss: 0.046
[28,     1] loss: 0.041
[29,     1] loss: 0.038
[30,     1] loss: 0.034
[31,     1] loss: 0.031
[32,     1] loss: 0.027
[33,     1] loss: 0.025
[34,     1] loss: 0.022
[35,     1] loss: 0.020
[36,     1] loss: 0.019
[37,     1] loss: 0.017
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.012
[48,     1] loss: 0.012
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.012
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.011
Early stopping applied (best metric=0.49811968207359314)
Finished Training
Total time taken: 26.087448358535767
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.676
[3,     1] loss: 0.640
[4,     1] loss: 0.607
[5,     1] loss: 0.567
[6,     1] loss: 0.524
[7,     1] loss: 0.477
[8,     1] loss: 0.426
[9,     1] loss: 0.373
[10,     1] loss: 0.317
[11,     1] loss: 0.275
[12,     1] loss: 0.233
[13,     1] loss: 0.198
[14,     1] loss: 0.165
[15,     1] loss: 0.136
[16,     1] loss: 0.103
[17,     1] loss: 0.080
[18,     1] loss: 0.074
[19,     1] loss: 0.057
[20,     1] loss: 0.059
[21,     1] loss: 0.082
[22,     1] loss: 0.059
[23,     1] loss: 0.060
[24,     1] loss: 0.046
[25,     1] loss: 0.045
[26,     1] loss: 0.044
[27,     1] loss: 0.032
[28,     1] loss: 0.030
[29,     1] loss: 0.027
[30,     1] loss: 0.023
[31,     1] loss: 0.023
[32,     1] loss: 0.021
[33,     1] loss: 0.020
[34,     1] loss: 0.019
[35,     1] loss: 0.018
[36,     1] loss: 0.018
[37,     1] loss: 0.017
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.015
[41,     1] loss: 0.015
[42,     1] loss: 0.014
[43,     1] loss: 0.013
[44,     1] loss: 0.012
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.010
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.009
Early stopping applied (best metric=0.41979390382766724)
Finished Training
Total time taken: 27.14146375656128
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.640
[4,     1] loss: 0.601
[5,     1] loss: 0.551
[6,     1] loss: 0.502
[7,     1] loss: 0.455
[8,     1] loss: 0.397
[9,     1] loss: 0.358
[10,     1] loss: 0.307
[11,     1] loss: 0.265
[12,     1] loss: 0.227
[13,     1] loss: 0.210
[14,     1] loss: 0.178
[15,     1] loss: 0.160
[16,     1] loss: 0.138
[17,     1] loss: 0.118
[18,     1] loss: 0.104
[19,     1] loss: 0.089
[20,     1] loss: 0.076
[21,     1] loss: 0.067
[22,     1] loss: 0.057
[23,     1] loss: 0.045
[24,     1] loss: 0.034
[25,     1] loss: 0.027
[26,     1] loss: 0.022
[27,     1] loss: 0.019
[28,     1] loss: 0.017
[29,     1] loss: 0.015
[30,     1] loss: 0.013
[31,     1] loss: 0.013
[32,     1] loss: 0.012
[33,     1] loss: 0.011
[34,     1] loss: 0.011
[35,     1] loss: 0.010
[36,     1] loss: 0.010
[37,     1] loss: 0.011
[38,     1] loss: 0.010
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.009
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.007
Early stopping applied (best metric=0.46364232897758484)
Finished Training
Total time taken: 28.7194926738739
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.656
[4,     1] loss: 0.618
[5,     1] loss: 0.574
[6,     1] loss: 0.520
[7,     1] loss: 0.467
[8,     1] loss: 0.396
[9,     1] loss: 0.329
[10,     1] loss: 0.263
[11,     1] loss: 0.207
[12,     1] loss: 0.157
[13,     1] loss: 0.102
[14,     1] loss: 0.076
[15,     1] loss: 0.051
[16,     1] loss: 0.039
[17,     1] loss: 0.035
[18,     1] loss: 0.044
[19,     1] loss: 0.015
[20,     1] loss: 0.027
[21,     1] loss: 0.011
[22,     1] loss: 0.012
[23,     1] loss: 0.015
[24,     1] loss: 0.027
[25,     1] loss: 0.006
[26,     1] loss: 0.055
[27,     1] loss: 0.085
[28,     1] loss: 0.009
[29,     1] loss: 0.040
[30,     1] loss: 0.031
[31,     1] loss: 0.023
[32,     1] loss: 0.022
[33,     1] loss: 0.023
[34,     1] loss: 0.023
[35,     1] loss: 0.024
[36,     1] loss: 0.022
[37,     1] loss: 0.018
[38,     1] loss: 0.017
[39,     1] loss: 0.015
[40,     1] loss: 0.013
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.008
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.007
[59,     1] loss: 0.007
[60,     1] loss: 0.007
Early stopping applied (best metric=0.31384557485580444)
Finished Training
Total time taken: 28.683492183685303
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.690
[3,     1] loss: 0.656
[4,     1] loss: 0.614
[5,     1] loss: 0.563
[6,     1] loss: 0.503
[7,     1] loss: 0.443
[8,     1] loss: 0.384
[9,     1] loss: 0.330
[10,     1] loss: 0.275
[11,     1] loss: 0.227
[12,     1] loss: 0.190
[13,     1] loss: 0.172
[14,     1] loss: 0.127
[15,     1] loss: 0.109
[16,     1] loss: 0.094
[17,     1] loss: 0.090
[18,     1] loss: 0.074
[19,     1] loss: 0.067
[20,     1] loss: 0.051
[21,     1] loss: 0.049
[22,     1] loss: 0.107
[23,     1] loss: 0.026
[24,     1] loss: 0.027
[25,     1] loss: 0.024
[26,     1] loss: 0.023
[27,     1] loss: 0.022
[28,     1] loss: 0.021
[29,     1] loss: 0.018
[30,     1] loss: 0.018
[31,     1] loss: 0.017
[32,     1] loss: 0.015
[33,     1] loss: 0.016
[34,     1] loss: 0.014
[35,     1] loss: 0.013
[36,     1] loss: 0.013
[37,     1] loss: 0.012
[38,     1] loss: 0.016
[39,     1] loss: 0.012
[40,     1] loss: 0.029
[41,     1] loss: 0.012
[42,     1] loss: 0.051
[43,     1] loss: 0.275
[44,     1] loss: 0.114
[45,     1] loss: 0.121
[46,     1] loss: 0.119
[47,     1] loss: 0.086
[48,     1] loss: 0.078
[49,     1] loss: 0.076
[50,     1] loss: 0.066
[51,     1] loss: 0.050
[52,     1] loss: 0.040
[53,     1] loss: 0.040
[54,     1] loss: 0.027
[55,     1] loss: 0.023
[56,     1] loss: 0.019
[57,     1] loss: 0.017
[58,     1] loss: 0.014
[59,     1] loss: 0.013
[60,     1] loss: 0.011
[61,     1] loss: 0.011
[62,     1] loss: 0.010
[63,     1] loss: 0.010
[64,     1] loss: 0.009
[65,     1] loss: 0.010
[66,     1] loss: 0.010
[67,     1] loss: 0.010
[68,     1] loss: 0.011
[69,     1] loss: 0.011
[70,     1] loss: 0.011
[71,     1] loss: 0.012
[72,     1] loss: 0.012
[73,     1] loss: 0.013
[74,     1] loss: 0.012
[75,     1] loss: 0.013
[76,     1] loss: 0.012
[77,     1] loss: 0.012
[78,     1] loss: 0.011
[79,     1] loss: 0.011
[80,     1] loss: 0.010
[81,     1] loss: 0.010
[82,     1] loss: 0.009
[83,     1] loss: 0.009
[84,     1] loss: 0.009
[85,     1] loss: 0.008
[86,     1] loss: 0.008
[87,     1] loss: 0.008
[88,     1] loss: 0.007
[89,     1] loss: 0.007
[90,     1] loss: 0.007
[91,     1] loss: 0.007
[92,     1] loss: 0.007
[93,     1] loss: 0.007
[94,     1] loss: 0.007
[95,     1] loss: 0.007
[96,     1] loss: 0.007
[97,     1] loss: 0.008
[98,     1] loss: 0.008
[99,     1] loss: 0.008
Early stopping applied (best metric=0.430415540933609)
Finished Training
Total time taken: 49.83385229110718
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.615
[5,     1] loss: 0.567
[6,     1] loss: 0.509
[7,     1] loss: 0.437
[8,     1] loss: 0.363
[9,     1] loss: 0.292
[10,     1] loss: 0.231
[11,     1] loss: 0.177
[12,     1] loss: 0.147
[13,     1] loss: 0.128
[14,     1] loss: 0.089
[15,     1] loss: 0.078
[16,     1] loss: 0.096
[17,     1] loss: 0.070
[18,     1] loss: 0.055
[19,     1] loss: 0.072
[20,     1] loss: 0.064
[21,     1] loss: 0.038
[22,     1] loss: 0.034
[23,     1] loss: 0.078
[24,     1] loss: 0.051
[25,     1] loss: 0.037
[26,     1] loss: 0.028
[27,     1] loss: 0.048
[28,     1] loss: 0.046
[29,     1] loss: 0.030
[30,     1] loss: 0.026
[31,     1] loss: 0.036
[32,     1] loss: 0.036
[33,     1] loss: 0.032
[34,     1] loss: 0.025
[35,     1] loss: 0.021
[36,     1] loss: 0.019
[37,     1] loss: 0.018
[38,     1] loss: 0.015
[39,     1] loss: 0.014
[40,     1] loss: 0.013
[41,     1] loss: 0.012
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.009
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.009
[57,     1] loss: 0.009
Early stopping applied (best metric=0.43895477056503296)
Finished Training
Total time taken: 27.737473964691162
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.679
[3,     1] loss: 0.638
[4,     1] loss: 0.594
[5,     1] loss: 0.550
[6,     1] loss: 0.509
[7,     1] loss: 0.457
[8,     1] loss: 0.403
[9,     1] loss: 0.340
[10,     1] loss: 0.279
[11,     1] loss: 0.243
[12,     1] loss: 0.190
[13,     1] loss: 0.142
[14,     1] loss: 0.118
[15,     1] loss: 0.095
[16,     1] loss: 0.088
[17,     1] loss: 0.068
[18,     1] loss: 0.058
[19,     1] loss: 0.089
[20,     1] loss: 0.047
[21,     1] loss: 0.099
[22,     1] loss: 0.053
[23,     1] loss: 0.145
[24,     1] loss: 0.062
[25,     1] loss: 0.104
[26,     1] loss: 0.039
[27,     1] loss: 0.068
[28,     1] loss: 0.052
[29,     1] loss: 0.046
[30,     1] loss: 0.047
[31,     1] loss: 0.036
[32,     1] loss: 0.037
[33,     1] loss: 0.034
[34,     1] loss: 0.027
[35,     1] loss: 0.023
[36,     1] loss: 0.019
[37,     1] loss: 0.018
[38,     1] loss: 0.014
[39,     1] loss: 0.012
[40,     1] loss: 0.011
[41,     1] loss: 0.010
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.009
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.009
Early stopping applied (best metric=0.4172879159450531)
Finished Training
Total time taken: 27.3654727935791
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.673
[3,     1] loss: 0.623
[4,     1] loss: 0.567
[5,     1] loss: 0.512
[6,     1] loss: 0.467
[7,     1] loss: 0.424
[8,     1] loss: 0.388
[9,     1] loss: 0.362
[10,     1] loss: 0.344
[11,     1] loss: 0.324
[12,     1] loss: 0.294
[13,     1] loss: 0.273
[14,     1] loss: 0.254
[15,     1] loss: 0.232
[16,     1] loss: 0.211
[17,     1] loss: 0.194
[18,     1] loss: 0.172
[19,     1] loss: 0.154
[20,     1] loss: 0.140
[21,     1] loss: 0.123
[22,     1] loss: 0.111
[23,     1] loss: 0.097
[24,     1] loss: 0.088
[25,     1] loss: 0.101
[26,     1] loss: 0.112
[27,     1] loss: 0.072
[28,     1] loss: 0.214
[29,     1] loss: 0.134
[30,     1] loss: 0.183
[31,     1] loss: 0.089
[32,     1] loss: 0.101
[33,     1] loss: 0.105
[34,     1] loss: 0.095
[35,     1] loss: 0.092
[36,     1] loss: 0.076
[37,     1] loss: 0.076
[38,     1] loss: 0.068
[39,     1] loss: 0.068
[40,     1] loss: 0.063
[41,     1] loss: 0.058
[42,     1] loss: 0.054
[43,     1] loss: 0.051
[44,     1] loss: 0.047
[45,     1] loss: 0.043
[46,     1] loss: 0.042
[47,     1] loss: 0.037
[48,     1] loss: 0.036
[49,     1] loss: 0.034
[50,     1] loss: 0.032
[51,     1] loss: 0.031
[52,     1] loss: 0.029
[53,     1] loss: 0.028
[54,     1] loss: 0.028
[55,     1] loss: 0.025
Early stopping applied (best metric=0.48843836784362793)
Finished Training
Total time taken: 27.67641592025757
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.665
[4,     1] loss: 0.639
[5,     1] loss: 0.605
[6,     1] loss: 0.566
[7,     1] loss: 0.524
[8,     1] loss: 0.466
[9,     1] loss: 0.407
[10,     1] loss: 0.334
[11,     1] loss: 0.280
[12,     1] loss: 0.267
[13,     1] loss: 0.222
[14,     1] loss: 0.187
[15,     1] loss: 0.174
[16,     1] loss: 0.149
[17,     1] loss: 0.127
[18,     1] loss: 0.099
[19,     1] loss: 0.084
[20,     1] loss: 0.069
[21,     1] loss: 0.051
[22,     1] loss: 0.037
[23,     1] loss: 0.031
[24,     1] loss: 0.024
[25,     1] loss: 0.019
[26,     1] loss: 0.016
[27,     1] loss: 0.014
[28,     1] loss: 0.013
[29,     1] loss: 0.011
[30,     1] loss: 0.011
[31,     1] loss: 0.010
[32,     1] loss: 0.009
[33,     1] loss: 0.009
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.010
[37,     1] loss: 0.010
[38,     1] loss: 0.009
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.011
[42,     1] loss: 0.010
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.008
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.007
[57,     1] loss: 0.008
[58,     1] loss: 0.007
[59,     1] loss: 0.007
[60,     1] loss: 0.007
Early stopping applied (best metric=0.38717809319496155)
Finished Training
Total time taken: 29.903334856033325
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.670
[3,     1] loss: 0.611
[4,     1] loss: 0.566
[5,     1] loss: 0.519
[6,     1] loss: 0.469
[7,     1] loss: 0.421
[8,     1] loss: 0.355
[9,     1] loss: 0.294
[10,     1] loss: 0.240
[11,     1] loss: 0.190
[12,     1] loss: 0.148
[13,     1] loss: 0.116
[14,     1] loss: 0.089
[15,     1] loss: 0.086
[16,     1] loss: 0.079
[17,     1] loss: 0.076
[18,     1] loss: 0.068
[19,     1] loss: 0.049
[20,     1] loss: 0.047
[21,     1] loss: 0.044
[22,     1] loss: 0.036
[23,     1] loss: 0.033
[24,     1] loss: 0.030
[25,     1] loss: 0.026
[26,     1] loss: 0.020
[27,     1] loss: 0.017
[28,     1] loss: 0.016
[29,     1] loss: 0.014
[30,     1] loss: 0.014
[31,     1] loss: 0.012
[32,     1] loss: 0.011
[33,     1] loss: 0.010
[34,     1] loss: 0.010
[35,     1] loss: 0.010
[36,     1] loss: 0.009
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.006
[45,     1] loss: 0.005
[46,     1] loss: 0.005
[47,     1] loss: 0.005
[48,     1] loss: 0.005
[49,     1] loss: 0.004
[50,     1] loss: 0.004
[51,     1] loss: 0.005
[52,     1] loss: 0.004
[53,     1] loss: 0.004
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
Early stopping applied (best metric=0.42490944266319275)
Finished Training
Total time taken: 29.423332691192627
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.644
[4,     1] loss: 0.602
[5,     1] loss: 0.555
[6,     1] loss: 0.505
[7,     1] loss: 0.453
[8,     1] loss: 0.395
[9,     1] loss: 0.342
[10,     1] loss: 0.285
[11,     1] loss: 0.241
[12,     1] loss: 0.222
[13,     1] loss: 0.206
[14,     1] loss: 0.155
[15,     1] loss: 0.211
[16,     1] loss: 0.131
[17,     1] loss: 0.277
[18,     1] loss: 0.129
[19,     1] loss: 0.123
[20,     1] loss: 0.118
[21,     1] loss: 0.109
[22,     1] loss: 0.097
[23,     1] loss: 0.099
[24,     1] loss: 0.095
[25,     1] loss: 0.087
[26,     1] loss: 0.084
[27,     1] loss: 0.078
[28,     1] loss: 0.075
[29,     1] loss: 0.070
[30,     1] loss: 0.059
[31,     1] loss: 0.050
[32,     1] loss: 0.045
[33,     1] loss: 0.039
[34,     1] loss: 0.037
[35,     1] loss: 0.033
[36,     1] loss: 0.029
[37,     1] loss: 0.027
[38,     1] loss: 0.025
[39,     1] loss: 0.023
[40,     1] loss: 0.021
[41,     1] loss: 0.019
[42,     1] loss: 0.018
[43,     1] loss: 0.018
[44,     1] loss: 0.017
[45,     1] loss: 0.016
[46,     1] loss: 0.015
[47,     1] loss: 0.015
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.014
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.013
[54,     1] loss: 0.013
[55,     1] loss: 0.012
[56,     1] loss: 0.012
[57,     1] loss: 0.012
[58,     1] loss: 0.011
[59,     1] loss: 0.011
Early stopping applied (best metric=0.39212194085121155)
Finished Training
Total time taken: 30.62334156036377
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.661
[3,     1] loss: 0.588
[4,     1] loss: 0.520
[5,     1] loss: 0.470
[6,     1] loss: 0.428
[7,     1] loss: 0.372
[8,     1] loss: 0.315
[9,     1] loss: 0.269
[10,     1] loss: 0.216
[11,     1] loss: 0.179
[12,     1] loss: 0.143
[13,     1] loss: 0.116
[14,     1] loss: 0.093
[15,     1] loss: 0.075
[16,     1] loss: 0.057
[17,     1] loss: 0.056
[18,     1] loss: 0.032
[19,     1] loss: 0.028
[20,     1] loss: 0.019
[21,     1] loss: 0.021
[22,     1] loss: 0.013
[23,     1] loss: 0.020
[24,     1] loss: 0.009
[25,     1] loss: 0.069
[26,     1] loss: 0.016
[27,     1] loss: 0.052
[28,     1] loss: 0.130
[29,     1] loss: 0.167
[30,     1] loss: 0.039
[31,     1] loss: 0.072
[32,     1] loss: 0.055
[33,     1] loss: 0.051
[34,     1] loss: 0.061
[35,     1] loss: 0.059
[36,     1] loss: 0.056
[37,     1] loss: 0.048
[38,     1] loss: 0.041
[39,     1] loss: 0.036
[40,     1] loss: 0.030
[41,     1] loss: 0.025
[42,     1] loss: 0.020
[43,     1] loss: 0.017
[44,     1] loss: 0.014
[45,     1] loss: 0.012
[46,     1] loss: 0.011
[47,     1] loss: 0.010
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.009
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.010
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.010
Early stopping applied (best metric=0.4092905521392822)
Finished Training
Total time taken: 31.173351287841797
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.686
[3,     1] loss: 0.646
[4,     1] loss: 0.603
[5,     1] loss: 0.556
[6,     1] loss: 0.494
[7,     1] loss: 0.429
[8,     1] loss: 0.356
[9,     1] loss: 0.276
[10,     1] loss: 0.217
[11,     1] loss: 0.165
[12,     1] loss: 0.129
[13,     1] loss: 0.104
[14,     1] loss: 0.093
[15,     1] loss: 0.063
[16,     1] loss: 0.071
[17,     1] loss: 0.046
[18,     1] loss: 0.046
[19,     1] loss: 0.035
[20,     1] loss: 0.034
[21,     1] loss: 0.028
[22,     1] loss: 0.023
[23,     1] loss: 0.022
[24,     1] loss: 0.016
[25,     1] loss: 0.016
[26,     1] loss: 0.012
[27,     1] loss: 0.011
[28,     1] loss: 0.008
[29,     1] loss: 0.009
[30,     1] loss: 0.007
[31,     1] loss: 0.007
[32,     1] loss: 0.009
[33,     1] loss: 0.010
[34,     1] loss: 0.008
[35,     1] loss: 0.013
[36,     1] loss: 0.013
[37,     1] loss: 0.013
[38,     1] loss: 0.022
[39,     1] loss: 0.013
[40,     1] loss: 0.022
[41,     1] loss: 0.019
[42,     1] loss: 0.020
[43,     1] loss: 0.014
[44,     1] loss: 0.010
[45,     1] loss: 0.012
[46,     1] loss: 0.013
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.008
[50,     1] loss: 0.007
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
Early stopping applied (best metric=0.4218643307685852)
Finished Training
Total time taken: 29.828334093093872
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.678
[3,     1] loss: 0.651
[4,     1] loss: 0.620
[5,     1] loss: 0.583
[6,     1] loss: 0.534
[7,     1] loss: 0.485
[8,     1] loss: 0.418
[9,     1] loss: 0.350
[10,     1] loss: 0.281
[11,     1] loss: 0.249
[12,     1] loss: 0.236
[13,     1] loss: 0.172
[14,     1] loss: 0.152
[15,     1] loss: 0.136
[16,     1] loss: 0.112
[17,     1] loss: 0.104
[18,     1] loss: 0.083
[19,     1] loss: 0.070
[20,     1] loss: 0.059
[21,     1] loss: 0.044
[22,     1] loss: 0.035
[23,     1] loss: 0.030
[24,     1] loss: 0.029
[25,     1] loss: 0.034
[26,     1] loss: 0.022
[27,     1] loss: 0.030
[28,     1] loss: 0.030
[29,     1] loss: 0.018
[30,     1] loss: 0.014
[31,     1] loss: 0.014
[32,     1] loss: 0.014
[33,     1] loss: 0.014
[34,     1] loss: 0.014
[35,     1] loss: 0.013
[36,     1] loss: 0.012
[37,     1] loss: 0.011
[38,     1] loss: 0.012
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.008
[59,     1] loss: 0.007
[60,     1] loss: 0.007
[61,     1] loss: 0.007
[62,     1] loss: 0.007
Early stopping applied (best metric=0.3597637116909027)
Finished Training
Total time taken: 33.26137113571167
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.648
[4,     1] loss: 0.609
[5,     1] loss: 0.568
[6,     1] loss: 0.522
[7,     1] loss: 0.471
[8,     1] loss: 0.425
[9,     1] loss: 0.360
[10,     1] loss: 0.299
[11,     1] loss: 0.248
[12,     1] loss: 0.201
[13,     1] loss: 0.167
[14,     1] loss: 0.142
[15,     1] loss: 0.124
[16,     1] loss: 0.108
[17,     1] loss: 0.093
[18,     1] loss: 0.070
[19,     1] loss: 0.070
[20,     1] loss: 0.081
[21,     1] loss: 0.066
[22,     1] loss: 0.042
[23,     1] loss: 0.057
[24,     1] loss: 0.032
[25,     1] loss: 0.033
[26,     1] loss: 0.024
[27,     1] loss: 0.018
[28,     1] loss: 0.018
[29,     1] loss: 0.015
[30,     1] loss: 0.015
[31,     1] loss: 0.012
[32,     1] loss: 0.012
[33,     1] loss: 0.011
[34,     1] loss: 0.010
[35,     1] loss: 0.009
[36,     1] loss: 0.010
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.36145609617233276)
Finished Training
Total time taken: 30.391345024108887
{'Hydroxylation-P Validation Accuracy': 0.6623434749505102, 'Hydroxylation-P Validation Sensitivity': 0.8808253968253968, 'Hydroxylation-P Validation Specificity': 0.6154032620080802, 'Hydroxylation-P Validation Precision': 0.33798180354534185, 'Hydroxylation-P AUC ROC': 0.8299893534649632, 'Hydroxylation-P AUC PR': 0.5366445611881112, 'Hydroxylation-P MCC': 0.384488687716433, 'Hydroxylation-P F1': 0.485696930354995, 'Validation Loss (Hydroxylation-P)': 0.41148062944412234, 'Validation Loss (total)': 0.41148062944412234}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026778758413817318,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3264475874,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.029096180306807}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.673
[3,     1] loss: 0.647
[4,     1] loss: 0.621
[5,     1] loss: 0.597
[6,     1] loss: 0.574
[7,     1] loss: 0.548
[8,     1] loss: 0.521
[9,     1] loss: 0.493
[10,     1] loss: 0.464
[11,     1] loss: 0.434
[12,     1] loss: 0.407
[13,     1] loss: 0.380
[14,     1] loss: 0.353
[15,     1] loss: 0.326
[16,     1] loss: 0.310
[17,     1] loss: 0.292
[18,     1] loss: 0.274
[19,     1] loss: 0.253
[20,     1] loss: 0.240
[21,     1] loss: 0.219
[22,     1] loss: 0.201
[23,     1] loss: 0.187
[24,     1] loss: 0.172
[25,     1] loss: 0.159
[26,     1] loss: 0.146
[27,     1] loss: 0.135
[28,     1] loss: 0.126
[29,     1] loss: 0.115
[30,     1] loss: 0.109
[31,     1] loss: 0.102
[32,     1] loss: 0.100
[33,     1] loss: 0.093
[34,     1] loss: 0.090
[35,     1] loss: 0.084
[36,     1] loss: 0.081
[37,     1] loss: 0.080
[38,     1] loss: 0.076
[39,     1] loss: 0.076
[40,     1] loss: 0.074
[41,     1] loss: 0.073
[42,     1] loss: 0.072
[43,     1] loss: 0.071
[44,     1] loss: 0.071
[45,     1] loss: 0.069
[46,     1] loss: 0.068
[47,     1] loss: 0.069
[48,     1] loss: 0.069
[49,     1] loss: 0.069
[50,     1] loss: 0.068
[51,     1] loss: 0.069
[52,     1] loss: 0.068
[53,     1] loss: 0.067
[54,     1] loss: 0.067
[55,     1] loss: 0.068
[56,     1] loss: 0.068
[57,     1] loss: 0.068
[58,     1] loss: 0.065
[59,     1] loss: 0.066
[60,     1] loss: 0.067
[61,     1] loss: 0.067
[62,     1] loss: 0.066
[63,     1] loss: 0.064
[64,     1] loss: 0.064
[65,     1] loss: 0.063
[66,     1] loss: 0.061
[67,     1] loss: 0.061
Early stopping applied (best metric=0.397912859916687)
Finished Training
Total time taken: 32.90236711502075
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.684
[3,     1] loss: 0.662
[4,     1] loss: 0.643
[5,     1] loss: 0.626
[6,     1] loss: 0.607
[7,     1] loss: 0.585
[8,     1] loss: 0.562
[9,     1] loss: 0.540
[10,     1] loss: 0.515
[11,     1] loss: 0.485
[12,     1] loss: 0.456
[13,     1] loss: 0.426
[14,     1] loss: 0.399
[15,     1] loss: 0.373
[16,     1] loss: 0.349
[17,     1] loss: 0.327
[18,     1] loss: 0.302
[19,     1] loss: 0.275
[20,     1] loss: 0.255
[21,     1] loss: 0.240
[22,     1] loss: 0.222
[23,     1] loss: 0.202
[24,     1] loss: 0.185
[25,     1] loss: 0.173
[26,     1] loss: 0.164
[27,     1] loss: 0.154
[28,     1] loss: 0.143
[29,     1] loss: 0.135
[30,     1] loss: 0.128
[31,     1] loss: 0.121
[32,     1] loss: 0.114
[33,     1] loss: 0.109
[34,     1] loss: 0.103
[35,     1] loss: 0.098
[36,     1] loss: 0.094
[37,     1] loss: 0.088
[38,     1] loss: 0.088
[39,     1] loss: 0.086
[40,     1] loss: 0.082
[41,     1] loss: 0.082
[42,     1] loss: 0.078
[43,     1] loss: 0.077
[44,     1] loss: 0.077
[45,     1] loss: 0.077
[46,     1] loss: 0.075
[47,     1] loss: 0.073
[48,     1] loss: 0.074
[49,     1] loss: 0.073
[50,     1] loss: 0.073
[51,     1] loss: 0.070
[52,     1] loss: 0.071
[53,     1] loss: 0.071
[54,     1] loss: 0.071
[55,     1] loss: 0.071
[56,     1] loss: 0.070
[57,     1] loss: 0.071
[58,     1] loss: 0.069
[59,     1] loss: 0.068
[60,     1] loss: 0.068
[61,     1] loss: 0.067
[62,     1] loss: 0.065
[63,     1] loss: 0.065
[64,     1] loss: 0.065
[65,     1] loss: 0.064
[66,     1] loss: 0.062
[67,     1] loss: 0.059
[68,     1] loss: 0.059
[69,     1] loss: 0.057
[70,     1] loss: 0.056
[71,     1] loss: 0.149
[72,     1] loss: 0.998
[73,     1] loss: 0.407
[74,     1] loss: 0.376
[75,     1] loss: 0.389
[76,     1] loss: 0.391
[77,     1] loss: 0.389
[78,     1] loss: 0.377
[79,     1] loss: 0.363
[80,     1] loss: 0.352
[81,     1] loss: 0.333
[82,     1] loss: 0.312
[83,     1] loss: 0.284
[84,     1] loss: 0.260
[85,     1] loss: 0.237
[86,     1] loss: 0.210
[87,     1] loss: 0.194
[88,     1] loss: 0.178
[89,     1] loss: 0.160
[90,     1] loss: 0.145
[91,     1] loss: 0.131
[92,     1] loss: 0.119
[93,     1] loss: 0.108
[94,     1] loss: 0.095
[95,     1] loss: 0.090
[96,     1] loss: 0.084
[97,     1] loss: 0.078
[98,     1] loss: 0.072
[99,     1] loss: 0.076
[100,     1] loss: 0.067
[101,     1] loss: 0.072
[102,     1] loss: 0.063
[103,     1] loss: 0.068
[104,     1] loss: 0.064
[105,     1] loss: 0.063
[106,     1] loss: 0.062
[107,     1] loss: 0.066
[108,     1] loss: 0.065
[109,     1] loss: 0.066
[110,     1] loss: 0.067
[111,     1] loss: 0.065
[112,     1] loss: 0.067
[113,     1] loss: 0.068
[114,     1] loss: 0.065
[115,     1] loss: 0.067
[116,     1] loss: 0.067
[117,     1] loss: 0.067
[118,     1] loss: 0.067
[119,     1] loss: 0.067
[120,     1] loss: 0.068
[121,     1] loss: 0.068
[122,     1] loss: 0.066
[123,     1] loss: 0.065
[124,     1] loss: 0.068
[125,     1] loss: 0.066
[126,     1] loss: 0.066
[127,     1] loss: 0.067
[128,     1] loss: 0.066
[129,     1] loss: 0.065
[130,     1] loss: 0.064
[131,     1] loss: 0.064
[132,     1] loss: 0.064
[133,     1] loss: 0.065
[134,     1] loss: 0.066
[135,     1] loss: 0.066
[136,     1] loss: 0.066
Early stopping applied (best metric=0.3496597409248352)
Finished Training
Total time taken: 68.71077132225037
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.683
[3,     1] loss: 0.664
[4,     1] loss: 0.643
[5,     1] loss: 0.620
[6,     1] loss: 0.600
[7,     1] loss: 0.577
[8,     1] loss: 0.557
[9,     1] loss: 0.533
[10,     1] loss: 0.510
[11,     1] loss: 0.484
[12,     1] loss: 0.461
[13,     1] loss: 0.432
[14,     1] loss: 0.401
[15,     1] loss: 0.377
[16,     1] loss: 0.354
[17,     1] loss: 0.331
[18,     1] loss: 0.308
[19,     1] loss: 0.293
[20,     1] loss: 0.277
[21,     1] loss: 0.258
[22,     1] loss: 0.243
[23,     1] loss: 0.230
[24,     1] loss: 0.220
[25,     1] loss: 0.209
[26,     1] loss: 0.195
[27,     1] loss: 0.186
[28,     1] loss: 0.178
[29,     1] loss: 0.164
[30,     1] loss: 0.157
[31,     1] loss: 0.147
[32,     1] loss: 0.139
[33,     1] loss: 0.132
[34,     1] loss: 0.127
[35,     1] loss: 0.117
[36,     1] loss: 0.115
[37,     1] loss: 0.111
[38,     1] loss: 0.106
[39,     1] loss: 0.102
[40,     1] loss: 0.098
[41,     1] loss: 0.095
[42,     1] loss: 0.092
[43,     1] loss: 0.090
[44,     1] loss: 0.087
[45,     1] loss: 0.084
[46,     1] loss: 0.083
[47,     1] loss: 0.080
[48,     1] loss: 0.078
[49,     1] loss: 0.077
[50,     1] loss: 0.076
[51,     1] loss: 0.075
[52,     1] loss: 0.072
[53,     1] loss: 0.071
[54,     1] loss: 0.071
[55,     1] loss: 0.069
[56,     1] loss: 0.069
[57,     1] loss: 0.067
[58,     1] loss: 0.066
[59,     1] loss: 0.065
[60,     1] loss: 0.063
[61,     1] loss: 0.061
[62,     1] loss: 0.060
[63,     1] loss: 0.058
[64,     1] loss: 0.055
[65,     1] loss: 0.053
[66,     1] loss: 0.052
[67,     1] loss: 0.050
[68,     1] loss: 0.049
[69,     1] loss: 0.048
[70,     1] loss: 0.047
[71,     1] loss: 0.047
Early stopping applied (best metric=0.37280893325805664)
Finished Training
Total time taken: 37.47105956077576
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.658
[4,     1] loss: 0.639
[5,     1] loss: 0.618
[6,     1] loss: 0.594
[7,     1] loss: 0.570
[8,     1] loss: 0.543
[9,     1] loss: 0.514
[10,     1] loss: 0.485
[11,     1] loss: 0.452
[12,     1] loss: 0.421
[13,     1] loss: 0.396
[14,     1] loss: 0.376
[15,     1] loss: 0.365
[16,     1] loss: 0.356
[17,     1] loss: 0.350
[18,     1] loss: 0.345
[19,     1] loss: 0.342
[20,     1] loss: 0.338
[21,     1] loss: 0.336
[22,     1] loss: 0.332
[23,     1] loss: 0.329
[24,     1] loss: 0.327
[25,     1] loss: 0.324
[26,     1] loss: 0.320
[27,     1] loss: 0.316
[28,     1] loss: 0.314
[29,     1] loss: 0.310
[30,     1] loss: 0.307
[31,     1] loss: 0.303
[32,     1] loss: 0.298
[33,     1] loss: 0.295
[34,     1] loss: 0.289
[35,     1] loss: 0.283
[36,     1] loss: 0.279
[37,     1] loss: 0.270
[38,     1] loss: 0.264
[39,     1] loss: 0.258
[40,     1] loss: 0.250
[41,     1] loss: 0.245
[42,     1] loss: 0.237
[43,     1] loss: 0.229
[44,     1] loss: 0.222
[45,     1] loss: 0.217
[46,     1] loss: 0.209
[47,     1] loss: 0.204
[48,     1] loss: 0.197
[49,     1] loss: 0.191
[50,     1] loss: 0.187
[51,     1] loss: 0.182
[52,     1] loss: 0.176
[53,     1] loss: 0.171
[54,     1] loss: 0.168
[55,     1] loss: 0.164
[56,     1] loss: 0.161
[57,     1] loss: 0.158
[58,     1] loss: 0.156
[59,     1] loss: 0.152
[60,     1] loss: 0.150
[61,     1] loss: 0.147
Early stopping applied (best metric=0.44836586713790894)
Finished Training
Total time taken: 33.068140268325806
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.673
[3,     1] loss: 0.650
[4,     1] loss: 0.631
[5,     1] loss: 0.611
[6,     1] loss: 0.589
[7,     1] loss: 0.569
[8,     1] loss: 0.542
[9,     1] loss: 0.517
[10,     1] loss: 0.489
[11,     1] loss: 0.459
[12,     1] loss: 0.428
[13,     1] loss: 0.393
[14,     1] loss: 0.368
[15,     1] loss: 0.338
[16,     1] loss: 0.309
[17,     1] loss: 0.285
[18,     1] loss: 0.261
[19,     1] loss: 0.235
[20,     1] loss: 0.214
[21,     1] loss: 0.197
[22,     1] loss: 0.178
[23,     1] loss: 0.162
[24,     1] loss: 0.146
[25,     1] loss: 0.136
[26,     1] loss: 0.126
[27,     1] loss: 0.116
[28,     1] loss: 0.108
[29,     1] loss: 0.098
[30,     1] loss: 0.096
[31,     1] loss: 0.090
[32,     1] loss: 0.087
[33,     1] loss: 0.083
[34,     1] loss: 0.079
[35,     1] loss: 0.076
[36,     1] loss: 0.075
[37,     1] loss: 0.074
[38,     1] loss: 0.074
[39,     1] loss: 0.072
[40,     1] loss: 0.070
[41,     1] loss: 0.071
[42,     1] loss: 0.072
[43,     1] loss: 0.071
[44,     1] loss: 0.072
[45,     1] loss: 0.072
[46,     1] loss: 0.071
[47,     1] loss: 0.072
[48,     1] loss: 0.071
[49,     1] loss: 0.070
[50,     1] loss: 0.070
[51,     1] loss: 0.071
[52,     1] loss: 0.071
[53,     1] loss: 0.070
[54,     1] loss: 0.071
[55,     1] loss: 0.069
[56,     1] loss: 0.069
[57,     1] loss: 0.068
[58,     1] loss: 0.067
[59,     1] loss: 0.066
[60,     1] loss: 0.065
[61,     1] loss: 0.062
[62,     1] loss: 0.059
Early stopping applied (best metric=0.43289634585380554)
Finished Training
Total time taken: 33.56666350364685
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.671
[5,     1] loss: 0.659
[6,     1] loss: 0.646
[7,     1] loss: 0.632
[8,     1] loss: 0.618
[9,     1] loss: 0.601
[10,     1] loss: 0.581
[11,     1] loss: 0.559
[12,     1] loss: 0.539
[13,     1] loss: 0.515
[14,     1] loss: 0.483
[15,     1] loss: 0.458
[16,     1] loss: 0.428
[17,     1] loss: 0.395
[18,     1] loss: 0.365
[19,     1] loss: 0.339
[20,     1] loss: 0.318
[21,     1] loss: 0.301
[22,     1] loss: 0.273
[23,     1] loss: 0.257
[24,     1] loss: 0.238
[25,     1] loss: 0.222
[26,     1] loss: 0.207
[27,     1] loss: 0.191
[28,     1] loss: 0.179
[29,     1] loss: 0.168
[30,     1] loss: 0.157
[31,     1] loss: 0.145
[32,     1] loss: 0.138
[33,     1] loss: 0.127
[34,     1] loss: 0.118
[35,     1] loss: 0.111
[36,     1] loss: 0.105
[37,     1] loss: 0.098
[38,     1] loss: 0.097
[39,     1] loss: 0.092
[40,     1] loss: 0.087
[41,     1] loss: 0.085
[42,     1] loss: 0.084
[43,     1] loss: 0.083
[44,     1] loss: 0.080
[45,     1] loss: 0.078
[46,     1] loss: 0.079
[47,     1] loss: 0.077
[48,     1] loss: 0.076
[49,     1] loss: 0.073
[50,     1] loss: 0.076
[51,     1] loss: 0.074
[52,     1] loss: 0.072
[53,     1] loss: 0.073
[54,     1] loss: 0.073
[55,     1] loss: 0.072
[56,     1] loss: 0.071
[57,     1] loss: 0.071
[58,     1] loss: 0.071
[59,     1] loss: 0.071
[60,     1] loss: 0.069
[61,     1] loss: 0.069
[62,     1] loss: 0.068
[63,     1] loss: 0.068
[64,     1] loss: 0.067
[65,     1] loss: 0.065
[66,     1] loss: 0.064
Early stopping applied (best metric=0.4331110715866089)
Finished Training
Total time taken: 35.91898798942566
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.652
[4,     1] loss: 0.634
[5,     1] loss: 0.615
[6,     1] loss: 0.598
[7,     1] loss: 0.579
[8,     1] loss: 0.556
[9,     1] loss: 0.531
[10,     1] loss: 0.505
[11,     1] loss: 0.476
[12,     1] loss: 0.447
[13,     1] loss: 0.414
[14,     1] loss: 0.380
[15,     1] loss: 0.350
[16,     1] loss: 0.323
[17,     1] loss: 0.295
[18,     1] loss: 0.269
[19,     1] loss: 0.245
[20,     1] loss: 0.221
[21,     1] loss: 0.201
[22,     1] loss: 0.183
[23,     1] loss: 0.166
[24,     1] loss: 0.153
[25,     1] loss: 0.140
[26,     1] loss: 0.130
[27,     1] loss: 0.119
[28,     1] loss: 0.112
[29,     1] loss: 0.104
[30,     1] loss: 0.100
[31,     1] loss: 0.096
[32,     1] loss: 0.091
[33,     1] loss: 0.089
[34,     1] loss: 0.084
[35,     1] loss: 0.084
[36,     1] loss: 0.081
[37,     1] loss: 0.082
[38,     1] loss: 0.078
[39,     1] loss: 0.078
[40,     1] loss: 0.079
[41,     1] loss: 0.078
[42,     1] loss: 0.077
[43,     1] loss: 0.079
[44,     1] loss: 0.078
[45,     1] loss: 0.079
[46,     1] loss: 0.079
[47,     1] loss: 0.080
[48,     1] loss: 0.079
[49,     1] loss: 0.081
[50,     1] loss: 0.080
[51,     1] loss: 0.079
[52,     1] loss: 0.079
[53,     1] loss: 0.080
[54,     1] loss: 0.080
[55,     1] loss: 0.080
[56,     1] loss: 0.079
[57,     1] loss: 0.079
[58,     1] loss: 0.079
[59,     1] loss: 0.078
[60,     1] loss: 0.076
[61,     1] loss: 0.077
Early stopping applied (best metric=0.4320250153541565)
Finished Training
Total time taken: 33.08137035369873
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.666
[4,     1] loss: 0.650
[5,     1] loss: 0.631
[6,     1] loss: 0.612
[7,     1] loss: 0.591
[8,     1] loss: 0.570
[9,     1] loss: 0.545
[10,     1] loss: 0.523
[11,     1] loss: 0.495
[12,     1] loss: 0.468
[13,     1] loss: 0.441
[14,     1] loss: 0.410
[15,     1] loss: 0.386
[16,     1] loss: 0.356
[17,     1] loss: 0.331
[18,     1] loss: 0.304
[19,     1] loss: 0.278
[20,     1] loss: 0.254
[21,     1] loss: 0.234
[22,     1] loss: 0.213
[23,     1] loss: 0.192
[24,     1] loss: 0.171
[25,     1] loss: 0.154
[26,     1] loss: 0.139
[27,     1] loss: 0.128
[28,     1] loss: 0.116
[29,     1] loss: 0.109
[30,     1] loss: 0.099
[31,     1] loss: 0.092
[32,     1] loss: 0.090
[33,     1] loss: 0.082
[34,     1] loss: 0.078
[35,     1] loss: 0.075
[36,     1] loss: 0.072
[37,     1] loss: 0.069
[38,     1] loss: 0.068
[39,     1] loss: 0.068
[40,     1] loss: 0.065
[41,     1] loss: 0.064
[42,     1] loss: 0.066
[43,     1] loss: 0.065
[44,     1] loss: 0.065
[45,     1] loss: 0.065
[46,     1] loss: 0.065
[47,     1] loss: 0.064
[48,     1] loss: 0.065
[49,     1] loss: 0.065
[50,     1] loss: 0.065
[51,     1] loss: 0.065
[52,     1] loss: 0.065
[53,     1] loss: 0.065
[54,     1] loss: 0.065
[55,     1] loss: 0.064
[56,     1] loss: 0.064
[57,     1] loss: 0.063
[58,     1] loss: 0.064
[59,     1] loss: 0.065
[60,     1] loss: 0.063
[61,     1] loss: 0.062
[62,     1] loss: 0.062
[63,     1] loss: 0.059
[64,     1] loss: 0.057
[65,     1] loss: 0.056
[66,     1] loss: 0.054
[67,     1] loss: 0.082
[68,     1] loss: 0.376
[69,     1] loss: 0.863
[70,     1] loss: 0.460
[71,     1] loss: 0.464
[72,     1] loss: 0.407
Early stopping applied (best metric=0.39564958214759827)
Finished Training
Total time taken: 39.904688119888306
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.680
[4,     1] loss: 0.671
[5,     1] loss: 0.661
[6,     1] loss: 0.649
[7,     1] loss: 0.639
[8,     1] loss: 0.625
[9,     1] loss: 0.613
[10,     1] loss: 0.599
[11,     1] loss: 0.585
[12,     1] loss: 0.568
[13,     1] loss: 0.551
[14,     1] loss: 0.536
[15,     1] loss: 0.516
[16,     1] loss: 0.500
[17,     1] loss: 0.479
[18,     1] loss: 0.456
[19,     1] loss: 0.437
[20,     1] loss: 0.418
[21,     1] loss: 0.396
[22,     1] loss: 0.377
[23,     1] loss: 0.366
[24,     1] loss: 0.343
[25,     1] loss: 0.315
[26,     1] loss: 0.300
[27,     1] loss: 0.275
[28,     1] loss: 0.262
[29,     1] loss: 0.242
[30,     1] loss: 0.230
[31,     1] loss: 0.215
[32,     1] loss: 0.202
[33,     1] loss: 0.188
[34,     1] loss: 0.177
[35,     1] loss: 0.166
[36,     1] loss: 0.159
[37,     1] loss: 0.148
[38,     1] loss: 0.140
[39,     1] loss: 0.134
[40,     1] loss: 0.125
[41,     1] loss: 0.121
[42,     1] loss: 0.117
[43,     1] loss: 0.111
[44,     1] loss: 0.108
[45,     1] loss: 0.106
[46,     1] loss: 0.101
[47,     1] loss: 0.100
[48,     1] loss: 0.098
[49,     1] loss: 0.095
[50,     1] loss: 0.092
[51,     1] loss: 0.091
[52,     1] loss: 0.088
[53,     1] loss: 0.086
[54,     1] loss: 0.083
[55,     1] loss: 0.079
[56,     1] loss: 0.077
[57,     1] loss: 0.077
[58,     1] loss: 0.073
[59,     1] loss: 0.071
[60,     1] loss: 0.071
[61,     1] loss: 0.066
[62,     1] loss: 0.067
[63,     1] loss: 0.065
[64,     1] loss: 0.065
[65,     1] loss: 0.063
[66,     1] loss: 0.062
[67,     1] loss: 0.062
[68,     1] loss: 0.061
[69,     1] loss: 0.061
[70,     1] loss: 0.060
[71,     1] loss: 0.059
Early stopping applied (best metric=0.431595116853714)
Finished Training
Total time taken: 38.294429063797
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.679
[3,     1] loss: 0.663
[4,     1] loss: 0.649
[5,     1] loss: 0.636
[6,     1] loss: 0.624
[7,     1] loss: 0.608
[8,     1] loss: 0.593
[9,     1] loss: 0.577
[10,     1] loss: 0.557
[11,     1] loss: 0.535
[12,     1] loss: 0.508
[13,     1] loss: 0.481
[14,     1] loss: 0.458
[15,     1] loss: 0.429
[16,     1] loss: 0.397
[17,     1] loss: 0.371
[18,     1] loss: 0.346
[19,     1] loss: 0.324
[20,     1] loss: 0.302
[21,     1] loss: 0.278
[22,     1] loss: 0.260
[23,     1] loss: 0.243
[24,     1] loss: 0.220
[25,     1] loss: 0.207
[26,     1] loss: 0.191
[27,     1] loss: 0.181
[28,     1] loss: 0.169
[29,     1] loss: 0.159
[30,     1] loss: 0.149
[31,     1] loss: 0.140
[32,     1] loss: 0.134
[33,     1] loss: 0.126
[34,     1] loss: 0.122
[35,     1] loss: 0.117
[36,     1] loss: 0.112
[37,     1] loss: 0.110
[38,     1] loss: 0.106
[39,     1] loss: 0.105
[40,     1] loss: 0.104
[41,     1] loss: 0.102
[42,     1] loss: 0.100
[43,     1] loss: 0.100
[44,     1] loss: 0.097
[45,     1] loss: 0.096
[46,     1] loss: 0.095
[47,     1] loss: 0.095
[48,     1] loss: 0.095
[49,     1] loss: 0.095
[50,     1] loss: 0.095
[51,     1] loss: 0.095
[52,     1] loss: 0.096
[53,     1] loss: 0.095
[54,     1] loss: 0.093
[55,     1] loss: 0.094
[56,     1] loss: 0.094
[57,     1] loss: 0.094
[58,     1] loss: 0.093
[59,     1] loss: 0.093
[60,     1] loss: 0.091
[61,     1] loss: 0.091
[62,     1] loss: 0.091
[63,     1] loss: 0.091
[64,     1] loss: 0.090
[65,     1] loss: 0.088
[66,     1] loss: 0.086
[67,     1] loss: 0.088
[68,     1] loss: 0.218
[69,     1] loss: 0.659
[70,     1] loss: 0.588
[71,     1] loss: 0.482
[72,     1] loss: 0.442
[73,     1] loss: 0.430
[74,     1] loss: 0.439
[75,     1] loss: 0.446
[76,     1] loss: 0.449
[77,     1] loss: 0.443
[78,     1] loss: 0.428
[79,     1] loss: 0.418
[80,     1] loss: 0.402
[81,     1] loss: 0.388
[82,     1] loss: 0.372
[83,     1] loss: 0.357
[84,     1] loss: 0.343
[85,     1] loss: 0.329
[86,     1] loss: 0.313
[87,     1] loss: 0.294
[88,     1] loss: 0.269
[89,     1] loss: 0.241
[90,     1] loss: 0.221
[91,     1] loss: 0.200
[92,     1] loss: 0.182
[93,     1] loss: 0.163
[94,     1] loss: 0.148
[95,     1] loss: 0.132
[96,     1] loss: 0.115
[97,     1] loss: 0.103
[98,     1] loss: 0.093
[99,     1] loss: 0.083
[100,     1] loss: 0.075
[101,     1] loss: 0.069
[102,     1] loss: 0.064
[103,     1] loss: 0.060
[104,     1] loss: 0.057
[105,     1] loss: 0.055
[106,     1] loss: 0.054
[107,     1] loss: 0.053
[108,     1] loss: 0.053
[109,     1] loss: 0.054
[110,     1] loss: 0.055
[111,     1] loss: 0.054
[112,     1] loss: 0.057
[113,     1] loss: 0.057
[114,     1] loss: 0.059
[115,     1] loss: 0.060
[116,     1] loss: 0.060
[117,     1] loss: 0.063
[118,     1] loss: 0.062
[119,     1] loss: 0.065
[120,     1] loss: 0.063
[121,     1] loss: 0.065
[122,     1] loss: 0.065
[123,     1] loss: 0.064
[124,     1] loss: 0.065
[125,     1] loss: 0.064
[126,     1] loss: 0.064
[127,     1] loss: 0.062
[128,     1] loss: 0.063
[129,     1] loss: 0.063
[130,     1] loss: 0.061
[131,     1] loss: 0.061
[132,     1] loss: 0.059
[133,     1] loss: 0.062
[134,     1] loss: 0.061
[135,     1] loss: 0.060
[136,     1] loss: 0.062
[137,     1] loss: 0.059
[138,     1] loss: 0.059
[139,     1] loss: 0.061
[140,     1] loss: 0.061
[141,     1] loss: 0.060
[142,     1] loss: 0.059
[143,     1] loss: 0.059
[144,     1] loss: 0.061
[145,     1] loss: 0.061
[146,     1] loss: 0.061
[147,     1] loss: 0.062
[148,     1] loss: 0.063
[149,     1] loss: 0.065
[150,     1] loss: 0.249
[151,     1] loss: 0.413
[152,     1] loss: 1.368
[153,     1] loss: 1.213
[154,     1] loss: 0.982
[155,     1] loss: 0.825
[156,     1] loss: 0.733
[157,     1] loss: 0.698
[158,     1] loss: 0.689
[159,     1] loss: 0.688
[160,     1] loss: 0.689
[161,     1] loss: 0.690
[162,     1] loss: 0.691
[163,     1] loss: 0.692
[164,     1] loss: 0.692
[165,     1] loss: 0.692
[166,     1] loss: 0.693
[167,     1] loss: 0.693
[168,     1] loss: 0.693
[169,     1] loss: 0.693
[170,     1] loss: 0.693
[171,     1] loss: 0.693
[172,     1] loss: 0.693
[173,     1] loss: 0.693
[174,     1] loss: 0.693
[175,     1] loss: 0.693
[176,     1] loss: 0.693
[177,     1] loss: 0.693
[178,     1] loss: 0.693
[179,     1] loss: 0.693
[180,     1] loss: 0.693
[181,     1] loss: 0.693
[182,     1] loss: 0.693
[183,     1] loss: 0.693
[184,     1] loss: 0.693
[185,     1] loss: 0.693
[186,     1] loss: 0.693
[187,     1] loss: 0.693
[188,     1] loss: 0.693
[189,     1] loss: 0.693
[190,     1] loss: 0.693
Early stopping applied (best metric=0.2971658706665039)
Finished Training
Total time taken: 102.8571548461914
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.662
[4,     1] loss: 0.641
[5,     1] loss: 0.625
[6,     1] loss: 0.610
[7,     1] loss: 0.594
[8,     1] loss: 0.575
[9,     1] loss: 0.556
[10,     1] loss: 0.537
[11,     1] loss: 0.515
[12,     1] loss: 0.494
[13,     1] loss: 0.470
[14,     1] loss: 0.448
[15,     1] loss: 0.424
[16,     1] loss: 0.399
[17,     1] loss: 0.374
[18,     1] loss: 0.353
[19,     1] loss: 0.332
[20,     1] loss: 0.303
[21,     1] loss: 0.283
[22,     1] loss: 0.266
[23,     1] loss: 0.247
[24,     1] loss: 0.235
[25,     1] loss: 0.217
[26,     1] loss: 0.204
[27,     1] loss: 0.189
[28,     1] loss: 0.176
[29,     1] loss: 0.164
[30,     1] loss: 0.155
[31,     1] loss: 0.148
[32,     1] loss: 0.138
[33,     1] loss: 0.130
[34,     1] loss: 0.123
[35,     1] loss: 0.118
[36,     1] loss: 0.112
[37,     1] loss: 0.106
[38,     1] loss: 0.102
[39,     1] loss: 0.097
[40,     1] loss: 0.093
[41,     1] loss: 0.091
[42,     1] loss: 0.086
[43,     1] loss: 0.085
[44,     1] loss: 0.084
[45,     1] loss: 0.081
[46,     1] loss: 0.080
[47,     1] loss: 0.079
[48,     1] loss: 0.078
[49,     1] loss: 0.077
[50,     1] loss: 0.075
[51,     1] loss: 0.074
[52,     1] loss: 0.075
[53,     1] loss: 0.073
[54,     1] loss: 0.075
[55,     1] loss: 0.074
[56,     1] loss: 0.074
[57,     1] loss: 0.073
[58,     1] loss: 0.071
[59,     1] loss: 0.073
[60,     1] loss: 0.071
[61,     1] loss: 0.068
[62,     1] loss: 0.069
[63,     1] loss: 0.065
[64,     1] loss: 0.065
[65,     1] loss: 0.063
[66,     1] loss: 0.061
[67,     1] loss: 0.056
[68,     1] loss: 0.055
[69,     1] loss: 0.072
[70,     1] loss: 0.484
[71,     1] loss: 0.530
[72,     1] loss: 0.506
[73,     1] loss: 0.449
[74,     1] loss: 0.459
[75,     1] loss: 0.432
[76,     1] loss: 0.411
[77,     1] loss: 0.407
[78,     1] loss: 0.412
Early stopping applied (best metric=0.37773022055625916)
Finished Training
Total time taken: 43.10548162460327
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.629
[7,     1] loss: 0.611
[8,     1] loss: 0.591
[9,     1] loss: 0.570
[10,     1] loss: 0.546
[11,     1] loss: 0.522
[12,     1] loss: 0.495
[13,     1] loss: 0.468
[14,     1] loss: 0.438
[15,     1] loss: 0.409
[16,     1] loss: 0.381
[17,     1] loss: 0.349
[18,     1] loss: 0.323
[19,     1] loss: 0.297
[20,     1] loss: 0.274
[21,     1] loss: 0.257
[22,     1] loss: 0.234
[23,     1] loss: 0.213
[24,     1] loss: 0.195
[25,     1] loss: 0.179
[26,     1] loss: 0.165
[27,     1] loss: 0.151
[28,     1] loss: 0.137
[29,     1] loss: 0.130
[30,     1] loss: 0.121
[31,     1] loss: 0.115
[32,     1] loss: 0.108
[33,     1] loss: 0.102
[34,     1] loss: 0.098
[35,     1] loss: 0.093
[36,     1] loss: 0.090
[37,     1] loss: 0.088
[38,     1] loss: 0.087
[39,     1] loss: 0.085
[40,     1] loss: 0.083
[41,     1] loss: 0.082
[42,     1] loss: 0.082
[43,     1] loss: 0.081
[44,     1] loss: 0.082
[45,     1] loss: 0.080
[46,     1] loss: 0.082
[47,     1] loss: 0.081
[48,     1] loss: 0.081
[49,     1] loss: 0.081
[50,     1] loss: 0.083
[51,     1] loss: 0.082
[52,     1] loss: 0.083
[53,     1] loss: 0.082
[54,     1] loss: 0.082
[55,     1] loss: 0.082
[56,     1] loss: 0.080
[57,     1] loss: 0.081
[58,     1] loss: 0.080
[59,     1] loss: 0.078
[60,     1] loss: 0.077
[61,     1] loss: 0.073
[62,     1] loss: 0.072
[63,     1] loss: 0.112
[64,     1] loss: 0.277
[65,     1] loss: 0.639
[66,     1] loss: 0.485
[67,     1] loss: 0.492
[68,     1] loss: 0.476
[69,     1] loss: 0.421
[70,     1] loss: 0.419
[71,     1] loss: 0.425
[72,     1] loss: 0.422
[73,     1] loss: 0.426
[74,     1] loss: 0.419
[75,     1] loss: 0.408
[76,     1] loss: 0.390
[77,     1] loss: 0.370
[78,     1] loss: 0.355
[79,     1] loss: 0.337
[80,     1] loss: 0.317
[81,     1] loss: 0.304
[82,     1] loss: 0.289
[83,     1] loss: 0.272
[84,     1] loss: 0.257
[85,     1] loss: 0.240
[86,     1] loss: 0.218
[87,     1] loss: 0.199
[88,     1] loss: 0.179
[89,     1] loss: 0.161
[90,     1] loss: 0.186
[91,     1] loss: 0.137
[92,     1] loss: 0.125
[93,     1] loss: 0.117
[94,     1] loss: 0.110
[95,     1] loss: 0.101
[96,     1] loss: 0.097
[97,     1] loss: 0.092
[98,     1] loss: 0.088
[99,     1] loss: 0.084
[100,     1] loss: 0.081
[101,     1] loss: 0.078
[102,     1] loss: 0.077
[103,     1] loss: 0.073
[104,     1] loss: 0.076
[105,     1] loss: 0.077
[106,     1] loss: 0.078
[107,     1] loss: 0.077
[108,     1] loss: 0.078
Early stopping applied (best metric=0.3553616404533386)
Finished Training
Total time taken: 62.198514223098755
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.672
[3,     1] loss: 0.650
[4,     1] loss: 0.631
[5,     1] loss: 0.612
[6,     1] loss: 0.591
[7,     1] loss: 0.573
[8,     1] loss: 0.549
[9,     1] loss: 0.528
[10,     1] loss: 0.501
[11,     1] loss: 0.478
[12,     1] loss: 0.456
[13,     1] loss: 0.429
[14,     1] loss: 0.406
[15,     1] loss: 0.384
[16,     1] loss: 0.364
[17,     1] loss: 0.345
[18,     1] loss: 0.326
[19,     1] loss: 0.315
[20,     1] loss: 0.298
[21,     1] loss: 0.281
[22,     1] loss: 0.269
[23,     1] loss: 0.256
[24,     1] loss: 0.241
[25,     1] loss: 0.237
[26,     1] loss: 0.225
[27,     1] loss: 0.216
[28,     1] loss: 0.214
[29,     1] loss: 0.201
[30,     1] loss: 0.195
[31,     1] loss: 0.189
[32,     1] loss: 0.180
[33,     1] loss: 0.173
[34,     1] loss: 0.167
[35,     1] loss: 0.158
[36,     1] loss: 0.150
[37,     1] loss: 0.144
[38,     1] loss: 0.138
[39,     1] loss: 0.134
[40,     1] loss: 0.129
[41,     1] loss: 0.123
[42,     1] loss: 0.116
[43,     1] loss: 0.115
[44,     1] loss: 0.109
[45,     1] loss: 0.106
[46,     1] loss: 0.103
[47,     1] loss: 0.103
[48,     1] loss: 0.100
[49,     1] loss: 0.097
[50,     1] loss: 0.095
[51,     1] loss: 0.094
[52,     1] loss: 0.092
[53,     1] loss: 0.091
[54,     1] loss: 0.088
[55,     1] loss: 0.088
[56,     1] loss: 0.086
[57,     1] loss: 0.087
[58,     1] loss: 0.084
[59,     1] loss: 0.084
[60,     1] loss: 0.083
[61,     1] loss: 0.081
[62,     1] loss: 0.082
[63,     1] loss: 0.081
[64,     1] loss: 0.079
[65,     1] loss: 0.079
[66,     1] loss: 0.079
[67,     1] loss: 0.076
[68,     1] loss: 0.074
[69,     1] loss: 0.071
[70,     1] loss: 0.070
Early stopping applied (best metric=0.38284003734588623)
Finished Training
Total time taken: 41.10946035385132
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.669
[4,     1] loss: 0.656
[5,     1] loss: 0.641
[6,     1] loss: 0.626
[7,     1] loss: 0.605
[8,     1] loss: 0.583
[9,     1] loss: 0.560
[10,     1] loss: 0.536
[11,     1] loss: 0.504
[12,     1] loss: 0.476
[13,     1] loss: 0.443
[14,     1] loss: 0.410
[15,     1] loss: 0.374
[16,     1] loss: 0.346
[17,     1] loss: 0.311
[18,     1] loss: 0.285
[19,     1] loss: 0.261
[20,     1] loss: 0.237
[21,     1] loss: 0.219
[22,     1] loss: 0.201
[23,     1] loss: 0.180
[24,     1] loss: 0.169
[25,     1] loss: 0.153
[26,     1] loss: 0.141
[27,     1] loss: 0.127
[28,     1] loss: 0.121
[29,     1] loss: 0.110
[30,     1] loss: 0.105
[31,     1] loss: 0.099
[32,     1] loss: 0.094
[33,     1] loss: 0.093
[34,     1] loss: 0.088
[35,     1] loss: 0.086
[36,     1] loss: 0.082
[37,     1] loss: 0.082
[38,     1] loss: 0.079
[39,     1] loss: 0.079
[40,     1] loss: 0.079
[41,     1] loss: 0.077
[42,     1] loss: 0.077
[43,     1] loss: 0.076
[44,     1] loss: 0.076
[45,     1] loss: 0.078
[46,     1] loss: 0.076
[47,     1] loss: 0.076
[48,     1] loss: 0.076
[49,     1] loss: 0.076
[50,     1] loss: 0.076
[51,     1] loss: 0.076
[52,     1] loss: 0.076
[53,     1] loss: 0.074
[54,     1] loss: 0.073
[55,     1] loss: 0.071
[56,     1] loss: 0.070
[57,     1] loss: 0.067
[58,     1] loss: 0.065
[59,     1] loss: 0.063
[60,     1] loss: 0.062
[61,     1] loss: 0.063
[62,     1] loss: 0.276
[63,     1] loss: 0.366
[64,     1] loss: 0.358
[65,     1] loss: 0.274
[66,     1] loss: 0.245
[67,     1] loss: 0.246
[68,     1] loss: 0.238
[69,     1] loss: 0.227
[70,     1] loss: 0.215
[71,     1] loss: 0.208
[72,     1] loss: 0.195
[73,     1] loss: 0.182
[74,     1] loss: 0.171
[75,     1] loss: 0.161
[76,     1] loss: 0.152
[77,     1] loss: 0.145
[78,     1] loss: 0.136
[79,     1] loss: 0.127
[80,     1] loss: 0.123
[81,     1] loss: 0.115
[82,     1] loss: 0.110
[83,     1] loss: 0.104
[84,     1] loss: 0.096
[85,     1] loss: 0.091
[86,     1] loss: 0.086
[87,     1] loss: 0.083
[88,     1] loss: 0.076
[89,     1] loss: 0.074
[90,     1] loss: 0.069
[91,     1] loss: 0.068
[92,     1] loss: 0.064
[93,     1] loss: 0.064
[94,     1] loss: 0.064
[95,     1] loss: 0.064
[96,     1] loss: 0.060
[97,     1] loss: 0.061
[98,     1] loss: 0.062
[99,     1] loss: 0.063
[100,     1] loss: 0.062
[101,     1] loss: 0.064
[102,     1] loss: 0.061
[103,     1] loss: 0.062
[104,     1] loss: 0.063
[105,     1] loss: 0.062
[106,     1] loss: 0.065
[107,     1] loss: 0.065
[108,     1] loss: 0.064
[109,     1] loss: 0.065
[110,     1] loss: 0.065
[111,     1] loss: 0.063
[112,     1] loss: 0.065
[113,     1] loss: 0.064
[114,     1] loss: 0.065
[115,     1] loss: 0.062
[116,     1] loss: 0.066
[117,     1] loss: 0.066
[118,     1] loss: 0.065
Early stopping applied (best metric=0.3984425663948059)
Finished Training
Total time taken: 68.53076887130737
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.695
[3,     1] loss: 0.687
[4,     1] loss: 0.679
[5,     1] loss: 0.672
[6,     1] loss: 0.664
[7,     1] loss: 0.653
[8,     1] loss: 0.642
[9,     1] loss: 0.629
[10,     1] loss: 0.616
[11,     1] loss: 0.599
[12,     1] loss: 0.581
[13,     1] loss: 0.562
[14,     1] loss: 0.541
[15,     1] loss: 0.525
[16,     1] loss: 0.503
[17,     1] loss: 0.483
[18,     1] loss: 0.468
[19,     1] loss: 0.463
[20,     1] loss: 0.447
[21,     1] loss: 0.426
[22,     1] loss: 0.416
[23,     1] loss: 0.399
[24,     1] loss: 0.389
[25,     1] loss: 0.374
[26,     1] loss: 0.362
[27,     1] loss: 0.352
[28,     1] loss: 0.343
[29,     1] loss: 0.334
[30,     1] loss: 0.321
[31,     1] loss: 0.311
[32,     1] loss: 0.300
[33,     1] loss: 0.289
[34,     1] loss: 0.275
[35,     1] loss: 0.264
[36,     1] loss: 0.253
[37,     1] loss: 0.239
[38,     1] loss: 0.223
[39,     1] loss: 0.209
[40,     1] loss: 0.191
[41,     1] loss: 0.181
[42,     1] loss: 0.165
[43,     1] loss: 0.151
[44,     1] loss: 0.138
[45,     1] loss: 0.125
[46,     1] loss: 0.116
[47,     1] loss: 0.105
[48,     1] loss: 0.096
[49,     1] loss: 0.087
[50,     1] loss: 0.082
[51,     1] loss: 0.074
[52,     1] loss: 0.067
[53,     1] loss: 0.061
[54,     1] loss: 0.057
[55,     1] loss: 0.053
[56,     1] loss: 0.050
[57,     1] loss: 0.045
[58,     1] loss: 0.042
[59,     1] loss: 0.041
[60,     1] loss: 0.039
[61,     1] loss: 0.038
[62,     1] loss: 0.036
[63,     1] loss: 0.038
[64,     1] loss: 0.035
[65,     1] loss: 0.035
[66,     1] loss: 0.032
[67,     1] loss: 0.033
[68,     1] loss: 0.033
[69,     1] loss: 0.033
[70,     1] loss: 0.033
[71,     1] loss: 0.035
[72,     1] loss: 0.034
[73,     1] loss: 0.034
[74,     1] loss: 0.035
[75,     1] loss: 0.034
[76,     1] loss: 0.037
[77,     1] loss: 0.036
[78,     1] loss: 0.037
[79,     1] loss: 0.038
[80,     1] loss: 0.036
[81,     1] loss: 0.038
[82,     1] loss: 0.038
[83,     1] loss: 0.038
[84,     1] loss: 0.037
[85,     1] loss: 0.037
[86,     1] loss: 0.038
Early stopping applied (best metric=0.4490039646625519)
Finished Training
Total time taken: 51.303945779800415
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.644
[4,     1] loss: 0.619
[5,     1] loss: 0.598
[6,     1] loss: 0.574
[7,     1] loss: 0.553
[8,     1] loss: 0.525
[9,     1] loss: 0.503
[10,     1] loss: 0.475
[11,     1] loss: 0.445
[12,     1] loss: 0.421
[13,     1] loss: 0.389
[14,     1] loss: 0.369
[15,     1] loss: 0.342
[16,     1] loss: 0.316
[17,     1] loss: 0.292
[18,     1] loss: 0.269
[19,     1] loss: 0.250
[20,     1] loss: 0.235
[21,     1] loss: 0.218
[22,     1] loss: 0.203
[23,     1] loss: 0.188
[24,     1] loss: 0.173
[25,     1] loss: 0.160
[26,     1] loss: 0.152
[27,     1] loss: 0.136
[28,     1] loss: 0.130
[29,     1] loss: 0.121
[30,     1] loss: 0.113
[31,     1] loss: 0.104
[32,     1] loss: 0.103
[33,     1] loss: 0.097
[34,     1] loss: 0.095
[35,     1] loss: 0.091
[36,     1] loss: 0.089
[37,     1] loss: 0.086
[38,     1] loss: 0.081
[39,     1] loss: 0.081
[40,     1] loss: 0.081
[41,     1] loss: 0.080
[42,     1] loss: 0.078
[43,     1] loss: 0.077
[44,     1] loss: 0.076
[45,     1] loss: 0.074
[46,     1] loss: 0.075
[47,     1] loss: 0.074
[48,     1] loss: 0.074
[49,     1] loss: 0.075
[50,     1] loss: 0.074
[51,     1] loss: 0.072
[52,     1] loss: 0.073
[53,     1] loss: 0.073
[54,     1] loss: 0.073
[55,     1] loss: 0.072
[56,     1] loss: 0.072
[57,     1] loss: 0.071
[58,     1] loss: 0.071
[59,     1] loss: 0.069
[60,     1] loss: 0.070
Early stopping applied (best metric=0.4772051274776459)
Finished Training
Total time taken: 36.10542631149292
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.664
[4,     1] loss: 0.649
[5,     1] loss: 0.632
[6,     1] loss: 0.618
[7,     1] loss: 0.599
[8,     1] loss: 0.582
[9,     1] loss: 0.563
[10,     1] loss: 0.544
[11,     1] loss: 0.518
[12,     1] loss: 0.498
[13,     1] loss: 0.477
[14,     1] loss: 0.459
[15,     1] loss: 0.441
[16,     1] loss: 0.423
[17,     1] loss: 0.407
[18,     1] loss: 0.398
[19,     1] loss: 0.396
[20,     1] loss: 0.374
[21,     1] loss: 0.376
[22,     1] loss: 0.364
[23,     1] loss: 0.362
[24,     1] loss: 0.356
[25,     1] loss: 0.351
[26,     1] loss: 0.347
[27,     1] loss: 0.344
[28,     1] loss: 0.342
[29,     1] loss: 0.340
[30,     1] loss: 0.336
[31,     1] loss: 0.335
[32,     1] loss: 0.333
[33,     1] loss: 0.331
[34,     1] loss: 0.329
[35,     1] loss: 0.327
[36,     1] loss: 0.325
[37,     1] loss: 0.322
[38,     1] loss: 0.318
[39,     1] loss: 0.315
[40,     1] loss: 0.309
[41,     1] loss: 0.305
[42,     1] loss: 0.301
[43,     1] loss: 0.295
[44,     1] loss: 0.291
[45,     1] loss: 0.283
[46,     1] loss: 0.278
[47,     1] loss: 0.271
[48,     1] loss: 0.263
[49,     1] loss: 0.263
[50,     1] loss: 0.248
[51,     1] loss: 0.242
[52,     1] loss: 0.241
[53,     1] loss: 0.249
[54,     1] loss: 0.236
[55,     1] loss: 0.219
[56,     1] loss: 0.208
[57,     1] loss: 0.198
[58,     1] loss: 0.185
[59,     1] loss: 0.175
[60,     1] loss: 0.167
[61,     1] loss: 0.156
[62,     1] loss: 0.145
[63,     1] loss: 0.138
[64,     1] loss: 0.128
[65,     1] loss: 0.121
[66,     1] loss: 0.112
[67,     1] loss: 0.103
[68,     1] loss: 0.096
[69,     1] loss: 0.092
[70,     1] loss: 0.085
[71,     1] loss: 0.081
[72,     1] loss: 0.077
[73,     1] loss: 0.072
[74,     1] loss: 0.071
[75,     1] loss: 0.069
[76,     1] loss: 0.064
[77,     1] loss: 0.064
[78,     1] loss: 0.061
[79,     1] loss: 0.061
[80,     1] loss: 0.060
[81,     1] loss: 0.059
[82,     1] loss: 0.059
[83,     1] loss: 0.058
[84,     1] loss: 0.057
[85,     1] loss: 0.058
[86,     1] loss: 0.057
[87,     1] loss: 0.058
[88,     1] loss: 0.057
[89,     1] loss: 0.059
[90,     1] loss: 0.058
[91,     1] loss: 0.059
[92,     1] loss: 0.058
[93,     1] loss: 0.060
[94,     1] loss: 0.059
[95,     1] loss: 0.061
[96,     1] loss: 0.060
[97,     1] loss: 0.061
[98,     1] loss: 0.060
[99,     1] loss: 0.060
[100,     1] loss: 0.061
[101,     1] loss: 0.060
[102,     1] loss: 0.060
[103,     1] loss: 0.060
[104,     1] loss: 0.061
[105,     1] loss: 0.060
[106,     1] loss: 0.061
[107,     1] loss: 0.062
[108,     1] loss: 0.063
[109,     1] loss: 0.061
[110,     1] loss: 0.062
[111,     1] loss: 0.063
Early stopping applied (best metric=0.34671294689178467)
Finished Training
Total time taken: 66.68615484237671
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.692
[3,     1] loss: 0.673
[4,     1] loss: 0.658
[5,     1] loss: 0.643
[6,     1] loss: 0.627
[7,     1] loss: 0.610
[8,     1] loss: 0.592
[9,     1] loss: 0.573
[10,     1] loss: 0.552
[11,     1] loss: 0.530
[12,     1] loss: 0.506
[13,     1] loss: 0.479
[14,     1] loss: 0.450
[15,     1] loss: 0.424
[16,     1] loss: 0.399
[17,     1] loss: 0.373
[18,     1] loss: 0.349
[19,     1] loss: 0.327
[20,     1] loss: 0.302
[21,     1] loss: 0.281
[22,     1] loss: 0.262
[23,     1] loss: 0.242
[24,     1] loss: 0.223
[25,     1] loss: 0.207
[26,     1] loss: 0.192
[27,     1] loss: 0.178
[28,     1] loss: 0.168
[29,     1] loss: 0.169
[30,     1] loss: 0.156
[31,     1] loss: 0.138
[32,     1] loss: 0.147
[33,     1] loss: 0.131
[34,     1] loss: 0.128
[35,     1] loss: 0.119
[36,     1] loss: 0.117
[37,     1] loss: 0.113
[38,     1] loss: 0.109
[39,     1] loss: 0.106
[40,     1] loss: 0.104
[41,     1] loss: 0.101
[42,     1] loss: 0.100
[43,     1] loss: 0.098
[44,     1] loss: 0.098
[45,     1] loss: 0.097
[46,     1] loss: 0.095
[47,     1] loss: 0.094
[48,     1] loss: 0.094
[49,     1] loss: 0.093
[50,     1] loss: 0.096
[51,     1] loss: 0.094
[52,     1] loss: 0.093
[53,     1] loss: 0.093
[54,     1] loss: 0.091
[55,     1] loss: 0.091
[56,     1] loss: 0.092
[57,     1] loss: 0.091
[58,     1] loss: 0.091
[59,     1] loss: 0.088
[60,     1] loss: 0.088
[61,     1] loss: 0.087
[62,     1] loss: 0.080
[63,     1] loss: 0.075
[64,     1] loss: 0.072
[65,     1] loss: 0.073
[66,     1] loss: 0.208
[67,     1] loss: 0.794
[68,     1] loss: 0.603
[69,     1] loss: 0.418
[70,     1] loss: 0.411
[71,     1] loss: 0.399
[72,     1] loss: 0.379
[73,     1] loss: 0.355
[74,     1] loss: 0.357
[75,     1] loss: 0.357
[76,     1] loss: 0.352
[77,     1] loss: 0.339
[78,     1] loss: 0.327
[79,     1] loss: 0.312
[80,     1] loss: 0.299
[81,     1] loss: 0.283
[82,     1] loss: 0.269
[83,     1] loss: 0.258
[84,     1] loss: 0.243
[85,     1] loss: 0.232
[86,     1] loss: 0.217
[87,     1] loss: 0.206
[88,     1] loss: 0.194
[89,     1] loss: 0.184
[90,     1] loss: 0.173
[91,     1] loss: 0.162
[92,     1] loss: 0.154
[93,     1] loss: 0.146
[94,     1] loss: 0.135
[95,     1] loss: 0.128
[96,     1] loss: 0.122
[97,     1] loss: 0.113
[98,     1] loss: 0.110
[99,     1] loss: 0.104
[100,     1] loss: 0.099
[101,     1] loss: 0.096
[102,     1] loss: 0.095
[103,     1] loss: 0.093
[104,     1] loss: 0.089
[105,     1] loss: 0.088
[106,     1] loss: 0.087
[107,     1] loss: 0.086
[108,     1] loss: 0.086
[109,     1] loss: 0.085
[110,     1] loss: 0.083
[111,     1] loss: 0.084
[112,     1] loss: 0.085
[113,     1] loss: 0.086
Early stopping applied (best metric=0.37667086720466614)
Finished Training
Total time taken: 67.13376116752625
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.677
[3,     1] loss: 0.661
[4,     1] loss: 0.649
[5,     1] loss: 0.633
[6,     1] loss: 0.619
[7,     1] loss: 0.603
[8,     1] loss: 0.587
[9,     1] loss: 0.569
[10,     1] loss: 0.551
[11,     1] loss: 0.532
[12,     1] loss: 0.509
[13,     1] loss: 0.496
[14,     1] loss: 0.478
[15,     1] loss: 0.465
[16,     1] loss: 0.460
[17,     1] loss: 0.432
[18,     1] loss: 0.433
[19,     1] loss: 0.410
[20,     1] loss: 0.402
[21,     1] loss: 0.387
[22,     1] loss: 0.385
[23,     1] loss: 0.372
[24,     1] loss: 0.368
[25,     1] loss: 0.360
[26,     1] loss: 0.357
[27,     1] loss: 0.353
[28,     1] loss: 0.344
[29,     1] loss: 0.341
[30,     1] loss: 0.343
[31,     1] loss: 0.336
[32,     1] loss: 0.335
[33,     1] loss: 0.327
[34,     1] loss: 0.322
[35,     1] loss: 0.319
[36,     1] loss: 0.311
[37,     1] loss: 0.304
[38,     1] loss: 0.298
[39,     1] loss: 0.291
[40,     1] loss: 0.282
[41,     1] loss: 0.273
[42,     1] loss: 0.266
[43,     1] loss: 0.254
[44,     1] loss: 0.242
[45,     1] loss: 0.231
[46,     1] loss: 0.219
[47,     1] loss: 0.205
[48,     1] loss: 0.192
[49,     1] loss: 0.179
[50,     1] loss: 0.165
[51,     1] loss: 0.150
[52,     1] loss: 0.139
[53,     1] loss: 0.127
[54,     1] loss: 0.115
[55,     1] loss: 0.105
[56,     1] loss: 0.095
[57,     1] loss: 0.085
[58,     1] loss: 0.077
[59,     1] loss: 0.071
[60,     1] loss: 0.067
[61,     1] loss: 0.060
[62,     1] loss: 0.056
[63,     1] loss: 0.050
[64,     1] loss: 0.048
[65,     1] loss: 0.044
[66,     1] loss: 0.042
[67,     1] loss: 0.039
[68,     1] loss: 0.038
[69,     1] loss: 0.035
[70,     1] loss: 0.036
[71,     1] loss: 0.034
[72,     1] loss: 0.035
[73,     1] loss: 0.034
[74,     1] loss: 0.034
[75,     1] loss: 0.034
[76,     1] loss: 0.035
[77,     1] loss: 0.036
[78,     1] loss: 0.035
[79,     1] loss: 0.036
[80,     1] loss: 0.038
[81,     1] loss: 0.037
[82,     1] loss: 0.038
[83,     1] loss: 0.038
[84,     1] loss: 0.040
[85,     1] loss: 0.041
[86,     1] loss: 0.039
[87,     1] loss: 0.041
[88,     1] loss: 0.041
[89,     1] loss: 0.040
[90,     1] loss: 0.040
Early stopping applied (best metric=0.39822977781295776)
Finished Training
Total time taken: 53.94966411590576
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.660
[4,     1] loss: 0.639
[5,     1] loss: 0.621
[6,     1] loss: 0.597
[7,     1] loss: 0.577
[8,     1] loss: 0.548
[9,     1] loss: 0.522
[10,     1] loss: 0.491
[11,     1] loss: 0.464
[12,     1] loss: 0.429
[13,     1] loss: 0.396
[14,     1] loss: 0.364
[15,     1] loss: 0.336
[16,     1] loss: 0.310
[17,     1] loss: 0.286
[18,     1] loss: 0.266
[19,     1] loss: 0.246
[20,     1] loss: 0.227
[21,     1] loss: 0.210
[22,     1] loss: 0.194
[23,     1] loss: 0.179
[24,     1] loss: 0.173
[25,     1] loss: 0.164
[26,     1] loss: 0.152
[27,     1] loss: 0.134
[28,     1] loss: 0.129
[29,     1] loss: 0.120
[30,     1] loss: 0.116
[31,     1] loss: 0.108
[32,     1] loss: 0.105
[33,     1] loss: 0.100
[34,     1] loss: 0.097
[35,     1] loss: 0.095
[36,     1] loss: 0.093
[37,     1] loss: 0.090
[38,     1] loss: 0.090
[39,     1] loss: 0.090
[40,     1] loss: 0.088
[41,     1] loss: 0.086
[42,     1] loss: 0.087
[43,     1] loss: 0.085
[44,     1] loss: 0.085
[45,     1] loss: 0.086
[46,     1] loss: 0.086
[47,     1] loss: 0.085
[48,     1] loss: 0.084
[49,     1] loss: 0.084
[50,     1] loss: 0.084
[51,     1] loss: 0.085
[52,     1] loss: 0.085
[53,     1] loss: 0.085
[54,     1] loss: 0.083
[55,     1] loss: 0.082
[56,     1] loss: 0.083
[57,     1] loss: 0.081
[58,     1] loss: 0.081
[59,     1] loss: 0.079
[60,     1] loss: 0.077
[61,     1] loss: 0.073
[62,     1] loss: 0.083
[63,     1] loss: 0.327
Early stopping applied (best metric=0.39266952872276306)
Finished Training
Total time taken: 38.95643711090088
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.676
[3,     1] loss: 0.647
[4,     1] loss: 0.619
[5,     1] loss: 0.588
[6,     1] loss: 0.559
[7,     1] loss: 0.532
[8,     1] loss: 0.498
[9,     1] loss: 0.464
[10,     1] loss: 0.441
[11,     1] loss: 0.416
[12,     1] loss: 0.396
[13,     1] loss: 0.380
[14,     1] loss: 0.362
[15,     1] loss: 0.350
[16,     1] loss: 0.338
[17,     1] loss: 0.328
[18,     1] loss: 0.314
[19,     1] loss: 0.304
[20,     1] loss: 0.295
[21,     1] loss: 0.285
[22,     1] loss: 0.277
[23,     1] loss: 0.267
[24,     1] loss: 0.258
[25,     1] loss: 0.249
[26,     1] loss: 0.239
[27,     1] loss: 0.228
[28,     1] loss: 0.220
[29,     1] loss: 0.209
[30,     1] loss: 0.201
[31,     1] loss: 0.192
[32,     1] loss: 0.184
[33,     1] loss: 0.175
[34,     1] loss: 0.167
[35,     1] loss: 0.160
[36,     1] loss: 0.149
[37,     1] loss: 0.144
[38,     1] loss: 0.138
[39,     1] loss: 0.132
[40,     1] loss: 0.127
[41,     1] loss: 0.120
[42,     1] loss: 0.115
[43,     1] loss: 0.112
[44,     1] loss: 0.110
[45,     1] loss: 0.106
[46,     1] loss: 0.103
[47,     1] loss: 0.101
[48,     1] loss: 0.101
[49,     1] loss: 0.097
[50,     1] loss: 0.096
[51,     1] loss: 0.095
[52,     1] loss: 0.094
[53,     1] loss: 0.093
[54,     1] loss: 0.093
[55,     1] loss: 0.095
[56,     1] loss: 0.092
[57,     1] loss: 0.092
[58,     1] loss: 0.091
[59,     1] loss: 0.092
[60,     1] loss: 0.090
[61,     1] loss: 0.091
Early stopping applied (best metric=0.4403842091560364)
Finished Training
Total time taken: 39.451454162597656
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.666
[4,     1] loss: 0.652
[5,     1] loss: 0.638
[6,     1] loss: 0.622
[7,     1] loss: 0.606
[8,     1] loss: 0.587
[9,     1] loss: 0.567
[10,     1] loss: 0.542
[11,     1] loss: 0.520
[12,     1] loss: 0.492
[13,     1] loss: 0.463
[14,     1] loss: 0.436
[15,     1] loss: 0.406
[16,     1] loss: 0.373
[17,     1] loss: 0.348
[18,     1] loss: 0.314
[19,     1] loss: 0.284
[20,     1] loss: 0.263
[21,     1] loss: 0.237
[22,     1] loss: 0.214
[23,     1] loss: 0.196
[24,     1] loss: 0.179
[25,     1] loss: 0.163
[26,     1] loss: 0.150
[27,     1] loss: 0.136
[28,     1] loss: 0.129
[29,     1] loss: 0.118
[30,     1] loss: 0.111
[31,     1] loss: 0.104
[32,     1] loss: 0.096
[33,     1] loss: 0.094
[34,     1] loss: 0.087
[35,     1] loss: 0.083
[36,     1] loss: 0.081
[37,     1] loss: 0.078
[38,     1] loss: 0.076
[39,     1] loss: 0.076
[40,     1] loss: 0.074
[41,     1] loss: 0.074
[42,     1] loss: 0.073
[43,     1] loss: 0.074
[44,     1] loss: 0.073
[45,     1] loss: 0.074
[46,     1] loss: 0.073
[47,     1] loss: 0.073
[48,     1] loss: 0.073
[49,     1] loss: 0.074
[50,     1] loss: 0.074
[51,     1] loss: 0.071
[52,     1] loss: 0.073
[53,     1] loss: 0.074
[54,     1] loss: 0.074
[55,     1] loss: 0.072
[56,     1] loss: 0.071
[57,     1] loss: 0.071
[58,     1] loss: 0.071
[59,     1] loss: 0.070
[60,     1] loss: 0.070
[61,     1] loss: 0.068
[62,     1] loss: 0.068
[63,     1] loss: 0.066
[64,     1] loss: 0.064
[65,     1] loss: 0.063
[66,     1] loss: 0.060
[67,     1] loss: 0.133
[68,     1] loss: 0.994
Early stopping applied (best metric=0.4062058925628662)
Finished Training
Total time taken: 44.79250144958496
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.656
[4,     1] loss: 0.638
[5,     1] loss: 0.619
[6,     1] loss: 0.595
[7,     1] loss: 0.567
[8,     1] loss: 0.541
[9,     1] loss: 0.508
[10,     1] loss: 0.478
[11,     1] loss: 0.447
[12,     1] loss: 0.421
[13,     1] loss: 0.400
[14,     1] loss: 0.384
[15,     1] loss: 0.375
[16,     1] loss: 0.369
[17,     1] loss: 0.364
[18,     1] loss: 0.362
[19,     1] loss: 0.359
[20,     1] loss: 0.358
[21,     1] loss: 0.356
[22,     1] loss: 0.355
[23,     1] loss: 0.354
[24,     1] loss: 0.354
[25,     1] loss: 0.353
[26,     1] loss: 0.354
[27,     1] loss: 0.354
[28,     1] loss: 0.354
[29,     1] loss: 0.354
[30,     1] loss: 0.354
[31,     1] loss: 0.355
[32,     1] loss: 0.356
[33,     1] loss: 0.356
[34,     1] loss: 0.356
[35,     1] loss: 0.356
[36,     1] loss: 0.356
[37,     1] loss: 0.356
[38,     1] loss: 0.357
[39,     1] loss: 0.357
[40,     1] loss: 0.356
[41,     1] loss: 0.356
[42,     1] loss: 0.356
[43,     1] loss: 0.355
[44,     1] loss: 0.355
[45,     1] loss: 0.355
[46,     1] loss: 0.355
[47,     1] loss: 0.356
[48,     1] loss: 0.356
[49,     1] loss: 0.356
[50,     1] loss: 0.357
[51,     1] loss: 0.359
[52,     1] loss: 0.364
[53,     1] loss: 0.369
[54,     1] loss: 0.365
[55,     1] loss: 0.359
[56,     1] loss: 0.358
[57,     1] loss: 0.357
[58,     1] loss: 0.355
[59,     1] loss: 0.354
[60,     1] loss: 0.359
Early stopping applied (best metric=0.4431128203868866)
Finished Training
Total time taken: 38.416443824768066
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.682
[3,     1] loss: 0.661
[4,     1] loss: 0.643
[5,     1] loss: 0.628
[6,     1] loss: 0.611
[7,     1] loss: 0.595
[8,     1] loss: 0.575
[9,     1] loss: 0.554
[10,     1] loss: 0.533
[11,     1] loss: 0.510
[12,     1] loss: 0.485
[13,     1] loss: 0.458
[14,     1] loss: 0.430
[15,     1] loss: 0.402
[16,     1] loss: 0.376
[17,     1] loss: 0.347
[18,     1] loss: 0.324
[19,     1] loss: 0.300
[20,     1] loss: 0.277
[21,     1] loss: 0.259
[22,     1] loss: 0.246
[23,     1] loss: 0.222
[24,     1] loss: 0.206
[25,     1] loss: 0.192
[26,     1] loss: 0.181
[27,     1] loss: 0.168
[28,     1] loss: 0.156
[29,     1] loss: 0.148
[30,     1] loss: 0.139
[31,     1] loss: 0.133
[32,     1] loss: 0.126
[33,     1] loss: 0.120
[34,     1] loss: 0.116
[35,     1] loss: 0.115
[36,     1] loss: 0.114
[37,     1] loss: 0.107
[38,     1] loss: 0.106
[39,     1] loss: 0.105
[40,     1] loss: 0.103
[41,     1] loss: 0.102
[42,     1] loss: 0.102
[43,     1] loss: 0.098
[44,     1] loss: 0.098
[45,     1] loss: 0.098
[46,     1] loss: 0.096
[47,     1] loss: 0.097
[48,     1] loss: 0.096
[49,     1] loss: 0.095
[50,     1] loss: 0.098
[51,     1] loss: 0.096
[52,     1] loss: 0.095
[53,     1] loss: 0.096
[54,     1] loss: 0.096
[55,     1] loss: 0.094
[56,     1] loss: 0.093
[57,     1] loss: 0.095
[58,     1] loss: 0.095
[59,     1] loss: 0.094
[60,     1] loss: 0.092
[61,     1] loss: 0.091
[62,     1] loss: 0.093
[63,     1] loss: 0.092
[64,     1] loss: 0.092
[65,     1] loss: 0.099
[66,     1] loss: 0.368
[67,     1] loss: 0.416
[68,     1] loss: 0.303
[69,     1] loss: 0.325
Early stopping applied (best metric=0.41877079010009766)
Finished Training
Total time taken: 44.2264986038208
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.687
[3,     1] loss: 0.663
[4,     1] loss: 0.640
[5,     1] loss: 0.617
[6,     1] loss: 0.593
[7,     1] loss: 0.570
[8,     1] loss: 0.542
[9,     1] loss: 0.516
[10,     1] loss: 0.493
[11,     1] loss: 0.461
[12,     1] loss: 0.439
[13,     1] loss: 0.413
[14,     1] loss: 0.390
[15,     1] loss: 0.369
[16,     1] loss: 0.347
[17,     1] loss: 0.332
[18,     1] loss: 0.316
[19,     1] loss: 0.299
[20,     1] loss: 0.286
[21,     1] loss: 0.269
[22,     1] loss: 0.256
[23,     1] loss: 0.244
[24,     1] loss: 0.234
[25,     1] loss: 0.223
[26,     1] loss: 0.213
[27,     1] loss: 0.204
[28,     1] loss: 0.196
[29,     1] loss: 0.190
[30,     1] loss: 0.183
[31,     1] loss: 0.176
[32,     1] loss: 0.170
[33,     1] loss: 0.161
[34,     1] loss: 0.157
[35,     1] loss: 0.151
[36,     1] loss: 0.146
[37,     1] loss: 0.138
[38,     1] loss: 0.134
[39,     1] loss: 0.128
[40,     1] loss: 0.123
[41,     1] loss: 0.118
[42,     1] loss: 0.115
[43,     1] loss: 0.110
[44,     1] loss: 0.105
[45,     1] loss: 0.105
[46,     1] loss: 0.100
[47,     1] loss: 0.098
[48,     1] loss: 0.096
[49,     1] loss: 0.093
[50,     1] loss: 0.090
[51,     1] loss: 0.090
[52,     1] loss: 0.088
[53,     1] loss: 0.086
[54,     1] loss: 0.084
[55,     1] loss: 0.083
[56,     1] loss: 0.081
[57,     1] loss: 0.080
[58,     1] loss: 0.079
[59,     1] loss: 0.078
[60,     1] loss: 0.078
[61,     1] loss: 0.075
[62,     1] loss: 0.075
[63,     1] loss: 0.075
[64,     1] loss: 0.074
[65,     1] loss: 0.072
[66,     1] loss: 0.069
[67,     1] loss: 0.068
[68,     1] loss: 0.065
[69,     1] loss: 0.063
[70,     1] loss: 0.076
[71,     1] loss: 0.495
[72,     1] loss: 0.368
[73,     1] loss: 0.343
[74,     1] loss: 0.337
[75,     1] loss: 0.335
[76,     1] loss: 0.320
[77,     1] loss: 0.321
[78,     1] loss: 0.301
[79,     1] loss: 0.281
Early stopping applied (best metric=0.41818371415138245)
Finished Training
Total time taken: 50.089571475982666
{'Hydroxylation-P Validation Accuracy': 0.7990373280544135, 'Hydroxylation-P Validation Sensitivity': 0.7567619047619047, 'Hydroxylation-P Validation Specificity': 0.8080846925033668, 'Hydroxylation-P Validation Precision': 0.46544373156714564, 'Hydroxylation-P AUC ROC': 0.8372446470433152, 'Hydroxylation-P AUC PR': 0.5511393569633786, 'Hydroxylation-P MCC': 0.47799713014857564, 'Hydroxylation-P F1': 0.5726464847510806, 'Validation Loss (Hydroxylation-P)': 0.40290858030319215, 'Validation Loss (total)': 0.40290858030319215}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009623661637947435,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 239296089,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.46772791497629}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005143793431657271,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1782834030,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.245420929714387}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.688
[3,     1] loss: 0.667
[4,     1] loss: 0.645
[5,     1] loss: 0.616
[6,     1] loss: 0.579
[7,     1] loss: 0.539
[8,     1] loss: 0.494
[9,     1] loss: 0.434
[10,     1] loss: 0.368
[11,     1] loss: 0.303
[12,     1] loss: 0.250
[13,     1] loss: 0.221
[14,     1] loss: 0.183
[15,     1] loss: 0.145
[16,     1] loss: 0.096
[17,     1] loss: 0.090
[18,     1] loss: 0.053
[19,     1] loss: 0.038
[20,     1] loss: 0.037
[21,     1] loss: 0.030
[22,     1] loss: 0.032
[23,     1] loss: 0.024
[24,     1] loss: 0.053
[25,     1] loss: 0.047
[26,     1] loss: 0.061
[27,     1] loss: 0.056
[28,     1] loss: 0.035
[29,     1] loss: 0.072
[30,     1] loss: 0.071
[31,     1] loss: 0.062
[32,     1] loss: 0.051
[33,     1] loss: 0.046
[34,     1] loss: 0.047
[35,     1] loss: 0.044
[36,     1] loss: 0.041
[37,     1] loss: 0.038
[38,     1] loss: 0.034
[39,     1] loss: 0.031
[40,     1] loss: 0.028
[41,     1] loss: 0.026
[42,     1] loss: 0.023
[43,     1] loss: 0.021
[44,     1] loss: 0.020
[45,     1] loss: 0.019
[46,     1] loss: 0.018
[47,     1] loss: 0.018
[48,     1] loss: 0.017
[49,     1] loss: 0.017
[50,     1] loss: 0.016
[51,     1] loss: 0.017
[52,     1] loss: 0.016
[53,     1] loss: 0.015
[54,     1] loss: 0.015
[55,     1] loss: 0.016
[56,     1] loss: 0.014
[57,     1] loss: 0.014
[58,     1] loss: 0.012
[59,     1] loss: 0.012
[60,     1] loss: 0.010
[61,     1] loss: 0.010
[62,     1] loss: 0.009
Early stopping applied (best metric=0.34807708859443665)
Finished Training
Total time taken: 38.769431591033936
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.642
[4,     1] loss: 0.608
[5,     1] loss: 0.574
[6,     1] loss: 0.534
[7,     1] loss: 0.488
[8,     1] loss: 0.438
[9,     1] loss: 0.383
[10,     1] loss: 0.338
[11,     1] loss: 0.298
[12,     1] loss: 0.267
[13,     1] loss: 0.240
[14,     1] loss: 0.217
[15,     1] loss: 0.198
[16,     1] loss: 0.171
[17,     1] loss: 0.154
[18,     1] loss: 0.139
[19,     1] loss: 0.123
[20,     1] loss: 0.103
[21,     1] loss: 0.088
[22,     1] loss: 0.085
[23,     1] loss: 0.099
[24,     1] loss: 0.090
[25,     1] loss: 0.083
[26,     1] loss: 0.069
[27,     1] loss: 0.056
[28,     1] loss: 0.051
[29,     1] loss: 0.052
[30,     1] loss: 0.045
[31,     1] loss: 0.042
[32,     1] loss: 0.036
[33,     1] loss: 0.038
[34,     1] loss: 0.032
[35,     1] loss: 0.035
[36,     1] loss: 0.034
[37,     1] loss: 0.040
[38,     1] loss: 0.032
[39,     1] loss: 0.048
[40,     1] loss: 0.032
[41,     1] loss: 0.061
[42,     1] loss: 0.056
[43,     1] loss: 0.054
[44,     1] loss: 0.103
[45,     1] loss: 0.211
[46,     1] loss: 0.131
[47,     1] loss: 0.086
[48,     1] loss: 0.087
[49,     1] loss: 0.093
[50,     1] loss: 0.087
[51,     1] loss: 0.078
[52,     1] loss: 0.073
[53,     1] loss: 0.067
[54,     1] loss: 0.062
[55,     1] loss: 0.056
[56,     1] loss: 0.051
[57,     1] loss: 0.046
[58,     1] loss: 0.042
[59,     1] loss: 0.039
Early stopping applied (best metric=0.4264276325702667)
Finished Training
Total time taken: 37.38842010498047
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.680
[3,     1] loss: 0.649
[4,     1] loss: 0.615
[5,     1] loss: 0.576
[6,     1] loss: 0.528
[7,     1] loss: 0.474
[8,     1] loss: 0.415
[9,     1] loss: 0.350
[10,     1] loss: 0.291
[11,     1] loss: 0.242
[12,     1] loss: 0.205
[13,     1] loss: 0.173
[14,     1] loss: 0.143
[15,     1] loss: 0.120
[16,     1] loss: 0.104
[17,     1] loss: 0.104
[18,     1] loss: 0.076
[19,     1] loss: 0.092
[20,     1] loss: 0.065
[21,     1] loss: 0.048
[22,     1] loss: 0.040
[23,     1] loss: 0.039
[24,     1] loss: 0.035
[25,     1] loss: 0.032
[26,     1] loss: 0.031
[27,     1] loss: 0.027
[28,     1] loss: 0.024
[29,     1] loss: 0.022
[30,     1] loss: 0.020
[31,     1] loss: 0.020
[32,     1] loss: 0.020
[33,     1] loss: 0.019
[34,     1] loss: 0.019
[35,     1] loss: 0.018
[36,     1] loss: 0.017
[37,     1] loss: 0.018
[38,     1] loss: 0.017
[39,     1] loss: 0.017
[40,     1] loss: 0.016
[41,     1] loss: 0.016
[42,     1] loss: 0.016
[43,     1] loss: 0.015
[44,     1] loss: 0.016
[45,     1] loss: 0.014
[46,     1] loss: 0.015
[47,     1] loss: 0.015
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.013
[51,     1] loss: 0.014
[52,     1] loss: 0.013
[53,     1] loss: 0.013
[54,     1] loss: 0.013
[55,     1] loss: 0.012
[56,     1] loss: 0.012
[57,     1] loss: 0.011
[58,     1] loss: 0.011
[59,     1] loss: 0.011
Early stopping applied (best metric=0.444726824760437)
Finished Training
Total time taken: 38.26565718650818
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.656
[4,     1] loss: 0.626
[5,     1] loss: 0.590
[6,     1] loss: 0.553
[7,     1] loss: 0.511
[8,     1] loss: 0.468
[9,     1] loss: 0.416
[10,     1] loss: 0.357
[11,     1] loss: 0.306
[12,     1] loss: 0.257
[13,     1] loss: 0.225
[14,     1] loss: 0.230
[15,     1] loss: 0.162
[16,     1] loss: 0.262
[17,     1] loss: 0.138
[18,     1] loss: 0.216
[19,     1] loss: 0.150
[20,     1] loss: 0.116
[21,     1] loss: 0.131
[22,     1] loss: 0.119
[23,     1] loss: 0.099
[24,     1] loss: 0.087
[25,     1] loss: 0.078
[26,     1] loss: 0.072
[27,     1] loss: 0.064
[28,     1] loss: 0.057
[29,     1] loss: 0.051
[30,     1] loss: 0.046
[31,     1] loss: 0.041
[32,     1] loss: 0.038
[33,     1] loss: 0.035
[34,     1] loss: 0.032
[35,     1] loss: 0.029
[36,     1] loss: 0.026
[37,     1] loss: 0.024
[38,     1] loss: 0.023
[39,     1] loss: 0.021
[40,     1] loss: 0.021
[41,     1] loss: 0.020
[42,     1] loss: 0.019
[43,     1] loss: 0.019
[44,     1] loss: 0.018
[45,     1] loss: 0.018
[46,     1] loss: 0.018
[47,     1] loss: 0.017
[48,     1] loss: 0.017
[49,     1] loss: 0.016
[50,     1] loss: 0.016
[51,     1] loss: 0.016
[52,     1] loss: 0.016
[53,     1] loss: 0.016
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.015
[57,     1] loss: 0.015
[58,     1] loss: 0.015
Early stopping applied (best metric=0.4367882013320923)
Finished Training
Total time taken: 38.19680118560791
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.650
[4,     1] loss: 0.616
[5,     1] loss: 0.577
[6,     1] loss: 0.533
[7,     1] loss: 0.490
[8,     1] loss: 0.441
[9,     1] loss: 0.395
[10,     1] loss: 0.349
[11,     1] loss: 0.301
[12,     1] loss: 0.259
[13,     1] loss: 0.204
[14,     1] loss: 0.183
[15,     1] loss: 0.149
[16,     1] loss: 0.119
[17,     1] loss: 0.110
[18,     1] loss: 0.125
[19,     1] loss: 0.099
[20,     1] loss: 0.081
[21,     1] loss: 0.079
[22,     1] loss: 0.075
[23,     1] loss: 0.065
[24,     1] loss: 0.055
[25,     1] loss: 0.049
[26,     1] loss: 0.046
[27,     1] loss: 0.041
[28,     1] loss: 0.040
[29,     1] loss: 0.037
[30,     1] loss: 0.035
[31,     1] loss: 0.033
[32,     1] loss: 0.032
[33,     1] loss: 0.030
[34,     1] loss: 0.029
[35,     1] loss: 0.028
[36,     1] loss: 0.028
[37,     1] loss: 0.027
[38,     1] loss: 0.025
[39,     1] loss: 0.025
[40,     1] loss: 0.023
[41,     1] loss: 0.023
[42,     1] loss: 0.022
[43,     1] loss: 0.022
[44,     1] loss: 0.022
[45,     1] loss: 0.021
[46,     1] loss: 0.021
[47,     1] loss: 0.021
[48,     1] loss: 0.020
[49,     1] loss: 0.019
[50,     1] loss: 0.020
[51,     1] loss: 0.019
[52,     1] loss: 0.019
[53,     1] loss: 0.019
[54,     1] loss: 0.018
[55,     1] loss: 0.019
[56,     1] loss: 0.018
[57,     1] loss: 0.018
Early stopping applied (best metric=0.4311560392379761)
Finished Training
Total time taken: 35.771401166915894
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.653
[4,     1] loss: 0.619
[5,     1] loss: 0.580
[6,     1] loss: 0.543
[7,     1] loss: 0.498
[8,     1] loss: 0.450
[9,     1] loss: 0.405
[10,     1] loss: 0.378
[11,     1] loss: 0.347
[12,     1] loss: 0.321
[13,     1] loss: 0.294
[14,     1] loss: 0.265
[15,     1] loss: 0.241
[16,     1] loss: 0.217
[17,     1] loss: 0.192
[18,     1] loss: 0.165
[19,     1] loss: 0.143
[20,     1] loss: 0.122
[21,     1] loss: 0.109
[22,     1] loss: 0.096
[23,     1] loss: 0.089
[24,     1] loss: 0.081
[25,     1] loss: 0.075
[26,     1] loss: 0.061
[27,     1] loss: 0.051
[28,     1] loss: 0.043
[29,     1] loss: 0.041
[30,     1] loss: 0.032
[31,     1] loss: 0.033
[32,     1] loss: 0.028
[33,     1] loss: 0.027
[34,     1] loss: 0.026
[35,     1] loss: 0.027
[36,     1] loss: 0.025
[37,     1] loss: 0.023
[38,     1] loss: 0.022
[39,     1] loss: 0.022
[40,     1] loss: 0.022
[41,     1] loss: 0.022
[42,     1] loss: 0.022
[43,     1] loss: 0.021
[44,     1] loss: 0.021
[45,     1] loss: 0.020
[46,     1] loss: 0.021
[47,     1] loss: 0.020
[48,     1] loss: 0.020
[49,     1] loss: 0.019
[50,     1] loss: 0.020
[51,     1] loss: 0.018
[52,     1] loss: 0.018
[53,     1] loss: 0.018
[54,     1] loss: 0.018
[55,     1] loss: 0.016
Early stopping applied (best metric=0.49864745140075684)
Finished Training
Total time taken: 35.65639853477478
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.661
[4,     1] loss: 0.635
[5,     1] loss: 0.606
[6,     1] loss: 0.572
[7,     1] loss: 0.535
[8,     1] loss: 0.495
[9,     1] loss: 0.455
[10,     1] loss: 0.416
[11,     1] loss: 0.375
[12,     1] loss: 0.335
[13,     1] loss: 0.303
[14,     1] loss: 0.258
[15,     1] loss: 0.227
[16,     1] loss: 0.214
[17,     1] loss: 0.178
[18,     1] loss: 0.154
[19,     1] loss: 0.161
[20,     1] loss: 0.180
[21,     1] loss: 0.124
[22,     1] loss: 0.135
[23,     1] loss: 0.093
[24,     1] loss: 0.097
[25,     1] loss: 0.070
[26,     1] loss: 0.065
[27,     1] loss: 0.060
[28,     1] loss: 0.052
[29,     1] loss: 0.045
[30,     1] loss: 0.042
[31,     1] loss: 0.038
[32,     1] loss: 0.034
[33,     1] loss: 0.031
[34,     1] loss: 0.028
[35,     1] loss: 0.026
[36,     1] loss: 0.025
[37,     1] loss: 0.024
[38,     1] loss: 0.022
[39,     1] loss: 0.022
[40,     1] loss: 0.022
[41,     1] loss: 0.022
[42,     1] loss: 0.022
[43,     1] loss: 0.021
[44,     1] loss: 0.023
[45,     1] loss: 0.022
[46,     1] loss: 0.022
[47,     1] loss: 0.023
[48,     1] loss: 0.023
[49,     1] loss: 0.022
[50,     1] loss: 0.022
[51,     1] loss: 0.021
[52,     1] loss: 0.021
[53,     1] loss: 0.021
[54,     1] loss: 0.020
[55,     1] loss: 0.020
[56,     1] loss: 0.019
[57,     1] loss: 0.017
[58,     1] loss: 0.018
[59,     1] loss: 0.017
[60,     1] loss: 0.015
[61,     1] loss: 0.015
[62,     1] loss: 0.015
Early stopping applied (best metric=0.38819900155067444)
Finished Training
Total time taken: 40.33345985412598
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.692
[3,     1] loss: 0.674
[4,     1] loss: 0.656
[5,     1] loss: 0.633
[6,     1] loss: 0.604
[7,     1] loss: 0.572
[8,     1] loss: 0.536
[9,     1] loss: 0.494
[10,     1] loss: 0.449
[11,     1] loss: 0.401
[12,     1] loss: 0.351
[13,     1] loss: 0.343
[14,     1] loss: 0.272
[15,     1] loss: 0.265
[16,     1] loss: 0.219
[17,     1] loss: 0.188
[18,     1] loss: 0.166
[19,     1] loss: 0.140
[20,     1] loss: 0.118
[21,     1] loss: 0.103
[22,     1] loss: 0.088
[23,     1] loss: 0.071
[24,     1] loss: 0.100
[25,     1] loss: 0.131
[26,     1] loss: 0.051
[27,     1] loss: 0.056
[28,     1] loss: 0.053
[29,     1] loss: 0.040
[30,     1] loss: 0.041
[31,     1] loss: 0.043
[32,     1] loss: 0.035
[33,     1] loss: 0.035
[34,     1] loss: 0.033
[35,     1] loss: 0.029
[36,     1] loss: 0.028
[37,     1] loss: 0.027
[38,     1] loss: 0.024
[39,     1] loss: 0.023
[40,     1] loss: 0.022
[41,     1] loss: 0.021
[42,     1] loss: 0.019
[43,     1] loss: 0.018
[44,     1] loss: 0.018
[45,     1] loss: 0.016
[46,     1] loss: 0.016
[47,     1] loss: 0.016
[48,     1] loss: 0.016
[49,     1] loss: 0.015
[50,     1] loss: 0.016
[51,     1] loss: 0.016
[52,     1] loss: 0.016
[53,     1] loss: 0.015
[54,     1] loss: 0.016
[55,     1] loss: 0.017
[56,     1] loss: 0.016
[57,     1] loss: 0.015
[58,     1] loss: 0.015
[59,     1] loss: 0.014
[60,     1] loss: 0.014
[61,     1] loss: 0.013
[62,     1] loss: 0.013
Early stopping applied (best metric=0.37990128993988037)
Finished Training
Total time taken: 41.090479373931885
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.697
[3,     1] loss: 0.674
[4,     1] loss: 0.645
[5,     1] loss: 0.615
[6,     1] loss: 0.579
[7,     1] loss: 0.534
[8,     1] loss: 0.481
[9,     1] loss: 0.428
[10,     1] loss: 0.371
[11,     1] loss: 0.317
[12,     1] loss: 0.283
[13,     1] loss: 0.269
[14,     1] loss: 0.206
[15,     1] loss: 0.209
[16,     1] loss: 0.150
[17,     1] loss: 0.136
[18,     1] loss: 0.109
[19,     1] loss: 0.089
[20,     1] loss: 0.074
[21,     1] loss: 0.064
[22,     1] loss: 0.051
[23,     1] loss: 0.044
[24,     1] loss: 0.039
[25,     1] loss: 0.032
[26,     1] loss: 0.028
[27,     1] loss: 0.027
[28,     1] loss: 0.023
[29,     1] loss: 0.021
[30,     1] loss: 0.020
[31,     1] loss: 0.020
[32,     1] loss: 0.019
[33,     1] loss: 0.018
[34,     1] loss: 0.019
[35,     1] loss: 0.019
[36,     1] loss: 0.019
[37,     1] loss: 0.017
[38,     1] loss: 0.019
[39,     1] loss: 0.018
[40,     1] loss: 0.018
[41,     1] loss: 0.018
[42,     1] loss: 0.017
[43,     1] loss: 0.017
[44,     1] loss: 0.017
[45,     1] loss: 0.017
[46,     1] loss: 0.017
[47,     1] loss: 0.017
[48,     1] loss: 0.015
[49,     1] loss: 0.015
[50,     1] loss: 0.015
[51,     1] loss: 0.015
[52,     1] loss: 0.013
[53,     1] loss: 0.014
[54,     1] loss: 0.013
[55,     1] loss: 0.013
[56,     1] loss: 0.014
[57,     1] loss: 0.012
[58,     1] loss: 0.013
[59,     1] loss: 0.012
[60,     1] loss: 0.012
[61,     1] loss: 0.012
[62,     1] loss: 0.012
[63,     1] loss: 0.012
[64,     1] loss: 0.011
Early stopping applied (best metric=0.3143516778945923)
Finished Training
Total time taken: 43.14317560195923
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.663
[4,     1] loss: 0.638
[5,     1] loss: 0.611
[6,     1] loss: 0.581
[7,     1] loss: 0.544
[8,     1] loss: 0.508
[9,     1] loss: 0.472
[10,     1] loss: 0.434
[11,     1] loss: 0.395
[12,     1] loss: 0.374
[13,     1] loss: 0.335
[14,     1] loss: 0.308
[15,     1] loss: 0.257
[16,     1] loss: 0.238
[17,     1] loss: 0.205
[18,     1] loss: 0.184
[19,     1] loss: 0.163
[20,     1] loss: 0.141
[21,     1] loss: 0.123
[22,     1] loss: 0.112
[23,     1] loss: 0.101
[24,     1] loss: 0.095
[25,     1] loss: 0.083
[26,     1] loss: 0.082
[27,     1] loss: 0.074
[28,     1] loss: 0.075
[29,     1] loss: 0.066
[30,     1] loss: 0.057
[31,     1] loss: 0.055
[32,     1] loss: 0.053
[33,     1] loss: 0.049
[34,     1] loss: 0.046
[35,     1] loss: 0.042
[36,     1] loss: 0.039
[37,     1] loss: 0.038
[38,     1] loss: 0.036
[39,     1] loss: 0.035
[40,     1] loss: 0.032
[41,     1] loss: 0.031
[42,     1] loss: 0.031
[43,     1] loss: 0.029
[44,     1] loss: 0.027
[45,     1] loss: 0.025
[46,     1] loss: 0.023
[47,     1] loss: 0.020
[48,     1] loss: 0.017
[49,     1] loss: 0.015
[50,     1] loss: 0.012
[51,     1] loss: 0.010
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.008
[59,     1] loss: 0.008
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.008
[63,     1] loss: 0.009
Early stopping applied (best metric=0.4159906506538391)
Finished Training
Total time taken: 42.45847725868225
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.669
[4,     1] loss: 0.648
[5,     1] loss: 0.624
[6,     1] loss: 0.596
[7,     1] loss: 0.562
[8,     1] loss: 0.522
[9,     1] loss: 0.479
[10,     1] loss: 0.436
[11,     1] loss: 0.385
[12,     1] loss: 0.343
[13,     1] loss: 0.291
[14,     1] loss: 0.244
[15,     1] loss: 0.201
[16,     1] loss: 0.166
[17,     1] loss: 0.136
[18,     1] loss: 0.115
[19,     1] loss: 0.105
[20,     1] loss: 0.090
[21,     1] loss: 0.082
[22,     1] loss: 0.072
[23,     1] loss: 0.070
[24,     1] loss: 0.063
[25,     1] loss: 0.058
[26,     1] loss: 0.056
[27,     1] loss: 0.052
[28,     1] loss: 0.051
[29,     1] loss: 0.047
[30,     1] loss: 0.045
[31,     1] loss: 0.042
[32,     1] loss: 0.042
[33,     1] loss: 0.039
[34,     1] loss: 0.037
[35,     1] loss: 0.036
[36,     1] loss: 0.034
[37,     1] loss: 0.034
[38,     1] loss: 0.033
[39,     1] loss: 0.033
[40,     1] loss: 0.031
[41,     1] loss: 0.031
[42,     1] loss: 0.028
[43,     1] loss: 0.028
[44,     1] loss: 0.027
[45,     1] loss: 0.025
[46,     1] loss: 0.024
[47,     1] loss: 0.024
[48,     1] loss: 0.024
[49,     1] loss: 0.022
[50,     1] loss: 0.021
[51,     1] loss: 0.021
[52,     1] loss: 0.020
[53,     1] loss: 0.019
[54,     1] loss: 0.018
[55,     1] loss: 0.016
[56,     1] loss: 0.015
[57,     1] loss: 0.013
[58,     1] loss: 0.011
[59,     1] loss: 0.010
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.007
Early stopping applied (best metric=0.3546270728111267)
Finished Training
Total time taken: 42.119919538497925
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.656
[4,     1] loss: 0.624
[5,     1] loss: 0.584
[6,     1] loss: 0.544
[7,     1] loss: 0.489
[8,     1] loss: 0.428
[9,     1] loss: 0.362
[10,     1] loss: 0.303
[11,     1] loss: 0.245
[12,     1] loss: 0.189
[13,     1] loss: 0.154
[14,     1] loss: 0.145
[15,     1] loss: 0.116
[16,     1] loss: 0.072
[17,     1] loss: 0.074
[18,     1] loss: 0.048
[19,     1] loss: 0.059
[20,     1] loss: 0.033
[21,     1] loss: 0.038
[22,     1] loss: 0.024
[23,     1] loss: 0.026
[24,     1] loss: 0.020
[25,     1] loss: 0.018
[26,     1] loss: 0.017
[27,     1] loss: 0.016
[28,     1] loss: 0.015
[29,     1] loss: 0.015
[30,     1] loss: 0.015
[31,     1] loss: 0.015
[32,     1] loss: 0.016
[33,     1] loss: 0.015
[34,     1] loss: 0.015
[35,     1] loss: 0.014
[36,     1] loss: 0.015
[37,     1] loss: 0.014
[38,     1] loss: 0.014
[39,     1] loss: 0.014
[40,     1] loss: 0.015
[41,     1] loss: 0.014
[42,     1] loss: 0.014
[43,     1] loss: 0.013
[44,     1] loss: 0.014
[45,     1] loss: 0.013
[46,     1] loss: 0.013
[47,     1] loss: 0.012
[48,     1] loss: 0.012
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.009
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.009
Early stopping applied (best metric=0.34513965249061584)
Finished Training
Total time taken: 40.54446196556091
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.651
[4,     1] loss: 0.617
[5,     1] loss: 0.583
[6,     1] loss: 0.543
[7,     1] loss: 0.497
[8,     1] loss: 0.445
[9,     1] loss: 0.393
[10,     1] loss: 0.350
[11,     1] loss: 0.304
[12,     1] loss: 0.267
[13,     1] loss: 0.236
[14,     1] loss: 0.210
[15,     1] loss: 0.183
[16,     1] loss: 0.162
[17,     1] loss: 0.141
[18,     1] loss: 0.121
[19,     1] loss: 0.122
[20,     1] loss: 0.102
[21,     1] loss: 0.084
[22,     1] loss: 0.092
[23,     1] loss: 0.097
[24,     1] loss: 0.121
[25,     1] loss: 0.063
[26,     1] loss: 0.064
[27,     1] loss: 0.060
[28,     1] loss: 0.060
[29,     1] loss: 0.054
[30,     1] loss: 0.051
[31,     1] loss: 0.048
[32,     1] loss: 0.045
[33,     1] loss: 0.042
[34,     1] loss: 0.040
[35,     1] loss: 0.037
[36,     1] loss: 0.035
[37,     1] loss: 0.034
[38,     1] loss: 0.032
[39,     1] loss: 0.030
[40,     1] loss: 0.029
[41,     1] loss: 0.028
[42,     1] loss: 0.027
[43,     1] loss: 0.026
[44,     1] loss: 0.025
[45,     1] loss: 0.025
[46,     1] loss: 0.024
[47,     1] loss: 0.025
[48,     1] loss: 0.023
[49,     1] loss: 0.024
[50,     1] loss: 0.023
[51,     1] loss: 0.023
[52,     1] loss: 0.022
[53,     1] loss: 0.022
[54,     1] loss: 0.022
[55,     1] loss: 0.021
[56,     1] loss: 0.022
[57,     1] loss: 0.022
[58,     1] loss: 0.021
[59,     1] loss: 0.020
[60,     1] loss: 0.020
Early stopping applied (best metric=0.4196568429470062)
Finished Training
Total time taken: 40.89145612716675
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.655
[4,     1] loss: 0.622
[5,     1] loss: 0.591
[6,     1] loss: 0.561
[7,     1] loss: 0.531
[8,     1] loss: 0.503
[9,     1] loss: 0.459
[10,     1] loss: 0.416
[11,     1] loss: 0.382
[12,     1] loss: 0.357
[13,     1] loss: 0.337
[14,     1] loss: 0.323
[15,     1] loss: 0.308
[16,     1] loss: 0.299
[17,     1] loss: 0.287
[18,     1] loss: 0.273
[19,     1] loss: 0.261
[20,     1] loss: 0.247
[21,     1] loss: 0.232
[22,     1] loss: 0.217
[23,     1] loss: 0.199
[24,     1] loss: 0.186
[25,     1] loss: 0.171
[26,     1] loss: 0.154
[27,     1] loss: 0.140
[28,     1] loss: 0.130
[29,     1] loss: 0.118
[30,     1] loss: 0.109
[31,     1] loss: 0.105
[32,     1] loss: 0.112
[33,     1] loss: 0.091
[34,     1] loss: 0.118
[35,     1] loss: 0.083
[36,     1] loss: 0.097
[37,     1] loss: 0.120
[38,     1] loss: 0.088
[39,     1] loss: 0.101
[40,     1] loss: 0.085
[41,     1] loss: 0.097
[42,     1] loss: 0.082
[43,     1] loss: 0.081
[44,     1] loss: 0.078
[45,     1] loss: 0.072
[46,     1] loss: 0.068
[47,     1] loss: 0.065
[48,     1] loss: 0.062
[49,     1] loss: 0.061
[50,     1] loss: 0.058
[51,     1] loss: 0.056
[52,     1] loss: 0.056
[53,     1] loss: 0.054
[54,     1] loss: 0.051
[55,     1] loss: 0.050
[56,     1] loss: 0.048
[57,     1] loss: 0.047
[58,     1] loss: 0.045
[59,     1] loss: 0.045
Early stopping applied (best metric=0.42709770798683167)
Finished Training
Total time taken: 40.34846234321594
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.681
[3,     1] loss: 0.647
[4,     1] loss: 0.613
[5,     1] loss: 0.572
[6,     1] loss: 0.529
[7,     1] loss: 0.474
[8,     1] loss: 0.417
[9,     1] loss: 0.364
[10,     1] loss: 0.312
[11,     1] loss: 0.264
[12,     1] loss: 0.228
[13,     1] loss: 0.192
[14,     1] loss: 0.162
[15,     1] loss: 0.132
[16,     1] loss: 0.151
[17,     1] loss: 0.114
[18,     1] loss: 0.113
[19,     1] loss: 0.087
[20,     1] loss: 0.087
[21,     1] loss: 0.053
[22,     1] loss: 0.044
[23,     1] loss: 0.043
[24,     1] loss: 0.036
[25,     1] loss: 0.031
[26,     1] loss: 0.029
[27,     1] loss: 0.025
[28,     1] loss: 0.023
[29,     1] loss: 0.021
[30,     1] loss: 0.020
[31,     1] loss: 0.019
[32,     1] loss: 0.018
[33,     1] loss: 0.018
[34,     1] loss: 0.018
[35,     1] loss: 0.017
[36,     1] loss: 0.017
[37,     1] loss: 0.017
[38,     1] loss: 0.016
[39,     1] loss: 0.016
[40,     1] loss: 0.015
[41,     1] loss: 0.016
[42,     1] loss: 0.015
[43,     1] loss: 0.015
[44,     1] loss: 0.015
[45,     1] loss: 0.015
[46,     1] loss: 0.015
[47,     1] loss: 0.015
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.014
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.012
[54,     1] loss: 0.012
[55,     1] loss: 0.012
[56,     1] loss: 0.011
[57,     1] loss: 0.010
[58,     1] loss: 0.010
[59,     1] loss: 0.010
Early stopping applied (best metric=0.38626548647880554)
Finished Training
Total time taken: 40.855480909347534
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.692
[3,     1] loss: 0.657
[4,     1] loss: 0.619
[5,     1] loss: 0.579
[6,     1] loss: 0.542
[7,     1] loss: 0.503
[8,     1] loss: 0.466
[9,     1] loss: 0.420
[10,     1] loss: 0.368
[11,     1] loss: 0.317
[12,     1] loss: 0.275
[13,     1] loss: 0.234
[14,     1] loss: 0.192
[15,     1] loss: 0.156
[16,     1] loss: 0.126
[17,     1] loss: 0.099
[18,     1] loss: 0.085
[19,     1] loss: 0.066
[20,     1] loss: 0.056
[21,     1] loss: 0.065
[22,     1] loss: 0.052
[23,     1] loss: 0.055
[24,     1] loss: 0.040
[25,     1] loss: 0.042
[26,     1] loss: 0.031
[27,     1] loss: 0.027
[28,     1] loss: 0.029
[29,     1] loss: 0.023
[30,     1] loss: 0.023
[31,     1] loss: 0.021
[32,     1] loss: 0.021
[33,     1] loss: 0.020
[34,     1] loss: 0.020
[35,     1] loss: 0.020
[36,     1] loss: 0.020
[37,     1] loss: 0.019
[38,     1] loss: 0.020
[39,     1] loss: 0.020
[40,     1] loss: 0.019
[41,     1] loss: 0.019
[42,     1] loss: 0.019
[43,     1] loss: 0.018
[44,     1] loss: 0.018
[45,     1] loss: 0.017
[46,     1] loss: 0.017
[47,     1] loss: 0.017
[48,     1] loss: 0.015
[49,     1] loss: 0.015
[50,     1] loss: 0.014
[51,     1] loss: 0.014
[52,     1] loss: 0.012
[53,     1] loss: 0.012
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.008
[59,     1] loss: 0.008
[60,     1] loss: 0.007
Early stopping applied (best metric=0.42966461181640625)
Finished Training
Total time taken: 40.59045481681824
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.687
[3,     1] loss: 0.658
[4,     1] loss: 0.626
[5,     1] loss: 0.592
[6,     1] loss: 0.549
[7,     1] loss: 0.508
[8,     1] loss: 0.451
[9,     1] loss: 0.391
[10,     1] loss: 0.335
[11,     1] loss: 0.283
[12,     1] loss: 0.239
[13,     1] loss: 0.201
[14,     1] loss: 0.166
[15,     1] loss: 0.138
[16,     1] loss: 0.137
[17,     1] loss: 0.145
[18,     1] loss: 0.124
[19,     1] loss: 0.089
[20,     1] loss: 0.090
[21,     1] loss: 0.077
[22,     1] loss: 0.065
[23,     1] loss: 0.060
[24,     1] loss: 0.053
[25,     1] loss: 0.045
[26,     1] loss: 0.039
[27,     1] loss: 0.037
[28,     1] loss: 0.037
[29,     1] loss: 0.026
[30,     1] loss: 0.026
[31,     1] loss: 0.022
[32,     1] loss: 0.023
[33,     1] loss: 0.020
[34,     1] loss: 0.018
[35,     1] loss: 0.019
[36,     1] loss: 0.017
[37,     1] loss: 0.016
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.013
[41,     1] loss: 0.014
[42,     1] loss: 0.014
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.008
[58,     1] loss: 0.008
[59,     1] loss: 0.008
[60,     1] loss: 0.007
[61,     1] loss: 0.007
Early stopping applied (best metric=0.3707374930381775)
Finished Training
Total time taken: 41.30306029319763
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.650
[4,     1] loss: 0.619
[5,     1] loss: 0.584
[6,     1] loss: 0.543
[7,     1] loss: 0.501
[8,     1] loss: 0.456
[9,     1] loss: 0.406
[10,     1] loss: 0.357
[11,     1] loss: 0.328
[12,     1] loss: 0.279
[13,     1] loss: 0.239
[14,     1] loss: 0.205
[15,     1] loss: 0.183
[16,     1] loss: 0.162
[17,     1] loss: 0.138
[18,     1] loss: 0.119
[19,     1] loss: 0.099
[20,     1] loss: 0.090
[21,     1] loss: 0.076
[22,     1] loss: 0.074
[23,     1] loss: 0.065
[24,     1] loss: 0.055
[25,     1] loss: 0.049
[26,     1] loss: 0.045
[27,     1] loss: 0.043
[28,     1] loss: 0.038
[29,     1] loss: 0.035
[30,     1] loss: 0.034
[31,     1] loss: 0.032
[32,     1] loss: 0.031
[33,     1] loss: 0.028
[34,     1] loss: 0.027
[35,     1] loss: 0.026
[36,     1] loss: 0.025
[37,     1] loss: 0.024
[38,     1] loss: 0.023
[39,     1] loss: 0.023
[40,     1] loss: 0.023
[41,     1] loss: 0.021
[42,     1] loss: 0.022
[43,     1] loss: 0.021
[44,     1] loss: 0.021
[45,     1] loss: 0.021
[46,     1] loss: 0.020
[47,     1] loss: 0.020
[48,     1] loss: 0.021
[49,     1] loss: 0.019
[50,     1] loss: 0.018
[51,     1] loss: 0.017
[52,     1] loss: 0.017
[53,     1] loss: 0.016
[54,     1] loss: 0.016
[55,     1] loss: 0.014
[56,     1] loss: 0.013
[57,     1] loss: 0.013
[58,     1] loss: 0.012
[59,     1] loss: 0.011
Early stopping applied (best metric=0.46798810362815857)
Finished Training
Total time taken: 40.10746359825134
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.655
[4,     1] loss: 0.625
[5,     1] loss: 0.591
[6,     1] loss: 0.557
[7,     1] loss: 0.512
[8,     1] loss: 0.463
[9,     1] loss: 0.407
[10,     1] loss: 0.350
[11,     1] loss: 0.289
[12,     1] loss: 0.240
[13,     1] loss: 0.194
[14,     1] loss: 0.161
[15,     1] loss: 0.130
[16,     1] loss: 0.107
[17,     1] loss: 0.090
[18,     1] loss: 0.083
[19,     1] loss: 0.061
[20,     1] loss: 0.052
[21,     1] loss: 0.041
[22,     1] loss: 0.035
[23,     1] loss: 0.026
[24,     1] loss: 0.022
[25,     1] loss: 0.021
[26,     1] loss: 0.019
[27,     1] loss: 0.017
[28,     1] loss: 0.017
[29,     1] loss: 0.016
[30,     1] loss: 0.016
[31,     1] loss: 0.015
[32,     1] loss: 0.016
[33,     1] loss: 0.016
[34,     1] loss: 0.017
[35,     1] loss: 0.017
[36,     1] loss: 0.017
[37,     1] loss: 0.017
[38,     1] loss: 0.017
[39,     1] loss: 0.017
[40,     1] loss: 0.017
[41,     1] loss: 0.018
[42,     1] loss: 0.018
[43,     1] loss: 0.017
[44,     1] loss: 0.017
[45,     1] loss: 0.016
[46,     1] loss: 0.016
[47,     1] loss: 0.015
[48,     1] loss: 0.015
[49,     1] loss: 0.013
[50,     1] loss: 0.014
[51,     1] loss: 0.014
[52,     1] loss: 0.013
[53,     1] loss: 0.013
[54,     1] loss: 0.012
[55,     1] loss: 0.012
[56,     1] loss: 0.011
[57,     1] loss: 0.011
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.010
[61,     1] loss: 0.009
Early stopping applied (best metric=0.3960634768009186)
Finished Training
Total time taken: 41.82807922363281
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.657
[4,     1] loss: 0.629
[5,     1] loss: 0.595
[6,     1] loss: 0.554
[7,     1] loss: 0.507
[8,     1] loss: 0.449
[9,     1] loss: 0.384
[10,     1] loss: 0.323
[11,     1] loss: 0.263
[12,     1] loss: 0.209
[13,     1] loss: 0.159
[14,     1] loss: 0.113
[15,     1] loss: 0.084
[16,     1] loss: 0.065
[17,     1] loss: 0.052
[18,     1] loss: 0.039
[19,     1] loss: 0.028
[20,     1] loss: 0.023
[21,     1] loss: 0.019
[22,     1] loss: 0.016
[23,     1] loss: 0.014
[24,     1] loss: 0.013
[25,     1] loss: 0.012
[26,     1] loss: 0.012
[27,     1] loss: 0.011
[28,     1] loss: 0.011
[29,     1] loss: 0.012
[30,     1] loss: 0.012
[31,     1] loss: 0.014
[32,     1] loss: 0.014
[33,     1] loss: 0.015
[34,     1] loss: 0.015
[35,     1] loss: 0.016
[36,     1] loss: 0.017
[37,     1] loss: 0.017
[38,     1] loss: 0.017
[39,     1] loss: 0.016
[40,     1] loss: 0.016
[41,     1] loss: 0.017
[42,     1] loss: 0.016
[43,     1] loss: 0.015
[44,     1] loss: 0.014
[45,     1] loss: 0.014
[46,     1] loss: 0.014
[47,     1] loss: 0.013
[48,     1] loss: 0.013
[49,     1] loss: 0.012
[50,     1] loss: 0.012
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.010
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.009
[59,     1] loss: 0.010
[60,     1] loss: 0.010
Early stopping applied (best metric=0.3816634714603424)
Finished Training
Total time taken: 41.546472787857056
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.658
[4,     1] loss: 0.630
[5,     1] loss: 0.598
[6,     1] loss: 0.555
[7,     1] loss: 0.505
[8,     1] loss: 0.451
[9,     1] loss: 0.395
[10,     1] loss: 0.342
[11,     1] loss: 0.298
[12,     1] loss: 0.258
[13,     1] loss: 0.223
[14,     1] loss: 0.188
[15,     1] loss: 0.157
[16,     1] loss: 0.130
[17,     1] loss: 0.107
[18,     1] loss: 0.099
[19,     1] loss: 0.094
[20,     1] loss: 0.071
[21,     1] loss: 0.062
[22,     1] loss: 0.058
[23,     1] loss: 0.051
[24,     1] loss: 0.046
[25,     1] loss: 0.042
[26,     1] loss: 0.039
[27,     1] loss: 0.037
[28,     1] loss: 0.035
[29,     1] loss: 0.033
[30,     1] loss: 0.033
[31,     1] loss: 0.031
[32,     1] loss: 0.031
[33,     1] loss: 0.031
[34,     1] loss: 0.030
[35,     1] loss: 0.029
[36,     1] loss: 0.030
[37,     1] loss: 0.029
[38,     1] loss: 0.029
[39,     1] loss: 0.028
[40,     1] loss: 0.028
[41,     1] loss: 0.027
[42,     1] loss: 0.027
[43,     1] loss: 0.027
[44,     1] loss: 0.027
[45,     1] loss: 0.026
[46,     1] loss: 0.027
[47,     1] loss: 0.026
[48,     1] loss: 0.025
[49,     1] loss: 0.024
[50,     1] loss: 0.024
[51,     1] loss: 0.024
[52,     1] loss: 0.023
[53,     1] loss: 0.022
[54,     1] loss: 0.022
[55,     1] loss: 0.021
[56,     1] loss: 0.020
[57,     1] loss: 0.019
[58,     1] loss: 0.015
[59,     1] loss: 0.015
[60,     1] loss: 0.011
Early stopping applied (best metric=0.410505473613739)
Finished Training
Total time taken: 41.64146685600281
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.671
[3,     1] loss: 0.630
[4,     1] loss: 0.594
[5,     1] loss: 0.556
[6,     1] loss: 0.511
[7,     1] loss: 0.462
[8,     1] loss: 0.412
[9,     1] loss: 0.358
[10,     1] loss: 0.296
[11,     1] loss: 0.241
[12,     1] loss: 0.188
[13,     1] loss: 0.137
[14,     1] loss: 0.107
[15,     1] loss: 0.082
[16,     1] loss: 0.058
[17,     1] loss: 0.043
[18,     1] loss: 0.036
[19,     1] loss: 0.039
[20,     1] loss: 0.020
[21,     1] loss: 0.024
[22,     1] loss: 0.019
[23,     1] loss: 0.013
[24,     1] loss: 0.013
[25,     1] loss: 0.013
[26,     1] loss: 0.012
[27,     1] loss: 0.012
[28,     1] loss: 0.012
[29,     1] loss: 0.012
[30,     1] loss: 0.013
[31,     1] loss: 0.013
[32,     1] loss: 0.014
[33,     1] loss: 0.015
[34,     1] loss: 0.015
[35,     1] loss: 0.015
[36,     1] loss: 0.014
[37,     1] loss: 0.015
[38,     1] loss: 0.015
[39,     1] loss: 0.014
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.012
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.012
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.009
[60,     1] loss: 0.008
[61,     1] loss: 0.008
Early stopping applied (best metric=0.3319418430328369)
Finished Training
Total time taken: 42.389484882354736
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.687
[3,     1] loss: 0.651
[4,     1] loss: 0.616
[5,     1] loss: 0.576
[6,     1] loss: 0.529
[7,     1] loss: 0.475
[8,     1] loss: 0.430
[9,     1] loss: 0.375
[10,     1] loss: 0.320
[11,     1] loss: 0.285
[12,     1] loss: 0.245
[13,     1] loss: 0.210
[14,     1] loss: 0.183
[15,     1] loss: 0.154
[16,     1] loss: 0.128
[17,     1] loss: 0.113
[18,     1] loss: 0.095
[19,     1] loss: 0.085
[20,     1] loss: 0.094
[21,     1] loss: 0.096
[22,     1] loss: 0.099
[23,     1] loss: 0.090
[24,     1] loss: 0.080
[25,     1] loss: 0.077
[26,     1] loss: 0.073
[27,     1] loss: 0.066
[28,     1] loss: 0.065
[29,     1] loss: 0.063
[30,     1] loss: 0.061
[31,     1] loss: 0.056
[32,     1] loss: 0.052
[33,     1] loss: 0.047
[34,     1] loss: 0.044
[35,     1] loss: 0.040
[36,     1] loss: 0.035
[37,     1] loss: 0.031
[38,     1] loss: 0.029
[39,     1] loss: 0.027
[40,     1] loss: 0.025
[41,     1] loss: 0.022
[42,     1] loss: 0.021
[43,     1] loss: 0.020
[44,     1] loss: 0.019
[45,     1] loss: 0.017
[46,     1] loss: 0.016
[47,     1] loss: 0.016
[48,     1] loss: 0.015
[49,     1] loss: 0.014
[50,     1] loss: 0.013
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.012
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.009
[60,     1] loss: 0.009
Early stopping applied (best metric=0.4178861677646637)
Finished Training
Total time taken: 42.028470277786255
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.698
[3,     1] loss: 0.671
[4,     1] loss: 0.641
[5,     1] loss: 0.608
[6,     1] loss: 0.569
[7,     1] loss: 0.526
[8,     1] loss: 0.475
[9,     1] loss: 0.417
[10,     1] loss: 0.356
[11,     1] loss: 0.292
[12,     1] loss: 0.234
[13,     1] loss: 0.185
[14,     1] loss: 0.150
[15,     1] loss: 0.116
[16,     1] loss: 0.097
[17,     1] loss: 0.080
[18,     1] loss: 0.079
[19,     1] loss: 0.101
[20,     1] loss: 0.056
[21,     1] loss: 0.101
[22,     1] loss: 0.073
[23,     1] loss: 0.087
[24,     1] loss: 0.056
[25,     1] loss: 0.120
[26,     1] loss: 0.052
[27,     1] loss: 0.055
[28,     1] loss: 0.052
[29,     1] loss: 0.052
[30,     1] loss: 0.056
[31,     1] loss: 0.050
[32,     1] loss: 0.048
[33,     1] loss: 0.041
[34,     1] loss: 0.039
[35,     1] loss: 0.037
[36,     1] loss: 0.034
[37,     1] loss: 0.032
[38,     1] loss: 0.030
[39,     1] loss: 0.027
[40,     1] loss: 0.026
[41,     1] loss: 0.025
[42,     1] loss: 0.023
[43,     1] loss: 0.022
[44,     1] loss: 0.021
[45,     1] loss: 0.021
[46,     1] loss: 0.021
[47,     1] loss: 0.021
[48,     1] loss: 0.021
[49,     1] loss: 0.020
[50,     1] loss: 0.020
[51,     1] loss: 0.021
[52,     1] loss: 0.021
[53,     1] loss: 0.021
[54,     1] loss: 0.021
[55,     1] loss: 0.020
[56,     1] loss: 0.020
[57,     1] loss: 0.020
[58,     1] loss: 0.019
[59,     1] loss: 0.020
[60,     1] loss: 0.018
[61,     1] loss: 0.019
Early stopping applied (best metric=0.3466492295265198)
Finished Training
Total time taken: 42.819486141204834
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.654
[4,     1] loss: 0.620
[5,     1] loss: 0.581
[6,     1] loss: 0.536
[7,     1] loss: 0.485
[8,     1] loss: 0.424
[9,     1] loss: 0.358
[10,     1] loss: 0.298
[11,     1] loss: 0.238
[12,     1] loss: 0.185
[13,     1] loss: 0.145
[14,     1] loss: 0.119
[15,     1] loss: 0.094
[16,     1] loss: 0.080
[17,     1] loss: 0.065
[18,     1] loss: 0.051
[19,     1] loss: 0.041
[20,     1] loss: 0.039
[21,     1] loss: 0.033
[22,     1] loss: 0.029
[23,     1] loss: 0.026
[24,     1] loss: 0.023
[25,     1] loss: 0.021
[26,     1] loss: 0.020
[27,     1] loss: 0.019
[28,     1] loss: 0.018
[29,     1] loss: 0.018
[30,     1] loss: 0.018
[31,     1] loss: 0.018
[32,     1] loss: 0.018
[33,     1] loss: 0.018
[34,     1] loss: 0.018
[35,     1] loss: 0.018
[36,     1] loss: 0.017
[37,     1] loss: 0.018
[38,     1] loss: 0.018
[39,     1] loss: 0.018
[40,     1] loss: 0.018
[41,     1] loss: 0.017
[42,     1] loss: 0.016
[43,     1] loss: 0.016
[44,     1] loss: 0.015
[45,     1] loss: 0.014
[46,     1] loss: 0.014
[47,     1] loss: 0.013
[48,     1] loss: 0.013
[49,     1] loss: 0.013
[50,     1] loss: 0.012
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.010
Early stopping applied (best metric=0.4136820435523987)
Finished Training
Total time taken: 40.988473653793335
{'Hydroxylation-P Validation Accuracy': 0.6927834221613116, 'Hydroxylation-P Validation Sensitivity': 0.8535238095238095, 'Hydroxylation-P Validation Specificity': 0.658294179260811, 'Hydroxylation-P Validation Precision': 0.3610938322385698, 'Hydroxylation-P AUC ROC': 0.834263635046517, 'Hydroxylation-P AUC PR': 0.545363065617328, 'Hydroxylation-P MCC': 0.40167761268340485, 'Hydroxylation-P F1': 0.5031670355649228, 'Validation Loss (Hydroxylation-P)': 0.39935338139534, 'Validation Loss (total)': 0.39935338139534}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001700665723196779,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1944003602,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.559494832924234}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.668
[4,     1] loss: 0.654
[5,     1] loss: 0.640
[6,     1] loss: 0.627
[7,     1] loss: 0.613
[8,     1] loss: 0.597
[9,     1] loss: 0.583
[10,     1] loss: 0.567
[11,     1] loss: 0.549
[12,     1] loss: 0.533
[13,     1] loss: 0.514
[14,     1] loss: 0.493
[15,     1] loss: 0.473
[16,     1] loss: 0.451
[17,     1] loss: 0.433
[18,     1] loss: 0.408
[19,     1] loss: 0.390
[20,     1] loss: 0.366
[21,     1] loss: 0.345
[22,     1] loss: 0.323
[23,     1] loss: 0.307
[24,     1] loss: 0.290
[25,     1] loss: 0.271
[26,     1] loss: 0.256
[27,     1] loss: 0.242
[28,     1] loss: 0.228
[29,     1] loss: 0.213
[30,     1] loss: 0.201
[31,     1] loss: 0.188
[32,     1] loss: 0.178
[33,     1] loss: 0.168
[34,     1] loss: 0.160
[35,     1] loss: 0.156
[36,     1] loss: 0.147
[37,     1] loss: 0.137
[38,     1] loss: 0.131
[39,     1] loss: 0.127
[40,     1] loss: 0.121
[41,     1] loss: 0.117
[42,     1] loss: 0.111
[43,     1] loss: 0.109
[44,     1] loss: 0.107
[45,     1] loss: 0.105
[46,     1] loss: 0.103
[47,     1] loss: 0.101
[48,     1] loss: 0.099
[49,     1] loss: 0.098
[50,     1] loss: 0.095
[51,     1] loss: 0.094
[52,     1] loss: 0.095
[53,     1] loss: 0.095
[54,     1] loss: 0.093
[55,     1] loss: 0.094
[56,     1] loss: 0.094
[57,     1] loss: 0.093
[58,     1] loss: 0.093
[59,     1] loss: 0.092
[60,     1] loss: 0.093
[61,     1] loss: 0.093
[62,     1] loss: 0.092
[63,     1] loss: 0.092
[64,     1] loss: 0.092
[65,     1] loss: 0.094
[66,     1] loss: 0.093
[67,     1] loss: 0.092
[68,     1] loss: 0.092
[69,     1] loss: 0.091
[70,     1] loss: 0.093
[71,     1] loss: 0.092
Early stopping applied (best metric=0.40176138281822205)
Finished Training
Total time taken: 49.14521074295044
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.667
[4,     1] loss: 0.655
[5,     1] loss: 0.642
[6,     1] loss: 0.629
[7,     1] loss: 0.617
[8,     1] loss: 0.604
[9,     1] loss: 0.592
[10,     1] loss: 0.578
[11,     1] loss: 0.563
[12,     1] loss: 0.550
[13,     1] loss: 0.533
[14,     1] loss: 0.515
[15,     1] loss: 0.499
[16,     1] loss: 0.485
[17,     1] loss: 0.465
[18,     1] loss: 0.446
[19,     1] loss: 0.423
[20,     1] loss: 0.407
[21,     1] loss: 0.387
[22,     1] loss: 0.370
[23,     1] loss: 0.354
[24,     1] loss: 0.333
[25,     1] loss: 0.316
[26,     1] loss: 0.301
[27,     1] loss: 0.284
[28,     1] loss: 0.268
[29,     1] loss: 0.257
[30,     1] loss: 0.239
[31,     1] loss: 0.223
[32,     1] loss: 0.216
[33,     1] loss: 0.198
[34,     1] loss: 0.191
[35,     1] loss: 0.182
[36,     1] loss: 0.172
[37,     1] loss: 0.163
[38,     1] loss: 0.156
[39,     1] loss: 0.148
[40,     1] loss: 0.143
[41,     1] loss: 0.140
[42,     1] loss: 0.132
[43,     1] loss: 0.129
[44,     1] loss: 0.122
[45,     1] loss: 0.118
[46,     1] loss: 0.115
[47,     1] loss: 0.114
[48,     1] loss: 0.111
[49,     1] loss: 0.109
[50,     1] loss: 0.108
[51,     1] loss: 0.107
[52,     1] loss: 0.104
[53,     1] loss: 0.103
[54,     1] loss: 0.102
[55,     1] loss: 0.101
[56,     1] loss: 0.101
[57,     1] loss: 0.099
[58,     1] loss: 0.100
[59,     1] loss: 0.101
[60,     1] loss: 0.100
[61,     1] loss: 0.098
[62,     1] loss: 0.097
[63,     1] loss: 0.098
[64,     1] loss: 0.098
[65,     1] loss: 0.099
[66,     1] loss: 0.098
[67,     1] loss: 0.096
[68,     1] loss: 0.096
[69,     1] loss: 0.097
[70,     1] loss: 0.096
[71,     1] loss: 0.097
Early stopping applied (best metric=0.4575107991695404)
Finished Training
Total time taken: 49.3773090839386
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.664
[4,     1] loss: 0.650
[5,     1] loss: 0.637
[6,     1] loss: 0.626
[7,     1] loss: 0.614
[8,     1] loss: 0.602
[9,     1] loss: 0.590
[10,     1] loss: 0.575
[11,     1] loss: 0.562
[12,     1] loss: 0.545
[13,     1] loss: 0.527
[14,     1] loss: 0.512
[15,     1] loss: 0.496
[16,     1] loss: 0.476
[17,     1] loss: 0.459
[18,     1] loss: 0.442
[19,     1] loss: 0.426
[20,     1] loss: 0.412
[21,     1] loss: 0.394
[22,     1] loss: 0.384
[23,     1] loss: 0.369
[24,     1] loss: 0.356
[25,     1] loss: 0.347
[26,     1] loss: 0.336
[27,     1] loss: 0.327
[28,     1] loss: 0.319
[29,     1] loss: 0.310
[30,     1] loss: 0.304
[31,     1] loss: 0.297
[32,     1] loss: 0.290
[33,     1] loss: 0.286
[34,     1] loss: 0.278
[35,     1] loss: 0.275
[36,     1] loss: 0.268
[37,     1] loss: 0.263
[38,     1] loss: 0.259
[39,     1] loss: 0.252
[40,     1] loss: 0.249
[41,     1] loss: 0.245
[42,     1] loss: 0.240
[43,     1] loss: 0.234
[44,     1] loss: 0.230
[45,     1] loss: 0.225
[46,     1] loss: 0.224
[47,     1] loss: 0.220
[48,     1] loss: 0.216
[49,     1] loss: 0.212
[50,     1] loss: 0.211
[51,     1] loss: 0.208
[52,     1] loss: 0.202
[53,     1] loss: 0.201
[54,     1] loss: 0.200
[55,     1] loss: 0.197
[56,     1] loss: 0.194
[57,     1] loss: 0.192
[58,     1] loss: 0.189
[59,     1] loss: 0.187
[60,     1] loss: 0.187
[61,     1] loss: 0.182
[62,     1] loss: 0.180
[63,     1] loss: 0.179
[64,     1] loss: 0.176
[65,     1] loss: 0.174
[66,     1] loss: 0.172
[67,     1] loss: 0.169
[68,     1] loss: 0.167
[69,     1] loss: 0.167
[70,     1] loss: 0.164
[71,     1] loss: 0.163
[72,     1] loss: 0.163
[73,     1] loss: 0.159
[74,     1] loss: 0.158
[75,     1] loss: 0.156
[76,     1] loss: 0.156
[77,     1] loss: 0.153
[78,     1] loss: 0.150
[79,     1] loss: 0.150
[80,     1] loss: 0.150
[81,     1] loss: 0.152
[82,     1] loss: 0.331
[83,     1] loss: 0.931
[84,     1] loss: 0.607
[85,     1] loss: 0.422
[86,     1] loss: 0.401
[87,     1] loss: 0.410
[88,     1] loss: 0.420
[89,     1] loss: 0.423
[90,     1] loss: 0.430
[91,     1] loss: 0.427
[92,     1] loss: 0.425
[93,     1] loss: 0.419
[94,     1] loss: 0.416
[95,     1] loss: 0.409
[96,     1] loss: 0.406
[97,     1] loss: 0.399
[98,     1] loss: 0.393
[99,     1] loss: 0.383
[100,     1] loss: 0.375
[101,     1] loss: 0.367
[102,     1] loss: 0.360
[103,     1] loss: 0.353
[104,     1] loss: 0.342
[105,     1] loss: 0.332
[106,     1] loss: 0.322
[107,     1] loss: 0.312
[108,     1] loss: 0.301
[109,     1] loss: 0.288
[110,     1] loss: 0.276
[111,     1] loss: 0.264
[112,     1] loss: 0.256
[113,     1] loss: 0.247
[114,     1] loss: 0.239
[115,     1] loss: 0.234
[116,     1] loss: 0.225
[117,     1] loss: 0.218
[118,     1] loss: 0.211
[119,     1] loss: 0.207
[120,     1] loss: 0.202
[121,     1] loss: 0.197
[122,     1] loss: 0.192
[123,     1] loss: 0.188
[124,     1] loss: 0.184
[125,     1] loss: 0.181
[126,     1] loss: 0.178
[127,     1] loss: 0.175
[128,     1] loss: 0.174
Early stopping applied (best metric=0.27727049589157104)
Finished Training
Total time taken: 89.81476902961731
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.683
[3,     1] loss: 0.668
[4,     1] loss: 0.657
[5,     1] loss: 0.644
[6,     1] loss: 0.633
[7,     1] loss: 0.620
[8,     1] loss: 0.611
[9,     1] loss: 0.597
[10,     1] loss: 0.582
[11,     1] loss: 0.571
[12,     1] loss: 0.559
[13,     1] loss: 0.543
[14,     1] loss: 0.531
[15,     1] loss: 0.518
[16,     1] loss: 0.506
[17,     1] loss: 0.493
[18,     1] loss: 0.477
[19,     1] loss: 0.465
[20,     1] loss: 0.449
[21,     1] loss: 0.435
[22,     1] loss: 0.423
[23,     1] loss: 0.407
[24,     1] loss: 0.393
[25,     1] loss: 0.379
[26,     1] loss: 0.367
[27,     1] loss: 0.357
[28,     1] loss: 0.338
[29,     1] loss: 0.329
[30,     1] loss: 0.316
[31,     1] loss: 0.307
[32,     1] loss: 0.300
[33,     1] loss: 0.291
[34,     1] loss: 0.282
[35,     1] loss: 0.277
[36,     1] loss: 0.270
[37,     1] loss: 0.262
[38,     1] loss: 0.256
[39,     1] loss: 0.249
[40,     1] loss: 0.244
[41,     1] loss: 0.237
[42,     1] loss: 0.231
[43,     1] loss: 0.228
[44,     1] loss: 0.223
[45,     1] loss: 0.220
[46,     1] loss: 0.214
[47,     1] loss: 0.213
[48,     1] loss: 0.207
[49,     1] loss: 0.201
[50,     1] loss: 0.200
[51,     1] loss: 0.200
[52,     1] loss: 0.194
[53,     1] loss: 0.193
[54,     1] loss: 0.189
[55,     1] loss: 0.188
[56,     1] loss: 0.185
[57,     1] loss: 0.181
[58,     1] loss: 0.180
[59,     1] loss: 0.178
[60,     1] loss: 0.176
[61,     1] loss: 0.174
[62,     1] loss: 0.171
[63,     1] loss: 0.172
[64,     1] loss: 0.168
[65,     1] loss: 0.168
[66,     1] loss: 0.166
[67,     1] loss: 0.162
[68,     1] loss: 0.160
[69,     1] loss: 0.162
[70,     1] loss: 0.157
[71,     1] loss: 0.157
[72,     1] loss: 0.156
[73,     1] loss: 0.152
[74,     1] loss: 0.152
[75,     1] loss: 0.152
[76,     1] loss: 0.149
[77,     1] loss: 0.149
[78,     1] loss: 0.146
[79,     1] loss: 0.146
[80,     1] loss: 0.147
[81,     1] loss: 0.143
[82,     1] loss: 0.141
[83,     1] loss: 0.141
[84,     1] loss: 0.136
[85,     1] loss: 0.135
[86,     1] loss: 0.131
[87,     1] loss: 0.128
[88,     1] loss: 0.129
[89,     1] loss: 0.135
[90,     1] loss: 0.387
[91,     1] loss: 0.452
[92,     1] loss: 0.325
[93,     1] loss: 0.304
[94,     1] loss: 0.298
[95,     1] loss: 0.288
[96,     1] loss: 0.276
[97,     1] loss: 0.268
[98,     1] loss: 0.258
[99,     1] loss: 0.249
[100,     1] loss: 0.243
[101,     1] loss: 0.229
[102,     1] loss: 0.223
[103,     1] loss: 0.215
[104,     1] loss: 0.206
[105,     1] loss: 0.204
[106,     1] loss: 0.192
Early stopping applied (best metric=0.40531161427497864)
Finished Training
Total time taken: 75.28661823272705
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.681
[3,     1] loss: 0.662
[4,     1] loss: 0.649
[5,     1] loss: 0.637
[6,     1] loss: 0.624
[7,     1] loss: 0.609
[8,     1] loss: 0.593
[9,     1] loss: 0.578
[10,     1] loss: 0.559
[11,     1] loss: 0.540
[12,     1] loss: 0.524
[13,     1] loss: 0.508
[14,     1] loss: 0.487
[15,     1] loss: 0.469
[16,     1] loss: 0.453
[17,     1] loss: 0.438
[18,     1] loss: 0.428
[19,     1] loss: 0.412
[20,     1] loss: 0.403
[21,     1] loss: 0.392
[22,     1] loss: 0.382
[23,     1] loss: 0.374
[24,     1] loss: 0.367
[25,     1] loss: 0.359
[26,     1] loss: 0.355
[27,     1] loss: 0.348
[28,     1] loss: 0.342
[29,     1] loss: 0.340
[30,     1] loss: 0.335
[31,     1] loss: 0.330
[32,     1] loss: 0.326
[33,     1] loss: 0.322
[34,     1] loss: 0.315
[35,     1] loss: 0.311
[36,     1] loss: 0.306
[37,     1] loss: 0.300
[38,     1] loss: 0.295
[39,     1] loss: 0.288
[40,     1] loss: 0.281
[41,     1] loss: 0.273
[42,     1] loss: 0.266
[43,     1] loss: 0.259
[44,     1] loss: 0.253
[45,     1] loss: 0.245
[46,     1] loss: 0.237
[47,     1] loss: 0.230
[48,     1] loss: 0.221
[49,     1] loss: 0.214
[50,     1] loss: 0.207
[51,     1] loss: 0.199
[52,     1] loss: 0.191
[53,     1] loss: 0.186
[54,     1] loss: 0.178
[55,     1] loss: 0.172
[56,     1] loss: 0.165
[57,     1] loss: 0.160
[58,     1] loss: 0.157
[59,     1] loss: 0.152
[60,     1] loss: 0.149
[61,     1] loss: 0.145
[62,     1] loss: 0.141
[63,     1] loss: 0.138
[64,     1] loss: 0.134
[65,     1] loss: 0.133
[66,     1] loss: 0.129
[67,     1] loss: 0.127
[68,     1] loss: 0.126
[69,     1] loss: 0.125
[70,     1] loss: 0.122
[71,     1] loss: 0.121
[72,     1] loss: 0.120
[73,     1] loss: 0.119
[74,     1] loss: 0.116
[75,     1] loss: 0.114
[76,     1] loss: 0.122
[77,     1] loss: 0.173
[78,     1] loss: 0.337
[79,     1] loss: 0.279
[80,     1] loss: 0.246
[81,     1] loss: 0.249
[82,     1] loss: 0.233
[83,     1] loss: 0.236
[84,     1] loss: 0.208
[85,     1] loss: 0.198
[86,     1] loss: 0.192
[87,     1] loss: 0.185
[88,     1] loss: 0.179
[89,     1] loss: 0.174
[90,     1] loss: 0.168
[91,     1] loss: 0.164
[92,     1] loss: 0.155
[93,     1] loss: 0.153
[94,     1] loss: 0.146
[95,     1] loss: 0.143
[96,     1] loss: 0.136
[97,     1] loss: 0.130
[98,     1] loss: 0.125
[99,     1] loss: 0.116
[100,     1] loss: 0.113
[101,     1] loss: 0.107
[102,     1] loss: 0.100
[103,     1] loss: 0.096
[104,     1] loss: 0.092
[105,     1] loss: 0.088
[106,     1] loss: 0.086
[107,     1] loss: 0.082
[108,     1] loss: 0.080
[109,     1] loss: 0.076
[110,     1] loss: 0.074
[111,     1] loss: 0.070
[112,     1] loss: 0.069
[113,     1] loss: 0.068
[114,     1] loss: 0.066
[115,     1] loss: 0.065
[116,     1] loss: 0.065
[117,     1] loss: 0.063
[118,     1] loss: 0.064
[119,     1] loss: 0.062
[120,     1] loss: 0.063
[121,     1] loss: 0.063
[122,     1] loss: 0.063
[123,     1] loss: 0.064
[124,     1] loss: 0.066
[125,     1] loss: 0.064
[126,     1] loss: 0.064
[127,     1] loss: 0.068
[128,     1] loss: 0.066
[129,     1] loss: 0.068
[130,     1] loss: 0.068
[131,     1] loss: 0.067
[132,     1] loss: 0.068
[133,     1] loss: 0.067
[134,     1] loss: 0.070
[135,     1] loss: 0.068
[136,     1] loss: 0.070
[137,     1] loss: 0.070
[138,     1] loss: 0.070
[139,     1] loss: 0.070
[140,     1] loss: 0.070
[141,     1] loss: 0.069
[142,     1] loss: 0.069
[143,     1] loss: 0.070
[144,     1] loss: 0.069
[145,     1] loss: 0.069
[146,     1] loss: 0.069
[147,     1] loss: 0.069
[148,     1] loss: 0.071
[149,     1] loss: 0.069
[150,     1] loss: 0.070
[151,     1] loss: 0.070
[152,     1] loss: 0.070
[153,     1] loss: 0.070
[154,     1] loss: 0.070
[155,     1] loss: 0.070
[156,     1] loss: 0.069
[157,     1] loss: 0.070
[158,     1] loss: 0.068
[159,     1] loss: 0.068
Early stopping applied (best metric=0.36508649587631226)
Finished Training
Total time taken: 113.96329593658447
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.674
[3,     1] loss: 0.656
[4,     1] loss: 0.641
[5,     1] loss: 0.627
[6,     1] loss: 0.611
[7,     1] loss: 0.599
[8,     1] loss: 0.582
[9,     1] loss: 0.568
[10,     1] loss: 0.552
[11,     1] loss: 0.534
[12,     1] loss: 0.516
[13,     1] loss: 0.496
[14,     1] loss: 0.478
[15,     1] loss: 0.456
[16,     1] loss: 0.434
[17,     1] loss: 0.412
[18,     1] loss: 0.386
[19,     1] loss: 0.368
[20,     1] loss: 0.345
[21,     1] loss: 0.326
[22,     1] loss: 0.301
[23,     1] loss: 0.282
[24,     1] loss: 0.267
[25,     1] loss: 0.244
[26,     1] loss: 0.228
[27,     1] loss: 0.215
[28,     1] loss: 0.199
[29,     1] loss: 0.185
[30,     1] loss: 0.172
[31,     1] loss: 0.166
[32,     1] loss: 0.153
[33,     1] loss: 0.144
[34,     1] loss: 0.137
[35,     1] loss: 0.131
[36,     1] loss: 0.126
[37,     1] loss: 0.120
[38,     1] loss: 0.113
[39,     1] loss: 0.112
[40,     1] loss: 0.106
[41,     1] loss: 0.106
[42,     1] loss: 0.103
[43,     1] loss: 0.100
[44,     1] loss: 0.097
[45,     1] loss: 0.095
[46,     1] loss: 0.095
[47,     1] loss: 0.094
[48,     1] loss: 0.092
[49,     1] loss: 0.092
[50,     1] loss: 0.092
[51,     1] loss: 0.093
[52,     1] loss: 0.093
[53,     1] loss: 0.090
[54,     1] loss: 0.092
[55,     1] loss: 0.091
[56,     1] loss: 0.092
[57,     1] loss: 0.091
[58,     1] loss: 0.094
[59,     1] loss: 0.091
[60,     1] loss: 0.093
[61,     1] loss: 0.092
[62,     1] loss: 0.092
[63,     1] loss: 0.092
[64,     1] loss: 0.093
[65,     1] loss: 0.093
[66,     1] loss: 0.092
[67,     1] loss: 0.093
[68,     1] loss: 0.092
[69,     1] loss: 0.093
[70,     1] loss: 0.092
[71,     1] loss: 0.092
[72,     1] loss: 0.092
[73,     1] loss: 0.091
[74,     1] loss: 0.093
[75,     1] loss: 0.091
[76,     1] loss: 0.091
[77,     1] loss: 0.091
[78,     1] loss: 0.091
[79,     1] loss: 0.097
[80,     1] loss: 0.150
[81,     1] loss: 0.761
[82,     1] loss: 0.281
[83,     1] loss: 0.360
Early stopping applied (best metric=0.38494303822517395)
Finished Training
Total time taken: 60.45167922973633
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.676
[4,     1] loss: 0.664
[5,     1] loss: 0.653
[6,     1] loss: 0.644
[7,     1] loss: 0.632
[8,     1] loss: 0.621
[9,     1] loss: 0.607
[10,     1] loss: 0.594
[11,     1] loss: 0.580
[12,     1] loss: 0.564
[13,     1] loss: 0.544
[14,     1] loss: 0.528
[15,     1] loss: 0.510
[16,     1] loss: 0.487
[17,     1] loss: 0.467
[18,     1] loss: 0.447
[19,     1] loss: 0.427
[20,     1] loss: 0.407
[21,     1] loss: 0.384
[22,     1] loss: 0.366
[23,     1] loss: 0.348
[24,     1] loss: 0.327
[25,     1] loss: 0.311
[26,     1] loss: 0.291
[27,     1] loss: 0.277
[28,     1] loss: 0.255
[29,     1] loss: 0.243
[30,     1] loss: 0.227
[31,     1] loss: 0.216
[32,     1] loss: 0.205
[33,     1] loss: 0.191
[34,     1] loss: 0.185
[35,     1] loss: 0.171
[36,     1] loss: 0.162
[37,     1] loss: 0.153
[38,     1] loss: 0.147
[39,     1] loss: 0.141
[40,     1] loss: 0.135
[41,     1] loss: 0.127
[42,     1] loss: 0.123
[43,     1] loss: 0.121
[44,     1] loss: 0.117
[45,     1] loss: 0.113
[46,     1] loss: 0.108
[47,     1] loss: 0.105
[48,     1] loss: 0.104
[49,     1] loss: 0.102
[50,     1] loss: 0.099
[51,     1] loss: 0.096
[52,     1] loss: 0.096
[53,     1] loss: 0.094
[54,     1] loss: 0.094
[55,     1] loss: 0.093
[56,     1] loss: 0.091
[57,     1] loss: 0.089
[58,     1] loss: 0.089
[59,     1] loss: 0.089
[60,     1] loss: 0.089
[61,     1] loss: 0.090
[62,     1] loss: 0.086
[63,     1] loss: 0.087
[64,     1] loss: 0.088
[65,     1] loss: 0.105
[66,     1] loss: 0.200
[67,     1] loss: 0.187
[68,     1] loss: 0.217
[69,     1] loss: 0.156
[70,     1] loss: 0.179
[71,     1] loss: 0.158
[72,     1] loss: 0.150
[73,     1] loss: 0.144
[74,     1] loss: 0.140
[75,     1] loss: 0.139
[76,     1] loss: 0.133
[77,     1] loss: 0.129
[78,     1] loss: 0.124
[79,     1] loss: 0.121
[80,     1] loss: 0.116
[81,     1] loss: 0.113
[82,     1] loss: 0.110
[83,     1] loss: 0.106
[84,     1] loss: 0.104
[85,     1] loss: 0.101
[86,     1] loss: 0.100
[87,     1] loss: 0.096
[88,     1] loss: 0.095
[89,     1] loss: 0.093
[90,     1] loss: 0.090
[91,     1] loss: 0.089
[92,     1] loss: 0.089
[93,     1] loss: 0.086
[94,     1] loss: 0.082
[95,     1] loss: 0.078
[96,     1] loss: 0.075
[97,     1] loss: 0.071
[98,     1] loss: 0.071
[99,     1] loss: 0.071
[100,     1] loss: 0.068
[101,     1] loss: 0.066
[102,     1] loss: 0.064
[103,     1] loss: 0.064
[104,     1] loss: 0.064
[105,     1] loss: 0.065
[106,     1] loss: 0.063
[107,     1] loss: 0.064
[108,     1] loss: 0.063
[109,     1] loss: 0.063
[110,     1] loss: 0.063
[111,     1] loss: 0.062
[112,     1] loss: 0.064
[113,     1] loss: 0.064
[114,     1] loss: 0.063
[115,     1] loss: 0.067
[116,     1] loss: 0.066
[117,     1] loss: 0.065
[118,     1] loss: 0.065
[119,     1] loss: 0.066
Early stopping applied (best metric=0.3728543221950531)
Finished Training
Total time taken: 86.90983653068542
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.680
[3,     1] loss: 0.661
[4,     1] loss: 0.646
[5,     1] loss: 0.634
[6,     1] loss: 0.624
[7,     1] loss: 0.611
[8,     1] loss: 0.598
[9,     1] loss: 0.587
[10,     1] loss: 0.574
[11,     1] loss: 0.562
[12,     1] loss: 0.548
[13,     1] loss: 0.532
[14,     1] loss: 0.516
[15,     1] loss: 0.501
[16,     1] loss: 0.486
[17,     1] loss: 0.469
[18,     1] loss: 0.452
[19,     1] loss: 0.433
[20,     1] loss: 0.416
[21,     1] loss: 0.399
[22,     1] loss: 0.379
[23,     1] loss: 0.361
[24,     1] loss: 0.341
[25,     1] loss: 0.325
[26,     1] loss: 0.306
[27,     1] loss: 0.289
[28,     1] loss: 0.277
[29,     1] loss: 0.260
[30,     1] loss: 0.249
[31,     1] loss: 0.236
[32,     1] loss: 0.224
[33,     1] loss: 0.214
[34,     1] loss: 0.202
[35,     1] loss: 0.193
[36,     1] loss: 0.183
[37,     1] loss: 0.174
[38,     1] loss: 0.168
[39,     1] loss: 0.161
[40,     1] loss: 0.154
[41,     1] loss: 0.147
[42,     1] loss: 0.141
[43,     1] loss: 0.136
[44,     1] loss: 0.131
[45,     1] loss: 0.130
[46,     1] loss: 0.123
[47,     1] loss: 0.119
[48,     1] loss: 0.117
[49,     1] loss: 0.114
[50,     1] loss: 0.112
[51,     1] loss: 0.108
[52,     1] loss: 0.105
[53,     1] loss: 0.101
[54,     1] loss: 0.103
[55,     1] loss: 0.101
[56,     1] loss: 0.099
[57,     1] loss: 0.097
[58,     1] loss: 0.096
[59,     1] loss: 0.096
[60,     1] loss: 0.093
[61,     1] loss: 0.094
[62,     1] loss: 0.093
[63,     1] loss: 0.093
[64,     1] loss: 0.091
[65,     1] loss: 0.090
[66,     1] loss: 0.089
[67,     1] loss: 0.088
[68,     1] loss: 0.087
[69,     1] loss: 0.086
[70,     1] loss: 0.087
[71,     1] loss: 0.085
[72,     1] loss: 0.086
[73,     1] loss: 0.084
[74,     1] loss: 0.083
[75,     1] loss: 0.083
[76,     1] loss: 0.083
[77,     1] loss: 0.080
[78,     1] loss: 0.079
[79,     1] loss: 0.079
[80,     1] loss: 0.078
[81,     1] loss: 0.078
Early stopping applied (best metric=0.4069805145263672)
Finished Training
Total time taken: 59.72419857978821
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.654
[4,     1] loss: 0.639
[5,     1] loss: 0.624
[6,     1] loss: 0.611
[7,     1] loss: 0.597
[8,     1] loss: 0.583
[9,     1] loss: 0.568
[10,     1] loss: 0.548
[11,     1] loss: 0.530
[12,     1] loss: 0.512
[13,     1] loss: 0.489
[14,     1] loss: 0.472
[15,     1] loss: 0.450
[16,     1] loss: 0.428
[17,     1] loss: 0.412
[18,     1] loss: 0.393
[19,     1] loss: 0.378
[20,     1] loss: 0.360
[21,     1] loss: 0.347
[22,     1] loss: 0.333
[23,     1] loss: 0.320
[24,     1] loss: 0.309
[25,     1] loss: 0.297
[26,     1] loss: 0.282
[27,     1] loss: 0.272
[28,     1] loss: 0.263
[29,     1] loss: 0.251
[30,     1] loss: 0.240
[31,     1] loss: 0.232
[32,     1] loss: 0.219
[33,     1] loss: 0.215
[34,     1] loss: 0.206
[35,     1] loss: 0.197
[36,     1] loss: 0.191
[37,     1] loss: 0.186
[38,     1] loss: 0.180
[39,     1] loss: 0.173
[40,     1] loss: 0.169
[41,     1] loss: 0.164
[42,     1] loss: 0.161
[43,     1] loss: 0.156
[44,     1] loss: 0.152
[45,     1] loss: 0.149
[46,     1] loss: 0.146
[47,     1] loss: 0.143
[48,     1] loss: 0.140
[49,     1] loss: 0.138
[50,     1] loss: 0.135
[51,     1] loss: 0.134
[52,     1] loss: 0.132
[53,     1] loss: 0.128
[54,     1] loss: 0.128
[55,     1] loss: 0.125
[56,     1] loss: 0.124
[57,     1] loss: 0.122
[58,     1] loss: 0.120
[59,     1] loss: 0.119
[60,     1] loss: 0.118
[61,     1] loss: 0.115
[62,     1] loss: 0.116
[63,     1] loss: 0.115
[64,     1] loss: 0.114
[65,     1] loss: 0.112
[66,     1] loss: 0.112
[67,     1] loss: 0.111
[68,     1] loss: 0.110
[69,     1] loss: 0.108
[70,     1] loss: 0.108
[71,     1] loss: 0.109
[72,     1] loss: 0.107
[73,     1] loss: 0.106
[74,     1] loss: 0.107
[75,     1] loss: 0.106
[76,     1] loss: 0.107
[77,     1] loss: 0.106
[78,     1] loss: 0.105
[79,     1] loss: 0.104
[80,     1] loss: 0.104
[81,     1] loss: 0.103
[82,     1] loss: 0.104
[83,     1] loss: 0.105
[84,     1] loss: 0.103
[85,     1] loss: 0.104
[86,     1] loss: 0.103
[87,     1] loss: 0.104
[88,     1] loss: 0.104
[89,     1] loss: 0.105
[90,     1] loss: 0.128
[91,     1] loss: 0.355
[92,     1] loss: 0.760
[93,     1] loss: 0.655
[94,     1] loss: 0.464
[95,     1] loss: 0.390
[96,     1] loss: 0.399
[97,     1] loss: 0.395
[98,     1] loss: 0.383
[99,     1] loss: 0.375
[100,     1] loss: 0.369
[101,     1] loss: 0.362
[102,     1] loss: 0.353
[103,     1] loss: 0.337
[104,     1] loss: 0.322
[105,     1] loss: 0.309
[106,     1] loss: 0.294
[107,     1] loss: 0.284
[108,     1] loss: 0.267
[109,     1] loss: 0.259
[110,     1] loss: 0.246
[111,     1] loss: 0.241
[112,     1] loss: 0.231
[113,     1] loss: 0.225
[114,     1] loss: 0.216
[115,     1] loss: 0.210
[116,     1] loss: 0.204
[117,     1] loss: 0.195
[118,     1] loss: 0.191
[119,     1] loss: 0.184
[120,     1] loss: 0.178
[121,     1] loss: 0.170
[122,     1] loss: 0.159
[123,     1] loss: 0.157
[124,     1] loss: 0.146
[125,     1] loss: 0.141
[126,     1] loss: 0.133
[127,     1] loss: 0.131
[128,     1] loss: 0.121
[129,     1] loss: 0.116
[130,     1] loss: 0.114
[131,     1] loss: 0.113
[132,     1] loss: 0.105
[133,     1] loss: 0.101
[134,     1] loss: 0.100
[135,     1] loss: 0.097
[136,     1] loss: 0.094
[137,     1] loss: 0.094
[138,     1] loss: 0.091
[139,     1] loss: 0.092
[140,     1] loss: 0.089
[141,     1] loss: 0.088
[142,     1] loss: 0.087
[143,     1] loss: 0.085
[144,     1] loss: 0.085
[145,     1] loss: 0.083
Early stopping applied (best metric=0.41535094380378723)
Finished Training
Total time taken: 107.78784108161926
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.673
[3,     1] loss: 0.654
[4,     1] loss: 0.643
[5,     1] loss: 0.632
[6,     1] loss: 0.619
[7,     1] loss: 0.606
[8,     1] loss: 0.594
[9,     1] loss: 0.579
[10,     1] loss: 0.570
[11,     1] loss: 0.554
[12,     1] loss: 0.539
[13,     1] loss: 0.523
[14,     1] loss: 0.507
[15,     1] loss: 0.491
[16,     1] loss: 0.469
[17,     1] loss: 0.452
[18,     1] loss: 0.432
[19,     1] loss: 0.414
[20,     1] loss: 0.397
[21,     1] loss: 0.377
[22,     1] loss: 0.359
[23,     1] loss: 0.340
[24,     1] loss: 0.326
[25,     1] loss: 0.306
[26,     1] loss: 0.293
[27,     1] loss: 0.274
[28,     1] loss: 0.263
[29,     1] loss: 0.246
[30,     1] loss: 0.232
[31,     1] loss: 0.222
[32,     1] loss: 0.211
[33,     1] loss: 0.199
[34,     1] loss: 0.187
[35,     1] loss: 0.177
[36,     1] loss: 0.167
[37,     1] loss: 0.162
[38,     1] loss: 0.152
[39,     1] loss: 0.145
[40,     1] loss: 0.143
[41,     1] loss: 0.134
[42,     1] loss: 0.132
[43,     1] loss: 0.126
[44,     1] loss: 0.121
[45,     1] loss: 0.120
[46,     1] loss: 0.116
[47,     1] loss: 0.115
[48,     1] loss: 0.114
[49,     1] loss: 0.110
[50,     1] loss: 0.107
[51,     1] loss: 0.108
[52,     1] loss: 0.105
[53,     1] loss: 0.103
[54,     1] loss: 0.104
[55,     1] loss: 0.103
[56,     1] loss: 0.102
[57,     1] loss: 0.099
[58,     1] loss: 0.099
[59,     1] loss: 0.100
[60,     1] loss: 0.100
[61,     1] loss: 0.100
[62,     1] loss: 0.099
[63,     1] loss: 0.101
[64,     1] loss: 0.100
[65,     1] loss: 0.102
Early stopping applied (best metric=0.45016416907310486)
Finished Training
Total time taken: 49.12400484085083
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.664
[4,     1] loss: 0.653
[5,     1] loss: 0.642
[6,     1] loss: 0.630
[7,     1] loss: 0.620
[8,     1] loss: 0.608
[9,     1] loss: 0.595
[10,     1] loss: 0.581
[11,     1] loss: 0.568
[12,     1] loss: 0.553
[13,     1] loss: 0.537
[14,     1] loss: 0.519
[15,     1] loss: 0.504
[16,     1] loss: 0.488
[17,     1] loss: 0.468
[18,     1] loss: 0.451
[19,     1] loss: 0.431
[20,     1] loss: 0.411
[21,     1] loss: 0.391
[22,     1] loss: 0.371
[23,     1] loss: 0.353
[24,     1] loss: 0.332
[25,     1] loss: 0.314
[26,     1] loss: 0.296
[27,     1] loss: 0.278
[28,     1] loss: 0.260
[29,     1] loss: 0.244
[30,     1] loss: 0.229
[31,     1] loss: 0.215
[32,     1] loss: 0.202
[33,     1] loss: 0.187
[34,     1] loss: 0.178
[35,     1] loss: 0.168
[36,     1] loss: 0.159
[37,     1] loss: 0.151
[38,     1] loss: 0.142
[39,     1] loss: 0.136
[40,     1] loss: 0.130
[41,     1] loss: 0.122
[42,     1] loss: 0.122
[43,     1] loss: 0.117
[44,     1] loss: 0.111
[45,     1] loss: 0.109
[46,     1] loss: 0.105
[47,     1] loss: 0.102
[48,     1] loss: 0.102
[49,     1] loss: 0.103
[50,     1] loss: 0.098
[51,     1] loss: 0.096
[52,     1] loss: 0.099
[53,     1] loss: 0.096
[54,     1] loss: 0.094
[55,     1] loss: 0.096
[56,     1] loss: 0.094
[57,     1] loss: 0.095
[58,     1] loss: 0.094
[59,     1] loss: 0.094
[60,     1] loss: 0.094
[61,     1] loss: 0.094
[62,     1] loss: 0.093
[63,     1] loss: 0.095
[64,     1] loss: 0.095
[65,     1] loss: 0.096
[66,     1] loss: 0.103
[67,     1] loss: 0.152
[68,     1] loss: 0.274
[69,     1] loss: 0.271
[70,     1] loss: 0.279
[71,     1] loss: 0.229
[72,     1] loss: 0.223
[73,     1] loss: 0.213
[74,     1] loss: 0.198
[75,     1] loss: 0.190
[76,     1] loss: 0.182
[77,     1] loss: 0.174
[78,     1] loss: 0.166
[79,     1] loss: 0.160
[80,     1] loss: 0.153
[81,     1] loss: 0.146
[82,     1] loss: 0.142
[83,     1] loss: 0.136
[84,     1] loss: 0.132
[85,     1] loss: 0.128
[86,     1] loss: 0.126
[87,     1] loss: 0.121
[88,     1] loss: 0.120
[89,     1] loss: 0.118
[90,     1] loss: 0.116
[91,     1] loss: 0.112
[92,     1] loss: 0.111
[93,     1] loss: 0.110
[94,     1] loss: 0.109
[95,     1] loss: 0.108
[96,     1] loss: 0.108
[97,     1] loss: 0.106
[98,     1] loss: 0.104
[99,     1] loss: 0.105
[100,     1] loss: 0.101
[101,     1] loss: 0.098
[102,     1] loss: 0.097
[103,     1] loss: 0.095
[104,     1] loss: 0.091
[105,     1] loss: 0.090
[106,     1] loss: 0.088
[107,     1] loss: 0.087
[108,     1] loss: 0.087
[109,     1] loss: 0.086
[110,     1] loss: 0.082
[111,     1] loss: 0.082
[112,     1] loss: 0.081
[113,     1] loss: 0.081
[114,     1] loss: 0.080
[115,     1] loss: 0.079
[116,     1] loss: 0.079
[117,     1] loss: 0.081
[118,     1] loss: 0.080
[119,     1] loss: 0.080
[120,     1] loss: 0.081
[121,     1] loss: 0.080
[122,     1] loss: 0.079
[123,     1] loss: 0.081
[124,     1] loss: 0.079
[125,     1] loss: 0.082
[126,     1] loss: 0.079
[127,     1] loss: 0.079
[128,     1] loss: 0.081
[129,     1] loss: 0.083
[130,     1] loss: 0.084
[131,     1] loss: 0.082
[132,     1] loss: 0.083
[133,     1] loss: 0.081
[134,     1] loss: 0.083
[135,     1] loss: 0.082
[136,     1] loss: 0.082
[137,     1] loss: 0.082
[138,     1] loss: 0.084
[139,     1] loss: 0.085
[140,     1] loss: 0.084
[141,     1] loss: 0.087
[142,     1] loss: 0.084
[143,     1] loss: 0.084
[144,     1] loss: 0.084
[145,     1] loss: 0.084
[146,     1] loss: 0.085
[147,     1] loss: 0.083
[148,     1] loss: 0.083
[149,     1] loss: 0.083
[150,     1] loss: 0.083
[151,     1] loss: 0.083
[152,     1] loss: 0.083
[153,     1] loss: 0.083
[154,     1] loss: 0.083
[155,     1] loss: 0.085
[156,     1] loss: 0.085
[157,     1] loss: 0.086
[158,     1] loss: 0.126
[159,     1] loss: 0.349
[160,     1] loss: 0.573
[161,     1] loss: 0.525
[162,     1] loss: 0.438
[163,     1] loss: 0.415
[164,     1] loss: 0.414
[165,     1] loss: 0.397
[166,     1] loss: 0.387
[167,     1] loss: 0.381
[168,     1] loss: 0.359
[169,     1] loss: 0.346
[170,     1] loss: 0.330
[171,     1] loss: 0.312
[172,     1] loss: 0.290
[173,     1] loss: 0.269
[174,     1] loss: 0.254
[175,     1] loss: 0.241
[176,     1] loss: 0.223
[177,     1] loss: 0.214
[178,     1] loss: 0.205
[179,     1] loss: 0.193
[180,     1] loss: 0.184
[181,     1] loss: 0.175
[182,     1] loss: 0.169
[183,     1] loss: 0.162
[184,     1] loss: 0.149
[185,     1] loss: 0.142
[186,     1] loss: 0.135
[187,     1] loss: 0.128
[188,     1] loss: 0.123
[189,     1] loss: 0.119
[190,     1] loss: 0.118
[191,     1] loss: 0.112
[192,     1] loss: 0.112
[193,     1] loss: 0.106
[194,     1] loss: 0.106
[195,     1] loss: 0.104
[196,     1] loss: 0.102
[197,     1] loss: 0.100
[198,     1] loss: 0.101
[199,     1] loss: 0.101
[200,     1] loss: 0.099
Finished Training
Total time taken: 149.77539348602295
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.681
[3,     1] loss: 0.660
[4,     1] loss: 0.639
[5,     1] loss: 0.621
[6,     1] loss: 0.605
[7,     1] loss: 0.589
[8,     1] loss: 0.572
[9,     1] loss: 0.556
[10,     1] loss: 0.536
[11,     1] loss: 0.519
[12,     1] loss: 0.496
[13,     1] loss: 0.479
[14,     1] loss: 0.457
[15,     1] loss: 0.436
[16,     1] loss: 0.412
[17,     1] loss: 0.390
[18,     1] loss: 0.368
[19,     1] loss: 0.349
[20,     1] loss: 0.327
[21,     1] loss: 0.305
[22,     1] loss: 0.284
[23,     1] loss: 0.269
[24,     1] loss: 0.249
[25,     1] loss: 0.231
[26,     1] loss: 0.215
[27,     1] loss: 0.200
[28,     1] loss: 0.187
[29,     1] loss: 0.174
[30,     1] loss: 0.166
[31,     1] loss: 0.156
[32,     1] loss: 0.148
[33,     1] loss: 0.140
[34,     1] loss: 0.134
[35,     1] loss: 0.127
[36,     1] loss: 0.123
[37,     1] loss: 0.117
[38,     1] loss: 0.115
[39,     1] loss: 0.112
[40,     1] loss: 0.109
[41,     1] loss: 0.104
[42,     1] loss: 0.104
[43,     1] loss: 0.102
[44,     1] loss: 0.101
[45,     1] loss: 0.099
[46,     1] loss: 0.096
[47,     1] loss: 0.095
[48,     1] loss: 0.094
[49,     1] loss: 0.094
[50,     1] loss: 0.092
[51,     1] loss: 0.093
[52,     1] loss: 0.092
[53,     1] loss: 0.090
[54,     1] loss: 0.092
[55,     1] loss: 0.091
[56,     1] loss: 0.089
[57,     1] loss: 0.090
[58,     1] loss: 0.091
[59,     1] loss: 0.090
[60,     1] loss: 0.090
[61,     1] loss: 0.090
[62,     1] loss: 0.090
[63,     1] loss: 0.089
[64,     1] loss: 0.089
[65,     1] loss: 0.087
[66,     1] loss: 0.087
[67,     1] loss: 0.086
[68,     1] loss: 0.089
[69,     1] loss: 0.085
[70,     1] loss: 0.086
[71,     1] loss: 0.085
[72,     1] loss: 0.085
Early stopping applied (best metric=0.4003216624259949)
Finished Training
Total time taken: 55.21462965011597
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.674
[4,     1] loss: 0.662
[5,     1] loss: 0.652
[6,     1] loss: 0.642
[7,     1] loss: 0.630
[8,     1] loss: 0.618
[9,     1] loss: 0.606
[10,     1] loss: 0.592
[11,     1] loss: 0.576
[12,     1] loss: 0.561
[13,     1] loss: 0.545
[14,     1] loss: 0.527
[15,     1] loss: 0.510
[16,     1] loss: 0.490
[17,     1] loss: 0.471
[18,     1] loss: 0.453
[19,     1] loss: 0.432
[20,     1] loss: 0.414
[21,     1] loss: 0.393
[22,     1] loss: 0.378
[23,     1] loss: 0.357
[24,     1] loss: 0.340
[25,     1] loss: 0.321
[26,     1] loss: 0.303
[27,     1] loss: 0.284
[28,     1] loss: 0.271
[29,     1] loss: 0.257
[30,     1] loss: 0.240
[31,     1] loss: 0.229
[32,     1] loss: 0.214
[33,     1] loss: 0.202
[34,     1] loss: 0.191
[35,     1] loss: 0.180
[36,     1] loss: 0.173
[37,     1] loss: 0.165
[38,     1] loss: 0.156
[39,     1] loss: 0.148
[40,     1] loss: 0.145
[41,     1] loss: 0.137
[42,     1] loss: 0.133
[43,     1] loss: 0.130
[44,     1] loss: 0.128
[45,     1] loss: 0.122
[46,     1] loss: 0.120
[47,     1] loss: 0.118
[48,     1] loss: 0.115
[49,     1] loss: 0.114
[50,     1] loss: 0.112
[51,     1] loss: 0.109
[52,     1] loss: 0.108
[53,     1] loss: 0.108
[54,     1] loss: 0.107
[55,     1] loss: 0.106
[56,     1] loss: 0.106
[57,     1] loss: 0.108
[58,     1] loss: 0.106
[59,     1] loss: 0.106
[60,     1] loss: 0.106
[61,     1] loss: 0.106
[62,     1] loss: 0.105
[63,     1] loss: 0.105
[64,     1] loss: 0.107
[65,     1] loss: 0.107
[66,     1] loss: 0.105
[67,     1] loss: 0.105
[68,     1] loss: 0.107
[69,     1] loss: 0.106
[70,     1] loss: 0.107
[71,     1] loss: 0.106
[72,     1] loss: 0.105
[73,     1] loss: 0.106
[74,     1] loss: 0.105
[75,     1] loss: 0.106
[76,     1] loss: 0.103
[77,     1] loss: 0.104
[78,     1] loss: 0.103
[79,     1] loss: 0.105
[80,     1] loss: 0.103
[81,     1] loss: 0.103
[82,     1] loss: 0.105
[83,     1] loss: 0.105
[84,     1] loss: 0.105
[85,     1] loss: 0.143
[86,     1] loss: 0.293
[87,     1] loss: 0.562
[88,     1] loss: 0.491
Early stopping applied (best metric=0.39847812056541443)
Finished Training
Total time taken: 67.68977069854736
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.663
[5,     1] loss: 0.653
[6,     1] loss: 0.641
[7,     1] loss: 0.628
[8,     1] loss: 0.615
[9,     1] loss: 0.604
[10,     1] loss: 0.589
[11,     1] loss: 0.575
[12,     1] loss: 0.561
[13,     1] loss: 0.544
[14,     1] loss: 0.528
[15,     1] loss: 0.509
[16,     1] loss: 0.490
[17,     1] loss: 0.470
[18,     1] loss: 0.452
[19,     1] loss: 0.432
[20,     1] loss: 0.411
[21,     1] loss: 0.393
[22,     1] loss: 0.371
[23,     1] loss: 0.349
[24,     1] loss: 0.333
[25,     1] loss: 0.313
[26,     1] loss: 0.292
[27,     1] loss: 0.276
[28,     1] loss: 0.258
[29,     1] loss: 0.242
[30,     1] loss: 0.229
[31,     1] loss: 0.214
[32,     1] loss: 0.202
[33,     1] loss: 0.192
[34,     1] loss: 0.179
[35,     1] loss: 0.169
[36,     1] loss: 0.163
[37,     1] loss: 0.154
[38,     1] loss: 0.146
[39,     1] loss: 0.139
[40,     1] loss: 0.132
[41,     1] loss: 0.127
[42,     1] loss: 0.124
[43,     1] loss: 0.118
[44,     1] loss: 0.116
[45,     1] loss: 0.111
[46,     1] loss: 0.110
[47,     1] loss: 0.105
[48,     1] loss: 0.105
[49,     1] loss: 0.102
[50,     1] loss: 0.101
[51,     1] loss: 0.100
[52,     1] loss: 0.100
[53,     1] loss: 0.099
[54,     1] loss: 0.098
[55,     1] loss: 0.098
[56,     1] loss: 0.098
[57,     1] loss: 0.098
[58,     1] loss: 0.099
[59,     1] loss: 0.097
[60,     1] loss: 0.097
[61,     1] loss: 0.095
[62,     1] loss: 0.097
[63,     1] loss: 0.097
[64,     1] loss: 0.096
[65,     1] loss: 0.096
[66,     1] loss: 0.097
[67,     1] loss: 0.097
[68,     1] loss: 0.098
[69,     1] loss: 0.097
[70,     1] loss: 0.097
[71,     1] loss: 0.117
[72,     1] loss: 0.193
[73,     1] loss: 0.396
[74,     1] loss: 0.225
[75,     1] loss: 0.261
[76,     1] loss: 0.233
[77,     1] loss: 0.209
[78,     1] loss: 0.201
[79,     1] loss: 0.186
Early stopping applied (best metric=0.4126078486442566)
Finished Training
Total time taken: 61.23581624031067
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.638
[6,     1] loss: 0.623
[7,     1] loss: 0.613
[8,     1] loss: 0.598
[9,     1] loss: 0.582
[10,     1] loss: 0.568
[11,     1] loss: 0.551
[12,     1] loss: 0.536
[13,     1] loss: 0.517
[14,     1] loss: 0.498
[15,     1] loss: 0.480
[16,     1] loss: 0.458
[17,     1] loss: 0.440
[18,     1] loss: 0.419
[19,     1] loss: 0.399
[20,     1] loss: 0.380
[21,     1] loss: 0.359
[22,     1] loss: 0.341
[23,     1] loss: 0.325
[24,     1] loss: 0.301
[25,     1] loss: 0.287
[26,     1] loss: 0.271
[27,     1] loss: 0.254
[28,     1] loss: 0.240
[29,     1] loss: 0.229
[30,     1] loss: 0.222
[31,     1] loss: 0.211
[32,     1] loss: 0.195
[33,     1] loss: 0.191
[34,     1] loss: 0.176
[35,     1] loss: 0.170
[36,     1] loss: 0.163
[37,     1] loss: 0.154
[38,     1] loss: 0.149
[39,     1] loss: 0.144
[40,     1] loss: 0.137
[41,     1] loss: 0.132
[42,     1] loss: 0.128
[43,     1] loss: 0.125
[44,     1] loss: 0.122
[45,     1] loss: 0.116
[46,     1] loss: 0.112
[47,     1] loss: 0.111
[48,     1] loss: 0.108
[49,     1] loss: 0.107
[50,     1] loss: 0.105
[51,     1] loss: 0.103
[52,     1] loss: 0.099
[53,     1] loss: 0.099
[54,     1] loss: 0.097
[55,     1] loss: 0.097
[56,     1] loss: 0.097
[57,     1] loss: 0.097
[58,     1] loss: 0.097
[59,     1] loss: 0.096
[60,     1] loss: 0.095
[61,     1] loss: 0.092
[62,     1] loss: 0.092
[63,     1] loss: 0.092
[64,     1] loss: 0.092
[65,     1] loss: 0.092
[66,     1] loss: 0.088
[67,     1] loss: 0.089
[68,     1] loss: 0.091
[69,     1] loss: 0.092
[70,     1] loss: 0.090
[71,     1] loss: 0.089
[72,     1] loss: 0.088
[73,     1] loss: 0.089
[74,     1] loss: 0.089
[75,     1] loss: 0.089
[76,     1] loss: 0.088
[77,     1] loss: 0.088
[78,     1] loss: 0.087
[79,     1] loss: 0.086
[80,     1] loss: 0.085
[81,     1] loss: 0.084
[82,     1] loss: 0.085
[83,     1] loss: 0.086
Early stopping applied (best metric=0.39163917303085327)
Finished Training
Total time taken: 68.96778416633606
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.669
[4,     1] loss: 0.655
[5,     1] loss: 0.643
[6,     1] loss: 0.630
[7,     1] loss: 0.615
[8,     1] loss: 0.600
[9,     1] loss: 0.584
[10,     1] loss: 0.567
[11,     1] loss: 0.551
[12,     1] loss: 0.532
[13,     1] loss: 0.515
[14,     1] loss: 0.493
[15,     1] loss: 0.476
[16,     1] loss: 0.455
[17,     1] loss: 0.434
[18,     1] loss: 0.414
[19,     1] loss: 0.395
[20,     1] loss: 0.374
[21,     1] loss: 0.357
[22,     1] loss: 0.340
[23,     1] loss: 0.328
[24,     1] loss: 0.312
[25,     1] loss: 0.302
[26,     1] loss: 0.290
[27,     1] loss: 0.283
[28,     1] loss: 0.272
[29,     1] loss: 0.264
[30,     1] loss: 0.258
[31,     1] loss: 0.250
[32,     1] loss: 0.242
[33,     1] loss: 0.234
[34,     1] loss: 0.225
[35,     1] loss: 0.219
[36,     1] loss: 0.211
[37,     1] loss: 0.205
[38,     1] loss: 0.201
[39,     1] loss: 0.197
[40,     1] loss: 0.192
[41,     1] loss: 0.182
[42,     1] loss: 0.179
[43,     1] loss: 0.172
[44,     1] loss: 0.169
[45,     1] loss: 0.164
[46,     1] loss: 0.157
[47,     1] loss: 0.150
[48,     1] loss: 0.145
[49,     1] loss: 0.143
[50,     1] loss: 0.139
[51,     1] loss: 0.133
[52,     1] loss: 0.130
[53,     1] loss: 0.126
[54,     1] loss: 0.122
[55,     1] loss: 0.119
[56,     1] loss: 0.116
[57,     1] loss: 0.113
[58,     1] loss: 0.109
[59,     1] loss: 0.107
[60,     1] loss: 0.105
[61,     1] loss: 0.103
[62,     1] loss: 0.101
[63,     1] loss: 0.098
[64,     1] loss: 0.097
[65,     1] loss: 0.095
[66,     1] loss: 0.094
[67,     1] loss: 0.090
[68,     1] loss: 0.089
[69,     1] loss: 0.088
[70,     1] loss: 0.087
[71,     1] loss: 0.086
[72,     1] loss: 0.083
[73,     1] loss: 0.081
[74,     1] loss: 0.080
[75,     1] loss: 0.083
[76,     1] loss: 0.079
[77,     1] loss: 0.079
[78,     1] loss: 0.077
[79,     1] loss: 0.077
[80,     1] loss: 0.076
[81,     1] loss: 0.073
[82,     1] loss: 0.074
[83,     1] loss: 0.071
[84,     1] loss: 0.070
[85,     1] loss: 0.069
[86,     1] loss: 0.067
[87,     1] loss: 0.067
[88,     1] loss: 0.070
[89,     1] loss: 0.194
[90,     1] loss: 0.449
[91,     1] loss: 0.300
[92,     1] loss: 0.583
[93,     1] loss: 0.389
[94,     1] loss: 0.438
[95,     1] loss: 0.427
[96,     1] loss: 0.405
[97,     1] loss: 0.400
[98,     1] loss: 0.395
[99,     1] loss: 0.399
[100,     1] loss: 0.398
[101,     1] loss: 0.396
[102,     1] loss: 0.389
[103,     1] loss: 0.386
[104,     1] loss: 0.378
[105,     1] loss: 0.370
[106,     1] loss: 0.360
[107,     1] loss: 0.348
[108,     1] loss: 0.335
[109,     1] loss: 0.323
[110,     1] loss: 0.308
[111,     1] loss: 0.296
[112,     1] loss: 0.282
[113,     1] loss: 0.268
[114,     1] loss: 0.255
[115,     1] loss: 0.244
[116,     1] loss: 0.231
[117,     1] loss: 0.221
[118,     1] loss: 0.208
[119,     1] loss: 0.199
[120,     1] loss: 0.190
[121,     1] loss: 0.180
[122,     1] loss: 0.175
[123,     1] loss: 0.168
[124,     1] loss: 0.161
[125,     1] loss: 0.157
[126,     1] loss: 0.152
[127,     1] loss: 0.149
[128,     1] loss: 0.142
[129,     1] loss: 0.139
[130,     1] loss: 0.136
Early stopping applied (best metric=0.3241128921508789)
Finished Training
Total time taken: 110.04189372062683
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.664
[4,     1] loss: 0.652
[5,     1] loss: 0.641
[6,     1] loss: 0.629
[7,     1] loss: 0.618
[8,     1] loss: 0.606
[9,     1] loss: 0.593
[10,     1] loss: 0.582
[11,     1] loss: 0.565
[12,     1] loss: 0.549
[13,     1] loss: 0.534
[14,     1] loss: 0.518
[15,     1] loss: 0.502
[16,     1] loss: 0.484
[17,     1] loss: 0.465
[18,     1] loss: 0.449
[19,     1] loss: 0.431
[20,     1] loss: 0.419
[21,     1] loss: 0.404
[22,     1] loss: 0.387
[23,     1] loss: 0.368
[24,     1] loss: 0.354
[25,     1] loss: 0.342
[26,     1] loss: 0.327
[27,     1] loss: 0.314
[28,     1] loss: 0.298
[29,     1] loss: 0.286
[30,     1] loss: 0.273
[31,     1] loss: 0.261
[32,     1] loss: 0.250
[33,     1] loss: 0.237
[34,     1] loss: 0.225
[35,     1] loss: 0.217
[36,     1] loss: 0.207
[37,     1] loss: 0.197
[38,     1] loss: 0.189
[39,     1] loss: 0.180
[40,     1] loss: 0.172
[41,     1] loss: 0.165
[42,     1] loss: 0.161
[43,     1] loss: 0.154
[44,     1] loss: 0.148
[45,     1] loss: 0.143
[46,     1] loss: 0.139
[47,     1] loss: 0.135
[48,     1] loss: 0.131
[49,     1] loss: 0.131
[50,     1] loss: 0.125
[51,     1] loss: 0.125
[52,     1] loss: 0.122
[53,     1] loss: 0.121
[54,     1] loss: 0.119
[55,     1] loss: 0.116
[56,     1] loss: 0.115
[57,     1] loss: 0.114
[58,     1] loss: 0.112
[59,     1] loss: 0.114
[60,     1] loss: 0.110
[61,     1] loss: 0.111
[62,     1] loss: 0.111
[63,     1] loss: 0.111
[64,     1] loss: 0.109
[65,     1] loss: 0.110
[66,     1] loss: 0.108
[67,     1] loss: 0.109
[68,     1] loss: 0.108
[69,     1] loss: 0.109
[70,     1] loss: 0.109
[71,     1] loss: 0.148
[72,     1] loss: 0.199
[73,     1] loss: 0.490
[74,     1] loss: 0.222
[75,     1] loss: 0.273
[76,     1] loss: 0.227
[77,     1] loss: 0.217
[78,     1] loss: 0.221
[79,     1] loss: 0.202
[80,     1] loss: 0.199
[81,     1] loss: 0.196
[82,     1] loss: 0.188
[83,     1] loss: 0.182
[84,     1] loss: 0.181
[85,     1] loss: 0.175
[86,     1] loss: 0.171
[87,     1] loss: 0.166
[88,     1] loss: 0.162
[89,     1] loss: 0.159
[90,     1] loss: 0.156
[91,     1] loss: 0.152
[92,     1] loss: 0.150
[93,     1] loss: 0.145
[94,     1] loss: 0.144
[95,     1] loss: 0.139
[96,     1] loss: 0.137
[97,     1] loss: 0.134
[98,     1] loss: 0.131
[99,     1] loss: 0.128
[100,     1] loss: 0.122
[101,     1] loss: 0.120
[102,     1] loss: 0.116
[103,     1] loss: 0.113
[104,     1] loss: 0.110
[105,     1] loss: 0.109
[106,     1] loss: 0.103
[107,     1] loss: 0.102
[108,     1] loss: 0.098
[109,     1] loss: 0.097
[110,     1] loss: 0.094
[111,     1] loss: 0.092
[112,     1] loss: 0.091
[113,     1] loss: 0.088
[114,     1] loss: 0.090
[115,     1] loss: 0.087
[116,     1] loss: 0.086
[117,     1] loss: 0.088
[118,     1] loss: 0.085
Early stopping applied (best metric=0.35342371463775635)
Finished Training
Total time taken: 99.70378851890564
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.677
[3,     1] loss: 0.659
[4,     1] loss: 0.643
[5,     1] loss: 0.632
[6,     1] loss: 0.616
[7,     1] loss: 0.603
[8,     1] loss: 0.586
[9,     1] loss: 0.568
[10,     1] loss: 0.553
[11,     1] loss: 0.533
[12,     1] loss: 0.516
[13,     1] loss: 0.496
[14,     1] loss: 0.477
[15,     1] loss: 0.457
[16,     1] loss: 0.434
[17,     1] loss: 0.416
[18,     1] loss: 0.396
[19,     1] loss: 0.380
[20,     1] loss: 0.360
[21,     1] loss: 0.341
[22,     1] loss: 0.322
[23,     1] loss: 0.304
[24,     1] loss: 0.287
[25,     1] loss: 0.270
[26,     1] loss: 0.253
[27,     1] loss: 0.238
[28,     1] loss: 0.226
[29,     1] loss: 0.210
[30,     1] loss: 0.196
[31,     1] loss: 0.185
[32,     1] loss: 0.177
[33,     1] loss: 0.168
[34,     1] loss: 0.160
[35,     1] loss: 0.150
[36,     1] loss: 0.141
[37,     1] loss: 0.138
[38,     1] loss: 0.131
[39,     1] loss: 0.127
[40,     1] loss: 0.123
[41,     1] loss: 0.121
[42,     1] loss: 0.116
[43,     1] loss: 0.111
[44,     1] loss: 0.110
[45,     1] loss: 0.108
[46,     1] loss: 0.105
[47,     1] loss: 0.106
[48,     1] loss: 0.103
[49,     1] loss: 0.102
[50,     1] loss: 0.102
[51,     1] loss: 0.103
[52,     1] loss: 0.100
[53,     1] loss: 0.102
[54,     1] loss: 0.100
[55,     1] loss: 0.100
[56,     1] loss: 0.101
[57,     1] loss: 0.101
[58,     1] loss: 0.100
[59,     1] loss: 0.100
[60,     1] loss: 0.099
[61,     1] loss: 0.101
[62,     1] loss: 0.102
[63,     1] loss: 0.100
[64,     1] loss: 0.101
[65,     1] loss: 0.101
[66,     1] loss: 0.099
[67,     1] loss: 0.100
[68,     1] loss: 0.101
[69,     1] loss: 0.102
[70,     1] loss: 0.102
[71,     1] loss: 0.110
[72,     1] loss: 0.139
[73,     1] loss: 0.478
[74,     1] loss: 0.284
[75,     1] loss: 0.300
[76,     1] loss: 0.263
[77,     1] loss: 0.242
[78,     1] loss: 0.238
[79,     1] loss: 0.230
[80,     1] loss: 0.219
[81,     1] loss: 0.204
[82,     1] loss: 0.197
[83,     1] loss: 0.191
[84,     1] loss: 0.184
[85,     1] loss: 0.178
[86,     1] loss: 0.175
[87,     1] loss: 0.169
[88,     1] loss: 0.164
[89,     1] loss: 0.160
[90,     1] loss: 0.153
[91,     1] loss: 0.151
[92,     1] loss: 0.144
[93,     1] loss: 0.142
[94,     1] loss: 0.141
[95,     1] loss: 0.136
[96,     1] loss: 0.133
[97,     1] loss: 0.132
[98,     1] loss: 0.129
[99,     1] loss: 0.126
[100,     1] loss: 0.123
[101,     1] loss: 0.120
[102,     1] loss: 0.120
[103,     1] loss: 0.118
[104,     1] loss: 0.113
[105,     1] loss: 0.113
[106,     1] loss: 0.109
[107,     1] loss: 0.109
[108,     1] loss: 0.107
[109,     1] loss: 0.102
[110,     1] loss: 0.098
[111,     1] loss: 0.096
[112,     1] loss: 0.094
[113,     1] loss: 0.092
[114,     1] loss: 0.088
[115,     1] loss: 0.087
[116,     1] loss: 0.087
[117,     1] loss: 0.083
[118,     1] loss: 0.083
[119,     1] loss: 0.082
[120,     1] loss: 0.083
[121,     1] loss: 0.082
[122,     1] loss: 0.081
[123,     1] loss: 0.079
[124,     1] loss: 0.081
[125,     1] loss: 0.080
[126,     1] loss: 0.082
[127,     1] loss: 0.081
[128,     1] loss: 0.082
[129,     1] loss: 0.082
[130,     1] loss: 0.083
[131,     1] loss: 0.083
[132,     1] loss: 0.082
[133,     1] loss: 0.085
[134,     1] loss: 0.084
[135,     1] loss: 0.084
[136,     1] loss: 0.090
[137,     1] loss: 0.196
[138,     1] loss: 0.229
[139,     1] loss: 0.302
[140,     1] loss: 0.243
[141,     1] loss: 0.212
[142,     1] loss: 0.190
[143,     1] loss: 0.188
[144,     1] loss: 0.171
[145,     1] loss: 0.161
[146,     1] loss: 0.157
[147,     1] loss: 0.148
[148,     1] loss: 0.145
[149,     1] loss: 0.139
[150,     1] loss: 0.132
[151,     1] loss: 0.128
[152,     1] loss: 0.121
[153,     1] loss: 0.121
[154,     1] loss: 0.113
[155,     1] loss: 0.108
[156,     1] loss: 0.105
[157,     1] loss: 0.101
[158,     1] loss: 0.098
[159,     1] loss: 0.097
[160,     1] loss: 0.096
[161,     1] loss: 0.095
[162,     1] loss: 0.095
[163,     1] loss: 0.093
[164,     1] loss: 0.091
[165,     1] loss: 0.091
[166,     1] loss: 0.089
[167,     1] loss: 0.091
[168,     1] loss: 0.090
[169,     1] loss: 0.088
[170,     1] loss: 0.091
[171,     1] loss: 0.090
[172,     1] loss: 0.093
[173,     1] loss: 0.090
[174,     1] loss: 0.091
[175,     1] loss: 0.090
[176,     1] loss: 0.093
[177,     1] loss: 0.092
[178,     1] loss: 0.093
[179,     1] loss: 0.093
[180,     1] loss: 0.090
Early stopping applied (best metric=0.3041267693042755)
Finished Training
Total time taken: 153.23842358589172
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.670
[4,     1] loss: 0.657
[5,     1] loss: 0.643
[6,     1] loss: 0.629
[7,     1] loss: 0.616
[8,     1] loss: 0.599
[9,     1] loss: 0.581
[10,     1] loss: 0.565
[11,     1] loss: 0.545
[12,     1] loss: 0.526
[13,     1] loss: 0.507
[14,     1] loss: 0.486
[15,     1] loss: 0.466
[16,     1] loss: 0.447
[17,     1] loss: 0.430
[18,     1] loss: 0.412
[19,     1] loss: 0.395
[20,     1] loss: 0.377
[21,     1] loss: 0.361
[22,     1] loss: 0.347
[23,     1] loss: 0.329
[24,     1] loss: 0.315
[25,     1] loss: 0.299
[26,     1] loss: 0.285
[27,     1] loss: 0.269
[28,     1] loss: 0.259
[29,     1] loss: 0.245
[30,     1] loss: 0.234
[31,     1] loss: 0.220
[32,     1] loss: 0.213
[33,     1] loss: 0.200
[34,     1] loss: 0.190
[35,     1] loss: 0.181
[36,     1] loss: 0.174
[37,     1] loss: 0.168
[38,     1] loss: 0.162
[39,     1] loss: 0.155
[40,     1] loss: 0.149
[41,     1] loss: 0.145
[42,     1] loss: 0.140
[43,     1] loss: 0.138
[44,     1] loss: 0.133
[45,     1] loss: 0.131
[46,     1] loss: 0.129
[47,     1] loss: 0.128
[48,     1] loss: 0.123
[49,     1] loss: 0.122
[50,     1] loss: 0.121
[51,     1] loss: 0.120
[52,     1] loss: 0.119
[53,     1] loss: 0.119
[54,     1] loss: 0.117
[55,     1] loss: 0.116
[56,     1] loss: 0.116
[57,     1] loss: 0.113
[58,     1] loss: 0.114
[59,     1] loss: 0.113
[60,     1] loss: 0.112
[61,     1] loss: 0.113
[62,     1] loss: 0.112
[63,     1] loss: 0.112
Early stopping applied (best metric=0.4768853783607483)
Finished Training
Total time taken: 54.4744393825531
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.663
[5,     1] loss: 0.647
[6,     1] loss: 0.632
[7,     1] loss: 0.616
[8,     1] loss: 0.603
[9,     1] loss: 0.583
[10,     1] loss: 0.564
[11,     1] loss: 0.543
[12,     1] loss: 0.523
[13,     1] loss: 0.498
[14,     1] loss: 0.477
[15,     1] loss: 0.455
[16,     1] loss: 0.436
[17,     1] loss: 0.417
[18,     1] loss: 0.402
[19,     1] loss: 0.388
[20,     1] loss: 0.375
[21,     1] loss: 0.362
[22,     1] loss: 0.350
[23,     1] loss: 0.341
[24,     1] loss: 0.330
[25,     1] loss: 0.320
[26,     1] loss: 0.311
[27,     1] loss: 0.303
[28,     1] loss: 0.295
[29,     1] loss: 0.285
[30,     1] loss: 0.276
[31,     1] loss: 0.266
[32,     1] loss: 0.254
[33,     1] loss: 0.247
[34,     1] loss: 0.238
[35,     1] loss: 0.231
[36,     1] loss: 0.225
[37,     1] loss: 0.217
[38,     1] loss: 0.207
[39,     1] loss: 0.201
[40,     1] loss: 0.195
[41,     1] loss: 0.189
[42,     1] loss: 0.181
[43,     1] loss: 0.174
[44,     1] loss: 0.171
[45,     1] loss: 0.165
[46,     1] loss: 0.160
[47,     1] loss: 0.156
[48,     1] loss: 0.150
[49,     1] loss: 0.145
[50,     1] loss: 0.143
[51,     1] loss: 0.141
[52,     1] loss: 0.136
[53,     1] loss: 0.133
[54,     1] loss: 0.132
[55,     1] loss: 0.129
[56,     1] loss: 0.127
[57,     1] loss: 0.124
[58,     1] loss: 0.122
[59,     1] loss: 0.123
[60,     1] loss: 0.123
[61,     1] loss: 0.121
[62,     1] loss: 0.120
[63,     1] loss: 0.117
[64,     1] loss: 0.118
[65,     1] loss: 0.117
[66,     1] loss: 0.117
[67,     1] loss: 0.115
[68,     1] loss: 0.117
[69,     1] loss: 0.115
[70,     1] loss: 0.114
[71,     1] loss: 0.117
[72,     1] loss: 0.116
[73,     1] loss: 0.114
[74,     1] loss: 0.114
[75,     1] loss: 0.114
[76,     1] loss: 0.114
[77,     1] loss: 0.114
[78,     1] loss: 0.114
[79,     1] loss: 0.114
[80,     1] loss: 0.114
[81,     1] loss: 0.114
[82,     1] loss: 0.113
[83,     1] loss: 0.113
[84,     1] loss: 0.114
[85,     1] loss: 0.114
[86,     1] loss: 0.113
[87,     1] loss: 0.114
[88,     1] loss: 0.113
[89,     1] loss: 0.114
[90,     1] loss: 0.190
[91,     1] loss: 0.503
[92,     1] loss: 0.464
[93,     1] loss: 0.369
[94,     1] loss: 0.368
[95,     1] loss: 0.373
[96,     1] loss: 0.325
[97,     1] loss: 0.302
[98,     1] loss: 0.299
[99,     1] loss: 0.274
[100,     1] loss: 0.268
[101,     1] loss: 0.257
[102,     1] loss: 0.248
[103,     1] loss: 0.241
[104,     1] loss: 0.230
[105,     1] loss: 0.222
[106,     1] loss: 0.212
[107,     1] loss: 0.206
[108,     1] loss: 0.200
[109,     1] loss: 0.191
[110,     1] loss: 0.182
[111,     1] loss: 0.172
[112,     1] loss: 0.164
[113,     1] loss: 0.154
[114,     1] loss: 0.144
[115,     1] loss: 0.136
[116,     1] loss: 0.130
[117,     1] loss: 0.123
[118,     1] loss: 0.115
[119,     1] loss: 0.112
[120,     1] loss: 0.101
[121,     1] loss: 0.097
[122,     1] loss: 0.091
[123,     1] loss: 0.087
[124,     1] loss: 0.086
[125,     1] loss: 0.080
[126,     1] loss: 0.078
[127,     1] loss: 0.078
[128,     1] loss: 0.075
[129,     1] loss: 0.075
[130,     1] loss: 0.074
[131,     1] loss: 0.072
[132,     1] loss: 0.072
[133,     1] loss: 0.072
[134,     1] loss: 0.071
[135,     1] loss: 0.071
[136,     1] loss: 0.074
[137,     1] loss: 0.072
[138,     1] loss: 0.073
[139,     1] loss: 0.073
[140,     1] loss: 0.071
[141,     1] loss: 0.073
[142,     1] loss: 0.072
[143,     1] loss: 0.072
[144,     1] loss: 0.075
[145,     1] loss: 0.074
[146,     1] loss: 0.075
[147,     1] loss: 0.077
[148,     1] loss: 0.076
[149,     1] loss: 0.076
[150,     1] loss: 0.076
[151,     1] loss: 0.075
Early stopping applied (best metric=0.37059518694877625)
Finished Training
Total time taken: 130.86247563362122
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.711
[2,     1] loss: 0.693
[3,     1] loss: 0.678
[4,     1] loss: 0.664
[5,     1] loss: 0.650
[6,     1] loss: 0.637
[7,     1] loss: 0.620
[8,     1] loss: 0.605
[9,     1] loss: 0.589
[10,     1] loss: 0.571
[11,     1] loss: 0.552
[12,     1] loss: 0.533
[13,     1] loss: 0.513
[14,     1] loss: 0.488
[15,     1] loss: 0.472
[16,     1] loss: 0.453
[17,     1] loss: 0.434
[18,     1] loss: 0.415
[19,     1] loss: 0.400
[20,     1] loss: 0.382
[21,     1] loss: 0.369
[22,     1] loss: 0.350
[23,     1] loss: 0.337
[24,     1] loss: 0.325
[25,     1] loss: 0.314
[26,     1] loss: 0.301
[27,     1] loss: 0.289
[28,     1] loss: 0.276
[29,     1] loss: 0.264
[30,     1] loss: 0.256
[31,     1] loss: 0.244
[32,     1] loss: 0.237
[33,     1] loss: 0.228
[34,     1] loss: 0.220
[35,     1] loss: 0.213
[36,     1] loss: 0.206
[37,     1] loss: 0.201
[38,     1] loss: 0.195
[39,     1] loss: 0.190
[40,     1] loss: 0.186
[41,     1] loss: 0.183
[42,     1] loss: 0.179
[43,     1] loss: 0.175
[44,     1] loss: 0.173
[45,     1] loss: 0.169
[46,     1] loss: 0.167
[47,     1] loss: 0.164
[48,     1] loss: 0.163
[49,     1] loss: 0.161
[50,     1] loss: 0.160
[51,     1] loss: 0.157
[52,     1] loss: 0.156
[53,     1] loss: 0.155
[54,     1] loss: 0.152
[55,     1] loss: 0.151
[56,     1] loss: 0.150
[57,     1] loss: 0.150
[58,     1] loss: 0.149
[59,     1] loss: 0.150
[60,     1] loss: 0.148
[61,     1] loss: 0.147
[62,     1] loss: 0.148
[63,     1] loss: 0.147
[64,     1] loss: 0.146
[65,     1] loss: 0.145
[66,     1] loss: 0.147
[67,     1] loss: 0.145
[68,     1] loss: 0.144
[69,     1] loss: 0.145
[70,     1] loss: 0.141
[71,     1] loss: 0.142
[72,     1] loss: 0.143
[73,     1] loss: 0.141
[74,     1] loss: 0.142
[75,     1] loss: 0.142
[76,     1] loss: 0.139
[77,     1] loss: 0.140
[78,     1] loss: 0.143
[79,     1] loss: 0.139
[80,     1] loss: 0.141
[81,     1] loss: 0.140
[82,     1] loss: 0.139
[83,     1] loss: 0.139
[84,     1] loss: 0.140
[85,     1] loss: 0.138
[86,     1] loss: 0.136
[87,     1] loss: 0.136
[88,     1] loss: 0.136
[89,     1] loss: 0.137
[90,     1] loss: 0.145
[91,     1] loss: 0.395
[92,     1] loss: 0.681
[93,     1] loss: 0.560
[94,     1] loss: 0.484
[95,     1] loss: 0.456
[96,     1] loss: 0.427
[97,     1] loss: 0.420
[98,     1] loss: 0.418
[99,     1] loss: 0.408
[100,     1] loss: 0.391
[101,     1] loss: 0.380
[102,     1] loss: 0.363
[103,     1] loss: 0.347
[104,     1] loss: 0.332
[105,     1] loss: 0.319
[106,     1] loss: 0.311
[107,     1] loss: 0.297
[108,     1] loss: 0.289
[109,     1] loss: 0.279
[110,     1] loss: 0.270
[111,     1] loss: 0.261
[112,     1] loss: 0.257
[113,     1] loss: 0.250
[114,     1] loss: 0.244
[115,     1] loss: 0.236
[116,     1] loss: 0.232
[117,     1] loss: 0.223
[118,     1] loss: 0.216
[119,     1] loss: 0.209
[120,     1] loss: 0.201
[121,     1] loss: 0.193
[122,     1] loss: 0.183
[123,     1] loss: 0.175
[124,     1] loss: 0.170
[125,     1] loss: 0.162
[126,     1] loss: 0.149
[127,     1] loss: 0.145
[128,     1] loss: 0.164
[129,     1] loss: 0.192
[130,     1] loss: 0.198
[131,     1] loss: 0.197
[132,     1] loss: 0.193
[133,     1] loss: 0.195
[134,     1] loss: 0.191
[135,     1] loss: 0.190
[136,     1] loss: 0.188
[137,     1] loss: 0.183
[138,     1] loss: 0.181
[139,     1] loss: 0.179
[140,     1] loss: 0.173
[141,     1] loss: 0.170
[142,     1] loss: 0.168
[143,     1] loss: 0.162
[144,     1] loss: 0.155
[145,     1] loss: 0.154
[146,     1] loss: 0.152
[147,     1] loss: 0.147
[148,     1] loss: 0.145
[149,     1] loss: 0.141
[150,     1] loss: 0.137
[151,     1] loss: 0.131
[152,     1] loss: 0.128
[153,     1] loss: 0.124
[154,     1] loss: 0.117
[155,     1] loss: 0.115
[156,     1] loss: 0.107
[157,     1] loss: 0.107
[158,     1] loss: 0.104
[159,     1] loss: 0.100
[160,     1] loss: 0.098
[161,     1] loss: 0.100
[162,     1] loss: 0.096
[163,     1] loss: 0.093
[164,     1] loss: 0.093
[165,     1] loss: 0.092
[166,     1] loss: 0.090
[167,     1] loss: 0.089
[168,     1] loss: 0.088
[169,     1] loss: 0.088
[170,     1] loss: 0.086
[171,     1] loss: 0.088
[172,     1] loss: 0.088
[173,     1] loss: 0.088
[174,     1] loss: 0.087
[175,     1] loss: 0.087
[176,     1] loss: 0.089
[177,     1] loss: 0.087
[178,     1] loss: 0.085
[179,     1] loss: 0.090
[180,     1] loss: 0.089
[181,     1] loss: 0.090
[182,     1] loss: 0.089
[183,     1] loss: 0.091
[184,     1] loss: 0.091
[185,     1] loss: 0.090
[186,     1] loss: 0.086
[187,     1] loss: 0.089
[188,     1] loss: 0.088
[189,     1] loss: 0.089
[190,     1] loss: 0.089
[191,     1] loss: 0.090
[192,     1] loss: 0.091
[193,     1] loss: 0.091
[194,     1] loss: 0.091
[195,     1] loss: 0.092
[196,     1] loss: 0.090
[197,     1] loss: 0.089
[198,     1] loss: 0.091
[199,     1] loss: 0.089
[200,     1] loss: 0.091
Finished Training
Total time taken: 174.17520427703857
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.684
[3,     1] loss: 0.669
[4,     1] loss: 0.656
[5,     1] loss: 0.646
[6,     1] loss: 0.635
[7,     1] loss: 0.623
[8,     1] loss: 0.612
[9,     1] loss: 0.600
[10,     1] loss: 0.587
[11,     1] loss: 0.574
[12,     1] loss: 0.561
[13,     1] loss: 0.545
[14,     1] loss: 0.529
[15,     1] loss: 0.512
[16,     1] loss: 0.496
[17,     1] loss: 0.478
[18,     1] loss: 0.459
[19,     1] loss: 0.439
[20,     1] loss: 0.419
[21,     1] loss: 0.401
[22,     1] loss: 0.384
[23,     1] loss: 0.363
[24,     1] loss: 0.347
[25,     1] loss: 0.331
[26,     1] loss: 0.313
[27,     1] loss: 0.297
[28,     1] loss: 0.281
[29,     1] loss: 0.266
[30,     1] loss: 0.251
[31,     1] loss: 0.241
[32,     1] loss: 0.225
[33,     1] loss: 0.215
[34,     1] loss: 0.202
[35,     1] loss: 0.193
[36,     1] loss: 0.181
[37,     1] loss: 0.174
[38,     1] loss: 0.168
[39,     1] loss: 0.158
[40,     1] loss: 0.150
[41,     1] loss: 0.144
[42,     1] loss: 0.137
[43,     1] loss: 0.134
[44,     1] loss: 0.128
[45,     1] loss: 0.126
[46,     1] loss: 0.122
[47,     1] loss: 0.119
[48,     1] loss: 0.116
[49,     1] loss: 0.113
[50,     1] loss: 0.112
[51,     1] loss: 0.109
[52,     1] loss: 0.107
[53,     1] loss: 0.106
[54,     1] loss: 0.105
[55,     1] loss: 0.104
[56,     1] loss: 0.103
[57,     1] loss: 0.102
[58,     1] loss: 0.103
[59,     1] loss: 0.103
[60,     1] loss: 0.103
[61,     1] loss: 0.102
[62,     1] loss: 0.102
[63,     1] loss: 0.101
[64,     1] loss: 0.101
[65,     1] loss: 0.101
[66,     1] loss: 0.103
[67,     1] loss: 0.101
[68,     1] loss: 0.099
[69,     1] loss: 0.101
[70,     1] loss: 0.100
[71,     1] loss: 0.102
[72,     1] loss: 0.099
Early stopping applied (best metric=0.42537111043930054)
Finished Training
Total time taken: 64.21052384376526
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.675
[3,     1] loss: 0.656
[4,     1] loss: 0.640
[5,     1] loss: 0.624
[6,     1] loss: 0.608
[7,     1] loss: 0.596
[8,     1] loss: 0.580
[9,     1] loss: 0.562
[10,     1] loss: 0.549
[11,     1] loss: 0.527
[12,     1] loss: 0.511
[13,     1] loss: 0.490
[14,     1] loss: 0.467
[15,     1] loss: 0.449
[16,     1] loss: 0.426
[17,     1] loss: 0.405
[18,     1] loss: 0.382
[19,     1] loss: 0.359
[20,     1] loss: 0.339
[21,     1] loss: 0.316
[22,     1] loss: 0.301
[23,     1] loss: 0.279
[24,     1] loss: 0.262
[25,     1] loss: 0.246
[26,     1] loss: 0.234
[27,     1] loss: 0.218
[28,     1] loss: 0.209
[29,     1] loss: 0.196
[30,     1] loss: 0.185
[31,     1] loss: 0.175
[32,     1] loss: 0.166
[33,     1] loss: 0.159
[34,     1] loss: 0.151
[35,     1] loss: 0.143
[36,     1] loss: 0.137
[37,     1] loss: 0.134
[38,     1] loss: 0.130
[39,     1] loss: 0.124
[40,     1] loss: 0.121
[41,     1] loss: 0.117
[42,     1] loss: 0.115
[43,     1] loss: 0.114
[44,     1] loss: 0.111
[45,     1] loss: 0.109
[46,     1] loss: 0.109
[47,     1] loss: 0.106
[48,     1] loss: 0.106
[49,     1] loss: 0.106
[50,     1] loss: 0.105
[51,     1] loss: 0.103
[52,     1] loss: 0.105
[53,     1] loss: 0.103
[54,     1] loss: 0.103
[55,     1] loss: 0.101
[56,     1] loss: 0.104
[57,     1] loss: 0.101
[58,     1] loss: 0.102
[59,     1] loss: 0.103
[60,     1] loss: 0.102
[61,     1] loss: 0.100
[62,     1] loss: 0.104
[63,     1] loss: 0.101
[64,     1] loss: 0.102
[65,     1] loss: 0.101
[66,     1] loss: 0.099
[67,     1] loss: 0.102
[68,     1] loss: 0.099
[69,     1] loss: 0.100
[70,     1] loss: 0.122
[71,     1] loss: 0.184
[72,     1] loss: 0.192
Early stopping applied (best metric=0.41981011629104614)
Finished Training
Total time taken: 64.47154784202576
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.683
[3,     1] loss: 0.664
[4,     1] loss: 0.649
[5,     1] loss: 0.635
[6,     1] loss: 0.622
[7,     1] loss: 0.606
[8,     1] loss: 0.591
[9,     1] loss: 0.573
[10,     1] loss: 0.556
[11,     1] loss: 0.535
[12,     1] loss: 0.514
[13,     1] loss: 0.494
[14,     1] loss: 0.472
[15,     1] loss: 0.450
[16,     1] loss: 0.423
[17,     1] loss: 0.403
[18,     1] loss: 0.376
[19,     1] loss: 0.351
[20,     1] loss: 0.328
[21,     1] loss: 0.306
[22,     1] loss: 0.286
[23,     1] loss: 0.266
[24,     1] loss: 0.246
[25,     1] loss: 0.228
[26,     1] loss: 0.215
[27,     1] loss: 0.202
[28,     1] loss: 0.186
[29,     1] loss: 0.170
[30,     1] loss: 0.159
[31,     1] loss: 0.148
[32,     1] loss: 0.140
[33,     1] loss: 0.132
[34,     1] loss: 0.123
[35,     1] loss: 0.119
[36,     1] loss: 0.113
[37,     1] loss: 0.109
[38,     1] loss: 0.103
[39,     1] loss: 0.101
[40,     1] loss: 0.098
[41,     1] loss: 0.096
[42,     1] loss: 0.093
[43,     1] loss: 0.091
[44,     1] loss: 0.089
[45,     1] loss: 0.089
[46,     1] loss: 0.089
[47,     1] loss: 0.088
[48,     1] loss: 0.086
[49,     1] loss: 0.086
[50,     1] loss: 0.086
[51,     1] loss: 0.087
[52,     1] loss: 0.088
[53,     1] loss: 0.087
[54,     1] loss: 0.089
[55,     1] loss: 0.088
[56,     1] loss: 0.089
[57,     1] loss: 0.089
[58,     1] loss: 0.089
[59,     1] loss: 0.089
[60,     1] loss: 0.089
[61,     1] loss: 0.091
[62,     1] loss: 0.090
[63,     1] loss: 0.091
[64,     1] loss: 0.091
[65,     1] loss: 0.091
[66,     1] loss: 0.091
[67,     1] loss: 0.091
[68,     1] loss: 0.091
[69,     1] loss: 0.089
[70,     1] loss: 0.090
[71,     1] loss: 0.091
[72,     1] loss: 0.096
Early stopping applied (best metric=0.3689306080341339)
Finished Training
Total time taken: 64.88691520690918
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.673
[3,     1] loss: 0.656
[4,     1] loss: 0.640
[5,     1] loss: 0.629
[6,     1] loss: 0.616
[7,     1] loss: 0.603
[8,     1] loss: 0.589
[9,     1] loss: 0.573
[10,     1] loss: 0.559
[11,     1] loss: 0.542
[12,     1] loss: 0.527
[13,     1] loss: 0.509
[14,     1] loss: 0.491
[15,     1] loss: 0.473
[16,     1] loss: 0.456
[17,     1] loss: 0.433
[18,     1] loss: 0.410
[19,     1] loss: 0.389
[20,     1] loss: 0.368
[21,     1] loss: 0.349
[22,     1] loss: 0.328
[23,     1] loss: 0.310
[24,     1] loss: 0.291
[25,     1] loss: 0.271
[26,     1] loss: 0.253
[27,     1] loss: 0.237
[28,     1] loss: 0.223
[29,     1] loss: 0.207
[30,     1] loss: 0.196
[31,     1] loss: 0.186
[32,     1] loss: 0.176
[33,     1] loss: 0.168
[34,     1] loss: 0.157
[35,     1] loss: 0.153
[36,     1] loss: 0.144
[37,     1] loss: 0.138
[38,     1] loss: 0.132
[39,     1] loss: 0.129
[40,     1] loss: 0.125
[41,     1] loss: 0.121
[42,     1] loss: 0.117
[43,     1] loss: 0.115
[44,     1] loss: 0.112
[45,     1] loss: 0.109
[46,     1] loss: 0.109
[47,     1] loss: 0.107
[48,     1] loss: 0.105
[49,     1] loss: 0.104
[50,     1] loss: 0.102
[51,     1] loss: 0.103
[52,     1] loss: 0.101
[53,     1] loss: 0.100
[54,     1] loss: 0.100
[55,     1] loss: 0.100
[56,     1] loss: 0.100
[57,     1] loss: 0.099
[58,     1] loss: 0.098
[59,     1] loss: 0.101
[60,     1] loss: 0.097
[61,     1] loss: 0.099
[62,     1] loss: 0.097
[63,     1] loss: 0.098
[64,     1] loss: 0.099
[65,     1] loss: 0.098
[66,     1] loss: 0.098
[67,     1] loss: 0.097
[68,     1] loss: 0.098
[69,     1] loss: 0.096
[70,     1] loss: 0.095
[71,     1] loss: 0.097
[72,     1] loss: 0.097
[73,     1] loss: 0.095
Early stopping applied (best metric=0.3984498381614685)
Finished Training
Total time taken: 66.00619530677795
{'Hydroxylation-P Validation Accuracy': 0.8132098776711842, 'Hydroxylation-P Validation Sensitivity': 0.7486349206349207, 'Hydroxylation-P Validation Specificity': 0.8270612000598534, 'Hydroxylation-P Validation Precision': 0.4905498780749517, 'Hydroxylation-P AUC ROC': 0.8467751295041435, 'Hydroxylation-P AUC PR': 0.5877616399299209, 'Hydroxylation-P MCC': 0.49656655701038, 'Hydroxylation-P F1': 0.5890023287625477, 'Validation Loss (Hydroxylation-P)': 0.38676470041275024, 'Validation Loss (total)': 0.38676470041275024}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006405470550656336,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1792873836,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.773821367505086}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.687
[3,     1] loss: 0.676
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006883782399002733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2239765789,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.349756995774249}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.655
[4,     1] loss: 0.616
[5,     1] loss: 0.568
[6,     1] loss: 0.507
[7,     1] loss: 0.435
[8,     1] loss: 0.360
[9,     1] loss: 0.289
[10,     1] loss: 0.246
[11,     1] loss: 0.189
[12,     1] loss: 0.148
[13,     1] loss: 0.113
[14,     1] loss: 0.073
[15,     1] loss: 0.053
[16,     1] loss: 0.037
[17,     1] loss: 0.030
[18,     1] loss: 0.023
[19,     1] loss: 0.016
[20,     1] loss: 0.010
[21,     1] loss: 0.007
[22,     1] loss: 0.005
[23,     1] loss: 0.004
[24,     1] loss: 0.003
[25,     1] loss: 0.002
[26,     1] loss: 0.002
[27,     1] loss: 0.001
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002831860660279434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2750091297,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.36711850668868}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003574727037033843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1208960606,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.599271481910993}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.694
[3,     1] loss: 0.682
[4,     1] loss: 0.669
[5,     1] loss: 0.658
[6,     1] loss: 0.646
[7,     1] loss: 0.634
[8,     1] loss: 0.622
[9,     1] loss: 0.607
[10,     1] loss: 0.590
[11,     1] loss: 0.570
[12,     1] loss: 0.551
[13,     1] loss: 0.531
[14,     1] loss: 0.511
[15,     1] loss: 0.486
[16,     1] loss: 0.462
[17,     1] loss: 0.453
[18,     1] loss: 0.428
[19,     1] loss: 0.424
[20,     1] loss: 0.395
[21,     1] loss: 0.380
[22,     1] loss: 0.365
[23,     1] loss: 0.352
[24,     1] loss: 0.336
[25,     1] loss: 0.325
[26,     1] loss: 0.309
[27,     1] loss: 0.297
[28,     1] loss: 0.285
[29,     1] loss: 0.273
[30,     1] loss: 0.262
[31,     1] loss: 0.253
[32,     1] loss: 0.244
[33,     1] loss: 0.234
[34,     1] loss: 0.225
[35,     1] loss: 0.213
[36,     1] loss: 0.205
[37,     1] loss: 0.197
[38,     1] loss: 0.192
[39,     1] loss: 0.185
[40,     1] loss: 0.176
[41,     1] loss: 0.169
[42,     1] loss: 0.162
[43,     1] loss: 0.153
[44,     1] loss: 0.145
[45,     1] loss: 0.141
[46,     1] loss: 0.138
[47,     1] loss: 0.132
[48,     1] loss: 0.136
[49,     1] loss: 0.304
[50,     1] loss: 0.286
[51,     1] loss: 0.571
[52,     1] loss: 0.457
[53,     1] loss: 0.422
[54,     1] loss: 0.391
[55,     1] loss: 0.402
[56,     1] loss: 0.400
[57,     1] loss: 0.398
[58,     1] loss: 0.396
[59,     1] loss: 0.389
[60,     1] loss: 0.381
[61,     1] loss: 0.370
[62,     1] loss: 0.356
[63,     1] loss: 0.340
[64,     1] loss: 0.324
[65,     1] loss: 0.309
[66,     1] loss: 0.308
[67,     1] loss: 0.278
[68,     1] loss: 0.272
[69,     1] loss: 0.260
[70,     1] loss: 0.244
[71,     1] loss: 0.231
[72,     1] loss: 0.223
[73,     1] loss: 0.214
[74,     1] loss: 0.201
[75,     1] loss: 0.194
[76,     1] loss: 0.184
[77,     1] loss: 0.179
[78,     1] loss: 0.175
[79,     1] loss: 0.172
[80,     1] loss: 0.167
[81,     1] loss: 0.164
[82,     1] loss: 0.160
[83,     1] loss: 0.159
[84,     1] loss: 0.154
[85,     1] loss: 0.151
[86,     1] loss: 0.152
[87,     1] loss: 0.152
[88,     1] loss: 0.149
[89,     1] loss: 0.151
[90,     1] loss: 0.147
[91,     1] loss: 0.148
[92,     1] loss: 0.145
[93,     1] loss: 0.144
[94,     1] loss: 0.144
[95,     1] loss: 0.145
[96,     1] loss: 0.441
[97,     1] loss: 0.908
[98,     1] loss: 0.764
[99,     1] loss: 0.631
[100,     1] loss: 0.585
[101,     1] loss: 0.608
[102,     1] loss: 0.631
[103,     1] loss: 0.642
[104,     1] loss: 0.646
Early stopping applied (best metric=0.3945067524909973)
Finished Training
Total time taken: 93.54974675178528
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.610
[8,     1] loss: 0.594
[9,     1] loss: 0.576
[10,     1] loss: 0.556
[11,     1] loss: 0.537
[12,     1] loss: 0.513
[13,     1] loss: 0.491
[14,     1] loss: 0.471
[15,     1] loss: 0.450
[16,     1] loss: 0.428
[17,     1] loss: 0.403
[18,     1] loss: 0.385
[19,     1] loss: 0.364
[20,     1] loss: 0.347
[21,     1] loss: 0.330
[22,     1] loss: 0.311
[23,     1] loss: 0.294
[24,     1] loss: 0.279
[25,     1] loss: 0.269
[26,     1] loss: 0.256
[27,     1] loss: 0.249
[28,     1] loss: 0.238
[29,     1] loss: 0.226
[30,     1] loss: 0.220
[31,     1] loss: 0.212
[32,     1] loss: 0.203
[33,     1] loss: 0.198
[34,     1] loss: 0.190
[35,     1] loss: 0.185
[36,     1] loss: 0.178
[37,     1] loss: 0.177
[38,     1] loss: 0.181
[39,     1] loss: 0.193
[40,     1] loss: 0.203
[41,     1] loss: 0.193
[42,     1] loss: 0.195
[43,     1] loss: 0.191
[44,     1] loss: 0.187
[45,     1] loss: 0.182
[46,     1] loss: 0.180
[47,     1] loss: 0.187
[48,     1] loss: 0.198
[49,     1] loss: 0.185
[50,     1] loss: 0.182
[51,     1] loss: 0.177
[52,     1] loss: 0.173
[53,     1] loss: 0.168
[54,     1] loss: 0.165
[55,     1] loss: 0.156
[56,     1] loss: 0.149
[57,     1] loss: 0.144
[58,     1] loss: 0.142
[59,     1] loss: 0.137
[60,     1] loss: 0.135
[61,     1] loss: 0.134
[62,     1] loss: 0.132
[63,     1] loss: 0.130
[64,     1] loss: 0.130
[65,     1] loss: 0.126
[66,     1] loss: 0.125
[67,     1] loss: 0.130
[68,     1] loss: 0.129
[69,     1] loss: 0.129
[70,     1] loss: 0.128
[71,     1] loss: 0.127
[72,     1] loss: 0.130
[73,     1] loss: 0.132
[74,     1] loss: 0.129
[75,     1] loss: 0.133
[76,     1] loss: 0.134
[77,     1] loss: 0.132
[78,     1] loss: 0.132
[79,     1] loss: 0.132
[80,     1] loss: 0.133
[81,     1] loss: 0.134
[82,     1] loss: 0.135
[83,     1] loss: 0.194
[84,     1] loss: 0.193
[85,     1] loss: 0.451
[86,     1] loss: 0.611
[87,     1] loss: 0.531
[88,     1] loss: 0.467
[89,     1] loss: 0.438
[90,     1] loss: 0.440
[91,     1] loss: 0.440
Early stopping applied (best metric=0.4265327453613281)
Finished Training
Total time taken: 82.30633592605591
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.610
[7,     1] loss: 0.593
[8,     1] loss: 0.574
[9,     1] loss: 0.551
[10,     1] loss: 0.528
[11,     1] loss: 0.505
[12,     1] loss: 0.480
[13,     1] loss: 0.457
[14,     1] loss: 0.435
[15,     1] loss: 0.416
[16,     1] loss: 0.396
[17,     1] loss: 0.383
[18,     1] loss: 0.371
[19,     1] loss: 0.360
[20,     1] loss: 0.349
[21,     1] loss: 0.338
[22,     1] loss: 0.328
[23,     1] loss: 0.318
[24,     1] loss: 0.312
[25,     1] loss: 0.302
[26,     1] loss: 0.292
[27,     1] loss: 0.282
[28,     1] loss: 0.276
[29,     1] loss: 0.265
[30,     1] loss: 0.259
[31,     1] loss: 0.251
[32,     1] loss: 0.245
[33,     1] loss: 0.236
[34,     1] loss: 0.229
[35,     1] loss: 0.219
[36,     1] loss: 0.213
[37,     1] loss: 0.205
[38,     1] loss: 0.199
[39,     1] loss: 0.194
[40,     1] loss: 0.189
[41,     1] loss: 0.184
[42,     1] loss: 0.182
[43,     1] loss: 0.174
[44,     1] loss: 0.166
[45,     1] loss: 0.191
[46,     1] loss: 0.236
[47,     1] loss: 0.664
[48,     1] loss: 0.568
[49,     1] loss: 0.452
[50,     1] loss: 0.395
[51,     1] loss: 0.381
[52,     1] loss: 0.370
[53,     1] loss: 0.371
[54,     1] loss: 0.365
[55,     1] loss: 0.356
[56,     1] loss: 0.347
[57,     1] loss: 0.338
[58,     1] loss: 0.321
[59,     1] loss: 0.309
[60,     1] loss: 0.300
[61,     1] loss: 0.290
[62,     1] loss: 0.276
[63,     1] loss: 0.282
[64,     1] loss: 0.270
[65,     1] loss: 0.254
[66,     1] loss: 0.342
[67,     1] loss: 0.273
[68,     1] loss: 0.292
[69,     1] loss: 0.290
[70,     1] loss: 0.283
[71,     1] loss: 0.277
[72,     1] loss: 0.272
[73,     1] loss: 0.270
[74,     1] loss: 0.269
[75,     1] loss: 0.264
[76,     1] loss: 0.258
[77,     1] loss: 0.252
[78,     1] loss: 0.244
[79,     1] loss: 0.236
[80,     1] loss: 0.231
[81,     1] loss: 0.225
[82,     1] loss: 0.215
[83,     1] loss: 0.220
[84,     1] loss: 0.222
[85,     1] loss: 0.220
[86,     1] loss: 0.217
[87,     1] loss: 0.209
[88,     1] loss: 0.204
[89,     1] loss: 0.197
[90,     1] loss: 0.191
[91,     1] loss: 0.200
[92,     1] loss: 0.213
[93,     1] loss: 0.218
[94,     1] loss: 0.216
[95,     1] loss: 0.211
[96,     1] loss: 0.210
[97,     1] loss: 0.205
[98,     1] loss: 0.201
[99,     1] loss: 0.196
[100,     1] loss: 0.189
[101,     1] loss: 0.193
[102,     1] loss: 0.238
[103,     1] loss: 0.268
[104,     1] loss: 0.508
[105,     1] loss: 0.319
[106,     1] loss: 0.441
[107,     1] loss: 0.378
[108,     1] loss: 0.398
[109,     1] loss: 0.358
[110,     1] loss: 0.344
[111,     1] loss: 0.353
[112,     1] loss: 0.333
[113,     1] loss: 0.320
[114,     1] loss: 0.317
[115,     1] loss: 0.307
[116,     1] loss: 0.297
[117,     1] loss: 0.289
[118,     1] loss: 0.283
[119,     1] loss: 0.270
[120,     1] loss: 0.274
[121,     1] loss: 0.270
[122,     1] loss: 0.264
[123,     1] loss: 0.254
[124,     1] loss: 0.244
[125,     1] loss: 0.238
[126,     1] loss: 0.233
[127,     1] loss: 0.227
Early stopping applied (best metric=0.4024799168109894)
Finished Training
Total time taken: 116.01996445655823
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.693
[3,     1] loss: 0.680
[4,     1] loss: 0.670
[5,     1] loss: 0.660
[6,     1] loss: 0.650
[7,     1] loss: 0.638
[8,     1] loss: 0.625
[9,     1] loss: 0.611
[10,     1] loss: 0.595
[11,     1] loss: 0.580
[12,     1] loss: 0.564
[13,     1] loss: 0.553
[14,     1] loss: 0.538
[15,     1] loss: 0.526
[16,     1] loss: 0.507
[17,     1] loss: 0.493
[18,     1] loss: 0.473
[19,     1] loss: 0.456
[20,     1] loss: 0.444
[21,     1] loss: 0.441
[22,     1] loss: 0.435
[23,     1] loss: 0.413
[24,     1] loss: 0.407
[25,     1] loss: 0.398
[26,     1] loss: 0.391
[27,     1] loss: 0.386
[28,     1] loss: 0.377
[29,     1] loss: 0.369
[30,     1] loss: 0.364
[31,     1] loss: 0.358
[32,     1] loss: 0.357
[33,     1] loss: 0.348
[34,     1] loss: 0.350
[35,     1] loss: 0.339
[36,     1] loss: 0.338
[37,     1] loss: 0.331
[38,     1] loss: 0.327
[39,     1] loss: 0.324
[40,     1] loss: 0.319
[41,     1] loss: 0.316
[42,     1] loss: 0.312
[43,     1] loss: 0.307
[44,     1] loss: 0.299
[45,     1] loss: 0.295
[46,     1] loss: 0.290
[47,     1] loss: 0.306
[48,     1] loss: 0.335
[49,     1] loss: 0.318
[50,     1] loss: 0.305
[51,     1] loss: 0.302
[52,     1] loss: 0.298
[53,     1] loss: 0.291
[54,     1] loss: 0.281
[55,     1] loss: 0.272
[56,     1] loss: 0.264
[57,     1] loss: 0.260
[58,     1] loss: 0.252
[59,     1] loss: 0.241
[60,     1] loss: 0.237
[61,     1] loss: 0.243
[62,     1] loss: 0.246
[63,     1] loss: 0.269
[64,     1] loss: 0.246
[65,     1] loss: 0.243
[66,     1] loss: 0.235
[67,     1] loss: 0.232
[68,     1] loss: 0.228
[69,     1] loss: 0.220
[70,     1] loss: 0.216
[71,     1] loss: 0.208
[72,     1] loss: 0.202
[73,     1] loss: 0.200
[74,     1] loss: 0.202
[75,     1] loss: 0.207
[76,     1] loss: 0.208
[77,     1] loss: 0.226
[78,     1] loss: 0.328
[79,     1] loss: 0.324
[80,     1] loss: 0.385
[81,     1] loss: 0.354
[82,     1] loss: 0.298
[83,     1] loss: 0.284
[84,     1] loss: 0.290
[85,     1] loss: 0.290
[86,     1] loss: 0.272
[87,     1] loss: 0.271
[88,     1] loss: 0.271
[89,     1] loss: 0.262
[90,     1] loss: 0.255
[91,     1] loss: 0.247
[92,     1] loss: 0.238
[93,     1] loss: 0.234
[94,     1] loss: 0.228
[95,     1] loss: 0.222
[96,     1] loss: 0.221
[97,     1] loss: 0.217
[98,     1] loss: 0.213
[99,     1] loss: 0.212
[100,     1] loss: 0.211
[101,     1] loss: 0.212
[102,     1] loss: 0.211
[103,     1] loss: 0.212
[104,     1] loss: 0.209
[105,     1] loss: 0.211
[106,     1] loss: 0.209
[107,     1] loss: 0.210
[108,     1] loss: 0.211
Early stopping applied (best metric=0.3511837124824524)
Finished Training
Total time taken: 99.36308073997498
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.658
[5,     1] loss: 0.645
[6,     1] loss: 0.632
[7,     1] loss: 0.619
[8,     1] loss: 0.607
[9,     1] loss: 0.591
[10,     1] loss: 0.573
[11,     1] loss: 0.553
[12,     1] loss: 0.531
[13,     1] loss: 0.509
[14,     1] loss: 0.486
[15,     1] loss: 0.460
[16,     1] loss: 0.440
[17,     1] loss: 0.417
[18,     1] loss: 0.396
[19,     1] loss: 0.374
[20,     1] loss: 0.354
[21,     1] loss: 0.339
[22,     1] loss: 0.320
[23,     1] loss: 0.304
[24,     1] loss: 0.285
[25,     1] loss: 0.277
[26,     1] loss: 0.265
[27,     1] loss: 0.253
[28,     1] loss: 0.242
[29,     1] loss: 0.236
[30,     1] loss: 0.231
[31,     1] loss: 0.220
[32,     1] loss: 0.214
[33,     1] loss: 0.208
[34,     1] loss: 0.204
[35,     1] loss: 0.203
[36,     1] loss: 0.199
[37,     1] loss: 0.196
[38,     1] loss: 0.194
[39,     1] loss: 0.192
[40,     1] loss: 0.190
[41,     1] loss: 0.188
[42,     1] loss: 0.187
[43,     1] loss: 0.189
[44,     1] loss: 0.213
[45,     1] loss: 0.315
[46,     1] loss: 0.465
[47,     1] loss: 0.458
[48,     1] loss: 0.456
[49,     1] loss: 0.443
[50,     1] loss: 0.434
[51,     1] loss: 0.436
[52,     1] loss: 0.439
[53,     1] loss: 0.442
[54,     1] loss: 0.443
[55,     1] loss: 0.440
[56,     1] loss: 0.434
[57,     1] loss: 0.426
[58,     1] loss: 0.419
[59,     1] loss: 0.409
[60,     1] loss: 0.396
[61,     1] loss: 0.380
[62,     1] loss: 0.369
[63,     1] loss: 0.357
[64,     1] loss: 0.344
[65,     1] loss: 0.333
[66,     1] loss: 0.322
[67,     1] loss: 0.312
[68,     1] loss: 0.300
[69,     1] loss: 0.288
[70,     1] loss: 0.282
[71,     1] loss: 0.271
[72,     1] loss: 0.264
[73,     1] loss: 0.257
[74,     1] loss: 0.247
[75,     1] loss: 0.240
[76,     1] loss: 0.235
[77,     1] loss: 0.229
[78,     1] loss: 0.225
[79,     1] loss: 0.217
[80,     1] loss: 0.213
[81,     1] loss: 0.210
[82,     1] loss: 0.204
[83,     1] loss: 0.203
[84,     1] loss: 0.199
[85,     1] loss: 0.193
[86,     1] loss: 0.190
[87,     1] loss: 0.187
[88,     1] loss: 0.183
[89,     1] loss: 0.187
[90,     1] loss: 0.348
[91,     1] loss: 0.382
[92,     1] loss: 0.656
[93,     1] loss: 0.607
[94,     1] loss: 0.571
[95,     1] loss: 0.556
[96,     1] loss: 0.552
[97,     1] loss: 0.553
[98,     1] loss: 0.547
[99,     1] loss: 0.537
[100,     1] loss: 0.526
[101,     1] loss: 0.526
[102,     1] loss: 0.525
[103,     1] loss: 0.521
[104,     1] loss: 0.520
[105,     1] loss: 0.514
[106,     1] loss: 0.508
[107,     1] loss: 0.504
[108,     1] loss: 0.497
[109,     1] loss: 0.491
[110,     1] loss: 0.483
[111,     1] loss: 0.473
[112,     1] loss: 0.465
[113,     1] loss: 0.456
[114,     1] loss: 0.447
[115,     1] loss: 0.433
[116,     1] loss: 0.424
[117,     1] loss: 0.412
[118,     1] loss: 0.403
[119,     1] loss: 0.416
[120,     1] loss: 0.384
[121,     1] loss: 0.404
[122,     1] loss: 0.405
[123,     1] loss: 0.387
[124,     1] loss: 0.373
[125,     1] loss: 0.360
[126,     1] loss: 0.350
[127,     1] loss: 0.338
[128,     1] loss: 0.321
[129,     1] loss: 0.311
[130,     1] loss: 0.305
[131,     1] loss: 0.318
[132,     1] loss: 0.361
[133,     1] loss: 0.315
[134,     1] loss: 0.359
[135,     1] loss: 0.421
[136,     1] loss: 0.426
[137,     1] loss: 0.397
[138,     1] loss: 0.389
Early stopping applied (best metric=0.3278975486755371)
Finished Training
Total time taken: 127.56875491142273
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.683
[3,     1] loss: 0.660
[4,     1] loss: 0.638
[5,     1] loss: 0.621
[6,     1] loss: 0.598
[7,     1] loss: 0.580
[8,     1] loss: 0.558
[9,     1] loss: 0.536
[10,     1] loss: 0.513
[11,     1] loss: 0.488
[12,     1] loss: 0.464
[13,     1] loss: 0.441
[14,     1] loss: 0.417
[15,     1] loss: 0.401
[16,     1] loss: 0.384
[17,     1] loss: 0.371
[18,     1] loss: 0.357
[19,     1] loss: 0.343
[20,     1] loss: 0.332
[21,     1] loss: 0.321
[22,     1] loss: 0.315
[23,     1] loss: 0.303
[24,     1] loss: 0.294
[25,     1] loss: 0.288
[26,     1] loss: 0.274
[27,     1] loss: 0.269
[28,     1] loss: 0.259
[29,     1] loss: 0.252
[30,     1] loss: 0.245
[31,     1] loss: 0.238
[32,     1] loss: 0.231
[33,     1] loss: 0.226
[34,     1] loss: 0.220
[35,     1] loss: 0.215
[36,     1] loss: 0.210
[37,     1] loss: 0.206
[38,     1] loss: 0.201
[39,     1] loss: 0.195
[40,     1] loss: 0.192
[41,     1] loss: 0.192
[42,     1] loss: 0.187
[43,     1] loss: 0.186
[44,     1] loss: 0.203
[45,     1] loss: 0.222
[46,     1] loss: 0.482
[47,     1] loss: 0.487
[48,     1] loss: 0.460
[49,     1] loss: 0.435
[50,     1] loss: 0.438
[51,     1] loss: 0.431
[52,     1] loss: 0.420
[53,     1] loss: 0.415
[54,     1] loss: 0.406
[55,     1] loss: 0.388
[56,     1] loss: 0.373
[57,     1] loss: 0.358
[58,     1] loss: 0.341
[59,     1] loss: 0.326
[60,     1] loss: 0.315
[61,     1] loss: 0.303
[62,     1] loss: 0.291
[63,     1] loss: 0.283
[64,     1] loss: 0.270
[65,     1] loss: 0.261
[66,     1] loss: 0.247
[67,     1] loss: 0.239
[68,     1] loss: 0.238
[69,     1] loss: 0.235
[70,     1] loss: 0.217
[71,     1] loss: 0.244
[72,     1] loss: 0.238
[73,     1] loss: 0.243
[74,     1] loss: 0.246
[75,     1] loss: 0.253
[76,     1] loss: 0.235
[77,     1] loss: 0.241
[78,     1] loss: 0.229
[79,     1] loss: 0.226
[80,     1] loss: 0.223
[81,     1] loss: 0.215
[82,     1] loss: 0.213
[83,     1] loss: 0.240
[84,     1] loss: 0.262
[85,     1] loss: 0.207
[86,     1] loss: 0.227
[87,     1] loss: 0.217
[88,     1] loss: 0.213
[89,     1] loss: 0.210
[90,     1] loss: 0.208
[91,     1] loss: 0.200
Early stopping applied (best metric=0.38480255007743835)
Finished Training
Total time taken: 84.86562871932983
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.658
[5,     1] loss: 0.645
[6,     1] loss: 0.631
[7,     1] loss: 0.615
[8,     1] loss: 0.599
[9,     1] loss: 0.581
[10,     1] loss: 0.563
[11,     1] loss: 0.541
[12,     1] loss: 0.522
[13,     1] loss: 0.496
[14,     1] loss: 0.475
[15,     1] loss: 0.453
[16,     1] loss: 0.428
[17,     1] loss: 0.406
[18,     1] loss: 0.388
[19,     1] loss: 0.371
[20,     1] loss: 0.354
[21,     1] loss: 0.335
[22,     1] loss: 0.321
[23,     1] loss: 0.305
[24,     1] loss: 0.290
[25,     1] loss: 0.276
[26,     1] loss: 0.270
[27,     1] loss: 0.260
[28,     1] loss: 0.253
[29,     1] loss: 0.241
[30,     1] loss: 0.236
[31,     1] loss: 0.223
[32,     1] loss: 0.216
[33,     1] loss: 0.209
[34,     1] loss: 0.201
[35,     1] loss: 0.193
[36,     1] loss: 0.188
[37,     1] loss: 0.185
[38,     1] loss: 0.178
[39,     1] loss: 0.175
[40,     1] loss: 0.168
[41,     1] loss: 0.165
[42,     1] loss: 0.169
[43,     1] loss: 0.208
[44,     1] loss: 0.473
[45,     1] loss: 0.344
[46,     1] loss: 0.383
[47,     1] loss: 0.355
[48,     1] loss: 0.335
[49,     1] loss: 0.346
[50,     1] loss: 0.334
[51,     1] loss: 0.326
[52,     1] loss: 0.312
[53,     1] loss: 0.296
[54,     1] loss: 0.281
[55,     1] loss: 0.270
[56,     1] loss: 0.259
[57,     1] loss: 0.249
[58,     1] loss: 0.236
[59,     1] loss: 0.226
[60,     1] loss: 0.219
[61,     1] loss: 0.213
[62,     1] loss: 0.216
[63,     1] loss: 0.221
[64,     1] loss: 0.223
[65,     1] loss: 0.215
[66,     1] loss: 0.208
[67,     1] loss: 0.205
[68,     1] loss: 0.200
[69,     1] loss: 0.197
[70,     1] loss: 0.190
[71,     1] loss: 0.186
[72,     1] loss: 0.181
[73,     1] loss: 0.177
[74,     1] loss: 0.171
[75,     1] loss: 0.169
[76,     1] loss: 0.164
[77,     1] loss: 0.162
[78,     1] loss: 0.162
[79,     1] loss: 0.163
[80,     1] loss: 0.208
[81,     1] loss: 0.202
[82,     1] loss: 0.192
[83,     1] loss: 0.211
[84,     1] loss: 0.321
[85,     1] loss: 0.527
[86,     1] loss: 0.401
[87,     1] loss: 0.387
[88,     1] loss: 0.371
[89,     1] loss: 0.379
[90,     1] loss: 0.365
[91,     1] loss: 0.354
[92,     1] loss: 0.355
[93,     1] loss: 0.350
[94,     1] loss: 0.340
[95,     1] loss: 0.328
[96,     1] loss: 0.320
[97,     1] loss: 0.311
[98,     1] loss: 0.305
[99,     1] loss: 0.295
[100,     1] loss: 0.290
[101,     1] loss: 0.286
[102,     1] loss: 0.298
[103,     1] loss: 0.294
[104,     1] loss: 0.286
[105,     1] loss: 0.273
[106,     1] loss: 0.276
[107,     1] loss: 0.263
[108,     1] loss: 0.257
[109,     1] loss: 0.249
[110,     1] loss: 0.242
[111,     1] loss: 0.236
[112,     1] loss: 0.230
[113,     1] loss: 0.224
[114,     1] loss: 0.219
[115,     1] loss: 0.215
[116,     1] loss: 0.211
[117,     1] loss: 0.208
[118,     1] loss: 0.214
[119,     1] loss: 0.260
[120,     1] loss: 0.280
[121,     1] loss: 0.283
[122,     1] loss: 0.265
[123,     1] loss: 0.256
[124,     1] loss: 0.247
[125,     1] loss: 0.241
[126,     1] loss: 0.241
[127,     1] loss: 0.235
Early stopping applied (best metric=0.3660490810871124)
Finished Training
Total time taken: 119.72160601615906
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.670
[4,     1] loss: 0.658
[5,     1] loss: 0.648
[6,     1] loss: 0.637
[7,     1] loss: 0.625
[8,     1] loss: 0.610
[9,     1] loss: 0.597
[10,     1] loss: 0.581
[11,     1] loss: 0.564
[12,     1] loss: 0.551
[13,     1] loss: 0.534
[14,     1] loss: 0.517
[15,     1] loss: 0.496
[16,     1] loss: 0.477
[17,     1] loss: 0.458
[18,     1] loss: 0.437
[19,     1] loss: 0.417
[20,     1] loss: 0.396
[21,     1] loss: 0.381
[22,     1] loss: 0.361
[23,     1] loss: 0.345
[24,     1] loss: 0.330
[25,     1] loss: 0.311
[26,     1] loss: 0.296
[27,     1] loss: 0.295
[28,     1] loss: 0.266
[29,     1] loss: 0.270
[30,     1] loss: 0.259
[31,     1] loss: 0.242
[32,     1] loss: 0.232
[33,     1] loss: 0.217
[34,     1] loss: 0.209
[35,     1] loss: 0.203
[36,     1] loss: 0.191
[37,     1] loss: 0.182
[38,     1] loss: 0.175
[39,     1] loss: 0.167
[40,     1] loss: 0.161
[41,     1] loss: 0.151
[42,     1] loss: 0.145
[43,     1] loss: 0.135
[44,     1] loss: 0.132
[45,     1] loss: 0.126
[46,     1] loss: 0.120
[47,     1] loss: 0.118
[48,     1] loss: 0.113
[49,     1] loss: 0.110
[50,     1] loss: 0.106
[51,     1] loss: 0.106
[52,     1] loss: 0.107
[53,     1] loss: 0.104
[54,     1] loss: 0.104
[55,     1] loss: 0.102
[56,     1] loss: 0.102
[57,     1] loss: 0.102
[58,     1] loss: 0.101
[59,     1] loss: 0.102
[60,     1] loss: 0.179
[61,     1] loss: 0.658
[62,     1] loss: 0.550
[63,     1] loss: 0.536
[64,     1] loss: 0.492
[65,     1] loss: 0.499
[66,     1] loss: 0.502
[67,     1] loss: 0.512
[68,     1] loss: 0.521
[69,     1] loss: 0.525
[70,     1] loss: 0.525
[71,     1] loss: 0.527
[72,     1] loss: 0.522
[73,     1] loss: 0.518
[74,     1] loss: 0.510
[75,     1] loss: 0.503
[76,     1] loss: 0.495
[77,     1] loss: 0.487
[78,     1] loss: 0.480
[79,     1] loss: 0.470
[80,     1] loss: 0.461
[81,     1] loss: 0.448
Early stopping applied (best metric=0.4039941728115082)
Finished Training
Total time taken: 76.94288873672485
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.668
[4,     1] loss: 0.653
[5,     1] loss: 0.636
[6,     1] loss: 0.619
[7,     1] loss: 0.601
[8,     1] loss: 0.581
[9,     1] loss: 0.561
[10,     1] loss: 0.536
[11,     1] loss: 0.510
[12,     1] loss: 0.486
[13,     1] loss: 0.458
[14,     1] loss: 0.432
[15,     1] loss: 0.407
[16,     1] loss: 0.385
[17,     1] loss: 0.366
[18,     1] loss: 0.342
[19,     1] loss: 0.326
[20,     1] loss: 0.311
[21,     1] loss: 0.294
[22,     1] loss: 0.293
[23,     1] loss: 0.276
[24,     1] loss: 0.262
[25,     1] loss: 0.254
[26,     1] loss: 0.244
[27,     1] loss: 0.235
[28,     1] loss: 0.229
[29,     1] loss: 0.222
[30,     1] loss: 0.214
[31,     1] loss: 0.208
[32,     1] loss: 0.204
[33,     1] loss: 0.198
[34,     1] loss: 0.194
[35,     1] loss: 0.193
[36,     1] loss: 0.186
[37,     1] loss: 0.185
[38,     1] loss: 0.182
[39,     1] loss: 0.179
[40,     1] loss: 0.179
[41,     1] loss: 0.179
[42,     1] loss: 0.174
[43,     1] loss: 0.173
[44,     1] loss: 0.172
[45,     1] loss: 0.170
[46,     1] loss: 0.170
[47,     1] loss: 0.171
[48,     1] loss: 0.168
[49,     1] loss: 0.231
[50,     1] loss: 0.525
[51,     1] loss: 0.367
[52,     1] loss: 0.435
[53,     1] loss: 0.444
[54,     1] loss: 0.378
[55,     1] loss: 0.431
[56,     1] loss: 0.379
[57,     1] loss: 0.386
[58,     1] loss: 0.371
[59,     1] loss: 0.364
[60,     1] loss: 0.351
[61,     1] loss: 0.334
[62,     1] loss: 0.326
[63,     1] loss: 0.312
[64,     1] loss: 0.300
[65,     1] loss: 0.285
[66,     1] loss: 0.272
[67,     1] loss: 0.259
[68,     1] loss: 0.248
[69,     1] loss: 0.237
[70,     1] loss: 0.222
[71,     1] loss: 0.217
[72,     1] loss: 0.207
[73,     1] loss: 0.212
[74,     1] loss: 0.233
[75,     1] loss: 0.219
[76,     1] loss: 0.205
[77,     1] loss: 0.210
[78,     1] loss: 0.201
[79,     1] loss: 0.196
[80,     1] loss: 0.190
[81,     1] loss: 0.184
[82,     1] loss: 0.178
[83,     1] loss: 0.167
[84,     1] loss: 0.164
[85,     1] loss: 0.163
[86,     1] loss: 0.162
[87,     1] loss: 0.168
[88,     1] loss: 0.211
[89,     1] loss: 0.222
[90,     1] loss: 0.227
[91,     1] loss: 0.215
[92,     1] loss: 0.216
[93,     1] loss: 0.200
[94,     1] loss: 0.218
[95,     1] loss: 0.321
[96,     1] loss: 0.413
[97,     1] loss: 0.349
[98,     1] loss: 0.266
[99,     1] loss: 0.322
[100,     1] loss: 0.270
[101,     1] loss: 0.246
[102,     1] loss: 0.269
[103,     1] loss: 0.286
[104,     1] loss: 0.289
[105,     1] loss: 0.283
[106,     1] loss: 0.284
[107,     1] loss: 0.278
[108,     1] loss: 0.278
[109,     1] loss: 0.273
[110,     1] loss: 0.268
[111,     1] loss: 0.263
[112,     1] loss: 0.254
[113,     1] loss: 0.245
[114,     1] loss: 0.239
[115,     1] loss: 0.234
[116,     1] loss: 0.229
Early stopping applied (best metric=0.36504173278808594)
Finished Training
Total time taken: 110.68933963775635
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.685
[3,     1] loss: 0.666
[4,     1] loss: 0.652
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.610
[8,     1] loss: 0.596
[9,     1] loss: 0.579
[10,     1] loss: 0.558
[11,     1] loss: 0.538
[12,     1] loss: 0.517
[13,     1] loss: 0.496
[14,     1] loss: 0.473
[15,     1] loss: 0.454
[16,     1] loss: 0.433
[17,     1] loss: 0.412
[18,     1] loss: 0.390
[19,     1] loss: 0.367
[20,     1] loss: 0.351
[21,     1] loss: 0.332
[22,     1] loss: 0.316
[23,     1] loss: 0.300
[24,     1] loss: 0.293
[25,     1] loss: 0.284
[26,     1] loss: 0.265
[27,     1] loss: 0.257
[28,     1] loss: 0.244
[29,     1] loss: 0.232
[30,     1] loss: 0.222
[31,     1] loss: 0.210
[32,     1] loss: 0.205
[33,     1] loss: 0.196
[34,     1] loss: 0.190
[35,     1] loss: 0.182
[36,     1] loss: 0.179
[37,     1] loss: 0.172
[38,     1] loss: 0.166
[39,     1] loss: 0.163
[40,     1] loss: 0.154
[41,     1] loss: 0.147
[42,     1] loss: 0.147
[43,     1] loss: 0.238
[44,     1] loss: 0.606
[45,     1] loss: 0.429
[46,     1] loss: 0.341
[47,     1] loss: 0.358
[48,     1] loss: 0.375
[49,     1] loss: 0.365
[50,     1] loss: 0.359
[51,     1] loss: 0.353
[52,     1] loss: 0.350
[53,     1] loss: 0.338
[54,     1] loss: 0.324
[55,     1] loss: 0.312
[56,     1] loss: 0.300
[57,     1] loss: 0.282
[58,     1] loss: 0.265
[59,     1] loss: 0.255
[60,     1] loss: 0.242
[61,     1] loss: 0.229
[62,     1] loss: 0.217
[63,     1] loss: 0.213
[64,     1] loss: 0.210
[65,     1] loss: 0.231
[66,     1] loss: 0.231
[67,     1] loss: 0.204
[68,     1] loss: 0.194
[69,     1] loss: 0.184
[70,     1] loss: 0.179
[71,     1] loss: 0.176
[72,     1] loss: 0.172
[73,     1] loss: 0.165
[74,     1] loss: 0.167
[75,     1] loss: 0.165
[76,     1] loss: 0.160
[77,     1] loss: 0.159
[78,     1] loss: 0.157
[79,     1] loss: 0.152
[80,     1] loss: 0.152
[81,     1] loss: 0.152
[82,     1] loss: 0.148
[83,     1] loss: 0.150
[84,     1] loss: 0.188
[85,     1] loss: 0.347
[86,     1] loss: 0.382
[87,     1] loss: 0.337
[88,     1] loss: 0.757
[89,     1] loss: 0.395
[90,     1] loss: 0.527
[91,     1] loss: 0.501
[92,     1] loss: 0.467
[93,     1] loss: 0.469
[94,     1] loss: 0.479
[95,     1] loss: 0.494
[96,     1] loss: 0.501
[97,     1] loss: 0.502
[98,     1] loss: 0.498
[99,     1] loss: 0.495
[100,     1] loss: 0.492
[101,     1] loss: 0.487
[102,     1] loss: 0.482
[103,     1] loss: 0.478
[104,     1] loss: 0.474
[105,     1] loss: 0.470
[106,     1] loss: 0.464
[107,     1] loss: 0.458
[108,     1] loss: 0.453
[109,     1] loss: 0.445
[110,     1] loss: 0.437
Early stopping applied (best metric=0.3743201494216919)
Finished Training
Total time taken: 105.49888467788696
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.682
[3,     1] loss: 0.662
[4,     1] loss: 0.644
[5,     1] loss: 0.628
[6,     1] loss: 0.612
[7,     1] loss: 0.595
[8,     1] loss: 0.575
[9,     1] loss: 0.557
[10,     1] loss: 0.534
[11,     1] loss: 0.516
[12,     1] loss: 0.492
[13,     1] loss: 0.468
[14,     1] loss: 0.443
[15,     1] loss: 0.422
[16,     1] loss: 0.400
[17,     1] loss: 0.380
[18,     1] loss: 0.358
[19,     1] loss: 0.340
[20,     1] loss: 0.322
[21,     1] loss: 0.303
[22,     1] loss: 0.289
[23,     1] loss: 0.275
[24,     1] loss: 0.262
[25,     1] loss: 0.251
[26,     1] loss: 0.238
[27,     1] loss: 0.227
[28,     1] loss: 0.218
[29,     1] loss: 0.208
[30,     1] loss: 0.201
[31,     1] loss: 0.194
[32,     1] loss: 0.190
[33,     1] loss: 0.185
[34,     1] loss: 0.199
[35,     1] loss: 0.193
[36,     1] loss: 0.226
[37,     1] loss: 0.206
[38,     1] loss: 0.199
[39,     1] loss: 0.205
[40,     1] loss: 0.203
[41,     1] loss: 0.203
[42,     1] loss: 0.198
[43,     1] loss: 0.194
[44,     1] loss: 0.191
[45,     1] loss: 0.183
[46,     1] loss: 0.179
[47,     1] loss: 0.173
[48,     1] loss: 0.168
[49,     1] loss: 0.191
[50,     1] loss: 0.172
[51,     1] loss: 0.163
[52,     1] loss: 0.157
[53,     1] loss: 0.169
[54,     1] loss: 0.309
[55,     1] loss: 0.214
[56,     1] loss: 0.237
[57,     1] loss: 0.231
[58,     1] loss: 0.216
[59,     1] loss: 0.227
[60,     1] loss: 0.223
[61,     1] loss: 0.219
[62,     1] loss: 0.216
[63,     1] loss: 0.206
[64,     1] loss: 0.200
[65,     1] loss: 0.197
[66,     1] loss: 0.187
[67,     1] loss: 0.178
[68,     1] loss: 0.172
[69,     1] loss: 0.174
[70,     1] loss: 0.237
[71,     1] loss: 0.222
[72,     1] loss: 0.242
[73,     1] loss: 0.225
[74,     1] loss: 0.207
[75,     1] loss: 0.220
[76,     1] loss: 0.240
[77,     1] loss: 0.233
[78,     1] loss: 0.211
[79,     1] loss: 0.214
[80,     1] loss: 0.215
[81,     1] loss: 0.223
[82,     1] loss: 0.239
[83,     1] loss: 0.228
[84,     1] loss: 0.237
[85,     1] loss: 0.228
[86,     1] loss: 0.232
[87,     1] loss: 0.210
[88,     1] loss: 0.216
[89,     1] loss: 0.205
[90,     1] loss: 0.208
[91,     1] loss: 0.205
[92,     1] loss: 0.198
[93,     1] loss: 0.191
[94,     1] loss: 0.189
[95,     1] loss: 0.186
[96,     1] loss: 0.183
[97,     1] loss: 0.180
[98,     1] loss: 0.179
[99,     1] loss: 0.177
[100,     1] loss: 0.173
[101,     1] loss: 0.172
[102,     1] loss: 0.173
[103,     1] loss: 0.171
[104,     1] loss: 0.170
[105,     1] loss: 0.170
[106,     1] loss: 0.167
[107,     1] loss: 0.170
[108,     1] loss: 0.170
[109,     1] loss: 0.170
[110,     1] loss: 0.170
[111,     1] loss: 0.175
[112,     1] loss: 0.223
[113,     1] loss: 0.284
[114,     1] loss: 0.346
[115,     1] loss: 0.593
[116,     1] loss: 0.475
[117,     1] loss: 0.403
[118,     1] loss: 0.411
[119,     1] loss: 0.383
[120,     1] loss: 0.383
[121,     1] loss: 0.375
[122,     1] loss: 0.367
[123,     1] loss: 0.363
[124,     1] loss: 0.356
[125,     1] loss: 0.348
[126,     1] loss: 0.339
[127,     1] loss: 0.336
[128,     1] loss: 0.326
[129,     1] loss: 0.322
[130,     1] loss: 0.313
[131,     1] loss: 0.308
[132,     1] loss: 0.299
[133,     1] loss: 0.293
[134,     1] loss: 0.282
[135,     1] loss: 0.277
[136,     1] loss: 0.270
[137,     1] loss: 0.262
[138,     1] loss: 0.255
[139,     1] loss: 0.249
[140,     1] loss: 0.241
[141,     1] loss: 0.232
[142,     1] loss: 0.228
[143,     1] loss: 0.220
[144,     1] loss: 0.214
[145,     1] loss: 0.207
[146,     1] loss: 0.207
[147,     1] loss: 0.201
[148,     1] loss: 0.198
[149,     1] loss: 0.194
[150,     1] loss: 0.210
[151,     1] loss: 0.414
Early stopping applied (best metric=0.39062121510505676)
Finished Training
Total time taken: 145.58573842048645
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.652
[5,     1] loss: 0.640
[6,     1] loss: 0.627
[7,     1] loss: 0.613
[8,     1] loss: 0.596
[9,     1] loss: 0.579
[10,     1] loss: 0.563
[11,     1] loss: 0.542
[12,     1] loss: 0.524
[13,     1] loss: 0.504
[14,     1] loss: 0.485
[15,     1] loss: 0.461
[16,     1] loss: 0.443
[17,     1] loss: 0.430
[18,     1] loss: 0.416
[19,     1] loss: 0.396
[20,     1] loss: 0.381
[21,     1] loss: 0.366
[22,     1] loss: 0.354
[23,     1] loss: 0.341
[24,     1] loss: 0.327
[25,     1] loss: 0.314
[26,     1] loss: 0.303
[27,     1] loss: 0.292
[28,     1] loss: 0.281
[29,     1] loss: 0.273
[30,     1] loss: 0.263
[31,     1] loss: 0.255
[32,     1] loss: 0.248
[33,     1] loss: 0.240
[34,     1] loss: 0.232
[35,     1] loss: 0.224
[36,     1] loss: 0.218
[37,     1] loss: 0.209
[38,     1] loss: 0.206
[39,     1] loss: 0.196
[40,     1] loss: 0.191
[41,     1] loss: 0.185
[42,     1] loss: 0.178
[43,     1] loss: 0.173
[44,     1] loss: 0.164
[45,     1] loss: 0.165
[46,     1] loss: 0.222
[47,     1] loss: 0.437
[48,     1] loss: 0.389
[49,     1] loss: 0.363
[50,     1] loss: 0.348
[51,     1] loss: 0.360
[52,     1] loss: 0.343
[53,     1] loss: 0.336
[54,     1] loss: 0.327
[55,     1] loss: 0.317
[56,     1] loss: 0.305
[57,     1] loss: 0.300
[58,     1] loss: 0.283
[59,     1] loss: 0.270
[60,     1] loss: 0.260
[61,     1] loss: 0.249
[62,     1] loss: 0.238
[63,     1] loss: 0.228
[64,     1] loss: 0.216
[65,     1] loss: 0.207
[66,     1] loss: 0.196
[67,     1] loss: 0.188
[68,     1] loss: 0.180
[69,     1] loss: 0.174
[70,     1] loss: 0.171
[71,     1] loss: 0.166
[72,     1] loss: 0.160
[73,     1] loss: 0.161
[74,     1] loss: 0.157
[75,     1] loss: 0.156
[76,     1] loss: 0.152
[77,     1] loss: 0.152
[78,     1] loss: 0.149
[79,     1] loss: 0.148
[80,     1] loss: 0.151
[81,     1] loss: 0.149
[82,     1] loss: 0.150
[83,     1] loss: 0.150
[84,     1] loss: 0.151
Early stopping applied (best metric=0.359002947807312)
Finished Training
Total time taken: 82.03702449798584
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.688
[3,     1] loss: 0.670
[4,     1] loss: 0.656
[5,     1] loss: 0.643
[6,     1] loss: 0.628
[7,     1] loss: 0.613
[8,     1] loss: 0.595
[9,     1] loss: 0.577
[10,     1] loss: 0.560
[11,     1] loss: 0.540
[12,     1] loss: 0.521
[13,     1] loss: 0.500
[14,     1] loss: 0.480
[15,     1] loss: 0.459
[16,     1] loss: 0.442
[17,     1] loss: 0.422
[18,     1] loss: 0.403
[19,     1] loss: 0.383
[20,     1] loss: 0.365
[21,     1] loss: 0.346
[22,     1] loss: 0.328
[23,     1] loss: 0.315
[24,     1] loss: 0.298
[25,     1] loss: 0.283
[26,     1] loss: 0.267
[27,     1] loss: 0.255
[28,     1] loss: 0.240
[29,     1] loss: 0.231
[30,     1] loss: 0.222
[31,     1] loss: 0.212
[32,     1] loss: 0.205
[33,     1] loss: 0.197
[34,     1] loss: 0.187
[35,     1] loss: 0.183
[36,     1] loss: 0.178
[37,     1] loss: 0.174
[38,     1] loss: 0.170
[39,     1] loss: 0.162
[40,     1] loss: 0.161
[41,     1] loss: 0.163
[42,     1] loss: 0.177
[43,     1] loss: 0.174
[44,     1] loss: 0.170
[45,     1] loss: 0.168
[46,     1] loss: 0.181
[47,     1] loss: 0.227
[48,     1] loss: 0.360
[49,     1] loss: 0.465
[50,     1] loss: 0.389
[51,     1] loss: 0.336
[52,     1] loss: 0.309
[53,     1] loss: 0.319
[54,     1] loss: 0.320
[55,     1] loss: 0.311
[56,     1] loss: 0.305
[57,     1] loss: 0.299
[58,     1] loss: 0.289
[59,     1] loss: 0.281
[60,     1] loss: 0.270
[61,     1] loss: 0.257
[62,     1] loss: 0.243
[63,     1] loss: 0.232
[64,     1] loss: 0.219
[65,     1] loss: 0.208
[66,     1] loss: 0.200
[67,     1] loss: 0.189
[68,     1] loss: 0.184
[69,     1] loss: 0.178
[70,     1] loss: 0.170
[71,     1] loss: 0.165
[72,     1] loss: 0.162
[73,     1] loss: 0.156
[74,     1] loss: 0.155
[75,     1] loss: 0.154
[76,     1] loss: 0.154
[77,     1] loss: 0.153
[78,     1] loss: 0.149
[79,     1] loss: 0.150
[80,     1] loss: 0.149
[81,     1] loss: 0.148
[82,     1] loss: 0.151
[83,     1] loss: 0.151
[84,     1] loss: 0.154
[85,     1] loss: 0.155
[86,     1] loss: 0.155
[87,     1] loss: 0.177
[88,     1] loss: 0.371
[89,     1] loss: 0.573
[90,     1] loss: 0.442
[91,     1] loss: 0.443
[92,     1] loss: 0.435
[93,     1] loss: 0.443
[94,     1] loss: 0.422
[95,     1] loss: 0.408
Early stopping applied (best metric=0.3074657618999481)
Finished Training
Total time taken: 93.07208323478699
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.676
[4,     1] loss: 0.668
[5,     1] loss: 0.659
[6,     1] loss: 0.652
[7,     1] loss: 0.644
[8,     1] loss: 0.635
[9,     1] loss: 0.626
[10,     1] loss: 0.613
[11,     1] loss: 0.602
[12,     1] loss: 0.589
[13,     1] loss: 0.573
[14,     1] loss: 0.557
[15,     1] loss: 0.539
[16,     1] loss: 0.522
[17,     1] loss: 0.503
[18,     1] loss: 0.489
[19,     1] loss: 0.480
[20,     1] loss: 0.465
[21,     1] loss: 0.450
[22,     1] loss: 0.428
[23,     1] loss: 0.418
[24,     1] loss: 0.398
[25,     1] loss: 0.383
[26,     1] loss: 0.368
[27,     1] loss: 0.353
[28,     1] loss: 0.340
[29,     1] loss: 0.330
[30,     1] loss: 0.318
[31,     1] loss: 0.304
[32,     1] loss: 0.293
[33,     1] loss: 0.281
[34,     1] loss: 0.270
[35,     1] loss: 0.261
[36,     1] loss: 0.253
[37,     1] loss: 0.242
[38,     1] loss: 0.231
[39,     1] loss: 0.222
[40,     1] loss: 0.214
[41,     1] loss: 0.202
[42,     1] loss: 0.195
[43,     1] loss: 0.191
[44,     1] loss: 0.532
[45,     1] loss: 0.366
[46,     1] loss: 0.737
[47,     1] loss: 0.558
[48,     1] loss: 0.462
[49,     1] loss: 0.482
[50,     1] loss: 0.515
[51,     1] loss: 0.532
[52,     1] loss: 0.544
[53,     1] loss: 0.551
[54,     1] loss: 0.555
[55,     1] loss: 0.558
[56,     1] loss: 0.560
[57,     1] loss: 0.560
[58,     1] loss: 0.561
[59,     1] loss: 0.559
[60,     1] loss: 0.559
[61,     1] loss: 0.555
[62,     1] loss: 0.552
[63,     1] loss: 0.550
[64,     1] loss: 0.547
[65,     1] loss: 0.541
[66,     1] loss: 0.536
[67,     1] loss: 0.532
[68,     1] loss: 0.528
[69,     1] loss: 0.521
[70,     1] loss: 0.515
[71,     1] loss: 0.509
[72,     1] loss: 0.502
[73,     1] loss: 0.494
[74,     1] loss: 0.487
[75,     1] loss: 0.483
[76,     1] loss: 0.476
[77,     1] loss: 0.477
[78,     1] loss: 0.465
[79,     1] loss: 0.459
[80,     1] loss: 0.448
Early stopping applied (best metric=0.38853374123573303)
Finished Training
Total time taken: 78.90176296234131
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.681
[3,     1] loss: 0.654
[4,     1] loss: 0.632
[5,     1] loss: 0.612
[6,     1] loss: 0.592
[7,     1] loss: 0.571
[8,     1] loss: 0.549
[9,     1] loss: 0.526
[10,     1] loss: 0.503
[11,     1] loss: 0.480
[12,     1] loss: 0.461
[13,     1] loss: 0.441
[14,     1] loss: 0.426
[15,     1] loss: 0.406
[16,     1] loss: 0.388
[17,     1] loss: 0.376
[18,     1] loss: 0.364
[19,     1] loss: 0.352
[20,     1] loss: 0.341
[21,     1] loss: 0.328
[22,     1] loss: 0.316
[23,     1] loss: 0.307
[24,     1] loss: 0.297
[25,     1] loss: 0.288
[26,     1] loss: 0.280
[27,     1] loss: 0.272
[28,     1] loss: 0.264
[29,     1] loss: 0.254
[30,     1] loss: 0.246
[31,     1] loss: 0.238
[32,     1] loss: 0.232
[33,     1] loss: 0.225
[34,     1] loss: 0.218
[35,     1] loss: 0.213
[36,     1] loss: 0.206
[37,     1] loss: 0.202
[38,     1] loss: 0.198
[39,     1] loss: 0.193
[40,     1] loss: 0.188
[41,     1] loss: 0.185
[42,     1] loss: 0.183
[43,     1] loss: 0.176
[44,     1] loss: 0.174
[45,     1] loss: 0.163
[46,     1] loss: 0.169
[47,     1] loss: 0.272
[48,     1] loss: 0.720
[49,     1] loss: 0.534
[50,     1] loss: 0.500
[51,     1] loss: 0.481
[52,     1] loss: 0.451
[53,     1] loss: 0.454
[54,     1] loss: 0.462
[55,     1] loss: 0.464
[56,     1] loss: 0.467
[57,     1] loss: 0.463
[58,     1] loss: 0.452
[59,     1] loss: 0.439
[60,     1] loss: 0.425
[61,     1] loss: 0.411
[62,     1] loss: 0.398
[63,     1] loss: 0.385
[64,     1] loss: 0.370
[65,     1] loss: 0.355
[66,     1] loss: 0.341
[67,     1] loss: 0.330
[68,     1] loss: 0.315
[69,     1] loss: 0.301
[70,     1] loss: 0.293
[71,     1] loss: 0.282
[72,     1] loss: 0.269
[73,     1] loss: 0.258
[74,     1] loss: 0.261
[75,     1] loss: 0.258
[76,     1] loss: 0.248
[77,     1] loss: 0.229
[78,     1] loss: 0.244
[79,     1] loss: 0.239
[80,     1] loss: 0.247
[81,     1] loss: 0.246
[82,     1] loss: 0.242
[83,     1] loss: 0.234
[84,     1] loss: 0.229
[85,     1] loss: 0.225
[86,     1] loss: 0.222
[87,     1] loss: 0.216
[88,     1] loss: 0.207
[89,     1] loss: 0.201
[90,     1] loss: 0.194
[91,     1] loss: 0.191
[92,     1] loss: 0.190
[93,     1] loss: 0.183
[94,     1] loss: 0.180
[95,     1] loss: 0.175
[96,     1] loss: 0.170
[97,     1] loss: 0.167
[98,     1] loss: 0.164
[99,     1] loss: 0.161
[100,     1] loss: 0.161
[101,     1] loss: 0.156
[102,     1] loss: 0.154
[103,     1] loss: 0.156
[104,     1] loss: 0.153
[105,     1] loss: 0.153
[106,     1] loss: 0.151
[107,     1] loss: 0.152
[108,     1] loss: 0.150
[109,     1] loss: 0.149
[110,     1] loss: 0.149
[111,     1] loss: 0.152
[112,     1] loss: 0.151
[113,     1] loss: 0.151
[114,     1] loss: 0.152
[115,     1] loss: 0.151
[116,     1] loss: 0.152
[117,     1] loss: 0.153
[118,     1] loss: 0.153
[119,     1] loss: 0.204
[120,     1] loss: 0.356
[121,     1] loss: 0.827
[122,     1] loss: 0.791
[123,     1] loss: 0.717
[124,     1] loss: 0.659
[125,     1] loss: 0.613
[126,     1] loss: 0.615
[127,     1] loss: 0.614
[128,     1] loss: 0.607
[129,     1] loss: 0.611
[130,     1] loss: 0.618
[131,     1] loss: 0.624
[132,     1] loss: 0.627
[133,     1] loss: 0.628
[134,     1] loss: 0.625
[135,     1] loss: 0.623
[136,     1] loss: 0.621
[137,     1] loss: 0.619
[138,     1] loss: 0.617
[139,     1] loss: 0.615
[140,     1] loss: 0.613
[141,     1] loss: 0.609
[142,     1] loss: 0.604
[143,     1] loss: 0.601
[144,     1] loss: 0.596
[145,     1] loss: 0.591
[146,     1] loss: 0.584
[147,     1] loss: 0.580
[148,     1] loss: 0.571
[149,     1] loss: 0.564
[150,     1] loss: 0.557
[151,     1] loss: 0.549
[152,     1] loss: 0.539
[153,     1] loss: 0.533
[154,     1] loss: 0.524
[155,     1] loss: 0.513
[156,     1] loss: 0.505
[157,     1] loss: 0.502
[158,     1] loss: 0.491
[159,     1] loss: 0.479
[160,     1] loss: 0.464
[161,     1] loss: 0.452
[162,     1] loss: 0.441
[163,     1] loss: 0.433
Early stopping applied (best metric=0.3912041485309601)
Finished Training
Total time taken: 161.3307580947876
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.678
[4,     1] loss: 0.666
[5,     1] loss: 0.655
[6,     1] loss: 0.642
[7,     1] loss: 0.627
[8,     1] loss: 0.609
[9,     1] loss: 0.592
[10,     1] loss: 0.570
[11,     1] loss: 0.551
[12,     1] loss: 0.530
[13,     1] loss: 0.508
[14,     1] loss: 0.485
[15,     1] loss: 0.464
[16,     1] loss: 0.441
[17,     1] loss: 0.417
[18,     1] loss: 0.399
[19,     1] loss: 0.378
[20,     1] loss: 0.360
[21,     1] loss: 0.341
[22,     1] loss: 0.330
[23,     1] loss: 0.313
[24,     1] loss: 0.299
[25,     1] loss: 0.284
[26,     1] loss: 0.272
[27,     1] loss: 0.258
[28,     1] loss: 0.247
[29,     1] loss: 0.237
[30,     1] loss: 0.227
[31,     1] loss: 0.219
[32,     1] loss: 0.209
[33,     1] loss: 0.200
[34,     1] loss: 0.195
[35,     1] loss: 0.187
[36,     1] loss: 0.182
[37,     1] loss: 0.178
[38,     1] loss: 0.174
[39,     1] loss: 0.172
[40,     1] loss: 0.170
[41,     1] loss: 0.165
[42,     1] loss: 0.164
[43,     1] loss: 0.161
[44,     1] loss: 0.158
[45,     1] loss: 0.158
[46,     1] loss: 0.160
[47,     1] loss: 0.216
[48,     1] loss: 0.755
[49,     1] loss: 0.664
[50,     1] loss: 0.535
[51,     1] loss: 0.446
[52,     1] loss: 0.450
[53,     1] loss: 0.451
[54,     1] loss: 0.455
[55,     1] loss: 0.451
[56,     1] loss: 0.445
[57,     1] loss: 0.434
[58,     1] loss: 0.423
[59,     1] loss: 0.413
[60,     1] loss: 0.401
[61,     1] loss: 0.391
[62,     1] loss: 0.382
[63,     1] loss: 0.369
[64,     1] loss: 0.358
[65,     1] loss: 0.347
[66,     1] loss: 0.338
[67,     1] loss: 0.330
[68,     1] loss: 0.323
[69,     1] loss: 0.312
[70,     1] loss: 0.302
[71,     1] loss: 0.295
[72,     1] loss: 0.299
[73,     1] loss: 0.304
[74,     1] loss: 0.298
[75,     1] loss: 0.313
[76,     1] loss: 0.283
[77,     1] loss: 0.289
[78,     1] loss: 0.270
[79,     1] loss: 0.263
[80,     1] loss: 0.259
[81,     1] loss: 0.246
[82,     1] loss: 0.238
[83,     1] loss: 0.231
Early stopping applied (best metric=0.380281925201416)
Finished Training
Total time taken: 83.1453537940979
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.694
[3,     1] loss: 0.673
[4,     1] loss: 0.659
[5,     1] loss: 0.643
[6,     1] loss: 0.630
[7,     1] loss: 0.616
[8,     1] loss: 0.600
[9,     1] loss: 0.588
[10,     1] loss: 0.571
[11,     1] loss: 0.559
[12,     1] loss: 0.538
[13,     1] loss: 0.521
[14,     1] loss: 0.504
[15,     1] loss: 0.488
[16,     1] loss: 0.470
[17,     1] loss: 0.454
[18,     1] loss: 0.441
[19,     1] loss: 0.425
[20,     1] loss: 0.413
[21,     1] loss: 0.398
[22,     1] loss: 0.388
[23,     1] loss: 0.376
[24,     1] loss: 0.359
[25,     1] loss: 0.349
[26,     1] loss: 0.336
[27,     1] loss: 0.322
[28,     1] loss: 0.310
[29,     1] loss: 0.299
[30,     1] loss: 0.289
[31,     1] loss: 0.276
[32,     1] loss: 0.267
[33,     1] loss: 0.256
[34,     1] loss: 0.248
[35,     1] loss: 0.240
[36,     1] loss: 0.231
[37,     1] loss: 0.223
[38,     1] loss: 0.218
[39,     1] loss: 0.211
[40,     1] loss: 0.207
[41,     1] loss: 0.200
[42,     1] loss: 0.196
[43,     1] loss: 0.189
[44,     1] loss: 0.181
[45,     1] loss: 0.226
[46,     1] loss: 0.568
[47,     1] loss: 0.429
[48,     1] loss: 0.398
[49,     1] loss: 0.402
[50,     1] loss: 0.420
[51,     1] loss: 0.411
[52,     1] loss: 0.414
[53,     1] loss: 0.420
[54,     1] loss: 0.415
[55,     1] loss: 0.411
[56,     1] loss: 0.400
[57,     1] loss: 0.389
[58,     1] loss: 0.376
[59,     1] loss: 0.364
[60,     1] loss: 0.350
[61,     1] loss: 0.340
[62,     1] loss: 0.323
[63,     1] loss: 0.314
[64,     1] loss: 0.301
[65,     1] loss: 0.305
[66,     1] loss: 0.282
[67,     1] loss: 0.287
[68,     1] loss: 0.282
[69,     1] loss: 0.279
[70,     1] loss: 0.268
[71,     1] loss: 0.254
[72,     1] loss: 0.246
[73,     1] loss: 0.239
[74,     1] loss: 0.228
[75,     1] loss: 0.219
[76,     1] loss: 0.210
[77,     1] loss: 0.205
[78,     1] loss: 0.198
[79,     1] loss: 0.196
[80,     1] loss: 0.192
[81,     1] loss: 0.185
[82,     1] loss: 0.178
[83,     1] loss: 0.177
[84,     1] loss: 0.176
[85,     1] loss: 0.176
[86,     1] loss: 0.172
[87,     1] loss: 0.172
[88,     1] loss: 0.171
[89,     1] loss: 0.171
[90,     1] loss: 0.173
[91,     1] loss: 0.173
[92,     1] loss: 0.174
[93,     1] loss: 0.179
[94,     1] loss: 0.273
[95,     1] loss: 0.519
[96,     1] loss: 0.498
[97,     1] loss: 0.480
[98,     1] loss: 0.512
[99,     1] loss: 0.442
[100,     1] loss: 0.433
[101,     1] loss: 0.441
[102,     1] loss: 0.446
[103,     1] loss: 0.436
[104,     1] loss: 0.433
[105,     1] loss: 0.425
[106,     1] loss: 0.422
[107,     1] loss: 0.414
[108,     1] loss: 0.406
[109,     1] loss: 0.400
[110,     1] loss: 0.396
[111,     1] loss: 0.391
[112,     1] loss: 0.382
[113,     1] loss: 0.375
[114,     1] loss: 0.368
[115,     1] loss: 0.361
[116,     1] loss: 0.352
[117,     1] loss: 0.343
[118,     1] loss: 0.335
[119,     1] loss: 0.327
[120,     1] loss: 0.315
[121,     1] loss: 0.310
[122,     1] loss: 0.301
[123,     1] loss: 0.297
[124,     1] loss: 0.292
[125,     1] loss: 0.333
[126,     1] loss: 0.343
Early stopping applied (best metric=0.3760828971862793)
Finished Training
Total time taken: 126.55361676216125
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.667
[4,     1] loss: 0.650
[5,     1] loss: 0.633
[6,     1] loss: 0.616
[7,     1] loss: 0.598
[8,     1] loss: 0.577
[9,     1] loss: 0.554
[10,     1] loss: 0.529
[11,     1] loss: 0.507
[12,     1] loss: 0.484
[13,     1] loss: 0.461
[14,     1] loss: 0.437
[15,     1] loss: 0.416
[16,     1] loss: 0.396
[17,     1] loss: 0.371
[18,     1] loss: 0.360
[19,     1] loss: 0.344
[20,     1] loss: 0.324
[21,     1] loss: 0.312
[22,     1] loss: 0.300
[23,     1] loss: 0.285
[24,     1] loss: 0.275
[25,     1] loss: 0.263
[26,     1] loss: 0.253
[27,     1] loss: 0.242
[28,     1] loss: 0.233
[29,     1] loss: 0.225
[30,     1] loss: 0.215
[31,     1] loss: 0.206
[32,     1] loss: 0.200
[33,     1] loss: 0.192
[34,     1] loss: 0.187
[35,     1] loss: 0.178
[36,     1] loss: 0.173
[37,     1] loss: 0.166
[38,     1] loss: 0.161
[39,     1] loss: 0.155
[40,     1] loss: 0.150
[41,     1] loss: 0.143
[42,     1] loss: 0.137
[43,     1] loss: 0.144
[44,     1] loss: 0.457
[45,     1] loss: 0.407
[46,     1] loss: 0.318
[47,     1] loss: 0.394
[48,     1] loss: 0.368
[49,     1] loss: 0.368
[50,     1] loss: 0.362
[51,     1] loss: 0.376
[52,     1] loss: 0.373
[53,     1] loss: 0.357
[54,     1] loss: 0.346
[55,     1] loss: 0.333
[56,     1] loss: 0.315
[57,     1] loss: 0.304
[58,     1] loss: 0.292
[59,     1] loss: 0.278
[60,     1] loss: 0.267
[61,     1] loss: 0.254
[62,     1] loss: 0.243
[63,     1] loss: 0.235
[64,     1] loss: 0.225
[65,     1] loss: 0.214
[66,     1] loss: 0.206
[67,     1] loss: 0.200
[68,     1] loss: 0.193
[69,     1] loss: 0.187
[70,     1] loss: 0.182
[71,     1] loss: 0.175
[72,     1] loss: 0.173
[73,     1] loss: 0.169
[74,     1] loss: 0.164
[75,     1] loss: 0.164
[76,     1] loss: 0.163
[77,     1] loss: 0.209
[78,     1] loss: 0.173
[79,     1] loss: 0.229
[80,     1] loss: 0.305
[81,     1] loss: 0.641
[82,     1] loss: 0.406
[83,     1] loss: 0.443
[84,     1] loss: 0.450
[85,     1] loss: 0.417
[86,     1] loss: 0.425
[87,     1] loss: 0.440
[88,     1] loss: 0.441
[89,     1] loss: 0.435
[90,     1] loss: 0.427
[91,     1] loss: 0.422
[92,     1] loss: 0.411
[93,     1] loss: 0.404
[94,     1] loss: 0.395
[95,     1] loss: 0.385
[96,     1] loss: 0.376
[97,     1] loss: 0.368
[98,     1] loss: 0.359
[99,     1] loss: 0.349
[100,     1] loss: 0.337
[101,     1] loss: 0.329
[102,     1] loss: 0.318
[103,     1] loss: 0.308
[104,     1] loss: 0.299
[105,     1] loss: 0.287
[106,     1] loss: 0.277
[107,     1] loss: 0.265
[108,     1] loss: 0.259
[109,     1] loss: 0.267
[110,     1] loss: 0.308
[111,     1] loss: 0.288
[112,     1] loss: 0.293
[113,     1] loss: 0.274
Early stopping applied (best metric=0.39427322149276733)
Finished Training
Total time taken: 114.76468706130981
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.688
[3,     1] loss: 0.664
[4,     1] loss: 0.640
[5,     1] loss: 0.619
[6,     1] loss: 0.598
[7,     1] loss: 0.577
[8,     1] loss: 0.554
[9,     1] loss: 0.528
[10,     1] loss: 0.500
[11,     1] loss: 0.475
[12,     1] loss: 0.463
[13,     1] loss: 0.448
[14,     1] loss: 0.430
[15,     1] loss: 0.417
[16,     1] loss: 0.401
[17,     1] loss: 0.395
[18,     1] loss: 0.385
[19,     1] loss: 0.377
[20,     1] loss: 0.371
[21,     1] loss: 0.368
[22,     1] loss: 0.364
[23,     1] loss: 0.362
[24,     1] loss: 0.365
[25,     1] loss: 0.369
[26,     1] loss: 0.370
[27,     1] loss: 0.359
[28,     1] loss: 0.367
[29,     1] loss: 0.361
[30,     1] loss: 0.367
[31,     1] loss: 0.361
[32,     1] loss: 0.365
[33,     1] loss: 0.363
[34,     1] loss: 0.365
[35,     1] loss: 0.360
[36,     1] loss: 0.382
[37,     1] loss: 0.426
[38,     1] loss: 0.400
[39,     1] loss: 0.392
[40,     1] loss: 0.378
[41,     1] loss: 0.383
[42,     1] loss: 0.377
[43,     1] loss: 0.370
[44,     1] loss: 0.368
[45,     1] loss: 0.365
[46,     1] loss: 0.360
[47,     1] loss: 0.359
[48,     1] loss: 0.360
[49,     1] loss: 0.361
[50,     1] loss: 0.357
[51,     1] loss: 0.360
[52,     1] loss: 0.360
[53,     1] loss: 0.357
[54,     1] loss: 0.357
[55,     1] loss: 0.358
[56,     1] loss: 0.356
[57,     1] loss: 0.356
[58,     1] loss: 0.354
[59,     1] loss: 0.353
[60,     1] loss: 0.352
[61,     1] loss: 0.351
[62,     1] loss: 0.350
[63,     1] loss: 0.353
[64,     1] loss: 0.351
[65,     1] loss: 0.350
[66,     1] loss: 0.356
[67,     1] loss: 0.369
[68,     1] loss: 0.400
[69,     1] loss: 0.430
[70,     1] loss: 0.409
[71,     1] loss: 0.408
[72,     1] loss: 0.389
[73,     1] loss: 0.377
[74,     1] loss: 0.363
[75,     1] loss: 0.355
[76,     1] loss: 0.350
[77,     1] loss: 0.345
[78,     1] loss: 0.342
[79,     1] loss: 0.341
[80,     1] loss: 0.338
[81,     1] loss: 0.336
[82,     1] loss: 0.335
[83,     1] loss: 0.332
[84,     1] loss: 0.330
[85,     1] loss: 0.329
[86,     1] loss: 0.329
[87,     1] loss: 0.327
[88,     1] loss: 0.325
[89,     1] loss: 0.321
[90,     1] loss: 0.319
[91,     1] loss: 0.317
[92,     1] loss: 0.313
[93,     1] loss: 0.309
[94,     1] loss: 0.304
[95,     1] loss: 0.299
[96,     1] loss: 0.292
[97,     1] loss: 0.287
[98,     1] loss: 0.282
[99,     1] loss: 0.273
[100,     1] loss: 0.265
[101,     1] loss: 0.256
[102,     1] loss: 0.246
[103,     1] loss: 0.287
[104,     1] loss: 0.637
[105,     1] loss: 0.562
[106,     1] loss: 0.522
[107,     1] loss: 0.475
[108,     1] loss: 0.459
[109,     1] loss: 0.450
[110,     1] loss: 0.441
[111,     1] loss: 0.411
[112,     1] loss: 0.393
[113,     1] loss: 0.367
[114,     1] loss: 0.348
[115,     1] loss: 0.314
[116,     1] loss: 0.289
[117,     1] loss: 0.264
[118,     1] loss: 0.247
[119,     1] loss: 0.234
[120,     1] loss: 0.207
[121,     1] loss: 0.191
[122,     1] loss: 0.189
[123,     1] loss: 0.169
[124,     1] loss: 0.186
[125,     1] loss: 0.151
[126,     1] loss: 0.149
[127,     1] loss: 0.135
[128,     1] loss: 0.132
[129,     1] loss: 0.129
[130,     1] loss: 0.123
[131,     1] loss: 0.117
[132,     1] loss: 0.117
[133,     1] loss: 0.114
[134,     1] loss: 0.113
[135,     1] loss: 0.112
[136,     1] loss: 0.110
[137,     1] loss: 0.107
[138,     1] loss: 0.110
[139,     1] loss: 0.110
[140,     1] loss: 0.107
[141,     1] loss: 0.108
[142,     1] loss: 0.106
[143,     1] loss: 0.109
[144,     1] loss: 0.106
[145,     1] loss: 0.109
[146,     1] loss: 0.107
[147,     1] loss: 0.109
[148,     1] loss: 0.109
[149,     1] loss: 0.111
[150,     1] loss: 0.111
[151,     1] loss: 0.113
[152,     1] loss: 0.112
[153,     1] loss: 0.116
[154,     1] loss: 0.117
[155,     1] loss: 0.124
[156,     1] loss: 0.300
[157,     1] loss: 0.415
[158,     1] loss: 1.225
[159,     1] loss: 0.579
[160,     1] loss: 0.602
[161,     1] loss: 0.650
[162,     1] loss: 0.652
[163,     1] loss: 0.643
[164,     1] loss: 0.641
[165,     1] loss: 0.638
[166,     1] loss: 0.640
[167,     1] loss: 0.645
[168,     1] loss: 0.655
[169,     1] loss: 0.661
[170,     1] loss: 0.666
[171,     1] loss: 0.671
[172,     1] loss: 0.674
[173,     1] loss: 0.677
[174,     1] loss: 0.680
[175,     1] loss: 0.682
[176,     1] loss: 0.683
[177,     1] loss: 0.685
[178,     1] loss: 0.686
Early stopping applied (best metric=0.321146160364151)
Finished Training
Total time taken: 181.42564702033997
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.608
[8,     1] loss: 0.591
[9,     1] loss: 0.572
[10,     1] loss: 0.555
[11,     1] loss: 0.530
[12,     1] loss: 0.509
[13,     1] loss: 0.488
[14,     1] loss: 0.462
[15,     1] loss: 0.443
[16,     1] loss: 0.418
[17,     1] loss: 0.396
[18,     1] loss: 0.375
[19,     1] loss: 0.357
[20,     1] loss: 0.336
[21,     1] loss: 0.319
[22,     1] loss: 0.303
[23,     1] loss: 0.289
[24,     1] loss: 0.277
[25,     1] loss: 0.269
[26,     1] loss: 0.295
[27,     1] loss: 0.248
[28,     1] loss: 0.251
[29,     1] loss: 0.244
[30,     1] loss: 0.229
[31,     1] loss: 0.222
[32,     1] loss: 0.216
[33,     1] loss: 0.209
[34,     1] loss: 0.203
[35,     1] loss: 0.198
[36,     1] loss: 0.189
[37,     1] loss: 0.183
[38,     1] loss: 0.179
[39,     1] loss: 0.173
[40,     1] loss: 0.168
[41,     1] loss: 0.163
[42,     1] loss: 0.158
[43,     1] loss: 0.151
[44,     1] loss: 0.146
[45,     1] loss: 0.192
[46,     1] loss: 0.271
[47,     1] loss: 0.330
[48,     1] loss: 0.356
[49,     1] loss: 0.322
[50,     1] loss: 0.364
[51,     1] loss: 0.343
[52,     1] loss: 0.338
[53,     1] loss: 0.324
[54,     1] loss: 0.317
[55,     1] loss: 0.311
[56,     1] loss: 0.305
[57,     1] loss: 0.290
[58,     1] loss: 0.279
[59,     1] loss: 0.264
[60,     1] loss: 0.252
[61,     1] loss: 0.248
[62,     1] loss: 0.237
[63,     1] loss: 0.229
[64,     1] loss: 0.215
[65,     1] loss: 0.212
[66,     1] loss: 0.205
[67,     1] loss: 0.199
[68,     1] loss: 0.194
[69,     1] loss: 0.189
[70,     1] loss: 0.183
[71,     1] loss: 0.183
[72,     1] loss: 0.178
[73,     1] loss: 0.174
[74,     1] loss: 0.173
[75,     1] loss: 0.172
[76,     1] loss: 0.172
[77,     1] loss: 0.170
[78,     1] loss: 0.168
[79,     1] loss: 0.167
[80,     1] loss: 0.167
[81,     1] loss: 0.165
[82,     1] loss: 0.164
[83,     1] loss: 0.162
[84,     1] loss: 0.161
[85,     1] loss: 0.161
[86,     1] loss: 0.162
[87,     1] loss: 0.162
Early stopping applied (best metric=0.3226715922355652)
Finished Training
Total time taken: 89.62730050086975
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.671
[4,     1] loss: 0.657
[5,     1] loss: 0.645
[6,     1] loss: 0.631
[7,     1] loss: 0.616
[8,     1] loss: 0.603
[9,     1] loss: 0.588
[10,     1] loss: 0.572
[11,     1] loss: 0.553
[12,     1] loss: 0.535
[13,     1] loss: 0.515
[14,     1] loss: 0.493
[15,     1] loss: 0.471
[16,     1] loss: 0.447
[17,     1] loss: 0.421
[18,     1] loss: 0.399
[19,     1] loss: 0.376
[20,     1] loss: 0.354
[21,     1] loss: 0.335
[22,     1] loss: 0.313
[23,     1] loss: 0.296
[24,     1] loss: 0.280
[25,     1] loss: 0.263
[26,     1] loss: 0.246
[27,     1] loss: 0.239
[28,     1] loss: 0.226
[29,     1] loss: 0.214
[30,     1] loss: 0.204
[31,     1] loss: 0.198
[32,     1] loss: 0.189
[33,     1] loss: 0.183
[34,     1] loss: 0.177
[35,     1] loss: 0.171
[36,     1] loss: 0.166
[37,     1] loss: 0.164
[38,     1] loss: 0.165
[39,     1] loss: 0.178
[40,     1] loss: 0.180
[41,     1] loss: 0.175
[42,     1] loss: 0.175
[43,     1] loss: 0.169
[44,     1] loss: 0.161
[45,     1] loss: 0.154
[46,     1] loss: 0.150
[47,     1] loss: 0.188
[48,     1] loss: 0.154
[49,     1] loss: 0.256
[50,     1] loss: 0.437
[51,     1] loss: 0.349
[52,     1] loss: 0.307
[53,     1] loss: 0.326
[54,     1] loss: 0.311
[55,     1] loss: 0.306
[56,     1] loss: 0.293
[57,     1] loss: 0.291
[58,     1] loss: 0.281
[59,     1] loss: 0.273
[60,     1] loss: 0.269
[61,     1] loss: 0.259
[62,     1] loss: 0.251
[63,     1] loss: 0.244
[64,     1] loss: 0.237
[65,     1] loss: 0.231
[66,     1] loss: 0.218
[67,     1] loss: 0.211
[68,     1] loss: 0.201
[69,     1] loss: 0.196
[70,     1] loss: 0.190
[71,     1] loss: 0.185
[72,     1] loss: 0.181
[73,     1] loss: 0.177
[74,     1] loss: 0.174
[75,     1] loss: 0.170
[76,     1] loss: 0.169
[77,     1] loss: 0.167
[78,     1] loss: 0.165
[79,     1] loss: 0.162
[80,     1] loss: 0.162
[81,     1] loss: 0.161
[82,     1] loss: 0.160
[83,     1] loss: 0.157
[84,     1] loss: 0.157
[85,     1] loss: 0.157
[86,     1] loss: 0.156
[87,     1] loss: 0.154
[88,     1] loss: 0.155
[89,     1] loss: 0.150
[90,     1] loss: 0.152
[91,     1] loss: 0.155
[92,     1] loss: 0.156
[93,     1] loss: 0.226
[94,     1] loss: 0.184
[95,     1] loss: 0.320
[96,     1] loss: 1.081
[97,     1] loss: 0.820
[98,     1] loss: 0.645
[99,     1] loss: 0.629
[100,     1] loss: 0.666
[101,     1] loss: 0.675
[102,     1] loss: 0.664
[103,     1] loss: 0.656
[104,     1] loss: 0.652
[105,     1] loss: 0.652
[106,     1] loss: 0.659
[107,     1] loss: 0.665
[108,     1] loss: 0.670
[109,     1] loss: 0.674
[110,     1] loss: 0.677
[111,     1] loss: 0.680
[112,     1] loss: 0.682
[113,     1] loss: 0.684
[114,     1] loss: 0.685
[115,     1] loss: 0.687
[116,     1] loss: 0.688
[117,     1] loss: 0.689
[118,     1] loss: 0.689
[119,     1] loss: 0.690
[120,     1] loss: 0.691
[121,     1] loss: 0.691
[122,     1] loss: 0.692
Early stopping applied (best metric=0.4057733118534088)
Finished Training
Total time taken: 126.00986647605896
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.658
[4,     1] loss: 0.643
[5,     1] loss: 0.629
[6,     1] loss: 0.615
[7,     1] loss: 0.599
[8,     1] loss: 0.581
[9,     1] loss: 0.565
[10,     1] loss: 0.544
[11,     1] loss: 0.525
[12,     1] loss: 0.505
[13,     1] loss: 0.486
[14,     1] loss: 0.462
[15,     1] loss: 0.444
[16,     1] loss: 0.431
[17,     1] loss: 0.403
[18,     1] loss: 0.403
[19,     1] loss: 0.378
[20,     1] loss: 0.367
[21,     1] loss: 0.353
[22,     1] loss: 0.345
[23,     1] loss: 0.338
[24,     1] loss: 0.328
[25,     1] loss: 0.320
[26,     1] loss: 0.312
[27,     1] loss: 0.308
[28,     1] loss: 0.300
[29,     1] loss: 0.292
[30,     1] loss: 0.287
[31,     1] loss: 0.280
[32,     1] loss: 0.277
[33,     1] loss: 0.270
[34,     1] loss: 0.263
[35,     1] loss: 0.259
[36,     1] loss: 0.251
[37,     1] loss: 0.248
[38,     1] loss: 0.243
[39,     1] loss: 0.242
[40,     1] loss: 0.245
[41,     1] loss: 0.258
[42,     1] loss: 0.250
[43,     1] loss: 0.255
[44,     1] loss: 0.250
[45,     1] loss: 0.249
[46,     1] loss: 0.252
[47,     1] loss: 0.263
[48,     1] loss: 0.291
[49,     1] loss: 0.265
[50,     1] loss: 0.252
[51,     1] loss: 0.251
[52,     1] loss: 0.238
[53,     1] loss: 0.231
[54,     1] loss: 0.233
[55,     1] loss: 0.231
[56,     1] loss: 0.223
[57,     1] loss: 0.209
[58,     1] loss: 0.204
[59,     1] loss: 0.200
[60,     1] loss: 0.195
[61,     1] loss: 0.191
[62,     1] loss: 0.184
[63,     1] loss: 0.180
[64,     1] loss: 0.176
[65,     1] loss: 0.171
[66,     1] loss: 0.170
[67,     1] loss: 0.167
[68,     1] loss: 0.163
[69,     1] loss: 0.163
[70,     1] loss: 0.159
[71,     1] loss: 0.159
[72,     1] loss: 0.157
[73,     1] loss: 0.155
[74,     1] loss: 0.154
[75,     1] loss: 0.153
[76,     1] loss: 0.153
[77,     1] loss: 0.149
[78,     1] loss: 0.150
[79,     1] loss: 0.147
[80,     1] loss: 0.149
[81,     1] loss: 0.149
[82,     1] loss: 0.146
[83,     1] loss: 0.147
[84,     1] loss: 0.146
[85,     1] loss: 0.146
[86,     1] loss: 0.144
[87,     1] loss: 0.145
[88,     1] loss: 0.146
[89,     1] loss: 0.266
[90,     1] loss: 0.997
[91,     1] loss: 0.947
[92,     1] loss: 0.758
[93,     1] loss: 0.664
[94,     1] loss: 0.656
[95,     1] loss: 0.663
[96,     1] loss: 0.671
[97,     1] loss: 0.676
[98,     1] loss: 0.679
[99,     1] loss: 0.682
[100,     1] loss: 0.684
[101,     1] loss: 0.686
[102,     1] loss: 0.687
[103,     1] loss: 0.688
[104,     1] loss: 0.689
[105,     1] loss: 0.690
[106,     1] loss: 0.690
Early stopping applied (best metric=0.309944212436676)
Finished Training
Total time taken: 110.26748156547546
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.687
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.650
[6,     1] loss: 0.636
[7,     1] loss: 0.619
[8,     1] loss: 0.600
[9,     1] loss: 0.579
[10,     1] loss: 0.556
[11,     1] loss: 0.531
[12,     1] loss: 0.509
[13,     1] loss: 0.488
[14,     1] loss: 0.467
[15,     1] loss: 0.448
[16,     1] loss: 0.429
[17,     1] loss: 0.414
[18,     1] loss: 0.398
[19,     1] loss: 0.384
[20,     1] loss: 0.370
[21,     1] loss: 0.356
[22,     1] loss: 0.343
[23,     1] loss: 0.331
[24,     1] loss: 0.318
[25,     1] loss: 0.304
[26,     1] loss: 0.296
[27,     1] loss: 0.283
[28,     1] loss: 0.275
[29,     1] loss: 0.262
[30,     1] loss: 0.256
[31,     1] loss: 0.247
[32,     1] loss: 0.240
[33,     1] loss: 0.230
[34,     1] loss: 0.226
[35,     1] loss: 0.219
[36,     1] loss: 0.214
[37,     1] loss: 0.209
[38,     1] loss: 0.204
[39,     1] loss: 0.200
[40,     1] loss: 0.196
[41,     1] loss: 0.192
[42,     1] loss: 0.189
[43,     1] loss: 0.187
[44,     1] loss: 0.183
[45,     1] loss: 0.181
[46,     1] loss: 0.177
[47,     1] loss: 0.210
[48,     1] loss: 0.400
[49,     1] loss: 0.512
[50,     1] loss: 0.499
[51,     1] loss: 0.471
[52,     1] loss: 0.438
[53,     1] loss: 0.418
[54,     1] loss: 0.395
[55,     1] loss: 0.389
[56,     1] loss: 0.379
[57,     1] loss: 0.364
[58,     1] loss: 0.353
[59,     1] loss: 0.344
[60,     1] loss: 0.336
[61,     1] loss: 0.330
[62,     1] loss: 0.324
[63,     1] loss: 0.316
[64,     1] loss: 0.312
[65,     1] loss: 0.307
[66,     1] loss: 0.303
[67,     1] loss: 0.297
[68,     1] loss: 0.294
[69,     1] loss: 0.289
[70,     1] loss: 0.287
[71,     1] loss: 0.279
[72,     1] loss: 0.275
[73,     1] loss: 0.266
[74,     1] loss: 0.259
[75,     1] loss: 0.259
[76,     1] loss: 0.248
[77,     1] loss: 0.264
[78,     1] loss: 0.268
[79,     1] loss: 0.258
[80,     1] loss: 0.261
[81,     1] loss: 0.246
[82,     1] loss: 0.244
[83,     1] loss: 0.242
[84,     1] loss: 0.242
[85,     1] loss: 0.229
[86,     1] loss: 0.223
[87,     1] loss: 0.235
[88,     1] loss: 0.238
[89,     1] loss: 0.240
[90,     1] loss: 0.241
[91,     1] loss: 0.439
[92,     1] loss: 0.364
[93,     1] loss: 0.268
[94,     1] loss: 0.306
[95,     1] loss: 0.262
Early stopping applied (best metric=0.35542407631874084)
Finished Training
Total time taken: 99.77290654182434
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.680
[3,     1] loss: 0.664
[4,     1] loss: 0.648
[5,     1] loss: 0.634
[6,     1] loss: 0.617
[7,     1] loss: 0.601
[8,     1] loss: 0.583
[9,     1] loss: 0.565
[10,     1] loss: 0.545
[11,     1] loss: 0.524
[12,     1] loss: 0.502
[13,     1] loss: 0.480
[14,     1] loss: 0.458
[15,     1] loss: 0.437
[16,     1] loss: 0.413
[17,     1] loss: 0.392
[18,     1] loss: 0.368
[19,     1] loss: 0.353
[20,     1] loss: 0.330
[21,     1] loss: 0.313
[22,     1] loss: 0.298
[23,     1] loss: 0.278
[24,     1] loss: 0.270
[25,     1] loss: 0.254
[26,     1] loss: 0.242
[27,     1] loss: 0.230
[28,     1] loss: 0.220
[29,     1] loss: 0.210
[30,     1] loss: 0.202
[31,     1] loss: 0.195
[32,     1] loss: 0.189
[33,     1] loss: 0.185
[34,     1] loss: 0.181
[35,     1] loss: 0.173
[36,     1] loss: 0.171
[37,     1] loss: 0.164
[38,     1] loss: 0.162
[39,     1] loss: 0.161
[40,     1] loss: 0.160
[41,     1] loss: 0.161
[42,     1] loss: 0.181
[43,     1] loss: 0.227
[44,     1] loss: 0.651
[45,     1] loss: 0.504
[46,     1] loss: 0.349
[47,     1] loss: 0.389
[48,     1] loss: 0.390
[49,     1] loss: 0.383
[50,     1] loss: 0.390
[51,     1] loss: 0.388
[52,     1] loss: 0.378
[53,     1] loss: 0.371
[54,     1] loss: 0.357
[55,     1] loss: 0.349
[56,     1] loss: 0.338
[57,     1] loss: 0.326
[58,     1] loss: 0.316
[59,     1] loss: 0.306
[60,     1] loss: 0.295
[61,     1] loss: 0.284
[62,     1] loss: 0.273
[63,     1] loss: 0.261
[64,     1] loss: 0.252
[65,     1] loss: 0.242
[66,     1] loss: 0.234
[67,     1] loss: 0.224
[68,     1] loss: 0.217
[69,     1] loss: 0.212
[70,     1] loss: 0.244
[71,     1] loss: 0.226
[72,     1] loss: 0.264
[73,     1] loss: 0.265
[74,     1] loss: 0.283
[75,     1] loss: 0.273
[76,     1] loss: 0.264
[77,     1] loss: 0.256
[78,     1] loss: 0.253
[79,     1] loss: 0.245
[80,     1] loss: 0.240
[81,     1] loss: 0.229
[82,     1] loss: 0.226
[83,     1] loss: 0.221
[84,     1] loss: 0.214
[85,     1] loss: 0.208
[86,     1] loss: 0.206
[87,     1] loss: 0.200
[88,     1] loss: 0.200
[89,     1] loss: 0.196
[90,     1] loss: 0.197
[91,     1] loss: 0.203
[92,     1] loss: 0.201
[93,     1] loss: 0.205
[94,     1] loss: 0.197
[95,     1] loss: 0.193
[96,     1] loss: 0.192
[97,     1] loss: 0.186
[98,     1] loss: 0.181
[99,     1] loss: 0.180
[100,     1] loss: 0.175
[101,     1] loss: 0.171
[102,     1] loss: 0.169
[103,     1] loss: 0.168
[104,     1] loss: 0.165
[105,     1] loss: 0.167
[106,     1] loss: 0.162
[107,     1] loss: 0.163
[108,     1] loss: 0.164
[109,     1] loss: 0.162
[110,     1] loss: 0.162
[111,     1] loss: 0.162
[112,     1] loss: 0.162
[113,     1] loss: 0.163
[114,     1] loss: 0.163
[115,     1] loss: 0.177
[116,     1] loss: 0.309
[117,     1] loss: 0.772
[118,     1] loss: 0.579
[119,     1] loss: 0.577
[120,     1] loss: 0.581
[121,     1] loss: 0.574
[122,     1] loss: 0.556
[123,     1] loss: 0.536
[124,     1] loss: 0.522
[125,     1] loss: 0.521
[126,     1] loss: 0.519
[127,     1] loss: 0.519
[128,     1] loss: 0.518
[129,     1] loss: 0.513
[130,     1] loss: 0.508
[131,     1] loss: 0.499
[132,     1] loss: 0.492
Early stopping applied (best metric=0.37621286511421204)
Finished Training
Total time taken: 139.08829069137573
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.664
[4,     1] loss: 0.650
[5,     1] loss: 0.636
[6,     1] loss: 0.621
[7,     1] loss: 0.604
[8,     1] loss: 0.586
[9,     1] loss: 0.570
[10,     1] loss: 0.549
[11,     1] loss: 0.531
[12,     1] loss: 0.507
[13,     1] loss: 0.489
[14,     1] loss: 0.467
[15,     1] loss: 0.445
[16,     1] loss: 0.424
[17,     1] loss: 0.404
[18,     1] loss: 0.389
[19,     1] loss: 0.378
[20,     1] loss: 0.365
[21,     1] loss: 0.353
[22,     1] loss: 0.344
[23,     1] loss: 0.335
[24,     1] loss: 0.328
[25,     1] loss: 0.319
[26,     1] loss: 0.310
[27,     1] loss: 0.305
[28,     1] loss: 0.295
[29,     1] loss: 0.288
[30,     1] loss: 0.282
[31,     1] loss: 0.278
[32,     1] loss: 0.271
[33,     1] loss: 0.264
[34,     1] loss: 0.259
[35,     1] loss: 0.252
[36,     1] loss: 0.246
[37,     1] loss: 0.240
[38,     1] loss: 0.234
[39,     1] loss: 0.229
[40,     1] loss: 0.225
[41,     1] loss: 0.220
[42,     1] loss: 0.213
[43,     1] loss: 0.208
[44,     1] loss: 0.206
[45,     1] loss: 0.199
[46,     1] loss: 0.198
[47,     1] loss: 0.194
[48,     1] loss: 0.191
[49,     1] loss: 0.189
[50,     1] loss: 0.198
[51,     1] loss: 0.268
[52,     1] loss: 0.616
[53,     1] loss: 0.424
[54,     1] loss: 0.479
[55,     1] loss: 0.460
[56,     1] loss: 0.484
[57,     1] loss: 0.491
[58,     1] loss: 0.497
[59,     1] loss: 0.502
[60,     1] loss: 0.508
[61,     1] loss: 0.508
[62,     1] loss: 0.507
[63,     1] loss: 0.508
[64,     1] loss: 0.503
[65,     1] loss: 0.498
[66,     1] loss: 0.490
[67,     1] loss: 0.483
[68,     1] loss: 0.474
[69,     1] loss: 0.462
[70,     1] loss: 0.451
[71,     1] loss: 0.438
[72,     1] loss: 0.425
[73,     1] loss: 0.408
[74,     1] loss: 0.393
[75,     1] loss: 0.377
[76,     1] loss: 0.361
[77,     1] loss: 0.346
[78,     1] loss: 0.330
[79,     1] loss: 0.315
[80,     1] loss: 0.304
[81,     1] loss: 0.291
[82,     1] loss: 0.281
[83,     1] loss: 0.271
[84,     1] loss: 0.260
[85,     1] loss: 0.255
[86,     1] loss: 0.246
[87,     1] loss: 0.237
[88,     1] loss: 0.230
[89,     1] loss: 0.227
[90,     1] loss: 0.218
[91,     1] loss: 0.214
[92,     1] loss: 0.211
[93,     1] loss: 0.203
[94,     1] loss: 0.202
[95,     1] loss: 0.195
[96,     1] loss: 0.202
[97,     1] loss: 0.257
[98,     1] loss: 0.333
[99,     1] loss: 0.612
[100,     1] loss: 0.515
[101,     1] loss: 0.460
[102,     1] loss: 0.454
[103,     1] loss: 0.444
[104,     1] loss: 0.447
[105,     1] loss: 0.451
[106,     1] loss: 0.450
[107,     1] loss: 0.455
[108,     1] loss: 0.451
[109,     1] loss: 0.449
[110,     1] loss: 0.442
[111,     1] loss: 0.435
[112,     1] loss: 0.426
[113,     1] loss: 0.420
[114,     1] loss: 0.409
[115,     1] loss: 0.400
[116,     1] loss: 0.391
[117,     1] loss: 0.380
[118,     1] loss: 0.368
[119,     1] loss: 0.354
[120,     1] loss: 0.339
[121,     1] loss: 0.326
[122,     1] loss: 0.310
[123,     1] loss: 0.296
[124,     1] loss: 0.285
[125,     1] loss: 0.288
[126,     1] loss: 0.269
[127,     1] loss: 0.259
[128,     1] loss: 0.247
[129,     1] loss: 0.273
[130,     1] loss: 0.257
[131,     1] loss: 0.239
[132,     1] loss: 0.246
[133,     1] loss: 0.245
[134,     1] loss: 0.249
[135,     1] loss: 0.230
[136,     1] loss: 0.222
[137,     1] loss: 0.211
[138,     1] loss: 0.226
[139,     1] loss: 0.214
[140,     1] loss: 0.206
[141,     1] loss: 0.198
[142,     1] loss: 0.197
Early stopping applied (best metric=0.3840838670730591)
Finished Training
Total time taken: 150.69463062286377
{'Hydroxylation-P Validation Accuracy': 0.7953636668189432, 'Hydroxylation-P Validation Sensitivity': 0.7887619047619048, 'Hydroxylation-P Validation Specificity': 0.7968008379470298, 'Hydroxylation-P Validation Precision': 0.4703452198323395, 'Hydroxylation-P AUC ROC': 0.8602815720532315, 'Hydroxylation-P AUC PR': 0.6476641546724885, 'Hydroxylation-P MCC': 0.49335877915647053, 'Hydroxylation-P F1': 0.5833422139530567, 'Validation Loss (Hydroxylation-P)': 0.37038121223449705, 'Validation Loss (total)': 0.37038121223449705}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006484210650910772,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3910568193,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.549754404367189}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.683
[3,     1] loss: 0.658
[4,     1] loss: 0.634
[5,     1] loss: 0.603
[6,     1] loss: 0.565
[7,     1] loss: 0.523
[8,     1] loss: 0.482
[9,     1] loss: 0.440
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006264103100115989,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2144511763,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.9545119479064175}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.654
[4,     1] loss: 0.616
[5,     1] loss: 0.569
[6,     1] loss: 0.510
[7,     1] loss: 0.439
[8,     1] loss: 0.368
[9,     1] loss: 0.308
[10,     1] loss: 0.288
[11,     1] loss: 0.200
[12,     1] loss: 0.178
[13,     1] loss: 0.145
[14,     1] loss: 0.125
[15,     1] loss: 0.095
[16,     1] loss: 0.091
[17,     1] loss: 0.075
[18,     1] loss: 0.063
[19,     1] loss: 0.054
[20,     1] loss: 0.047
[21,     1] loss: 0.040
[22,     1] loss: 0.036
[23,     1] loss: 0.030
[24,     1] loss: 0.025
[25,     1] loss: 0.018
[26,     1] loss: 0.021
[27,     1] loss: 0.014
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00962107774706142,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2601530368,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.835184391481988}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.631
[4,     1] loss: 0.586
[5,     1] loss: 0.539
[6,     1] loss: 0.485
[7,     1] loss: 0.437
[8,     1] loss: 0.385
[9,     1] loss: 0.345
[10,     1] loss: 0.280
[11,     1] loss: 0.217
[12,     1] loss: 0.182
[13,     1] loss: 0.213
[14,     1] loss: 0.172
[15,     1] loss: 0.161
[16,     1] loss: 0.112
[17,     1] loss: 0.088
[18,     1] loss: 0.088
[19,     1] loss: 0.072
[20,     1] loss: 0.048
[21,     1] loss: 0.038
[22,     1] loss: 0.031
[23,     1] loss: 0.020
[24,     1] loss: 0.013
[25,     1] loss: 0.010
[26,     1] loss: 0.007
[27,     1] loss: 0.005
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004105466054212916,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3526908005,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.707694079990175}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.662
[4,     1] loss: 0.629
[5,     1] loss: 0.589
[6,     1] loss: 0.542
[7,     1] loss: 0.501
[8,     1] loss: 0.464
[9,     1] loss: 0.429
[10,     1] loss: 0.400
[11,     1] loss: 0.383
[12,     1] loss: 0.367
[13,     1] loss: 0.350
[14,     1] loss: 0.333
[15,     1] loss: 0.312
[16,     1] loss: 0.291
[17,     1] loss: 0.270
[18,     1] loss: 0.244
[19,     1] loss: 0.221
[20,     1] loss: 0.195
[21,     1] loss: 0.173
[22,     1] loss: 0.151
[23,     1] loss: 0.131
[24,     1] loss: 0.118
[25,     1] loss: 0.103
[26,     1] loss: 0.087
[27,     1] loss: 0.070
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003337781990183057,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2156089051,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.575931574107255}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.651
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00045828963674642085,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 572542605,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.06122601981472}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.695
[3,     1] loss: 0.685
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00037658959397531954,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1028795380,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.975803815271841}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.671
[3,     1] loss: 0.656
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0065435583626000235,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 70495629,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.431505390292061}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.661
[4,     1] loss: 0.639
[5,     1] loss: 0.607
[6,     1] loss: 0.578
[7,     1] loss: 0.545
[8,     1] loss: 0.514
[9,     1] loss: 0.480
[10,     1] loss: 0.452
[11,     1] loss: 0.438
[12,     1] loss: 0.433
[13,     1] loss: 0.395
[14,     1] loss: 0.378
[15,     1] loss: 0.362
[16,     1] loss: 0.361
[17,     1] loss: 0.331
[18,     1] loss: 0.323
[19,     1] loss: 0.294
[20,     1] loss: 0.286
[21,     1] loss: 0.258
[22,     1] loss: 0.235
[23,     1] loss: 0.214
[24,     1] loss: 0.194
[25,     1] loss: 0.175
[26,     1] loss: 0.160
[27,     1] loss: 0.142
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006338635120100782,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 145969101,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9523526869782417}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.671
[4,     1] loss: 0.660
[5,     1] loss: 0.648
[6,     1] loss: 0.635
[7,     1] loss: 0.624
[8,     1] loss: 0.613
[9,     1] loss: 0.600
[10,     1] loss: 0.588
[11,     1] loss: 0.576
[12,     1] loss: 0.558
[13,     1] loss: 0.544
[14,     1] loss: 0.530
[15,     1] loss: 0.514
[16,     1] loss: 0.498
[17,     1] loss: 0.482
[18,     1] loss: 0.466
[19,     1] loss: 0.447
[20,     1] loss: 0.430
[21,     1] loss: 0.411
[22,     1] loss: 0.397
[23,     1] loss: 0.378
[24,     1] loss: 0.358
[25,     1] loss: 0.341
[26,     1] loss: 0.322
[27,     1] loss: 0.305
[28,     1] loss: 0.288
[29,     1] loss: 0.272
[30,     1] loss: 0.254
[31,     1] loss: 0.238
[32,     1] loss: 0.223
[33,     1] loss: 0.210
[34,     1] loss: 0.192
[35,     1] loss: 0.180
[36,     1] loss: 0.167
[37,     1] loss: 0.155
[38,     1] loss: 0.140
[39,     1] loss: 0.131
[40,     1] loss: 0.120
[41,     1] loss: 0.112
[42,     1] loss: 0.102
[43,     1] loss: 0.091
[44,     1] loss: 0.088
[45,     1] loss: 0.079
[46,     1] loss: 0.073
[47,     1] loss: 0.066
[48,     1] loss: 0.060
[49,     1] loss: 0.056
[50,     1] loss: 0.050
[51,     1] loss: 0.045
[52,     1] loss: 0.043
[53,     1] loss: 0.039
[54,     1] loss: 0.036
[55,     1] loss: 0.034
[56,     1] loss: 0.031
[57,     1] loss: 0.029
[58,     1] loss: 0.027
[59,     1] loss: 0.024
[60,     1] loss: 0.022
[61,     1] loss: 0.020
[62,     1] loss: 0.019
[63,     1] loss: 0.019
[64,     1] loss: 0.017
[65,     1] loss: 0.017
[66,     1] loss: 0.015
[67,     1] loss: 0.014
[68,     1] loss: 0.013
[69,     1] loss: 0.013
[70,     1] loss: 0.013
[71,     1] loss: 0.012
[72,     1] loss: 0.011
[73,     1] loss: 0.010
[74,     1] loss: 0.010
[75,     1] loss: 0.010
Early stopping applied (best metric=0.40678709745407104)
Finished Training
Total time taken: 81.12010431289673
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.664
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.622
[7,     1] loss: 0.608
[8,     1] loss: 0.594
[9,     1] loss: 0.580
[10,     1] loss: 0.570
[11,     1] loss: 0.558
[12,     1] loss: 0.545
[13,     1] loss: 0.532
[14,     1] loss: 0.517
[15,     1] loss: 0.504
[16,     1] loss: 0.488
[17,     1] loss: 0.480
[18,     1] loss: 0.468
[19,     1] loss: 0.453
[20,     1] loss: 0.440
[21,     1] loss: 0.433
[22,     1] loss: 0.420
[23,     1] loss: 0.410
[24,     1] loss: 0.401
[25,     1] loss: 0.389
[26,     1] loss: 0.382
[27,     1] loss: 0.373
[28,     1] loss: 0.367
[29,     1] loss: 0.357
[30,     1] loss: 0.348
[31,     1] loss: 0.340
[32,     1] loss: 0.335
[33,     1] loss: 0.328
[34,     1] loss: 0.321
[35,     1] loss: 0.314
[36,     1] loss: 0.307
[37,     1] loss: 0.299
[38,     1] loss: 0.293
[39,     1] loss: 0.286
[40,     1] loss: 0.278
[41,     1] loss: 0.272
[42,     1] loss: 0.261
[43,     1] loss: 0.256
[44,     1] loss: 0.247
[45,     1] loss: 0.238
[46,     1] loss: 0.232
[47,     1] loss: 0.224
[48,     1] loss: 0.216
[49,     1] loss: 0.208
[50,     1] loss: 0.198
[51,     1] loss: 0.191
[52,     1] loss: 0.185
[53,     1] loss: 0.175
[54,     1] loss: 0.169
[55,     1] loss: 0.159
[56,     1] loss: 0.153
[57,     1] loss: 0.148
[58,     1] loss: 0.143
[59,     1] loss: 0.137
[60,     1] loss: 0.132
[61,     1] loss: 0.126
[62,     1] loss: 0.122
[63,     1] loss: 0.117
[64,     1] loss: 0.113
[65,     1] loss: 0.106
[66,     1] loss: 0.105
[67,     1] loss: 0.100
[68,     1] loss: 0.095
[69,     1] loss: 0.090
[70,     1] loss: 0.087
[71,     1] loss: 0.085
[72,     1] loss: 0.082
Early stopping applied (best metric=0.413368821144104)
Finished Training
Total time taken: 78.32847309112549
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.683
[3,     1] loss: 0.668
[4,     1] loss: 0.654
[5,     1] loss: 0.642
[6,     1] loss: 0.630
[7,     1] loss: 0.620
[8,     1] loss: 0.608
[9,     1] loss: 0.596
[10,     1] loss: 0.581
[11,     1] loss: 0.568
[12,     1] loss: 0.555
[13,     1] loss: 0.541
[14,     1] loss: 0.522
[15,     1] loss: 0.507
[16,     1] loss: 0.493
[17,     1] loss: 0.477
[18,     1] loss: 0.460
[19,     1] loss: 0.444
[20,     1] loss: 0.425
[21,     1] loss: 0.409
[22,     1] loss: 0.393
[23,     1] loss: 0.374
[24,     1] loss: 0.351
[25,     1] loss: 0.337
[26,     1] loss: 0.321
[27,     1] loss: 0.302
[28,     1] loss: 0.286
[29,     1] loss: 0.268
[30,     1] loss: 0.253
[31,     1] loss: 0.235
[32,     1] loss: 0.220
[33,     1] loss: 0.204
[34,     1] loss: 0.194
[35,     1] loss: 0.178
[36,     1] loss: 0.169
[37,     1] loss: 0.156
[38,     1] loss: 0.144
[39,     1] loss: 0.134
[40,     1] loss: 0.127
[41,     1] loss: 0.118
[42,     1] loss: 0.109
[43,     1] loss: 0.102
[44,     1] loss: 0.095
[45,     1] loss: 0.087
[46,     1] loss: 0.083
[47,     1] loss: 0.076
[48,     1] loss: 0.072
[49,     1] loss: 0.067
[50,     1] loss: 0.062
[51,     1] loss: 0.060
[52,     1] loss: 0.054
[53,     1] loss: 0.050
[54,     1] loss: 0.048
[55,     1] loss: 0.044
[56,     1] loss: 0.041
[57,     1] loss: 0.038
[58,     1] loss: 0.037
[59,     1] loss: 0.035
[60,     1] loss: 0.032
[61,     1] loss: 0.031
[62,     1] loss: 0.030
[63,     1] loss: 0.026
[64,     1] loss: 0.027
[65,     1] loss: 0.025
[66,     1] loss: 0.023
[67,     1] loss: 0.023
[68,     1] loss: 0.021
[69,     1] loss: 0.021
[70,     1] loss: 0.020
[71,     1] loss: 0.019
[72,     1] loss: 0.018
[73,     1] loss: 0.017
[74,     1] loss: 0.018
[75,     1] loss: 0.016
[76,     1] loss: 0.016
[77,     1] loss: 0.015
[78,     1] loss: 0.014
[79,     1] loss: 0.014
[80,     1] loss: 0.013
[81,     1] loss: 0.013
[82,     1] loss: 0.013
[83,     1] loss: 0.013
Early stopping applied (best metric=0.3412650227546692)
Finished Training
Total time taken: 90.43495607376099
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.674
[4,     1] loss: 0.664
[5,     1] loss: 0.655
[6,     1] loss: 0.646
[7,     1] loss: 0.638
[8,     1] loss: 0.627
[9,     1] loss: 0.619
[10,     1] loss: 0.608
[11,     1] loss: 0.596
[12,     1] loss: 0.585
[13,     1] loss: 0.575
[14,     1] loss: 0.564
[15,     1] loss: 0.549
[16,     1] loss: 0.536
[17,     1] loss: 0.523
[18,     1] loss: 0.510
[19,     1] loss: 0.497
[20,     1] loss: 0.481
[21,     1] loss: 0.464
[22,     1] loss: 0.449
[23,     1] loss: 0.432
[24,     1] loss: 0.416
[25,     1] loss: 0.398
[26,     1] loss: 0.384
[27,     1] loss: 0.361
[28,     1] loss: 0.343
[29,     1] loss: 0.329
[30,     1] loss: 0.311
[31,     1] loss: 0.294
[32,     1] loss: 0.276
[33,     1] loss: 0.257
[34,     1] loss: 0.246
[35,     1] loss: 0.228
[36,     1] loss: 0.212
[37,     1] loss: 0.198
[38,     1] loss: 0.184
[39,     1] loss: 0.172
[40,     1] loss: 0.159
[41,     1] loss: 0.149
[42,     1] loss: 0.140
[43,     1] loss: 0.129
[44,     1] loss: 0.120
[45,     1] loss: 0.112
[46,     1] loss: 0.105
[47,     1] loss: 0.098
[48,     1] loss: 0.091
[49,     1] loss: 0.086
[50,     1] loss: 0.080
[51,     1] loss: 0.076
[52,     1] loss: 0.070
[53,     1] loss: 0.065
[54,     1] loss: 0.061
[55,     1] loss: 0.058
[56,     1] loss: 0.052
[57,     1] loss: 0.051
[58,     1] loss: 0.047
[59,     1] loss: 0.045
[60,     1] loss: 0.042
[61,     1] loss: 0.040
[62,     1] loss: 0.038
[63,     1] loss: 0.035
[64,     1] loss: 0.033
[65,     1] loss: 0.031
[66,     1] loss: 0.029
[67,     1] loss: 0.028
[68,     1] loss: 0.026
[69,     1] loss: 0.026
[70,     1] loss: 0.024
[71,     1] loss: 0.023
[72,     1] loss: 0.021
[73,     1] loss: 0.021
[74,     1] loss: 0.020
[75,     1] loss: 0.019
[76,     1] loss: 0.017
[77,     1] loss: 0.017
Early stopping applied (best metric=0.45881569385528564)
Finished Training
Total time taken: 82.26617908477783
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.649
[6,     1] loss: 0.636
[7,     1] loss: 0.623
[8,     1] loss: 0.608
[9,     1] loss: 0.595
[10,     1] loss: 0.584
[11,     1] loss: 0.566
[12,     1] loss: 0.553
[13,     1] loss: 0.537
[14,     1] loss: 0.520
[15,     1] loss: 0.501
[16,     1] loss: 0.486
[17,     1] loss: 0.467
[18,     1] loss: 0.444
[19,     1] loss: 0.424
[20,     1] loss: 0.407
[21,     1] loss: 0.385
[22,     1] loss: 0.365
[23,     1] loss: 0.349
[24,     1] loss: 0.326
[25,     1] loss: 0.311
[26,     1] loss: 0.291
[27,     1] loss: 0.268
[28,     1] loss: 0.249
[29,     1] loss: 0.235
[30,     1] loss: 0.218
[31,     1] loss: 0.200
[32,     1] loss: 0.187
[33,     1] loss: 0.174
[34,     1] loss: 0.162
[35,     1] loss: 0.148
[36,     1] loss: 0.134
[37,     1] loss: 0.125
[38,     1] loss: 0.115
[39,     1] loss: 0.105
[40,     1] loss: 0.096
[41,     1] loss: 0.086
[42,     1] loss: 0.080
[43,     1] loss: 0.073
[44,     1] loss: 0.066
[45,     1] loss: 0.061
[46,     1] loss: 0.056
[47,     1] loss: 0.051
[48,     1] loss: 0.046
[49,     1] loss: 0.043
[50,     1] loss: 0.038
[51,     1] loss: 0.036
[52,     1] loss: 0.033
[53,     1] loss: 0.030
[54,     1] loss: 0.029
[55,     1] loss: 0.025
[56,     1] loss: 0.024
[57,     1] loss: 0.023
[58,     1] loss: 0.020
[59,     1] loss: 0.019
[60,     1] loss: 0.019
[61,     1] loss: 0.018
[62,     1] loss: 0.017
[63,     1] loss: 0.016
[64,     1] loss: 0.015
[65,     1] loss: 0.015
[66,     1] loss: 0.013
[67,     1] loss: 0.013
[68,     1] loss: 0.012
[69,     1] loss: 0.012
[70,     1] loss: 0.011
[71,     1] loss: 0.011
[72,     1] loss: 0.011
Early stopping applied (best metric=0.4490036368370056)
Finished Training
Total time taken: 77.19446301460266
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.683
[3,     1] loss: 0.672
[4,     1] loss: 0.660
[5,     1] loss: 0.650
[6,     1] loss: 0.638
[7,     1] loss: 0.626
[8,     1] loss: 0.615
[9,     1] loss: 0.605
[10,     1] loss: 0.590
[11,     1] loss: 0.577
[12,     1] loss: 0.561
[13,     1] loss: 0.549
[14,     1] loss: 0.537
[15,     1] loss: 0.514
[16,     1] loss: 0.501
[17,     1] loss: 0.483
[18,     1] loss: 0.473
[19,     1] loss: 0.453
[20,     1] loss: 0.436
[21,     1] loss: 0.418
[22,     1] loss: 0.403
[23,     1] loss: 0.383
[24,     1] loss: 0.368
[25,     1] loss: 0.349
[26,     1] loss: 0.333
[27,     1] loss: 0.315
[28,     1] loss: 0.301
[29,     1] loss: 0.282
[30,     1] loss: 0.269
[31,     1] loss: 0.253
[32,     1] loss: 0.242
[33,     1] loss: 0.227
[34,     1] loss: 0.215
[35,     1] loss: 0.200
[36,     1] loss: 0.189
[37,     1] loss: 0.176
[38,     1] loss: 0.166
[39,     1] loss: 0.153
[40,     1] loss: 0.143
[41,     1] loss: 0.134
[42,     1] loss: 0.127
[43,     1] loss: 0.118
[44,     1] loss: 0.110
[45,     1] loss: 0.102
[46,     1] loss: 0.095
[47,     1] loss: 0.089
[48,     1] loss: 0.081
[49,     1] loss: 0.077
[50,     1] loss: 0.072
[51,     1] loss: 0.066
[52,     1] loss: 0.062
[53,     1] loss: 0.059
[54,     1] loss: 0.055
[55,     1] loss: 0.052
[56,     1] loss: 0.049
[57,     1] loss: 0.045
[58,     1] loss: 0.043
[59,     1] loss: 0.042
[60,     1] loss: 0.039
[61,     1] loss: 0.037
[62,     1] loss: 0.036
[63,     1] loss: 0.033
[64,     1] loss: 0.032
[65,     1] loss: 0.031
[66,     1] loss: 0.030
[67,     1] loss: 0.028
[68,     1] loss: 0.028
[69,     1] loss: 0.026
Early stopping applied (best metric=0.4748055934906006)
Finished Training
Total time taken: 74.34944581985474
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.693
[3,     1] loss: 0.685
[4,     1] loss: 0.674
[5,     1] loss: 0.664
[6,     1] loss: 0.652
[7,     1] loss: 0.641
[8,     1] loss: 0.629
[9,     1] loss: 0.617
[10,     1] loss: 0.605
[11,     1] loss: 0.590
[12,     1] loss: 0.576
[13,     1] loss: 0.564
[14,     1] loss: 0.550
[15,     1] loss: 0.534
[16,     1] loss: 0.518
[17,     1] loss: 0.501
[18,     1] loss: 0.486
[19,     1] loss: 0.472
[20,     1] loss: 0.453
[21,     1] loss: 0.438
[22,     1] loss: 0.423
[23,     1] loss: 0.405
[24,     1] loss: 0.389
[25,     1] loss: 0.377
[26,     1] loss: 0.358
[27,     1] loss: 0.343
[28,     1] loss: 0.326
[29,     1] loss: 0.316
[30,     1] loss: 0.301
[31,     1] loss: 0.286
[32,     1] loss: 0.273
[33,     1] loss: 0.257
[34,     1] loss: 0.241
[35,     1] loss: 0.232
[36,     1] loss: 0.217
[37,     1] loss: 0.207
[38,     1] loss: 0.193
[39,     1] loss: 0.181
[40,     1] loss: 0.172
[41,     1] loss: 0.158
[42,     1] loss: 0.151
[43,     1] loss: 0.141
[44,     1] loss: 0.131
[45,     1] loss: 0.122
[46,     1] loss: 0.112
[47,     1] loss: 0.103
[48,     1] loss: 0.096
[49,     1] loss: 0.089
[50,     1] loss: 0.082
[51,     1] loss: 0.075
[52,     1] loss: 0.070
[53,     1] loss: 0.065
[54,     1] loss: 0.062
[55,     1] loss: 0.057
[56,     1] loss: 0.052
[57,     1] loss: 0.049
[58,     1] loss: 0.046
[59,     1] loss: 0.042
[60,     1] loss: 0.038
[61,     1] loss: 0.036
[62,     1] loss: 0.034
[63,     1] loss: 0.032
[64,     1] loss: 0.030
[65,     1] loss: 0.028
[66,     1] loss: 0.029
[67,     1] loss: 0.026
[68,     1] loss: 0.024
[69,     1] loss: 0.023
[70,     1] loss: 0.022
[71,     1] loss: 0.020
[72,     1] loss: 0.020
[73,     1] loss: 0.019
[74,     1] loss: 0.017
[75,     1] loss: 0.016
[76,     1] loss: 0.015
[77,     1] loss: 0.015
[78,     1] loss: 0.015
Early stopping applied (best metric=0.4212445914745331)
Finished Training
Total time taken: 84.4745180606842
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.670
[5,     1] loss: 0.661
[6,     1] loss: 0.648
[7,     1] loss: 0.637
[8,     1] loss: 0.625
[9,     1] loss: 0.612
[10,     1] loss: 0.599
[11,     1] loss: 0.586
[12,     1] loss: 0.574
[13,     1] loss: 0.558
[14,     1] loss: 0.543
[15,     1] loss: 0.524
[16,     1] loss: 0.510
[17,     1] loss: 0.495
[18,     1] loss: 0.477
[19,     1] loss: 0.458
[20,     1] loss: 0.443
[21,     1] loss: 0.421
[22,     1] loss: 0.407
[23,     1] loss: 0.388
[24,     1] loss: 0.372
[25,     1] loss: 0.354
[26,     1] loss: 0.333
[27,     1] loss: 0.315
[28,     1] loss: 0.296
[29,     1] loss: 0.278
[30,     1] loss: 0.259
[31,     1] loss: 0.245
[32,     1] loss: 0.228
[33,     1] loss: 0.216
[34,     1] loss: 0.200
[35,     1] loss: 0.181
[36,     1] loss: 0.167
[37,     1] loss: 0.157
[38,     1] loss: 0.145
[39,     1] loss: 0.133
[40,     1] loss: 0.122
[41,     1] loss: 0.111
[42,     1] loss: 0.103
[43,     1] loss: 0.095
[44,     1] loss: 0.087
[45,     1] loss: 0.078
[46,     1] loss: 0.072
[47,     1] loss: 0.065
[48,     1] loss: 0.061
[49,     1] loss: 0.057
[50,     1] loss: 0.053
[51,     1] loss: 0.049
[52,     1] loss: 0.045
[53,     1] loss: 0.042
[54,     1] loss: 0.039
[55,     1] loss: 0.037
[56,     1] loss: 0.035
[57,     1] loss: 0.033
[58,     1] loss: 0.029
[59,     1] loss: 0.029
[60,     1] loss: 0.028
[61,     1] loss: 0.026
[62,     1] loss: 0.026
[63,     1] loss: 0.022
[64,     1] loss: 0.023
[65,     1] loss: 0.021
[66,     1] loss: 0.021
[67,     1] loss: 0.019
[68,     1] loss: 0.019
[69,     1] loss: 0.018
[70,     1] loss: 0.017
[71,     1] loss: 0.016
[72,     1] loss: 0.016
[73,     1] loss: 0.014
[74,     1] loss: 0.015
[75,     1] loss: 0.014
[76,     1] loss: 0.015
[77,     1] loss: 0.013
[78,     1] loss: 0.012
[79,     1] loss: 0.013
[80,     1] loss: 0.012
[81,     1] loss: 0.012
[82,     1] loss: 0.012
[83,     1] loss: 0.011
[84,     1] loss: 0.011
[85,     1] loss: 0.010
Early stopping applied (best metric=0.32985639572143555)
Finished Training
Total time taken: 92.31855630874634
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.680
[4,     1] loss: 0.669
[5,     1] loss: 0.659
[6,     1] loss: 0.648
[7,     1] loss: 0.636
[8,     1] loss: 0.627
[9,     1] loss: 0.615
[10,     1] loss: 0.606
[11,     1] loss: 0.591
[12,     1] loss: 0.576
[13,     1] loss: 0.566
[14,     1] loss: 0.551
[15,     1] loss: 0.537
[16,     1] loss: 0.524
[17,     1] loss: 0.511
[18,     1] loss: 0.490
[19,     1] loss: 0.477
[20,     1] loss: 0.457
[21,     1] loss: 0.445
[22,     1] loss: 0.430
[23,     1] loss: 0.409
[24,     1] loss: 0.392
[25,     1] loss: 0.372
[26,     1] loss: 0.352
[27,     1] loss: 0.332
[28,     1] loss: 0.315
[29,     1] loss: 0.294
[30,     1] loss: 0.279
[31,     1] loss: 0.258
[32,     1] loss: 0.241
[33,     1] loss: 0.221
[34,     1] loss: 0.208
[35,     1] loss: 0.191
[36,     1] loss: 0.175
[37,     1] loss: 0.161
[38,     1] loss: 0.148
[39,     1] loss: 0.132
[40,     1] loss: 0.123
[41,     1] loss: 0.112
[42,     1] loss: 0.103
[43,     1] loss: 0.094
[44,     1] loss: 0.084
[45,     1] loss: 0.076
[46,     1] loss: 0.070
[47,     1] loss: 0.065
[48,     1] loss: 0.058
[49,     1] loss: 0.052
[50,     1] loss: 0.049
[51,     1] loss: 0.043
[52,     1] loss: 0.041
[53,     1] loss: 0.038
[54,     1] loss: 0.035
[55,     1] loss: 0.033
[56,     1] loss: 0.030
[57,     1] loss: 0.028
[58,     1] loss: 0.026
[59,     1] loss: 0.025
[60,     1] loss: 0.022
[61,     1] loss: 0.022
[62,     1] loss: 0.020
[63,     1] loss: 0.019
[64,     1] loss: 0.019
[65,     1] loss: 0.018
[66,     1] loss: 0.017
[67,     1] loss: 0.016
[68,     1] loss: 0.016
[69,     1] loss: 0.014
[70,     1] loss: 0.014
[71,     1] loss: 0.013
[72,     1] loss: 0.013
[73,     1] loss: 0.013
[74,     1] loss: 0.013
[75,     1] loss: 0.012
[76,     1] loss: 0.011
[77,     1] loss: 0.011
[78,     1] loss: 0.011
[79,     1] loss: 0.010
[80,     1] loss: 0.010
Early stopping applied (best metric=0.3654330372810364)
Finished Training
Total time taken: 87.57552552223206
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.685
[3,     1] loss: 0.662
[4,     1] loss: 0.640
[5,     1] loss: 0.621
[6,     1] loss: 0.604
[7,     1] loss: 0.584
[8,     1] loss: 0.567
[9,     1] loss: 0.548
[10,     1] loss: 0.527
[11,     1] loss: 0.507
[12,     1] loss: 0.491
[13,     1] loss: 0.469
[14,     1] loss: 0.452
[15,     1] loss: 0.431
[16,     1] loss: 0.412
[17,     1] loss: 0.390
[18,     1] loss: 0.372
[19,     1] loss: 0.349
[20,     1] loss: 0.329
[21,     1] loss: 0.306
[22,     1] loss: 0.288
[23,     1] loss: 0.270
[24,     1] loss: 0.253
[25,     1] loss: 0.235
[26,     1] loss: 0.214
[27,     1] loss: 0.196
[28,     1] loss: 0.183
[29,     1] loss: 0.167
[30,     1] loss: 0.152
[31,     1] loss: 0.140
[32,     1] loss: 0.126
[33,     1] loss: 0.115
[34,     1] loss: 0.103
[35,     1] loss: 0.094
[36,     1] loss: 0.086
[37,     1] loss: 0.079
[38,     1] loss: 0.072
[39,     1] loss: 0.068
[40,     1] loss: 0.060
[41,     1] loss: 0.055
[42,     1] loss: 0.049
[43,     1] loss: 0.047
[44,     1] loss: 0.043
[45,     1] loss: 0.039
[46,     1] loss: 0.037
[47,     1] loss: 0.035
[48,     1] loss: 0.031
[49,     1] loss: 0.028
[50,     1] loss: 0.026
[51,     1] loss: 0.025
[52,     1] loss: 0.024
[53,     1] loss: 0.022
[54,     1] loss: 0.020
[55,     1] loss: 0.020
[56,     1] loss: 0.019
[57,     1] loss: 0.018
[58,     1] loss: 0.016
[59,     1] loss: 0.016
[60,     1] loss: 0.015
[61,     1] loss: 0.014
[62,     1] loss: 0.014
[63,     1] loss: 0.013
[64,     1] loss: 0.013
[65,     1] loss: 0.012
[66,     1] loss: 0.011
[67,     1] loss: 0.012
[68,     1] loss: 0.011
[69,     1] loss: 0.011
[70,     1] loss: 0.010
[71,     1] loss: 0.009
[72,     1] loss: 0.010
Early stopping applied (best metric=0.40633299946784973)
Finished Training
Total time taken: 78.89233660697937
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.673
[4,     1] loss: 0.662
[5,     1] loss: 0.649
[6,     1] loss: 0.637
[7,     1] loss: 0.628
[8,     1] loss: 0.617
[9,     1] loss: 0.606
[10,     1] loss: 0.593
[11,     1] loss: 0.582
[12,     1] loss: 0.571
[13,     1] loss: 0.555
[14,     1] loss: 0.541
[15,     1] loss: 0.531
[16,     1] loss: 0.516
[17,     1] loss: 0.502
[18,     1] loss: 0.487
[19,     1] loss: 0.469
[20,     1] loss: 0.458
[21,     1] loss: 0.442
[22,     1] loss: 0.428
[23,     1] loss: 0.412
[24,     1] loss: 0.397
[25,     1] loss: 0.385
[26,     1] loss: 0.372
[27,     1] loss: 0.360
[28,     1] loss: 0.346
[29,     1] loss: 0.333
[30,     1] loss: 0.316
[31,     1] loss: 0.305
[32,     1] loss: 0.289
[33,     1] loss: 0.277
[34,     1] loss: 0.260
[35,     1] loss: 0.249
[36,     1] loss: 0.236
[37,     1] loss: 0.224
[38,     1] loss: 0.208
[39,     1] loss: 0.197
[40,     1] loss: 0.183
[41,     1] loss: 0.174
[42,     1] loss: 0.159
[43,     1] loss: 0.147
[44,     1] loss: 0.136
[45,     1] loss: 0.126
[46,     1] loss: 0.114
[47,     1] loss: 0.108
[48,     1] loss: 0.100
[49,     1] loss: 0.088
[50,     1] loss: 0.081
[51,     1] loss: 0.073
[52,     1] loss: 0.067
[53,     1] loss: 0.061
[54,     1] loss: 0.056
[55,     1] loss: 0.052
[56,     1] loss: 0.048
[57,     1] loss: 0.044
[58,     1] loss: 0.040
[59,     1] loss: 0.038
[60,     1] loss: 0.035
[61,     1] loss: 0.032
[62,     1] loss: 0.029
[63,     1] loss: 0.027
[64,     1] loss: 0.025
[65,     1] loss: 0.022
[66,     1] loss: 0.022
[67,     1] loss: 0.020
[68,     1] loss: 0.019
[69,     1] loss: 0.018
[70,     1] loss: 0.016
[71,     1] loss: 0.017
[72,     1] loss: 0.015
Early stopping applied (best metric=0.4567895829677582)
Finished Training
Total time taken: 79.37512540817261
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.658
[4,     1] loss: 0.641
[5,     1] loss: 0.628
[6,     1] loss: 0.615
[7,     1] loss: 0.600
[8,     1] loss: 0.587
[9,     1] loss: 0.573
[10,     1] loss: 0.557
[11,     1] loss: 0.540
[12,     1] loss: 0.525
[13,     1] loss: 0.508
[14,     1] loss: 0.490
[15,     1] loss: 0.474
[16,     1] loss: 0.456
[17,     1] loss: 0.438
[18,     1] loss: 0.418
[19,     1] loss: 0.402
[20,     1] loss: 0.384
[21,     1] loss: 0.368
[22,     1] loss: 0.348
[23,     1] loss: 0.327
[24,     1] loss: 0.311
[25,     1] loss: 0.298
[26,     1] loss: 0.280
[27,     1] loss: 0.265
[28,     1] loss: 0.251
[29,     1] loss: 0.237
[30,     1] loss: 0.223
[31,     1] loss: 0.209
[32,     1] loss: 0.197
[33,     1] loss: 0.186
[34,     1] loss: 0.178
[35,     1] loss: 0.164
[36,     1] loss: 0.158
[37,     1] loss: 0.148
[38,     1] loss: 0.136
[39,     1] loss: 0.126
[40,     1] loss: 0.120
[41,     1] loss: 0.112
[42,     1] loss: 0.104
[43,     1] loss: 0.099
[44,     1] loss: 0.093
[45,     1] loss: 0.086
[46,     1] loss: 0.081
[47,     1] loss: 0.073
[48,     1] loss: 0.069
[49,     1] loss: 0.061
[50,     1] loss: 0.058
[51,     1] loss: 0.056
[52,     1] loss: 0.051
[53,     1] loss: 0.047
[54,     1] loss: 0.043
[55,     1] loss: 0.040
[56,     1] loss: 0.038
[57,     1] loss: 0.035
[58,     1] loss: 0.032
[59,     1] loss: 0.030
[60,     1] loss: 0.027
[61,     1] loss: 0.026
[62,     1] loss: 0.024
[63,     1] loss: 0.022
[64,     1] loss: 0.021
[65,     1] loss: 0.021
[66,     1] loss: 0.019
[67,     1] loss: 0.018
[68,     1] loss: 0.017
[69,     1] loss: 0.016
[70,     1] loss: 0.015
[71,     1] loss: 0.015
[72,     1] loss: 0.014
[73,     1] loss: 0.013
[74,     1] loss: 0.012
[75,     1] loss: 0.012
[76,     1] loss: 0.011
[77,     1] loss: 0.010
[78,     1] loss: 0.010
[79,     1] loss: 0.010
[80,     1] loss: 0.009
Early stopping applied (best metric=0.38124290108680725)
Finished Training
Total time taken: 88.47554111480713
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.677
[3,     1] loss: 0.664
[4,     1] loss: 0.655
[5,     1] loss: 0.645
[6,     1] loss: 0.635
[7,     1] loss: 0.625
[8,     1] loss: 0.616
[9,     1] loss: 0.607
[10,     1] loss: 0.597
[11,     1] loss: 0.586
[12,     1] loss: 0.577
[13,     1] loss: 0.566
[14,     1] loss: 0.555
[15,     1] loss: 0.545
[16,     1] loss: 0.533
[17,     1] loss: 0.525
[18,     1] loss: 0.509
[19,     1] loss: 0.501
[20,     1] loss: 0.486
[21,     1] loss: 0.478
[22,     1] loss: 0.463
[23,     1] loss: 0.450
[24,     1] loss: 0.439
[25,     1] loss: 0.425
[26,     1] loss: 0.414
[27,     1] loss: 0.402
[28,     1] loss: 0.391
[29,     1] loss: 0.377
[30,     1] loss: 0.369
[31,     1] loss: 0.356
[32,     1] loss: 0.345
[33,     1] loss: 0.336
[34,     1] loss: 0.325
[35,     1] loss: 0.312
[36,     1] loss: 0.301
[37,     1] loss: 0.293
[38,     1] loss: 0.282
[39,     1] loss: 0.273
[40,     1] loss: 0.260
[41,     1] loss: 0.252
[42,     1] loss: 0.243
[43,     1] loss: 0.232
[44,     1] loss: 0.222
[45,     1] loss: 0.211
[46,     1] loss: 0.201
[47,     1] loss: 0.191
[48,     1] loss: 0.180
[49,     1] loss: 0.169
[50,     1] loss: 0.160
[51,     1] loss: 0.150
[52,     1] loss: 0.141
[53,     1] loss: 0.131
[54,     1] loss: 0.119
[55,     1] loss: 0.108
[56,     1] loss: 0.098
[57,     1] loss: 0.090
[58,     1] loss: 0.083
[59,     1] loss: 0.075
[60,     1] loss: 0.069
[61,     1] loss: 0.064
[62,     1] loss: 0.059
[63,     1] loss: 0.052
[64,     1] loss: 0.047
[65,     1] loss: 0.041
[66,     1] loss: 0.037
[67,     1] loss: 0.034
[68,     1] loss: 0.032
[69,     1] loss: 0.029
[70,     1] loss: 0.026
[71,     1] loss: 0.023
[72,     1] loss: 0.022
[73,     1] loss: 0.020
[74,     1] loss: 0.020
[75,     1] loss: 0.016
[76,     1] loss: 0.015
[77,     1] loss: 0.014
[78,     1] loss: 0.013
[79,     1] loss: 0.012
[80,     1] loss: 0.011
[81,     1] loss: 0.011
[82,     1] loss: 0.010
[83,     1] loss: 0.009
[84,     1] loss: 0.009
[85,     1] loss: 0.009
[86,     1] loss: 0.007
[87,     1] loss: 0.007
[88,     1] loss: 0.007
[89,     1] loss: 0.007
[90,     1] loss: 0.007
[91,     1] loss: 0.006
[92,     1] loss: 0.006
[93,     1] loss: 0.006
[94,     1] loss: 0.006
[95,     1] loss: 0.005
[96,     1] loss: 0.005
[97,     1] loss: 0.005
[98,     1] loss: 0.005
[99,     1] loss: 0.004
Early stopping applied (best metric=0.38933688402175903)
Finished Training
Total time taken: 109.70966863632202
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.659
[4,     1] loss: 0.647
[5,     1] loss: 0.634
[6,     1] loss: 0.624
[7,     1] loss: 0.612
[8,     1] loss: 0.602
[9,     1] loss: 0.591
[10,     1] loss: 0.579
[11,     1] loss: 0.567
[12,     1] loss: 0.553
[13,     1] loss: 0.539
[14,     1] loss: 0.527
[15,     1] loss: 0.511
[16,     1] loss: 0.496
[17,     1] loss: 0.482
[18,     1] loss: 0.468
[19,     1] loss: 0.453
[20,     1] loss: 0.436
[21,     1] loss: 0.420
[22,     1] loss: 0.407
[23,     1] loss: 0.390
[24,     1] loss: 0.375
[25,     1] loss: 0.359
[26,     1] loss: 0.343
[27,     1] loss: 0.328
[28,     1] loss: 0.313
[29,     1] loss: 0.299
[30,     1] loss: 0.283
[31,     1] loss: 0.268
[32,     1] loss: 0.255
[33,     1] loss: 0.241
[34,     1] loss: 0.226
[35,     1] loss: 0.212
[36,     1] loss: 0.201
[37,     1] loss: 0.188
[38,     1] loss: 0.178
[39,     1] loss: 0.164
[40,     1] loss: 0.154
[41,     1] loss: 0.143
[42,     1] loss: 0.132
[43,     1] loss: 0.124
[44,     1] loss: 0.112
[45,     1] loss: 0.104
[46,     1] loss: 0.097
[47,     1] loss: 0.091
[48,     1] loss: 0.083
[49,     1] loss: 0.078
[50,     1] loss: 0.071
[51,     1] loss: 0.067
[52,     1] loss: 0.060
[53,     1] loss: 0.056
[54,     1] loss: 0.053
[55,     1] loss: 0.050
[56,     1] loss: 0.044
[57,     1] loss: 0.041
[58,     1] loss: 0.039
[59,     1] loss: 0.036
[60,     1] loss: 0.033
[61,     1] loss: 0.030
[62,     1] loss: 0.030
[63,     1] loss: 0.027
[64,     1] loss: 0.026
[65,     1] loss: 0.024
[66,     1] loss: 0.023
[67,     1] loss: 0.021
[68,     1] loss: 0.021
[69,     1] loss: 0.019
[70,     1] loss: 0.017
[71,     1] loss: 0.017
[72,     1] loss: 0.017
[73,     1] loss: 0.015
Early stopping applied (best metric=0.4541611075401306)
Finished Training
Total time taken: 81.53451085090637
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.676
[3,     1] loss: 0.657
[4,     1] loss: 0.645
[5,     1] loss: 0.631
[6,     1] loss: 0.619
[7,     1] loss: 0.607
[8,     1] loss: 0.594
[9,     1] loss: 0.583
[10,     1] loss: 0.572
[11,     1] loss: 0.561
[12,     1] loss: 0.548
[13,     1] loss: 0.534
[14,     1] loss: 0.518
[15,     1] loss: 0.506
[16,     1] loss: 0.492
[17,     1] loss: 0.476
[18,     1] loss: 0.465
[19,     1] loss: 0.451
[20,     1] loss: 0.438
[21,     1] loss: 0.424
[22,     1] loss: 0.408
[23,     1] loss: 0.395
[24,     1] loss: 0.383
[25,     1] loss: 0.368
[26,     1] loss: 0.359
[27,     1] loss: 0.348
[28,     1] loss: 0.337
[29,     1] loss: 0.328
[30,     1] loss: 0.319
[31,     1] loss: 0.311
[32,     1] loss: 0.303
[33,     1] loss: 0.294
[34,     1] loss: 0.287
[35,     1] loss: 0.279
[36,     1] loss: 0.272
[37,     1] loss: 0.264
[38,     1] loss: 0.257
[39,     1] loss: 0.250
[40,     1] loss: 0.245
[41,     1] loss: 0.236
[42,     1] loss: 0.231
[43,     1] loss: 0.223
[44,     1] loss: 0.216
[45,     1] loss: 0.209
[46,     1] loss: 0.201
[47,     1] loss: 0.193
[48,     1] loss: 0.188
[49,     1] loss: 0.179
[50,     1] loss: 0.170
[51,     1] loss: 0.163
[52,     1] loss: 0.155
[53,     1] loss: 0.145
[54,     1] loss: 0.138
[55,     1] loss: 0.130
[56,     1] loss: 0.122
[57,     1] loss: 0.117
[58,     1] loss: 0.111
[59,     1] loss: 0.098
[60,     1] loss: 0.093
[61,     1] loss: 0.086
[62,     1] loss: 0.080
[63,     1] loss: 0.075
[64,     1] loss: 0.069
[65,     1] loss: 0.062
[66,     1] loss: 0.057
[67,     1] loss: 0.053
[68,     1] loss: 0.050
[69,     1] loss: 0.045
[70,     1] loss: 0.041
[71,     1] loss: 0.037
[72,     1] loss: 0.036
Early stopping applied (best metric=0.38983848690986633)
Finished Training
Total time taken: 80.59150671958923
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.678
[3,     1] loss: 0.659
[4,     1] loss: 0.645
[5,     1] loss: 0.630
[6,     1] loss: 0.617
[7,     1] loss: 0.606
[8,     1] loss: 0.593
[9,     1] loss: 0.581
[10,     1] loss: 0.566
[11,     1] loss: 0.551
[12,     1] loss: 0.537
[13,     1] loss: 0.523
[14,     1] loss: 0.507
[15,     1] loss: 0.488
[16,     1] loss: 0.474
[17,     1] loss: 0.457
[18,     1] loss: 0.438
[19,     1] loss: 0.421
[20,     1] loss: 0.401
[21,     1] loss: 0.383
[22,     1] loss: 0.366
[23,     1] loss: 0.342
[24,     1] loss: 0.324
[25,     1] loss: 0.300
[26,     1] loss: 0.287
[27,     1] loss: 0.263
[28,     1] loss: 0.245
[29,     1] loss: 0.228
[30,     1] loss: 0.213
[31,     1] loss: 0.194
[32,     1] loss: 0.179
[33,     1] loss: 0.165
[34,     1] loss: 0.150
[35,     1] loss: 0.139
[36,     1] loss: 0.128
[37,     1] loss: 0.117
[38,     1] loss: 0.108
[39,     1] loss: 0.097
[40,     1] loss: 0.087
[41,     1] loss: 0.081
[42,     1] loss: 0.076
[43,     1] loss: 0.067
[44,     1] loss: 0.062
[45,     1] loss: 0.057
[46,     1] loss: 0.053
[47,     1] loss: 0.049
[48,     1] loss: 0.044
[49,     1] loss: 0.040
[50,     1] loss: 0.037
[51,     1] loss: 0.035
[52,     1] loss: 0.032
[53,     1] loss: 0.029
[54,     1] loss: 0.028
[55,     1] loss: 0.026
[56,     1] loss: 0.024
[57,     1] loss: 0.022
[58,     1] loss: 0.022
[59,     1] loss: 0.020
[60,     1] loss: 0.019
[61,     1] loss: 0.017
[62,     1] loss: 0.016
[63,     1] loss: 0.015
[64,     1] loss: 0.015
[65,     1] loss: 0.014
[66,     1] loss: 0.013
[67,     1] loss: 0.012
[68,     1] loss: 0.013
[69,     1] loss: 0.012
[70,     1] loss: 0.011
[71,     1] loss: 0.010
Early stopping applied (best metric=0.47202613949775696)
Finished Training
Total time taken: 80.15048050880432
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.678
[3,     1] loss: 0.666
[4,     1] loss: 0.654
[5,     1] loss: 0.642
[6,     1] loss: 0.632
[7,     1] loss: 0.622
[8,     1] loss: 0.610
[9,     1] loss: 0.598
[10,     1] loss: 0.586
[11,     1] loss: 0.575
[12,     1] loss: 0.562
[13,     1] loss: 0.546
[14,     1] loss: 0.532
[15,     1] loss: 0.516
[16,     1] loss: 0.501
[17,     1] loss: 0.485
[18,     1] loss: 0.467
[19,     1] loss: 0.452
[20,     1] loss: 0.435
[21,     1] loss: 0.413
[22,     1] loss: 0.398
[23,     1] loss: 0.378
[24,     1] loss: 0.361
[25,     1] loss: 0.345
[26,     1] loss: 0.327
[27,     1] loss: 0.312
[28,     1] loss: 0.288
[29,     1] loss: 0.275
[30,     1] loss: 0.260
[31,     1] loss: 0.243
[32,     1] loss: 0.227
[33,     1] loss: 0.211
[34,     1] loss: 0.198
[35,     1] loss: 0.186
[36,     1] loss: 0.172
[37,     1] loss: 0.158
[38,     1] loss: 0.151
[39,     1] loss: 0.137
[40,     1] loss: 0.124
[41,     1] loss: 0.116
[42,     1] loss: 0.109
[43,     1] loss: 0.099
[44,     1] loss: 0.092
[45,     1] loss: 0.085
[46,     1] loss: 0.082
[47,     1] loss: 0.071
[48,     1] loss: 0.067
[49,     1] loss: 0.062
[50,     1] loss: 0.056
[51,     1] loss: 0.055
[52,     1] loss: 0.048
[53,     1] loss: 0.046
[54,     1] loss: 0.041
[55,     1] loss: 0.038
[56,     1] loss: 0.037
[57,     1] loss: 0.035
[58,     1] loss: 0.030
[59,     1] loss: 0.029
[60,     1] loss: 0.026
[61,     1] loss: 0.026
[62,     1] loss: 0.025
[63,     1] loss: 0.022
[64,     1] loss: 0.021
[65,     1] loss: 0.021
[66,     1] loss: 0.018
[67,     1] loss: 0.018
[68,     1] loss: 0.018
[69,     1] loss: 0.016
[70,     1] loss: 0.016
[71,     1] loss: 0.014
[72,     1] loss: 0.014
[73,     1] loss: 0.013
[74,     1] loss: 0.012
[75,     1] loss: 0.012
[76,     1] loss: 0.011
[77,     1] loss: 0.011
Early stopping applied (best metric=0.3778057098388672)
Finished Training
Total time taken: 87.24852561950684
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.678
[3,     1] loss: 0.664
[4,     1] loss: 0.650
[5,     1] loss: 0.638
[6,     1] loss: 0.626
[7,     1] loss: 0.614
[8,     1] loss: 0.599
[9,     1] loss: 0.586
[10,     1] loss: 0.572
[11,     1] loss: 0.555
[12,     1] loss: 0.541
[13,     1] loss: 0.525
[14,     1] loss: 0.507
[15,     1] loss: 0.487
[16,     1] loss: 0.470
[17,     1] loss: 0.450
[18,     1] loss: 0.434
[19,     1] loss: 0.413
[20,     1] loss: 0.394
[21,     1] loss: 0.376
[22,     1] loss: 0.360
[23,     1] loss: 0.338
[24,     1] loss: 0.319
[25,     1] loss: 0.298
[26,     1] loss: 0.275
[27,     1] loss: 0.260
[28,     1] loss: 0.242
[29,     1] loss: 0.227
[30,     1] loss: 0.208
[31,     1] loss: 0.192
[32,     1] loss: 0.174
[33,     1] loss: 0.163
[34,     1] loss: 0.147
[35,     1] loss: 0.138
[36,     1] loss: 0.126
[37,     1] loss: 0.117
[38,     1] loss: 0.107
[39,     1] loss: 0.099
[40,     1] loss: 0.090
[41,     1] loss: 0.082
[42,     1] loss: 0.075
[43,     1] loss: 0.071
[44,     1] loss: 0.064
[45,     1] loss: 0.059
[46,     1] loss: 0.053
[47,     1] loss: 0.050
[48,     1] loss: 0.047
[49,     1] loss: 0.043
[50,     1] loss: 0.039
[51,     1] loss: 0.036
[52,     1] loss: 0.034
[53,     1] loss: 0.031
[54,     1] loss: 0.028
[55,     1] loss: 0.027
[56,     1] loss: 0.024
[57,     1] loss: 0.023
[58,     1] loss: 0.022
[59,     1] loss: 0.020
[60,     1] loss: 0.020
[61,     1] loss: 0.018
[62,     1] loss: 0.018
[63,     1] loss: 0.016
[64,     1] loss: 0.015
[65,     1] loss: 0.015
[66,     1] loss: 0.014
[67,     1] loss: 0.014
[68,     1] loss: 0.013
[69,     1] loss: 0.012
[70,     1] loss: 0.012
[71,     1] loss: 0.012
[72,     1] loss: 0.011
[73,     1] loss: 0.010
[74,     1] loss: 0.010
[75,     1] loss: 0.010
Early stopping applied (best metric=0.39970532059669495)
Finished Training
Total time taken: 85.17251062393188
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.676
[4,     1] loss: 0.666
[5,     1] loss: 0.655
[6,     1] loss: 0.642
[7,     1] loss: 0.632
[8,     1] loss: 0.619
[9,     1] loss: 0.606
[10,     1] loss: 0.595
[11,     1] loss: 0.578
[12,     1] loss: 0.566
[13,     1] loss: 0.544
[14,     1] loss: 0.534
[15,     1] loss: 0.513
[16,     1] loss: 0.497
[17,     1] loss: 0.481
[18,     1] loss: 0.464
[19,     1] loss: 0.443
[20,     1] loss: 0.422
[21,     1] loss: 0.407
[22,     1] loss: 0.387
[23,     1] loss: 0.367
[24,     1] loss: 0.347
[25,     1] loss: 0.325
[26,     1] loss: 0.306
[27,     1] loss: 0.289
[28,     1] loss: 0.272
[29,     1] loss: 0.252
[30,     1] loss: 0.238
[31,     1] loss: 0.217
[32,     1] loss: 0.199
[33,     1] loss: 0.187
[34,     1] loss: 0.171
[35,     1] loss: 0.157
[36,     1] loss: 0.144
[37,     1] loss: 0.130
[38,     1] loss: 0.120
[39,     1] loss: 0.109
[40,     1] loss: 0.100
[41,     1] loss: 0.093
[42,     1] loss: 0.083
[43,     1] loss: 0.076
[44,     1] loss: 0.070
[45,     1] loss: 0.063
[46,     1] loss: 0.056
[47,     1] loss: 0.054
[48,     1] loss: 0.048
[49,     1] loss: 0.044
[50,     1] loss: 0.041
[51,     1] loss: 0.037
[52,     1] loss: 0.035
[53,     1] loss: 0.032
[54,     1] loss: 0.030
[55,     1] loss: 0.028
[56,     1] loss: 0.025
[57,     1] loss: 0.023
[58,     1] loss: 0.023
[59,     1] loss: 0.022
[60,     1] loss: 0.019
[61,     1] loss: 0.019
[62,     1] loss: 0.018
[63,     1] loss: 0.016
[64,     1] loss: 0.015
[65,     1] loss: 0.015
[66,     1] loss: 0.014
[67,     1] loss: 0.013
[68,     1] loss: 0.013
[69,     1] loss: 0.012
[70,     1] loss: 0.012
[71,     1] loss: 0.012
[72,     1] loss: 0.011
[73,     1] loss: 0.011
[74,     1] loss: 0.010
[75,     1] loss: 0.010
[76,     1] loss: 0.009
[77,     1] loss: 0.009
[78,     1] loss: 0.009
[79,     1] loss: 0.009
[80,     1] loss: 0.008
[81,     1] loss: 0.008
[82,     1] loss: 0.008
[83,     1] loss: 0.007
Early stopping applied (best metric=0.2989877164363861)
Finished Training
Total time taken: 94.49958848953247
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.659
[4,     1] loss: 0.645
[5,     1] loss: 0.631
[6,     1] loss: 0.618
[7,     1] loss: 0.602
[8,     1] loss: 0.589
[9,     1] loss: 0.574
[10,     1] loss: 0.559
[11,     1] loss: 0.543
[12,     1] loss: 0.527
[13,     1] loss: 0.511
[14,     1] loss: 0.497
[15,     1] loss: 0.480
[16,     1] loss: 0.468
[17,     1] loss: 0.453
[18,     1] loss: 0.435
[19,     1] loss: 0.422
[20,     1] loss: 0.408
[21,     1] loss: 0.395
[22,     1] loss: 0.380
[23,     1] loss: 0.367
[24,     1] loss: 0.356
[25,     1] loss: 0.345
[26,     1] loss: 0.333
[27,     1] loss: 0.324
[28,     1] loss: 0.315
[29,     1] loss: 0.308
[30,     1] loss: 0.296
[31,     1] loss: 0.287
[32,     1] loss: 0.278
[33,     1] loss: 0.272
[34,     1] loss: 0.264
[35,     1] loss: 0.255
[36,     1] loss: 0.246
[37,     1] loss: 0.240
[38,     1] loss: 0.229
[39,     1] loss: 0.222
[40,     1] loss: 0.215
[41,     1] loss: 0.206
[42,     1] loss: 0.196
[43,     1] loss: 0.187
[44,     1] loss: 0.179
[45,     1] loss: 0.171
[46,     1] loss: 0.163
[47,     1] loss: 0.156
[48,     1] loss: 0.149
[49,     1] loss: 0.140
[50,     1] loss: 0.134
[51,     1] loss: 0.127
[52,     1] loss: 0.121
[53,     1] loss: 0.113
[54,     1] loss: 0.110
[55,     1] loss: 0.104
[56,     1] loss: 0.099
[57,     1] loss: 0.093
[58,     1] loss: 0.086
[59,     1] loss: 0.084
[60,     1] loss: 0.079
[61,     1] loss: 0.075
[62,     1] loss: 0.072
[63,     1] loss: 0.067
[64,     1] loss: 0.065
[65,     1] loss: 0.061
[66,     1] loss: 0.058
[67,     1] loss: 0.054
[68,     1] loss: 0.052
[69,     1] loss: 0.050
Early stopping applied (best metric=0.4173957407474518)
Finished Training
Total time taken: 79.10407447814941
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.689
[3,     1] loss: 0.679
[4,     1] loss: 0.669
[5,     1] loss: 0.661
[6,     1] loss: 0.651
[7,     1] loss: 0.641
[8,     1] loss: 0.633
[9,     1] loss: 0.623
[10,     1] loss: 0.613
[11,     1] loss: 0.600
[12,     1] loss: 0.589
[13,     1] loss: 0.578
[14,     1] loss: 0.562
[15,     1] loss: 0.549
[16,     1] loss: 0.535
[17,     1] loss: 0.522
[18,     1] loss: 0.504
[19,     1] loss: 0.488
[20,     1] loss: 0.474
[21,     1] loss: 0.455
[22,     1] loss: 0.436
[23,     1] loss: 0.421
[24,     1] loss: 0.402
[25,     1] loss: 0.385
[26,     1] loss: 0.367
[27,     1] loss: 0.348
[28,     1] loss: 0.330
[29,     1] loss: 0.309
[30,     1] loss: 0.291
[31,     1] loss: 0.277
[32,     1] loss: 0.256
[33,     1] loss: 0.241
[34,     1] loss: 0.223
[35,     1] loss: 0.207
[36,     1] loss: 0.194
[37,     1] loss: 0.174
[38,     1] loss: 0.163
[39,     1] loss: 0.147
[40,     1] loss: 0.138
[41,     1] loss: 0.126
[42,     1] loss: 0.114
[43,     1] loss: 0.104
[44,     1] loss: 0.096
[45,     1] loss: 0.087
[46,     1] loss: 0.078
[47,     1] loss: 0.073
[48,     1] loss: 0.066
[49,     1] loss: 0.061
[50,     1] loss: 0.054
[51,     1] loss: 0.052
[52,     1] loss: 0.047
[53,     1] loss: 0.043
[54,     1] loss: 0.041
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.033
[58,     1] loss: 0.028
[59,     1] loss: 0.027
[60,     1] loss: 0.025
[61,     1] loss: 0.023
[62,     1] loss: 0.022
[63,     1] loss: 0.020
[64,     1] loss: 0.020
[65,     1] loss: 0.018
[66,     1] loss: 0.017
[67,     1] loss: 0.016
[68,     1] loss: 0.015
[69,     1] loss: 0.015
[70,     1] loss: 0.014
[71,     1] loss: 0.013
[72,     1] loss: 0.012
[73,     1] loss: 0.012
[74,     1] loss: 0.012
[75,     1] loss: 0.012
[76,     1] loss: 0.011
[77,     1] loss: 0.011
[78,     1] loss: 0.010
[79,     1] loss: 0.010
Early stopping applied (best metric=0.42329999804496765)
Finished Training
Total time taken: 90.6755440235138
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.670
[4,     1] loss: 0.659
[5,     1] loss: 0.648
[6,     1] loss: 0.637
[7,     1] loss: 0.629
[8,     1] loss: 0.615
[9,     1] loss: 0.608
[10,     1] loss: 0.596
[11,     1] loss: 0.581
[12,     1] loss: 0.571
[13,     1] loss: 0.559
[14,     1] loss: 0.543
[15,     1] loss: 0.527
[16,     1] loss: 0.515
[17,     1] loss: 0.502
[18,     1] loss: 0.483
[19,     1] loss: 0.473
[20,     1] loss: 0.456
[21,     1] loss: 0.435
[22,     1] loss: 0.420
[23,     1] loss: 0.403
[24,     1] loss: 0.383
[25,     1] loss: 0.369
[26,     1] loss: 0.349
[27,     1] loss: 0.331
[28,     1] loss: 0.309
[29,     1] loss: 0.290
[30,     1] loss: 0.273
[31,     1] loss: 0.254
[32,     1] loss: 0.243
[33,     1] loss: 0.221
[34,     1] loss: 0.207
[35,     1] loss: 0.192
[36,     1] loss: 0.178
[37,     1] loss: 0.161
[38,     1] loss: 0.149
[39,     1] loss: 0.136
[40,     1] loss: 0.127
[41,     1] loss: 0.112
[42,     1] loss: 0.103
[43,     1] loss: 0.095
[44,     1] loss: 0.086
[45,     1] loss: 0.079
[46,     1] loss: 0.072
[47,     1] loss: 0.065
[48,     1] loss: 0.059
[49,     1] loss: 0.055
[50,     1] loss: 0.049
[51,     1] loss: 0.045
[52,     1] loss: 0.043
[53,     1] loss: 0.038
[54,     1] loss: 0.035
[55,     1] loss: 0.034
[56,     1] loss: 0.031
[57,     1] loss: 0.028
[58,     1] loss: 0.026
[59,     1] loss: 0.023
[60,     1] loss: 0.022
[61,     1] loss: 0.021
[62,     1] loss: 0.020
[63,     1] loss: 0.018
[64,     1] loss: 0.018
[65,     1] loss: 0.017
[66,     1] loss: 0.014
[67,     1] loss: 0.014
[68,     1] loss: 0.013
[69,     1] loss: 0.012
[70,     1] loss: 0.012
[71,     1] loss: 0.012
[72,     1] loss: 0.011
[73,     1] loss: 0.011
[74,     1] loss: 0.010
[75,     1] loss: 0.009
[76,     1] loss: 0.010
[77,     1] loss: 0.009
[78,     1] loss: 0.008
[79,     1] loss: 0.008
[80,     1] loss: 0.008
[81,     1] loss: 0.008
[82,     1] loss: 0.007
Early stopping applied (best metric=0.3576880991458893)
Finished Training
Total time taken: 94.58056783676147
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.679
[3,     1] loss: 0.665
[4,     1] loss: 0.654
[5,     1] loss: 0.642
[6,     1] loss: 0.632
[7,     1] loss: 0.620
[8,     1] loss: 0.610
[9,     1] loss: 0.596
[10,     1] loss: 0.586
[11,     1] loss: 0.574
[12,     1] loss: 0.564
[13,     1] loss: 0.551
[14,     1] loss: 0.538
[15,     1] loss: 0.523
[16,     1] loss: 0.509
[17,     1] loss: 0.493
[18,     1] loss: 0.479
[19,     1] loss: 0.463
[20,     1] loss: 0.447
[21,     1] loss: 0.428
[22,     1] loss: 0.415
[23,     1] loss: 0.399
[24,     1] loss: 0.378
[25,     1] loss: 0.365
[26,     1] loss: 0.343
[27,     1] loss: 0.326
[28,     1] loss: 0.307
[29,     1] loss: 0.289
[30,     1] loss: 0.269
[31,     1] loss: 0.255
[32,     1] loss: 0.237
[33,     1] loss: 0.217
[34,     1] loss: 0.197
[35,     1] loss: 0.187
[36,     1] loss: 0.168
[37,     1] loss: 0.153
[38,     1] loss: 0.139
[39,     1] loss: 0.128
[40,     1] loss: 0.115
[41,     1] loss: 0.102
[42,     1] loss: 0.096
[43,     1] loss: 0.084
[44,     1] loss: 0.075
[45,     1] loss: 0.069
[46,     1] loss: 0.061
[47,     1] loss: 0.059
[48,     1] loss: 0.050
[49,     1] loss: 0.047
[50,     1] loss: 0.041
[51,     1] loss: 0.038
[52,     1] loss: 0.035
[53,     1] loss: 0.031
[54,     1] loss: 0.029
[55,     1] loss: 0.026
[56,     1] loss: 0.025
[57,     1] loss: 0.022
[58,     1] loss: 0.021
[59,     1] loss: 0.019
[60,     1] loss: 0.017
[61,     1] loss: 0.016
[62,     1] loss: 0.015
[63,     1] loss: 0.014
[64,     1] loss: 0.013
[65,     1] loss: 0.013
[66,     1] loss: 0.012
[67,     1] loss: 0.011
[68,     1] loss: 0.010
[69,     1] loss: 0.010
[70,     1] loss: 0.009
[71,     1] loss: 0.009
[72,     1] loss: 0.008
[73,     1] loss: 0.008
[74,     1] loss: 0.008
[75,     1] loss: 0.007
[76,     1] loss: 0.007
[77,     1] loss: 0.007
[78,     1] loss: 0.006
[79,     1] loss: 0.006
Early stopping applied (best metric=0.43306049704551697)
Finished Training
Total time taken: 91.49854898452759
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.666
[4,     1] loss: 0.657
[5,     1] loss: 0.647
[6,     1] loss: 0.635
[7,     1] loss: 0.624
[8,     1] loss: 0.615
[9,     1] loss: 0.602
[10,     1] loss: 0.590
[11,     1] loss: 0.574
[12,     1] loss: 0.565
[13,     1] loss: 0.548
[14,     1] loss: 0.535
[15,     1] loss: 0.515
[16,     1] loss: 0.497
[17,     1] loss: 0.483
[18,     1] loss: 0.466
[19,     1] loss: 0.447
[20,     1] loss: 0.428
[21,     1] loss: 0.408
[22,     1] loss: 0.390
[23,     1] loss: 0.372
[24,     1] loss: 0.347
[25,     1] loss: 0.325
[26,     1] loss: 0.305
[27,     1] loss: 0.286
[28,     1] loss: 0.271
[29,     1] loss: 0.250
[30,     1] loss: 0.228
[31,     1] loss: 0.211
[32,     1] loss: 0.198
[33,     1] loss: 0.180
[34,     1] loss: 0.164
[35,     1] loss: 0.150
[36,     1] loss: 0.137
[37,     1] loss: 0.125
[38,     1] loss: 0.115
[39,     1] loss: 0.105
[40,     1] loss: 0.096
[41,     1] loss: 0.087
[42,     1] loss: 0.079
[43,     1] loss: 0.072
[44,     1] loss: 0.066
[45,     1] loss: 0.059
[46,     1] loss: 0.055
[47,     1] loss: 0.049
[48,     1] loss: 0.044
[49,     1] loss: 0.042
[50,     1] loss: 0.038
[51,     1] loss: 0.035
[52,     1] loss: 0.033
[53,     1] loss: 0.030
[54,     1] loss: 0.027
[55,     1] loss: 0.026
[56,     1] loss: 0.024
[57,     1] loss: 0.022
[58,     1] loss: 0.022
[59,     1] loss: 0.021
[60,     1] loss: 0.019
[61,     1] loss: 0.018
[62,     1] loss: 0.017
[63,     1] loss: 0.016
[64,     1] loss: 0.015
[65,     1] loss: 0.014
[66,     1] loss: 0.013
[67,     1] loss: 0.013
[68,     1] loss: 0.012
[69,     1] loss: 0.011
[70,     1] loss: 0.012
[71,     1] loss: 0.010
[72,     1] loss: 0.010
[73,     1] loss: 0.010
[74,     1] loss: 0.009
[75,     1] loss: 0.008
[76,     1] loss: 0.009
[77,     1] loss: 0.008
Early stopping applied (best metric=0.40624481439590454)
Finished Training
Total time taken: 89.62716507911682
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.662
[4,     1] loss: 0.649
[5,     1] loss: 0.634
[6,     1] loss: 0.622
[7,     1] loss: 0.609
[8,     1] loss: 0.592
[9,     1] loss: 0.579
[10,     1] loss: 0.561
[11,     1] loss: 0.549
[12,     1] loss: 0.533
[13,     1] loss: 0.515
[14,     1] loss: 0.501
[15,     1] loss: 0.482
[16,     1] loss: 0.463
[17,     1] loss: 0.446
[18,     1] loss: 0.425
[19,     1] loss: 0.405
[20,     1] loss: 0.388
[21,     1] loss: 0.368
[22,     1] loss: 0.351
[23,     1] loss: 0.327
[24,     1] loss: 0.310
[25,     1] loss: 0.288
[26,     1] loss: 0.276
[27,     1] loss: 0.253
[28,     1] loss: 0.234
[29,     1] loss: 0.219
[30,     1] loss: 0.199
[31,     1] loss: 0.184
[32,     1] loss: 0.167
[33,     1] loss: 0.156
[34,     1] loss: 0.139
[35,     1] loss: 0.129
[36,     1] loss: 0.120
[37,     1] loss: 0.106
[38,     1] loss: 0.096
[39,     1] loss: 0.087
[40,     1] loss: 0.081
[41,     1] loss: 0.074
[42,     1] loss: 0.066
[43,     1] loss: 0.061
[44,     1] loss: 0.055
[45,     1] loss: 0.051
[46,     1] loss: 0.047
[47,     1] loss: 0.045
[48,     1] loss: 0.039
[49,     1] loss: 0.038
[50,     1] loss: 0.034
[51,     1] loss: 0.032
[52,     1] loss: 0.032
[53,     1] loss: 0.028
[54,     1] loss: 0.027
[55,     1] loss: 0.026
[56,     1] loss: 0.024
[57,     1] loss: 0.024
[58,     1] loss: 0.021
[59,     1] loss: 0.022
[60,     1] loss: 0.020
[61,     1] loss: 0.018
[62,     1] loss: 0.018
[63,     1] loss: 0.018
[64,     1] loss: 0.017
[65,     1] loss: 0.016
[66,     1] loss: 0.015
[67,     1] loss: 0.016
[68,     1] loss: 0.015
[69,     1] loss: 0.015
[70,     1] loss: 0.014
[71,     1] loss: 0.014
[72,     1] loss: 0.013
[73,     1] loss: 0.012
[74,     1] loss: 0.012
[75,     1] loss: 0.012
[76,     1] loss: 0.011
[77,     1] loss: 0.011
[78,     1] loss: 0.010
[79,     1] loss: 0.011
[80,     1] loss: 0.010
[81,     1] loss: 0.010
Early stopping applied (best metric=0.35375723242759705)
Finished Training
Total time taken: 94.88757991790771
{'Hydroxylation-P Validation Accuracy': 0.7889476473275469, 'Hydroxylation-P Validation Sensitivity': 0.7357142857142858, 'Hydroxylation-P Validation Specificity': 0.8004952865479575, 'Hydroxylation-P Validation Precision': 0.45492413165189133, 'Hydroxylation-P AUC ROC': 0.8449351909250906, 'Hydroxylation-P AUC PR': 0.5780636306167931, 'Hydroxylation-P MCC': 0.4558555132798077, 'Hydroxylation-P F1': 0.5554420625852128, 'Validation Loss (Hydroxylation-P)': 0.40313012480735777, 'Validation Loss (total)': 0.40313012480735777}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005408828860604792,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 874638569,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.852554864984231}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003499472216505446,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 167283697,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.256238417485797}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.654
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005640159590236553,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1278473178,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.978795343414525}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.687
[3,     1] loss: 0.668
[4,     1] loss: 0.654
[5,     1] loss: 0.640
[6,     1] loss: 0.625
[7,     1] loss: 0.611
[8,     1] loss: 0.596
[9,     1] loss: 0.582
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009609598450545697,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3821810046,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.929611400180745}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.655
[4,     1] loss: 0.609
[5,     1] loss: 0.551
[6,     1] loss: 0.484
[7,     1] loss: 0.417
[8,     1] loss: 0.364
[9,     1] loss: 0.267
[10,     1] loss: 0.198
[11,     1] loss: 0.148
[12,     1] loss: 0.123
[13,     1] loss: 0.101
[14,     1] loss: 0.085
[15,     1] loss: 0.080
[16,     1] loss: 0.062
[17,     1] loss: 0.048
[18,     1] loss: 0.040
[19,     1] loss: 0.025
[20,     1] loss: 0.021
[21,     1] loss: 0.049
[22,     1] loss: 0.361
[23,     1] loss: 0.297
[24,     1] loss: 0.100
[25,     1] loss: 0.085
[26,     1] loss: 0.121
[27,     1] loss: 0.154
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008199742047989435,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 856548571,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.4897220790486396}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.674
[3,     1] loss: 0.608
[4,     1] loss: 0.545
[5,     1] loss: 0.517
[6,     1] loss: 0.471
[7,     1] loss: 0.461
[8,     1] loss: 0.432
[9,     1] loss: 0.403
[10,     1] loss: 0.394
[11,     1] loss: 0.390
[12,     1] loss: 0.367
[13,     1] loss: 0.370
[14,     1] loss: 0.340
[15,     1] loss: 0.326
[16,     1] loss: 0.314
[17,     1] loss: 0.305
[18,     1] loss: 0.309
[19,     1] loss: 0.304
[20,     1] loss: 0.288
[21,     1] loss: 0.284
[22,     1] loss: 0.263
[23,     1] loss: 0.254
[24,     1] loss: 0.231
[25,     1] loss: 0.210
[26,     1] loss: 0.184
[27,     1] loss: 0.163
[28,     1] loss: 0.141
[29,     1] loss: 0.127
[30,     1] loss: 0.121
[31,     1] loss: 0.115
[32,     1] loss: 0.113
[33,     1] loss: 0.120
[34,     1] loss: 0.109
[35,     1] loss: 0.097
[36,     1] loss: 0.085
[37,     1] loss: 0.073
[38,     1] loss: 0.066
[39,     1] loss: 0.057
[40,     1] loss: 0.054
[41,     1] loss: 0.045
[42,     1] loss: 0.035
[43,     1] loss: 0.032
[44,     1] loss: 0.018
[45,     1] loss: 0.012
[46,     1] loss: 0.009
[47,     1] loss: 0.006
[48,     1] loss: 0.005
[49,     1] loss: 0.004
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.002
Early stopping applied (best metric=0.47616809606552124)
Finished Training
Total time taken: 62.21639561653137
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.650
[4,     1] loss: 0.606
[5,     1] loss: 0.552
[6,     1] loss: 0.508
[7,     1] loss: 0.451
[8,     1] loss: 0.403
[9,     1] loss: 0.354
[10,     1] loss: 0.313
[11,     1] loss: 0.260
[12,     1] loss: 0.202
[13,     1] loss: 0.147
[14,     1] loss: 0.132
[15,     1] loss: 0.115
[16,     1] loss: 0.082
[17,     1] loss: 0.052
[18,     1] loss: 0.033
[19,     1] loss: 0.028
[20,     1] loss: 0.022
[21,     1] loss: 0.018
[22,     1] loss: 0.011
[23,     1] loss: 0.009
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.004
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.002
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.002
Early stopping applied (best metric=0.4186486005783081)
Finished Training
Total time taken: 68.45641374588013
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.671
[3,     1] loss: 0.611
[4,     1] loss: 0.540
[5,     1] loss: 0.456
[6,     1] loss: 0.390
[7,     1] loss: 0.331
[8,     1] loss: 0.231
[9,     1] loss: 0.179
[10,     1] loss: 0.129
[11,     1] loss: 0.103
[12,     1] loss: 0.079
[13,     1] loss: 0.051
[14,     1] loss: 0.048
[15,     1] loss: 0.034
[16,     1] loss: 0.030
[17,     1] loss: 0.014
[18,     1] loss: 0.020
[19,     1] loss: 0.005
[20,     1] loss: 0.006
[21,     1] loss: 0.006
[22,     1] loss: 0.002
[23,     1] loss: 0.001
[24,     1] loss: 0.001
[25,     1] loss: 0.002
[26,     1] loss: 0.002
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.003
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
Early stopping applied (best metric=0.39294397830963135)
Finished Training
Total time taken: 66.24839735031128
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.645
[4,     1] loss: 0.579
[5,     1] loss: 0.498
[6,     1] loss: 0.400
[7,     1] loss: 0.322
[8,     1] loss: 0.246
[9,     1] loss: 0.189
[10,     1] loss: 0.144
[11,     1] loss: 0.109
[12,     1] loss: 0.078
[13,     1] loss: 0.073
[14,     1] loss: 0.038
[15,     1] loss: 0.037
[16,     1] loss: 0.024
[17,     1] loss: 0.022
[18,     1] loss: 0.012
[19,     1] loss: 0.014
[20,     1] loss: 0.010
[21,     1] loss: 0.007
[22,     1] loss: 0.003
[23,     1] loss: 0.003
[24,     1] loss: 0.003
[25,     1] loss: 0.002
[26,     1] loss: 0.002
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.001
[55,     1] loss: 0.002
[56,     1] loss: 0.001
[57,     1] loss: 0.001
Early stopping applied (best metric=0.3812677562236786)
Finished Training
Total time taken: 67.54841685295105
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.675
[3,     1] loss: 0.620
[4,     1] loss: 0.561
[5,     1] loss: 0.511
[6,     1] loss: 0.460
[7,     1] loss: 0.418
[8,     1] loss: 0.348
[9,     1] loss: 0.277
[10,     1] loss: 0.198
[11,     1] loss: 0.147
[12,     1] loss: 0.096
[13,     1] loss: 0.054
[14,     1] loss: 0.033
[15,     1] loss: 0.031
[16,     1] loss: 0.021
[17,     1] loss: 0.045
[18,     1] loss: 0.013
[19,     1] loss: 0.143
[20,     1] loss: 0.054
[21,     1] loss: 0.027
[22,     1] loss: 0.042
[23,     1] loss: 0.005
[24,     1] loss: 0.024
[25,     1] loss: 0.013
[26,     1] loss: 0.006
[27,     1] loss: 0.009
[28,     1] loss: 0.010
[29,     1] loss: 0.009
[30,     1] loss: 0.007
[31,     1] loss: 0.006
[32,     1] loss: 0.005
[33,     1] loss: 0.005
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.005
[39,     1] loss: 0.004
[40,     1] loss: 0.004
[41,     1] loss: 0.004
[42,     1] loss: 0.004
[43,     1] loss: 0.003
[44,     1] loss: 0.003
[45,     1] loss: 0.003
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.002
[54,     1] loss: 0.003
Early stopping applied (best metric=0.42319583892822266)
Finished Training
Total time taken: 64.40638852119446
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.671
[3,     1] loss: 0.617
[4,     1] loss: 0.549
[5,     1] loss: 0.470
[6,     1] loss: 0.385
[7,     1] loss: 0.304
[8,     1] loss: 0.252
[9,     1] loss: 0.210
[10,     1] loss: 0.149
[11,     1] loss: 0.125
[12,     1] loss: 0.115
[13,     1] loss: 0.081
[14,     1] loss: 0.051
[15,     1] loss: 0.052
[16,     1] loss: 0.037
[17,     1] loss: 0.025
[18,     1] loss: 0.016
[19,     1] loss: 0.014
[20,     1] loss: 0.008
[21,     1] loss: 0.007
[22,     1] loss: 0.005
[23,     1] loss: 0.004
[24,     1] loss: 0.003
[25,     1] loss: 0.002
[26,     1] loss: 0.002
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.003
[45,     1] loss: 0.003
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
Early stopping applied (best metric=0.4395040273666382)
Finished Training
Total time taken: 65.67201805114746
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.646
[4,     1] loss: 0.598
[5,     1] loss: 0.537
[6,     1] loss: 0.481
[7,     1] loss: 0.418
[8,     1] loss: 0.345
[9,     1] loss: 0.271
[10,     1] loss: 0.228
[11,     1] loss: 0.191
[12,     1] loss: 0.144
[13,     1] loss: 0.140
[14,     1] loss: 0.106
[15,     1] loss: 0.086
[16,     1] loss: 0.145
[17,     1] loss: 0.070
[18,     1] loss: 0.111
[19,     1] loss: 0.062
[20,     1] loss: 0.059
[21,     1] loss: 0.051
[22,     1] loss: 0.040
[23,     1] loss: 0.033
[24,     1] loss: 0.026
[25,     1] loss: 0.022
[26,     1] loss: 0.015
[27,     1] loss: 0.015
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.005
[32,     1] loss: 0.004
[33,     1] loss: 0.003
[34,     1] loss: 0.003
[35,     1] loss: 0.002
[36,     1] loss: 0.002
[37,     1] loss: 0.002
[38,     1] loss: 0.002
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.002
[43,     1] loss: 0.001
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.003
[54,     1] loss: 0.003
[55,     1] loss: 0.003
[56,     1] loss: 0.003
[57,     1] loss: 0.002
[58,     1] loss: 0.002
Early stopping applied (best metric=0.3820336163043976)
Finished Training
Total time taken: 69.46543836593628
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.693
[3,     1] loss: 0.675
[4,     1] loss: 0.650
[5,     1] loss: 0.619
[6,     1] loss: 0.577
[7,     1] loss: 0.544
[8,     1] loss: 0.498
[9,     1] loss: 0.456
[10,     1] loss: 0.417
[11,     1] loss: 0.381
[12,     1] loss: 0.322
[13,     1] loss: 0.324
[14,     1] loss: 0.257
[15,     1] loss: 0.212
[16,     1] loss: 0.306
[17,     1] loss: 0.325
[18,     1] loss: 0.241
[19,     1] loss: 0.172
[20,     1] loss: 0.161
[21,     1] loss: 0.167
[22,     1] loss: 0.113
[23,     1] loss: 0.092
[24,     1] loss: 0.081
[25,     1] loss: 0.066
[26,     1] loss: 0.052
[27,     1] loss: 0.037
[28,     1] loss: 0.025
[29,     1] loss: 0.018
[30,     1] loss: 0.014
[31,     1] loss: 0.010
[32,     1] loss: 0.008
[33,     1] loss: 0.006
[34,     1] loss: 0.005
[35,     1] loss: 0.004
[36,     1] loss: 0.003
[37,     1] loss: 0.003
[38,     1] loss: 0.003
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.003
[54,     1] loss: 0.003
[55,     1] loss: 0.003
[56,     1] loss: 0.004
[57,     1] loss: 0.003
Early stopping applied (best metric=0.4192992150783539)
Finished Training
Total time taken: 68.47442054748535
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.686
[3,     1] loss: 0.640
[4,     1] loss: 0.582
[5,     1] loss: 0.507
[6,     1] loss: 0.426
[7,     1] loss: 0.365
[8,     1] loss: 0.306
[9,     1] loss: 0.270
[10,     1] loss: 0.235
[11,     1] loss: 0.285
[12,     1] loss: 0.265
[13,     1] loss: 0.214
[14,     1] loss: 0.162
[15,     1] loss: 0.153
[16,     1] loss: 0.134
[17,     1] loss: 0.119
[18,     1] loss: 0.087
[19,     1] loss: 0.087
[20,     1] loss: 0.063
[21,     1] loss: 0.050
[22,     1] loss: 0.041
[23,     1] loss: 0.034
[24,     1] loss: 0.025
[25,     1] loss: 0.019
[26,     1] loss: 0.015
[27,     1] loss: 0.011
[28,     1] loss: 0.007
[29,     1] loss: 0.005
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.002
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.003
[54,     1] loss: 0.003
[55,     1] loss: 0.003
[56,     1] loss: 0.003
Early stopping applied (best metric=0.4580623209476471)
Finished Training
Total time taken: 67.51118922233582
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.629
[4,     1] loss: 0.579
[5,     1] loss: 0.524
[6,     1] loss: 0.477
[7,     1] loss: 0.426
[8,     1] loss: 0.371
[9,     1] loss: 0.310
[10,     1] loss: 0.239
[11,     1] loss: 0.168
[12,     1] loss: 0.151
[13,     1] loss: 0.122
[14,     1] loss: 0.076
[15,     1] loss: 0.066
[16,     1] loss: 0.057
[17,     1] loss: 0.045
[18,     1] loss: 0.035
[19,     1] loss: 0.027
[20,     1] loss: 0.015
[21,     1] loss: 0.009
[22,     1] loss: 0.008
[23,     1] loss: 0.007
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.004
[27,     1] loss: 0.003
[28,     1] loss: 0.003
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.001
[34,     1] loss: 0.002
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.002
[39,     1] loss: 0.001
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
Early stopping applied (best metric=0.448087215423584)
Finished Training
Total time taken: 66.53739762306213
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.676
[3,     1] loss: 0.622
[4,     1] loss: 0.568
[5,     1] loss: 0.516
[6,     1] loss: 0.462
[7,     1] loss: 0.396
[8,     1] loss: 0.315
[9,     1] loss: 0.243
[10,     1] loss: 0.178
[11,     1] loss: 0.125
[12,     1] loss: 0.086
[13,     1] loss: 0.042
[14,     1] loss: 0.036
[15,     1] loss: 0.071
[16,     1] loss: 0.026
[17,     1] loss: 0.036
[18,     1] loss: 0.006
[19,     1] loss: 0.169
[20,     1] loss: 0.004
[21,     1] loss: 0.250
[22,     1] loss: 0.024
[23,     1] loss: 0.019
[24,     1] loss: 0.040
[25,     1] loss: 0.019
[26,     1] loss: 0.014
[27,     1] loss: 0.017
[28,     1] loss: 0.015
[29,     1] loss: 0.016
[30,     1] loss: 0.015
[31,     1] loss: 0.016
[32,     1] loss: 0.015
[33,     1] loss: 0.013
[34,     1] loss: 0.012
[35,     1] loss: 0.011
[36,     1] loss: 0.010
[37,     1] loss: 0.009
[38,     1] loss: 0.008
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.005
[44,     1] loss: 0.005
[45,     1] loss: 0.005
[46,     1] loss: 0.004
[47,     1] loss: 0.005
[48,     1] loss: 0.004
[49,     1] loss: 0.004
[50,     1] loss: 0.004
[51,     1] loss: 0.004
[52,     1] loss: 0.004
[53,     1] loss: 0.004
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.003
Early stopping applied (best metric=0.30309343338012695)
Finished Training
Total time taken: 70.45442223548889
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.671
[3,     1] loss: 0.604
[4,     1] loss: 0.539
[5,     1] loss: 0.473
[6,     1] loss: 0.413
[7,     1] loss: 0.340
[8,     1] loss: 0.263
[9,     1] loss: 0.219
[10,     1] loss: 0.163
[11,     1] loss: 0.124
[12,     1] loss: 0.124
[13,     1] loss: 0.100
[14,     1] loss: 0.081
[15,     1] loss: 0.055
[16,     1] loss: 0.041
[17,     1] loss: 0.032
[18,     1] loss: 0.022
[19,     1] loss: 0.017
[20,     1] loss: 0.013
[21,     1] loss: 0.009
[22,     1] loss: 0.007
[23,     1] loss: 0.006
[24,     1] loss: 0.004
[25,     1] loss: 0.003
[26,     1] loss: 0.002
[27,     1] loss: 0.002
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.001
[31,     1] loss: 0.002
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.003
[54,     1] loss: 0.003
[55,     1] loss: 0.002
[56,     1] loss: 0.002
Early stopping applied (best metric=0.4399363100528717)
Finished Training
Total time taken: 68.08140826225281
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.711
[2,     1] loss: 0.699
[3,     1] loss: 0.654
[4,     1] loss: 0.597
[5,     1] loss: 0.544
[6,     1] loss: 0.499
[7,     1] loss: 0.440
[8,     1] loss: 0.381
[9,     1] loss: 0.304
[10,     1] loss: 0.239
[11,     1] loss: 0.173
[12,     1] loss: 0.125
[13,     1] loss: 0.091
[14,     1] loss: 0.081
[15,     1] loss: 0.047
[16,     1] loss: 0.045
[17,     1] loss: 0.030
[18,     1] loss: 0.024
[19,     1] loss: 0.020
[20,     1] loss: 0.018
[21,     1] loss: 0.012
[22,     1] loss: 0.010
[23,     1] loss: 0.006
[24,     1] loss: 0.005
[25,     1] loss: 0.003
[26,     1] loss: 0.002
[27,     1] loss: 0.002
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.002
[59,     1] loss: 0.001
Early stopping applied (best metric=0.3950079083442688)
Finished Training
Total time taken: 71.86743330955505
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.672
[3,     1] loss: 0.619
[4,     1] loss: 0.551
[5,     1] loss: 0.469
[6,     1] loss: 0.382
[7,     1] loss: 0.304
[8,     1] loss: 0.252
[9,     1] loss: 0.189
[10,     1] loss: 0.141
[11,     1] loss: 0.100
[12,     1] loss: 0.073
[13,     1] loss: 0.041
[14,     1] loss: 0.026
[15,     1] loss: 0.018
[16,     1] loss: 0.008
[17,     1] loss: 0.005
[18,     1] loss: 0.003
[19,     1] loss: 0.003
[20,     1] loss: 0.002
[21,     1] loss: 0.002
[22,     1] loss: 0.001
[23,     1] loss: 0.002
[24,     1] loss: 0.001
[25,     1] loss: 0.011
[26,     1] loss: 0.001
[27,     1] loss: 0.002
[28,     1] loss: 0.010
[29,     1] loss: 0.003
[30,     1] loss: 0.042
[31,     1] loss: 0.004
[32,     1] loss: 0.100
[33,     1] loss: 0.007
[34,     1] loss: 0.128
[35,     1] loss: 0.021
[36,     1] loss: 0.022
[37,     1] loss: 0.023
[38,     1] loss: 0.026
[39,     1] loss: 0.021
[40,     1] loss: 0.019
[41,     1] loss: 0.019
[42,     1] loss: 0.016
[43,     1] loss: 0.013
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.005
[49,     1] loss: 0.004
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
Early stopping applied (best metric=0.3559529185295105)
Finished Training
Total time taken: 69.78617215156555
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.656
[3,     1] loss: 0.594
[4,     1] loss: 0.525
[5,     1] loss: 0.448
[6,     1] loss: 0.382
[7,     1] loss: 0.314
[8,     1] loss: 0.262
[9,     1] loss: 0.208
[10,     1] loss: 0.160
[11,     1] loss: 0.122
[12,     1] loss: 0.083
[13,     1] loss: 0.055
[14,     1] loss: 0.048
[15,     1] loss: 0.028
[16,     1] loss: 0.027
[17,     1] loss: 0.013
[18,     1] loss: 0.012
[19,     1] loss: 0.007
[20,     1] loss: 0.005
[21,     1] loss: 0.004
[22,     1] loss: 0.003
[23,     1] loss: 0.002
[24,     1] loss: 0.002
[25,     1] loss: 0.001
[26,     1] loss: 0.001
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.001
Early stopping applied (best metric=0.4139748811721802)
Finished Training
Total time taken: 70.25947499275208
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.633
[4,     1] loss: 0.566
[5,     1] loss: 0.502
[6,     1] loss: 0.429
[7,     1] loss: 0.358
[8,     1] loss: 0.282
[9,     1] loss: 0.224
[10,     1] loss: 0.177
[11,     1] loss: 0.144
[12,     1] loss: 0.123
[13,     1] loss: 0.095
[14,     1] loss: 0.072
[15,     1] loss: 0.079
[16,     1] loss: 0.093
[17,     1] loss: 0.047
[18,     1] loss: 0.110
[19,     1] loss: 0.067
[20,     1] loss: 0.022
[21,     1] loss: 0.032
[22,     1] loss: 0.026
[23,     1] loss: 0.032
[24,     1] loss: 0.022
[25,     1] loss: 0.012
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.005
[31,     1] loss: 0.005
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.003
[35,     1] loss: 0.003
[36,     1] loss: 0.002
[37,     1] loss: 0.002
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.003
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
Early stopping applied (best metric=0.40397343039512634)
Finished Training
Total time taken: 70.26035499572754
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.662
[3,     1] loss: 0.604
[4,     1] loss: 0.553
[5,     1] loss: 0.496
[6,     1] loss: 0.431
[7,     1] loss: 0.363
[8,     1] loss: 0.283
[9,     1] loss: 0.198
[10,     1] loss: 0.146
[11,     1] loss: 0.156
[12,     1] loss: 0.107
[13,     1] loss: 0.097
[14,     1] loss: 0.070
[15,     1] loss: 0.046
[16,     1] loss: 0.044
[17,     1] loss: 0.028
[18,     1] loss: 0.033
[19,     1] loss: 0.013
[20,     1] loss: 0.010
[21,     1] loss: 0.006
[22,     1] loss: 0.005
[23,     1] loss: 0.003
[24,     1] loss: 0.003
[25,     1] loss: 0.002
[26,     1] loss: 0.002
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
Early stopping applied (best metric=0.39841166138648987)
Finished Training
Total time taken: 70.47437310218811
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.677
[3,     1] loss: 0.623
[4,     1] loss: 0.562
[5,     1] loss: 0.510
[6,     1] loss: 0.442
[7,     1] loss: 0.383
[8,     1] loss: 0.313
[9,     1] loss: 0.290
[10,     1] loss: 0.232
[11,     1] loss: 0.172
[12,     1] loss: 0.136
[13,     1] loss: 0.108
[14,     1] loss: 0.102
[15,     1] loss: 0.081
[16,     1] loss: 0.071
[17,     1] loss: 0.058
[18,     1] loss: 0.044
[19,     1] loss: 0.031
[20,     1] loss: 0.026
[21,     1] loss: 0.020
[22,     1] loss: 0.015
[23,     1] loss: 0.011
[24,     1] loss: 0.007
[25,     1] loss: 0.005
[26,     1] loss: 0.003
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.003
[54,     1] loss: 0.003
[55,     1] loss: 0.003
[56,     1] loss: 0.003
[57,     1] loss: 0.003
[58,     1] loss: 0.003
Early stopping applied (best metric=0.3941956162452698)
Finished Training
Total time taken: 71.92937016487122
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.625
[4,     1] loss: 0.564
[5,     1] loss: 0.489
[6,     1] loss: 0.425
[7,     1] loss: 0.366
[8,     1] loss: 0.360
[9,     1] loss: 0.313
[10,     1] loss: 0.263
[11,     1] loss: 0.227
[12,     1] loss: 0.196
[13,     1] loss: 0.141
[14,     1] loss: 0.110
[15,     1] loss: 0.079
[16,     1] loss: 0.059
[17,     1] loss: 0.048
[18,     1] loss: 0.032
[19,     1] loss: 0.017
[20,     1] loss: 0.012
[21,     1] loss: 0.009
[22,     1] loss: 0.005
[23,     1] loss: 0.003
[24,     1] loss: 0.002
[25,     1] loss: 0.002
[26,     1] loss: 0.001
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
Early stopping applied (best metric=0.4883059859275818)
Finished Training
Total time taken: 67.30034971237183
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.678
[3,     1] loss: 0.612
[4,     1] loss: 0.546
[5,     1] loss: 0.472
[6,     1] loss: 0.410
[7,     1] loss: 0.328
[8,     1] loss: 0.247
[9,     1] loss: 0.174
[10,     1] loss: 0.153
[11,     1] loss: 0.094
[12,     1] loss: 0.065
[13,     1] loss: 0.038
[14,     1] loss: 0.030
[15,     1] loss: 0.021
[16,     1] loss: 0.016
[17,     1] loss: 0.011
[18,     1] loss: 0.009
[19,     1] loss: 0.024
[20,     1] loss: 0.006
[21,     1] loss: 0.073
[22,     1] loss: 0.128
[23,     1] loss: 0.024
[24,     1] loss: 0.021
[25,     1] loss: 0.054
[26,     1] loss: 0.016
[27,     1] loss: 0.035
[28,     1] loss: 0.018
[29,     1] loss: 0.011
[30,     1] loss: 0.015
[31,     1] loss: 0.011
[32,     1] loss: 0.009
[33,     1] loss: 0.011
[34,     1] loss: 0.010
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.009
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.007
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.005
[44,     1] loss: 0.005
[45,     1] loss: 0.004
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.002
[53,     1] loss: 0.003
[54,     1] loss: 0.002
[55,     1] loss: 0.003
[56,     1] loss: 0.003
[57,     1] loss: 0.003
Early stopping applied (best metric=0.362625390291214)
Finished Training
Total time taken: 71.0762848854065
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.700
[3,     1] loss: 0.660
[4,     1] loss: 0.604
[5,     1] loss: 0.530
[6,     1] loss: 0.474
[7,     1] loss: 0.413
[8,     1] loss: 0.334
[9,     1] loss: 0.260
[10,     1] loss: 0.224
[11,     1] loss: 0.185
[12,     1] loss: 0.113
[13,     1] loss: 0.096
[14,     1] loss: 0.064
[15,     1] loss: 0.051
[16,     1] loss: 0.031
[17,     1] loss: 0.023
[18,     1] loss: 0.014
[19,     1] loss: 0.009
[20,     1] loss: 0.005
[21,     1] loss: 0.004
[22,     1] loss: 0.003
[23,     1] loss: 0.002
[24,     1] loss: 0.001
[25,     1] loss: 0.001
[26,     1] loss: 0.001
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.001
[58,     1] loss: 0.001
Early stopping applied (best metric=0.40933752059936523)
Finished Training
Total time taken: 72.53138375282288
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.694
[3,     1] loss: 0.669
[4,     1] loss: 0.634
[5,     1] loss: 0.593
[6,     1] loss: 0.550
[7,     1] loss: 0.517
[8,     1] loss: 0.469
[9,     1] loss: 0.422
[10,     1] loss: 0.374
[11,     1] loss: 0.349
[12,     1] loss: 0.303
[13,     1] loss: 0.267
[14,     1] loss: 0.238
[15,     1] loss: 0.176
[16,     1] loss: 0.143
[17,     1] loss: 0.139
[18,     1] loss: 0.083
[19,     1] loss: 0.107
[20,     1] loss: 0.051
[21,     1] loss: 0.050
[22,     1] loss: 0.019
[23,     1] loss: 0.009
[24,     1] loss: 0.008
[25,     1] loss: 0.011
[26,     1] loss: 0.005
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.002
[59,     1] loss: 0.001
[60,     1] loss: 0.002
[61,     1] loss: 0.001
Early stopping applied (best metric=0.4338919520378113)
Finished Training
Total time taken: 76.55240440368652
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.686
[3,     1] loss: 0.638
[4,     1] loss: 0.575
[5,     1] loss: 0.508
[6,     1] loss: 0.441
[7,     1] loss: 0.370
[8,     1] loss: 0.346
[9,     1] loss: 0.256
[10,     1] loss: 0.221
[11,     1] loss: 0.212
[12,     1] loss: 0.157
[13,     1] loss: 0.132
[14,     1] loss: 0.091
[15,     1] loss: 0.078
[16,     1] loss: 0.050
[17,     1] loss: 0.039
[18,     1] loss: 0.023
[19,     1] loss: 0.016
[20,     1] loss: 0.008
[21,     1] loss: 0.006
[22,     1] loss: 0.004
[23,     1] loss: 0.004
[24,     1] loss: 0.002
[25,     1] loss: 0.002
[26,     1] loss: 0.001
[27,     1] loss: 0.001
[28,     1] loss: 0.001
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.003
[45,     1] loss: 0.002
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
Early stopping applied (best metric=0.37576064467430115)
Finished Training
Total time taken: 71.82537317276001
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.637
[4,     1] loss: 0.584
[5,     1] loss: 0.513
[6,     1] loss: 0.435
[7,     1] loss: 0.340
[8,     1] loss: 0.256
[9,     1] loss: 0.250
[10,     1] loss: 0.162
[11,     1] loss: 0.159
[12,     1] loss: 0.112
[13,     1] loss: 0.141
[14,     1] loss: 0.100
[15,     1] loss: 0.050
[16,     1] loss: 0.051
[17,     1] loss: 0.050
[18,     1] loss: 0.035
[19,     1] loss: 0.026
[20,     1] loss: 0.020
[21,     1] loss: 0.012
[22,     1] loss: 0.010
[23,     1] loss: 0.007
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.003
[27,     1] loss: 0.003
[28,     1] loss: 0.003
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.003
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.003
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.002
Early stopping applied (best metric=0.3859752118587494)
Finished Training
Total time taken: 73.37336897850037
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.671
[3,     1] loss: 0.607
[4,     1] loss: 0.545
[5,     1] loss: 0.493
[6,     1] loss: 0.458
[7,     1] loss: 0.405
[8,     1] loss: 0.361
[9,     1] loss: 0.319
[10,     1] loss: 0.275
[11,     1] loss: 0.226
[12,     1] loss: 0.188
[13,     1] loss: 0.156
[14,     1] loss: 0.120
[15,     1] loss: 0.095
[16,     1] loss: 0.077
[17,     1] loss: 0.063
[18,     1] loss: 0.047
[19,     1] loss: 0.033
[20,     1] loss: 0.057
[21,     1] loss: 0.039
[22,     1] loss: 0.019
[23,     1] loss: 0.017
[24,     1] loss: 0.073
[25,     1] loss: 0.021
[26,     1] loss: 0.082
[27,     1] loss: 0.052
[28,     1] loss: 0.019
[29,     1] loss: 0.024
[30,     1] loss: 0.017
[31,     1] loss: 0.010
[32,     1] loss: 0.011
[33,     1] loss: 0.012
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.006
[40,     1] loss: 0.005
[41,     1] loss: 0.005
[42,     1] loss: 0.005
[43,     1] loss: 0.004
[44,     1] loss: 0.004
[45,     1] loss: 0.003
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
Early stopping applied (best metric=0.4939993619918823)
Finished Training
Total time taken: 68.7023651599884
{'Hydroxylation-P Validation Accuracy': 0.6118724531749657, 'Hydroxylation-P Validation Sensitivity': 0.8874603174603175, 'Hydroxylation-P Validation Specificity': 0.5526245698039802, 'Hydroxylation-P Validation Precision': 0.3105070108643463, 'Hydroxylation-P AUC ROC': 0.839206308829083, 'Hydroxylation-P AUC PR': 0.5701886498740991, 'Hydroxylation-P MCC': 0.3427525822402572, 'Hydroxylation-P F1': 0.45621884757152603, 'Validation Loss (Hydroxylation-P)': 0.41174611568450925, 'Validation Loss (total)': 0.41174611568450925}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001438856554656808,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 981888722,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.710028814856003}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.680
[3,     1] loss: 0.666
[4,     1] loss: 0.657
[5,     1] loss: 0.646
[6,     1] loss: 0.638
[7,     1] loss: 0.628
[8,     1] loss: 0.620
[9,     1] loss: 0.610
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005608238860456867,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1505892584,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.281825782896519}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.694
[3,     1] loss: 0.682
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006142637698770166,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 294149635,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.758411098357044}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004787875950047736,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3292411560,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.29575318157845}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.684
[3,     1] loss: 0.663
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.616
[7,     1] loss: 0.599
[8,     1] loss: 0.582
[9,     1] loss: 0.558
[10,     1] loss: 0.539
[11,     1] loss: 0.519
[12,     1] loss: 0.500
[13,     1] loss: 0.477
[14,     1] loss: 0.462
[15,     1] loss: 0.437
[16,     1] loss: 0.419
[17,     1] loss: 0.413
[18,     1] loss: 0.391
[19,     1] loss: 0.377
[20,     1] loss: 0.363
[21,     1] loss: 0.354
[22,     1] loss: 0.348
[23,     1] loss: 0.359
[24,     1] loss: 0.327
[25,     1] loss: 0.338
[26,     1] loss: 0.363
[27,     1] loss: 0.345
[28,     1] loss: 0.351
[29,     1] loss: 0.329
[30,     1] loss: 0.324
[31,     1] loss: 0.320
[32,     1] loss: 0.310
[33,     1] loss: 0.302
[34,     1] loss: 0.292
[35,     1] loss: 0.286
[36,     1] loss: 0.276
[37,     1] loss: 0.265
[38,     1] loss: 0.259
[39,     1] loss: 0.252
[40,     1] loss: 0.245
[41,     1] loss: 0.244
[42,     1] loss: 0.274
[43,     1] loss: 0.371
[44,     1] loss: 0.613
[45,     1] loss: 0.376
[46,     1] loss: 0.430
[47,     1] loss: 0.453
[48,     1] loss: 0.460
[49,     1] loss: 0.457
[50,     1] loss: 0.457
[51,     1] loss: 0.447
[52,     1] loss: 0.442
[53,     1] loss: 0.429
[54,     1] loss: 0.419
[55,     1] loss: 0.408
[56,     1] loss: 0.398
[57,     1] loss: 0.387
[58,     1] loss: 0.377
[59,     1] loss: 0.368
[60,     1] loss: 0.355
[61,     1] loss: 0.345
[62,     1] loss: 0.336
[63,     1] loss: 0.331
[64,     1] loss: 0.323
[65,     1] loss: 0.317
[66,     1] loss: 0.310
[67,     1] loss: 0.304
[68,     1] loss: 0.307
[69,     1] loss: 0.348
[70,     1] loss: 0.391
[71,     1] loss: 0.771
[72,     1] loss: 0.485
[73,     1] loss: 0.531
[74,     1] loss: 0.516
[75,     1] loss: 0.524
[76,     1] loss: 0.531
[77,     1] loss: 0.532
[78,     1] loss: 0.536
[79,     1] loss: 0.532
[80,     1] loss: 0.531
[81,     1] loss: 0.529
[82,     1] loss: 0.524
[83,     1] loss: 0.521
[84,     1] loss: 0.517
[85,     1] loss: 0.509
[86,     1] loss: 0.503
Early stopping applied (best metric=0.3990939259529114)
Finished Training
Total time taken: 107.86653780937195
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.671
[4,     1] loss: 0.659
[5,     1] loss: 0.645
[6,     1] loss: 0.629
[7,     1] loss: 0.613
[8,     1] loss: 0.596
[9,     1] loss: 0.577
[10,     1] loss: 0.558
[11,     1] loss: 0.532
[12,     1] loss: 0.508
[13,     1] loss: 0.489
[14,     1] loss: 0.461
[15,     1] loss: 0.435
[16,     1] loss: 0.415
[17,     1] loss: 0.403
[18,     1] loss: 0.374
[19,     1] loss: 0.378
[20,     1] loss: 0.361
[21,     1] loss: 0.348
[22,     1] loss: 0.324
[23,     1] loss: 0.315
[24,     1] loss: 0.298
[25,     1] loss: 0.286
[26,     1] loss: 0.274
[27,     1] loss: 0.262
[28,     1] loss: 0.250
[29,     1] loss: 0.240
[30,     1] loss: 0.235
[31,     1] loss: 0.226
[32,     1] loss: 0.219
[33,     1] loss: 0.214
[34,     1] loss: 0.205
[35,     1] loss: 0.197
[36,     1] loss: 0.188
[37,     1] loss: 0.184
[38,     1] loss: 0.230
[39,     1] loss: 0.200
[40,     1] loss: 0.214
[41,     1] loss: 0.194
[42,     1] loss: 0.196
[43,     1] loss: 0.256
[44,     1] loss: 0.501
[45,     1] loss: 0.406
[46,     1] loss: 0.406
[47,     1] loss: 0.442
[48,     1] loss: 0.439
[49,     1] loss: 0.442
[50,     1] loss: 0.447
[51,     1] loss: 0.455
[52,     1] loss: 0.446
[53,     1] loss: 0.437
[54,     1] loss: 0.425
[55,     1] loss: 0.412
[56,     1] loss: 0.398
[57,     1] loss: 0.380
[58,     1] loss: 0.363
[59,     1] loss: 0.346
[60,     1] loss: 0.333
[61,     1] loss: 0.316
[62,     1] loss: 0.304
[63,     1] loss: 0.306
[64,     1] loss: 0.286
[65,     1] loss: 0.275
[66,     1] loss: 0.266
[67,     1] loss: 0.265
[68,     1] loss: 0.289
[69,     1] loss: 0.259
[70,     1] loss: 0.252
Early stopping applied (best metric=0.36235666275024414)
Finished Training
Total time taken: 88.50545310974121
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.692
[3,     1] loss: 0.680
[4,     1] loss: 0.668
[5,     1] loss: 0.654
[6,     1] loss: 0.636
[7,     1] loss: 0.615
[8,     1] loss: 0.592
[9,     1] loss: 0.565
[10,     1] loss: 0.541
[11,     1] loss: 0.516
[12,     1] loss: 0.491
[13,     1] loss: 0.470
[14,     1] loss: 0.451
[15,     1] loss: 0.436
[16,     1] loss: 0.421
[17,     1] loss: 0.409
[18,     1] loss: 0.400
[19,     1] loss: 0.391
[20,     1] loss: 0.381
[21,     1] loss: 0.374
[22,     1] loss: 0.364
[23,     1] loss: 0.355
[24,     1] loss: 0.343
[25,     1] loss: 0.339
[26,     1] loss: 0.339
[27,     1] loss: 0.333
[28,     1] loss: 0.335
[29,     1] loss: 0.312
[30,     1] loss: 0.317
[31,     1] loss: 0.295
[32,     1] loss: 0.294
[33,     1] loss: 0.275
[34,     1] loss: 0.265
[35,     1] loss: 0.258
[36,     1] loss: 0.245
[37,     1] loss: 0.236
[38,     1] loss: 0.227
[39,     1] loss: 0.216
[40,     1] loss: 0.208
[41,     1] loss: 0.200
[42,     1] loss: 0.193
[43,     1] loss: 0.188
[44,     1] loss: 0.182
[45,     1] loss: 0.178
[46,     1] loss: 0.170
[47,     1] loss: 0.163
[48,     1] loss: 0.173
[49,     1] loss: 0.165
[50,     1] loss: 0.155
[51,     1] loss: 0.149
[52,     1] loss: 0.190
[53,     1] loss: 0.177
[54,     1] loss: 0.541
[55,     1] loss: 0.630
[56,     1] loss: 0.459
[57,     1] loss: 0.522
[58,     1] loss: 0.530
[59,     1] loss: 0.527
[60,     1] loss: 0.541
[61,     1] loss: 0.556
[62,     1] loss: 0.569
[63,     1] loss: 0.576
[64,     1] loss: 0.583
[65,     1] loss: 0.585
[66,     1] loss: 0.588
[67,     1] loss: 0.592
[68,     1] loss: 0.592
[69,     1] loss: 0.594
[70,     1] loss: 0.594
[71,     1] loss: 0.593
[72,     1] loss: 0.591
[73,     1] loss: 0.590
[74,     1] loss: 0.586
[75,     1] loss: 0.583
[76,     1] loss: 0.579
[77,     1] loss: 0.575
[78,     1] loss: 0.572
[79,     1] loss: 0.566
[80,     1] loss: 0.560
[81,     1] loss: 0.554
[82,     1] loss: 0.545
[83,     1] loss: 0.537
[84,     1] loss: 0.529
[85,     1] loss: 0.522
[86,     1] loss: 0.512
[87,     1] loss: 0.498
[88,     1] loss: 0.482
[89,     1] loss: 0.466
[90,     1] loss: 0.449
[91,     1] loss: 0.435
[92,     1] loss: 0.431
[93,     1] loss: 0.427
[94,     1] loss: 0.414
[95,     1] loss: 0.535
[96,     1] loss: 0.504
[97,     1] loss: 0.473
[98,     1] loss: 0.460
[99,     1] loss: 0.457
[100,     1] loss: 0.454
Early stopping applied (best metric=0.4101262390613556)
Finished Training
Total time taken: 126.56646656990051
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.663
[5,     1] loss: 0.648
[6,     1] loss: 0.632
[7,     1] loss: 0.616
[8,     1] loss: 0.599
[9,     1] loss: 0.581
[10,     1] loss: 0.561
[11,     1] loss: 0.539
[12,     1] loss: 0.514
[13,     1] loss: 0.490
[14,     1] loss: 0.462
[15,     1] loss: 0.435
[16,     1] loss: 0.412
[17,     1] loss: 0.386
[18,     1] loss: 0.361
[19,     1] loss: 0.340
[20,     1] loss: 0.318
[21,     1] loss: 0.298
[22,     1] loss: 0.279
[23,     1] loss: 0.262
[24,     1] loss: 0.247
[25,     1] loss: 0.232
[26,     1] loss: 0.222
[27,     1] loss: 0.208
[28,     1] loss: 0.204
[29,     1] loss: 0.197
[30,     1] loss: 0.185
[31,     1] loss: 0.178
[32,     1] loss: 0.171
[33,     1] loss: 0.166
[34,     1] loss: 0.158
[35,     1] loss: 0.152
[36,     1] loss: 0.147
[37,     1] loss: 0.158
[38,     1] loss: 0.186
[39,     1] loss: 0.486
[40,     1] loss: 0.362
[41,     1] loss: 0.357
[42,     1] loss: 0.356
[43,     1] loss: 0.367
[44,     1] loss: 0.366
[45,     1] loss: 0.361
[46,     1] loss: 0.355
[47,     1] loss: 0.340
[48,     1] loss: 0.325
[49,     1] loss: 0.310
[50,     1] loss: 0.296
[51,     1] loss: 0.282
[52,     1] loss: 0.268
[53,     1] loss: 0.254
[54,     1] loss: 0.238
[55,     1] loss: 0.228
[56,     1] loss: 0.215
[57,     1] loss: 0.206
[58,     1] loss: 0.196
[59,     1] loss: 0.188
[60,     1] loss: 0.180
[61,     1] loss: 0.175
[62,     1] loss: 0.174
[63,     1] loss: 0.172
[64,     1] loss: 0.181
[65,     1] loss: 0.183
[66,     1] loss: 0.175
[67,     1] loss: 0.170
[68,     1] loss: 0.163
[69,     1] loss: 0.159
[70,     1] loss: 0.174
[71,     1] loss: 0.227
[72,     1] loss: 0.532
[73,     1] loss: 0.475
[74,     1] loss: 0.439
[75,     1] loss: 0.431
[76,     1] loss: 0.440
[77,     1] loss: 0.436
[78,     1] loss: 0.428
[79,     1] loss: 0.417
[80,     1] loss: 0.406
[81,     1] loss: 0.392
[82,     1] loss: 0.382
[83,     1] loss: 0.370
[84,     1] loss: 0.358
[85,     1] loss: 0.348
[86,     1] loss: 0.333
[87,     1] loss: 0.324
[88,     1] loss: 0.311
[89,     1] loss: 0.302
[90,     1] loss: 0.295
[91,     1] loss: 0.282
[92,     1] loss: 0.271
[93,     1] loss: 0.266
[94,     1] loss: 0.251
[95,     1] loss: 0.252
[96,     1] loss: 0.271
[97,     1] loss: 0.256
[98,     1] loss: 0.250
[99,     1] loss: 0.240
[100,     1] loss: 0.229
[101,     1] loss: 0.222
[102,     1] loss: 0.215
[103,     1] loss: 0.207
[104,     1] loss: 0.200
[105,     1] loss: 0.204
[106,     1] loss: 0.203
[107,     1] loss: 0.239
[108,     1] loss: 0.245
[109,     1] loss: 0.223
[110,     1] loss: 0.219
[111,     1] loss: 0.213
[112,     1] loss: 0.205
Early stopping applied (best metric=0.36402201652526855)
Finished Training
Total time taken: 142.234721660614
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.684
[3,     1] loss: 0.665
[4,     1] loss: 0.647
[5,     1] loss: 0.625
[6,     1] loss: 0.602
[7,     1] loss: 0.578
[8,     1] loss: 0.551
[9,     1] loss: 0.526
[10,     1] loss: 0.505
[11,     1] loss: 0.485
[12,     1] loss: 0.466
[13,     1] loss: 0.451
[14,     1] loss: 0.439
[15,     1] loss: 0.430
[16,     1] loss: 0.419
[17,     1] loss: 0.413
[18,     1] loss: 0.407
[19,     1] loss: 0.402
[20,     1] loss: 0.398
[21,     1] loss: 0.395
[22,     1] loss: 0.392
[23,     1] loss: 0.389
[24,     1] loss: 0.387
[25,     1] loss: 0.386
[26,     1] loss: 0.385
[27,     1] loss: 0.384
[28,     1] loss: 0.382
[29,     1] loss: 0.381
[30,     1] loss: 0.381
[31,     1] loss: 0.381
[32,     1] loss: 0.381
[33,     1] loss: 0.384
[34,     1] loss: 0.413
[35,     1] loss: 0.473
[36,     1] loss: 0.443
[37,     1] loss: 0.442
[38,     1] loss: 0.436
[39,     1] loss: 0.421
[40,     1] loss: 0.415
[41,     1] loss: 0.403
[42,     1] loss: 0.397
[43,     1] loss: 0.392
[44,     1] loss: 0.382
[45,     1] loss: 0.378
[46,     1] loss: 0.375
[47,     1] loss: 0.373
[48,     1] loss: 0.373
[49,     1] loss: 0.372
[50,     1] loss: 0.373
[51,     1] loss: 0.373
[52,     1] loss: 0.384
[53,     1] loss: 0.378
[54,     1] loss: 0.382
[55,     1] loss: 0.379
[56,     1] loss: 0.379
[57,     1] loss: 0.376
[58,     1] loss: 0.373
[59,     1] loss: 0.372
[60,     1] loss: 0.375
[61,     1] loss: 0.383
Early stopping applied (best metric=0.48431769013404846)
Finished Training
Total time taken: 78.4819929599762
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.679
[3,     1] loss: 0.646
[4,     1] loss: 0.622
[5,     1] loss: 0.598
[6,     1] loss: 0.576
[7,     1] loss: 0.552
[8,     1] loss: 0.529
[9,     1] loss: 0.507
[10,     1] loss: 0.487
[11,     1] loss: 0.471
[12,     1] loss: 0.450
[13,     1] loss: 0.437
[14,     1] loss: 0.423
[15,     1] loss: 0.419
[16,     1] loss: 0.406
[17,     1] loss: 0.397
[18,     1] loss: 0.389
[19,     1] loss: 0.380
[20,     1] loss: 0.376
[21,     1] loss: 0.372
[22,     1] loss: 0.369
[23,     1] loss: 0.366
[24,     1] loss: 0.362
[25,     1] loss: 0.358
[26,     1] loss: 0.352
[27,     1] loss: 0.348
[28,     1] loss: 0.344
[29,     1] loss: 0.339
[30,     1] loss: 0.337
[31,     1] loss: 0.333
[32,     1] loss: 0.326
[33,     1] loss: 0.320
[34,     1] loss: 0.313
[35,     1] loss: 0.306
[36,     1] loss: 0.298
[37,     1] loss: 0.291
[38,     1] loss: 0.282
[39,     1] loss: 0.274
[40,     1] loss: 0.261
[41,     1] loss: 0.252
[42,     1] loss: 0.231
[43,     1] loss: 0.277
[44,     1] loss: 0.483
[45,     1] loss: 0.666
[46,     1] loss: 0.516
[47,     1] loss: 0.605
[48,     1] loss: 0.543
[49,     1] loss: 0.518
[50,     1] loss: 0.537
[51,     1] loss: 0.540
[52,     1] loss: 0.539
[53,     1] loss: 0.539
[54,     1] loss: 0.537
[55,     1] loss: 0.528
[56,     1] loss: 0.519
[57,     1] loss: 0.507
[58,     1] loss: 0.498
[59,     1] loss: 0.485
[60,     1] loss: 0.469
[61,     1] loss: 0.449
[62,     1] loss: 0.430
[63,     1] loss: 0.411
[64,     1] loss: 0.394
[65,     1] loss: 0.374
[66,     1] loss: 0.360
[67,     1] loss: 0.341
[68,     1] loss: 0.326
[69,     1] loss: 0.308
[70,     1] loss: 0.291
[71,     1] loss: 0.273
[72,     1] loss: 0.257
[73,     1] loss: 0.246
[74,     1] loss: 0.247
[75,     1] loss: 0.230
[76,     1] loss: 0.227
[77,     1] loss: 0.219
[78,     1] loss: 0.202
[79,     1] loss: 0.211
[80,     1] loss: 0.220
[81,     1] loss: 0.215
[82,     1] loss: 0.207
[83,     1] loss: 0.198
[84,     1] loss: 0.190
[85,     1] loss: 0.253
[86,     1] loss: 0.742
[87,     1] loss: 0.499
[88,     1] loss: 0.443
[89,     1] loss: 0.444
[90,     1] loss: 0.461
[91,     1] loss: 0.473
[92,     1] loss: 0.483
[93,     1] loss: 0.486
[94,     1] loss: 0.483
[95,     1] loss: 0.482
[96,     1] loss: 0.478
[97,     1] loss: 0.468
[98,     1] loss: 0.458
[99,     1] loss: 0.446
[100,     1] loss: 0.427
[101,     1] loss: 0.409
[102,     1] loss: 0.395
[103,     1] loss: 0.374
[104,     1] loss: 0.362
[105,     1] loss: 0.346
[106,     1] loss: 0.340
[107,     1] loss: 0.338
[108,     1] loss: 0.314
[109,     1] loss: 0.296
[110,     1] loss: 0.305
[111,     1] loss: 0.293
[112,     1] loss: 0.316
[113,     1] loss: 0.491
[114,     1] loss: 0.400
[115,     1] loss: 0.359
[116,     1] loss: 0.343
[117,     1] loss: 0.337
[118,     1] loss: 0.326
[119,     1] loss: 0.313
[120,     1] loss: 0.325
[121,     1] loss: 0.316
[122,     1] loss: 0.297
[123,     1] loss: 0.350
[124,     1] loss: 0.303
[125,     1] loss: 0.326
[126,     1] loss: 0.286
[127,     1] loss: 0.300
[128,     1] loss: 0.304
[129,     1] loss: 0.306
[130,     1] loss: 0.316
[131,     1] loss: 0.293
[132,     1] loss: 0.290
[133,     1] loss: 0.281
[134,     1] loss: 0.277
[135,     1] loss: 0.274
[136,     1] loss: 0.270
[137,     1] loss: 0.264
[138,     1] loss: 0.258
[139,     1] loss: 0.252
Early stopping applied (best metric=0.42945799231529236)
Finished Training
Total time taken: 177.90751552581787
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.651
[5,     1] loss: 0.636
[6,     1] loss: 0.623
[7,     1] loss: 0.604
[8,     1] loss: 0.586
[9,     1] loss: 0.562
[10,     1] loss: 0.542
[11,     1] loss: 0.517
[12,     1] loss: 0.494
[13,     1] loss: 0.469
[14,     1] loss: 0.445
[15,     1] loss: 0.418
[16,     1] loss: 0.397
[17,     1] loss: 0.372
[18,     1] loss: 0.354
[19,     1] loss: 0.337
[20,     1] loss: 0.319
[21,     1] loss: 0.300
[22,     1] loss: 0.286
[23,     1] loss: 0.272
[24,     1] loss: 0.258
[25,     1] loss: 0.247
[26,     1] loss: 0.245
[27,     1] loss: 0.232
[28,     1] loss: 0.224
[29,     1] loss: 0.214
[30,     1] loss: 0.208
[31,     1] loss: 0.204
[32,     1] loss: 0.207
[33,     1] loss: 0.203
[34,     1] loss: 0.198
[35,     1] loss: 0.192
[36,     1] loss: 0.185
[37,     1] loss: 0.180
[38,     1] loss: 0.182
[39,     1] loss: 0.223
[40,     1] loss: 0.378
[41,     1] loss: 0.390
[42,     1] loss: 0.356
[43,     1] loss: 0.377
[44,     1] loss: 0.352
[45,     1] loss: 0.375
[46,     1] loss: 0.365
[47,     1] loss: 0.366
[48,     1] loss: 0.355
[49,     1] loss: 0.343
[50,     1] loss: 0.333
[51,     1] loss: 0.319
[52,     1] loss: 0.306
[53,     1] loss: 0.295
[54,     1] loss: 0.283
[55,     1] loss: 0.265
[56,     1] loss: 0.257
[57,     1] loss: 0.250
[58,     1] loss: 0.237
[59,     1] loss: 0.251
[60,     1] loss: 0.227
[61,     1] loss: 0.229
[62,     1] loss: 0.241
[63,     1] loss: 0.229
[64,     1] loss: 0.218
[65,     1] loss: 0.216
[66,     1] loss: 0.210
[67,     1] loss: 0.223
[68,     1] loss: 0.268
[69,     1] loss: 0.369
[70,     1] loss: 0.430
[71,     1] loss: 0.392
[72,     1] loss: 0.384
[73,     1] loss: 0.354
[74,     1] loss: 0.346
[75,     1] loss: 0.330
[76,     1] loss: 0.323
[77,     1] loss: 0.320
[78,     1] loss: 0.309
[79,     1] loss: 0.302
[80,     1] loss: 0.341
[81,     1] loss: 0.330
[82,     1] loss: 0.320
[83,     1] loss: 0.332
[84,     1] loss: 0.316
Early stopping applied (best metric=0.3874875605106354)
Finished Training
Total time taken: 108.47720980644226
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.646
[4,     1] loss: 0.626
[5,     1] loss: 0.608
[6,     1] loss: 0.590
[7,     1] loss: 0.566
[8,     1] loss: 0.547
[9,     1] loss: 0.525
[10,     1] loss: 0.499
[11,     1] loss: 0.476
[12,     1] loss: 0.448
[13,     1] loss: 0.424
[14,     1] loss: 0.399
[15,     1] loss: 0.376
[16,     1] loss: 0.358
[17,     1] loss: 0.340
[18,     1] loss: 0.320
[19,     1] loss: 0.301
[20,     1] loss: 0.285
[21,     1] loss: 0.273
[22,     1] loss: 0.264
[23,     1] loss: 0.253
[24,     1] loss: 0.242
[25,     1] loss: 0.230
[26,     1] loss: 0.220
[27,     1] loss: 0.213
[28,     1] loss: 0.208
[29,     1] loss: 0.199
[30,     1] loss: 0.192
[31,     1] loss: 0.187
[32,     1] loss: 0.183
[33,     1] loss: 0.177
[34,     1] loss: 0.173
[35,     1] loss: 0.169
[36,     1] loss: 0.167
[37,     1] loss: 0.162
[38,     1] loss: 0.152
[39,     1] loss: 0.156
[40,     1] loss: 0.360
[41,     1] loss: 0.727
[42,     1] loss: 0.622
[43,     1] loss: 0.535
[44,     1] loss: 0.512
[45,     1] loss: 0.502
[46,     1] loss: 0.518
[47,     1] loss: 0.523
[48,     1] loss: 0.523
[49,     1] loss: 0.518
[50,     1] loss: 0.513
[51,     1] loss: 0.505
[52,     1] loss: 0.495
[53,     1] loss: 0.483
[54,     1] loss: 0.469
[55,     1] loss: 0.455
[56,     1] loss: 0.438
[57,     1] loss: 0.425
[58,     1] loss: 0.413
[59,     1] loss: 0.398
[60,     1] loss: 0.390
[61,     1] loss: 0.388
[62,     1] loss: 0.382
[63,     1] loss: 0.368
[64,     1] loss: 0.369
Early stopping applied (best metric=0.39479178190231323)
Finished Training
Total time taken: 83.0656042098999
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.682
[3,     1] loss: 0.658
[4,     1] loss: 0.634
[5,     1] loss: 0.607
[6,     1] loss: 0.577
[7,     1] loss: 0.547
[8,     1] loss: 0.520
[9,     1] loss: 0.488
[10,     1] loss: 0.468
[11,     1] loss: 0.447
[12,     1] loss: 0.426
[13,     1] loss: 0.405
[14,     1] loss: 0.398
[15,     1] loss: 0.387
[16,     1] loss: 0.383
[17,     1] loss: 0.377
[18,     1] loss: 0.374
[19,     1] loss: 0.371
[20,     1] loss: 0.371
[21,     1] loss: 0.370
[22,     1] loss: 0.369
[23,     1] loss: 0.369
[24,     1] loss: 0.369
[25,     1] loss: 0.369
[26,     1] loss: 0.373
[27,     1] loss: 0.373
[28,     1] loss: 0.371
[29,     1] loss: 0.371
[30,     1] loss: 0.371
[31,     1] loss: 0.370
[32,     1] loss: 0.368
[33,     1] loss: 0.366
[34,     1] loss: 0.371
[35,     1] loss: 0.378
[36,     1] loss: 0.404
[37,     1] loss: 0.436
[38,     1] loss: 0.432
[39,     1] loss: 0.409
[40,     1] loss: 0.394
[41,     1] loss: 0.382
[42,     1] loss: 0.376
[43,     1] loss: 0.366
[44,     1] loss: 0.360
[45,     1] loss: 0.355
[46,     1] loss: 0.352
[47,     1] loss: 0.346
[48,     1] loss: 0.343
[49,     1] loss: 0.341
[50,     1] loss: 0.338
[51,     1] loss: 0.335
[52,     1] loss: 0.328
[53,     1] loss: 0.326
[54,     1] loss: 0.324
[55,     1] loss: 0.317
[56,     1] loss: 0.322
[57,     1] loss: 0.334
[58,     1] loss: 0.328
[59,     1] loss: 0.329
[60,     1] loss: 0.342
[61,     1] loss: 0.373
[62,     1] loss: 0.361
[63,     1] loss: 0.340
[64,     1] loss: 0.325
[65,     1] loss: 0.315
[66,     1] loss: 0.298
[67,     1] loss: 0.286
[68,     1] loss: 0.273
[69,     1] loss: 0.262
[70,     1] loss: 0.252
[71,     1] loss: 0.238
[72,     1] loss: 0.228
[73,     1] loss: 0.218
[74,     1] loss: 0.206
[75,     1] loss: 0.197
[76,     1] loss: 0.184
[77,     1] loss: 0.195
[78,     1] loss: 0.210
[79,     1] loss: 0.213
[80,     1] loss: 0.198
[81,     1] loss: 0.200
[82,     1] loss: 0.193
[83,     1] loss: 0.185
[84,     1] loss: 0.179
[85,     1] loss: 0.169
[86,     1] loss: 0.183
[87,     1] loss: 0.182
[88,     1] loss: 0.182
[89,     1] loss: 0.178
[90,     1] loss: 0.163
[91,     1] loss: 0.171
[92,     1] loss: 0.171
[93,     1] loss: 0.163
[94,     1] loss: 0.155
[95,     1] loss: 0.155
[96,     1] loss: 0.148
[97,     1] loss: 0.141
[98,     1] loss: 0.142
[99,     1] loss: 0.138
[100,     1] loss: 0.135
[101,     1] loss: 0.131
[102,     1] loss: 0.127
[103,     1] loss: 0.126
[104,     1] loss: 0.125
[105,     1] loss: 0.128
[106,     1] loss: 0.126
[107,     1] loss: 0.128
[108,     1] loss: 0.128
[109,     1] loss: 0.129
[110,     1] loss: 0.129
[111,     1] loss: 0.130
[112,     1] loss: 0.132
[113,     1] loss: 0.132
[114,     1] loss: 0.135
[115,     1] loss: 0.133
[116,     1] loss: 0.136
[117,     1] loss: 0.135
[118,     1] loss: 0.139
[119,     1] loss: 0.142
[120,     1] loss: 0.220
[121,     1] loss: 0.543
[122,     1] loss: 0.748
[123,     1] loss: 0.628
[124,     1] loss: 0.572
[125,     1] loss: 0.592
[126,     1] loss: 0.601
[127,     1] loss: 0.604
[128,     1] loss: 0.605
[129,     1] loss: 0.604
[130,     1] loss: 0.603
[131,     1] loss: 0.601
[132,     1] loss: 0.598
[133,     1] loss: 0.595
[134,     1] loss: 0.589
[135,     1] loss: 0.583
[136,     1] loss: 0.575
[137,     1] loss: 0.563
[138,     1] loss: 0.551
[139,     1] loss: 0.550
[140,     1] loss: 0.538
[141,     1] loss: 0.521
[142,     1] loss: 0.499
[143,     1] loss: 0.486
[144,     1] loss: 0.473
[145,     1] loss: 0.454
[146,     1] loss: 0.439
[147,     1] loss: 0.426
[148,     1] loss: 0.412
[149,     1] loss: 0.405
[150,     1] loss: 0.397
[151,     1] loss: 0.393
[152,     1] loss: 0.388
[153,     1] loss: 0.387
[154,     1] loss: 0.386
[155,     1] loss: 0.385
[156,     1] loss: 0.384
[157,     1] loss: 0.433
[158,     1] loss: 0.440
[159,     1] loss: 0.510
[160,     1] loss: 0.433
[161,     1] loss: 0.431
[162,     1] loss: 0.423
[163,     1] loss: 0.409
[164,     1] loss: 0.404
Early stopping applied (best metric=0.4124840795993805)
Finished Training
Total time taken: 212.78471183776855
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.680
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.637
[6,     1] loss: 0.621
[7,     1] loss: 0.599
[8,     1] loss: 0.579
[9,     1] loss: 0.557
[10,     1] loss: 0.535
[11,     1] loss: 0.514
[12,     1] loss: 0.495
[13,     1] loss: 0.477
[14,     1] loss: 0.465
[15,     1] loss: 0.457
[16,     1] loss: 0.439
[17,     1] loss: 0.426
[18,     1] loss: 0.427
[19,     1] loss: 0.420
[20,     1] loss: 0.405
[21,     1] loss: 0.412
[22,     1] loss: 0.396
[23,     1] loss: 0.386
[24,     1] loss: 0.377
[25,     1] loss: 0.367
[26,     1] loss: 0.358
[27,     1] loss: 0.347
[28,     1] loss: 0.335
[29,     1] loss: 0.323
[30,     1] loss: 0.312
[31,     1] loss: 0.303
[32,     1] loss: 0.292
[33,     1] loss: 0.281
[34,     1] loss: 0.269
[35,     1] loss: 0.257
[36,     1] loss: 0.244
[37,     1] loss: 0.232
[38,     1] loss: 0.228
[39,     1] loss: 0.217
[40,     1] loss: 0.212
[41,     1] loss: 0.201
[42,     1] loss: 0.191
[43,     1] loss: 0.189
[44,     1] loss: 0.328
[45,     1] loss: 0.844
[46,     1] loss: 0.712
[47,     1] loss: 0.586
[48,     1] loss: 0.623
[49,     1] loss: 0.625
[50,     1] loss: 0.606
[51,     1] loss: 0.597
[52,     1] loss: 0.610
[53,     1] loss: 0.620
[54,     1] loss: 0.626
[55,     1] loss: 0.628
[56,     1] loss: 0.630
[57,     1] loss: 0.629
[58,     1] loss: 0.629
[59,     1] loss: 0.631
[60,     1] loss: 0.631
[61,     1] loss: 0.632
[62,     1] loss: 0.632
[63,     1] loss: 0.631
[64,     1] loss: 0.630
[65,     1] loss: 0.630
[66,     1] loss: 0.629
[67,     1] loss: 0.626
[68,     1] loss: 0.624
[69,     1] loss: 0.621
[70,     1] loss: 0.617
[71,     1] loss: 0.613
[72,     1] loss: 0.608
[73,     1] loss: 0.603
[74,     1] loss: 0.597
[75,     1] loss: 0.588
[76,     1] loss: 0.579
[77,     1] loss: 0.571
[78,     1] loss: 0.560
[79,     1] loss: 0.552
[80,     1] loss: 0.550
[81,     1] loss: 0.538
[82,     1] loss: 0.523
[83,     1] loss: 0.510
[84,     1] loss: 0.501
[85,     1] loss: 0.503
[86,     1] loss: 0.491
[87,     1] loss: 0.477
[88,     1] loss: 0.479
[89,     1] loss: 0.490
[90,     1] loss: 0.530
[91,     1] loss: 0.479
Early stopping applied (best metric=0.44947609305381775)
Finished Training
Total time taken: 119.08262777328491
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.697
[3,     1] loss: 0.683
[4,     1] loss: 0.669
[5,     1] loss: 0.655
[6,     1] loss: 0.639
[7,     1] loss: 0.622
[8,     1] loss: 0.605
[9,     1] loss: 0.585
[10,     1] loss: 0.568
[11,     1] loss: 0.546
[12,     1] loss: 0.518
[13,     1] loss: 0.494
[14,     1] loss: 0.469
[15,     1] loss: 0.445
[16,     1] loss: 0.420
[17,     1] loss: 0.401
[18,     1] loss: 0.376
[19,     1] loss: 0.357
[20,     1] loss: 0.335
[21,     1] loss: 0.315
[22,     1] loss: 0.294
[23,     1] loss: 0.276
[24,     1] loss: 0.259
[25,     1] loss: 0.245
[26,     1] loss: 0.229
[27,     1] loss: 0.218
[28,     1] loss: 0.206
[29,     1] loss: 0.196
[30,     1] loss: 0.185
[31,     1] loss: 0.179
[32,     1] loss: 0.172
[33,     1] loss: 0.167
[34,     1] loss: 0.157
[35,     1] loss: 0.150
[36,     1] loss: 0.232
[37,     1] loss: 0.387
[38,     1] loss: 0.517
[39,     1] loss: 0.449
[40,     1] loss: 0.383
[41,     1] loss: 0.372
[42,     1] loss: 0.388
[43,     1] loss: 0.392
[44,     1] loss: 0.388
[45,     1] loss: 0.378
[46,     1] loss: 0.364
[47,     1] loss: 0.349
[48,     1] loss: 0.336
[49,     1] loss: 0.322
[50,     1] loss: 0.308
[51,     1] loss: 0.294
[52,     1] loss: 0.280
[53,     1] loss: 0.266
[54,     1] loss: 0.251
[55,     1] loss: 0.239
[56,     1] loss: 0.227
[57,     1] loss: 0.213
[58,     1] loss: 0.198
[59,     1] loss: 0.245
[60,     1] loss: 0.261
[61,     1] loss: 0.653
[62,     1] loss: 0.430
[63,     1] loss: 0.394
[64,     1] loss: 0.395
[65,     1] loss: 0.438
[66,     1] loss: 0.441
[67,     1] loss: 0.439
[68,     1] loss: 0.440
[69,     1] loss: 0.433
[70,     1] loss: 0.423
[71,     1] loss: 0.412
[72,     1] loss: 0.401
[73,     1] loss: 0.387
[74,     1] loss: 0.372
[75,     1] loss: 0.360
[76,     1] loss: 0.352
[77,     1] loss: 0.336
[78,     1] loss: 0.325
[79,     1] loss: 0.312
[80,     1] loss: 0.299
[81,     1] loss: 0.291
[82,     1] loss: 0.281
[83,     1] loss: 0.292
[84,     1] loss: 0.273
[85,     1] loss: 0.278
[86,     1] loss: 0.268
[87,     1] loss: 0.250
[88,     1] loss: 0.241
[89,     1] loss: 0.228
[90,     1] loss: 0.219
[91,     1] loss: 0.206
[92,     1] loss: 0.196
[93,     1] loss: 0.187
[94,     1] loss: 0.180
[95,     1] loss: 0.174
[96,     1] loss: 0.170
[97,     1] loss: 0.166
[98,     1] loss: 0.163
[99,     1] loss: 0.166
[100,     1] loss: 0.162
[101,     1] loss: 0.161
[102,     1] loss: 0.163
[103,     1] loss: 0.166
[104,     1] loss: 0.176
[105,     1] loss: 0.388
[106,     1] loss: 0.735
[107,     1] loss: 0.500
[108,     1] loss: 0.554
[109,     1] loss: 0.503
[110,     1] loss: 0.496
[111,     1] loss: 0.509
[112,     1] loss: 0.515
[113,     1] loss: 0.514
Early stopping applied (best metric=0.42769041657447815)
Finished Training
Total time taken: 148.27511310577393
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.657
[4,     1] loss: 0.638
[5,     1] loss: 0.619
[6,     1] loss: 0.599
[7,     1] loss: 0.578
[8,     1] loss: 0.557
[9,     1] loss: 0.532
[10,     1] loss: 0.507
[11,     1] loss: 0.483
[12,     1] loss: 0.460
[13,     1] loss: 0.435
[14,     1] loss: 0.414
[15,     1] loss: 0.392
[16,     1] loss: 0.370
[17,     1] loss: 0.350
[18,     1] loss: 0.328
[19,     1] loss: 0.313
[20,     1] loss: 0.297
[21,     1] loss: 0.280
[22,     1] loss: 0.267
[23,     1] loss: 0.256
[24,     1] loss: 0.247
[25,     1] loss: 0.243
[26,     1] loss: 0.230
[27,     1] loss: 0.222
[28,     1] loss: 0.214
[29,     1] loss: 0.213
[30,     1] loss: 0.218
[31,     1] loss: 0.217
[32,     1] loss: 0.218
[33,     1] loss: 0.211
[34,     1] loss: 0.204
[35,     1] loss: 0.200
[36,     1] loss: 0.193
[37,     1] loss: 0.189
[38,     1] loss: 0.183
[39,     1] loss: 0.183
[40,     1] loss: 0.241
[41,     1] loss: 0.568
[42,     1] loss: 0.422
[43,     1] loss: 0.372
[44,     1] loss: 0.397
[45,     1] loss: 0.400
[46,     1] loss: 0.402
[47,     1] loss: 0.407
[48,     1] loss: 0.402
[49,     1] loss: 0.394
[50,     1] loss: 0.384
[51,     1] loss: 0.368
[52,     1] loss: 0.352
[53,     1] loss: 0.336
[54,     1] loss: 0.317
[55,     1] loss: 0.302
[56,     1] loss: 0.287
[57,     1] loss: 0.271
[58,     1] loss: 0.258
[59,     1] loss: 0.247
[60,     1] loss: 0.236
[61,     1] loss: 0.226
[62,     1] loss: 0.221
[63,     1] loss: 0.211
[64,     1] loss: 0.206
[65,     1] loss: 0.203
[66,     1] loss: 0.200
[67,     1] loss: 0.309
[68,     1] loss: 0.270
[69,     1] loss: 0.285
[70,     1] loss: 0.257
[71,     1] loss: 0.262
[72,     1] loss: 0.254
[73,     1] loss: 0.252
[74,     1] loss: 0.242
[75,     1] loss: 0.235
[76,     1] loss: 0.229
[77,     1] loss: 0.228
[78,     1] loss: 0.292
[79,     1] loss: 0.235
[80,     1] loss: 0.235
[81,     1] loss: 0.232
[82,     1] loss: 0.229
[83,     1] loss: 0.221
[84,     1] loss: 0.215
[85,     1] loss: 0.208
[86,     1] loss: 0.206
[87,     1] loss: 0.205
[88,     1] loss: 0.197
[89,     1] loss: 0.196
[90,     1] loss: 0.195
[91,     1] loss: 0.193
[92,     1] loss: 0.191
[93,     1] loss: 0.190
[94,     1] loss: 0.188
[95,     1] loss: 0.189
[96,     1] loss: 0.190
[97,     1] loss: 0.222
[98,     1] loss: 0.240
[99,     1] loss: 0.303
[100,     1] loss: 0.918
[101,     1] loss: 0.786
[102,     1] loss: 0.684
[103,     1] loss: 0.630
Early stopping applied (best metric=0.4110766053199768)
Finished Training
Total time taken: 136.19070482254028
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.660
[4,     1] loss: 0.644
[5,     1] loss: 0.626
[6,     1] loss: 0.607
[7,     1] loss: 0.585
[8,     1] loss: 0.560
[9,     1] loss: 0.534
[10,     1] loss: 0.506
[11,     1] loss: 0.478
[12,     1] loss: 0.454
[13,     1] loss: 0.428
[14,     1] loss: 0.405
[15,     1] loss: 0.384
[16,     1] loss: 0.363
[17,     1] loss: 0.344
[18,     1] loss: 0.324
[19,     1] loss: 0.306
[20,     1] loss: 0.293
[21,     1] loss: 0.275
[22,     1] loss: 0.258
[23,     1] loss: 0.248
[24,     1] loss: 0.237
[25,     1] loss: 0.225
[26,     1] loss: 0.212
[27,     1] loss: 0.203
[28,     1] loss: 0.194
[29,     1] loss: 0.188
[30,     1] loss: 0.193
[31,     1] loss: 0.225
[32,     1] loss: 0.474
[33,     1] loss: 0.248
[34,     1] loss: 0.292
[35,     1] loss: 0.272
[36,     1] loss: 0.292
[37,     1] loss: 0.278
[38,     1] loss: 0.282
[39,     1] loss: 0.275
[40,     1] loss: 0.265
[41,     1] loss: 0.253
[42,     1] loss: 0.240
[43,     1] loss: 0.226
[44,     1] loss: 0.222
[45,     1] loss: 0.218
[46,     1] loss: 0.244
[47,     1] loss: 0.237
[48,     1] loss: 0.228
[49,     1] loss: 0.216
[50,     1] loss: 0.213
[51,     1] loss: 0.204
[52,     1] loss: 0.201
[53,     1] loss: 0.192
[54,     1] loss: 0.185
[55,     1] loss: 0.179
[56,     1] loss: 0.175
[57,     1] loss: 0.175
[58,     1] loss: 0.208
[59,     1] loss: 0.196
[60,     1] loss: 0.186
[61,     1] loss: 0.175
[62,     1] loss: 0.174
[63,     1] loss: 0.184
[64,     1] loss: 0.179
[65,     1] loss: 0.191
[66,     1] loss: 0.182
[67,     1] loss: 0.177
[68,     1] loss: 0.172
[69,     1] loss: 0.170
[70,     1] loss: 0.164
[71,     1] loss: 0.161
[72,     1] loss: 0.157
[73,     1] loss: 0.158
[74,     1] loss: 0.163
[75,     1] loss: 0.231
[76,     1] loss: 0.619
[77,     1] loss: 0.465
[78,     1] loss: 0.397
[79,     1] loss: 0.321
[80,     1] loss: 0.338
[81,     1] loss: 0.346
[82,     1] loss: 0.340
[83,     1] loss: 0.332
[84,     1] loss: 0.322
[85,     1] loss: 0.314
[86,     1] loss: 0.301
[87,     1] loss: 0.293
[88,     1] loss: 0.282
[89,     1] loss: 0.271
[90,     1] loss: 0.259
[91,     1] loss: 0.250
[92,     1] loss: 0.240
[93,     1] loss: 0.230
[94,     1] loss: 0.224
[95,     1] loss: 0.217
[96,     1] loss: 0.208
[97,     1] loss: 0.205
[98,     1] loss: 0.199
Early stopping applied (best metric=0.39982473850250244)
Finished Training
Total time taken: 129.98343324661255
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.652
[5,     1] loss: 0.639
[6,     1] loss: 0.625
[7,     1] loss: 0.609
[8,     1] loss: 0.591
[9,     1] loss: 0.573
[10,     1] loss: 0.552
[11,     1] loss: 0.530
[12,     1] loss: 0.510
[13,     1] loss: 0.486
[14,     1] loss: 0.463
[15,     1] loss: 0.434
[16,     1] loss: 0.410
[17,     1] loss: 0.386
[18,     1] loss: 0.360
[19,     1] loss: 0.337
[20,     1] loss: 0.319
[21,     1] loss: 0.296
[22,     1] loss: 0.275
[23,     1] loss: 0.259
[24,     1] loss: 0.242
[25,     1] loss: 0.228
[26,     1] loss: 0.215
[27,     1] loss: 0.202
[28,     1] loss: 0.193
[29,     1] loss: 0.180
[30,     1] loss: 0.173
[31,     1] loss: 0.165
[32,     1] loss: 0.160
[33,     1] loss: 0.157
[34,     1] loss: 0.159
[35,     1] loss: 0.173
[36,     1] loss: 0.191
[37,     1] loss: 0.252
[38,     1] loss: 0.211
[39,     1] loss: 0.210
[40,     1] loss: 0.227
[41,     1] loss: 0.216
[42,     1] loss: 0.204
[43,     1] loss: 0.201
[44,     1] loss: 0.191
[45,     1] loss: 0.196
[46,     1] loss: 0.174
[47,     1] loss: 0.176
[48,     1] loss: 0.175
[49,     1] loss: 0.174
[50,     1] loss: 0.164
[51,     1] loss: 0.159
[52,     1] loss: 0.155
[53,     1] loss: 0.145
[54,     1] loss: 0.143
[55,     1] loss: 0.145
[56,     1] loss: 0.152
[57,     1] loss: 0.147
[58,     1] loss: 0.139
[59,     1] loss: 0.145
[60,     1] loss: 0.141
[61,     1] loss: 0.138
[62,     1] loss: 0.139
[63,     1] loss: 0.137
Early stopping applied (best metric=0.4572876989841461)
Finished Training
Total time taken: 84.43905591964722
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.660
[4,     1] loss: 0.640
[5,     1] loss: 0.620
[6,     1] loss: 0.599
[7,     1] loss: 0.577
[8,     1] loss: 0.555
[9,     1] loss: 0.531
[10,     1] loss: 0.508
[11,     1] loss: 0.484
[12,     1] loss: 0.461
[13,     1] loss: 0.435
[14,     1] loss: 0.407
[15,     1] loss: 0.381
[16,     1] loss: 0.358
[17,     1] loss: 0.335
[18,     1] loss: 0.313
[19,     1] loss: 0.298
[20,     1] loss: 0.294
[21,     1] loss: 0.265
[22,     1] loss: 0.268
[23,     1] loss: 0.259
[24,     1] loss: 0.240
[25,     1] loss: 0.241
[26,     1] loss: 0.225
[27,     1] loss: 0.219
[28,     1] loss: 0.210
[29,     1] loss: 0.203
[30,     1] loss: 0.196
[31,     1] loss: 0.191
[32,     1] loss: 0.183
[33,     1] loss: 0.177
[34,     1] loss: 0.170
[35,     1] loss: 0.165
[36,     1] loss: 0.230
[37,     1] loss: 0.273
[38,     1] loss: 0.615
[39,     1] loss: 0.426
[40,     1] loss: 0.444
[41,     1] loss: 0.458
[42,     1] loss: 0.459
[43,     1] loss: 0.471
[44,     1] loss: 0.481
[45,     1] loss: 0.485
[46,     1] loss: 0.482
[47,     1] loss: 0.476
[48,     1] loss: 0.468
[49,     1] loss: 0.459
[50,     1] loss: 0.448
[51,     1] loss: 0.442
[52,     1] loss: 0.430
[53,     1] loss: 0.422
[54,     1] loss: 0.412
[55,     1] loss: 0.403
[56,     1] loss: 0.391
[57,     1] loss: 0.382
[58,     1] loss: 0.370
[59,     1] loss: 0.360
[60,     1] loss: 0.351
[61,     1] loss: 0.337
[62,     1] loss: 0.330
[63,     1] loss: 0.329
[64,     1] loss: 0.356
[65,     1] loss: 0.336
[66,     1] loss: 0.326
[67,     1] loss: 0.320
[68,     1] loss: 0.313
[69,     1] loss: 0.300
[70,     1] loss: 0.289
[71,     1] loss: 0.281
[72,     1] loss: 0.273
[73,     1] loss: 0.282
[74,     1] loss: 0.327
[75,     1] loss: 0.303
[76,     1] loss: 0.407
[77,     1] loss: 0.620
[78,     1] loss: 0.616
[79,     1] loss: 0.617
[80,     1] loss: 0.602
[81,     1] loss: 0.594
[82,     1] loss: 0.593
[83,     1] loss: 0.595
[84,     1] loss: 0.599
Early stopping applied (best metric=0.3921512961387634)
Finished Training
Total time taken: 112.29358386993408
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.698
[3,     1] loss: 0.687
[4,     1] loss: 0.678
[5,     1] loss: 0.667
[6,     1] loss: 0.656
[7,     1] loss: 0.643
[8,     1] loss: 0.628
[9,     1] loss: 0.612
[10,     1] loss: 0.595
[11,     1] loss: 0.575
[12,     1] loss: 0.558
[13,     1] loss: 0.539
[14,     1] loss: 0.517
[15,     1] loss: 0.499
[16,     1] loss: 0.475
[17,     1] loss: 0.455
[18,     1] loss: 0.432
[19,     1] loss: 0.412
[20,     1] loss: 0.391
[21,     1] loss: 0.375
[22,     1] loss: 0.354
[23,     1] loss: 0.337
[24,     1] loss: 0.323
[25,     1] loss: 0.307
[26,     1] loss: 0.294
[27,     1] loss: 0.281
[28,     1] loss: 0.265
[29,     1] loss: 0.253
[30,     1] loss: 0.242
[31,     1] loss: 0.233
[32,     1] loss: 0.225
[33,     1] loss: 0.217
[34,     1] loss: 0.208
[35,     1] loss: 0.199
[36,     1] loss: 0.189
[37,     1] loss: 0.174
[38,     1] loss: 0.182
[39,     1] loss: 0.404
[40,     1] loss: 0.585
[41,     1] loss: 0.496
[42,     1] loss: 0.509
[43,     1] loss: 0.489
[44,     1] loss: 0.500
[45,     1] loss: 0.509
[46,     1] loss: 0.514
[47,     1] loss: 0.520
[48,     1] loss: 0.520
[49,     1] loss: 0.522
[50,     1] loss: 0.522
[51,     1] loss: 0.517
[52,     1] loss: 0.513
[53,     1] loss: 0.504
[54,     1] loss: 0.495
[55,     1] loss: 0.486
[56,     1] loss: 0.475
[57,     1] loss: 0.465
[58,     1] loss: 0.451
[59,     1] loss: 0.436
[60,     1] loss: 0.423
[61,     1] loss: 0.407
[62,     1] loss: 0.400
[63,     1] loss: 0.407
[64,     1] loss: 0.393
[65,     1] loss: 0.389
[66,     1] loss: 0.389
[67,     1] loss: 0.417
[68,     1] loss: 0.389
[69,     1] loss: 0.377
[70,     1] loss: 0.359
[71,     1] loss: 0.339
[72,     1] loss: 0.330
[73,     1] loss: 0.311
[74,     1] loss: 0.295
[75,     1] loss: 0.282
[76,     1] loss: 0.278
[77,     1] loss: 0.265
[78,     1] loss: 0.275
[79,     1] loss: 0.268
[80,     1] loss: 0.270
[81,     1] loss: 0.255
[82,     1] loss: 0.249
[83,     1] loss: 0.238
[84,     1] loss: 0.234
[85,     1] loss: 0.226
[86,     1] loss: 0.224
[87,     1] loss: 0.215
[88,     1] loss: 0.211
[89,     1] loss: 0.207
[90,     1] loss: 0.200
[91,     1] loss: 0.192
[92,     1] loss: 0.188
[93,     1] loss: 0.184
[94,     1] loss: 0.190
[95,     1] loss: 0.226
[96,     1] loss: 0.211
[97,     1] loss: 0.227
[98,     1] loss: 0.443
[99,     1] loss: 0.540
[100,     1] loss: 0.401
[101,     1] loss: 0.513
[102,     1] loss: 0.439
[103,     1] loss: 0.430
[104,     1] loss: 0.431
[105,     1] loss: 0.423
[106,     1] loss: 0.424
[107,     1] loss: 0.421
[108,     1] loss: 0.419
[109,     1] loss: 0.416
[110,     1] loss: 0.415
[111,     1] loss: 0.411
[112,     1] loss: 0.405
[113,     1] loss: 0.402
[114,     1] loss: 0.398
[115,     1] loss: 0.387
[116,     1] loss: 0.381
[117,     1] loss: 0.374
[118,     1] loss: 0.366
[119,     1] loss: 0.353
[120,     1] loss: 0.345
Early stopping applied (best metric=0.4095216393470764)
Finished Training
Total time taken: 161.06084847450256
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.672
[3,     1] loss: 0.641
[4,     1] loss: 0.617
[5,     1] loss: 0.596
[6,     1] loss: 0.574
[7,     1] loss: 0.552
[8,     1] loss: 0.531
[9,     1] loss: 0.505
[10,     1] loss: 0.487
[11,     1] loss: 0.463
[12,     1] loss: 0.446
[13,     1] loss: 0.443
[14,     1] loss: 0.423
[15,     1] loss: 0.394
[16,     1] loss: 0.383
[17,     1] loss: 0.371
[18,     1] loss: 0.362
[19,     1] loss: 0.350
[20,     1] loss: 0.340
[21,     1] loss: 0.329
[22,     1] loss: 0.321
[23,     1] loss: 0.314
[24,     1] loss: 0.321
[25,     1] loss: 0.322
[26,     1] loss: 0.301
[27,     1] loss: 0.294
[28,     1] loss: 0.287
[29,     1] loss: 0.281
[30,     1] loss: 0.272
[31,     1] loss: 0.266
[32,     1] loss: 0.259
[33,     1] loss: 0.256
[34,     1] loss: 0.257
[35,     1] loss: 0.253
[36,     1] loss: 0.244
[37,     1] loss: 0.233
[38,     1] loss: 0.228
[39,     1] loss: 0.221
[40,     1] loss: 0.236
[41,     1] loss: 0.464
[42,     1] loss: 0.518
[43,     1] loss: 0.478
[44,     1] loss: 0.480
[45,     1] loss: 0.478
[46,     1] loss: 0.476
[47,     1] loss: 0.476
[48,     1] loss: 0.471
[49,     1] loss: 0.464
[50,     1] loss: 0.454
[51,     1] loss: 0.447
[52,     1] loss: 0.439
[53,     1] loss: 0.426
[54,     1] loss: 0.413
[55,     1] loss: 0.400
[56,     1] loss: 0.389
[57,     1] loss: 0.378
[58,     1] loss: 0.369
[59,     1] loss: 0.382
[60,     1] loss: 0.400
[61,     1] loss: 0.363
[62,     1] loss: 0.353
[63,     1] loss: 0.353
[64,     1] loss: 0.343
[65,     1] loss: 0.338
[66,     1] loss: 0.323
[67,     1] loss: 0.314
[68,     1] loss: 0.307
[69,     1] loss: 0.297
[70,     1] loss: 0.291
[71,     1] loss: 0.286
[72,     1] loss: 0.279
[73,     1] loss: 0.277
[74,     1] loss: 0.298
[75,     1] loss: 0.367
[76,     1] loss: 0.360
[77,     1] loss: 0.341
[78,     1] loss: 0.335
[79,     1] loss: 0.321
[80,     1] loss: 0.314
[81,     1] loss: 0.296
[82,     1] loss: 0.281
[83,     1] loss: 0.262
[84,     1] loss: 0.245
[85,     1] loss: 0.226
[86,     1] loss: 0.256
[87,     1] loss: 0.211
[88,     1] loss: 0.207
[89,     1] loss: 0.192
[90,     1] loss: 0.188
[91,     1] loss: 0.198
[92,     1] loss: 0.187
[93,     1] loss: 0.225
[94,     1] loss: 0.707
[95,     1] loss: 0.601
[96,     1] loss: 0.617
[97,     1] loss: 0.539
[98,     1] loss: 0.533
[99,     1] loss: 0.541
[100,     1] loss: 0.558
[101,     1] loss: 0.573
[102,     1] loss: 0.585
[103,     1] loss: 0.588
[104,     1] loss: 0.589
[105,     1] loss: 0.590
[106,     1] loss: 0.590
Early stopping applied (best metric=0.37998583912849426)
Finished Training
Total time taken: 143.19074749946594
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.663
[4,     1] loss: 0.649
[5,     1] loss: 0.633
[6,     1] loss: 0.618
[7,     1] loss: 0.598
[8,     1] loss: 0.576
[9,     1] loss: 0.552
[10,     1] loss: 0.529
[11,     1] loss: 0.502
[12,     1] loss: 0.478
[13,     1] loss: 0.452
[14,     1] loss: 0.430
[15,     1] loss: 0.414
[16,     1] loss: 0.394
[17,     1] loss: 0.377
[18,     1] loss: 0.358
[19,     1] loss: 0.343
[20,     1] loss: 0.325
[21,     1] loss: 0.310
[22,     1] loss: 0.294
[23,     1] loss: 0.278
[24,     1] loss: 0.261
[25,     1] loss: 0.245
[26,     1] loss: 0.230
[27,     1] loss: 0.216
[28,     1] loss: 0.205
[29,     1] loss: 0.193
[30,     1] loss: 0.183
[31,     1] loss: 0.175
[32,     1] loss: 0.169
[33,     1] loss: 0.161
[34,     1] loss: 0.154
[35,     1] loss: 0.152
[36,     1] loss: 0.148
[37,     1] loss: 0.144
[38,     1] loss: 0.138
[39,     1] loss: 0.132
[40,     1] loss: 0.123
[41,     1] loss: 0.227
[42,     1] loss: 0.845
[43,     1] loss: 0.671
[44,     1] loss: 0.545
[45,     1] loss: 0.494
[46,     1] loss: 0.508
[47,     1] loss: 0.518
[48,     1] loss: 0.528
[49,     1] loss: 0.533
[50,     1] loss: 0.529
[51,     1] loss: 0.519
[52,     1] loss: 0.511
[53,     1] loss: 0.498
[54,     1] loss: 0.486
[55,     1] loss: 0.472
[56,     1] loss: 0.457
[57,     1] loss: 0.445
[58,     1] loss: 0.435
[59,     1] loss: 0.423
[60,     1] loss: 0.412
[61,     1] loss: 0.403
[62,     1] loss: 0.402
[63,     1] loss: 0.397
[64,     1] loss: 0.404
[65,     1] loss: 0.393
[66,     1] loss: 0.392
[67,     1] loss: 0.388
[68,     1] loss: 0.391
[69,     1] loss: 0.393
[70,     1] loss: 0.388
[71,     1] loss: 0.384
[72,     1] loss: 0.397
[73,     1] loss: 0.390
[74,     1] loss: 0.391
[75,     1] loss: 0.391
[76,     1] loss: 0.385
[77,     1] loss: 0.381
[78,     1] loss: 0.388
[79,     1] loss: 0.394
[80,     1] loss: 0.394
[81,     1] loss: 0.368
[82,     1] loss: 0.409
[83,     1] loss: 0.384
[84,     1] loss: 0.449
[85,     1] loss: 0.396
[86,     1] loss: 0.422
[87,     1] loss: 0.399
[88,     1] loss: 0.414
Early stopping applied (best metric=0.37076127529144287)
Finished Training
Total time taken: 121.87960934638977
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.674
[3,     1] loss: 0.644
[4,     1] loss: 0.619
[5,     1] loss: 0.596
[6,     1] loss: 0.570
[7,     1] loss: 0.544
[8,     1] loss: 0.514
[9,     1] loss: 0.489
[10,     1] loss: 0.460
[11,     1] loss: 0.434
[12,     1] loss: 0.413
[13,     1] loss: 0.389
[14,     1] loss: 0.364
[15,     1] loss: 0.343
[16,     1] loss: 0.320
[17,     1] loss: 0.308
[18,     1] loss: 0.290
[19,     1] loss: 0.278
[20,     1] loss: 0.266
[21,     1] loss: 0.252
[22,     1] loss: 0.240
[23,     1] loss: 0.229
[24,     1] loss: 0.218
[25,     1] loss: 0.213
[26,     1] loss: 0.204
[27,     1] loss: 0.199
[28,     1] loss: 0.193
[29,     1] loss: 0.187
[30,     1] loss: 0.182
[31,     1] loss: 0.179
[32,     1] loss: 0.175
[33,     1] loss: 0.170
[34,     1] loss: 0.169
[35,     1] loss: 0.160
[36,     1] loss: 0.148
[37,     1] loss: 0.139
[38,     1] loss: 0.330
[39,     1] loss: 0.614
[40,     1] loss: 0.451
[41,     1] loss: 0.406
[42,     1] loss: 0.389
[43,     1] loss: 0.428
[44,     1] loss: 0.393
[45,     1] loss: 0.434
[46,     1] loss: 0.405
[47,     1] loss: 0.411
[48,     1] loss: 0.395
[49,     1] loss: 0.388
[50,     1] loss: 0.367
[51,     1] loss: 0.346
[52,     1] loss: 0.333
[53,     1] loss: 0.323
[54,     1] loss: 0.310
[55,     1] loss: 0.284
[56,     1] loss: 0.275
[57,     1] loss: 0.286
[58,     1] loss: 0.260
[59,     1] loss: 0.265
[60,     1] loss: 0.246
[61,     1] loss: 0.239
[62,     1] loss: 0.222
[63,     1] loss: 0.220
[64,     1] loss: 0.214
[65,     1] loss: 0.194
[66,     1] loss: 0.191
[67,     1] loss: 0.183
[68,     1] loss: 0.170
[69,     1] loss: 0.166
[70,     1] loss: 0.155
[71,     1] loss: 0.150
[72,     1] loss: 0.148
[73,     1] loss: 0.143
[74,     1] loss: 0.140
[75,     1] loss: 0.139
[76,     1] loss: 0.138
[77,     1] loss: 0.135
[78,     1] loss: 0.136
[79,     1] loss: 0.136
[80,     1] loss: 0.138
[81,     1] loss: 0.137
[82,     1] loss: 0.138
[83,     1] loss: 0.139
[84,     1] loss: 0.139
[85,     1] loss: 0.141
[86,     1] loss: 0.142
[87,     1] loss: 0.163
[88,     1] loss: 0.411
[89,     1] loss: 0.929
[90,     1] loss: 0.573
[91,     1] loss: 0.720
[92,     1] loss: 0.593
[93,     1] loss: 0.589
[94,     1] loss: 0.612
[95,     1] loss: 0.624
[96,     1] loss: 0.629
[97,     1] loss: 0.634
[98,     1] loss: 0.639
[99,     1] loss: 0.645
[100,     1] loss: 0.649
[101,     1] loss: 0.655
[102,     1] loss: 0.659
[103,     1] loss: 0.664
[104,     1] loss: 0.667
Early stopping applied (best metric=0.3961584270000458)
Finished Training
Total time taken: 141.57223415374756
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.657
[4,     1] loss: 0.641
[5,     1] loss: 0.625
[6,     1] loss: 0.605
[7,     1] loss: 0.588
[8,     1] loss: 0.567
[9,     1] loss: 0.546
[10,     1] loss: 0.525
[11,     1] loss: 0.502
[12,     1] loss: 0.473
[13,     1] loss: 0.445
[14,     1] loss: 0.416
[15,     1] loss: 0.388
[16,     1] loss: 0.363
[17,     1] loss: 0.339
[18,     1] loss: 0.318
[19,     1] loss: 0.303
[20,     1] loss: 0.291
[21,     1] loss: 0.266
[22,     1] loss: 0.251
[23,     1] loss: 0.240
[24,     1] loss: 0.228
[25,     1] loss: 0.216
[26,     1] loss: 0.207
[27,     1] loss: 0.198
[28,     1] loss: 0.188
[29,     1] loss: 0.179
[30,     1] loss: 0.172
[31,     1] loss: 0.166
[32,     1] loss: 0.160
[33,     1] loss: 0.153
[34,     1] loss: 0.152
[35,     1] loss: 0.196
[36,     1] loss: 0.340
[37,     1] loss: 0.519
[38,     1] loss: 0.309
[39,     1] loss: 0.362
[40,     1] loss: 0.381
[41,     1] loss: 0.384
[42,     1] loss: 0.393
[43,     1] loss: 0.392
[44,     1] loss: 0.381
[45,     1] loss: 0.367
[46,     1] loss: 0.352
[47,     1] loss: 0.334
[48,     1] loss: 0.319
[49,     1] loss: 0.302
[50,     1] loss: 0.284
[51,     1] loss: 0.269
[52,     1] loss: 0.262
[53,     1] loss: 0.244
[54,     1] loss: 0.229
[55,     1] loss: 0.221
[56,     1] loss: 0.209
[57,     1] loss: 0.196
[58,     1] loss: 0.188
[59,     1] loss: 0.191
[60,     1] loss: 0.306
[61,     1] loss: 0.234
[62,     1] loss: 0.254
[63,     1] loss: 0.242
[64,     1] loss: 0.239
[65,     1] loss: 0.229
[66,     1] loss: 0.218
[67,     1] loss: 0.212
[68,     1] loss: 0.202
[69,     1] loss: 0.195
[70,     1] loss: 0.187
[71,     1] loss: 0.181
[72,     1] loss: 0.176
[73,     1] loss: 0.209
[74,     1] loss: 0.241
[75,     1] loss: 0.278
[76,     1] loss: 0.530
[77,     1] loss: 0.427
[78,     1] loss: 0.425
[79,     1] loss: 0.406
[80,     1] loss: 0.388
[81,     1] loss: 0.375
[82,     1] loss: 0.369
[83,     1] loss: 0.354
[84,     1] loss: 0.344
[85,     1] loss: 0.331
[86,     1] loss: 0.316
[87,     1] loss: 0.306
[88,     1] loss: 0.293
[89,     1] loss: 0.292
[90,     1] loss: 0.275
[91,     1] loss: 0.269
[92,     1] loss: 0.272
[93,     1] loss: 0.250
[94,     1] loss: 0.262
[95,     1] loss: 0.268
[96,     1] loss: 0.257
[97,     1] loss: 0.249
[98,     1] loss: 0.237
[99,     1] loss: 0.228
[100,     1] loss: 0.218
[101,     1] loss: 0.210
[102,     1] loss: 0.216
[103,     1] loss: 0.200
[104,     1] loss: 0.215
[105,     1] loss: 0.287
[106,     1] loss: 0.286
Early stopping applied (best metric=0.34998029470443726)
Finished Training
Total time taken: 145.05573892593384
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.681
[3,     1] loss: 0.656
[4,     1] loss: 0.632
[5,     1] loss: 0.605
[6,     1] loss: 0.578
[7,     1] loss: 0.551
[8,     1] loss: 0.520
[9,     1] loss: 0.491
[10,     1] loss: 0.466
[11,     1] loss: 0.439
[12,     1] loss: 0.425
[13,     1] loss: 0.405
[14,     1] loss: 0.390
[15,     1] loss: 0.381
[16,     1] loss: 0.369
[17,     1] loss: 0.360
[18,     1] loss: 0.354
[19,     1] loss: 0.349
[20,     1] loss: 0.341
[21,     1] loss: 0.334
[22,     1] loss: 0.329
[23,     1] loss: 0.322
[24,     1] loss: 0.315
[25,     1] loss: 0.309
[26,     1] loss: 0.303
[27,     1] loss: 0.295
[28,     1] loss: 0.286
[29,     1] loss: 0.278
[30,     1] loss: 0.271
[31,     1] loss: 0.262
[32,     1] loss: 0.256
[33,     1] loss: 0.247
[34,     1] loss: 0.238
[35,     1] loss: 0.232
[36,     1] loss: 0.224
[37,     1] loss: 0.217
[38,     1] loss: 0.210
[39,     1] loss: 0.204
[40,     1] loss: 0.198
[41,     1] loss: 0.187
[42,     1] loss: 0.333
[43,     1] loss: 0.397
[44,     1] loss: 0.656
[45,     1] loss: 0.559
[46,     1] loss: 0.549
[47,     1] loss: 0.535
[48,     1] loss: 0.533
[49,     1] loss: 0.533
[50,     1] loss: 0.529
[51,     1] loss: 0.525
[52,     1] loss: 0.521
[53,     1] loss: 0.516
[54,     1] loss: 0.502
[55,     1] loss: 0.490
[56,     1] loss: 0.477
[57,     1] loss: 0.463
[58,     1] loss: 0.448
[59,     1] loss: 0.428
[60,     1] loss: 0.410
[61,     1] loss: 0.394
[62,     1] loss: 0.379
[63,     1] loss: 0.371
[64,     1] loss: 0.360
[65,     1] loss: 0.350
[66,     1] loss: 0.338
[67,     1] loss: 0.325
[68,     1] loss: 0.343
[69,     1] loss: 0.333
[70,     1] loss: 0.343
[71,     1] loss: 0.334
[72,     1] loss: 0.456
[73,     1] loss: 0.475
[74,     1] loss: 0.391
[75,     1] loss: 0.439
[76,     1] loss: 0.397
[77,     1] loss: 0.386
[78,     1] loss: 0.373
[79,     1] loss: 0.358
[80,     1] loss: 0.349
[81,     1] loss: 0.335
[82,     1] loss: 0.325
[83,     1] loss: 0.320
[84,     1] loss: 0.304
[85,     1] loss: 0.290
[86,     1] loss: 0.298
[87,     1] loss: 0.297
[88,     1] loss: 0.290
[89,     1] loss: 0.289
[90,     1] loss: 0.273
[91,     1] loss: 0.261
[92,     1] loss: 0.249
[93,     1] loss: 0.242
[94,     1] loss: 0.230
[95,     1] loss: 0.228
[96,     1] loss: 0.235
[97,     1] loss: 0.219
[98,     1] loss: 0.216
[99,     1] loss: 0.203
[100,     1] loss: 0.197
[101,     1] loss: 0.187
[102,     1] loss: 0.184
[103,     1] loss: 0.183
[104,     1] loss: 0.180
[105,     1] loss: 0.174
[106,     1] loss: 0.175
[107,     1] loss: 0.171
[108,     1] loss: 0.171
[109,     1] loss: 0.166
[110,     1] loss: 0.163
[111,     1] loss: 0.167
[112,     1] loss: 0.163
[113,     1] loss: 0.161
[114,     1] loss: 0.162
[115,     1] loss: 0.159
[116,     1] loss: 0.157
[117,     1] loss: 0.154
[118,     1] loss: 0.150
[119,     1] loss: 0.154
[120,     1] loss: 0.173
[121,     1] loss: 0.874
[122,     1] loss: 0.926
[123,     1] loss: 0.691
[124,     1] loss: 0.692
[125,     1] loss: 0.679
[126,     1] loss: 0.666
[127,     1] loss: 0.667
Early stopping applied (best metric=0.43245357275009155)
Finished Training
Total time taken: 174.61787271499634
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.672
[4,     1] loss: 0.655
[5,     1] loss: 0.639
[6,     1] loss: 0.621
[7,     1] loss: 0.601
[8,     1] loss: 0.582
[9,     1] loss: 0.557
[10,     1] loss: 0.532
[11,     1] loss: 0.505
[12,     1] loss: 0.477
[13,     1] loss: 0.452
[14,     1] loss: 0.427
[15,     1] loss: 0.403
[16,     1] loss: 0.379
[17,     1] loss: 0.353
[18,     1] loss: 0.329
[19,     1] loss: 0.308
[20,     1] loss: 0.291
[21,     1] loss: 0.273
[22,     1] loss: 0.260
[23,     1] loss: 0.246
[24,     1] loss: 0.234
[25,     1] loss: 0.224
[26,     1] loss: 0.214
[27,     1] loss: 0.206
[28,     1] loss: 0.199
[29,     1] loss: 0.194
[30,     1] loss: 0.191
[31,     1] loss: 0.185
[32,     1] loss: 0.179
[33,     1] loss: 0.174
[34,     1] loss: 0.191
[35,     1] loss: 0.239
[36,     1] loss: 0.412
[37,     1] loss: 0.344
[38,     1] loss: 0.320
[39,     1] loss: 0.312
[40,     1] loss: 0.310
[41,     1] loss: 0.290
[42,     1] loss: 0.286
[43,     1] loss: 0.282
[44,     1] loss: 0.270
[45,     1] loss: 0.256
[46,     1] loss: 0.244
[47,     1] loss: 0.231
[48,     1] loss: 0.220
[49,     1] loss: 0.211
[50,     1] loss: 0.218
[51,     1] loss: 0.205
[52,     1] loss: 0.204
[53,     1] loss: 0.191
[54,     1] loss: 0.188
[55,     1] loss: 0.183
[56,     1] loss: 0.177
[57,     1] loss: 0.168
[58,     1] loss: 0.165
[59,     1] loss: 0.163
[60,     1] loss: 0.157
[61,     1] loss: 0.152
[62,     1] loss: 0.148
[63,     1] loss: 0.147
[64,     1] loss: 0.146
[65,     1] loss: 0.146
[66,     1] loss: 0.143
[67,     1] loss: 0.138
[68,     1] loss: 0.139
[69,     1] loss: 0.140
[70,     1] loss: 0.139
[71,     1] loss: 0.138
[72,     1] loss: 0.139
[73,     1] loss: 0.137
[74,     1] loss: 0.141
[75,     1] loss: 0.182
[76,     1] loss: 0.397
[77,     1] loss: 0.793
[78,     1] loss: 0.611
[79,     1] loss: 0.560
[80,     1] loss: 0.582
[81,     1] loss: 0.572
[82,     1] loss: 0.564
[83,     1] loss: 0.570
[84,     1] loss: 0.579
[85,     1] loss: 0.586
[86,     1] loss: 0.590
[87,     1] loss: 0.594
[88,     1] loss: 0.598
[89,     1] loss: 0.598
Early stopping applied (best metric=0.37335696816444397)
Finished Training
Total time taken: 123.18210482597351
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.671
[4,     1] loss: 0.656
[5,     1] loss: 0.639
[6,     1] loss: 0.622
[7,     1] loss: 0.603
[8,     1] loss: 0.584
[9,     1] loss: 0.564
[10,     1] loss: 0.542
[11,     1] loss: 0.521
[12,     1] loss: 0.496
[13,     1] loss: 0.471
[14,     1] loss: 0.445
[15,     1] loss: 0.421
[16,     1] loss: 0.401
[17,     1] loss: 0.380
[18,     1] loss: 0.359
[19,     1] loss: 0.338
[20,     1] loss: 0.323
[21,     1] loss: 0.308
[22,     1] loss: 0.289
[23,     1] loss: 0.274
[24,     1] loss: 0.261
[25,     1] loss: 0.254
[26,     1] loss: 0.251
[27,     1] loss: 0.231
[28,     1] loss: 0.223
[29,     1] loss: 0.216
[30,     1] loss: 0.203
[31,     1] loss: 0.195
[32,     1] loss: 0.188
[33,     1] loss: 0.186
[34,     1] loss: 0.214
[35,     1] loss: 0.236
[36,     1] loss: 0.321
[37,     1] loss: 0.330
[38,     1] loss: 0.279
[39,     1] loss: 0.270
[40,     1] loss: 0.270
[41,     1] loss: 0.275
[42,     1] loss: 0.268
[43,     1] loss: 0.260
[44,     1] loss: 0.247
[45,     1] loss: 0.251
[46,     1] loss: 0.279
[47,     1] loss: 0.269
[48,     1] loss: 0.260
[49,     1] loss: 0.240
[50,     1] loss: 0.239
[51,     1] loss: 0.227
[52,     1] loss: 0.214
[53,     1] loss: 0.206
[54,     1] loss: 0.204
[55,     1] loss: 0.227
[56,     1] loss: 0.194
[57,     1] loss: 0.211
[58,     1] loss: 0.246
[59,     1] loss: 0.248
[60,     1] loss: 0.241
[61,     1] loss: 0.228
[62,     1] loss: 0.227
[63,     1] loss: 0.225
[64,     1] loss: 0.215
[65,     1] loss: 0.206
[66,     1] loss: 0.197
[67,     1] loss: 0.198
[68,     1] loss: 0.195
[69,     1] loss: 0.195
[70,     1] loss: 0.179
[71,     1] loss: 0.179
[72,     1] loss: 0.177
[73,     1] loss: 0.170
[74,     1] loss: 0.170
[75,     1] loss: 0.164
[76,     1] loss: 0.162
[77,     1] loss: 0.157
[78,     1] loss: 0.158
[79,     1] loss: 0.157
[80,     1] loss: 0.156
[81,     1] loss: 0.154
[82,     1] loss: 0.155
[83,     1] loss: 0.154
[84,     1] loss: 0.154
[85,     1] loss: 0.153
[86,     1] loss: 0.154
[87,     1] loss: 0.152
[88,     1] loss: 0.154
[89,     1] loss: 0.153
[90,     1] loss: 0.153
[91,     1] loss: 0.156
[92,     1] loss: 0.197
[93,     1] loss: 0.313
[94,     1] loss: 0.781
[95,     1] loss: 0.680
Early stopping applied (best metric=0.34761732816696167)
Finished Training
Total time taken: 131.93303155899048
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.669
[4,     1] loss: 0.656
[5,     1] loss: 0.644
[6,     1] loss: 0.627
[7,     1] loss: 0.610
[8,     1] loss: 0.594
[9,     1] loss: 0.574
[10,     1] loss: 0.551
[11,     1] loss: 0.529
[12,     1] loss: 0.506
[13,     1] loss: 0.485
[14,     1] loss: 0.461
[15,     1] loss: 0.439
[16,     1] loss: 0.415
[17,     1] loss: 0.393
[18,     1] loss: 0.372
[19,     1] loss: 0.348
[20,     1] loss: 0.328
[21,     1] loss: 0.308
[22,     1] loss: 0.290
[23,     1] loss: 0.271
[24,     1] loss: 0.255
[25,     1] loss: 0.243
[26,     1] loss: 0.227
[27,     1] loss: 0.214
[28,     1] loss: 0.203
[29,     1] loss: 0.194
[30,     1] loss: 0.184
[31,     1] loss: 0.177
[32,     1] loss: 0.179
[33,     1] loss: 0.176
[34,     1] loss: 0.168
[35,     1] loss: 0.162
[36,     1] loss: 0.153
[37,     1] loss: 0.219
[38,     1] loss: 0.384
[39,     1] loss: 0.590
[40,     1] loss: 0.414
[41,     1] loss: 0.447
[42,     1] loss: 0.457
[43,     1] loss: 0.463
[44,     1] loss: 0.480
[45,     1] loss: 0.493
[46,     1] loss: 0.492
[47,     1] loss: 0.487
[48,     1] loss: 0.478
[49,     1] loss: 0.469
[50,     1] loss: 0.455
[51,     1] loss: 0.445
[52,     1] loss: 0.432
[53,     1] loss: 0.419
[54,     1] loss: 0.406
[55,     1] loss: 0.389
[56,     1] loss: 0.376
[57,     1] loss: 0.360
[58,     1] loss: 0.343
[59,     1] loss: 0.329
[60,     1] loss: 0.315
[61,     1] loss: 0.295
[62,     1] loss: 0.281
[63,     1] loss: 0.267
[64,     1] loss: 0.254
[65,     1] loss: 0.241
[66,     1] loss: 0.283
[67,     1] loss: 0.262
[68,     1] loss: 0.251
[69,     1] loss: 0.252
[70,     1] loss: 0.329
[71,     1] loss: 0.366
[72,     1] loss: 0.377
[73,     1] loss: 0.350
[74,     1] loss: 0.316
[75,     1] loss: 0.297
[76,     1] loss: 0.296
[77,     1] loss: 0.293
[78,     1] loss: 0.277
[79,     1] loss: 0.271
[80,     1] loss: 0.255
[81,     1] loss: 0.253
[82,     1] loss: 0.266
[83,     1] loss: 0.242
[84,     1] loss: 0.233
[85,     1] loss: 0.228
[86,     1] loss: 0.212
[87,     1] loss: 0.206
[88,     1] loss: 0.196
[89,     1] loss: 0.192
[90,     1] loss: 0.186
[91,     1] loss: 0.182
[92,     1] loss: 0.176
[93,     1] loss: 0.173
[94,     1] loss: 0.171
[95,     1] loss: 0.169
[96,     1] loss: 0.168
[97,     1] loss: 0.169
[98,     1] loss: 0.163
[99,     1] loss: 0.164
[100,     1] loss: 0.164
[101,     1] loss: 0.162
[102,     1] loss: 0.160
[103,     1] loss: 0.159
[104,     1] loss: 0.162
[105,     1] loss: 0.161
[106,     1] loss: 0.170
[107,     1] loss: 0.381
[108,     1] loss: 0.444
[109,     1] loss: 0.524
[110,     1] loss: 0.425
[111,     1] loss: 0.420
[112,     1] loss: 0.414
[113,     1] loss: 0.366
[114,     1] loss: 0.384
[115,     1] loss: 0.372
[116,     1] loss: 0.369
[117,     1] loss: 0.348
[118,     1] loss: 0.345
[119,     1] loss: 0.332
[120,     1] loss: 0.323
[121,     1] loss: 0.313
[122,     1] loss: 0.301
[123,     1] loss: 0.294
[124,     1] loss: 0.283
[125,     1] loss: 0.273
[126,     1] loss: 0.262
[127,     1] loss: 0.250
[128,     1] loss: 0.239
[129,     1] loss: 0.231
[130,     1] loss: 0.221
[131,     1] loss: 0.214
[132,     1] loss: 0.205
[133,     1] loss: 0.203
[134,     1] loss: 0.224
[135,     1] loss: 0.318
[136,     1] loss: 0.336
[137,     1] loss: 0.353
[138,     1] loss: 0.324
[139,     1] loss: 0.301
[140,     1] loss: 0.271
[141,     1] loss: 0.275
[142,     1] loss: 0.261
[143,     1] loss: 0.259
[144,     1] loss: 0.236
[145,     1] loss: 0.224
[146,     1] loss: 0.218
[147,     1] loss: 0.211
[148,     1] loss: 0.210
[149,     1] loss: 0.205
[150,     1] loss: 0.196
[151,     1] loss: 0.202
[152,     1] loss: 0.246
[153,     1] loss: 0.263
[154,     1] loss: 0.251
[155,     1] loss: 0.259
[156,     1] loss: 0.272
[157,     1] loss: 0.246
[158,     1] loss: 0.229
[159,     1] loss: 0.217
[160,     1] loss: 0.211
[161,     1] loss: 0.203
[162,     1] loss: 0.204
[163,     1] loss: 0.222
[164,     1] loss: 0.203
[165,     1] loss: 0.204
[166,     1] loss: 0.199
[167,     1] loss: 0.198
[168,     1] loss: 0.199
[169,     1] loss: 0.196
[170,     1] loss: 0.194
[171,     1] loss: 0.195
[172,     1] loss: 0.192
[173,     1] loss: 0.195
[174,     1] loss: 0.195
[175,     1] loss: 0.204
[176,     1] loss: 0.234
[177,     1] loss: 0.287
[178,     1] loss: 0.330
[179,     1] loss: 0.286
[180,     1] loss: 0.254
[181,     1] loss: 0.255
[182,     1] loss: 0.228
[183,     1] loss: 0.234
[184,     1] loss: 0.237
[185,     1] loss: 0.218
[186,     1] loss: 0.224
[187,     1] loss: 0.230
[188,     1] loss: 0.219
[189,     1] loss: 0.222
[190,     1] loss: 0.220
[191,     1] loss: 0.220
[192,     1] loss: 0.216
[193,     1] loss: 0.212
[194,     1] loss: 0.209
[195,     1] loss: 0.205
[196,     1] loss: 0.205
[197,     1] loss: 0.201
[198,     1] loss: 0.198
[199,     1] loss: 0.198
[200,     1] loss: 0.200
Finished Training
Total time taken: 276.9608964920044
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.667
[4,     1] loss: 0.649
[5,     1] loss: 0.632
[6,     1] loss: 0.614
[7,     1] loss: 0.592
[8,     1] loss: 0.572
[9,     1] loss: 0.551
[10,     1] loss: 0.525
[11,     1] loss: 0.501
[12,     1] loss: 0.469
[13,     1] loss: 0.440
[14,     1] loss: 0.414
[15,     1] loss: 0.382
[16,     1] loss: 0.359
[17,     1] loss: 0.336
[18,     1] loss: 0.317
[19,     1] loss: 0.299
[20,     1] loss: 0.277
[21,     1] loss: 0.264
[22,     1] loss: 0.249
[23,     1] loss: 0.237
[24,     1] loss: 0.221
[25,     1] loss: 0.211
[26,     1] loss: 0.203
[27,     1] loss: 0.197
[28,     1] loss: 0.186
[29,     1] loss: 0.182
[30,     1] loss: 0.176
[31,     1] loss: 0.175
[32,     1] loss: 0.168
[33,     1] loss: 0.165
[34,     1] loss: 0.163
[35,     1] loss: 0.178
[36,     1] loss: 0.357
[37,     1] loss: 0.439
[38,     1] loss: 0.362
[39,     1] loss: 0.409
[40,     1] loss: 0.403
[41,     1] loss: 0.396
[42,     1] loss: 0.397
[43,     1] loss: 0.401
[44,     1] loss: 0.387
[45,     1] loss: 0.377
[46,     1] loss: 0.363
[47,     1] loss: 0.345
[48,     1] loss: 0.331
[49,     1] loss: 0.315
[50,     1] loss: 0.300
[51,     1] loss: 0.288
[52,     1] loss: 0.271
[53,     1] loss: 0.261
[54,     1] loss: 0.250
[55,     1] loss: 0.239
[56,     1] loss: 0.228
[57,     1] loss: 0.219
[58,     1] loss: 0.212
[59,     1] loss: 0.208
[60,     1] loss: 0.197
[61,     1] loss: 0.192
[62,     1] loss: 0.190
[63,     1] loss: 0.187
[64,     1] loss: 0.187
[65,     1] loss: 0.189
[66,     1] loss: 0.181
[67,     1] loss: 0.180
[68,     1] loss: 0.178
[69,     1] loss: 0.180
[70,     1] loss: 0.201
[71,     1] loss: 0.218
[72,     1] loss: 0.325
[73,     1] loss: 0.441
[74,     1] loss: 0.375
[75,     1] loss: 0.422
[76,     1] loss: 0.443
[77,     1] loss: 0.416
[78,     1] loss: 0.412
[79,     1] loss: 0.397
[80,     1] loss: 0.384
[81,     1] loss: 0.373
[82,     1] loss: 0.364
[83,     1] loss: 0.351
[84,     1] loss: 0.342
[85,     1] loss: 0.332
[86,     1] loss: 0.322
[87,     1] loss: 0.313
[88,     1] loss: 0.303
[89,     1] loss: 0.290
[90,     1] loss: 0.279
[91,     1] loss: 0.266
[92,     1] loss: 0.258
[93,     1] loss: 0.245
[94,     1] loss: 0.238
[95,     1] loss: 0.230
[96,     1] loss: 0.222
[97,     1] loss: 0.215
[98,     1] loss: 0.212
[99,     1] loss: 0.207
[100,     1] loss: 0.226
[101,     1] loss: 0.324
Early stopping applied (best metric=0.4225420355796814)
Finished Training
Total time taken: 141.8112874031067
{'Hydroxylation-P Validation Accuracy': 0.7819739404091163, 'Hydroxylation-P Validation Sensitivity': 0.7316190476190476, 'Hydroxylation-P Validation Specificity': 0.792748765524465, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.8332118583280487, 'Hydroxylation-P AUC PR': 0.6024070939262033, 'Hydroxylation-P MCC': 0.4453046497381917, 'Hydroxylation-P F1': 0.53923589489788, 'Validation Loss (Hydroxylation-P)': 0.4007943344116211, 'Validation Loss (total)': 0.4007943344116211}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003957230032443041,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2360289056,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.167857560437673}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.687
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009308032481730007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2725474183,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.760071920076375}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.634
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007797841369797245,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 637750386,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.726017433414505}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.670
[3,     1] loss: 0.627
[4,     1] loss: 0.594
[5,     1] loss: 0.559
[6,     1] loss: 0.524
[7,     1] loss: 0.491
[8,     1] loss: 0.459
[9,     1] loss: 0.459
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003117482302533887,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1443672397,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.801111684536913}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019437413546123691,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2112833458,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.43413602112896}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.693
[3,     1] loss: 0.678
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005061916576937227,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2596564904,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.0700728634635235}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.689
[3,     1] loss: 0.664
[4,     1] loss: 0.639
[5,     1] loss: 0.607
[6,     1] loss: 0.564
[7,     1] loss: 0.516
[8,     1] loss: 0.463
[9,     1] loss: 0.408
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008513878565275389,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2005873013,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.45417960057843}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.698
[3,     1] loss: 0.690
[4,     1] loss: 0.683
[5,     1] loss: 0.674
[6,     1] loss: 0.668
[7,     1] loss: 0.660
[8,     1] loss: 0.653
[9,     1] loss: 0.647
[10,     1] loss: 0.641
[11,     1] loss: 0.635
[12,     1] loss: 0.628
[13,     1] loss: 0.622
[14,     1] loss: 0.614
[15,     1] loss: 0.607
[16,     1] loss: 0.602
[17,     1] loss: 0.593
[18,     1] loss: 0.586
[19,     1] loss: 0.579
[20,     1] loss: 0.571
[21,     1] loss: 0.561
[22,     1] loss: 0.554
[23,     1] loss: 0.546
[24,     1] loss: 0.539
[25,     1] loss: 0.528
[26,     1] loss: 0.521
[27,     1] loss: 0.509
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018668823922322391,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 295007302,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.656660005216809}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.679
[3,     1] loss: 0.656
[4,     1] loss: 0.638
[5,     1] loss: 0.618
[6,     1] loss: 0.598
[7,     1] loss: 0.581
[8,     1] loss: 0.562
[9,     1] loss: 0.539
[10,     1] loss: 0.519
[11,     1] loss: 0.490
[12,     1] loss: 0.468
[13,     1] loss: 0.442
[14,     1] loss: 0.417
[15,     1] loss: 0.393
[16,     1] loss: 0.367
[17,     1] loss: 0.341
[18,     1] loss: 0.317
[19,     1] loss: 0.296
[20,     1] loss: 0.276
[21,     1] loss: 0.255
[22,     1] loss: 0.233
[23,     1] loss: 0.215
[24,     1] loss: 0.197
[25,     1] loss: 0.183
[26,     1] loss: 0.170
[27,     1] loss: 0.156
[28,     1] loss: 0.148
[29,     1] loss: 0.139
[30,     1] loss: 0.127
[31,     1] loss: 0.121
[32,     1] loss: 0.115
[33,     1] loss: 0.110
[34,     1] loss: 0.103
[35,     1] loss: 0.100
[36,     1] loss: 0.096
[37,     1] loss: 0.093
[38,     1] loss: 0.088
[39,     1] loss: 0.087
[40,     1] loss: 0.085
[41,     1] loss: 0.083
[42,     1] loss: 0.081
[43,     1] loss: 0.081
[44,     1] loss: 0.080
[45,     1] loss: 0.078
[46,     1] loss: 0.077
[47,     1] loss: 0.077
[48,     1] loss: 0.077
[49,     1] loss: 0.076
[50,     1] loss: 0.076
[51,     1] loss: 0.075
[52,     1] loss: 0.075
[53,     1] loss: 0.074
[54,     1] loss: 0.075
[55,     1] loss: 0.075
[56,     1] loss: 0.073
[57,     1] loss: 0.074
[58,     1] loss: 0.073
[59,     1] loss: 0.074
[60,     1] loss: 0.073
[61,     1] loss: 0.072
[62,     1] loss: 0.072
[63,     1] loss: 0.071
[64,     1] loss: 0.073
[65,     1] loss: 0.070
Early stopping applied (best metric=0.41927531361579895)
Finished Training
Total time taken: 92.27942967414856
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.657
[4,     1] loss: 0.640
[5,     1] loss: 0.621
[6,     1] loss: 0.603
[7,     1] loss: 0.583
[8,     1] loss: 0.558
[9,     1] loss: 0.537
[10,     1] loss: 0.513
[11,     1] loss: 0.489
[12,     1] loss: 0.465
[13,     1] loss: 0.443
[14,     1] loss: 0.422
[15,     1] loss: 0.402
[16,     1] loss: 0.385
[17,     1] loss: 0.369
[18,     1] loss: 0.352
[19,     1] loss: 0.338
[20,     1] loss: 0.322
[21,     1] loss: 0.307
[22,     1] loss: 0.291
[23,     1] loss: 0.276
[24,     1] loss: 0.263
[25,     1] loss: 0.249
[26,     1] loss: 0.233
[27,     1] loss: 0.220
[28,     1] loss: 0.206
[29,     1] loss: 0.194
[30,     1] loss: 0.179
[31,     1] loss: 0.171
[32,     1] loss: 0.155
[33,     1] loss: 0.150
[34,     1] loss: 0.139
[35,     1] loss: 0.130
[36,     1] loss: 0.121
[37,     1] loss: 0.113
[38,     1] loss: 0.106
[39,     1] loss: 0.102
[40,     1] loss: 0.096
[41,     1] loss: 0.090
[42,     1] loss: 0.087
[43,     1] loss: 0.084
[44,     1] loss: 0.081
[45,     1] loss: 0.079
[46,     1] loss: 0.075
[47,     1] loss: 0.074
[48,     1] loss: 0.072
[49,     1] loss: 0.071
[50,     1] loss: 0.069
[51,     1] loss: 0.070
[52,     1] loss: 0.067
[53,     1] loss: 0.065
[54,     1] loss: 0.065
[55,     1] loss: 0.065
[56,     1] loss: 0.064
[57,     1] loss: 0.062
[58,     1] loss: 0.063
[59,     1] loss: 0.061
[60,     1] loss: 0.063
[61,     1] loss: 0.060
[62,     1] loss: 0.061
[63,     1] loss: 0.061
[64,     1] loss: 0.061
[65,     1] loss: 0.059
[66,     1] loss: 0.059
[67,     1] loss: 0.059
[68,     1] loss: 0.059
[69,     1] loss: 0.058
[70,     1] loss: 0.058
[71,     1] loss: 0.058
[72,     1] loss: 0.058
[73,     1] loss: 0.057
[74,     1] loss: 0.057
[75,     1] loss: 0.057
[76,     1] loss: 0.057
[77,     1] loss: 0.058
[78,     1] loss: 0.055
[79,     1] loss: 0.055
[80,     1] loss: 0.055
[81,     1] loss: 0.054
[82,     1] loss: 0.054
[83,     1] loss: 0.054
[84,     1] loss: 0.053
[85,     1] loss: 0.053
[86,     1] loss: 0.052
[87,     1] loss: 0.053
Early stopping applied (best metric=0.4346734285354614)
Finished Training
Total time taken: 123.36357617378235
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.661
[4,     1] loss: 0.649
[5,     1] loss: 0.635
[6,     1] loss: 0.622
[7,     1] loss: 0.610
[8,     1] loss: 0.593
[9,     1] loss: 0.578
[10,     1] loss: 0.559
[11,     1] loss: 0.539
[12,     1] loss: 0.521
[13,     1] loss: 0.494
[14,     1] loss: 0.471
[15,     1] loss: 0.450
[16,     1] loss: 0.420
[17,     1] loss: 0.395
[18,     1] loss: 0.365
[19,     1] loss: 0.341
[20,     1] loss: 0.312
[21,     1] loss: 0.286
[22,     1] loss: 0.265
[23,     1] loss: 0.244
[24,     1] loss: 0.218
[25,     1] loss: 0.202
[26,     1] loss: 0.184
[27,     1] loss: 0.168
[28,     1] loss: 0.152
[29,     1] loss: 0.138
[30,     1] loss: 0.124
[31,     1] loss: 0.113
[32,     1] loss: 0.100
[33,     1] loss: 0.093
[34,     1] loss: 0.086
[35,     1] loss: 0.079
[36,     1] loss: 0.075
[37,     1] loss: 0.067
[38,     1] loss: 0.064
[39,     1] loss: 0.060
[40,     1] loss: 0.056
[41,     1] loss: 0.054
[42,     1] loss: 0.051
[43,     1] loss: 0.050
[44,     1] loss: 0.049
[45,     1] loss: 0.048
[46,     1] loss: 0.047
[47,     1] loss: 0.046
[48,     1] loss: 0.046
[49,     1] loss: 0.045
[50,     1] loss: 0.045
[51,     1] loss: 0.045
[52,     1] loss: 0.045
[53,     1] loss: 0.045
[54,     1] loss: 0.045
[55,     1] loss: 0.044
[56,     1] loss: 0.046
[57,     1] loss: 0.045
[58,     1] loss: 0.045
[59,     1] loss: 0.045
[60,     1] loss: 0.045
[61,     1] loss: 0.045
[62,     1] loss: 0.047
[63,     1] loss: 0.046
[64,     1] loss: 0.045
[65,     1] loss: 0.045
[66,     1] loss: 0.045
[67,     1] loss: 0.044
[68,     1] loss: 0.044
[69,     1] loss: 0.045
[70,     1] loss: 0.043
[71,     1] loss: 0.044
[72,     1] loss: 0.043
[73,     1] loss: 0.042
Early stopping applied (best metric=0.4175858199596405)
Finished Training
Total time taken: 104.12246012687683
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.655
[4,     1] loss: 0.636
[5,     1] loss: 0.614
[6,     1] loss: 0.592
[7,     1] loss: 0.564
[8,     1] loss: 0.543
[9,     1] loss: 0.516
[10,     1] loss: 0.486
[11,     1] loss: 0.460
[12,     1] loss: 0.434
[13,     1] loss: 0.405
[14,     1] loss: 0.381
[15,     1] loss: 0.360
[16,     1] loss: 0.336
[17,     1] loss: 0.318
[18,     1] loss: 0.303
[19,     1] loss: 0.291
[20,     1] loss: 0.276
[21,     1] loss: 0.262
[22,     1] loss: 0.250
[23,     1] loss: 0.235
[24,     1] loss: 0.219
[25,     1] loss: 0.205
[26,     1] loss: 0.189
[27,     1] loss: 0.172
[28,     1] loss: 0.159
[29,     1] loss: 0.144
[30,     1] loss: 0.131
[31,     1] loss: 0.122
[32,     1] loss: 0.113
[33,     1] loss: 0.102
[34,     1] loss: 0.093
[35,     1] loss: 0.088
[36,     1] loss: 0.082
[37,     1] loss: 0.075
[38,     1] loss: 0.068
[39,     1] loss: 0.064
[40,     1] loss: 0.060
[41,     1] loss: 0.057
[42,     1] loss: 0.054
[43,     1] loss: 0.052
[44,     1] loss: 0.049
[45,     1] loss: 0.049
[46,     1] loss: 0.048
[47,     1] loss: 0.046
[48,     1] loss: 0.045
[49,     1] loss: 0.043
[50,     1] loss: 0.043
[51,     1] loss: 0.043
[52,     1] loss: 0.043
[53,     1] loss: 0.043
[54,     1] loss: 0.042
[55,     1] loss: 0.043
[56,     1] loss: 0.042
[57,     1] loss: 0.041
[58,     1] loss: 0.042
[59,     1] loss: 0.042
[60,     1] loss: 0.041
[61,     1] loss: 0.042
[62,     1] loss: 0.042
Early stopping applied (best metric=0.44965970516204834)
Finished Training
Total time taken: 89.26840496063232
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.669
[3,     1] loss: 0.643
[4,     1] loss: 0.624
[5,     1] loss: 0.600
[6,     1] loss: 0.577
[7,     1] loss: 0.550
[8,     1] loss: 0.528
[9,     1] loss: 0.500
[10,     1] loss: 0.472
[11,     1] loss: 0.440
[12,     1] loss: 0.414
[13,     1] loss: 0.386
[14,     1] loss: 0.366
[15,     1] loss: 0.342
[16,     1] loss: 0.323
[17,     1] loss: 0.309
[18,     1] loss: 0.294
[19,     1] loss: 0.280
[20,     1] loss: 0.264
[21,     1] loss: 0.248
[22,     1] loss: 0.234
[23,     1] loss: 0.220
[24,     1] loss: 0.207
[25,     1] loss: 0.192
[26,     1] loss: 0.179
[27,     1] loss: 0.167
[28,     1] loss: 0.157
[29,     1] loss: 0.147
[30,     1] loss: 0.138
[31,     1] loss: 0.130
[32,     1] loss: 0.121
[33,     1] loss: 0.116
[34,     1] loss: 0.112
[35,     1] loss: 0.106
[36,     1] loss: 0.101
[37,     1] loss: 0.098
[38,     1] loss: 0.093
[39,     1] loss: 0.090
[40,     1] loss: 0.088
[41,     1] loss: 0.086
[42,     1] loss: 0.082
[43,     1] loss: 0.081
[44,     1] loss: 0.078
[45,     1] loss: 0.075
[46,     1] loss: 0.075
[47,     1] loss: 0.072
[48,     1] loss: 0.070
[49,     1] loss: 0.069
[50,     1] loss: 0.069
[51,     1] loss: 0.069
[52,     1] loss: 0.066
[53,     1] loss: 0.066
[54,     1] loss: 0.065
[55,     1] loss: 0.064
[56,     1] loss: 0.065
[57,     1] loss: 0.063
[58,     1] loss: 0.063
[59,     1] loss: 0.062
[60,     1] loss: 0.061
[61,     1] loss: 0.062
Early stopping applied (best metric=0.44339603185653687)
Finished Training
Total time taken: 87.7648515701294
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.670
[3,     1] loss: 0.653
[4,     1] loss: 0.636
[5,     1] loss: 0.623
[6,     1] loss: 0.605
[7,     1] loss: 0.589
[8,     1] loss: 0.569
[9,     1] loss: 0.553
[10,     1] loss: 0.534
[11,     1] loss: 0.515
[12,     1] loss: 0.494
[13,     1] loss: 0.471
[14,     1] loss: 0.454
[15,     1] loss: 0.437
[16,     1] loss: 0.425
[17,     1] loss: 0.411
[18,     1] loss: 0.400
[19,     1] loss: 0.388
[20,     1] loss: 0.377
[21,     1] loss: 0.369
[22,     1] loss: 0.363
[23,     1] loss: 0.357
[24,     1] loss: 0.353
[25,     1] loss: 0.349
[26,     1] loss: 0.345
[27,     1] loss: 0.341
[28,     1] loss: 0.336
[29,     1] loss: 0.333
[30,     1] loss: 0.328
[31,     1] loss: 0.324
[32,     1] loss: 0.319
[33,     1] loss: 0.312
[34,     1] loss: 0.307
[35,     1] loss: 0.299
[36,     1] loss: 0.288
[37,     1] loss: 0.283
[38,     1] loss: 0.273
[39,     1] loss: 0.266
[40,     1] loss: 0.259
[41,     1] loss: 0.248
[42,     1] loss: 0.238
[43,     1] loss: 0.228
[44,     1] loss: 0.215
[45,     1] loss: 0.204
[46,     1] loss: 0.190
[47,     1] loss: 0.177
[48,     1] loss: 0.164
[49,     1] loss: 0.153
[50,     1] loss: 0.139
[51,     1] loss: 0.127
[52,     1] loss: 0.116
[53,     1] loss: 0.108
[54,     1] loss: 0.097
[55,     1] loss: 0.087
[56,     1] loss: 0.080
[57,     1] loss: 0.070
[58,     1] loss: 0.064
[59,     1] loss: 0.057
[60,     1] loss: 0.053
Early stopping applied (best metric=0.47840172052383423)
Finished Training
Total time taken: 86.45223569869995
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.671
[4,     1] loss: 0.660
[5,     1] loss: 0.649
[6,     1] loss: 0.636
[7,     1] loss: 0.621
[8,     1] loss: 0.605
[9,     1] loss: 0.590
[10,     1] loss: 0.571
[11,     1] loss: 0.551
[12,     1] loss: 0.531
[13,     1] loss: 0.508
[14,     1] loss: 0.487
[15,     1] loss: 0.461
[16,     1] loss: 0.435
[17,     1] loss: 0.410
[18,     1] loss: 0.383
[19,     1] loss: 0.356
[20,     1] loss: 0.332
[21,     1] loss: 0.303
[22,     1] loss: 0.283
[23,     1] loss: 0.254
[24,     1] loss: 0.231
[25,     1] loss: 0.211
[26,     1] loss: 0.192
[27,     1] loss: 0.175
[28,     1] loss: 0.157
[29,     1] loss: 0.142
[30,     1] loss: 0.129
[31,     1] loss: 0.117
[32,     1] loss: 0.107
[33,     1] loss: 0.094
[34,     1] loss: 0.089
[35,     1] loss: 0.081
[36,     1] loss: 0.075
[37,     1] loss: 0.068
[38,     1] loss: 0.063
[39,     1] loss: 0.059
[40,     1] loss: 0.057
[41,     1] loss: 0.053
[42,     1] loss: 0.051
[43,     1] loss: 0.048
[44,     1] loss: 0.046
[45,     1] loss: 0.044
[46,     1] loss: 0.042
[47,     1] loss: 0.040
[48,     1] loss: 0.039
[49,     1] loss: 0.039
[50,     1] loss: 0.038
[51,     1] loss: 0.038
[52,     1] loss: 0.038
[53,     1] loss: 0.038
[54,     1] loss: 0.037
[55,     1] loss: 0.038
[56,     1] loss: 0.037
[57,     1] loss: 0.037
[58,     1] loss: 0.037
[59,     1] loss: 0.036
[60,     1] loss: 0.035
[61,     1] loss: 0.037
[62,     1] loss: 0.036
[63,     1] loss: 0.035
[64,     1] loss: 0.036
[65,     1] loss: 0.035
[66,     1] loss: 0.035
[67,     1] loss: 0.035
[68,     1] loss: 0.035
[69,     1] loss: 0.033
[70,     1] loss: 0.034
[71,     1] loss: 0.033
[72,     1] loss: 0.033
Early stopping applied (best metric=0.3911537528038025)
Finished Training
Total time taken: 104.01845574378967
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.657
[5,     1] loss: 0.645
[6,     1] loss: 0.630
[7,     1] loss: 0.613
[8,     1] loss: 0.593
[9,     1] loss: 0.575
[10,     1] loss: 0.558
[11,     1] loss: 0.530
[12,     1] loss: 0.507
[13,     1] loss: 0.480
[14,     1] loss: 0.454
[15,     1] loss: 0.421
[16,     1] loss: 0.389
[17,     1] loss: 0.360
[18,     1] loss: 0.328
[19,     1] loss: 0.297
[20,     1] loss: 0.264
[21,     1] loss: 0.236
[22,     1] loss: 0.211
[23,     1] loss: 0.193
[24,     1] loss: 0.167
[25,     1] loss: 0.146
[26,     1] loss: 0.130
[27,     1] loss: 0.112
[28,     1] loss: 0.100
[29,     1] loss: 0.092
[30,     1] loss: 0.080
[31,     1] loss: 0.071
[32,     1] loss: 0.064
[33,     1] loss: 0.058
[34,     1] loss: 0.054
[35,     1] loss: 0.050
[36,     1] loss: 0.048
[37,     1] loss: 0.043
[38,     1] loss: 0.043
[39,     1] loss: 0.040
[40,     1] loss: 0.040
[41,     1] loss: 0.039
[42,     1] loss: 0.037
[43,     1] loss: 0.037
[44,     1] loss: 0.036
[45,     1] loss: 0.035
[46,     1] loss: 0.036
[47,     1] loss: 0.036
[48,     1] loss: 0.036
[49,     1] loss: 0.037
[50,     1] loss: 0.037
[51,     1] loss: 0.039
[52,     1] loss: 0.038
[53,     1] loss: 0.038
[54,     1] loss: 0.037
[55,     1] loss: 0.039
[56,     1] loss: 0.039
[57,     1] loss: 0.038
[58,     1] loss: 0.040
[59,     1] loss: 0.040
[60,     1] loss: 0.042
[61,     1] loss: 0.040
[62,     1] loss: 0.041
[63,     1] loss: 0.041
[64,     1] loss: 0.042
[65,     1] loss: 0.041
[66,     1] loss: 0.040
[67,     1] loss: 0.039
[68,     1] loss: 0.041
[69,     1] loss: 0.039
Early stopping applied (best metric=0.37972185015678406)
Finished Training
Total time taken: 100.02746081352234
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.680
[3,     1] loss: 0.659
[4,     1] loss: 0.640
[5,     1] loss: 0.621
[6,     1] loss: 0.604
[7,     1] loss: 0.583
[8,     1] loss: 0.562
[9,     1] loss: 0.541
[10,     1] loss: 0.516
[11,     1] loss: 0.493
[12,     1] loss: 0.466
[13,     1] loss: 0.442
[14,     1] loss: 0.420
[15,     1] loss: 0.396
[16,     1] loss: 0.371
[17,     1] loss: 0.349
[18,     1] loss: 0.326
[19,     1] loss: 0.303
[20,     1] loss: 0.287
[21,     1] loss: 0.267
[22,     1] loss: 0.247
[23,     1] loss: 0.230
[24,     1] loss: 0.212
[25,     1] loss: 0.195
[26,     1] loss: 0.182
[27,     1] loss: 0.163
[28,     1] loss: 0.149
[29,     1] loss: 0.138
[30,     1] loss: 0.127
[31,     1] loss: 0.112
[32,     1] loss: 0.105
[33,     1] loss: 0.095
[34,     1] loss: 0.087
[35,     1] loss: 0.081
[36,     1] loss: 0.074
[37,     1] loss: 0.070
[38,     1] loss: 0.065
[39,     1] loss: 0.062
[40,     1] loss: 0.058
[41,     1] loss: 0.056
[42,     1] loss: 0.053
[43,     1] loss: 0.052
[44,     1] loss: 0.050
[45,     1] loss: 0.047
[46,     1] loss: 0.047
[47,     1] loss: 0.047
[48,     1] loss: 0.043
[49,     1] loss: 0.043
[50,     1] loss: 0.043
[51,     1] loss: 0.043
[52,     1] loss: 0.043
[53,     1] loss: 0.043
[54,     1] loss: 0.043
[55,     1] loss: 0.043
[56,     1] loss: 0.043
[57,     1] loss: 0.042
[58,     1] loss: 0.043
[59,     1] loss: 0.043
[60,     1] loss: 0.043
[61,     1] loss: 0.045
[62,     1] loss: 0.044
[63,     1] loss: 0.042
[64,     1] loss: 0.044
[65,     1] loss: 0.044
[66,     1] loss: 0.045
[67,     1] loss: 0.044
Early stopping applied (best metric=0.4217142164707184)
Finished Training
Total time taken: 97.4384503364563
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.672
[3,     1] loss: 0.652
[4,     1] loss: 0.637
[5,     1] loss: 0.623
[6,     1] loss: 0.607
[7,     1] loss: 0.593
[8,     1] loss: 0.572
[9,     1] loss: 0.552
[10,     1] loss: 0.532
[11,     1] loss: 0.508
[12,     1] loss: 0.483
[13,     1] loss: 0.458
[14,     1] loss: 0.431
[15,     1] loss: 0.404
[16,     1] loss: 0.376
[17,     1] loss: 0.347
[18,     1] loss: 0.315
[19,     1] loss: 0.288
[20,     1] loss: 0.264
[21,     1] loss: 0.235
[22,     1] loss: 0.210
[23,     1] loss: 0.188
[24,     1] loss: 0.168
[25,     1] loss: 0.151
[26,     1] loss: 0.135
[27,     1] loss: 0.121
[28,     1] loss: 0.109
[29,     1] loss: 0.097
[30,     1] loss: 0.088
[31,     1] loss: 0.078
[32,     1] loss: 0.071
[33,     1] loss: 0.066
[34,     1] loss: 0.060
[35,     1] loss: 0.058
[36,     1] loss: 0.055
[37,     1] loss: 0.050
[38,     1] loss: 0.048
[39,     1] loss: 0.046
[40,     1] loss: 0.046
[41,     1] loss: 0.043
[42,     1] loss: 0.042
[43,     1] loss: 0.040
[44,     1] loss: 0.039
[45,     1] loss: 0.040
[46,     1] loss: 0.039
[47,     1] loss: 0.038
[48,     1] loss: 0.038
[49,     1] loss: 0.038
[50,     1] loss: 0.039
[51,     1] loss: 0.038
[52,     1] loss: 0.038
[53,     1] loss: 0.040
[54,     1] loss: 0.039
[55,     1] loss: 0.039
[56,     1] loss: 0.039
[57,     1] loss: 0.040
[58,     1] loss: 0.040
[59,     1] loss: 0.038
[60,     1] loss: 0.041
[61,     1] loss: 0.040
[62,     1] loss: 0.041
[63,     1] loss: 0.039
[64,     1] loss: 0.039
[65,     1] loss: 0.040
[66,     1] loss: 0.039
[67,     1] loss: 0.040
[68,     1] loss: 0.038
Early stopping applied (best metric=0.3590698540210724)
Finished Training
Total time taken: 98.99144625663757
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.667
[3,     1] loss: 0.638
[4,     1] loss: 0.619
[5,     1] loss: 0.598
[6,     1] loss: 0.579
[7,     1] loss: 0.555
[8,     1] loss: 0.530
[9,     1] loss: 0.502
[10,     1] loss: 0.477
[11,     1] loss: 0.451
[12,     1] loss: 0.423
[13,     1] loss: 0.395
[14,     1] loss: 0.366
[15,     1] loss: 0.341
[16,     1] loss: 0.318
[17,     1] loss: 0.296
[18,     1] loss: 0.274
[19,     1] loss: 0.251
[20,     1] loss: 0.232
[21,     1] loss: 0.212
[22,     1] loss: 0.195
[23,     1] loss: 0.180
[24,     1] loss: 0.169
[25,     1] loss: 0.154
[26,     1] loss: 0.140
[27,     1] loss: 0.130
[28,     1] loss: 0.120
[29,     1] loss: 0.111
[30,     1] loss: 0.102
[31,     1] loss: 0.097
[32,     1] loss: 0.089
[33,     1] loss: 0.082
[34,     1] loss: 0.076
[35,     1] loss: 0.072
[36,     1] loss: 0.069
[37,     1] loss: 0.064
[38,     1] loss: 0.061
[39,     1] loss: 0.059
[40,     1] loss: 0.056
[41,     1] loss: 0.055
[42,     1] loss: 0.054
[43,     1] loss: 0.052
[44,     1] loss: 0.051
[45,     1] loss: 0.049
[46,     1] loss: 0.049
[47,     1] loss: 0.048
[48,     1] loss: 0.048
[49,     1] loss: 0.048
[50,     1] loss: 0.047
[51,     1] loss: 0.047
[52,     1] loss: 0.046
[53,     1] loss: 0.046
[54,     1] loss: 0.046
[55,     1] loss: 0.046
[56,     1] loss: 0.046
[57,     1] loss: 0.047
[58,     1] loss: 0.047
[59,     1] loss: 0.047
[60,     1] loss: 0.045
[61,     1] loss: 0.045
[62,     1] loss: 0.044
[63,     1] loss: 0.045
Early stopping applied (best metric=0.442340612411499)
Finished Training
Total time taken: 92.50440621376038
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.660
[4,     1] loss: 0.642
[5,     1] loss: 0.626
[6,     1] loss: 0.607
[7,     1] loss: 0.588
[8,     1] loss: 0.569
[9,     1] loss: 0.544
[10,     1] loss: 0.520
[11,     1] loss: 0.500
[12,     1] loss: 0.473
[13,     1] loss: 0.446
[14,     1] loss: 0.419
[15,     1] loss: 0.390
[16,     1] loss: 0.363
[17,     1] loss: 0.335
[18,     1] loss: 0.305
[19,     1] loss: 0.275
[20,     1] loss: 0.247
[21,     1] loss: 0.225
[22,     1] loss: 0.205
[23,     1] loss: 0.179
[24,     1] loss: 0.161
[25,     1] loss: 0.144
[26,     1] loss: 0.128
[27,     1] loss: 0.115
[28,     1] loss: 0.100
[29,     1] loss: 0.091
[30,     1] loss: 0.082
[31,     1] loss: 0.072
[32,     1] loss: 0.065
[33,     1] loss: 0.058
[34,     1] loss: 0.054
[35,     1] loss: 0.050
[36,     1] loss: 0.047
[37,     1] loss: 0.044
[38,     1] loss: 0.042
[39,     1] loss: 0.041
[40,     1] loss: 0.039
[41,     1] loss: 0.038
[42,     1] loss: 0.038
[43,     1] loss: 0.036
[44,     1] loss: 0.036
[45,     1] loss: 0.037
[46,     1] loss: 0.035
[47,     1] loss: 0.036
[48,     1] loss: 0.036
[49,     1] loss: 0.038
[50,     1] loss: 0.037
[51,     1] loss: 0.038
[52,     1] loss: 0.038
[53,     1] loss: 0.039
[54,     1] loss: 0.039
[55,     1] loss: 0.039
[56,     1] loss: 0.039
[57,     1] loss: 0.042
[58,     1] loss: 0.041
[59,     1] loss: 0.042
[60,     1] loss: 0.042
[61,     1] loss: 0.040
[62,     1] loss: 0.041
[63,     1] loss: 0.042
[64,     1] loss: 0.042
[65,     1] loss: 0.041
[66,     1] loss: 0.043
[67,     1] loss: 0.041
[68,     1] loss: 0.041
[69,     1] loss: 0.042
[70,     1] loss: 0.041
Early stopping applied (best metric=0.3937380611896515)
Finished Training
Total time taken: 102.72047448158264
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.659
[4,     1] loss: 0.644
[5,     1] loss: 0.628
[6,     1] loss: 0.610
[7,     1] loss: 0.592
[8,     1] loss: 0.571
[9,     1] loss: 0.551
[10,     1] loss: 0.525
[11,     1] loss: 0.501
[12,     1] loss: 0.475
[13,     1] loss: 0.444
[14,     1] loss: 0.415
[15,     1] loss: 0.387
[16,     1] loss: 0.360
[17,     1] loss: 0.328
[18,     1] loss: 0.300
[19,     1] loss: 0.271
[20,     1] loss: 0.249
[21,     1] loss: 0.229
[22,     1] loss: 0.207
[23,     1] loss: 0.188
[24,     1] loss: 0.173
[25,     1] loss: 0.160
[26,     1] loss: 0.149
[27,     1] loss: 0.135
[28,     1] loss: 0.125
[29,     1] loss: 0.114
[30,     1] loss: 0.104
[31,     1] loss: 0.097
[32,     1] loss: 0.092
[33,     1] loss: 0.083
[34,     1] loss: 0.080
[35,     1] loss: 0.076
[36,     1] loss: 0.071
[37,     1] loss: 0.070
[38,     1] loss: 0.065
[39,     1] loss: 0.064
[40,     1] loss: 0.063
[41,     1] loss: 0.061
[42,     1] loss: 0.059
[43,     1] loss: 0.058
[44,     1] loss: 0.056
[45,     1] loss: 0.055
[46,     1] loss: 0.056
[47,     1] loss: 0.056
[48,     1] loss: 0.056
[49,     1] loss: 0.056
[50,     1] loss: 0.054
[51,     1] loss: 0.054
[52,     1] loss: 0.053
[53,     1] loss: 0.054
[54,     1] loss: 0.055
[55,     1] loss: 0.056
[56,     1] loss: 0.055
[57,     1] loss: 0.054
[58,     1] loss: 0.055
[59,     1] loss: 0.056
[60,     1] loss: 0.055
[61,     1] loss: 0.055
[62,     1] loss: 0.056
[63,     1] loss: 0.054
[64,     1] loss: 0.055
Early stopping applied (best metric=0.4643017053604126)
Finished Training
Total time taken: 94.1830313205719
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.659
[5,     1] loss: 0.647
[6,     1] loss: 0.634
[7,     1] loss: 0.621
[8,     1] loss: 0.607
[9,     1] loss: 0.592
[10,     1] loss: 0.576
[11,     1] loss: 0.557
[12,     1] loss: 0.539
[13,     1] loss: 0.523
[14,     1] loss: 0.503
[15,     1] loss: 0.483
[16,     1] loss: 0.460
[17,     1] loss: 0.441
[18,     1] loss: 0.418
[19,     1] loss: 0.395
[20,     1] loss: 0.371
[21,     1] loss: 0.351
[22,     1] loss: 0.328
[23,     1] loss: 0.307
[24,     1] loss: 0.282
[25,     1] loss: 0.260
[26,     1] loss: 0.238
[27,     1] loss: 0.218
[28,     1] loss: 0.201
[29,     1] loss: 0.181
[30,     1] loss: 0.168
[31,     1] loss: 0.151
[32,     1] loss: 0.137
[33,     1] loss: 0.127
[34,     1] loss: 0.117
[35,     1] loss: 0.107
[36,     1] loss: 0.099
[37,     1] loss: 0.092
[38,     1] loss: 0.087
[39,     1] loss: 0.082
[40,     1] loss: 0.076
[41,     1] loss: 0.075
[42,     1] loss: 0.072
[43,     1] loss: 0.068
[44,     1] loss: 0.067
[45,     1] loss: 0.066
[46,     1] loss: 0.063
[47,     1] loss: 0.062
[48,     1] loss: 0.062
[49,     1] loss: 0.060
[50,     1] loss: 0.059
[51,     1] loss: 0.059
[52,     1] loss: 0.058
[53,     1] loss: 0.058
[54,     1] loss: 0.058
[55,     1] loss: 0.058
[56,     1] loss: 0.058
[57,     1] loss: 0.058
[58,     1] loss: 0.059
[59,     1] loss: 0.059
[60,     1] loss: 0.058
[61,     1] loss: 0.057
[62,     1] loss: 0.059
[63,     1] loss: 0.060
[64,     1] loss: 0.059
[65,     1] loss: 0.058
Early stopping applied (best metric=0.4460168778896332)
Finished Training
Total time taken: 95.95442223548889
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.670
[4,     1] loss: 0.658
[5,     1] loss: 0.647
[6,     1] loss: 0.632
[7,     1] loss: 0.618
[8,     1] loss: 0.605
[9,     1] loss: 0.591
[10,     1] loss: 0.572
[11,     1] loss: 0.553
[12,     1] loss: 0.531
[13,     1] loss: 0.511
[14,     1] loss: 0.490
[15,     1] loss: 0.466
[16,     1] loss: 0.443
[17,     1] loss: 0.418
[18,     1] loss: 0.394
[19,     1] loss: 0.367
[20,     1] loss: 0.341
[21,     1] loss: 0.317
[22,     1] loss: 0.289
[23,     1] loss: 0.263
[24,     1] loss: 0.241
[25,     1] loss: 0.216
[26,     1] loss: 0.197
[27,     1] loss: 0.179
[28,     1] loss: 0.159
[29,     1] loss: 0.144
[30,     1] loss: 0.132
[31,     1] loss: 0.121
[32,     1] loss: 0.111
[33,     1] loss: 0.102
[34,     1] loss: 0.094
[35,     1] loss: 0.089
[36,     1] loss: 0.081
[37,     1] loss: 0.077
[38,     1] loss: 0.071
[39,     1] loss: 0.068
[40,     1] loss: 0.065
[41,     1] loss: 0.063
[42,     1] loss: 0.061
[43,     1] loss: 0.059
[44,     1] loss: 0.057
[45,     1] loss: 0.055
[46,     1] loss: 0.054
[47,     1] loss: 0.053
[48,     1] loss: 0.052
[49,     1] loss: 0.051
[50,     1] loss: 0.052
[51,     1] loss: 0.053
[52,     1] loss: 0.051
[53,     1] loss: 0.050
[54,     1] loss: 0.050
[55,     1] loss: 0.051
[56,     1] loss: 0.051
[57,     1] loss: 0.050
[58,     1] loss: 0.051
[59,     1] loss: 0.052
[60,     1] loss: 0.052
[61,     1] loss: 0.052
[62,     1] loss: 0.053
[63,     1] loss: 0.051
[64,     1] loss: 0.052
[65,     1] loss: 0.050
[66,     1] loss: 0.051
[67,     1] loss: 0.051
[68,     1] loss: 0.050
[69,     1] loss: 0.050
[70,     1] loss: 0.050
[71,     1] loss: 0.049
[72,     1] loss: 0.049
Early stopping applied (best metric=0.4095698297023773)
Finished Training
Total time taken: 106.3952043056488
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.675
[3,     1] loss: 0.653
[4,     1] loss: 0.633
[5,     1] loss: 0.611
[6,     1] loss: 0.591
[7,     1] loss: 0.572
[8,     1] loss: 0.551
[9,     1] loss: 0.529
[10,     1] loss: 0.507
[11,     1] loss: 0.483
[12,     1] loss: 0.460
[13,     1] loss: 0.439
[14,     1] loss: 0.420
[15,     1] loss: 0.400
[16,     1] loss: 0.381
[17,     1] loss: 0.367
[18,     1] loss: 0.349
[19,     1] loss: 0.335
[20,     1] loss: 0.322
[21,     1] loss: 0.306
[22,     1] loss: 0.293
[23,     1] loss: 0.280
[24,     1] loss: 0.265
[25,     1] loss: 0.253
[26,     1] loss: 0.238
[27,     1] loss: 0.222
[28,     1] loss: 0.214
[29,     1] loss: 0.201
[30,     1] loss: 0.190
[31,     1] loss: 0.180
[32,     1] loss: 0.167
[33,     1] loss: 0.157
[34,     1] loss: 0.149
[35,     1] loss: 0.136
[36,     1] loss: 0.129
[37,     1] loss: 0.118
[38,     1] loss: 0.121
[39,     1] loss: 0.110
[40,     1] loss: 0.106
[41,     1] loss: 0.100
[42,     1] loss: 0.098
[43,     1] loss: 0.092
[44,     1] loss: 0.088
[45,     1] loss: 0.085
[46,     1] loss: 0.085
[47,     1] loss: 0.081
[48,     1] loss: 0.080
[49,     1] loss: 0.076
[50,     1] loss: 0.072
[51,     1] loss: 0.071
[52,     1] loss: 0.069
[53,     1] loss: 0.067
[54,     1] loss: 0.067
[55,     1] loss: 0.066
[56,     1] loss: 0.066
[57,     1] loss: 0.064
[58,     1] loss: 0.063
[59,     1] loss: 0.063
[60,     1] loss: 0.064
[61,     1] loss: 0.060
[62,     1] loss: 0.062
Early stopping applied (best metric=0.4769725799560547)
Finished Training
Total time taken: 92.36141729354858
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.713
[2,     1] loss: 0.691
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.648
[6,     1] loss: 0.632
[7,     1] loss: 0.617
[8,     1] loss: 0.601
[9,     1] loss: 0.586
[10,     1] loss: 0.565
[11,     1] loss: 0.546
[12,     1] loss: 0.531
[13,     1] loss: 0.506
[14,     1] loss: 0.490
[15,     1] loss: 0.467
[16,     1] loss: 0.444
[17,     1] loss: 0.417
[18,     1] loss: 0.393
[19,     1] loss: 0.366
[20,     1] loss: 0.337
[21,     1] loss: 0.310
[22,     1] loss: 0.283
[23,     1] loss: 0.258
[24,     1] loss: 0.236
[25,     1] loss: 0.217
[26,     1] loss: 0.197
[27,     1] loss: 0.183
[28,     1] loss: 0.169
[29,     1] loss: 0.153
[30,     1] loss: 0.141
[31,     1] loss: 0.131
[32,     1] loss: 0.121
[33,     1] loss: 0.113
[34,     1] loss: 0.104
[35,     1] loss: 0.096
[36,     1] loss: 0.091
[37,     1] loss: 0.083
[38,     1] loss: 0.079
[39,     1] loss: 0.073
[40,     1] loss: 0.069
[41,     1] loss: 0.066
[42,     1] loss: 0.062
[43,     1] loss: 0.061
[44,     1] loss: 0.057
[45,     1] loss: 0.057
[46,     1] loss: 0.056
[47,     1] loss: 0.053
[48,     1] loss: 0.051
[49,     1] loss: 0.051
[50,     1] loss: 0.050
[51,     1] loss: 0.049
[52,     1] loss: 0.048
[53,     1] loss: 0.050
[54,     1] loss: 0.048
[55,     1] loss: 0.047
[56,     1] loss: 0.048
[57,     1] loss: 0.047
[58,     1] loss: 0.048
[59,     1] loss: 0.047
[60,     1] loss: 0.047
[61,     1] loss: 0.046
[62,     1] loss: 0.045
[63,     1] loss: 0.045
[64,     1] loss: 0.045
[65,     1] loss: 0.044
[66,     1] loss: 0.045
[67,     1] loss: 0.044
[68,     1] loss: 0.044
[69,     1] loss: 0.043
[70,     1] loss: 0.042
[71,     1] loss: 0.041
[72,     1] loss: 0.042
[73,     1] loss: 0.041
[74,     1] loss: 0.039
[75,     1] loss: 0.039
Early stopping applied (best metric=0.39283645153045654)
Finished Training
Total time taken: 111.48813319206238
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.665
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.615
[7,     1] loss: 0.600
[8,     1] loss: 0.580
[9,     1] loss: 0.562
[10,     1] loss: 0.543
[11,     1] loss: 0.519
[12,     1] loss: 0.495
[13,     1] loss: 0.472
[14,     1] loss: 0.445
[15,     1] loss: 0.423
[16,     1] loss: 0.394
[17,     1] loss: 0.368
[18,     1] loss: 0.348
[19,     1] loss: 0.332
[20,     1] loss: 0.301
[21,     1] loss: 0.284
[22,     1] loss: 0.256
[23,     1] loss: 0.235
[24,     1] loss: 0.215
[25,     1] loss: 0.195
[26,     1] loss: 0.179
[27,     1] loss: 0.162
[28,     1] loss: 0.146
[29,     1] loss: 0.133
[30,     1] loss: 0.123
[31,     1] loss: 0.109
[32,     1] loss: 0.103
[33,     1] loss: 0.093
[34,     1] loss: 0.086
[35,     1] loss: 0.079
[36,     1] loss: 0.076
[37,     1] loss: 0.070
[38,     1] loss: 0.066
[39,     1] loss: 0.060
[40,     1] loss: 0.059
[41,     1] loss: 0.054
[42,     1] loss: 0.053
[43,     1] loss: 0.051
[44,     1] loss: 0.049
[45,     1] loss: 0.049
[46,     1] loss: 0.047
[47,     1] loss: 0.046
[48,     1] loss: 0.047
[49,     1] loss: 0.045
[50,     1] loss: 0.045
[51,     1] loss: 0.045
[52,     1] loss: 0.044
[53,     1] loss: 0.043
[54,     1] loss: 0.044
[55,     1] loss: 0.044
[56,     1] loss: 0.045
[57,     1] loss: 0.044
[58,     1] loss: 0.045
[59,     1] loss: 0.045
[60,     1] loss: 0.044
[61,     1] loss: 0.045
[62,     1] loss: 0.045
[63,     1] loss: 0.044
[64,     1] loss: 0.044
[65,     1] loss: 0.045
[66,     1] loss: 0.044
[67,     1] loss: 0.044
[68,     1] loss: 0.042
[69,     1] loss: 0.044
[70,     1] loss: 0.043
[71,     1] loss: 0.042
[72,     1] loss: 0.044
Early stopping applied (best metric=0.40410318970680237)
Finished Training
Total time taken: 107.47248578071594
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.625
[7,     1] loss: 0.611
[8,     1] loss: 0.593
[9,     1] loss: 0.572
[10,     1] loss: 0.552
[11,     1] loss: 0.531
[12,     1] loss: 0.503
[13,     1] loss: 0.481
[14,     1] loss: 0.454
[15,     1] loss: 0.430
[16,     1] loss: 0.407
[17,     1] loss: 0.382
[18,     1] loss: 0.359
[19,     1] loss: 0.338
[20,     1] loss: 0.317
[21,     1] loss: 0.296
[22,     1] loss: 0.276
[23,     1] loss: 0.263
[24,     1] loss: 0.242
[25,     1] loss: 0.232
[26,     1] loss: 0.211
[27,     1] loss: 0.194
[28,     1] loss: 0.183
[29,     1] loss: 0.167
[30,     1] loss: 0.157
[31,     1] loss: 0.143
[32,     1] loss: 0.133
[33,     1] loss: 0.126
[34,     1] loss: 0.117
[35,     1] loss: 0.108
[36,     1] loss: 0.103
[37,     1] loss: 0.096
[38,     1] loss: 0.091
[39,     1] loss: 0.086
[40,     1] loss: 0.081
[41,     1] loss: 0.079
[42,     1] loss: 0.075
[43,     1] loss: 0.071
[44,     1] loss: 0.071
[45,     1] loss: 0.068
[46,     1] loss: 0.066
[47,     1] loss: 0.064
[48,     1] loss: 0.063
[49,     1] loss: 0.063
[50,     1] loss: 0.060
[51,     1] loss: 0.060
[52,     1] loss: 0.059
[53,     1] loss: 0.058
[54,     1] loss: 0.058
[55,     1] loss: 0.058
[56,     1] loss: 0.056
[57,     1] loss: 0.057
[58,     1] loss: 0.057
[59,     1] loss: 0.055
[60,     1] loss: 0.056
[61,     1] loss: 0.055
[62,     1] loss: 0.055
[63,     1] loss: 0.056
[64,     1] loss: 0.055
Early stopping applied (best metric=0.4471082091331482)
Finished Training
Total time taken: 96.24942541122437
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.671
[4,     1] loss: 0.659
[5,     1] loss: 0.649
[6,     1] loss: 0.636
[7,     1] loss: 0.621
[8,     1] loss: 0.608
[9,     1] loss: 0.589
[10,     1] loss: 0.571
[11,     1] loss: 0.552
[12,     1] loss: 0.528
[13,     1] loss: 0.504
[14,     1] loss: 0.482
[15,     1] loss: 0.456
[16,     1] loss: 0.432
[17,     1] loss: 0.414
[18,     1] loss: 0.396
[19,     1] loss: 0.381
[20,     1] loss: 0.368
[21,     1] loss: 0.358
[22,     1] loss: 0.348
[23,     1] loss: 0.339
[24,     1] loss: 0.332
[25,     1] loss: 0.323
[26,     1] loss: 0.315
[27,     1] loss: 0.308
[28,     1] loss: 0.299
[29,     1] loss: 0.293
[30,     1] loss: 0.283
[31,     1] loss: 0.276
[32,     1] loss: 0.268
[33,     1] loss: 0.260
[34,     1] loss: 0.252
[35,     1] loss: 0.245
[36,     1] loss: 0.236
[37,     1] loss: 0.231
[38,     1] loss: 0.224
[39,     1] loss: 0.214
[40,     1] loss: 0.207
[41,     1] loss: 0.198
[42,     1] loss: 0.193
[43,     1] loss: 0.187
[44,     1] loss: 0.179
[45,     1] loss: 0.173
[46,     1] loss: 0.168
[47,     1] loss: 0.164
[48,     1] loss: 0.158
[49,     1] loss: 0.155
[50,     1] loss: 0.149
[51,     1] loss: 0.145
[52,     1] loss: 0.143
[53,     1] loss: 0.137
[54,     1] loss: 0.133
[55,     1] loss: 0.130
[56,     1] loss: 0.128
[57,     1] loss: 0.126
[58,     1] loss: 0.121
[59,     1] loss: 0.121
[60,     1] loss: 0.115
[61,     1] loss: 0.114
[62,     1] loss: 0.114
[63,     1] loss: 0.111
[64,     1] loss: 0.110
Early stopping applied (best metric=0.45007458329200745)
Finished Training
Total time taken: 96.32841324806213
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.651
[4,     1] loss: 0.625
[5,     1] loss: 0.601
[6,     1] loss: 0.579
[7,     1] loss: 0.555
[8,     1] loss: 0.527
[9,     1] loss: 0.504
[10,     1] loss: 0.477
[11,     1] loss: 0.452
[12,     1] loss: 0.427
[13,     1] loss: 0.408
[14,     1] loss: 0.390
[15,     1] loss: 0.374
[16,     1] loss: 0.365
[17,     1] loss: 0.355
[18,     1] loss: 0.349
[19,     1] loss: 0.343
[20,     1] loss: 0.339
[21,     1] loss: 0.335
[22,     1] loss: 0.332
[23,     1] loss: 0.329
[24,     1] loss: 0.326
[25,     1] loss: 0.324
[26,     1] loss: 0.320
[27,     1] loss: 0.318
[28,     1] loss: 0.314
[29,     1] loss: 0.310
[30,     1] loss: 0.307
[31,     1] loss: 0.303
[32,     1] loss: 0.298
[33,     1] loss: 0.295
[34,     1] loss: 0.291
[35,     1] loss: 0.284
[36,     1] loss: 0.277
[37,     1] loss: 0.272
[38,     1] loss: 0.265
[39,     1] loss: 0.260
[40,     1] loss: 0.251
[41,     1] loss: 0.246
[42,     1] loss: 0.236
[43,     1] loss: 0.229
[44,     1] loss: 0.220
[45,     1] loss: 0.214
[46,     1] loss: 0.206
[47,     1] loss: 0.200
[48,     1] loss: 0.190
[49,     1] loss: 0.185
[50,     1] loss: 0.178
[51,     1] loss: 0.174
[52,     1] loss: 0.163
[53,     1] loss: 0.155
[54,     1] loss: 0.153
[55,     1] loss: 0.144
[56,     1] loss: 0.139
[57,     1] loss: 0.134
[58,     1] loss: 0.131
[59,     1] loss: 0.127
[60,     1] loss: 0.121
Early stopping applied (best metric=0.47194066643714905)
Finished Training
Total time taken: 90.55243253707886
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.657
[4,     1] loss: 0.641
[5,     1] loss: 0.625
[6,     1] loss: 0.608
[7,     1] loss: 0.588
[8,     1] loss: 0.568
[9,     1] loss: 0.539
[10,     1] loss: 0.521
[11,     1] loss: 0.498
[12,     1] loss: 0.476
[13,     1] loss: 0.458
[14,     1] loss: 0.435
[15,     1] loss: 0.419
[16,     1] loss: 0.403
[17,     1] loss: 0.392
[18,     1] loss: 0.382
[19,     1] loss: 0.374
[20,     1] loss: 0.366
[21,     1] loss: 0.360
[22,     1] loss: 0.354
[23,     1] loss: 0.348
[24,     1] loss: 0.343
[25,     1] loss: 0.338
[26,     1] loss: 0.330
[27,     1] loss: 0.324
[28,     1] loss: 0.319
[29,     1] loss: 0.313
[30,     1] loss: 0.307
[31,     1] loss: 0.299
[32,     1] loss: 0.293
[33,     1] loss: 0.288
[34,     1] loss: 0.281
[35,     1] loss: 0.274
[36,     1] loss: 0.267
[37,     1] loss: 0.258
[38,     1] loss: 0.251
[39,     1] loss: 0.243
[40,     1] loss: 0.237
[41,     1] loss: 0.229
[42,     1] loss: 0.221
[43,     1] loss: 0.215
[44,     1] loss: 0.208
[45,     1] loss: 0.203
[46,     1] loss: 0.195
[47,     1] loss: 0.189
[48,     1] loss: 0.181
[49,     1] loss: 0.177
[50,     1] loss: 0.170
[51,     1] loss: 0.164
[52,     1] loss: 0.159
[53,     1] loss: 0.155
[54,     1] loss: 0.149
[55,     1] loss: 0.144
[56,     1] loss: 0.141
[57,     1] loss: 0.139
[58,     1] loss: 0.134
[59,     1] loss: 0.131
[60,     1] loss: 0.130
[61,     1] loss: 0.126
[62,     1] loss: 0.124
[63,     1] loss: 0.123
[64,     1] loss: 0.120
[65,     1] loss: 0.119
[66,     1] loss: 0.118
[67,     1] loss: 0.116
[68,     1] loss: 0.114
[69,     1] loss: 0.114
[70,     1] loss: 0.115
[71,     1] loss: 0.112
[72,     1] loss: 0.111
[73,     1] loss: 0.111
[74,     1] loss: 0.109
[75,     1] loss: 0.110
[76,     1] loss: 0.110
[77,     1] loss: 0.110
[78,     1] loss: 0.110
[79,     1] loss: 0.109
[80,     1] loss: 0.111
[81,     1] loss: 0.109
[82,     1] loss: 0.108
[83,     1] loss: 0.110
[84,     1] loss: 0.109
[85,     1] loss: 0.108
[86,     1] loss: 0.109
[87,     1] loss: 0.109
[88,     1] loss: 0.107
[89,     1] loss: 0.109
[90,     1] loss: 0.107
[91,     1] loss: 0.108
[92,     1] loss: 0.108
[93,     1] loss: 0.108
[94,     1] loss: 0.108
[95,     1] loss: 0.107
[96,     1] loss: 0.107
[97,     1] loss: 0.106
[98,     1] loss: 0.104
[99,     1] loss: 0.093
[100,     1] loss: 0.075
[101,     1] loss: 0.316
[102,     1] loss: 0.717
[103,     1] loss: 0.329
[104,     1] loss: 0.298
[105,     1] loss: 0.244
[106,     1] loss: 0.265
[107,     1] loss: 0.244
[108,     1] loss: 0.232
[109,     1] loss: 0.223
[110,     1] loss: 0.215
[111,     1] loss: 0.203
[112,     1] loss: 0.191
[113,     1] loss: 0.180
[114,     1] loss: 0.168
[115,     1] loss: 0.157
[116,     1] loss: 0.151
[117,     1] loss: 0.143
[118,     1] loss: 0.135
[119,     1] loss: 0.125
[120,     1] loss: 0.120
[121,     1] loss: 0.113
[122,     1] loss: 0.103
[123,     1] loss: 0.097
[124,     1] loss: 0.088
[125,     1] loss: 0.081
[126,     1] loss: 0.073
[127,     1] loss: 0.066
[128,     1] loss: 0.063
[129,     1] loss: 0.063
[130,     1] loss: 0.062
[131,     1] loss: 0.060
[132,     1] loss: 0.057
[133,     1] loss: 0.055
[134,     1] loss: 0.054
[135,     1] loss: 0.056
[136,     1] loss: 0.054
[137,     1] loss: 0.053
[138,     1] loss: 0.052
[139,     1] loss: 0.053
[140,     1] loss: 0.053
[141,     1] loss: 0.052
[142,     1] loss: 0.052
[143,     1] loss: 0.054
[144,     1] loss: 0.053
[145,     1] loss: 0.054
[146,     1] loss: 0.054
[147,     1] loss: 0.053
[148,     1] loss: 0.052
[149,     1] loss: 0.052
[150,     1] loss: 0.054
[151,     1] loss: 0.053
[152,     1] loss: 0.054
[153,     1] loss: 0.053
[154,     1] loss: 0.053
[155,     1] loss: 0.053
[156,     1] loss: 0.053
[157,     1] loss: 0.054
[158,     1] loss: 0.053
[159,     1] loss: 0.054
[160,     1] loss: 0.054
[161,     1] loss: 0.053
[162,     1] loss: 0.053
[163,     1] loss: 0.053
[164,     1] loss: 0.054
[165,     1] loss: 0.053
[166,     1] loss: 0.053
[167,     1] loss: 0.053
[168,     1] loss: 0.053
[169,     1] loss: 0.053
[170,     1] loss: 0.053
[171,     1] loss: 0.052
[172,     1] loss: 0.053
[173,     1] loss: 0.053
[174,     1] loss: 0.054
[175,     1] loss: 0.053
[176,     1] loss: 0.054
[177,     1] loss: 0.054
[178,     1] loss: 0.053
[179,     1] loss: 0.053
Early stopping applied (best metric=0.3077443838119507)
Finished Training
Total time taken: 269.84120440483093
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.668
[4,     1] loss: 0.657
[5,     1] loss: 0.643
[6,     1] loss: 0.628
[7,     1] loss: 0.610
[8,     1] loss: 0.590
[9,     1] loss: 0.570
[10,     1] loss: 0.544
[11,     1] loss: 0.524
[12,     1] loss: 0.495
[13,     1] loss: 0.467
[14,     1] loss: 0.441
[15,     1] loss: 0.411
[16,     1] loss: 0.385
[17,     1] loss: 0.358
[18,     1] loss: 0.333
[19,     1] loss: 0.313
[20,     1] loss: 0.284
[21,     1] loss: 0.257
[22,     1] loss: 0.236
[23,     1] loss: 0.218
[24,     1] loss: 0.197
[25,     1] loss: 0.182
[26,     1] loss: 0.168
[27,     1] loss: 0.153
[28,     1] loss: 0.141
[29,     1] loss: 0.134
[30,     1] loss: 0.124
[31,     1] loss: 0.117
[32,     1] loss: 0.110
[33,     1] loss: 0.105
[34,     1] loss: 0.099
[35,     1] loss: 0.096
[36,     1] loss: 0.092
[37,     1] loss: 0.090
[38,     1] loss: 0.088
[39,     1] loss: 0.086
[40,     1] loss: 0.084
[41,     1] loss: 0.084
[42,     1] loss: 0.082
[43,     1] loss: 0.081
[44,     1] loss: 0.081
[45,     1] loss: 0.079
[46,     1] loss: 0.078
[47,     1] loss: 0.080
[48,     1] loss: 0.079
[49,     1] loss: 0.079
[50,     1] loss: 0.078
[51,     1] loss: 0.077
[52,     1] loss: 0.078
[53,     1] loss: 0.076
[54,     1] loss: 0.076
[55,     1] loss: 0.077
[56,     1] loss: 0.076
[57,     1] loss: 0.076
[58,     1] loss: 0.075
[59,     1] loss: 0.075
[60,     1] loss: 0.075
[61,     1] loss: 0.075
[62,     1] loss: 0.076
[63,     1] loss: 0.074
[64,     1] loss: 0.074
[65,     1] loss: 0.073
[66,     1] loss: 0.073
[67,     1] loss: 0.073
Early stopping applied (best metric=0.41820597648620605)
Finished Training
Total time taken: 102.03644728660583
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.667
[4,     1] loss: 0.650
[5,     1] loss: 0.634
[6,     1] loss: 0.619
[7,     1] loss: 0.605
[8,     1] loss: 0.588
[9,     1] loss: 0.566
[10,     1] loss: 0.547
[11,     1] loss: 0.525
[12,     1] loss: 0.506
[13,     1] loss: 0.479
[14,     1] loss: 0.455
[15,     1] loss: 0.431
[16,     1] loss: 0.405
[17,     1] loss: 0.382
[18,     1] loss: 0.355
[19,     1] loss: 0.326
[20,     1] loss: 0.297
[21,     1] loss: 0.275
[22,     1] loss: 0.251
[23,     1] loss: 0.228
[24,     1] loss: 0.203
[25,     1] loss: 0.187
[26,     1] loss: 0.170
[27,     1] loss: 0.153
[28,     1] loss: 0.138
[29,     1] loss: 0.126
[30,     1] loss: 0.115
[31,     1] loss: 0.105
[32,     1] loss: 0.095
[33,     1] loss: 0.088
[34,     1] loss: 0.082
[35,     1] loss: 0.075
[36,     1] loss: 0.070
[37,     1] loss: 0.065
[38,     1] loss: 0.059
[39,     1] loss: 0.059
[40,     1] loss: 0.055
[41,     1] loss: 0.052
[42,     1] loss: 0.050
[43,     1] loss: 0.048
[44,     1] loss: 0.049
[45,     1] loss: 0.046
[46,     1] loss: 0.045
[47,     1] loss: 0.043
[48,     1] loss: 0.042
[49,     1] loss: 0.042
[50,     1] loss: 0.043
[51,     1] loss: 0.042
[52,     1] loss: 0.043
[53,     1] loss: 0.041
[54,     1] loss: 0.042
[55,     1] loss: 0.041
[56,     1] loss: 0.042
[57,     1] loss: 0.041
[58,     1] loss: 0.042
[59,     1] loss: 0.041
[60,     1] loss: 0.042
[61,     1] loss: 0.042
[62,     1] loss: 0.042
[63,     1] loss: 0.041
[64,     1] loss: 0.040
[65,     1] loss: 0.040
[66,     1] loss: 0.041
[67,     1] loss: 0.039
[68,     1] loss: 0.040
Early stopping applied (best metric=0.3961690664291382)
Finished Training
Total time taken: 103.74245476722717
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.664
[5,     1] loss: 0.650
[6,     1] loss: 0.635
[7,     1] loss: 0.620
[8,     1] loss: 0.605
[9,     1] loss: 0.588
[10,     1] loss: 0.566
[11,     1] loss: 0.547
[12,     1] loss: 0.526
[13,     1] loss: 0.502
[14,     1] loss: 0.476
[15,     1] loss: 0.448
[16,     1] loss: 0.424
[17,     1] loss: 0.396
[18,     1] loss: 0.370
[19,     1] loss: 0.340
[20,     1] loss: 0.316
[21,     1] loss: 0.289
[22,     1] loss: 0.265
[23,     1] loss: 0.241
[24,     1] loss: 0.222
[25,     1] loss: 0.200
[26,     1] loss: 0.184
[27,     1] loss: 0.162
[28,     1] loss: 0.145
[29,     1] loss: 0.132
[30,     1] loss: 0.118
[31,     1] loss: 0.107
[32,     1] loss: 0.097
[33,     1] loss: 0.089
[34,     1] loss: 0.083
[35,     1] loss: 0.076
[36,     1] loss: 0.070
[37,     1] loss: 0.064
[38,     1] loss: 0.061
[39,     1] loss: 0.057
[40,     1] loss: 0.054
[41,     1] loss: 0.053
[42,     1] loss: 0.050
[43,     1] loss: 0.047
[44,     1] loss: 0.047
[45,     1] loss: 0.047
[46,     1] loss: 0.045
[47,     1] loss: 0.045
[48,     1] loss: 0.044
[49,     1] loss: 0.044
[50,     1] loss: 0.045
[51,     1] loss: 0.045
[52,     1] loss: 0.042
[53,     1] loss: 0.043
[54,     1] loss: 0.045
[55,     1] loss: 0.044
[56,     1] loss: 0.045
[57,     1] loss: 0.044
[58,     1] loss: 0.045
[59,     1] loss: 0.046
[60,     1] loss: 0.046
[61,     1] loss: 0.044
[62,     1] loss: 0.044
[63,     1] loss: 0.046
[64,     1] loss: 0.045
[65,     1] loss: 0.046
[66,     1] loss: 0.045
[67,     1] loss: 0.044
[68,     1] loss: 0.045
[69,     1] loss: 0.044
[70,     1] loss: 0.044
Early stopping applied (best metric=0.4255608320236206)
Finished Training
Total time taken: 107.03645992279053
{'Hydroxylation-P Validation Accuracy': 0.7817791584183544, 'Hydroxylation-P Validation Sensitivity': 0.7296507936507937, 'Hydroxylation-P Validation Specificity': 0.7930001496333982, 'Hydroxylation-P Validation Precision': 0.44661123995905533, 'Hydroxylation-P AUC ROC': 0.837587859743628, 'Hydroxylation-P AUC PR': 0.5810583528433528, 'Hydroxylation-P MCC': 0.4438193280143308, 'Hydroxylation-P F1': 0.547242614212362, 'Validation Loss (Hydroxylation-P)': 0.42165338873863223, 'Validation Loss (total)': 0.42165338873863223}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004601330790840325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 601620563,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.881216199614311}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017307316583082977,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1796317171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.64576687669318}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.674
[4,     1] loss: 0.665
[5,     1] loss: 0.657
[6,     1] loss: 0.648
[7,     1] loss: 0.638
[8,     1] loss: 0.626
[9,     1] loss: 0.616
[10,     1] loss: 0.603
[11,     1] loss: 0.592
[12,     1] loss: 0.579
[13,     1] loss: 0.564
[14,     1] loss: 0.549
[15,     1] loss: 0.537
[16,     1] loss: 0.524
[17,     1] loss: 0.514
[18,     1] loss: 0.504
[19,     1] loss: 0.494
[20,     1] loss: 0.485
[21,     1] loss: 0.476
[22,     1] loss: 0.469
[23,     1] loss: 0.464
[24,     1] loss: 0.456
[25,     1] loss: 0.451
[26,     1] loss: 0.446
[27,     1] loss: 0.443
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035466558633787874,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1659811368,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.334146149129538}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012762355288931885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3397467569,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.393085361697906}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.672
[3,     1] loss: 0.654
[4,     1] loss: 0.639
[5,     1] loss: 0.630
[6,     1] loss: 0.615
[7,     1] loss: 0.605
[8,     1] loss: 0.596
[9,     1] loss: 0.585
[10,     1] loss: 0.576
[11,     1] loss: 0.563
[12,     1] loss: 0.553
[13,     1] loss: 0.542
[14,     1] loss: 0.531
[15,     1] loss: 0.519
[16,     1] loss: 0.508
[17,     1] loss: 0.496
[18,     1] loss: 0.487
[19,     1] loss: 0.473
[20,     1] loss: 0.459
[21,     1] loss: 0.449
[22,     1] loss: 0.437
[23,     1] loss: 0.428
[24,     1] loss: 0.417
[25,     1] loss: 0.408
[26,     1] loss: 0.400
[27,     1] loss: 0.391
[28,     1] loss: 0.385
[29,     1] loss: 0.377
[30,     1] loss: 0.371
[31,     1] loss: 0.368
[32,     1] loss: 0.365
[33,     1] loss: 0.360
[34,     1] loss: 0.355
[35,     1] loss: 0.353
[36,     1] loss: 0.350
[37,     1] loss: 0.349
[38,     1] loss: 0.349
[39,     1] loss: 0.353
[40,     1] loss: 0.371
[41,     1] loss: 0.346
[42,     1] loss: 0.357
[43,     1] loss: 0.348
[44,     1] loss: 0.352
[45,     1] loss: 0.346
[46,     1] loss: 0.348
[47,     1] loss: 0.345
[48,     1] loss: 0.343
[49,     1] loss: 0.341
[50,     1] loss: 0.339
[51,     1] loss: 0.338
[52,     1] loss: 0.338
[53,     1] loss: 0.337
[54,     1] loss: 0.336
[55,     1] loss: 0.333
[56,     1] loss: 0.332
[57,     1] loss: 0.330
[58,     1] loss: 0.329
[59,     1] loss: 0.329
[60,     1] loss: 0.326
[61,     1] loss: 0.325
[62,     1] loss: 0.323
[63,     1] loss: 0.321
[64,     1] loss: 0.319
[65,     1] loss: 0.318
[66,     1] loss: 0.315
[67,     1] loss: 0.313
[68,     1] loss: 0.312
[69,     1] loss: 0.311
[70,     1] loss: 0.308
[71,     1] loss: 0.305
[72,     1] loss: 0.301
[73,     1] loss: 0.300
[74,     1] loss: 0.297
[75,     1] loss: 0.294
[76,     1] loss: 0.294
[77,     1] loss: 0.291
[78,     1] loss: 0.291
[79,     1] loss: 0.308
[80,     1] loss: 0.408
[81,     1] loss: 0.377
[82,     1] loss: 0.333
[83,     1] loss: 0.337
[84,     1] loss: 0.329
[85,     1] loss: 0.320
[86,     1] loss: 0.316
[87,     1] loss: 0.310
[88,     1] loss: 0.304
[89,     1] loss: 0.298
[90,     1] loss: 0.293
[91,     1] loss: 0.288
[92,     1] loss: 0.284
[93,     1] loss: 0.278
Early stopping applied (best metric=0.40875905752182007)
Finished Training
Total time taken: 141.50169897079468
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.687
[3,     1] loss: 0.673
[4,     1] loss: 0.663
[5,     1] loss: 0.654
[6,     1] loss: 0.644
[7,     1] loss: 0.635
[8,     1] loss: 0.626
[9,     1] loss: 0.616
[10,     1] loss: 0.610
[11,     1] loss: 0.599
[12,     1] loss: 0.591
[13,     1] loss: 0.581
[14,     1] loss: 0.572
[15,     1] loss: 0.560
[16,     1] loss: 0.552
[17,     1] loss: 0.541
[18,     1] loss: 0.531
[19,     1] loss: 0.520
[20,     1] loss: 0.511
[21,     1] loss: 0.499
[22,     1] loss: 0.490
[23,     1] loss: 0.480
[24,     1] loss: 0.471
[25,     1] loss: 0.458
[26,     1] loss: 0.450
[27,     1] loss: 0.439
[28,     1] loss: 0.431
[29,     1] loss: 0.422
[30,     1] loss: 0.409
[31,     1] loss: 0.403
[32,     1] loss: 0.394
[33,     1] loss: 0.383
[34,     1] loss: 0.374
[35,     1] loss: 0.366
[36,     1] loss: 0.359
[37,     1] loss: 0.351
[38,     1] loss: 0.342
[39,     1] loss: 0.332
[40,     1] loss: 0.324
[41,     1] loss: 0.320
[42,     1] loss: 0.312
[43,     1] loss: 0.306
[44,     1] loss: 0.302
[45,     1] loss: 0.295
[46,     1] loss: 0.289
[47,     1] loss: 0.286
[48,     1] loss: 0.281
[49,     1] loss: 0.276
[50,     1] loss: 0.271
[51,     1] loss: 0.267
[52,     1] loss: 0.262
[53,     1] loss: 0.260
[54,     1] loss: 0.256
[55,     1] loss: 0.255
[56,     1] loss: 0.252
[57,     1] loss: 0.249
[58,     1] loss: 0.247
[59,     1] loss: 0.244
[60,     1] loss: 0.243
[61,     1] loss: 0.242
[62,     1] loss: 0.238
[63,     1] loss: 0.239
[64,     1] loss: 0.238
[65,     1] loss: 0.238
[66,     1] loss: 0.241
[67,     1] loss: 0.237
[68,     1] loss: 0.235
[69,     1] loss: 0.236
[70,     1] loss: 0.233
[71,     1] loss: 0.234
[72,     1] loss: 0.232
Early stopping applied (best metric=0.4635348320007324)
Finished Training
Total time taken: 110.29748606681824
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.674
[4,     1] loss: 0.662
[5,     1] loss: 0.651
[6,     1] loss: 0.643
[7,     1] loss: 0.634
[8,     1] loss: 0.625
[9,     1] loss: 0.617
[10,     1] loss: 0.608
[11,     1] loss: 0.597
[12,     1] loss: 0.590
[13,     1] loss: 0.581
[14,     1] loss: 0.572
[15,     1] loss: 0.560
[16,     1] loss: 0.552
[17,     1] loss: 0.541
[18,     1] loss: 0.531
[19,     1] loss: 0.520
[20,     1] loss: 0.507
[21,     1] loss: 0.495
[22,     1] loss: 0.483
[23,     1] loss: 0.475
[24,     1] loss: 0.462
[25,     1] loss: 0.452
[26,     1] loss: 0.440
[27,     1] loss: 0.432
[28,     1] loss: 0.422
[29,     1] loss: 0.413
[30,     1] loss: 0.402
[31,     1] loss: 0.395
[32,     1] loss: 0.389
[33,     1] loss: 0.382
[34,     1] loss: 0.378
[35,     1] loss: 0.386
[36,     1] loss: 0.384
[37,     1] loss: 0.371
[38,     1] loss: 0.372
[39,     1] loss: 0.365
[40,     1] loss: 0.363
[41,     1] loss: 0.357
[42,     1] loss: 0.355
[43,     1] loss: 0.350
[44,     1] loss: 0.347
[45,     1] loss: 0.344
[46,     1] loss: 0.341
[47,     1] loss: 0.339
[48,     1] loss: 0.336
[49,     1] loss: 0.334
[50,     1] loss: 0.331
[51,     1] loss: 0.331
[52,     1] loss: 0.327
[53,     1] loss: 0.326
[54,     1] loss: 0.322
[55,     1] loss: 0.322
[56,     1] loss: 0.318
[57,     1] loss: 0.316
[58,     1] loss: 0.314
[59,     1] loss: 0.311
[60,     1] loss: 0.309
[61,     1] loss: 0.306
[62,     1] loss: 0.305
[63,     1] loss: 0.304
[64,     1] loss: 0.301
[65,     1] loss: 0.296
[66,     1] loss: 0.296
[67,     1] loss: 0.294
[68,     1] loss: 0.292
[69,     1] loss: 0.290
[70,     1] loss: 0.288
[71,     1] loss: 0.287
[72,     1] loss: 0.285
[73,     1] loss: 0.282
[74,     1] loss: 0.281
[75,     1] loss: 0.279
[76,     1] loss: 0.277
[77,     1] loss: 0.276
[78,     1] loss: 0.274
[79,     1] loss: 0.272
[80,     1] loss: 0.270
[81,     1] loss: 0.268
[82,     1] loss: 0.267
[83,     1] loss: 0.270
[84,     1] loss: 0.325
[85,     1] loss: 0.300
[86,     1] loss: 0.296
[87,     1] loss: 0.320
[88,     1] loss: 0.354
[89,     1] loss: 0.311
[90,     1] loss: 0.351
[91,     1] loss: 0.330
[92,     1] loss: 0.350
[93,     1] loss: 0.312
[94,     1] loss: 0.314
[95,     1] loss: 0.309
[96,     1] loss: 0.300
[97,     1] loss: 0.296
[98,     1] loss: 0.293
[99,     1] loss: 0.289
[100,     1] loss: 0.287
[101,     1] loss: 0.281
[102,     1] loss: 0.278
[103,     1] loss: 0.275
[104,     1] loss: 0.271
[105,     1] loss: 0.268
[106,     1] loss: 0.263
[107,     1] loss: 0.263
[108,     1] loss: 0.260
[109,     1] loss: 0.257
[110,     1] loss: 0.258
[111,     1] loss: 0.251
[112,     1] loss: 0.252
[113,     1] loss: 0.249
[114,     1] loss: 0.246
[115,     1] loss: 0.244
[116,     1] loss: 0.241
[117,     1] loss: 0.241
[118,     1] loss: 0.238
[119,     1] loss: 0.239
[120,     1] loss: 0.236
[121,     1] loss: 0.234
[122,     1] loss: 0.232
[123,     1] loss: 0.231
[124,     1] loss: 0.230
[125,     1] loss: 0.226
[126,     1] loss: 0.226
[127,     1] loss: 0.224
[128,     1] loss: 0.224
[129,     1] loss: 0.221
[130,     1] loss: 0.221
[131,     1] loss: 0.219
[132,     1] loss: 0.217
[133,     1] loss: 0.217
[134,     1] loss: 0.215
[135,     1] loss: 0.213
[136,     1] loss: 0.213
[137,     1] loss: 0.212
[138,     1] loss: 0.208
[139,     1] loss: 0.210
[140,     1] loss: 0.207
[141,     1] loss: 0.207
[142,     1] loss: 0.206
[143,     1] loss: 0.206
[144,     1] loss: 0.205
[145,     1] loss: 0.203
[146,     1] loss: 0.202
[147,     1] loss: 0.201
[148,     1] loss: 0.203
[149,     1] loss: 0.203
[150,     1] loss: 0.203
[151,     1] loss: 0.203
[152,     1] loss: 0.235
[153,     1] loss: 0.518
[154,     1] loss: 0.403
[155,     1] loss: 0.463
[156,     1] loss: 0.425
[157,     1] loss: 0.384
[158,     1] loss: 0.381
[159,     1] loss: 0.382
Early stopping applied (best metric=0.3722916841506958)
Finished Training
Total time taken: 242.9170799255371
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.685
[3,     1] loss: 0.676
[4,     1] loss: 0.669
[5,     1] loss: 0.662
[6,     1] loss: 0.656
[7,     1] loss: 0.649
[8,     1] loss: 0.645
[9,     1] loss: 0.639
[10,     1] loss: 0.632
[11,     1] loss: 0.624
[12,     1] loss: 0.617
[13,     1] loss: 0.609
[14,     1] loss: 0.601
[15,     1] loss: 0.594
[16,     1] loss: 0.585
[17,     1] loss: 0.575
[18,     1] loss: 0.566
[19,     1] loss: 0.555
[20,     1] loss: 0.547
[21,     1] loss: 0.535
[22,     1] loss: 0.523
[23,     1] loss: 0.512
[24,     1] loss: 0.500
[25,     1] loss: 0.489
[26,     1] loss: 0.477
[27,     1] loss: 0.465
[28,     1] loss: 0.452
[29,     1] loss: 0.441
[30,     1] loss: 0.426
[31,     1] loss: 0.416
[32,     1] loss: 0.402
[33,     1] loss: 0.392
[34,     1] loss: 0.381
[35,     1] loss: 0.368
[36,     1] loss: 0.357
[37,     1] loss: 0.348
[38,     1] loss: 0.339
[39,     1] loss: 0.326
[40,     1] loss: 0.316
[41,     1] loss: 0.308
[42,     1] loss: 0.302
[43,     1] loss: 0.292
[44,     1] loss: 0.283
[45,     1] loss: 0.275
[46,     1] loss: 0.271
[47,     1] loss: 0.261
[48,     1] loss: 0.260
[49,     1] loss: 0.251
[50,     1] loss: 0.247
[51,     1] loss: 0.240
[52,     1] loss: 0.241
[53,     1] loss: 0.233
[54,     1] loss: 0.231
[55,     1] loss: 0.229
[56,     1] loss: 0.226
[57,     1] loss: 0.221
[58,     1] loss: 0.220
[59,     1] loss: 0.218
[60,     1] loss: 0.225
[61,     1] loss: 0.244
[62,     1] loss: 0.242
[63,     1] loss: 0.241
[64,     1] loss: 0.235
[65,     1] loss: 0.234
[66,     1] loss: 0.233
[67,     1] loss: 0.228
[68,     1] loss: 0.226
[69,     1] loss: 0.222
[70,     1] loss: 0.222
[71,     1] loss: 0.220
[72,     1] loss: 0.218
[73,     1] loss: 0.215
[74,     1] loss: 0.214
[75,     1] loss: 0.213
[76,     1] loss: 0.213
[77,     1] loss: 0.211
[78,     1] loss: 0.210
[79,     1] loss: 0.211
[80,     1] loss: 0.210
[81,     1] loss: 0.207
[82,     1] loss: 0.209
[83,     1] loss: 0.207
[84,     1] loss: 0.208
[85,     1] loss: 0.209
[86,     1] loss: 0.208
[87,     1] loss: 0.209
[88,     1] loss: 0.207
[89,     1] loss: 0.206
[90,     1] loss: 0.206
[91,     1] loss: 0.203
[92,     1] loss: 0.209
[93,     1] loss: 0.283
[94,     1] loss: 0.410
[95,     1] loss: 0.277
[96,     1] loss: 0.299
[97,     1] loss: 0.274
[98,     1] loss: 0.275
[99,     1] loss: 0.265
[100,     1] loss: 0.263
[101,     1] loss: 0.258
[102,     1] loss: 0.247
[103,     1] loss: 0.247
[104,     1] loss: 0.244
[105,     1] loss: 0.238
[106,     1] loss: 0.235
[107,     1] loss: 0.232
[108,     1] loss: 0.228
[109,     1] loss: 0.225
[110,     1] loss: 0.221
[111,     1] loss: 0.219
[112,     1] loss: 0.217
[113,     1] loss: 0.217
[114,     1] loss: 0.215
[115,     1] loss: 0.211
[116,     1] loss: 0.212
[117,     1] loss: 0.210
[118,     1] loss: 0.210
[119,     1] loss: 0.209
[120,     1] loss: 0.207
[121,     1] loss: 0.205
[122,     1] loss: 0.207
[123,     1] loss: 0.207
Early stopping applied (best metric=0.3310774266719818)
Finished Training
Total time taken: 190.3568413257599
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.698
[3,     1] loss: 0.688
[4,     1] loss: 0.682
[5,     1] loss: 0.675
[6,     1] loss: 0.669
[7,     1] loss: 0.662
[8,     1] loss: 0.657
[9,     1] loss: 0.651
[10,     1] loss: 0.643
[11,     1] loss: 0.638
[12,     1] loss: 0.631
[13,     1] loss: 0.623
[14,     1] loss: 0.615
[15,     1] loss: 0.609
[16,     1] loss: 0.599
[17,     1] loss: 0.592
[18,     1] loss: 0.583
[19,     1] loss: 0.575
[20,     1] loss: 0.565
[21,     1] loss: 0.557
[22,     1] loss: 0.546
[23,     1] loss: 0.536
[24,     1] loss: 0.529
[25,     1] loss: 0.519
[26,     1] loss: 0.508
[27,     1] loss: 0.498
[28,     1] loss: 0.487
[29,     1] loss: 0.474
[30,     1] loss: 0.462
[31,     1] loss: 0.448
[32,     1] loss: 0.440
[33,     1] loss: 0.426
[34,     1] loss: 0.416
[35,     1] loss: 0.402
[36,     1] loss: 0.392
[37,     1] loss: 0.380
[38,     1] loss: 0.371
[39,     1] loss: 0.360
[40,     1] loss: 0.349
[41,     1] loss: 0.340
[42,     1] loss: 0.329
[43,     1] loss: 0.320
[44,     1] loss: 0.311
[45,     1] loss: 0.304
[46,     1] loss: 0.296
[47,     1] loss: 0.289
[48,     1] loss: 0.283
[49,     1] loss: 0.272
[50,     1] loss: 0.269
[51,     1] loss: 0.265
[52,     1] loss: 0.257
[53,     1] loss: 0.253
[54,     1] loss: 0.248
[55,     1] loss: 0.242
[56,     1] loss: 0.240
[57,     1] loss: 0.233
[58,     1] loss: 0.232
[59,     1] loss: 0.229
[60,     1] loss: 0.226
[61,     1] loss: 0.224
[62,     1] loss: 0.219
[63,     1] loss: 0.219
[64,     1] loss: 0.219
[65,     1] loss: 0.215
[66,     1] loss: 0.214
[67,     1] loss: 0.211
[68,     1] loss: 0.212
[69,     1] loss: 0.211
[70,     1] loss: 0.210
[71,     1] loss: 0.209
[72,     1] loss: 0.206
[73,     1] loss: 0.209
[74,     1] loss: 0.205
[75,     1] loss: 0.207
[76,     1] loss: 0.207
[77,     1] loss: 0.214
[78,     1] loss: 0.230
[79,     1] loss: 0.531
[80,     1] loss: 0.315
[81,     1] loss: 0.309
[82,     1] loss: 0.295
[83,     1] loss: 0.294
[84,     1] loss: 0.290
[85,     1] loss: 0.281
[86,     1] loss: 0.278
[87,     1] loss: 0.276
[88,     1] loss: 0.271
[89,     1] loss: 0.269
[90,     1] loss: 0.268
[91,     1] loss: 0.268
[92,     1] loss: 0.269
[93,     1] loss: 0.265
[94,     1] loss: 0.265
[95,     1] loss: 0.261
[96,     1] loss: 0.261
[97,     1] loss: 0.262
[98,     1] loss: 0.258
[99,     1] loss: 0.259
[100,     1] loss: 0.257
[101,     1] loss: 0.256
[102,     1] loss: 0.256
Early stopping applied (best metric=0.369166761636734)
Finished Training
Total time taken: 156.17530918121338
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.697
[3,     1] loss: 0.684
[4,     1] loss: 0.672
[5,     1] loss: 0.659
[6,     1] loss: 0.651
[7,     1] loss: 0.640
[8,     1] loss: 0.630
[9,     1] loss: 0.618
[10,     1] loss: 0.608
[11,     1] loss: 0.596
[12,     1] loss: 0.582
[13,     1] loss: 0.573
[14,     1] loss: 0.561
[15,     1] loss: 0.548
[16,     1] loss: 0.535
[17,     1] loss: 0.524
[18,     1] loss: 0.513
[19,     1] loss: 0.501
[20,     1] loss: 0.491
[21,     1] loss: 0.480
[22,     1] loss: 0.469
[23,     1] loss: 0.460
[24,     1] loss: 0.449
[25,     1] loss: 0.441
[26,     1] loss: 0.434
[27,     1] loss: 0.425
[28,     1] loss: 0.418
[29,     1] loss: 0.410
[30,     1] loss: 0.403
[31,     1] loss: 0.397
[32,     1] loss: 0.391
[33,     1] loss: 0.384
[34,     1] loss: 0.380
[35,     1] loss: 0.373
[36,     1] loss: 0.368
[37,     1] loss: 0.364
[38,     1] loss: 0.358
[39,     1] loss: 0.353
[40,     1] loss: 0.346
[41,     1] loss: 0.346
[42,     1] loss: 0.340
[43,     1] loss: 0.338
[44,     1] loss: 0.334
[45,     1] loss: 0.330
[46,     1] loss: 0.328
[47,     1] loss: 0.324
[48,     1] loss: 0.323
[49,     1] loss: 0.322
[50,     1] loss: 0.317
[51,     1] loss: 0.316
[52,     1] loss: 0.315
[53,     1] loss: 0.312
[54,     1] loss: 0.311
[55,     1] loss: 0.310
[56,     1] loss: 0.309
[57,     1] loss: 0.306
[58,     1] loss: 0.303
[59,     1] loss: 0.303
[60,     1] loss: 0.303
[61,     1] loss: 0.304
[62,     1] loss: 0.304
[63,     1] loss: 0.301
[64,     1] loss: 0.301
[65,     1] loss: 0.299
[66,     1] loss: 0.299
[67,     1] loss: 0.299
[68,     1] loss: 0.298
[69,     1] loss: 0.299
[70,     1] loss: 0.300
[71,     1] loss: 0.300
[72,     1] loss: 0.298
[73,     1] loss: 0.300
[74,     1] loss: 0.298
[75,     1] loss: 0.297
[76,     1] loss: 0.296
[77,     1] loss: 0.298
[78,     1] loss: 0.299
[79,     1] loss: 0.300
[80,     1] loss: 0.300
[81,     1] loss: 0.300
[82,     1] loss: 0.299
[83,     1] loss: 0.304
[84,     1] loss: 0.311
[85,     1] loss: 0.492
[86,     1] loss: 0.468
[87,     1] loss: 0.422
[88,     1] loss: 0.413
[89,     1] loss: 0.399
[90,     1] loss: 0.387
[91,     1] loss: 0.378
[92,     1] loss: 0.366
[93,     1] loss: 0.362
[94,     1] loss: 0.363
[95,     1] loss: 0.362
[96,     1] loss: 0.359
[97,     1] loss: 0.357
[98,     1] loss: 0.353
[99,     1] loss: 0.353
[100,     1] loss: 0.353
[101,     1] loss: 0.350
[102,     1] loss: 0.349
[103,     1] loss: 0.350
[104,     1] loss: 0.350
[105,     1] loss: 0.349
[106,     1] loss: 0.347
[107,     1] loss: 0.346
[108,     1] loss: 0.348
[109,     1] loss: 0.347
[110,     1] loss: 0.346
[111,     1] loss: 0.347
[112,     1] loss: 0.347
[113,     1] loss: 0.347
[114,     1] loss: 0.348
[115,     1] loss: 0.349
[116,     1] loss: 0.349
[117,     1] loss: 0.350
[118,     1] loss: 0.351
[119,     1] loss: 0.350
[120,     1] loss: 0.351
[121,     1] loss: 0.353
[122,     1] loss: 0.354
[123,     1] loss: 0.353
[124,     1] loss: 0.355
[125,     1] loss: 0.355
[126,     1] loss: 0.355
[127,     1] loss: 0.356
[128,     1] loss: 0.357
[129,     1] loss: 0.357
[130,     1] loss: 0.359
[131,     1] loss: 0.358
[132,     1] loss: 0.361
[133,     1] loss: 0.361
[134,     1] loss: 0.363
[135,     1] loss: 0.362
[136,     1] loss: 0.363
[137,     1] loss: 0.363
[138,     1] loss: 0.365
[139,     1] loss: 0.365
[140,     1] loss: 0.366
Early stopping applied (best metric=0.445316344499588)
Finished Training
Total time taken: 214.85247898101807
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.700
[3,     1] loss: 0.684
[4,     1] loss: 0.669
[5,     1] loss: 0.656
[6,     1] loss: 0.646
[7,     1] loss: 0.635
[8,     1] loss: 0.625
[9,     1] loss: 0.616
[10,     1] loss: 0.604
[11,     1] loss: 0.596
[12,     1] loss: 0.585
[13,     1] loss: 0.576
[14,     1] loss: 0.566
[15,     1] loss: 0.555
[16,     1] loss: 0.540
[17,     1] loss: 0.530
[18,     1] loss: 0.517
[19,     1] loss: 0.507
[20,     1] loss: 0.491
[21,     1] loss: 0.482
[22,     1] loss: 0.468
[23,     1] loss: 0.457
[24,     1] loss: 0.447
[25,     1] loss: 0.434
[26,     1] loss: 0.424
[27,     1] loss: 0.413
[28,     1] loss: 0.404
[29,     1] loss: 0.397
[30,     1] loss: 0.388
[31,     1] loss: 0.383
[32,     1] loss: 0.377
[33,     1] loss: 0.368
[34,     1] loss: 0.364
[35,     1] loss: 0.357
[36,     1] loss: 0.352
[37,     1] loss: 0.349
[38,     1] loss: 0.342
[39,     1] loss: 0.339
[40,     1] loss: 0.334
[41,     1] loss: 0.328
[42,     1] loss: 0.324
[43,     1] loss: 0.320
[44,     1] loss: 0.316
[45,     1] loss: 0.312
[46,     1] loss: 0.310
[47,     1] loss: 0.307
[48,     1] loss: 0.303
[49,     1] loss: 0.297
[50,     1] loss: 0.295
[51,     1] loss: 0.293
[52,     1] loss: 0.288
[53,     1] loss: 0.285
[54,     1] loss: 0.280
[55,     1] loss: 0.280
[56,     1] loss: 0.275
[57,     1] loss: 0.274
[58,     1] loss: 0.268
[59,     1] loss: 0.266
[60,     1] loss: 0.262
[61,     1] loss: 0.261
[62,     1] loss: 0.258
[63,     1] loss: 0.257
[64,     1] loss: 0.255
[65,     1] loss: 0.253
[66,     1] loss: 0.251
[67,     1] loss: 0.248
[68,     1] loss: 0.247
[69,     1] loss: 0.246
[70,     1] loss: 0.257
[71,     1] loss: 0.308
[72,     1] loss: 0.338
[73,     1] loss: 0.316
[74,     1] loss: 0.285
[75,     1] loss: 0.282
[76,     1] loss: 0.277
[77,     1] loss: 0.277
[78,     1] loss: 0.272
[79,     1] loss: 0.268
[80,     1] loss: 0.263
[81,     1] loss: 0.262
[82,     1] loss: 0.260
[83,     1] loss: 0.257
[84,     1] loss: 0.253
[85,     1] loss: 0.248
[86,     1] loss: 0.247
[87,     1] loss: 0.244
[88,     1] loss: 0.242
[89,     1] loss: 0.242
[90,     1] loss: 0.238
[91,     1] loss: 0.236
[92,     1] loss: 0.234
[93,     1] loss: 0.233
[94,     1] loss: 0.228
[95,     1] loss: 0.228
[96,     1] loss: 0.224
[97,     1] loss: 0.221
[98,     1] loss: 0.216
[99,     1] loss: 0.215
[100,     1] loss: 0.211
[101,     1] loss: 0.210
[102,     1] loss: 0.207
[103,     1] loss: 0.208
[104,     1] loss: 0.218
[105,     1] loss: 0.331
[106,     1] loss: 0.281
[107,     1] loss: 0.324
[108,     1] loss: 0.262
[109,     1] loss: 0.276
[110,     1] loss: 0.276
[111,     1] loss: 0.266
[112,     1] loss: 0.261
[113,     1] loss: 0.254
[114,     1] loss: 0.255
[115,     1] loss: 0.253
[116,     1] loss: 0.252
[117,     1] loss: 0.250
[118,     1] loss: 0.248
[119,     1] loss: 0.242
[120,     1] loss: 0.239
[121,     1] loss: 0.237
[122,     1] loss: 0.234
[123,     1] loss: 0.232
[124,     1] loss: 0.227
[125,     1] loss: 0.227
[126,     1] loss: 0.222
[127,     1] loss: 0.220
[128,     1] loss: 0.218
[129,     1] loss: 0.216
[130,     1] loss: 0.215
[131,     1] loss: 0.214
[132,     1] loss: 0.211
[133,     1] loss: 0.213
[134,     1] loss: 0.212
[135,     1] loss: 0.208
[136,     1] loss: 0.207
[137,     1] loss: 0.206
[138,     1] loss: 0.206
[139,     1] loss: 0.204
[140,     1] loss: 0.205
[141,     1] loss: 0.203
[142,     1] loss: 0.205
[143,     1] loss: 0.204
[144,     1] loss: 0.202
[145,     1] loss: 0.203
[146,     1] loss: 0.202
Early stopping applied (best metric=0.37092703580856323)
Finished Training
Total time taken: 225.08313512802124
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.690
[3,     1] loss: 0.679
[4,     1] loss: 0.672
[5,     1] loss: 0.664
[6,     1] loss: 0.657
[7,     1] loss: 0.650
[8,     1] loss: 0.643
[9,     1] loss: 0.637
[10,     1] loss: 0.629
[11,     1] loss: 0.622
[12,     1] loss: 0.617
[13,     1] loss: 0.608
[14,     1] loss: 0.601
[15,     1] loss: 0.592
[16,     1] loss: 0.584
[17,     1] loss: 0.577
[18,     1] loss: 0.566
[19,     1] loss: 0.559
[20,     1] loss: 0.550
[21,     1] loss: 0.541
[22,     1] loss: 0.532
[23,     1] loss: 0.521
[24,     1] loss: 0.512
[25,     1] loss: 0.504
[26,     1] loss: 0.494
[27,     1] loss: 0.483
[28,     1] loss: 0.473
[29,     1] loss: 0.464
[30,     1] loss: 0.454
[31,     1] loss: 0.442
[32,     1] loss: 0.435
[33,     1] loss: 0.426
[34,     1] loss: 0.416
[35,     1] loss: 0.406
[36,     1] loss: 0.399
[37,     1] loss: 0.387
[38,     1] loss: 0.380
[39,     1] loss: 0.368
[40,     1] loss: 0.362
[41,     1] loss: 0.351
[42,     1] loss: 0.343
[43,     1] loss: 0.335
[44,     1] loss: 0.326
[45,     1] loss: 0.318
[46,     1] loss: 0.314
[47,     1] loss: 0.345
[48,     1] loss: 0.305
[49,     1] loss: 0.308
[50,     1] loss: 0.300
[51,     1] loss: 0.299
[52,     1] loss: 0.288
[53,     1] loss: 0.283
[54,     1] loss: 0.278
[55,     1] loss: 0.270
[56,     1] loss: 0.267
[57,     1] loss: 0.262
[58,     1] loss: 0.258
[59,     1] loss: 0.253
[60,     1] loss: 0.250
[61,     1] loss: 0.247
[62,     1] loss: 0.239
[63,     1] loss: 0.238
[64,     1] loss: 0.236
[65,     1] loss: 0.231
[66,     1] loss: 0.231
[67,     1] loss: 0.227
[68,     1] loss: 0.229
[69,     1] loss: 0.223
[70,     1] loss: 0.226
[71,     1] loss: 0.221
[72,     1] loss: 0.221
[73,     1] loss: 0.219
[74,     1] loss: 0.220
[75,     1] loss: 0.219
[76,     1] loss: 0.217
[77,     1] loss: 0.217
[78,     1] loss: 0.217
[79,     1] loss: 0.215
[80,     1] loss: 0.216
[81,     1] loss: 0.218
[82,     1] loss: 0.214
[83,     1] loss: 0.215
[84,     1] loss: 0.212
[85,     1] loss: 0.213
[86,     1] loss: 0.217
[87,     1] loss: 0.281
[88,     1] loss: 0.320
[89,     1] loss: 0.461
[90,     1] loss: 0.316
[91,     1] loss: 0.356
[92,     1] loss: 0.346
[93,     1] loss: 0.327
[94,     1] loss: 0.322
[95,     1] loss: 0.320
[96,     1] loss: 0.311
[97,     1] loss: 0.303
[98,     1] loss: 0.302
[99,     1] loss: 0.294
[100,     1] loss: 0.290
[101,     1] loss: 0.283
[102,     1] loss: 0.282
[103,     1] loss: 0.276
[104,     1] loss: 0.273
[105,     1] loss: 0.269
[106,     1] loss: 0.267
[107,     1] loss: 0.259
[108,     1] loss: 0.259
[109,     1] loss: 0.256
[110,     1] loss: 0.254
[111,     1] loss: 0.252
[112,     1] loss: 0.251
[113,     1] loss: 0.248
[114,     1] loss: 0.245
[115,     1] loss: 0.243
[116,     1] loss: 0.243
[117,     1] loss: 0.242
[118,     1] loss: 0.240
[119,     1] loss: 0.241
[120,     1] loss: 0.240
[121,     1] loss: 0.239
[122,     1] loss: 0.238
[123,     1] loss: 0.242
[124,     1] loss: 0.237
[125,     1] loss: 0.237
[126,     1] loss: 0.238
[127,     1] loss: 0.238
[128,     1] loss: 0.236
[129,     1] loss: 0.237
[130,     1] loss: 0.236
[131,     1] loss: 0.237
[132,     1] loss: 0.237
[133,     1] loss: 0.243
[134,     1] loss: 0.260
[135,     1] loss: 0.297
Early stopping applied (best metric=0.36853212118148804)
Finished Training
Total time taken: 209.36659049987793
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.670
[4,     1] loss: 0.659
[5,     1] loss: 0.649
[6,     1] loss: 0.639
[7,     1] loss: 0.630
[8,     1] loss: 0.621
[9,     1] loss: 0.611
[10,     1] loss: 0.600
[11,     1] loss: 0.591
[12,     1] loss: 0.578
[13,     1] loss: 0.569
[14,     1] loss: 0.558
[15,     1] loss: 0.546
[16,     1] loss: 0.532
[17,     1] loss: 0.520
[18,     1] loss: 0.506
[19,     1] loss: 0.493
[20,     1] loss: 0.482
[21,     1] loss: 0.467
[22,     1] loss: 0.455
[23,     1] loss: 0.440
[24,     1] loss: 0.425
[25,     1] loss: 0.415
[26,     1] loss: 0.400
[27,     1] loss: 0.387
[28,     1] loss: 0.374
[29,     1] loss: 0.360
[30,     1] loss: 0.352
[31,     1] loss: 0.339
[32,     1] loss: 0.327
[33,     1] loss: 0.316
[34,     1] loss: 0.307
[35,     1] loss: 0.295
[36,     1] loss: 0.284
[37,     1] loss: 0.276
[38,     1] loss: 0.267
[39,     1] loss: 0.261
[40,     1] loss: 0.254
[41,     1] loss: 0.246
[42,     1] loss: 0.240
[43,     1] loss: 0.232
[44,     1] loss: 0.231
[45,     1] loss: 0.224
[46,     1] loss: 0.221
[47,     1] loss: 0.218
[48,     1] loss: 0.212
[49,     1] loss: 0.211
[50,     1] loss: 0.209
[51,     1] loss: 0.206
[52,     1] loss: 0.204
[53,     1] loss: 0.202
[54,     1] loss: 0.202
[55,     1] loss: 0.198
[56,     1] loss: 0.198
[57,     1] loss: 0.196
[58,     1] loss: 0.198
[59,     1] loss: 0.197
[60,     1] loss: 0.198
[61,     1] loss: 0.197
[62,     1] loss: 0.195
[63,     1] loss: 0.196
[64,     1] loss: 0.194
[65,     1] loss: 0.196
[66,     1] loss: 0.196
[67,     1] loss: 0.200
[68,     1] loss: 0.213
[69,     1] loss: 0.219
[70,     1] loss: 0.221
[71,     1] loss: 0.213
[72,     1] loss: 0.218
[73,     1] loss: 0.217
[74,     1] loss: 0.214
[75,     1] loss: 0.216
[76,     1] loss: 0.213
[77,     1] loss: 0.212
[78,     1] loss: 0.210
[79,     1] loss: 0.211
[80,     1] loss: 0.207
[81,     1] loss: 0.211
[82,     1] loss: 0.211
[83,     1] loss: 0.208
[84,     1] loss: 0.206
[85,     1] loss: 0.210
[86,     1] loss: 0.210
[87,     1] loss: 0.211
[88,     1] loss: 0.209
[89,     1] loss: 0.210
[90,     1] loss: 0.209
[91,     1] loss: 0.208
[92,     1] loss: 0.211
[93,     1] loss: 0.212
[94,     1] loss: 0.212
[95,     1] loss: 0.215
[96,     1] loss: 0.217
[97,     1] loss: 0.216
[98,     1] loss: 0.217
[99,     1] loss: 0.216
[100,     1] loss: 0.214
[101,     1] loss: 0.217
[102,     1] loss: 0.217
[103,     1] loss: 0.216
[104,     1] loss: 0.218
[105,     1] loss: 0.234
[106,     1] loss: 0.253
[107,     1] loss: 0.259
[108,     1] loss: 0.354
[109,     1] loss: 0.294
[110,     1] loss: 0.307
[111,     1] loss: 0.294
[112,     1] loss: 0.286
[113,     1] loss: 0.263
[114,     1] loss: 0.292
[115,     1] loss: 0.275
[116,     1] loss: 0.261
[117,     1] loss: 0.254
[118,     1] loss: 0.252
[119,     1] loss: 0.249
[120,     1] loss: 0.242
[121,     1] loss: 0.238
[122,     1] loss: 0.231
[123,     1] loss: 0.227
[124,     1] loss: 0.224
Early stopping applied (best metric=0.4270953834056854)
Finished Training
Total time taken: 193.2614779472351
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.688
[3,     1] loss: 0.674
[4,     1] loss: 0.664
[5,     1] loss: 0.654
[6,     1] loss: 0.644
[7,     1] loss: 0.637
[8,     1] loss: 0.628
[9,     1] loss: 0.619
[10,     1] loss: 0.607
[11,     1] loss: 0.595
[12,     1] loss: 0.587
[13,     1] loss: 0.576
[14,     1] loss: 0.565
[15,     1] loss: 0.552
[16,     1] loss: 0.541
[17,     1] loss: 0.530
[18,     1] loss: 0.517
[19,     1] loss: 0.504
[20,     1] loss: 0.493
[21,     1] loss: 0.481
[22,     1] loss: 0.470
[23,     1] loss: 0.457
[24,     1] loss: 0.449
[25,     1] loss: 0.438
[26,     1] loss: 0.429
[27,     1] loss: 0.418
[28,     1] loss: 0.410
[29,     1] loss: 0.402
[30,     1] loss: 0.395
[31,     1] loss: 0.385
[32,     1] loss: 0.377
[33,     1] loss: 0.370
[34,     1] loss: 0.361
[35,     1] loss: 0.357
[36,     1] loss: 0.350
[37,     1] loss: 0.344
[38,     1] loss: 0.337
[39,     1] loss: 0.331
[40,     1] loss: 0.325
[41,     1] loss: 0.319
[42,     1] loss: 0.314
[43,     1] loss: 0.308
[44,     1] loss: 0.304
[45,     1] loss: 0.300
[46,     1] loss: 0.296
[47,     1] loss: 0.291
[48,     1] loss: 0.287
[49,     1] loss: 0.283
[50,     1] loss: 0.281
[51,     1] loss: 0.275
[52,     1] loss: 0.275
[53,     1] loss: 0.272
[54,     1] loss: 0.270
[55,     1] loss: 0.265
[56,     1] loss: 0.262
[57,     1] loss: 0.259
[58,     1] loss: 0.258
[59,     1] loss: 0.255
[60,     1] loss: 0.254
[61,     1] loss: 0.251
[62,     1] loss: 0.249
[63,     1] loss: 0.248
[64,     1] loss: 0.248
[65,     1] loss: 0.248
[66,     1] loss: 0.257
[67,     1] loss: 0.254
[68,     1] loss: 0.256
[69,     1] loss: 0.251
[70,     1] loss: 0.249
[71,     1] loss: 0.249
[72,     1] loss: 0.245
[73,     1] loss: 0.244
[74,     1] loss: 0.241
[75,     1] loss: 0.238
[76,     1] loss: 0.236
[77,     1] loss: 0.237
[78,     1] loss: 0.234
[79,     1] loss: 0.233
[80,     1] loss: 0.231
[81,     1] loss: 0.230
[82,     1] loss: 0.229
[83,     1] loss: 0.227
[84,     1] loss: 0.228
[85,     1] loss: 0.224
[86,     1] loss: 0.224
[87,     1] loss: 0.224
[88,     1] loss: 0.224
[89,     1] loss: 0.225
[90,     1] loss: 0.222
[91,     1] loss: 0.225
[92,     1] loss: 0.222
[93,     1] loss: 0.223
[94,     1] loss: 0.223
[95,     1] loss: 0.222
[96,     1] loss: 0.226
[97,     1] loss: 0.245
[98,     1] loss: 0.246
[99,     1] loss: 0.245
[100,     1] loss: 0.243
[101,     1] loss: 0.243
[102,     1] loss: 0.240
[103,     1] loss: 0.262
[104,     1] loss: 0.252
[105,     1] loss: 0.258
[106,     1] loss: 0.275
[107,     1] loss: 0.250
[108,     1] loss: 0.271
[109,     1] loss: 0.247
[110,     1] loss: 0.253
[111,     1] loss: 0.246
[112,     1] loss: 0.245
[113,     1] loss: 0.241
[114,     1] loss: 0.238
[115,     1] loss: 0.230
Early stopping applied (best metric=0.46543216705322266)
Finished Training
Total time taken: 180.12886238098145
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.682
[4,     1] loss: 0.673
[5,     1] loss: 0.668
[6,     1] loss: 0.660
[7,     1] loss: 0.655
[8,     1] loss: 0.648
[9,     1] loss: 0.640
[10,     1] loss: 0.632
[11,     1] loss: 0.625
[12,     1] loss: 0.615
[13,     1] loss: 0.607
[14,     1] loss: 0.596
[15,     1] loss: 0.585
[16,     1] loss: 0.577
[17,     1] loss: 0.565
[18,     1] loss: 0.556
[19,     1] loss: 0.545
[20,     1] loss: 0.534
[21,     1] loss: 0.523
[22,     1] loss: 0.512
[23,     1] loss: 0.501
[24,     1] loss: 0.491
[25,     1] loss: 0.484
[26,     1] loss: 0.473
[27,     1] loss: 0.466
[28,     1] loss: 0.463
[29,     1] loss: 0.450
[30,     1] loss: 0.455
[31,     1] loss: 0.483
[32,     1] loss: 0.451
[33,     1] loss: 0.439
[34,     1] loss: 0.443
[35,     1] loss: 0.432
[36,     1] loss: 0.434
[37,     1] loss: 0.422
[38,     1] loss: 0.417
[39,     1] loss: 0.419
[40,     1] loss: 0.414
[41,     1] loss: 0.411
[42,     1] loss: 0.409
[43,     1] loss: 0.407
[44,     1] loss: 0.405
[45,     1] loss: 0.404
[46,     1] loss: 0.400
[47,     1] loss: 0.399
[48,     1] loss: 0.398
[49,     1] loss: 0.398
[50,     1] loss: 0.397
[51,     1] loss: 0.395
[52,     1] loss: 0.395
[53,     1] loss: 0.394
[54,     1] loss: 0.393
[55,     1] loss: 0.394
[56,     1] loss: 0.393
[57,     1] loss: 0.392
[58,     1] loss: 0.392
[59,     1] loss: 0.392
[60,     1] loss: 0.393
[61,     1] loss: 0.392
[62,     1] loss: 0.391
[63,     1] loss: 0.394
[64,     1] loss: 0.408
[65,     1] loss: 0.412
[66,     1] loss: 0.401
[67,     1] loss: 0.410
[68,     1] loss: 0.404
[69,     1] loss: 0.402
[70,     1] loss: 0.398
[71,     1] loss: 0.397
[72,     1] loss: 0.395
[73,     1] loss: 0.392
[74,     1] loss: 0.390
[75,     1] loss: 0.389
[76,     1] loss: 0.396
[77,     1] loss: 0.391
[78,     1] loss: 0.388
[79,     1] loss: 0.390
[80,     1] loss: 0.385
[81,     1] loss: 0.385
[82,     1] loss: 0.383
Early stopping applied (best metric=0.4021129608154297)
Finished Training
Total time taken: 129.40757727622986
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.680
[3,     1] loss: 0.657
[4,     1] loss: 0.639
[5,     1] loss: 0.625
[6,     1] loss: 0.615
[7,     1] loss: 0.601
[8,     1] loss: 0.590
[9,     1] loss: 0.581
[10,     1] loss: 0.567
[11,     1] loss: 0.555
[12,     1] loss: 0.543
[13,     1] loss: 0.533
[14,     1] loss: 0.520
[15,     1] loss: 0.509
[16,     1] loss: 0.493
[17,     1] loss: 0.483
[18,     1] loss: 0.468
[19,     1] loss: 0.455
[20,     1] loss: 0.443
[21,     1] loss: 0.430
[22,     1] loss: 0.420
[23,     1] loss: 0.409
[24,     1] loss: 0.401
[25,     1] loss: 0.392
[26,     1] loss: 0.383
[27,     1] loss: 0.374
[28,     1] loss: 0.367
[29,     1] loss: 0.361
[30,     1] loss: 0.355
[31,     1] loss: 0.349
[32,     1] loss: 0.343
[33,     1] loss: 0.337
[34,     1] loss: 0.334
[35,     1] loss: 0.326
[36,     1] loss: 0.322
[37,     1] loss: 0.318
[38,     1] loss: 0.313
[39,     1] loss: 0.309
[40,     1] loss: 0.303
[41,     1] loss: 0.298
[42,     1] loss: 0.293
[43,     1] loss: 0.289
[44,     1] loss: 0.285
[45,     1] loss: 0.278
[46,     1] loss: 0.272
[47,     1] loss: 0.273
[48,     1] loss: 0.269
[49,     1] loss: 0.272
[50,     1] loss: 0.264
[51,     1] loss: 0.259
[52,     1] loss: 0.257
[53,     1] loss: 0.253
[54,     1] loss: 0.249
[55,     1] loss: 0.246
[56,     1] loss: 0.241
[57,     1] loss: 0.237
[58,     1] loss: 0.237
[59,     1] loss: 0.233
[60,     1] loss: 0.229
[61,     1] loss: 0.226
[62,     1] loss: 0.226
[63,     1] loss: 0.223
[64,     1] loss: 0.218
[65,     1] loss: 0.219
[66,     1] loss: 0.215
[67,     1] loss: 0.215
[68,     1] loss: 0.214
[69,     1] loss: 0.211
[70,     1] loss: 0.211
[71,     1] loss: 0.208
[72,     1] loss: 0.208
[73,     1] loss: 0.205
[74,     1] loss: 0.206
[75,     1] loss: 0.204
[76,     1] loss: 0.204
[77,     1] loss: 0.204
[78,     1] loss: 0.207
[79,     1] loss: 0.216
[80,     1] loss: 0.233
[81,     1] loss: 0.327
[82,     1] loss: 0.346
[83,     1] loss: 0.319
[84,     1] loss: 0.305
[85,     1] loss: 0.292
[86,     1] loss: 0.281
[87,     1] loss: 0.276
[88,     1] loss: 0.266
[89,     1] loss: 0.263
[90,     1] loss: 0.262
[91,     1] loss: 0.259
[92,     1] loss: 0.256
[93,     1] loss: 0.254
[94,     1] loss: 0.254
[95,     1] loss: 0.248
[96,     1] loss: 0.247
[97,     1] loss: 0.246
[98,     1] loss: 0.244
[99,     1] loss: 0.243
[100,     1] loss: 0.242
[101,     1] loss: 0.244
[102,     1] loss: 0.241
[103,     1] loss: 0.239
[104,     1] loss: 0.238
[105,     1] loss: 0.238
[106,     1] loss: 0.238
[107,     1] loss: 0.235
[108,     1] loss: 0.237
[109,     1] loss: 0.234
[110,     1] loss: 0.236
[111,     1] loss: 0.233
[112,     1] loss: 0.233
[113,     1] loss: 0.234
[114,     1] loss: 0.231
[115,     1] loss: 0.231
[116,     1] loss: 0.224
[117,     1] loss: 0.236
[118,     1] loss: 0.253
[119,     1] loss: 0.252
[120,     1] loss: 0.247
[121,     1] loss: 0.247
[122,     1] loss: 0.243
[123,     1] loss: 0.235
[124,     1] loss: 0.233
[125,     1] loss: 0.224
[126,     1] loss: 0.220
[127,     1] loss: 0.214
[128,     1] loss: 0.209
[129,     1] loss: 0.212
[130,     1] loss: 0.225
[131,     1] loss: 0.247
[132,     1] loss: 0.246
[133,     1] loss: 0.243
[134,     1] loss: 0.237
[135,     1] loss: 0.233
[136,     1] loss: 0.226
[137,     1] loss: 0.224
[138,     1] loss: 0.222
[139,     1] loss: 0.214
[140,     1] loss: 0.211
[141,     1] loss: 0.211
[142,     1] loss: 0.209
[143,     1] loss: 0.202
[144,     1] loss: 0.200
[145,     1] loss: 0.198
[146,     1] loss: 0.193
[147,     1] loss: 0.192
[148,     1] loss: 0.189
[149,     1] loss: 0.185
[150,     1] loss: 0.187
[151,     1] loss: 0.187
[152,     1] loss: 0.186
[153,     1] loss: 0.186
[154,     1] loss: 0.185
[155,     1] loss: 0.183
[156,     1] loss: 0.182
[157,     1] loss: 0.184
[158,     1] loss: 0.184
[159,     1] loss: 0.183
[160,     1] loss: 0.183
[161,     1] loss: 0.188
[162,     1] loss: 0.183
[163,     1] loss: 0.187
[164,     1] loss: 0.186
[165,     1] loss: 0.184
[166,     1] loss: 0.187
[167,     1] loss: 0.187
[168,     1] loss: 0.187
[169,     1] loss: 0.186
[170,     1] loss: 0.186
[171,     1] loss: 0.189
[172,     1] loss: 0.189
[173,     1] loss: 0.188
[174,     1] loss: 0.188
[175,     1] loss: 0.189
[176,     1] loss: 0.190
[177,     1] loss: 0.191
[178,     1] loss: 0.189
[179,     1] loss: 0.190
[180,     1] loss: 0.193
[181,     1] loss: 0.192
[182,     1] loss: 0.192
[183,     1] loss: 0.194
[184,     1] loss: 0.192
[185,     1] loss: 0.194
[186,     1] loss: 0.194
[187,     1] loss: 0.204
[188,     1] loss: 0.486
[189,     1] loss: 0.620
[190,     1] loss: 0.540
[191,     1] loss: 0.495
[192,     1] loss: 0.476
[193,     1] loss: 0.462
[194,     1] loss: 0.450
[195,     1] loss: 0.443
[196,     1] loss: 0.445
[197,     1] loss: 0.441
[198,     1] loss: 0.440
[199,     1] loss: 0.442
[200,     1] loss: 0.439
Finished Training
Total time taken: 313.9454257488251
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.689
[3,     1] loss: 0.676
[4,     1] loss: 0.666
[5,     1] loss: 0.656
[6,     1] loss: 0.646
[7,     1] loss: 0.636
[8,     1] loss: 0.627
[9,     1] loss: 0.619
[10,     1] loss: 0.608
[11,     1] loss: 0.599
[12,     1] loss: 0.589
[13,     1] loss: 0.579
[14,     1] loss: 0.568
[15,     1] loss: 0.558
[16,     1] loss: 0.548
[17,     1] loss: 0.536
[18,     1] loss: 0.524
[19,     1] loss: 0.514
[20,     1] loss: 0.502
[21,     1] loss: 0.492
[22,     1] loss: 0.477
[23,     1] loss: 0.468
[24,     1] loss: 0.455
[25,     1] loss: 0.445
[26,     1] loss: 0.435
[27,     1] loss: 0.425
[28,     1] loss: 0.413
[29,     1] loss: 0.404
[30,     1] loss: 0.395
[31,     1] loss: 0.384
[32,     1] loss: 0.378
[33,     1] loss: 0.367
[34,     1] loss: 0.359
[35,     1] loss: 0.348
[36,     1] loss: 0.340
[37,     1] loss: 0.331
[38,     1] loss: 0.324
[39,     1] loss: 0.320
[40,     1] loss: 0.310
[41,     1] loss: 0.303
[42,     1] loss: 0.298
[43,     1] loss: 0.293
[44,     1] loss: 0.286
[45,     1] loss: 0.280
[46,     1] loss: 0.274
[47,     1] loss: 0.270
[48,     1] loss: 0.268
[49,     1] loss: 0.260
[50,     1] loss: 0.258
[51,     1] loss: 0.255
[52,     1] loss: 0.250
[53,     1] loss: 0.249
[54,     1] loss: 0.246
[55,     1] loss: 0.246
[56,     1] loss: 0.243
[57,     1] loss: 0.239
[58,     1] loss: 0.240
[59,     1] loss: 0.238
[60,     1] loss: 0.266
[61,     1] loss: 0.255
[62,     1] loss: 0.255
[63,     1] loss: 0.252
[64,     1] loss: 0.251
[65,     1] loss: 0.248
[66,     1] loss: 0.248
[67,     1] loss: 0.246
[68,     1] loss: 0.244
[69,     1] loss: 0.243
[70,     1] loss: 0.243
[71,     1] loss: 0.240
[72,     1] loss: 0.240
[73,     1] loss: 0.241
[74,     1] loss: 0.239
[75,     1] loss: 0.237
[76,     1] loss: 0.238
[77,     1] loss: 0.238
[78,     1] loss: 0.240
[79,     1] loss: 0.238
[80,     1] loss: 0.241
[81,     1] loss: 0.240
[82,     1] loss: 0.236
[83,     1] loss: 0.239
[84,     1] loss: 0.238
[85,     1] loss: 0.239
[86,     1] loss: 0.239
[87,     1] loss: 0.240
[88,     1] loss: 0.245
[89,     1] loss: 0.282
[90,     1] loss: 0.299
[91,     1] loss: 0.424
[92,     1] loss: 0.318
[93,     1] loss: 0.334
[94,     1] loss: 0.320
[95,     1] loss: 0.307
[96,     1] loss: 0.305
[97,     1] loss: 0.298
[98,     1] loss: 0.292
[99,     1] loss: 0.292
[100,     1] loss: 0.286
[101,     1] loss: 0.284
[102,     1] loss: 0.277
[103,     1] loss: 0.276
[104,     1] loss: 0.272
[105,     1] loss: 0.268
[106,     1] loss: 0.265
[107,     1] loss: 0.264
[108,     1] loss: 0.260
[109,     1] loss: 0.259
[110,     1] loss: 0.258
[111,     1] loss: 0.254
[112,     1] loss: 0.253
[113,     1] loss: 0.251
[114,     1] loss: 0.251
[115,     1] loss: 0.250
[116,     1] loss: 0.248
[117,     1] loss: 0.245
[118,     1] loss: 0.250
[119,     1] loss: 0.248
[120,     1] loss: 0.246
[121,     1] loss: 0.245
[122,     1] loss: 0.244
[123,     1] loss: 0.245
[124,     1] loss: 0.245
[125,     1] loss: 0.243
[126,     1] loss: 0.246
[127,     1] loss: 0.242
[128,     1] loss: 0.244
[129,     1] loss: 0.255
[130,     1] loss: 0.283
[131,     1] loss: 0.273
[132,     1] loss: 0.268
[133,     1] loss: 0.271
[134,     1] loss: 0.272
[135,     1] loss: 0.269
[136,     1] loss: 0.266
[137,     1] loss: 0.264
[138,     1] loss: 0.265
[139,     1] loss: 0.261
[140,     1] loss: 0.259
[141,     1] loss: 0.256
[142,     1] loss: 0.258
[143,     1] loss: 0.254
[144,     1] loss: 0.255
[145,     1] loss: 0.252
[146,     1] loss: 0.257
[147,     1] loss: 0.273
[148,     1] loss: 0.260
[149,     1] loss: 0.264
[150,     1] loss: 0.261
[151,     1] loss: 0.257
[152,     1] loss: 0.255
[153,     1] loss: 0.252
[154,     1] loss: 0.250
[155,     1] loss: 0.251
[156,     1] loss: 0.249
[157,     1] loss: 0.247
[158,     1] loss: 0.245
[159,     1] loss: 0.247
[160,     1] loss: 0.245
[161,     1] loss: 0.242
[162,     1] loss: 0.244
[163,     1] loss: 0.242
[164,     1] loss: 0.242
[165,     1] loss: 0.242
[166,     1] loss: 0.243
[167,     1] loss: 0.241
[168,     1] loss: 0.240
[169,     1] loss: 0.240
[170,     1] loss: 0.241
[171,     1] loss: 0.241
[172,     1] loss: 0.242
Early stopping applied (best metric=0.3697573244571686)
Finished Training
Total time taken: 273.1422863006592
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.680
[3,     1] loss: 0.661
[4,     1] loss: 0.647
[5,     1] loss: 0.637
[6,     1] loss: 0.624
[7,     1] loss: 0.615
[8,     1] loss: 0.604
[9,     1] loss: 0.594
[10,     1] loss: 0.583
[11,     1] loss: 0.576
[12,     1] loss: 0.565
[13,     1] loss: 0.553
[14,     1] loss: 0.545
[15,     1] loss: 0.535
[16,     1] loss: 0.523
[17,     1] loss: 0.514
[18,     1] loss: 0.502
[19,     1] loss: 0.495
[20,     1] loss: 0.482
[21,     1] loss: 0.472
[22,     1] loss: 0.464
[23,     1] loss: 0.452
[24,     1] loss: 0.441
[25,     1] loss: 0.432
[26,     1] loss: 0.423
[27,     1] loss: 0.412
[28,     1] loss: 0.402
[29,     1] loss: 0.393
[30,     1] loss: 0.384
[31,     1] loss: 0.374
[32,     1] loss: 0.367
[33,     1] loss: 0.359
[34,     1] loss: 0.348
[35,     1] loss: 0.342
[36,     1] loss: 0.335
[37,     1] loss: 0.326
[38,     1] loss: 0.319
[39,     1] loss: 0.313
[40,     1] loss: 0.305
[41,     1] loss: 0.296
[42,     1] loss: 0.291
[43,     1] loss: 0.287
[44,     1] loss: 0.282
[45,     1] loss: 0.276
[46,     1] loss: 0.270
[47,     1] loss: 0.266
[48,     1] loss: 0.264
[49,     1] loss: 0.258
[50,     1] loss: 0.255
[51,     1] loss: 0.251
[52,     1] loss: 0.248
[53,     1] loss: 0.246
[54,     1] loss: 0.243
[55,     1] loss: 0.242
[56,     1] loss: 0.238
[57,     1] loss: 0.235
[58,     1] loss: 0.237
[59,     1] loss: 0.235
[60,     1] loss: 0.236
[61,     1] loss: 0.242
[62,     1] loss: 0.258
[63,     1] loss: 0.255
[64,     1] loss: 0.255
[65,     1] loss: 0.253
[66,     1] loss: 0.248
[67,     1] loss: 0.247
[68,     1] loss: 0.248
[69,     1] loss: 0.243
[70,     1] loss: 0.244
[71,     1] loss: 0.241
[72,     1] loss: 0.239
[73,     1] loss: 0.239
[74,     1] loss: 0.239
[75,     1] loss: 0.238
[76,     1] loss: 0.238
[77,     1] loss: 0.235
[78,     1] loss: 0.237
[79,     1] loss: 0.234
[80,     1] loss: 0.234
[81,     1] loss: 0.235
[82,     1] loss: 0.236
[83,     1] loss: 0.232
[84,     1] loss: 0.232
[85,     1] loss: 0.232
[86,     1] loss: 0.233
[87,     1] loss: 0.234
[88,     1] loss: 0.233
[89,     1] loss: 0.232
[90,     1] loss: 0.240
[91,     1] loss: 0.256
[92,     1] loss: 0.265
[93,     1] loss: 0.257
[94,     1] loss: 0.257
[95,     1] loss: 0.254
[96,     1] loss: 0.252
[97,     1] loss: 0.247
[98,     1] loss: 0.245
[99,     1] loss: 0.240
[100,     1] loss: 0.235
[101,     1] loss: 0.231
[102,     1] loss: 0.230
[103,     1] loss: 0.244
[104,     1] loss: 0.232
[105,     1] loss: 0.238
[106,     1] loss: 0.226
[107,     1] loss: 0.224
[108,     1] loss: 0.222
[109,     1] loss: 0.220
[110,     1] loss: 0.212
[111,     1] loss: 0.213
[112,     1] loss: 0.209
[113,     1] loss: 0.207
Early stopping applied (best metric=0.3633003532886505)
Finished Training
Total time taken: 180.6432032585144
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.689
[3,     1] loss: 0.670
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.635
[7,     1] loss: 0.625
[8,     1] loss: 0.612
[9,     1] loss: 0.603
[10,     1] loss: 0.592
[11,     1] loss: 0.580
[12,     1] loss: 0.569
[13,     1] loss: 0.554
[14,     1] loss: 0.540
[15,     1] loss: 0.525
[16,     1] loss: 0.512
[17,     1] loss: 0.497
[18,     1] loss: 0.483
[19,     1] loss: 0.469
[20,     1] loss: 0.454
[21,     1] loss: 0.440
[22,     1] loss: 0.427
[23,     1] loss: 0.417
[24,     1] loss: 0.405
[25,     1] loss: 0.394
[26,     1] loss: 0.386
[27,     1] loss: 0.375
[28,     1] loss: 0.369
[29,     1] loss: 0.360
[30,     1] loss: 0.354
[31,     1] loss: 0.347
[32,     1] loss: 0.342
[33,     1] loss: 0.336
[34,     1] loss: 0.331
[35,     1] loss: 0.326
[36,     1] loss: 0.321
[37,     1] loss: 0.316
[38,     1] loss: 0.312
[39,     1] loss: 0.307
[40,     1] loss: 0.304
[41,     1] loss: 0.299
[42,     1] loss: 0.296
[43,     1] loss: 0.293
[44,     1] loss: 0.289
[45,     1] loss: 0.288
[46,     1] loss: 0.286
[47,     1] loss: 0.283
[48,     1] loss: 0.280
[49,     1] loss: 0.276
[50,     1] loss: 0.273
[51,     1] loss: 0.272
[52,     1] loss: 0.270
[53,     1] loss: 0.267
[54,     1] loss: 0.265
[55,     1] loss: 0.261
[56,     1] loss: 0.258
[57,     1] loss: 0.258
[58,     1] loss: 0.256
[59,     1] loss: 0.254
[60,     1] loss: 0.252
[61,     1] loss: 0.253
[62,     1] loss: 0.250
[63,     1] loss: 0.248
[64,     1] loss: 0.247
[65,     1] loss: 0.247
[66,     1] loss: 0.244
[67,     1] loss: 0.244
[68,     1] loss: 0.239
[69,     1] loss: 0.239
[70,     1] loss: 0.239
[71,     1] loss: 0.238
[72,     1] loss: 0.234
[73,     1] loss: 0.234
[74,     1] loss: 0.233
[75,     1] loss: 0.234
[76,     1] loss: 0.234
[77,     1] loss: 0.232
[78,     1] loss: 0.232
[79,     1] loss: 0.233
[80,     1] loss: 0.230
[81,     1] loss: 0.229
[82,     1] loss: 0.231
[83,     1] loss: 0.228
[84,     1] loss: 0.229
[85,     1] loss: 0.227
[86,     1] loss: 0.230
[87,     1] loss: 0.227
[88,     1] loss: 0.230
[89,     1] loss: 0.227
[90,     1] loss: 0.226
[91,     1] loss: 0.229
[92,     1] loss: 0.239
[93,     1] loss: 0.339
[94,     1] loss: 0.608
[95,     1] loss: 0.549
[96,     1] loss: 0.504
[97,     1] loss: 0.474
[98,     1] loss: 0.449
[99,     1] loss: 0.427
[100,     1] loss: 0.420
[101,     1] loss: 0.416
[102,     1] loss: 0.411
[103,     1] loss: 0.398
[104,     1] loss: 0.383
[105,     1] loss: 0.374
[106,     1] loss: 0.366
[107,     1] loss: 0.356
[108,     1] loss: 0.355
[109,     1] loss: 0.350
[110,     1] loss: 0.347
[111,     1] loss: 0.343
[112,     1] loss: 0.340
[113,     1] loss: 0.339
[114,     1] loss: 0.333
[115,     1] loss: 0.332
[116,     1] loss: 0.328
[117,     1] loss: 0.326
[118,     1] loss: 0.324
[119,     1] loss: 0.324
[120,     1] loss: 0.319
[121,     1] loss: 0.318
[122,     1] loss: 0.320
[123,     1] loss: 0.316
[124,     1] loss: 0.314
[125,     1] loss: 0.313
[126,     1] loss: 0.310
[127,     1] loss: 0.310
[128,     1] loss: 0.310
[129,     1] loss: 0.307
[130,     1] loss: 0.309
[131,     1] loss: 0.307
[132,     1] loss: 0.306
[133,     1] loss: 0.305
[134,     1] loss: 0.307
[135,     1] loss: 0.303
[136,     1] loss: 0.303
[137,     1] loss: 0.304
[138,     1] loss: 0.304
[139,     1] loss: 0.306
Early stopping applied (best metric=0.38837751746177673)
Finished Training
Total time taken: 223.02761149406433
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.696
[3,     1] loss: 0.688
[4,     1] loss: 0.679
[5,     1] loss: 0.672
[6,     1] loss: 0.667
[7,     1] loss: 0.660
[8,     1] loss: 0.655
[9,     1] loss: 0.647
[10,     1] loss: 0.639
[11,     1] loss: 0.634
[12,     1] loss: 0.628
[13,     1] loss: 0.621
[14,     1] loss: 0.614
[15,     1] loss: 0.606
[16,     1] loss: 0.599
[17,     1] loss: 0.590
[18,     1] loss: 0.583
[19,     1] loss: 0.573
[20,     1] loss: 0.564
[21,     1] loss: 0.554
[22,     1] loss: 0.545
[23,     1] loss: 0.536
[24,     1] loss: 0.526
[25,     1] loss: 0.513
[26,     1] loss: 0.505
[27,     1] loss: 0.492
[28,     1] loss: 0.484
[29,     1] loss: 0.474
[30,     1] loss: 0.463
[31,     1] loss: 0.453
[32,     1] loss: 0.445
[33,     1] loss: 0.434
[34,     1] loss: 0.427
[35,     1] loss: 0.418
[36,     1] loss: 0.409
[37,     1] loss: 0.401
[38,     1] loss: 0.394
[39,     1] loss: 0.388
[40,     1] loss: 0.379
[41,     1] loss: 0.373
[42,     1] loss: 0.368
[43,     1] loss: 0.374
[44,     1] loss: 0.358
[45,     1] loss: 0.354
[46,     1] loss: 0.347
[47,     1] loss: 0.342
[48,     1] loss: 0.337
[49,     1] loss: 0.332
[50,     1] loss: 0.328
[51,     1] loss: 0.323
[52,     1] loss: 0.318
[53,     1] loss: 0.315
[54,     1] loss: 0.315
[55,     1] loss: 0.307
[56,     1] loss: 0.303
[57,     1] loss: 0.300
[58,     1] loss: 0.297
[59,     1] loss: 0.296
[60,     1] loss: 0.290
[61,     1] loss: 0.289
[62,     1] loss: 0.286
[63,     1] loss: 0.282
[64,     1] loss: 0.282
[65,     1] loss: 0.281
[66,     1] loss: 0.279
[67,     1] loss: 0.276
[68,     1] loss: 0.275
[69,     1] loss: 0.273
[70,     1] loss: 0.271
[71,     1] loss: 0.270
[72,     1] loss: 0.268
[73,     1] loss: 0.267
[74,     1] loss: 0.266
[75,     1] loss: 0.267
[76,     1] loss: 0.263
[77,     1] loss: 0.265
[78,     1] loss: 0.267
[79,     1] loss: 0.338
[80,     1] loss: 0.345
[81,     1] loss: 0.457
[82,     1] loss: 0.341
[83,     1] loss: 0.373
[84,     1] loss: 0.356
[85,     1] loss: 0.347
[86,     1] loss: 0.339
[87,     1] loss: 0.335
[88,     1] loss: 0.328
[89,     1] loss: 0.324
[90,     1] loss: 0.320
[91,     1] loss: 0.312
[92,     1] loss: 0.310
[93,     1] loss: 0.304
[94,     1] loss: 0.305
[95,     1] loss: 0.298
[96,     1] loss: 0.294
[97,     1] loss: 0.293
[98,     1] loss: 0.290
[99,     1] loss: 0.287
[100,     1] loss: 0.287
[101,     1] loss: 0.284
[102,     1] loss: 0.282
[103,     1] loss: 0.281
[104,     1] loss: 0.280
[105,     1] loss: 0.277
[106,     1] loss: 0.277
[107,     1] loss: 0.276
[108,     1] loss: 0.276
[109,     1] loss: 0.275
[110,     1] loss: 0.275
[111,     1] loss: 0.275
[112,     1] loss: 0.275
[113,     1] loss: 0.272
[114,     1] loss: 0.275
[115,     1] loss: 0.274
[116,     1] loss: 0.275
Early stopping applied (best metric=0.4158499836921692)
Finished Training
Total time taken: 187.2744801044464
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.668
[4,     1] loss: 0.659
[5,     1] loss: 0.652
[6,     1] loss: 0.643
[7,     1] loss: 0.636
[8,     1] loss: 0.628
[9,     1] loss: 0.621
[10,     1] loss: 0.613
[11,     1] loss: 0.606
[12,     1] loss: 0.596
[13,     1] loss: 0.589
[14,     1] loss: 0.580
[15,     1] loss: 0.570
[16,     1] loss: 0.559
[17,     1] loss: 0.549
[18,     1] loss: 0.539
[19,     1] loss: 0.526
[20,     1] loss: 0.517
[21,     1] loss: 0.504
[22,     1] loss: 0.493
[23,     1] loss: 0.482
[24,     1] loss: 0.472
[25,     1] loss: 0.462
[26,     1] loss: 0.453
[27,     1] loss: 0.442
[28,     1] loss: 0.435
[29,     1] loss: 0.431
[30,     1] loss: 0.421
[31,     1] loss: 0.414
[32,     1] loss: 0.410
[33,     1] loss: 0.402
[34,     1] loss: 0.400
[35,     1] loss: 0.393
[36,     1] loss: 0.386
[37,     1] loss: 0.381
[38,     1] loss: 0.378
[39,     1] loss: 0.374
[40,     1] loss: 0.369
[41,     1] loss: 0.367
[42,     1] loss: 0.364
[43,     1] loss: 0.361
[44,     1] loss: 0.359
[45,     1] loss: 0.356
[46,     1] loss: 0.352
[47,     1] loss: 0.351
[48,     1] loss: 0.349
[49,     1] loss: 0.346
[50,     1] loss: 0.343
[51,     1] loss: 0.343
[52,     1] loss: 0.341
[53,     1] loss: 0.338
[54,     1] loss: 0.336
[55,     1] loss: 0.334
[56,     1] loss: 0.334
[57,     1] loss: 0.333
[58,     1] loss: 0.329
[59,     1] loss: 0.328
[60,     1] loss: 0.326
[61,     1] loss: 0.322
[62,     1] loss: 0.319
[63,     1] loss: 0.319
[64,     1] loss: 0.316
[65,     1] loss: 0.315
[66,     1] loss: 0.312
[67,     1] loss: 0.309
[68,     1] loss: 0.308
[69,     1] loss: 0.303
[70,     1] loss: 0.302
[71,     1] loss: 0.300
[72,     1] loss: 0.299
[73,     1] loss: 0.358
[74,     1] loss: 0.328
[75,     1] loss: 0.359
[76,     1] loss: 0.334
[77,     1] loss: 0.338
[78,     1] loss: 0.328
[79,     1] loss: 0.317
[80,     1] loss: 0.308
[81,     1] loss: 0.301
[82,     1] loss: 0.295
[83,     1] loss: 0.289
[84,     1] loss: 0.282
[85,     1] loss: 0.279
[86,     1] loss: 0.271
[87,     1] loss: 0.268
[88,     1] loss: 0.263
[89,     1] loss: 0.258
[90,     1] loss: 0.253
[91,     1] loss: 0.250
[92,     1] loss: 0.244
[93,     1] loss: 0.240
[94,     1] loss: 0.237
[95,     1] loss: 0.232
[96,     1] loss: 0.228
[97,     1] loss: 0.225
[98,     1] loss: 0.221
[99,     1] loss: 0.217
[100,     1] loss: 0.212
[101,     1] loss: 0.210
[102,     1] loss: 0.207
[103,     1] loss: 0.205
[104,     1] loss: 0.197
[105,     1] loss: 0.198
[106,     1] loss: 0.196
[107,     1] loss: 0.191
[108,     1] loss: 0.186
[109,     1] loss: 0.184
[110,     1] loss: 0.185
[111,     1] loss: 0.179
[112,     1] loss: 0.180
[113,     1] loss: 0.174
[114,     1] loss: 0.175
[115,     1] loss: 0.169
[116,     1] loss: 0.168
[117,     1] loss: 0.166
[118,     1] loss: 0.165
[119,     1] loss: 0.163
[120,     1] loss: 0.162
[121,     1] loss: 0.158
[122,     1] loss: 0.156
[123,     1] loss: 0.154
[124,     1] loss: 0.156
[125,     1] loss: 0.156
[126,     1] loss: 0.159
Early stopping applied (best metric=0.4056094288825989)
Finished Training
Total time taken: 204.08665442466736
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.678
[3,     1] loss: 0.656
[4,     1] loss: 0.641
[5,     1] loss: 0.632
[6,     1] loss: 0.618
[7,     1] loss: 0.609
[8,     1] loss: 0.599
[9,     1] loss: 0.588
[10,     1] loss: 0.577
[11,     1] loss: 0.566
[12,     1] loss: 0.556
[13,     1] loss: 0.543
[14,     1] loss: 0.534
[15,     1] loss: 0.523
[16,     1] loss: 0.510
[17,     1] loss: 0.499
[18,     1] loss: 0.487
[19,     1] loss: 0.479
[20,     1] loss: 0.467
[21,     1] loss: 0.456
[22,     1] loss: 0.444
[23,     1] loss: 0.434
[24,     1] loss: 0.427
[25,     1] loss: 0.417
[26,     1] loss: 0.407
[27,     1] loss: 0.400
[28,     1] loss: 0.388
[29,     1] loss: 0.378
[30,     1] loss: 0.370
[31,     1] loss: 0.361
[32,     1] loss: 0.352
[33,     1] loss: 0.342
[34,     1] loss: 0.335
[35,     1] loss: 0.326
[36,     1] loss: 0.319
[37,     1] loss: 0.309
[38,     1] loss: 0.302
[39,     1] loss: 0.293
[40,     1] loss: 0.289
[41,     1] loss: 0.283
[42,     1] loss: 0.277
[43,     1] loss: 0.268
[44,     1] loss: 0.262
[45,     1] loss: 0.256
[46,     1] loss: 0.251
[47,     1] loss: 0.247
[48,     1] loss: 0.243
[49,     1] loss: 0.239
[50,     1] loss: 0.234
[51,     1] loss: 0.228
[52,     1] loss: 0.228
[53,     1] loss: 0.223
[54,     1] loss: 0.222
[55,     1] loss: 0.218
[56,     1] loss: 0.216
[57,     1] loss: 0.214
[58,     1] loss: 0.211
[59,     1] loss: 0.211
[60,     1] loss: 0.207
[61,     1] loss: 0.207
[62,     1] loss: 0.205
[63,     1] loss: 0.203
[64,     1] loss: 0.203
[65,     1] loss: 0.203
[66,     1] loss: 0.204
[67,     1] loss: 0.204
[68,     1] loss: 0.203
[69,     1] loss: 0.204
[70,     1] loss: 0.206
[71,     1] loss: 0.206
[72,     1] loss: 0.212
[73,     1] loss: 0.214
[74,     1] loss: 0.219
[75,     1] loss: 0.214
[76,     1] loss: 0.210
[77,     1] loss: 0.208
[78,     1] loss: 0.207
[79,     1] loss: 0.208
[80,     1] loss: 0.205
[81,     1] loss: 0.206
[82,     1] loss: 0.205
[83,     1] loss: 0.204
[84,     1] loss: 0.205
[85,     1] loss: 0.203
[86,     1] loss: 0.203
[87,     1] loss: 0.205
[88,     1] loss: 0.204
[89,     1] loss: 0.203
[90,     1] loss: 0.204
[91,     1] loss: 0.203
[92,     1] loss: 0.204
[93,     1] loss: 0.204
[94,     1] loss: 0.203
[95,     1] loss: 0.203
[96,     1] loss: 0.203
[97,     1] loss: 0.202
[98,     1] loss: 0.205
[99,     1] loss: 0.205
[100,     1] loss: 0.204
[101,     1] loss: 0.207
[102,     1] loss: 0.254
[103,     1] loss: 0.569
[104,     1] loss: 0.374
[105,     1] loss: 0.407
[106,     1] loss: 0.358
[107,     1] loss: 0.345
[108,     1] loss: 0.332
[109,     1] loss: 0.306
[110,     1] loss: 0.301
[111,     1] loss: 0.300
[112,     1] loss: 0.297
[113,     1] loss: 0.292
[114,     1] loss: 0.288
[115,     1] loss: 0.281
[116,     1] loss: 0.280
[117,     1] loss: 0.276
[118,     1] loss: 0.275
[119,     1] loss: 0.268
[120,     1] loss: 0.263
[121,     1] loss: 0.259
[122,     1] loss: 0.255
[123,     1] loss: 0.251
[124,     1] loss: 0.244
[125,     1] loss: 0.238
[126,     1] loss: 0.233
[127,     1] loss: 0.230
[128,     1] loss: 0.226
[129,     1] loss: 0.224
[130,     1] loss: 0.219
[131,     1] loss: 0.218
[132,     1] loss: 0.216
[133,     1] loss: 0.212
[134,     1] loss: 0.208
[135,     1] loss: 0.206
[136,     1] loss: 0.204
[137,     1] loss: 0.204
[138,     1] loss: 0.201
[139,     1] loss: 0.199
[140,     1] loss: 0.201
[141,     1] loss: 0.200
[142,     1] loss: 0.197
[143,     1] loss: 0.193
[144,     1] loss: 0.198
[145,     1] loss: 0.194
[146,     1] loss: 0.192
[147,     1] loss: 0.193
[148,     1] loss: 0.194
[149,     1] loss: 0.191
[150,     1] loss: 0.192
Early stopping applied (best metric=0.39905083179473877)
Finished Training
Total time taken: 243.92562651634216
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.677
[4,     1] loss: 0.668
[5,     1] loss: 0.661
[6,     1] loss: 0.653
[7,     1] loss: 0.646
[8,     1] loss: 0.639
[9,     1] loss: 0.631
[10,     1] loss: 0.625
[11,     1] loss: 0.615
[12,     1] loss: 0.608
[13,     1] loss: 0.598
[14,     1] loss: 0.589
[15,     1] loss: 0.578
[16,     1] loss: 0.570
[17,     1] loss: 0.562
[18,     1] loss: 0.551
[19,     1] loss: 0.542
[20,     1] loss: 0.532
[21,     1] loss: 0.520
[22,     1] loss: 0.510
[23,     1] loss: 0.500
[24,     1] loss: 0.491
[25,     1] loss: 0.482
[26,     1] loss: 0.482
[27,     1] loss: 0.474
[28,     1] loss: 0.460
[29,     1] loss: 0.457
[30,     1] loss: 0.449
[31,     1] loss: 0.442
[32,     1] loss: 0.438
[33,     1] loss: 0.432
[34,     1] loss: 0.427
[35,     1] loss: 0.422
[36,     1] loss: 0.419
[37,     1] loss: 0.416
[38,     1] loss: 0.410
[39,     1] loss: 0.408
[40,     1] loss: 0.404
[41,     1] loss: 0.402
[42,     1] loss: 0.398
[43,     1] loss: 0.395
[44,     1] loss: 0.393
[45,     1] loss: 0.392
[46,     1] loss: 0.389
[47,     1] loss: 0.385
[48,     1] loss: 0.383
[49,     1] loss: 0.384
[50,     1] loss: 0.379
[51,     1] loss: 0.378
[52,     1] loss: 0.375
[53,     1] loss: 0.374
[54,     1] loss: 0.370
[55,     1] loss: 0.367
[56,     1] loss: 0.364
[57,     1] loss: 0.357
[58,     1] loss: 0.355
[59,     1] loss: 0.350
[60,     1] loss: 0.345
[61,     1] loss: 0.342
[62,     1] loss: 0.336
[63,     1] loss: 0.331
[64,     1] loss: 0.323
[65,     1] loss: 0.317
[66,     1] loss: 0.308
[67,     1] loss: 0.300
[68,     1] loss: 0.299
[69,     1] loss: 0.355
[70,     1] loss: 0.321
[71,     1] loss: 0.339
[72,     1] loss: 0.328
[73,     1] loss: 0.299
[74,     1] loss: 0.296
[75,     1] loss: 0.284
[76,     1] loss: 0.266
[77,     1] loss: 0.259
[78,     1] loss: 0.244
[79,     1] loss: 0.233
[80,     1] loss: 0.222
[81,     1] loss: 0.211
[82,     1] loss: 0.201
[83,     1] loss: 0.192
[84,     1] loss: 0.182
[85,     1] loss: 0.172
[86,     1] loss: 0.169
[87,     1] loss: 0.160
[88,     1] loss: 0.154
[89,     1] loss: 0.147
[90,     1] loss: 0.141
[91,     1] loss: 0.140
[92,     1] loss: 0.133
[93,     1] loss: 0.133
[94,     1] loss: 0.131
[95,     1] loss: 0.125
[96,     1] loss: 0.122
[97,     1] loss: 0.122
[98,     1] loss: 0.118
[99,     1] loss: 0.118
[100,     1] loss: 0.117
[101,     1] loss: 0.116
[102,     1] loss: 0.115
[103,     1] loss: 0.117
[104,     1] loss: 0.116
[105,     1] loss: 0.116
[106,     1] loss: 0.116
[107,     1] loss: 0.115
[108,     1] loss: 0.118
[109,     1] loss: 0.117
[110,     1] loss: 0.120
[111,     1] loss: 0.117
[112,     1] loss: 0.118
[113,     1] loss: 0.119
[114,     1] loss: 0.121
[115,     1] loss: 0.120
[116,     1] loss: 0.119
[117,     1] loss: 0.120
[118,     1] loss: 0.122
[119,     1] loss: 0.122
[120,     1] loss: 0.124
[121,     1] loss: 0.124
[122,     1] loss: 0.125
[123,     1] loss: 0.123
[124,     1] loss: 0.127
[125,     1] loss: 0.126
[126,     1] loss: 0.126
[127,     1] loss: 0.127
[128,     1] loss: 0.127
[129,     1] loss: 0.130
[130,     1] loss: 0.162
[131,     1] loss: 0.314
[132,     1] loss: 0.344
[133,     1] loss: 0.390
[134,     1] loss: 0.298
[135,     1] loss: 0.311
[136,     1] loss: 0.299
[137,     1] loss: 0.287
[138,     1] loss: 0.276
[139,     1] loss: 0.267
[140,     1] loss: 0.260
[141,     1] loss: 0.252
[142,     1] loss: 0.246
[143,     1] loss: 0.241
[144,     1] loss: 0.234
[145,     1] loss: 0.226
[146,     1] loss: 0.219
[147,     1] loss: 0.219
[148,     1] loss: 0.209
[149,     1] loss: 0.203
[150,     1] loss: 0.202
[151,     1] loss: 0.197
[152,     1] loss: 0.192
[153,     1] loss: 0.190
[154,     1] loss: 0.185
[155,     1] loss: 0.183
[156,     1] loss: 0.180
Early stopping applied (best metric=0.30490371584892273)
Finished Training
Total time taken: 255.13367176055908
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.644
[6,     1] loss: 0.635
[7,     1] loss: 0.627
[8,     1] loss: 0.617
[9,     1] loss: 0.610
[10,     1] loss: 0.601
[11,     1] loss: 0.590
[12,     1] loss: 0.581
[13,     1] loss: 0.571
[14,     1] loss: 0.557
[15,     1] loss: 0.545
[16,     1] loss: 0.538
[17,     1] loss: 0.524
[18,     1] loss: 0.512
[19,     1] loss: 0.504
[20,     1] loss: 0.492
[21,     1] loss: 0.482
[22,     1] loss: 0.472
[23,     1] loss: 0.464
[24,     1] loss: 0.455
[25,     1] loss: 0.443
[26,     1] loss: 0.435
[27,     1] loss: 0.426
[28,     1] loss: 0.419
[29,     1] loss: 0.410
[30,     1] loss: 0.403
[31,     1] loss: 0.398
[32,     1] loss: 0.390
[33,     1] loss: 0.386
[34,     1] loss: 0.380
[35,     1] loss: 0.374
[36,     1] loss: 0.370
[37,     1] loss: 0.367
[38,     1] loss: 0.363
[39,     1] loss: 0.360
[40,     1] loss: 0.356
[41,     1] loss: 0.351
[42,     1] loss: 0.349
[43,     1] loss: 0.346
[44,     1] loss: 0.341
[45,     1] loss: 0.339
[46,     1] loss: 0.336
[47,     1] loss: 0.333
[48,     1] loss: 0.331
[49,     1] loss: 0.326
[50,     1] loss: 0.324
[51,     1] loss: 0.320
[52,     1] loss: 0.317
[53,     1] loss: 0.315
[54,     1] loss: 0.310
[55,     1] loss: 0.308
[56,     1] loss: 0.305
[57,     1] loss: 0.301
[58,     1] loss: 0.297
[59,     1] loss: 0.295
[60,     1] loss: 0.291
[61,     1] loss: 0.287
[62,     1] loss: 0.283
[63,     1] loss: 0.280
[64,     1] loss: 0.276
[65,     1] loss: 0.273
[66,     1] loss: 0.272
[67,     1] loss: 0.335
[68,     1] loss: 0.314
[69,     1] loss: 0.363
[70,     1] loss: 0.337
[71,     1] loss: 0.311
[72,     1] loss: 0.314
[73,     1] loss: 0.302
[74,     1] loss: 0.290
[75,     1] loss: 0.287
[76,     1] loss: 0.279
[77,     1] loss: 0.267
[78,     1] loss: 0.261
[79,     1] loss: 0.254
[80,     1] loss: 0.248
[81,     1] loss: 0.243
[82,     1] loss: 0.237
[83,     1] loss: 0.233
[84,     1] loss: 0.230
[85,     1] loss: 0.223
[86,     1] loss: 0.221
[87,     1] loss: 0.214
[88,     1] loss: 0.211
[89,     1] loss: 0.210
[90,     1] loss: 0.205
[91,     1] loss: 0.204
[92,     1] loss: 0.199
[93,     1] loss: 0.198
[94,     1] loss: 0.195
[95,     1] loss: 0.192
[96,     1] loss: 0.187
[97,     1] loss: 0.184
[98,     1] loss: 0.180
[99,     1] loss: 0.178
[100,     1] loss: 0.178
[101,     1] loss: 0.176
[102,     1] loss: 0.171
[103,     1] loss: 0.169
[104,     1] loss: 0.170
[105,     1] loss: 0.169
[106,     1] loss: 0.168
[107,     1] loss: 0.162
[108,     1] loss: 0.164
[109,     1] loss: 0.164
[110,     1] loss: 0.163
[111,     1] loss: 0.160
[112,     1] loss: 0.160
[113,     1] loss: 0.157
[114,     1] loss: 0.158
[115,     1] loss: 0.156
[116,     1] loss: 0.158
[117,     1] loss: 0.156
[118,     1] loss: 0.157
[119,     1] loss: 0.155
[120,     1] loss: 0.156
[121,     1] loss: 0.157
[122,     1] loss: 0.156
[123,     1] loss: 0.155
[124,     1] loss: 0.160
[125,     1] loss: 0.300
[126,     1] loss: 0.524
[127,     1] loss: 0.396
[128,     1] loss: 0.411
[129,     1] loss: 0.420
[130,     1] loss: 0.398
[131,     1] loss: 0.395
[132,     1] loss: 0.379
[133,     1] loss: 0.371
[134,     1] loss: 0.372
[135,     1] loss: 0.360
[136,     1] loss: 0.358
Early stopping applied (best metric=0.3885134160518646)
Finished Training
Total time taken: 223.7289605140686
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.667
[4,     1] loss: 0.658
[5,     1] loss: 0.651
[6,     1] loss: 0.645
[7,     1] loss: 0.635
[8,     1] loss: 0.628
[9,     1] loss: 0.617
[10,     1] loss: 0.609
[11,     1] loss: 0.598
[12,     1] loss: 0.588
[13,     1] loss: 0.578
[14,     1] loss: 0.568
[15,     1] loss: 0.556
[16,     1] loss: 0.546
[17,     1] loss: 0.533
[18,     1] loss: 0.522
[19,     1] loss: 0.513
[20,     1] loss: 0.504
[21,     1] loss: 0.493
[22,     1] loss: 0.484
[23,     1] loss: 0.477
[24,     1] loss: 0.469
[25,     1] loss: 0.461
[26,     1] loss: 0.453
[27,     1] loss: 0.445
[28,     1] loss: 0.438
[29,     1] loss: 0.432
[30,     1] loss: 0.424
[31,     1] loss: 0.420
[32,     1] loss: 0.412
[33,     1] loss: 0.408
[34,     1] loss: 0.402
[35,     1] loss: 0.398
[36,     1] loss: 0.394
[37,     1] loss: 0.388
[38,     1] loss: 0.386
[39,     1] loss: 0.380
[40,     1] loss: 0.376
[41,     1] loss: 0.373
[42,     1] loss: 0.369
[43,     1] loss: 0.365
[44,     1] loss: 0.362
[45,     1] loss: 0.358
[46,     1] loss: 0.353
[47,     1] loss: 0.349
[48,     1] loss: 0.344
[49,     1] loss: 0.339
[50,     1] loss: 0.335
[51,     1] loss: 0.330
[52,     1] loss: 0.323
[53,     1] loss: 0.321
[54,     1] loss: 0.317
[55,     1] loss: 0.311
[56,     1] loss: 0.305
[57,     1] loss: 0.301
[58,     1] loss: 0.296
[59,     1] loss: 0.291
[60,     1] loss: 0.287
[61,     1] loss: 0.282
[62,     1] loss: 0.277
[63,     1] loss: 0.274
[64,     1] loss: 0.270
[65,     1] loss: 0.263
[66,     1] loss: 0.260
[67,     1] loss: 0.258
[68,     1] loss: 0.253
[69,     1] loss: 0.250
[70,     1] loss: 0.247
[71,     1] loss: 0.241
[72,     1] loss: 0.239
[73,     1] loss: 0.236
[74,     1] loss: 0.232
[75,     1] loss: 0.232
[76,     1] loss: 0.229
[77,     1] loss: 0.226
[78,     1] loss: 0.223
[79,     1] loss: 0.219
[80,     1] loss: 0.220
[81,     1] loss: 0.217
[82,     1] loss: 0.214
[83,     1] loss: 0.214
[84,     1] loss: 0.217
[85,     1] loss: 0.230
[86,     1] loss: 0.522
[87,     1] loss: 0.406
[88,     1] loss: 0.404
[89,     1] loss: 0.368
[90,     1] loss: 0.366
[91,     1] loss: 0.342
[92,     1] loss: 0.331
[93,     1] loss: 0.326
[94,     1] loss: 0.324
[95,     1] loss: 0.319
[96,     1] loss: 0.313
[97,     1] loss: 0.311
[98,     1] loss: 0.310
[99,     1] loss: 0.305
[100,     1] loss: 0.303
[101,     1] loss: 0.299
[102,     1] loss: 0.294
[103,     1] loss: 0.288
[104,     1] loss: 0.287
[105,     1] loss: 0.282
[106,     1] loss: 0.275
[107,     1] loss: 0.276
[108,     1] loss: 0.269
[109,     1] loss: 0.268
[110,     1] loss: 0.265
[111,     1] loss: 0.259
[112,     1] loss: 0.256
[113,     1] loss: 0.255
[114,     1] loss: 0.254
[115,     1] loss: 0.253
[116,     1] loss: 0.251
[117,     1] loss: 0.249
[118,     1] loss: 0.248
[119,     1] loss: 0.247
[120,     1] loss: 0.244
[121,     1] loss: 0.244
[122,     1] loss: 0.245
[123,     1] loss: 0.244
[124,     1] loss: 0.243
[125,     1] loss: 0.242
[126,     1] loss: 0.243
[127,     1] loss: 0.242
[128,     1] loss: 0.243
[129,     1] loss: 0.242
[130,     1] loss: 0.239
[131,     1] loss: 0.242
[132,     1] loss: 0.238
[133,     1] loss: 0.238
[134,     1] loss: 0.238
[135,     1] loss: 0.233
[136,     1] loss: 0.233
[137,     1] loss: 0.233
[138,     1] loss: 0.229
[139,     1] loss: 0.228
[140,     1] loss: 0.225
[141,     1] loss: 0.224
[142,     1] loss: 0.219
[143,     1] loss: 0.220
[144,     1] loss: 0.216
[145,     1] loss: 0.214
[146,     1] loss: 0.212
[147,     1] loss: 0.211
[148,     1] loss: 0.210
[149,     1] loss: 0.207
[150,     1] loss: 0.206
[151,     1] loss: 0.204
[152,     1] loss: 0.202
[153,     1] loss: 0.201
[154,     1] loss: 0.199
[155,     1] loss: 0.197
[156,     1] loss: 0.197
[157,     1] loss: 0.196
[158,     1] loss: 0.195
[159,     1] loss: 0.192
[160,     1] loss: 0.194
[161,     1] loss: 0.193
[162,     1] loss: 0.195
[163,     1] loss: 0.351
[164,     1] loss: 0.258
[165,     1] loss: 0.253
[166,     1] loss: 0.270
[167,     1] loss: 0.316
[168,     1] loss: 0.379
[169,     1] loss: 0.370
[170,     1] loss: 0.340
[171,     1] loss: 0.318
[172,     1] loss: 0.300
[173,     1] loss: 0.306
[174,     1] loss: 0.281
[175,     1] loss: 0.268
[176,     1] loss: 0.270
[177,     1] loss: 0.263
[178,     1] loss: 0.257
[179,     1] loss: 0.257
[180,     1] loss: 0.254
[181,     1] loss: 0.248
[182,     1] loss: 0.247
[183,     1] loss: 0.241
[184,     1] loss: 0.241
[185,     1] loss: 0.242
[186,     1] loss: 0.235
[187,     1] loss: 0.233
[188,     1] loss: 0.231
[189,     1] loss: 0.231
[190,     1] loss: 0.230
[191,     1] loss: 0.227
[192,     1] loss: 0.226
[193,     1] loss: 0.225
[194,     1] loss: 0.225
[195,     1] loss: 0.225
[196,     1] loss: 0.221
[197,     1] loss: 0.222
[198,     1] loss: 0.223
[199,     1] loss: 0.223
[200,     1] loss: 0.221
Finished Training
Total time taken: 327.92441749572754
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.665
[3,     1] loss: 0.648
[4,     1] loss: 0.633
[5,     1] loss: 0.622
[6,     1] loss: 0.612
[7,     1] loss: 0.600
[8,     1] loss: 0.591
[9,     1] loss: 0.579
[10,     1] loss: 0.571
[11,     1] loss: 0.559
[12,     1] loss: 0.547
[13,     1] loss: 0.538
[14,     1] loss: 0.529
[15,     1] loss: 0.516
[16,     1] loss: 0.502
[17,     1] loss: 0.491
[18,     1] loss: 0.479
[19,     1] loss: 0.470
[20,     1] loss: 0.460
[21,     1] loss: 0.447
[22,     1] loss: 0.436
[23,     1] loss: 0.428
[24,     1] loss: 0.416
[25,     1] loss: 0.409
[26,     1] loss: 0.399
[27,     1] loss: 0.392
[28,     1] loss: 0.385
[29,     1] loss: 0.377
[30,     1] loss: 0.371
[31,     1] loss: 0.364
[32,     1] loss: 0.360
[33,     1] loss: 0.357
[34,     1] loss: 0.351
[35,     1] loss: 0.349
[36,     1] loss: 0.354
[37,     1] loss: 0.341
[38,     1] loss: 0.361
[39,     1] loss: 0.342
[40,     1] loss: 0.343
[41,     1] loss: 0.336
[42,     1] loss: 0.339
[43,     1] loss: 0.333
[44,     1] loss: 0.328
[45,     1] loss: 0.327
[46,     1] loss: 0.323
[47,     1] loss: 0.323
[48,     1] loss: 0.319
[49,     1] loss: 0.314
[50,     1] loss: 0.312
[51,     1] loss: 0.309
[52,     1] loss: 0.309
[53,     1] loss: 0.306
[54,     1] loss: 0.301
[55,     1] loss: 0.299
[56,     1] loss: 0.300
[57,     1] loss: 0.296
[58,     1] loss: 0.293
[59,     1] loss: 0.293
[60,     1] loss: 0.289
[61,     1] loss: 0.289
[62,     1] loss: 0.287
[63,     1] loss: 0.285
[64,     1] loss: 0.283
[65,     1] loss: 0.281
[66,     1] loss: 0.279
[67,     1] loss: 0.279
[68,     1] loss: 0.277
[69,     1] loss: 0.278
[70,     1] loss: 0.275
[71,     1] loss: 0.274
[72,     1] loss: 0.273
[73,     1] loss: 0.272
[74,     1] loss: 0.270
[75,     1] loss: 0.268
[76,     1] loss: 0.269
[77,     1] loss: 0.266
[78,     1] loss: 0.265
[79,     1] loss: 0.278
[80,     1] loss: 0.373
[81,     1] loss: 0.445
[82,     1] loss: 0.348
[83,     1] loss: 0.365
[84,     1] loss: 0.350
[85,     1] loss: 0.335
[86,     1] loss: 0.326
[87,     1] loss: 0.321
[88,     1] loss: 0.315
[89,     1] loss: 0.305
[90,     1] loss: 0.300
[91,     1] loss: 0.297
[92,     1] loss: 0.291
[93,     1] loss: 0.284
[94,     1] loss: 0.284
[95,     1] loss: 0.277
[96,     1] loss: 0.273
[97,     1] loss: 0.269
[98,     1] loss: 0.268
[99,     1] loss: 0.263
[100,     1] loss: 0.265
[101,     1] loss: 0.262
[102,     1] loss: 0.258
[103,     1] loss: 0.257
[104,     1] loss: 0.254
[105,     1] loss: 0.254
[106,     1] loss: 0.253
[107,     1] loss: 0.253
[108,     1] loss: 0.251
[109,     1] loss: 0.249
[110,     1] loss: 0.247
[111,     1] loss: 0.246
[112,     1] loss: 0.244
[113,     1] loss: 0.244
[114,     1] loss: 0.242
[115,     1] loss: 0.239
[116,     1] loss: 0.243
[117,     1] loss: 0.239
[118,     1] loss: 0.238
[119,     1] loss: 0.235
[120,     1] loss: 0.235
[121,     1] loss: 0.234
[122,     1] loss: 0.233
[123,     1] loss: 0.230
[124,     1] loss: 0.227
[125,     1] loss: 0.225
[126,     1] loss: 0.225
[127,     1] loss: 0.222
Early stopping applied (best metric=0.3789512813091278)
Finished Training
Total time taken: 210.94402480125427
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.668
[4,     1] loss: 0.658
[5,     1] loss: 0.650
[6,     1] loss: 0.641
[7,     1] loss: 0.634
[8,     1] loss: 0.626
[9,     1] loss: 0.617
[10,     1] loss: 0.610
[11,     1] loss: 0.603
[12,     1] loss: 0.592
[13,     1] loss: 0.583
[14,     1] loss: 0.574
[15,     1] loss: 0.565
[16,     1] loss: 0.555
[17,     1] loss: 0.545
[18,     1] loss: 0.536
[19,     1] loss: 0.526
[20,     1] loss: 0.515
[21,     1] loss: 0.504
[22,     1] loss: 0.496
[23,     1] loss: 0.484
[24,     1] loss: 0.478
[25,     1] loss: 0.466
[26,     1] loss: 0.458
[27,     1] loss: 0.449
[28,     1] loss: 0.442
[29,     1] loss: 0.432
[30,     1] loss: 0.423
[31,     1] loss: 0.417
[32,     1] loss: 0.411
[33,     1] loss: 0.403
[34,     1] loss: 0.398
[35,     1] loss: 0.389
[36,     1] loss: 0.383
[37,     1] loss: 0.375
[38,     1] loss: 0.372
[39,     1] loss: 0.365
[40,     1] loss: 0.361
[41,     1] loss: 0.351
[42,     1] loss: 0.345
[43,     1] loss: 0.341
[44,     1] loss: 0.337
[45,     1] loss: 0.329
[46,     1] loss: 0.320
[47,     1] loss: 0.317
[48,     1] loss: 0.309
[49,     1] loss: 0.304
[50,     1] loss: 0.300
[51,     1] loss: 0.292
[52,     1] loss: 0.288
[53,     1] loss: 0.278
[54,     1] loss: 0.275
[55,     1] loss: 0.269
[56,     1] loss: 0.264
[57,     1] loss: 0.260
[58,     1] loss: 0.251
[59,     1] loss: 0.247
[60,     1] loss: 0.244
[61,     1] loss: 0.239
[62,     1] loss: 0.234
[63,     1] loss: 0.233
[64,     1] loss: 0.225
[65,     1] loss: 0.222
[66,     1] loss: 0.221
[67,     1] loss: 0.214
[68,     1] loss: 0.214
[69,     1] loss: 0.214
[70,     1] loss: 0.209
[71,     1] loss: 0.205
[72,     1] loss: 0.202
[73,     1] loss: 0.203
[74,     1] loss: 0.200
[75,     1] loss: 0.201
[76,     1] loss: 0.199
[77,     1] loss: 0.197
[78,     1] loss: 0.196
[79,     1] loss: 0.194
[80,     1] loss: 0.196
[81,     1] loss: 0.192
[82,     1] loss: 0.192
[83,     1] loss: 0.193
[84,     1] loss: 0.193
[85,     1] loss: 0.204
[86,     1] loss: 0.292
[87,     1] loss: 0.430
[88,     1] loss: 0.391
[89,     1] loss: 0.373
[90,     1] loss: 0.357
[91,     1] loss: 0.333
[92,     1] loss: 0.315
[93,     1] loss: 0.305
[94,     1] loss: 0.296
[95,     1] loss: 0.292
[96,     1] loss: 0.287
[97,     1] loss: 0.279
[98,     1] loss: 0.274
[99,     1] loss: 0.274
[100,     1] loss: 0.269
[101,     1] loss: 0.266
[102,     1] loss: 0.268
[103,     1] loss: 0.262
[104,     1] loss: 0.260
[105,     1] loss: 0.259
[106,     1] loss: 0.256
[107,     1] loss: 0.250
[108,     1] loss: 0.245
[109,     1] loss: 0.240
[110,     1] loss: 0.234
[111,     1] loss: 0.232
[112,     1] loss: 0.229
[113,     1] loss: 0.220
[114,     1] loss: 0.219
[115,     1] loss: 0.216
[116,     1] loss: 0.209
[117,     1] loss: 0.206
[118,     1] loss: 0.199
[119,     1] loss: 0.193
[120,     1] loss: 0.191
[121,     1] loss: 0.188
[122,     1] loss: 0.187
[123,     1] loss: 0.185
[124,     1] loss: 0.183
[125,     1] loss: 0.178
[126,     1] loss: 0.177
[127,     1] loss: 0.176
[128,     1] loss: 0.172
[129,     1] loss: 0.173
[130,     1] loss: 0.172
[131,     1] loss: 0.171
[132,     1] loss: 0.169
[133,     1] loss: 0.166
[134,     1] loss: 0.167
Early stopping applied (best metric=0.36176440119743347)
Finished Training
Total time taken: 223.21935391426086
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.673
[4,     1] loss: 0.662
[5,     1] loss: 0.652
[6,     1] loss: 0.643
[7,     1] loss: 0.636
[8,     1] loss: 0.627
[9,     1] loss: 0.617
[10,     1] loss: 0.607
[11,     1] loss: 0.600
[12,     1] loss: 0.589
[13,     1] loss: 0.578
[14,     1] loss: 0.569
[15,     1] loss: 0.559
[16,     1] loss: 0.547
[17,     1] loss: 0.537
[18,     1] loss: 0.526
[19,     1] loss: 0.515
[20,     1] loss: 0.503
[21,     1] loss: 0.490
[22,     1] loss: 0.477
[23,     1] loss: 0.465
[24,     1] loss: 0.458
[25,     1] loss: 0.442
[26,     1] loss: 0.430
[27,     1] loss: 0.422
[28,     1] loss: 0.409
[29,     1] loss: 0.397
[30,     1] loss: 0.386
[31,     1] loss: 0.377
[32,     1] loss: 0.365
[33,     1] loss: 0.354
[34,     1] loss: 0.346
[35,     1] loss: 0.334
[36,     1] loss: 0.326
[37,     1] loss: 0.318
[38,     1] loss: 0.308
[39,     1] loss: 0.301
[40,     1] loss: 0.293
[41,     1] loss: 0.284
[42,     1] loss: 0.278
[43,     1] loss: 0.268
[44,     1] loss: 0.263
[45,     1] loss: 0.259
[46,     1] loss: 0.250
[47,     1] loss: 0.249
[48,     1] loss: 0.242
[49,     1] loss: 0.236
[50,     1] loss: 0.235
[51,     1] loss: 0.230
[52,     1] loss: 0.225
[53,     1] loss: 0.224
[54,     1] loss: 0.221
[55,     1] loss: 0.217
[56,     1] loss: 0.218
[57,     1] loss: 0.215
[58,     1] loss: 0.213
[59,     1] loss: 0.212
[60,     1] loss: 0.212
[61,     1] loss: 0.209
[62,     1] loss: 0.207
[63,     1] loss: 0.207
[64,     1] loss: 0.208
[65,     1] loss: 0.208
[66,     1] loss: 0.206
[67,     1] loss: 0.208
[68,     1] loss: 0.209
[69,     1] loss: 0.236
[70,     1] loss: 0.272
[71,     1] loss: 0.414
[72,     1] loss: 0.273
[73,     1] loss: 0.320
[74,     1] loss: 0.299
[75,     1] loss: 0.278
[76,     1] loss: 0.278
[77,     1] loss: 0.276
[78,     1] loss: 0.277
[79,     1] loss: 0.273
[80,     1] loss: 0.270
[81,     1] loss: 0.267
[82,     1] loss: 0.264
[83,     1] loss: 0.262
[84,     1] loss: 0.261
[85,     1] loss: 0.256
[86,     1] loss: 0.254
[87,     1] loss: 0.253
[88,     1] loss: 0.250
[89,     1] loss: 0.251
[90,     1] loss: 0.249
[91,     1] loss: 0.251
[92,     1] loss: 0.248
[93,     1] loss: 0.247
[94,     1] loss: 0.246
[95,     1] loss: 0.245
[96,     1] loss: 0.244
[97,     1] loss: 0.244
[98,     1] loss: 0.245
[99,     1] loss: 0.247
[100,     1] loss: 0.243
[101,     1] loss: 0.244
[102,     1] loss: 0.246
[103,     1] loss: 0.248
[104,     1] loss: 0.243
[105,     1] loss: 0.245
[106,     1] loss: 0.243
[107,     1] loss: 0.243
[108,     1] loss: 0.247
[109,     1] loss: 0.241
[110,     1] loss: 0.243
[111,     1] loss: 0.238
[112,     1] loss: 0.233
[113,     1] loss: 0.232
[114,     1] loss: 0.233
[115,     1] loss: 0.267
[116,     1] loss: 0.260
[117,     1] loss: 0.253
[118,     1] loss: 0.248
[119,     1] loss: 0.244
[120,     1] loss: 0.239
[121,     1] loss: 0.238
[122,     1] loss: 0.228
[123,     1] loss: 0.225
[124,     1] loss: 0.222
[125,     1] loss: 0.220
[126,     1] loss: 0.220
[127,     1] loss: 0.235
[128,     1] loss: 0.277
[129,     1] loss: 0.274
[130,     1] loss: 0.305
[131,     1] loss: 0.273
[132,     1] loss: 0.279
[133,     1] loss: 0.271
[134,     1] loss: 0.270
[135,     1] loss: 0.266
[136,     1] loss: 0.267
[137,     1] loss: 0.258
[138,     1] loss: 0.257
[139,     1] loss: 0.252
[140,     1] loss: 0.250
[141,     1] loss: 0.249
[142,     1] loss: 0.242
[143,     1] loss: 0.242
[144,     1] loss: 0.239
[145,     1] loss: 0.235
[146,     1] loss: 0.237
[147,     1] loss: 0.236
[148,     1] loss: 0.237
[149,     1] loss: 0.237
[150,     1] loss: 0.234
[151,     1] loss: 0.233
[152,     1] loss: 0.230
[153,     1] loss: 0.230
[154,     1] loss: 0.227
[155,     1] loss: 0.228
[156,     1] loss: 0.225
[157,     1] loss: 0.227
[158,     1] loss: 0.227
[159,     1] loss: 0.225
[160,     1] loss: 0.226
[161,     1] loss: 0.223
[162,     1] loss: 0.224
[163,     1] loss: 0.223
[164,     1] loss: 0.222
[165,     1] loss: 0.221
[166,     1] loss: 0.226
[167,     1] loss: 0.219
[168,     1] loss: 0.223
[169,     1] loss: 0.220
[170,     1] loss: 0.224
[171,     1] loss: 0.220
[172,     1] loss: 0.220
[173,     1] loss: 0.223
[174,     1] loss: 0.219
[175,     1] loss: 0.224
[176,     1] loss: 0.223
[177,     1] loss: 0.219
[178,     1] loss: 0.224
[179,     1] loss: 0.222
[180,     1] loss: 0.223
[181,     1] loss: 0.223
[182,     1] loss: 0.222
[183,     1] loss: 0.222
[184,     1] loss: 0.222
[185,     1] loss: 0.223
[186,     1] loss: 0.222
[187,     1] loss: 0.224
[188,     1] loss: 0.223
[189,     1] loss: 0.225
[190,     1] loss: 0.222
[191,     1] loss: 0.226
[192,     1] loss: 0.266
[193,     1] loss: 0.278
[194,     1] loss: 0.334
[195,     1] loss: 0.467
[196,     1] loss: 0.362
[197,     1] loss: 0.387
[198,     1] loss: 0.342
[199,     1] loss: 0.342
[200,     1] loss: 0.324
Finished Training
Total time taken: 333.5131278038025
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.697
[3,     1] loss: 0.685
[4,     1] loss: 0.674
[5,     1] loss: 0.663
[6,     1] loss: 0.656
[7,     1] loss: 0.649
[8,     1] loss: 0.641
[9,     1] loss: 0.634
[10,     1] loss: 0.628
[11,     1] loss: 0.620
[12,     1] loss: 0.614
[13,     1] loss: 0.607
[14,     1] loss: 0.598
[15,     1] loss: 0.588
[16,     1] loss: 0.580
[17,     1] loss: 0.572
[18,     1] loss: 0.560
[19,     1] loss: 0.551
[20,     1] loss: 0.541
[21,     1] loss: 0.529
[22,     1] loss: 0.518
[23,     1] loss: 0.505
[24,     1] loss: 0.495
[25,     1] loss: 0.484
[26,     1] loss: 0.473
[27,     1] loss: 0.461
[28,     1] loss: 0.451
[29,     1] loss: 0.440
[30,     1] loss: 0.426
[31,     1] loss: 0.418
[32,     1] loss: 0.406
[33,     1] loss: 0.397
[34,     1] loss: 0.385
[35,     1] loss: 0.378
[36,     1] loss: 0.368
[37,     1] loss: 0.355
[38,     1] loss: 0.347
[39,     1] loss: 0.339
[40,     1] loss: 0.327
[41,     1] loss: 0.321
[42,     1] loss: 0.312
[43,     1] loss: 0.306
[44,     1] loss: 0.298
[45,     1] loss: 0.292
[46,     1] loss: 0.286
[47,     1] loss: 0.279
[48,     1] loss: 0.273
[49,     1] loss: 0.268
[50,     1] loss: 0.264
[51,     1] loss: 0.258
[52,     1] loss: 0.256
[53,     1] loss: 0.251
[54,     1] loss: 0.247
[55,     1] loss: 0.245
[56,     1] loss: 0.241
[57,     1] loss: 0.237
[58,     1] loss: 0.237
[59,     1] loss: 0.232
[60,     1] loss: 0.232
[61,     1] loss: 0.228
[62,     1] loss: 0.229
[63,     1] loss: 0.231
[64,     1] loss: 0.233
[65,     1] loss: 0.230
[66,     1] loss: 0.229
[67,     1] loss: 0.228
[68,     1] loss: 0.225
[69,     1] loss: 0.224
[70,     1] loss: 0.226
[71,     1] loss: 0.221
[72,     1] loss: 0.220
[73,     1] loss: 0.221
[74,     1] loss: 0.222
[75,     1] loss: 0.218
[76,     1] loss: 0.217
[77,     1] loss: 0.217
[78,     1] loss: 0.215
[79,     1] loss: 0.215
[80,     1] loss: 0.218
[81,     1] loss: 0.215
[82,     1] loss: 0.213
[83,     1] loss: 0.214
[84,     1] loss: 0.217
[85,     1] loss: 0.214
[86,     1] loss: 0.215
[87,     1] loss: 0.215
[88,     1] loss: 0.215
[89,     1] loss: 0.217
[90,     1] loss: 0.220
[91,     1] loss: 0.252
[92,     1] loss: 0.245
[93,     1] loss: 0.440
[94,     1] loss: 0.345
[95,     1] loss: 0.371
[96,     1] loss: 0.340
[97,     1] loss: 0.328
[98,     1] loss: 0.318
[99,     1] loss: 0.311
[100,     1] loss: 0.311
[101,     1] loss: 0.303
[102,     1] loss: 0.301
[103,     1] loss: 0.290
[104,     1] loss: 0.290
[105,     1] loss: 0.289
[106,     1] loss: 0.288
[107,     1] loss: 0.286
[108,     1] loss: 0.284
[109,     1] loss: 0.280
[110,     1] loss: 0.277
[111,     1] loss: 0.274
[112,     1] loss: 0.270
[113,     1] loss: 0.264
[114,     1] loss: 0.259
[115,     1] loss: 0.254
[116,     1] loss: 0.249
[117,     1] loss: 0.243
[118,     1] loss: 0.238
[119,     1] loss: 0.233
[120,     1] loss: 0.236
[121,     1] loss: 0.241
[122,     1] loss: 0.227
[123,     1] loss: 0.224
[124,     1] loss: 0.217
[125,     1] loss: 0.215
[126,     1] loss: 0.213
[127,     1] loss: 0.208
[128,     1] loss: 0.207
[129,     1] loss: 0.204
[130,     1] loss: 0.200
[131,     1] loss: 0.199
[132,     1] loss: 0.198
[133,     1] loss: 0.195
[134,     1] loss: 0.196
[135,     1] loss: 0.195
[136,     1] loss: 0.191
[137,     1] loss: 0.190
[138,     1] loss: 0.191
[139,     1] loss: 0.188
[140,     1] loss: 0.189
[141,     1] loss: 0.187
[142,     1] loss: 0.186
[143,     1] loss: 0.184
[144,     1] loss: 0.186
[145,     1] loss: 0.181
[146,     1] loss: 0.185
[147,     1] loss: 0.184
[148,     1] loss: 0.186
[149,     1] loss: 0.185
[150,     1] loss: 0.181
[151,     1] loss: 0.183
[152,     1] loss: 0.183
[153,     1] loss: 0.186
[154,     1] loss: 0.183
[155,     1] loss: 0.182
[156,     1] loss: 0.184
[157,     1] loss: 0.185
[158,     1] loss: 0.185
[159,     1] loss: 0.183
[160,     1] loss: 0.185
[161,     1] loss: 0.186
[162,     1] loss: 0.187
Early stopping applied (best metric=0.4098796248435974)
Finished Training
Total time taken: 273.06868982315063
{'Hydroxylation-P Validation Accuracy': 0.7649887416882392, 'Hydroxylation-P Validation Sensitivity': 0.7735555555555556, 'Hydroxylation-P Validation Specificity': 0.762913362262457, 'Hydroxylation-P Validation Precision': 0.4472343876519545, 'Hydroxylation-P AUC ROC': 0.8342384087556914, 'Hydroxylation-P AUC PR': 0.6041260653878239, 'Hydroxylation-P MCC': 0.45259062852341775, 'Hydroxylation-P F1': 0.5577626351264456, 'Validation Loss (Hydroxylation-P)': 0.3905608880519867, 'Validation Loss (total)': 0.3905608880519867}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005518470082666563,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3189723261,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.17359362983051}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.662
[4,     1] loss: 0.639
[5,     1] loss: 0.613
[6,     1] loss: 0.585
[7,     1] loss: 0.553
[8,     1] loss: 0.520
[9,     1] loss: 0.485
[10,     1] loss: 0.455
[11,     1] loss: 0.418
[12,     1] loss: 0.389
[13,     1] loss: 0.359
[14,     1] loss: 0.343
[15,     1] loss: 0.320
[16,     1] loss: 0.304
[17,     1] loss: 0.287
[18,     1] loss: 0.273
[19,     1] loss: 0.264
[20,     1] loss: 0.251
[21,     1] loss: 0.242
[22,     1] loss: 0.236
[23,     1] loss: 0.228
[24,     1] loss: 0.221
[25,     1] loss: 0.212
[26,     1] loss: 0.208
[27,     1] loss: 0.202
[28,     1] loss: 0.196
[29,     1] loss: 0.192
[30,     1] loss: 0.190
[31,     1] loss: 0.184
[32,     1] loss: 0.180
[33,     1] loss: 0.172
[34,     1] loss: 0.157
[35,     1] loss: 0.331
[36,     1] loss: 0.656
[37,     1] loss: 0.536
[38,     1] loss: 0.511
[39,     1] loss: 0.520
[40,     1] loss: 0.534
[41,     1] loss: 0.549
[42,     1] loss: 0.560
[43,     1] loss: 0.566
[44,     1] loss: 0.569
[45,     1] loss: 0.565
[46,     1] loss: 0.560
[47,     1] loss: 0.555
[48,     1] loss: 0.547
[49,     1] loss: 0.541
[50,     1] loss: 0.531
[51,     1] loss: 0.521
[52,     1] loss: 0.510
[53,     1] loss: 0.496
[54,     1] loss: 0.481
[55,     1] loss: 0.462
[56,     1] loss: 0.441
[57,     1] loss: 0.418
[58,     1] loss: 0.391
[59,     1] loss: 0.388
[60,     1] loss: 0.363
[61,     1] loss: 0.331
[62,     1] loss: 0.322
[63,     1] loss: 0.318
[64,     1] loss: 0.575
[65,     1] loss: 0.438
[66,     1] loss: 0.506
[67,     1] loss: 0.337
[68,     1] loss: 0.410
[69,     1] loss: 0.375
[70,     1] loss: 0.352
[71,     1] loss: 0.338
[72,     1] loss: 0.313
[73,     1] loss: 0.300
[74,     1] loss: 0.282
[75,     1] loss: 0.261
[76,     1] loss: 0.253
[77,     1] loss: 0.269
[78,     1] loss: 0.250
[79,     1] loss: 0.266
[80,     1] loss: 0.224
[81,     1] loss: 0.244
[82,     1] loss: 0.232
Early stopping applied (best metric=0.36557483673095703)
Finished Training
Total time taken: 137.9094433784485
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.680
[3,     1] loss: 0.654
[4,     1] loss: 0.633
[5,     1] loss: 0.608
[6,     1] loss: 0.582
[7,     1] loss: 0.555
[8,     1] loss: 0.522
[9,     1] loss: 0.490
[10,     1] loss: 0.453
[11,     1] loss: 0.419
[12,     1] loss: 0.391
[13,     1] loss: 0.361
[14,     1] loss: 0.331
[15,     1] loss: 0.309
[16,     1] loss: 0.287
[17,     1] loss: 0.269
[18,     1] loss: 0.255
[19,     1] loss: 0.231
[20,     1] loss: 0.217
[21,     1] loss: 0.204
[22,     1] loss: 0.191
[23,     1] loss: 0.180
[24,     1] loss: 0.171
[25,     1] loss: 0.162
[26,     1] loss: 0.156
[27,     1] loss: 0.150
[28,     1] loss: 0.144
[29,     1] loss: 0.139
[30,     1] loss: 0.139
[31,     1] loss: 0.174
[32,     1] loss: 0.174
[33,     1] loss: 0.349
[34,     1] loss: 0.449
[35,     1] loss: 0.410
[36,     1] loss: 0.375
[37,     1] loss: 0.363
[38,     1] loss: 0.370
[39,     1] loss: 0.369
[40,     1] loss: 0.352
[41,     1] loss: 0.341
[42,     1] loss: 0.330
[43,     1] loss: 0.314
[44,     1] loss: 0.302
[45,     1] loss: 0.291
[46,     1] loss: 0.272
[47,     1] loss: 0.261
[48,     1] loss: 0.247
[49,     1] loss: 0.239
[50,     1] loss: 0.306
[51,     1] loss: 0.241
[52,     1] loss: 0.236
[53,     1] loss: 0.232
[54,     1] loss: 0.226
[55,     1] loss: 0.215
[56,     1] loss: 0.204
[57,     1] loss: 0.195
[58,     1] loss: 0.185
[59,     1] loss: 0.176
[60,     1] loss: 0.168
[61,     1] loss: 0.161
[62,     1] loss: 0.155
[63,     1] loss: 0.149
[64,     1] loss: 0.142
[65,     1] loss: 0.141
[66,     1] loss: 0.139
[67,     1] loss: 0.136
[68,     1] loss: 0.134
[69,     1] loss: 0.132
[70,     1] loss: 0.131
[71,     1] loss: 0.131
[72,     1] loss: 0.130
[73,     1] loss: 0.128
Early stopping applied (best metric=0.35693496465682983)
Finished Training
Total time taken: 123.8224425315857
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.677
[4,     1] loss: 0.660
[5,     1] loss: 0.643
[6,     1] loss: 0.625
[7,     1] loss: 0.608
[8,     1] loss: 0.584
[9,     1] loss: 0.561
[10,     1] loss: 0.537
[11,     1] loss: 0.513
[12,     1] loss: 0.483
[13,     1] loss: 0.456
[14,     1] loss: 0.427
[15,     1] loss: 0.397
[16,     1] loss: 0.367
[17,     1] loss: 0.358
[18,     1] loss: 0.333
[19,     1] loss: 0.311
[20,     1] loss: 0.287
[21,     1] loss: 0.271
[22,     1] loss: 0.253
[23,     1] loss: 0.235
[24,     1] loss: 0.218
[25,     1] loss: 0.202
[26,     1] loss: 0.187
[27,     1] loss: 0.196
[28,     1] loss: 0.270
[29,     1] loss: 0.575
[30,     1] loss: 0.296
[31,     1] loss: 0.420
[32,     1] loss: 0.376
[33,     1] loss: 0.394
[34,     1] loss: 0.410
[35,     1] loss: 0.407
[36,     1] loss: 0.399
[37,     1] loss: 0.388
[38,     1] loss: 0.376
[39,     1] loss: 0.360
[40,     1] loss: 0.347
[41,     1] loss: 0.329
[42,     1] loss: 0.314
[43,     1] loss: 0.293
[44,     1] loss: 0.280
[45,     1] loss: 0.265
[46,     1] loss: 0.250
[47,     1] loss: 0.234
[48,     1] loss: 0.225
[49,     1] loss: 0.224
[50,     1] loss: 0.284
[51,     1] loss: 0.244
[52,     1] loss: 0.266
[53,     1] loss: 0.247
[54,     1] loss: 0.230
[55,     1] loss: 0.215
[56,     1] loss: 0.205
[57,     1] loss: 0.201
[58,     1] loss: 0.192
[59,     1] loss: 0.188
[60,     1] loss: 0.186
[61,     1] loss: 0.186
[62,     1] loss: 0.260
[63,     1] loss: 0.287
[64,     1] loss: 0.336
[65,     1] loss: 0.328
[66,     1] loss: 0.309
[67,     1] loss: 0.271
[68,     1] loss: 0.262
[69,     1] loss: 0.251
[70,     1] loss: 0.240
[71,     1] loss: 0.237
[72,     1] loss: 0.231
[73,     1] loss: 0.227
[74,     1] loss: 0.222
[75,     1] loss: 0.215
[76,     1] loss: 0.213
[77,     1] loss: 0.207
[78,     1] loss: 0.206
[79,     1] loss: 0.203
[80,     1] loss: 0.199
[81,     1] loss: 0.194
[82,     1] loss: 0.220
[83,     1] loss: 0.248
[84,     1] loss: 0.256
[85,     1] loss: 0.249
[86,     1] loss: 0.224
[87,     1] loss: 0.218
[88,     1] loss: 0.214
[89,     1] loss: 0.245
[90,     1] loss: 0.230
[91,     1] loss: 0.222
[92,     1] loss: 0.217
[93,     1] loss: 0.205
[94,     1] loss: 0.204
[95,     1] loss: 0.224
[96,     1] loss: 0.284
[97,     1] loss: 0.261
[98,     1] loss: 0.241
[99,     1] loss: 0.240
[100,     1] loss: 0.228
[101,     1] loss: 0.216
[102,     1] loss: 0.213
[103,     1] loss: 0.208
[104,     1] loss: 0.198
[105,     1] loss: 0.204
[106,     1] loss: 0.221
Early stopping applied (best metric=0.3399713337421417)
Finished Training
Total time taken: 179.37094926834106
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.671
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.632
[7,     1] loss: 0.619
[8,     1] loss: 0.600
[9,     1] loss: 0.585
[10,     1] loss: 0.566
[11,     1] loss: 0.548
[12,     1] loss: 0.528
[13,     1] loss: 0.508
[14,     1] loss: 0.498
[15,     1] loss: 0.495
[16,     1] loss: 0.478
[17,     1] loss: 0.453
[18,     1] loss: 0.458
[19,     1] loss: 0.428
[20,     1] loss: 0.419
[21,     1] loss: 0.390
[22,     1] loss: 0.382
[23,     1] loss: 0.357
[24,     1] loss: 0.337
[25,     1] loss: 0.316
[26,     1] loss: 0.298
[27,     1] loss: 0.277
[28,     1] loss: 0.260
[29,     1] loss: 0.254
[30,     1] loss: 0.275
[31,     1] loss: 0.256
[32,     1] loss: 0.224
[33,     1] loss: 0.227
[34,     1] loss: 0.211
[35,     1] loss: 0.198
[36,     1] loss: 0.183
[37,     1] loss: 0.173
[38,     1] loss: 0.173
[39,     1] loss: 0.196
[40,     1] loss: 0.164
[41,     1] loss: 0.166
[42,     1] loss: 0.153
[43,     1] loss: 0.133
[44,     1] loss: 0.124
[45,     1] loss: 0.117
[46,     1] loss: 0.108
[47,     1] loss: 0.102
[48,     1] loss: 0.097
[49,     1] loss: 0.092
[50,     1] loss: 0.090
[51,     1] loss: 0.089
[52,     1] loss: 0.087
[53,     1] loss: 0.087
[54,     1] loss: 0.087
[55,     1] loss: 0.090
[56,     1] loss: 0.090
[57,     1] loss: 0.091
[58,     1] loss: 0.091
[59,     1] loss: 0.094
[60,     1] loss: 0.095
[61,     1] loss: 0.096
[62,     1] loss: 0.111
[63,     1] loss: 0.276
[64,     1] loss: 0.421
[65,     1] loss: 1.034
[66,     1] loss: 0.687
Early stopping applied (best metric=0.41130712628364563)
Finished Training
Total time taken: 112.95429801940918
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.666
[4,     1] loss: 0.650
[5,     1] loss: 0.634
[6,     1] loss: 0.616
[7,     1] loss: 0.596
[8,     1] loss: 0.573
[9,     1] loss: 0.549
[10,     1] loss: 0.521
[11,     1] loss: 0.493
[12,     1] loss: 0.466
[13,     1] loss: 0.439
[14,     1] loss: 0.411
[15,     1] loss: 0.382
[16,     1] loss: 0.364
[17,     1] loss: 0.338
[18,     1] loss: 0.321
[19,     1] loss: 0.314
[20,     1] loss: 0.281
[21,     1] loss: 0.272
[22,     1] loss: 0.258
[23,     1] loss: 0.247
[24,     1] loss: 0.229
[25,     1] loss: 0.217
[26,     1] loss: 0.208
[27,     1] loss: 0.199
[28,     1] loss: 0.190
[29,     1] loss: 0.181
[30,     1] loss: 0.173
[31,     1] loss: 0.167
[32,     1] loss: 0.161
[33,     1] loss: 0.170
[34,     1] loss: 0.185
[35,     1] loss: 0.171
[36,     1] loss: 0.167
[37,     1] loss: 0.162
[38,     1] loss: 0.171
[39,     1] loss: 0.235
[40,     1] loss: 0.856
[41,     1] loss: 0.637
[42,     1] loss: 0.522
[43,     1] loss: 0.506
[44,     1] loss: 0.529
[45,     1] loss: 0.546
[46,     1] loss: 0.560
[47,     1] loss: 0.570
[48,     1] loss: 0.576
[49,     1] loss: 0.578
[50,     1] loss: 0.577
[51,     1] loss: 0.578
[52,     1] loss: 0.576
[53,     1] loss: 0.574
[54,     1] loss: 0.572
[55,     1] loss: 0.568
[56,     1] loss: 0.565
[57,     1] loss: 0.559
[58,     1] loss: 0.555
[59,     1] loss: 0.548
[60,     1] loss: 0.540
[61,     1] loss: 0.533
[62,     1] loss: 0.524
[63,     1] loss: 0.514
[64,     1] loss: 0.504
[65,     1] loss: 0.498
[66,     1] loss: 0.500
[67,     1] loss: 0.515
[68,     1] loss: 0.494
[69,     1] loss: 0.492
[70,     1] loss: 0.482
[71,     1] loss: 0.474
[72,     1] loss: 0.465
[73,     1] loss: 0.456
[74,     1] loss: 0.448
[75,     1] loss: 0.440
[76,     1] loss: 0.433
[77,     1] loss: 0.428
[78,     1] loss: 0.442
[79,     1] loss: 0.479
[80,     1] loss: 0.493
[81,     1] loss: 0.481
[82,     1] loss: 0.489
[83,     1] loss: 0.482
[84,     1] loss: 0.470
[85,     1] loss: 0.462
[86,     1] loss: 0.450
Early stopping applied (best metric=0.3865296244621277)
Finished Training
Total time taken: 147.3635265827179
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.664
[4,     1] loss: 0.647
[5,     1] loss: 0.629
[6,     1] loss: 0.608
[7,     1] loss: 0.587
[8,     1] loss: 0.560
[9,     1] loss: 0.532
[10,     1] loss: 0.506
[11,     1] loss: 0.477
[12,     1] loss: 0.444
[13,     1] loss: 0.412
[14,     1] loss: 0.390
[15,     1] loss: 0.367
[16,     1] loss: 0.340
[17,     1] loss: 0.314
[18,     1] loss: 0.294
[19,     1] loss: 0.276
[20,     1] loss: 0.261
[21,     1] loss: 0.246
[22,     1] loss: 0.230
[23,     1] loss: 0.219
[24,     1] loss: 0.210
[25,     1] loss: 0.200
[26,     1] loss: 0.192
[27,     1] loss: 0.185
[28,     1] loss: 0.177
[29,     1] loss: 0.171
[30,     1] loss: 0.166
[31,     1] loss: 0.164
[32,     1] loss: 0.161
[33,     1] loss: 0.167
[34,     1] loss: 0.210
[35,     1] loss: 0.615
[36,     1] loss: 0.574
[37,     1] loss: 0.485
[38,     1] loss: 0.444
[39,     1] loss: 0.440
[40,     1] loss: 0.440
[41,     1] loss: 0.442
[42,     1] loss: 0.436
[43,     1] loss: 0.426
[44,     1] loss: 0.408
[45,     1] loss: 0.398
[46,     1] loss: 0.381
[47,     1] loss: 0.368
[48,     1] loss: 0.351
[49,     1] loss: 0.342
[50,     1] loss: 0.334
[51,     1] loss: 0.330
[52,     1] loss: 0.317
[53,     1] loss: 0.310
[54,     1] loss: 0.306
[55,     1] loss: 0.302
[56,     1] loss: 0.304
[57,     1] loss: 0.317
[58,     1] loss: 0.327
[59,     1] loss: 0.336
[60,     1] loss: 0.363
[61,     1] loss: 0.556
[62,     1] loss: 0.427
[63,     1] loss: 0.480
[64,     1] loss: 0.387
[65,     1] loss: 0.411
Early stopping applied (best metric=0.3860331177711487)
Finished Training
Total time taken: 111.78988862037659
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.662
[4,     1] loss: 0.643
[5,     1] loss: 0.624
[6,     1] loss: 0.600
[7,     1] loss: 0.578
[8,     1] loss: 0.558
[9,     1] loss: 0.536
[10,     1] loss: 0.515
[11,     1] loss: 0.507
[12,     1] loss: 0.502
[13,     1] loss: 0.467
[14,     1] loss: 0.465
[15,     1] loss: 0.454
[16,     1] loss: 0.432
[17,     1] loss: 0.429
[18,     1] loss: 0.424
[19,     1] loss: 0.403
[20,     1] loss: 0.396
[21,     1] loss: 0.406
[22,     1] loss: 0.450
[23,     1] loss: 0.414
[24,     1] loss: 0.439
[25,     1] loss: 0.420
[26,     1] loss: 0.410
[27,     1] loss: 0.402
[28,     1] loss: 0.400
[29,     1] loss: 0.385
[30,     1] loss: 0.381
[31,     1] loss: 0.380
[32,     1] loss: 0.375
[33,     1] loss: 0.377
[34,     1] loss: 0.366
[35,     1] loss: 0.364
[36,     1] loss: 0.363
[37,     1] loss: 0.363
[38,     1] loss: 0.372
[39,     1] loss: 0.367
[40,     1] loss: 0.368
[41,     1] loss: 0.363
[42,     1] loss: 0.361
[43,     1] loss: 0.371
[44,     1] loss: 0.396
[45,     1] loss: 0.407
[46,     1] loss: 0.371
[47,     1] loss: 0.393
[48,     1] loss: 0.386
[49,     1] loss: 0.369
[50,     1] loss: 0.355
[51,     1] loss: 0.344
[52,     1] loss: 0.338
[53,     1] loss: 0.332
[54,     1] loss: 0.325
[55,     1] loss: 0.326
[56,     1] loss: 0.335
[57,     1] loss: 0.347
[58,     1] loss: 0.343
[59,     1] loss: 0.354
[60,     1] loss: 0.327
[61,     1] loss: 0.317
[62,     1] loss: 0.312
Early stopping applied (best metric=0.42865028977394104)
Finished Training
Total time taken: 106.8520040512085
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.668
[4,     1] loss: 0.649
[5,     1] loss: 0.632
[6,     1] loss: 0.611
[7,     1] loss: 0.589
[8,     1] loss: 0.568
[9,     1] loss: 0.544
[10,     1] loss: 0.521
[11,     1] loss: 0.494
[12,     1] loss: 0.469
[13,     1] loss: 0.449
[14,     1] loss: 0.427
[15,     1] loss: 0.405
[16,     1] loss: 0.385
[17,     1] loss: 0.364
[18,     1] loss: 0.344
[19,     1] loss: 0.329
[20,     1] loss: 0.311
[21,     1] loss: 0.293
[22,     1] loss: 0.277
[23,     1] loss: 0.257
[24,     1] loss: 0.245
[25,     1] loss: 0.228
[26,     1] loss: 0.216
[27,     1] loss: 0.206
[28,     1] loss: 0.197
[29,     1] loss: 0.189
[30,     1] loss: 0.182
[31,     1] loss: 0.178
[32,     1] loss: 0.174
[33,     1] loss: 0.168
[34,     1] loss: 0.165
[35,     1] loss: 0.157
[36,     1] loss: 0.144
[37,     1] loss: 0.214
[38,     1] loss: 0.268
[39,     1] loss: 0.994
[40,     1] loss: 0.541
[41,     1] loss: 0.574
[42,     1] loss: 0.545
[43,     1] loss: 0.548
[44,     1] loss: 0.558
[45,     1] loss: 0.569
[46,     1] loss: 0.576
[47,     1] loss: 0.581
[48,     1] loss: 0.584
[49,     1] loss: 0.584
[50,     1] loss: 0.583
[51,     1] loss: 0.581
[52,     1] loss: 0.576
[53,     1] loss: 0.572
[54,     1] loss: 0.567
[55,     1] loss: 0.561
[56,     1] loss: 0.555
[57,     1] loss: 0.547
[58,     1] loss: 0.538
[59,     1] loss: 0.533
[60,     1] loss: 0.525
[61,     1] loss: 0.515
[62,     1] loss: 0.508
[63,     1] loss: 0.500
[64,     1] loss: 0.492
[65,     1] loss: 0.481
[66,     1] loss: 0.473
[67,     1] loss: 0.465
[68,     1] loss: 0.466
[69,     1] loss: 0.454
[70,     1] loss: 0.448
[71,     1] loss: 0.450
[72,     1] loss: 0.441
[73,     1] loss: 0.436
[74,     1] loss: 0.427
[75,     1] loss: 0.435
[76,     1] loss: 0.470
[77,     1] loss: 0.628
[78,     1] loss: 0.602
[79,     1] loss: 0.594
[80,     1] loss: 0.583
[81,     1] loss: 0.562
[82,     1] loss: 0.538
[83,     1] loss: 0.532
[84,     1] loss: 0.521
[85,     1] loss: 0.511
Early stopping applied (best metric=0.3877449333667755)
Finished Training
Total time taken: 146.4484703540802
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.680
[3,     1] loss: 0.653
[4,     1] loss: 0.631
[5,     1] loss: 0.610
[6,     1] loss: 0.586
[7,     1] loss: 0.566
[8,     1] loss: 0.542
[9,     1] loss: 0.526
[10,     1] loss: 0.507
[11,     1] loss: 0.483
[12,     1] loss: 0.458
[13,     1] loss: 0.432
[14,     1] loss: 0.408
[15,     1] loss: 0.385
[16,     1] loss: 0.369
[17,     1] loss: 0.341
[18,     1] loss: 0.335
[19,     1] loss: 0.321
[20,     1] loss: 0.313
[21,     1] loss: 0.294
[22,     1] loss: 0.287
[23,     1] loss: 0.272
[24,     1] loss: 0.262
[25,     1] loss: 0.251
[26,     1] loss: 0.241
[27,     1] loss: 0.232
[28,     1] loss: 0.221
[29,     1] loss: 0.215
[30,     1] loss: 0.207
[31,     1] loss: 0.198
[32,     1] loss: 0.191
[33,     1] loss: 0.186
[34,     1] loss: 0.176
[35,     1] loss: 0.170
[36,     1] loss: 0.160
[37,     1] loss: 0.181
[38,     1] loss: 0.269
[39,     1] loss: 0.723
[40,     1] loss: 0.667
[41,     1] loss: 0.605
[42,     1] loss: 0.563
[43,     1] loss: 0.556
[44,     1] loss: 0.561
[45,     1] loss: 0.561
[46,     1] loss: 0.558
[47,     1] loss: 0.551
[48,     1] loss: 0.544
[49,     1] loss: 0.534
[50,     1] loss: 0.522
[51,     1] loss: 0.509
[52,     1] loss: 0.494
[53,     1] loss: 0.477
[54,     1] loss: 0.458
[55,     1] loss: 0.441
[56,     1] loss: 0.425
[57,     1] loss: 0.412
[58,     1] loss: 0.403
[59,     1] loss: 0.400
[60,     1] loss: 0.413
[61,     1] loss: 0.446
[62,     1] loss: 0.397
Early stopping applied (best metric=0.4096693694591522)
Finished Training
Total time taken: 107.73004364967346
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.674
[4,     1] loss: 0.661
[5,     1] loss: 0.646
[6,     1] loss: 0.629
[7,     1] loss: 0.609
[8,     1] loss: 0.588
[9,     1] loss: 0.564
[10,     1] loss: 0.536
[11,     1] loss: 0.504
[12,     1] loss: 0.476
[13,     1] loss: 0.447
[14,     1] loss: 0.423
[15,     1] loss: 0.403
[16,     1] loss: 0.370
[17,     1] loss: 0.357
[18,     1] loss: 0.348
[19,     1] loss: 0.306
[20,     1] loss: 0.321
[21,     1] loss: 0.287
[22,     1] loss: 0.276
[23,     1] loss: 0.260
[24,     1] loss: 0.248
[25,     1] loss: 0.235
[26,     1] loss: 0.214
[27,     1] loss: 0.203
[28,     1] loss: 0.189
[29,     1] loss: 0.177
[30,     1] loss: 0.162
[31,     1] loss: 0.150
[32,     1] loss: 0.140
[33,     1] loss: 0.127
[34,     1] loss: 0.143
[35,     1] loss: 0.285
[36,     1] loss: 0.732
[37,     1] loss: 0.381
[38,     1] loss: 0.457
[39,     1] loss: 0.434
[40,     1] loss: 0.444
[41,     1] loss: 0.467
[42,     1] loss: 0.477
[43,     1] loss: 0.474
[44,     1] loss: 0.470
[45,     1] loss: 0.466
[46,     1] loss: 0.460
[47,     1] loss: 0.450
[48,     1] loss: 0.443
[49,     1] loss: 0.436
[50,     1] loss: 0.426
[51,     1] loss: 0.418
[52,     1] loss: 0.409
[53,     1] loss: 0.395
[54,     1] loss: 0.387
[55,     1] loss: 0.376
[56,     1] loss: 0.366
[57,     1] loss: 0.356
[58,     1] loss: 0.344
[59,     1] loss: 0.335
[60,     1] loss: 0.323
[61,     1] loss: 0.310
[62,     1] loss: 0.299
[63,     1] loss: 0.288
[64,     1] loss: 0.276
[65,     1] loss: 0.266
[66,     1] loss: 0.305
[67,     1] loss: 0.330
[68,     1] loss: 0.642
[69,     1] loss: 0.484
[70,     1] loss: 0.533
[71,     1] loss: 0.442
[72,     1] loss: 0.457
[73,     1] loss: 0.461
[74,     1] loss: 0.432
[75,     1] loss: 0.417
[76,     1] loss: 0.399
[77,     1] loss: 0.386
Early stopping applied (best metric=0.33834022283554077)
Finished Training
Total time taken: 133.62448954582214
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.698
[3,     1] loss: 0.683
[4,     1] loss: 0.669
[5,     1] loss: 0.653
[6,     1] loss: 0.636
[7,     1] loss: 0.618
[8,     1] loss: 0.598
[9,     1] loss: 0.574
[10,     1] loss: 0.554
[11,     1] loss: 0.531
[12,     1] loss: 0.505
[13,     1] loss: 0.479
[14,     1] loss: 0.452
[15,     1] loss: 0.422
[16,     1] loss: 0.394
[17,     1] loss: 0.364
[18,     1] loss: 0.338
[19,     1] loss: 0.309
[20,     1] loss: 0.285
[21,     1] loss: 0.261
[22,     1] loss: 0.242
[23,     1] loss: 0.226
[24,     1] loss: 0.221
[25,     1] loss: 0.276
[26,     1] loss: 0.211
[27,     1] loss: 0.213
[28,     1] loss: 0.202
[29,     1] loss: 0.208
[30,     1] loss: 0.199
[31,     1] loss: 0.183
[32,     1] loss: 0.177
[33,     1] loss: 0.161
[34,     1] loss: 0.149
[35,     1] loss: 0.136
[36,     1] loss: 0.130
[37,     1] loss: 0.172
[38,     1] loss: 0.152
[39,     1] loss: 0.189
[40,     1] loss: 0.248
[41,     1] loss: 0.165
[42,     1] loss: 0.196
[43,     1] loss: 0.191
[44,     1] loss: 0.202
[45,     1] loss: 0.192
[46,     1] loss: 0.183
[47,     1] loss: 0.172
[48,     1] loss: 0.161
[49,     1] loss: 0.148
[50,     1] loss: 0.137
[51,     1] loss: 0.131
[52,     1] loss: 0.125
[53,     1] loss: 0.119
[54,     1] loss: 0.115
[55,     1] loss: 0.112
[56,     1] loss: 0.111
[57,     1] loss: 0.110
[58,     1] loss: 0.109
[59,     1] loss: 0.109
[60,     1] loss: 0.109
[61,     1] loss: 0.111
[62,     1] loss: 0.112
[63,     1] loss: 0.113
[64,     1] loss: 0.112
[65,     1] loss: 0.114
[66,     1] loss: 0.117
[67,     1] loss: 0.116
[68,     1] loss: 0.119
[69,     1] loss: 0.144
[70,     1] loss: 0.338
[71,     1] loss: 0.657
[72,     1] loss: 0.532
[73,     1] loss: 0.449
[74,     1] loss: 0.437
[75,     1] loss: 0.447
[76,     1] loss: 0.440
[77,     1] loss: 0.418
[78,     1] loss: 0.416
[79,     1] loss: 0.383
[80,     1] loss: 0.368
[81,     1] loss: 0.345
Early stopping applied (best metric=0.3849445879459381)
Finished Training
Total time taken: 140.82760858535767
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.623
[7,     1] loss: 0.605
[8,     1] loss: 0.589
[9,     1] loss: 0.566
[10,     1] loss: 0.543
[11,     1] loss: 0.517
[12,     1] loss: 0.493
[13,     1] loss: 0.465
[14,     1] loss: 0.433
[15,     1] loss: 0.405
[16,     1] loss: 0.378
[17,     1] loss: 0.353
[18,     1] loss: 0.334
[19,     1] loss: 0.311
[20,     1] loss: 0.288
[21,     1] loss: 0.272
[22,     1] loss: 0.256
[23,     1] loss: 0.241
[24,     1] loss: 0.227
[25,     1] loss: 0.215
[26,     1] loss: 0.205
[27,     1] loss: 0.199
[28,     1] loss: 0.191
[29,     1] loss: 0.184
[30,     1] loss: 0.178
[31,     1] loss: 0.173
[32,     1] loss: 0.167
[33,     1] loss: 0.156
[34,     1] loss: 0.184
[35,     1] loss: 0.354
[36,     1] loss: 0.714
[37,     1] loss: 0.561
[38,     1] loss: 0.557
[39,     1] loss: 0.556
[40,     1] loss: 0.528
[41,     1] loss: 0.532
[42,     1] loss: 0.546
[43,     1] loss: 0.549
[44,     1] loss: 0.548
[45,     1] loss: 0.544
[46,     1] loss: 0.540
[47,     1] loss: 0.536
[48,     1] loss: 0.525
[49,     1] loss: 0.512
[50,     1] loss: 0.496
[51,     1] loss: 0.485
[52,     1] loss: 0.470
[53,     1] loss: 0.456
[54,     1] loss: 0.439
[55,     1] loss: 0.423
[56,     1] loss: 0.412
[57,     1] loss: 0.405
[58,     1] loss: 0.395
[59,     1] loss: 0.384
[60,     1] loss: 0.378
[61,     1] loss: 0.381
[62,     1] loss: 0.372
[63,     1] loss: 0.401
Early stopping applied (best metric=0.4473247528076172)
Finished Training
Total time taken: 110.42474317550659
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.688
[3,     1] loss: 0.667
[4,     1] loss: 0.651
[5,     1] loss: 0.637
[6,     1] loss: 0.620
[7,     1] loss: 0.606
[8,     1] loss: 0.585
[9,     1] loss: 0.567
[10,     1] loss: 0.547
[11,     1] loss: 0.521
[12,     1] loss: 0.496
[13,     1] loss: 0.470
[14,     1] loss: 0.439
[15,     1] loss: 0.417
[16,     1] loss: 0.393
[17,     1] loss: 0.368
[18,     1] loss: 0.348
[19,     1] loss: 0.327
[20,     1] loss: 0.308
[21,     1] loss: 0.284
[22,     1] loss: 0.268
[23,     1] loss: 0.261
[24,     1] loss: 0.241
[25,     1] loss: 0.225
[26,     1] loss: 0.210
[27,     1] loss: 0.200
[28,     1] loss: 0.192
[29,     1] loss: 0.192
[30,     1] loss: 0.199
[31,     1] loss: 0.203
[32,     1] loss: 0.242
[33,     1] loss: 0.196
[34,     1] loss: 0.193
[35,     1] loss: 0.194
[36,     1] loss: 0.186
[37,     1] loss: 0.176
[38,     1] loss: 0.167
[39,     1] loss: 0.284
[40,     1] loss: 0.330
[41,     1] loss: 0.519
[42,     1] loss: 0.297
[43,     1] loss: 0.352
[44,     1] loss: 0.354
[45,     1] loss: 0.367
[46,     1] loss: 0.365
[47,     1] loss: 0.364
[48,     1] loss: 0.350
[49,     1] loss: 0.340
[50,     1] loss: 0.325
[51,     1] loss: 0.311
[52,     1] loss: 0.293
[53,     1] loss: 0.281
[54,     1] loss: 0.266
[55,     1] loss: 0.249
[56,     1] loss: 0.231
[57,     1] loss: 0.220
[58,     1] loss: 0.211
[59,     1] loss: 0.201
[60,     1] loss: 0.192
[61,     1] loss: 0.186
[62,     1] loss: 0.181
[63,     1] loss: 0.178
[64,     1] loss: 0.173
[65,     1] loss: 0.168
Early stopping applied (best metric=0.40241774916648865)
Finished Training
Total time taken: 113.721346616745
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.678
[3,     1] loss: 0.652
[4,     1] loss: 0.630
[5,     1] loss: 0.607
[6,     1] loss: 0.584
[7,     1] loss: 0.559
[8,     1] loss: 0.533
[9,     1] loss: 0.505
[10,     1] loss: 0.482
[11,     1] loss: 0.455
[12,     1] loss: 0.426
[13,     1] loss: 0.406
[14,     1] loss: 0.387
[15,     1] loss: 0.368
[16,     1] loss: 0.355
[17,     1] loss: 0.336
[18,     1] loss: 0.318
[19,     1] loss: 0.306
[20,     1] loss: 0.292
[21,     1] loss: 0.274
[22,     1] loss: 0.258
[23,     1] loss: 0.244
[24,     1] loss: 0.231
[25,     1] loss: 0.221
[26,     1] loss: 0.208
[27,     1] loss: 0.199
[28,     1] loss: 0.189
[29,     1] loss: 0.183
[30,     1] loss: 0.175
[31,     1] loss: 0.169
[32,     1] loss: 0.164
[33,     1] loss: 0.158
[34,     1] loss: 0.154
[35,     1] loss: 0.148
[36,     1] loss: 0.135
[37,     1] loss: 0.170
[38,     1] loss: 0.377
[39,     1] loss: 0.922
[40,     1] loss: 0.470
[41,     1] loss: 0.525
[42,     1] loss: 0.541
[43,     1] loss: 0.551
[44,     1] loss: 0.570
[45,     1] loss: 0.585
[46,     1] loss: 0.594
[47,     1] loss: 0.600
[48,     1] loss: 0.604
[49,     1] loss: 0.607
[50,     1] loss: 0.608
[51,     1] loss: 0.610
[52,     1] loss: 0.608
[53,     1] loss: 0.609
[54,     1] loss: 0.608
[55,     1] loss: 0.608
[56,     1] loss: 0.606
[57,     1] loss: 0.606
[58,     1] loss: 0.604
[59,     1] loss: 0.602
[60,     1] loss: 0.598
[61,     1] loss: 0.594
[62,     1] loss: 0.589
[63,     1] loss: 0.584
[64,     1] loss: 0.576
[65,     1] loss: 0.569
[66,     1] loss: 0.559
[67,     1] loss: 0.548
[68,     1] loss: 0.537
[69,     1] loss: 0.524
[70,     1] loss: 0.513
[71,     1] loss: 0.500
[72,     1] loss: 0.485
[73,     1] loss: 0.498
[74,     1] loss: 0.505
[75,     1] loss: 0.571
[76,     1] loss: 0.535
[77,     1] loss: 0.507
[78,     1] loss: 0.504
[79,     1] loss: 0.483
[80,     1] loss: 0.472
[81,     1] loss: 0.459
[82,     1] loss: 0.449
[83,     1] loss: 0.438
[84,     1] loss: 0.431
[85,     1] loss: 0.428
[86,     1] loss: 0.421
[87,     1] loss: 0.417
[88,     1] loss: 0.416
[89,     1] loss: 0.412
[90,     1] loss: 0.410
Early stopping applied (best metric=0.4299635589122772)
Finished Training
Total time taken: 157.35307455062866
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.664
[4,     1] loss: 0.647
[5,     1] loss: 0.629
[6,     1] loss: 0.611
[7,     1] loss: 0.590
[8,     1] loss: 0.568
[9,     1] loss: 0.544
[10,     1] loss: 0.519
[11,     1] loss: 0.496
[12,     1] loss: 0.473
[13,     1] loss: 0.454
[14,     1] loss: 0.443
[15,     1] loss: 0.439
[16,     1] loss: 0.422
[17,     1] loss: 0.405
[18,     1] loss: 0.388
[19,     1] loss: 0.378
[20,     1] loss: 0.367
[21,     1] loss: 0.356
[22,     1] loss: 0.347
[23,     1] loss: 0.337
[24,     1] loss: 0.325
[25,     1] loss: 0.320
[26,     1] loss: 0.320
[27,     1] loss: 0.302
[28,     1] loss: 0.290
[29,     1] loss: 0.286
[30,     1] loss: 0.275
[31,     1] loss: 0.265
[32,     1] loss: 0.251
[33,     1] loss: 0.236
[34,     1] loss: 0.223
[35,     1] loss: 0.218
[36,     1] loss: 0.318
[37,     1] loss: 0.262
[38,     1] loss: 0.662
[39,     1] loss: 0.377
[40,     1] loss: 0.408
[41,     1] loss: 0.421
[42,     1] loss: 0.436
[43,     1] loss: 0.444
[44,     1] loss: 0.449
[45,     1] loss: 0.443
[46,     1] loss: 0.433
[47,     1] loss: 0.418
[48,     1] loss: 0.405
[49,     1] loss: 0.385
[50,     1] loss: 0.364
[51,     1] loss: 0.344
[52,     1] loss: 0.327
[53,     1] loss: 0.308
[54,     1] loss: 0.290
[55,     1] loss: 0.274
[56,     1] loss: 0.257
[57,     1] loss: 0.249
[58,     1] loss: 0.268
[59,     1] loss: 0.293
[60,     1] loss: 0.321
[61,     1] loss: 0.345
[62,     1] loss: 0.326
[63,     1] loss: 0.300
[64,     1] loss: 0.300
[65,     1] loss: 0.293
[66,     1] loss: 0.282
[67,     1] loss: 0.270
[68,     1] loss: 0.260
[69,     1] loss: 0.249
[70,     1] loss: 0.236
[71,     1] loss: 0.226
[72,     1] loss: 0.216
[73,     1] loss: 0.209
[74,     1] loss: 0.207
[75,     1] loss: 0.251
[76,     1] loss: 0.223
[77,     1] loss: 0.247
[78,     1] loss: 0.253
[79,     1] loss: 0.233
[80,     1] loss: 0.236
[81,     1] loss: 0.285
[82,     1] loss: 0.345
[83,     1] loss: 0.470
[84,     1] loss: 0.377
[85,     1] loss: 0.336
[86,     1] loss: 0.354
[87,     1] loss: 0.344
[88,     1] loss: 0.322
[89,     1] loss: 0.312
[90,     1] loss: 0.303
[91,     1] loss: 0.291
[92,     1] loss: 0.278
[93,     1] loss: 0.269
[94,     1] loss: 0.258
[95,     1] loss: 0.250
[96,     1] loss: 0.237
[97,     1] loss: 0.231
[98,     1] loss: 0.222
[99,     1] loss: 0.216
[100,     1] loss: 0.210
[101,     1] loss: 0.207
[102,     1] loss: 0.203
[103,     1] loss: 0.199
[104,     1] loss: 0.201
[105,     1] loss: 0.230
[106,     1] loss: 0.306
[107,     1] loss: 0.283
[108,     1] loss: 0.270
[109,     1] loss: 0.253
[110,     1] loss: 0.243
[111,     1] loss: 0.241
[112,     1] loss: 0.224
[113,     1] loss: 0.219
[114,     1] loss: 0.216
[115,     1] loss: 0.274
[116,     1] loss: 0.292
[117,     1] loss: 0.292
[118,     1] loss: 0.284
[119,     1] loss: 0.265
[120,     1] loss: 0.264
[121,     1] loss: 0.257
[122,     1] loss: 0.243
[123,     1] loss: 0.233
[124,     1] loss: 0.222
[125,     1] loss: 0.213
Early stopping applied (best metric=0.34729281067848206)
Finished Training
Total time taken: 218.89000868797302
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.678
[4,     1] loss: 0.666
[5,     1] loss: 0.651
[6,     1] loss: 0.635
[7,     1] loss: 0.616
[8,     1] loss: 0.595
[9,     1] loss: 0.573
[10,     1] loss: 0.549
[11,     1] loss: 0.523
[12,     1] loss: 0.500
[13,     1] loss: 0.474
[14,     1] loss: 0.451
[15,     1] loss: 0.424
[16,     1] loss: 0.402
[17,     1] loss: 0.380
[18,     1] loss: 0.356
[19,     1] loss: 0.331
[20,     1] loss: 0.308
[21,     1] loss: 0.290
[22,     1] loss: 0.272
[23,     1] loss: 0.255
[24,     1] loss: 0.240
[25,     1] loss: 0.242
[26,     1] loss: 0.230
[27,     1] loss: 0.223
[28,     1] loss: 0.236
[29,     1] loss: 0.237
[30,     1] loss: 0.329
[31,     1] loss: 0.302
[32,     1] loss: 0.270
[33,     1] loss: 0.246
[34,     1] loss: 0.270
[35,     1] loss: 0.251
[36,     1] loss: 0.246
[37,     1] loss: 0.241
[38,     1] loss: 0.233
[39,     1] loss: 0.226
[40,     1] loss: 0.216
[41,     1] loss: 0.209
[42,     1] loss: 0.206
[43,     1] loss: 0.198
[44,     1] loss: 0.195
[45,     1] loss: 0.191
[46,     1] loss: 0.189
[47,     1] loss: 0.187
[48,     1] loss: 0.185
[49,     1] loss: 0.183
[50,     1] loss: 0.183
[51,     1] loss: 0.182
[52,     1] loss: 0.181
[53,     1] loss: 0.181
[54,     1] loss: 0.180
[55,     1] loss: 0.181
[56,     1] loss: 0.181
[57,     1] loss: 0.184
[58,     1] loss: 0.196
[59,     1] loss: 0.204
[60,     1] loss: 0.248
[61,     1] loss: 0.466
[62,     1] loss: 0.578
[63,     1] loss: 0.557
[64,     1] loss: 0.488
[65,     1] loss: 0.463
[66,     1] loss: 0.449
[67,     1] loss: 0.439
[68,     1] loss: 0.421
[69,     1] loss: 0.401
[70,     1] loss: 0.397
[71,     1] loss: 0.382
[72,     1] loss: 0.373
[73,     1] loss: 0.358
[74,     1] loss: 0.341
[75,     1] loss: 0.328
[76,     1] loss: 0.313
[77,     1] loss: 0.294
[78,     1] loss: 0.283
[79,     1] loss: 0.262
[80,     1] loss: 0.250
[81,     1] loss: 0.236
[82,     1] loss: 0.222
[83,     1] loss: 0.220
[84,     1] loss: 0.245
[85,     1] loss: 0.240
[86,     1] loss: 0.233
[87,     1] loss: 0.225
[88,     1] loss: 0.214
[89,     1] loss: 0.202
[90,     1] loss: 0.198
[91,     1] loss: 0.185
[92,     1] loss: 0.202
[93,     1] loss: 0.184
[94,     1] loss: 0.178
[95,     1] loss: 0.177
[96,     1] loss: 0.171
[97,     1] loss: 0.168
[98,     1] loss: 0.210
[99,     1] loss: 0.182
[100,     1] loss: 0.217
[101,     1] loss: 0.330
[102,     1] loss: 0.693
[103,     1] loss: 0.772
[104,     1] loss: 0.759
[105,     1] loss: 0.713
[106,     1] loss: 0.674
[107,     1] loss: 0.656
[108,     1] loss: 0.658
[109,     1] loss: 0.664
[110,     1] loss: 0.670
[111,     1] loss: 0.674
[112,     1] loss: 0.678
[113,     1] loss: 0.680
[114,     1] loss: 0.682
[115,     1] loss: 0.683
[116,     1] loss: 0.685
[117,     1] loss: 0.686
[118,     1] loss: 0.687
[119,     1] loss: 0.688
[120,     1] loss: 0.689
[121,     1] loss: 0.690
[122,     1] loss: 0.690
[123,     1] loss: 0.691
[124,     1] loss: 0.691
[125,     1] loss: 0.692
[126,     1] loss: 0.692
[127,     1] loss: 0.692
[128,     1] loss: 0.692
[129,     1] loss: 0.693
Early stopping applied (best metric=0.38057029247283936)
Finished Training
Total time taken: 226.65291261672974
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.688
[3,     1] loss: 0.674
[4,     1] loss: 0.662
[5,     1] loss: 0.648
[6,     1] loss: 0.632
[7,     1] loss: 0.614
[8,     1] loss: 0.590
[9,     1] loss: 0.567
[10,     1] loss: 0.539
[11,     1] loss: 0.509
[12,     1] loss: 0.478
[13,     1] loss: 0.450
[14,     1] loss: 0.425
[15,     1] loss: 0.400
[16,     1] loss: 0.373
[17,     1] loss: 0.351
[18,     1] loss: 0.323
[19,     1] loss: 0.306
[20,     1] loss: 0.283
[21,     1] loss: 0.261
[22,     1] loss: 0.244
[23,     1] loss: 0.228
[24,     1] loss: 0.210
[25,     1] loss: 0.199
[26,     1] loss: 0.187
[27,     1] loss: 0.177
[28,     1] loss: 0.167
[29,     1] loss: 0.158
[30,     1] loss: 0.150
[31,     1] loss: 0.140
[32,     1] loss: 0.129
[33,     1] loss: 0.119
[34,     1] loss: 0.208
[35,     1] loss: 0.600
[36,     1] loss: 0.412
[37,     1] loss: 0.393
[38,     1] loss: 0.411
[39,     1] loss: 0.413
[40,     1] loss: 0.419
[41,     1] loss: 0.421
[42,     1] loss: 0.418
[43,     1] loss: 0.409
[44,     1] loss: 0.386
[45,     1] loss: 0.366
[46,     1] loss: 0.346
[47,     1] loss: 0.325
[48,     1] loss: 0.303
[49,     1] loss: 0.285
[50,     1] loss: 0.282
[51,     1] loss: 0.264
[52,     1] loss: 0.239
[53,     1] loss: 0.231
[54,     1] loss: 0.246
[55,     1] loss: 0.234
[56,     1] loss: 0.220
[57,     1] loss: 0.208
[58,     1] loss: 0.193
[59,     1] loss: 0.187
[60,     1] loss: 0.176
[61,     1] loss: 0.169
[62,     1] loss: 0.159
[63,     1] loss: 0.175
[64,     1] loss: 0.205
[65,     1] loss: 0.211
[66,     1] loss: 0.213
[67,     1] loss: 0.202
[68,     1] loss: 0.197
[69,     1] loss: 0.199
[70,     1] loss: 0.221
[71,     1] loss: 0.429
[72,     1] loss: 0.332
[73,     1] loss: 0.324
[74,     1] loss: 0.267
[75,     1] loss: 0.269
[76,     1] loss: 0.245
[77,     1] loss: 0.243
[78,     1] loss: 0.227
[79,     1] loss: 0.221
[80,     1] loss: 0.216
[81,     1] loss: 0.198
[82,     1] loss: 0.194
[83,     1] loss: 0.188
[84,     1] loss: 0.177
[85,     1] loss: 0.167
[86,     1] loss: 0.158
[87,     1] loss: 0.155
[88,     1] loss: 0.149
[89,     1] loss: 0.147
[90,     1] loss: 0.143
[91,     1] loss: 0.141
[92,     1] loss: 0.138
[93,     1] loss: 0.136
[94,     1] loss: 0.137
[95,     1] loss: 0.135
[96,     1] loss: 0.135
[97,     1] loss: 0.143
[98,     1] loss: 0.366
[99,     1] loss: 0.253
[100,     1] loss: 0.571
[101,     1] loss: 0.669
[102,     1] loss: 0.482
[103,     1] loss: 0.453
[104,     1] loss: 0.510
[105,     1] loss: 0.524
[106,     1] loss: 0.518
[107,     1] loss: 0.505
[108,     1] loss: 0.498
[109,     1] loss: 0.486
[110,     1] loss: 0.479
[111,     1] loss: 0.468
[112,     1] loss: 0.459
[113,     1] loss: 0.444
[114,     1] loss: 0.437
[115,     1] loss: 0.432
[116,     1] loss: 0.424
[117,     1] loss: 0.400
[118,     1] loss: 0.390
[119,     1] loss: 0.377
[120,     1] loss: 0.354
[121,     1] loss: 0.333
[122,     1] loss: 0.315
[123,     1] loss: 0.316
[124,     1] loss: 0.353
[125,     1] loss: 0.410
[126,     1] loss: 0.354
[127,     1] loss: 0.334
[128,     1] loss: 0.319
[129,     1] loss: 0.293
Early stopping applied (best metric=0.3668835461139679)
Finished Training
Total time taken: 227.70622754096985
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.686
[3,     1] loss: 0.667
[4,     1] loss: 0.653
[5,     1] loss: 0.636
[6,     1] loss: 0.613
[7,     1] loss: 0.592
[8,     1] loss: 0.567
[9,     1] loss: 0.545
[10,     1] loss: 0.524
[11,     1] loss: 0.504
[12,     1] loss: 0.497
[13,     1] loss: 0.488
[14,     1] loss: 0.454
[15,     1] loss: 0.454
[16,     1] loss: 0.431
[17,     1] loss: 0.426
[18,     1] loss: 0.409
[19,     1] loss: 0.400
[20,     1] loss: 0.394
[21,     1] loss: 0.386
[22,     1] loss: 0.377
[23,     1] loss: 0.376
[24,     1] loss: 0.387
[25,     1] loss: 0.392
[26,     1] loss: 0.384
[27,     1] loss: 0.364
[28,     1] loss: 0.362
[29,     1] loss: 0.352
[30,     1] loss: 0.346
[31,     1] loss: 0.339
[32,     1] loss: 0.325
[33,     1] loss: 0.313
[34,     1] loss: 0.300
[35,     1] loss: 0.290
[36,     1] loss: 0.275
[37,     1] loss: 0.259
[38,     1] loss: 0.245
[39,     1] loss: 0.238
[40,     1] loss: 0.342
[41,     1] loss: 0.335
[42,     1] loss: 0.459
[43,     1] loss: 0.436
[44,     1] loss: 0.442
[45,     1] loss: 0.440
[46,     1] loss: 0.436
[47,     1] loss: 0.450
[48,     1] loss: 0.432
[49,     1] loss: 0.414
[50,     1] loss: 0.391
[51,     1] loss: 0.359
[52,     1] loss: 0.328
[53,     1] loss: 0.301
[54,     1] loss: 0.267
[55,     1] loss: 0.241
[56,     1] loss: 0.229
[57,     1] loss: 0.199
[58,     1] loss: 0.180
[59,     1] loss: 0.191
[60,     1] loss: 0.159
[61,     1] loss: 0.158
[62,     1] loss: 0.135
[63,     1] loss: 0.150
[64,     1] loss: 0.229
[65,     1] loss: 0.215
[66,     1] loss: 0.197
[67,     1] loss: 0.194
[68,     1] loss: 0.183
[69,     1] loss: 0.182
[70,     1] loss: 0.165
[71,     1] loss: 0.156
[72,     1] loss: 0.141
[73,     1] loss: 0.128
[74,     1] loss: 0.114
[75,     1] loss: 0.108
[76,     1] loss: 0.107
[77,     1] loss: 0.165
[78,     1] loss: 0.287
[79,     1] loss: 1.409
[80,     1] loss: 0.953
[81,     1] loss: 0.660
[82,     1] loss: 0.619
[83,     1] loss: 0.664
[84,     1] loss: 0.654
[85,     1] loss: 0.636
[86,     1] loss: 0.633
[87,     1] loss: 0.640
[88,     1] loss: 0.648
[89,     1] loss: 0.654
[90,     1] loss: 0.656
[91,     1] loss: 0.657
[92,     1] loss: 0.657
[93,     1] loss: 0.657
[94,     1] loss: 0.656
[95,     1] loss: 0.655
[96,     1] loss: 0.655
[97,     1] loss: 0.655
[98,     1] loss: 0.654
[99,     1] loss: 0.654
[100,     1] loss: 0.654
[101,     1] loss: 0.654
[102,     1] loss: 0.654
[103,     1] loss: 0.654
Early stopping applied (best metric=0.38184159994125366)
Finished Training
Total time taken: 182.8097894191742
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.684
[3,     1] loss: 0.661
[4,     1] loss: 0.640
[5,     1] loss: 0.617
[6,     1] loss: 0.592
[7,     1] loss: 0.565
[8,     1] loss: 0.539
[9,     1] loss: 0.509
[10,     1] loss: 0.485
[11,     1] loss: 0.468
[12,     1] loss: 0.445
[13,     1] loss: 0.416
[14,     1] loss: 0.398
[15,     1] loss: 0.382
[16,     1] loss: 0.365
[17,     1] loss: 0.350
[18,     1] loss: 0.332
[19,     1] loss: 0.318
[20,     1] loss: 0.307
[21,     1] loss: 0.301
[22,     1] loss: 0.302
[23,     1] loss: 0.280
[24,     1] loss: 0.266
[25,     1] loss: 0.263
[26,     1] loss: 0.251
[27,     1] loss: 0.235
[28,     1] loss: 0.219
[29,     1] loss: 0.205
[30,     1] loss: 0.191
[31,     1] loss: 0.177
[32,     1] loss: 0.161
[33,     1] loss: 0.148
[34,     1] loss: 0.137
[35,     1] loss: 0.134
[36,     1] loss: 0.276
[37,     1] loss: 0.499
[38,     1] loss: 0.302
[39,     1] loss: 0.542
[40,     1] loss: 0.436
[41,     1] loss: 0.495
[42,     1] loss: 0.525
[43,     1] loss: 0.548
[44,     1] loss: 0.567
[45,     1] loss: 0.577
[46,     1] loss: 0.589
[47,     1] loss: 0.596
[48,     1] loss: 0.601
[49,     1] loss: 0.604
[50,     1] loss: 0.608
[51,     1] loss: 0.611
[52,     1] loss: 0.612
[53,     1] loss: 0.613
[54,     1] loss: 0.614
[55,     1] loss: 0.614
[56,     1] loss: 0.615
[57,     1] loss: 0.614
[58,     1] loss: 0.614
[59,     1] loss: 0.614
[60,     1] loss: 0.613
[61,     1] loss: 0.611
[62,     1] loss: 0.609
[63,     1] loss: 0.607
[64,     1] loss: 0.604
[65,     1] loss: 0.601
[66,     1] loss: 0.597
[67,     1] loss: 0.592
[68,     1] loss: 0.585
[69,     1] loss: 0.580
Early stopping applied (best metric=0.35489848256111145)
Finished Training
Total time taken: 123.49940419197083
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.692
[3,     1] loss: 0.672
[4,     1] loss: 0.657
[5,     1] loss: 0.641
[6,     1] loss: 0.621
[7,     1] loss: 0.600
[8,     1] loss: 0.575
[9,     1] loss: 0.548
[10,     1] loss: 0.520
[11,     1] loss: 0.493
[12,     1] loss: 0.465
[13,     1] loss: 0.445
[14,     1] loss: 0.421
[15,     1] loss: 0.400
[16,     1] loss: 0.379
[17,     1] loss: 0.358
[18,     1] loss: 0.341
[19,     1] loss: 0.328
[20,     1] loss: 0.313
[21,     1] loss: 0.300
[22,     1] loss: 0.287
[23,     1] loss: 0.273
[24,     1] loss: 0.264
[25,     1] loss: 0.249
[26,     1] loss: 0.240
[27,     1] loss: 0.231
[28,     1] loss: 0.218
[29,     1] loss: 0.208
[30,     1] loss: 0.200
[31,     1] loss: 0.190
[32,     1] loss: 0.181
[33,     1] loss: 0.169
[34,     1] loss: 0.163
[35,     1] loss: 0.165
[36,     1] loss: 0.152
[37,     1] loss: 0.182
[38,     1] loss: 0.286
[39,     1] loss: 0.418
[40,     1] loss: 0.261
[41,     1] loss: 0.337
[42,     1] loss: 0.318
[43,     1] loss: 0.309
[44,     1] loss: 0.375
[45,     1] loss: 0.321
[46,     1] loss: 0.355
[47,     1] loss: 0.305
[48,     1] loss: 0.291
[49,     1] loss: 0.279
[50,     1] loss: 0.284
[51,     1] loss: 0.265
[52,     1] loss: 0.263
[53,     1] loss: 0.257
[54,     1] loss: 0.239
[55,     1] loss: 0.229
[56,     1] loss: 0.223
[57,     1] loss: 0.216
[58,     1] loss: 0.207
[59,     1] loss: 0.208
[60,     1] loss: 0.220
[61,     1] loss: 0.218
[62,     1] loss: 0.219
[63,     1] loss: 0.221
[64,     1] loss: 0.211
[65,     1] loss: 0.207
[66,     1] loss: 0.213
[67,     1] loss: 0.205
[68,     1] loss: 0.214
[69,     1] loss: 0.213
[70,     1] loss: 0.210
[71,     1] loss: 0.203
[72,     1] loss: 0.195
[73,     1] loss: 0.203
[74,     1] loss: 0.203
[75,     1] loss: 0.211
[76,     1] loss: 0.203
[77,     1] loss: 0.199
[78,     1] loss: 0.195
[79,     1] loss: 0.188
[80,     1] loss: 0.216
[81,     1] loss: 0.197
[82,     1] loss: 0.201
[83,     1] loss: 0.198
[84,     1] loss: 0.193
[85,     1] loss: 0.186
[86,     1] loss: 0.182
Early stopping applied (best metric=0.30622342228889465)
Finished Training
Total time taken: 153.87008380889893
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.683
[3,     1] loss: 0.659
[4,     1] loss: 0.637
[5,     1] loss: 0.615
[6,     1] loss: 0.590
[7,     1] loss: 0.565
[8,     1] loss: 0.534
[9,     1] loss: 0.505
[10,     1] loss: 0.470
[11,     1] loss: 0.438
[12,     1] loss: 0.410
[13,     1] loss: 0.392
[14,     1] loss: 0.365
[15,     1] loss: 0.339
[16,     1] loss: 0.319
[17,     1] loss: 0.301
[18,     1] loss: 0.285
[19,     1] loss: 0.263
[20,     1] loss: 0.250
[21,     1] loss: 0.235
[22,     1] loss: 0.223
[23,     1] loss: 0.211
[24,     1] loss: 0.202
[25,     1] loss: 0.188
[26,     1] loss: 0.173
[27,     1] loss: 0.166
[28,     1] loss: 0.157
[29,     1] loss: 0.151
[30,     1] loss: 0.148
[31,     1] loss: 0.150
[32,     1] loss: 0.149
[33,     1] loss: 0.143
[34,     1] loss: 0.128
[35,     1] loss: 0.124
[36,     1] loss: 0.113
[37,     1] loss: 0.141
[38,     1] loss: 0.343
[39,     1] loss: 0.860
[40,     1] loss: 0.541
[41,     1] loss: 0.559
[42,     1] loss: 0.595
[43,     1] loss: 0.587
[44,     1] loss: 0.587
[45,     1] loss: 0.600
[46,     1] loss: 0.612
[47,     1] loss: 0.621
[48,     1] loss: 0.629
[49,     1] loss: 0.635
[50,     1] loss: 0.640
[51,     1] loss: 0.644
[52,     1] loss: 0.647
[53,     1] loss: 0.650
[54,     1] loss: 0.652
[55,     1] loss: 0.655
[56,     1] loss: 0.657
[57,     1] loss: 0.660
[58,     1] loss: 0.661
[59,     1] loss: 0.663
[60,     1] loss: 0.664
[61,     1] loss: 0.666
[62,     1] loss: 0.668
[63,     1] loss: 0.669
[64,     1] loss: 0.671
Early stopping applied (best metric=0.41806623339653015)
Finished Training
Total time taken: 115.2590537071228
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.654
[5,     1] loss: 0.635
[6,     1] loss: 0.612
[7,     1] loss: 0.593
[8,     1] loss: 0.570
[9,     1] loss: 0.544
[10,     1] loss: 0.517
[11,     1] loss: 0.490
[12,     1] loss: 0.458
[13,     1] loss: 0.428
[14,     1] loss: 0.399
[15,     1] loss: 0.364
[16,     1] loss: 0.334
[17,     1] loss: 0.309
[18,     1] loss: 0.290
[19,     1] loss: 0.270
[20,     1] loss: 0.257
[21,     1] loss: 0.236
[22,     1] loss: 0.221
[23,     1] loss: 0.207
[24,     1] loss: 0.196
[25,     1] loss: 0.182
[26,     1] loss: 0.172
[27,     1] loss: 0.162
[28,     1] loss: 0.152
[29,     1] loss: 0.148
[30,     1] loss: 0.152
[31,     1] loss: 0.141
[32,     1] loss: 0.132
[33,     1] loss: 0.124
[34,     1] loss: 0.157
[35,     1] loss: 0.170
[36,     1] loss: 0.560
[37,     1] loss: 0.410
[38,     1] loss: 0.465
[39,     1] loss: 0.442
[40,     1] loss: 0.437
[41,     1] loss: 0.459
[42,     1] loss: 0.464
[43,     1] loss: 0.460
[44,     1] loss: 0.458
[45,     1] loss: 0.455
[46,     1] loss: 0.438
[47,     1] loss: 0.420
[48,     1] loss: 0.398
[49,     1] loss: 0.377
[50,     1] loss: 0.358
[51,     1] loss: 0.336
[52,     1] loss: 0.310
[53,     1] loss: 0.291
[54,     1] loss: 0.269
[55,     1] loss: 0.252
[56,     1] loss: 0.233
[57,     1] loss: 0.216
[58,     1] loss: 0.201
[59,     1] loss: 0.186
[60,     1] loss: 0.176
[61,     1] loss: 0.167
[62,     1] loss: 0.157
[63,     1] loss: 0.150
[64,     1] loss: 0.146
[65,     1] loss: 0.143
[66,     1] loss: 0.137
[67,     1] loss: 0.135
[68,     1] loss: 0.135
[69,     1] loss: 0.132
[70,     1] loss: 0.130
[71,     1] loss: 0.130
[72,     1] loss: 0.128
[73,     1] loss: 0.128
[74,     1] loss: 0.127
[75,     1] loss: 0.287
[76,     1] loss: 0.414
[77,     1] loss: 0.698
[78,     1] loss: 0.643
[79,     1] loss: 0.552
[80,     1] loss: 0.578
[81,     1] loss: 0.604
[82,     1] loss: 0.617
[83,     1] loss: 0.623
[84,     1] loss: 0.627
[85,     1] loss: 0.630
[86,     1] loss: 0.633
[87,     1] loss: 0.637
[88,     1] loss: 0.641
[89,     1] loss: 0.646
[90,     1] loss: 0.650
[91,     1] loss: 0.654
[92,     1] loss: 0.658
[93,     1] loss: 0.661
[94,     1] loss: 0.665
[95,     1] loss: 0.668
[96,     1] loss: 0.671
[97,     1] loss: 0.674
[98,     1] loss: 0.676
[99,     1] loss: 0.678
[100,     1] loss: 0.680
[101,     1] loss: 0.682
[102,     1] loss: 0.684
[103,     1] loss: 0.686
[104,     1] loss: 0.687
[105,     1] loss: 0.688
[106,     1] loss: 0.689
[107,     1] loss: 0.690
[108,     1] loss: 0.690
[109,     1] loss: 0.691
Early stopping applied (best metric=0.3495943546295166)
Finished Training
Total time taken: 195.68911600112915
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.632
[5,     1] loss: 0.610
[6,     1] loss: 0.587
[7,     1] loss: 0.564
[8,     1] loss: 0.539
[9,     1] loss: 0.512
[10,     1] loss: 0.486
[11,     1] loss: 0.459
[12,     1] loss: 0.438
[13,     1] loss: 0.434
[14,     1] loss: 0.398
[15,     1] loss: 0.388
[16,     1] loss: 0.374
[17,     1] loss: 0.359
[18,     1] loss: 0.342
[19,     1] loss: 0.328
[20,     1] loss: 0.321
[21,     1] loss: 0.307
[22,     1] loss: 0.296
[23,     1] loss: 0.287
[24,     1] loss: 0.273
[25,     1] loss: 0.263
[26,     1] loss: 0.252
[27,     1] loss: 0.240
[28,     1] loss: 0.230
[29,     1] loss: 0.216
[30,     1] loss: 0.213
[31,     1] loss: 0.288
[32,     1] loss: 0.604
[33,     1] loss: 0.317
[34,     1] loss: 0.394
[35,     1] loss: 0.400
[36,     1] loss: 0.406
[37,     1] loss: 0.409
[38,     1] loss: 0.407
[39,     1] loss: 0.399
[40,     1] loss: 0.381
[41,     1] loss: 0.365
[42,     1] loss: 0.344
[43,     1] loss: 0.323
[44,     1] loss: 0.299
[45,     1] loss: 0.283
[46,     1] loss: 0.289
[47,     1] loss: 0.264
[48,     1] loss: 0.243
[49,     1] loss: 0.236
[50,     1] loss: 0.254
[51,     1] loss: 0.240
[52,     1] loss: 0.224
[53,     1] loss: 0.206
[54,     1] loss: 0.209
[55,     1] loss: 0.203
[56,     1] loss: 0.197
[57,     1] loss: 0.182
[58,     1] loss: 0.179
[59,     1] loss: 0.172
[60,     1] loss: 0.163
[61,     1] loss: 0.155
[62,     1] loss: 0.151
[63,     1] loss: 0.156
[64,     1] loss: 0.309
Early stopping applied (best metric=0.3703171908855438)
Finished Training
Total time taken: 115.84926223754883
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.680
[3,     1] loss: 0.655
[4,     1] loss: 0.625
[5,     1] loss: 0.596
[6,     1] loss: 0.561
[7,     1] loss: 0.526
[8,     1] loss: 0.490
[9,     1] loss: 0.458
[10,     1] loss: 0.426
[11,     1] loss: 0.404
[12,     1] loss: 0.387
[13,     1] loss: 0.376
[14,     1] loss: 0.366
[15,     1] loss: 0.357
[16,     1] loss: 0.351
[17,     1] loss: 0.345
[18,     1] loss: 0.340
[19,     1] loss: 0.333
[20,     1] loss: 0.327
[21,     1] loss: 0.321
[22,     1] loss: 0.313
[23,     1] loss: 0.308
[24,     1] loss: 0.297
[25,     1] loss: 0.293
[26,     1] loss: 0.284
[27,     1] loss: 0.273
[28,     1] loss: 0.261
[29,     1] loss: 0.253
[30,     1] loss: 0.242
[31,     1] loss: 0.231
[32,     1] loss: 0.221
[33,     1] loss: 0.212
[34,     1] loss: 0.201
[35,     1] loss: 0.188
[36,     1] loss: 0.176
[37,     1] loss: 0.161
[38,     1] loss: 0.147
[39,     1] loss: 0.148
[40,     1] loss: 0.241
[41,     1] loss: 0.290
[42,     1] loss: 0.824
[43,     1] loss: 0.708
[44,     1] loss: 0.594
[45,     1] loss: 0.653
[46,     1] loss: 0.649
[47,     1] loss: 0.636
[48,     1] loss: 0.637
[49,     1] loss: 0.641
[50,     1] loss: 0.647
[51,     1] loss: 0.652
[52,     1] loss: 0.655
[53,     1] loss: 0.658
[54,     1] loss: 0.661
[55,     1] loss: 0.662
[56,     1] loss: 0.663
[57,     1] loss: 0.664
[58,     1] loss: 0.665
Early stopping applied (best metric=0.4513399600982666)
Finished Training
Total time taken: 105.35055422782898
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.687
[3,     1] loss: 0.667
[4,     1] loss: 0.650
[5,     1] loss: 0.631
[6,     1] loss: 0.612
[7,     1] loss: 0.592
[8,     1] loss: 0.565
[9,     1] loss: 0.539
[10,     1] loss: 0.511
[11,     1] loss: 0.483
[12,     1] loss: 0.453
[13,     1] loss: 0.430
[14,     1] loss: 0.392
[15,     1] loss: 0.366
[16,     1] loss: 0.341
[17,     1] loss: 0.312
[18,     1] loss: 0.314
[19,     1] loss: 0.344
[20,     1] loss: 0.276
[21,     1] loss: 0.323
[22,     1] loss: 0.266
[23,     1] loss: 0.297
[24,     1] loss: 0.265
[25,     1] loss: 0.257
[26,     1] loss: 0.250
[27,     1] loss: 0.236
[28,     1] loss: 0.223
[29,     1] loss: 0.212
[30,     1] loss: 0.204
[31,     1] loss: 0.191
[32,     1] loss: 0.182
[33,     1] loss: 0.173
[34,     1] loss: 0.174
[35,     1] loss: 0.191
[36,     1] loss: 0.168
[37,     1] loss: 0.162
[38,     1] loss: 0.151
[39,     1] loss: 0.170
[40,     1] loss: 0.270
[41,     1] loss: 0.707
[42,     1] loss: 0.346
[43,     1] loss: 0.379
[44,     1] loss: 0.412
[45,     1] loss: 0.428
[46,     1] loss: 0.439
[47,     1] loss: 0.441
[48,     1] loss: 0.436
[49,     1] loss: 0.428
[50,     1] loss: 0.419
[51,     1] loss: 0.406
[52,     1] loss: 0.393
[53,     1] loss: 0.376
[54,     1] loss: 0.358
[55,     1] loss: 0.344
[56,     1] loss: 0.329
[57,     1] loss: 0.310
[58,     1] loss: 0.295
[59,     1] loss: 0.277
[60,     1] loss: 0.259
[61,     1] loss: 0.251
[62,     1] loss: 0.236
[63,     1] loss: 0.239
[64,     1] loss: 0.220
[65,     1] loss: 0.210
[66,     1] loss: 0.193
[67,     1] loss: 0.188
[68,     1] loss: 0.210
[69,     1] loss: 0.205
[70,     1] loss: 0.237
[71,     1] loss: 0.244
[72,     1] loss: 0.226
[73,     1] loss: 0.193
[74,     1] loss: 0.180
[75,     1] loss: 0.180
[76,     1] loss: 0.172
[77,     1] loss: 0.197
[78,     1] loss: 0.228
[79,     1] loss: 0.221
[80,     1] loss: 0.217
[81,     1] loss: 0.211
[82,     1] loss: 0.200
Early stopping applied (best metric=0.38854125142097473)
Finished Training
Total time taken: 148.46929478645325
{'Hydroxylation-P Validation Accuracy': 0.7923790061418202, 'Hydroxylation-P Validation Sensitivity': 0.783015873015873, 'Hydroxylation-P Validation Specificity': 0.7944096962442017, 'Hydroxylation-P Validation Precision': 0.4598479245397812, 'Hydroxylation-P AUC ROC': 0.8572724099765099, 'Hydroxylation-P AUC PR': 0.60353342020509, 'Hydroxylation-P MCC': 0.48326093216317323, 'Hydroxylation-P F1': 0.5745562492642131, 'Validation Loss (Hydroxylation-P)': 0.3836390244960785, 'Validation Loss (total)': 0.3836390244960785}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021456188828543007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2496177610,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.320061995927873}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.667
[4,     1] loss: 0.656
[5,     1] loss: 0.646
[6,     1] loss: 0.635
[7,     1] loss: 0.626
[8,     1] loss: 0.615
[9,     1] loss: 0.605
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006005266886381441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 244605148,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.279468270600262}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.693
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009727864712310868,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3608705504,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.197372549620768}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.684
[3,     1] loss: 0.647
[4,     1] loss: 0.616
[5,     1] loss: 0.581
[6,     1] loss: 0.541
[7,     1] loss: 0.488
[8,     1] loss: 0.450
[9,     1] loss: 0.425
[10,     1] loss: 0.344
[11,     1] loss: 0.316
[12,     1] loss: 0.254
[13,     1] loss: 0.233
[14,     1] loss: 0.190
[15,     1] loss: 0.174
[16,     1] loss: 0.142
[17,     1] loss: 0.131
[18,     1] loss: 0.105
[19,     1] loss: 0.093
[20,     1] loss: 0.084
[21,     1] loss: 0.072
[22,     1] loss: 0.067
[23,     1] loss: 0.062
[24,     1] loss: 0.062
[25,     1] loss: 0.058
[26,     1] loss: 0.057
[27,     1] loss: 0.048
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021530770923978136,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2204111763,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.702885921915277}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.676
[4,     1] loss: 0.665
[5,     1] loss: 0.653
[6,     1] loss: 0.640
[7,     1] loss: 0.628
[8,     1] loss: 0.614
[9,     1] loss: 0.597
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009914284155686709,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1997077898,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.25490027465948}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.670
[3,     1] loss: 0.610
[4,     1] loss: 0.561
[5,     1] loss: 0.514
[6,     1] loss: 0.473
[7,     1] loss: 0.419
[8,     1] loss: 0.366
[9,     1] loss: 0.316
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00859171777894418,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4256192263,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.187652251680774}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.700
[3,     1] loss: 0.687
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009716283765046388,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3463576045,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.081548601353624}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009594029253157063,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2109195525,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.639717954533725}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.688
[3,     1] loss: 0.643
[4,     1] loss: 0.574
[5,     1] loss: 0.483
[6,     1] loss: 0.390
[7,     1] loss: 0.290
[8,     1] loss: 0.214
[9,     1] loss: 0.206
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018439799711351583,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1754935149,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.520134327914183}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00684992272647192,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1034803739,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.174833957034476}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.675
[4,     1] loss: 0.668
[5,     1] loss: 0.661
[6,     1] loss: 0.654
[7,     1] loss: 0.647
[8,     1] loss: 0.640
[9,     1] loss: 0.633
[10,     1] loss: 0.625
[11,     1] loss: 0.619
[12,     1] loss: 0.612
[13,     1] loss: 0.605
[14,     1] loss: 0.597
[15,     1] loss: 0.589
[16,     1] loss: 0.580
[17,     1] loss: 0.572
[18,     1] loss: 0.582
[19,     1] loss: 0.562
[20,     1] loss: 0.594
[21,     1] loss: 0.586
[22,     1] loss: 0.588
[23,     1] loss: 0.576
[24,     1] loss: 0.570
[25,     1] loss: 0.562
[26,     1] loss: 0.554
[27,     1] loss: 0.543
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008126094853632478,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3996754954,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.39683139039489}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.679
[3,     1] loss: 0.635
[4,     1] loss: 0.587
[5,     1] loss: 0.534
[6,     1] loss: 0.484
[7,     1] loss: 0.432
[8,     1] loss: 0.389
[9,     1] loss: 0.360
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007474890832022289,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1204939005,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.447993060570504}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.658
[4,     1] loss: 0.642
[5,     1] loss: 0.625
[6,     1] loss: 0.605
[7,     1] loss: 0.587
[8,     1] loss: 0.568
[9,     1] loss: 0.549
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00478479597854767,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1458187348,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.150000231210104}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.689
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005610218140574948,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3800811388,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.229152286853012}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.650
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006912426031233099,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4110328998,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.795322504637426}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.685
[3,     1] loss: 0.639
[4,     1] loss: 0.583
[5,     1] loss: 0.531
[6,     1] loss: 0.474
[7,     1] loss: 0.425
[8,     1] loss: 0.361
[9,     1] loss: 0.311
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004026725175096195,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1154899363,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.232845840374345}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.683
[3,     1] loss: 0.653
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007828467663132974,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1780351146,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.58530603283451}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.654
[4,     1] loss: 0.623
[5,     1] loss: 0.593
[6,     1] loss: 0.556
[7,     1] loss: 0.516
[8,     1] loss: 0.476
[9,     1] loss: 0.436
[10,     1] loss: 0.393
[11,     1] loss: 0.364
[12,     1] loss: 0.346
[13,     1] loss: 0.301
[14,     1] loss: 0.257
[15,     1] loss: 0.241
[16,     1] loss: 0.303
[17,     1] loss: 0.266
[18,     1] loss: 0.221
[19,     1] loss: 0.207
[20,     1] loss: 0.194
[21,     1] loss: 0.161
[22,     1] loss: 0.150
[23,     1] loss: 0.132
[24,     1] loss: 0.113
[25,     1] loss: 0.096
[26,     1] loss: 0.087
[27,     1] loss: 0.077
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005255814760245319,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2841940047,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.43222349117644}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008766293103325834,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3284243428,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.54254501407669}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005138481322851815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1182406241,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.011388648233011}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006580129891185629,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 929019165,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.23542022186916}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.686
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006729860387142453,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1441956754,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.365877201916547}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.674
[3,     1] loss: 0.642
[4,     1] loss: 0.617
[5,     1] loss: 0.594
[6,     1] loss: 0.574
[7,     1] loss: 0.553
[8,     1] loss: 0.533
[9,     1] loss: 0.513
[10,     1] loss: 0.492
[11,     1] loss: 0.474
[12,     1] loss: 0.461
[13,     1] loss: 0.446
[14,     1] loss: 0.434
[15,     1] loss: 0.422
[16,     1] loss: 0.411
[17,     1] loss: 0.401
[18,     1] loss: 0.390
[19,     1] loss: 0.376
[20,     1] loss: 0.364
[21,     1] loss: 0.353
[22,     1] loss: 0.341
[23,     1] loss: 0.325
[24,     1] loss: 0.312
[25,     1] loss: 0.295
[26,     1] loss: 0.280
[27,     1] loss: 0.265
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00046758951345199785,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4261861794,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.167075242665224}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.674
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006688368132410576,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 438928193,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.315898732454018}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.628
[4,     1] loss: 0.568
[5,     1] loss: 0.499
[6,     1] loss: 0.422
[7,     1] loss: 0.351
[8,     1] loss: 0.262
[9,     1] loss: 0.204
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00284457738591914,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1817745265,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.784844434077119}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.650
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038623027210124244,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1419984458,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.6105345052009676}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.656
[5,     1] loss: 0.627
[6,     1] loss: 0.587
[7,     1] loss: 0.544
[8,     1] loss: 0.491
[9,     1] loss: 0.429
[10,     1] loss: 0.362
[11,     1] loss: 0.294
[12,     1] loss: 0.236
[13,     1] loss: 0.182
[14,     1] loss: 0.143
[15,     1] loss: 0.110
[16,     1] loss: 0.086
[17,     1] loss: 0.068
[18,     1] loss: 0.056
[19,     1] loss: 0.046
[20,     1] loss: 0.036
[21,     1] loss: 0.028
[22,     1] loss: 0.024
[23,     1] loss: 0.018
[24,     1] loss: 0.013
[25,     1] loss: 0.010
[26,     1] loss: 0.008
[27,     1] loss: 0.006
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009964126017523254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 679355455,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.802374497796063}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.688
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050618918438009985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2024007907,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.669668339507409}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.657
[4,     1] loss: 0.626
[5,     1] loss: 0.592
[6,     1] loss: 0.555
[7,     1] loss: 0.515
[8,     1] loss: 0.474
[9,     1] loss: 0.431
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007703131165471781,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3730708480,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.1565633272385583}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.638
[4,     1] loss: 0.579
[5,     1] loss: 0.516
[6,     1] loss: 0.456
[7,     1] loss: 0.394
[8,     1] loss: 0.342
[9,     1] loss: 0.306
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009810291572995171,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2994975820,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.64216948160034}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006540030642205595,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2854839007,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.6401808906095585}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.680
[3,     1] loss: 0.643
[4,     1] loss: 0.596
[5,     1] loss: 0.533
[6,     1] loss: 0.464
[7,     1] loss: 0.384
[8,     1] loss: 0.307
[9,     1] loss: 0.239
[10,     1] loss: 0.237
[11,     1] loss: 0.186
[12,     1] loss: 0.153
[13,     1] loss: 0.121
[14,     1] loss: 0.097
[15,     1] loss: 0.092
[16,     1] loss: 0.073
[17,     1] loss: 0.050
[18,     1] loss: 0.062
[19,     1] loss: 0.039
[20,     1] loss: 0.039
[21,     1] loss: 0.030
[22,     1] loss: 0.028
[23,     1] loss: 0.024
[24,     1] loss: 0.020
[25,     1] loss: 0.018
[26,     1] loss: 0.018
[27,     1] loss: 0.017
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007241053192453046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 192165926,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.18013504996352}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.681
[3,     1] loss: 0.641
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012657482699965049,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1295731686,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.202387700449803}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.640
[6,     1] loss: 0.630
[7,     1] loss: 0.614
[8,     1] loss: 0.602
[9,     1] loss: 0.586
[10,     1] loss: 0.570
[11,     1] loss: 0.555
[12,     1] loss: 0.533
[13,     1] loss: 0.515
[14,     1] loss: 0.495
[15,     1] loss: 0.476
[16,     1] loss: 0.452
[17,     1] loss: 0.429
[18,     1] loss: 0.409
[19,     1] loss: 0.383
[20,     1] loss: 0.361
[21,     1] loss: 0.338
[22,     1] loss: 0.317
[23,     1] loss: 0.296
[24,     1] loss: 0.272
[25,     1] loss: 0.253
[26,     1] loss: 0.231
[27,     1] loss: 0.212
[28,     1] loss: 0.190
[29,     1] loss: 0.178
[30,     1] loss: 0.159
[31,     1] loss: 0.147
[32,     1] loss: 0.131
[33,     1] loss: 0.118
[34,     1] loss: 0.110
[35,     1] loss: 0.100
[36,     1] loss: 0.090
[37,     1] loss: 0.081
[38,     1] loss: 0.075
[39,     1] loss: 0.071
[40,     1] loss: 0.064
[41,     1] loss: 0.059
[42,     1] loss: 0.056
[43,     1] loss: 0.052
[44,     1] loss: 0.049
[45,     1] loss: 0.046
[46,     1] loss: 0.045
[47,     1] loss: 0.042
[48,     1] loss: 0.039
[49,     1] loss: 0.039
[50,     1] loss: 0.038
[51,     1] loss: 0.036
[52,     1] loss: 0.036
[53,     1] loss: 0.034
[54,     1] loss: 0.033
[55,     1] loss: 0.033
[56,     1] loss: 0.033
[57,     1] loss: 0.032
[58,     1] loss: 0.032
[59,     1] loss: 0.031
[60,     1] loss: 0.032
[61,     1] loss: 0.031
[62,     1] loss: 0.032
[63,     1] loss: 0.032
[64,     1] loss: 0.031
[65,     1] loss: 0.031
[66,     1] loss: 0.031
[67,     1] loss: 0.031
[68,     1] loss: 0.031
[69,     1] loss: 0.031
[70,     1] loss: 0.031
[71,     1] loss: 0.031
[72,     1] loss: 0.031
[73,     1] loss: 0.032
[74,     1] loss: 0.032
[75,     1] loss: 0.032
[76,     1] loss: 0.031
Early stopping applied (best metric=0.3503670394420624)
Finished Training
Total time taken: 142.4895875453949
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.691
[3,     1] loss: 0.679
[4,     1] loss: 0.669
[5,     1] loss: 0.657
[6,     1] loss: 0.646
[7,     1] loss: 0.634
[8,     1] loss: 0.621
[9,     1] loss: 0.605
[10,     1] loss: 0.593
[11,     1] loss: 0.577
[12,     1] loss: 0.561
[13,     1] loss: 0.542
[14,     1] loss: 0.524
[15,     1] loss: 0.505
[16,     1] loss: 0.485
[17,     1] loss: 0.463
[18,     1] loss: 0.444
[19,     1] loss: 0.420
[20,     1] loss: 0.394
[21,     1] loss: 0.376
[22,     1] loss: 0.352
[23,     1] loss: 0.332
[24,     1] loss: 0.309
[25,     1] loss: 0.290
[26,     1] loss: 0.267
[27,     1] loss: 0.250
[28,     1] loss: 0.232
[29,     1] loss: 0.215
[30,     1] loss: 0.197
[31,     1] loss: 0.184
[32,     1] loss: 0.166
[33,     1] loss: 0.153
[34,     1] loss: 0.142
[35,     1] loss: 0.132
[36,     1] loss: 0.121
[37,     1] loss: 0.111
[38,     1] loss: 0.105
[39,     1] loss: 0.096
[40,     1] loss: 0.088
[41,     1] loss: 0.085
[42,     1] loss: 0.080
[43,     1] loss: 0.073
[44,     1] loss: 0.071
[45,     1] loss: 0.067
[46,     1] loss: 0.063
[47,     1] loss: 0.060
[48,     1] loss: 0.057
[49,     1] loss: 0.055
[50,     1] loss: 0.054
[51,     1] loss: 0.051
[52,     1] loss: 0.050
[53,     1] loss: 0.048
[54,     1] loss: 0.047
[55,     1] loss: 0.047
[56,     1] loss: 0.046
[57,     1] loss: 0.046
[58,     1] loss: 0.046
[59,     1] loss: 0.045
[60,     1] loss: 0.044
[61,     1] loss: 0.044
[62,     1] loss: 0.043
[63,     1] loss: 0.042
[64,     1] loss: 0.043
[65,     1] loss: 0.042
[66,     1] loss: 0.042
[67,     1] loss: 0.042
[68,     1] loss: 0.042
[69,     1] loss: 0.042
[70,     1] loss: 0.041
[71,     1] loss: 0.042
[72,     1] loss: 0.043
[73,     1] loss: 0.041
[74,     1] loss: 0.041
[75,     1] loss: 0.041
[76,     1] loss: 0.040
[77,     1] loss: 0.040
[78,     1] loss: 0.040
Early stopping applied (best metric=0.36108970642089844)
Finished Training
Total time taken: 146.66108059883118
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.668
[4,     1] loss: 0.654
[5,     1] loss: 0.640
[6,     1] loss: 0.627
[7,     1] loss: 0.612
[8,     1] loss: 0.596
[9,     1] loss: 0.581
[10,     1] loss: 0.562
[11,     1] loss: 0.547
[12,     1] loss: 0.527
[13,     1] loss: 0.510
[14,     1] loss: 0.488
[15,     1] loss: 0.470
[16,     1] loss: 0.454
[17,     1] loss: 0.434
[18,     1] loss: 0.414
[19,     1] loss: 0.396
[20,     1] loss: 0.380
[21,     1] loss: 0.361
[22,     1] loss: 0.345
[23,     1] loss: 0.330
[24,     1] loss: 0.315
[25,     1] loss: 0.297
[26,     1] loss: 0.283
[27,     1] loss: 0.270
[28,     1] loss: 0.256
[29,     1] loss: 0.241
[30,     1] loss: 0.226
[31,     1] loss: 0.215
[32,     1] loss: 0.203
[33,     1] loss: 0.189
[34,     1] loss: 0.179
[35,     1] loss: 0.167
[36,     1] loss: 0.157
[37,     1] loss: 0.148
[38,     1] loss: 0.137
[39,     1] loss: 0.130
[40,     1] loss: 0.119
[41,     1] loss: 0.110
[42,     1] loss: 0.104
[43,     1] loss: 0.097
[44,     1] loss: 0.092
[45,     1] loss: 0.087
[46,     1] loss: 0.082
[47,     1] loss: 0.078
[48,     1] loss: 0.077
[49,     1] loss: 0.072
[50,     1] loss: 0.069
[51,     1] loss: 0.067
[52,     1] loss: 0.067
[53,     1] loss: 0.064
[54,     1] loss: 0.062
[55,     1] loss: 0.060
[56,     1] loss: 0.059
[57,     1] loss: 0.057
[58,     1] loss: 0.055
[59,     1] loss: 0.055
[60,     1] loss: 0.053
[61,     1] loss: 0.052
[62,     1] loss: 0.052
[63,     1] loss: 0.051
[64,     1] loss: 0.050
[65,     1] loss: 0.049
[66,     1] loss: 0.048
[67,     1] loss: 0.048
[68,     1] loss: 0.048
[69,     1] loss: 0.048
[70,     1] loss: 0.047
[71,     1] loss: 0.048
[72,     1] loss: 0.047
[73,     1] loss: 0.046
[74,     1] loss: 0.046
[75,     1] loss: 0.046
[76,     1] loss: 0.045
[77,     1] loss: 0.044
[78,     1] loss: 0.045
[79,     1] loss: 0.044
[80,     1] loss: 0.044
[81,     1] loss: 0.044
[82,     1] loss: 0.043
[83,     1] loss: 0.043
[84,     1] loss: 0.043
[85,     1] loss: 0.043
[86,     1] loss: 0.042
[87,     1] loss: 0.041
[88,     1] loss: 0.041
[89,     1] loss: 0.041
[90,     1] loss: 0.042
[91,     1] loss: 0.041
[92,     1] loss: 0.040
[93,     1] loss: 0.039
[94,     1] loss: 0.040
[95,     1] loss: 0.039
[96,     1] loss: 0.038
[97,     1] loss: 0.039
[98,     1] loss: 0.039
[99,     1] loss: 0.038
Early stopping applied (best metric=0.35005098581314087)
Finished Training
Total time taken: 186.25998497009277
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.682
[3,     1] loss: 0.666
[4,     1] loss: 0.654
[5,     1] loss: 0.641
[6,     1] loss: 0.628
[7,     1] loss: 0.616
[8,     1] loss: 0.600
[9,     1] loss: 0.586
[10,     1] loss: 0.568
[11,     1] loss: 0.552
[12,     1] loss: 0.536
[13,     1] loss: 0.517
[14,     1] loss: 0.495
[15,     1] loss: 0.478
[16,     1] loss: 0.458
[17,     1] loss: 0.437
[18,     1] loss: 0.416
[19,     1] loss: 0.395
[20,     1] loss: 0.373
[21,     1] loss: 0.351
[22,     1] loss: 0.329
[23,     1] loss: 0.307
[24,     1] loss: 0.294
[25,     1] loss: 0.271
[26,     1] loss: 0.254
[27,     1] loss: 0.234
[28,     1] loss: 0.216
[29,     1] loss: 0.200
[30,     1] loss: 0.184
[31,     1] loss: 0.171
[32,     1] loss: 0.155
[33,     1] loss: 0.143
[34,     1] loss: 0.133
[35,     1] loss: 0.122
[36,     1] loss: 0.111
[37,     1] loss: 0.104
[38,     1] loss: 0.094
[39,     1] loss: 0.089
[40,     1] loss: 0.083
[41,     1] loss: 0.076
[42,     1] loss: 0.071
[43,     1] loss: 0.068
[44,     1] loss: 0.064
[45,     1] loss: 0.061
[46,     1] loss: 0.056
[47,     1] loss: 0.055
[48,     1] loss: 0.053
[49,     1] loss: 0.052
[50,     1] loss: 0.050
[51,     1] loss: 0.047
[52,     1] loss: 0.045
[53,     1] loss: 0.045
[54,     1] loss: 0.044
[55,     1] loss: 0.044
[56,     1] loss: 0.043
[57,     1] loss: 0.042
[58,     1] loss: 0.042
[59,     1] loss: 0.040
[60,     1] loss: 0.041
[61,     1] loss: 0.041
[62,     1] loss: 0.041
[63,     1] loss: 0.040
[64,     1] loss: 0.040
[65,     1] loss: 0.039
[66,     1] loss: 0.039
[67,     1] loss: 0.039
[68,     1] loss: 0.039
[69,     1] loss: 0.040
[70,     1] loss: 0.040
[71,     1] loss: 0.039
[72,     1] loss: 0.039
[73,     1] loss: 0.040
[74,     1] loss: 0.040
[75,     1] loss: 0.039
[76,     1] loss: 0.039
[77,     1] loss: 0.040
[78,     1] loss: 0.039
[79,     1] loss: 0.038
[80,     1] loss: 0.038
[81,     1] loss: 0.039
[82,     1] loss: 0.038
Early stopping applied (best metric=0.38212400674819946)
Finished Training
Total time taken: 154.93746519088745
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.659
[4,     1] loss: 0.645
[5,     1] loss: 0.632
[6,     1] loss: 0.618
[7,     1] loss: 0.606
[8,     1] loss: 0.590
[9,     1] loss: 0.578
[10,     1] loss: 0.563
[11,     1] loss: 0.545
[12,     1] loss: 0.537
[13,     1] loss: 0.518
[14,     1] loss: 0.502
[15,     1] loss: 0.484
[16,     1] loss: 0.465
[17,     1] loss: 0.445
[18,     1] loss: 0.425
[19,     1] loss: 0.402
[20,     1] loss: 0.381
[21,     1] loss: 0.359
[22,     1] loss: 0.335
[23,     1] loss: 0.312
[24,     1] loss: 0.286
[25,     1] loss: 0.265
[26,     1] loss: 0.247
[27,     1] loss: 0.228
[28,     1] loss: 0.205
[29,     1] loss: 0.187
[30,     1] loss: 0.170
[31,     1] loss: 0.154
[32,     1] loss: 0.140
[33,     1] loss: 0.127
[34,     1] loss: 0.117
[35,     1] loss: 0.106
[36,     1] loss: 0.096
[37,     1] loss: 0.086
[38,     1] loss: 0.078
[39,     1] loss: 0.070
[40,     1] loss: 0.066
[41,     1] loss: 0.060
[42,     1] loss: 0.057
[43,     1] loss: 0.050
[44,     1] loss: 0.048
[45,     1] loss: 0.043
[46,     1] loss: 0.042
[47,     1] loss: 0.040
[48,     1] loss: 0.038
[49,     1] loss: 0.036
[50,     1] loss: 0.035
[51,     1] loss: 0.033
[52,     1] loss: 0.032
[53,     1] loss: 0.031
[54,     1] loss: 0.031
[55,     1] loss: 0.030
[56,     1] loss: 0.029
[57,     1] loss: 0.029
[58,     1] loss: 0.029
[59,     1] loss: 0.028
[60,     1] loss: 0.028
[61,     1] loss: 0.029
[62,     1] loss: 0.029
[63,     1] loss: 0.029
[64,     1] loss: 0.029
[65,     1] loss: 0.029
[66,     1] loss: 0.029
[67,     1] loss: 0.028
[68,     1] loss: 0.029
[69,     1] loss: 0.029
[70,     1] loss: 0.029
[71,     1] loss: 0.029
[72,     1] loss: 0.028
[73,     1] loss: 0.030
[74,     1] loss: 0.029
[75,     1] loss: 0.029
[76,     1] loss: 0.029
[77,     1] loss: 0.028
[78,     1] loss: 0.028
[79,     1] loss: 0.029
Early stopping applied (best metric=0.38203784823417664)
Finished Training
Total time taken: 149.80772280693054
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.662
[4,     1] loss: 0.649
[5,     1] loss: 0.637
[6,     1] loss: 0.626
[7,     1] loss: 0.612
[8,     1] loss: 0.600
[9,     1] loss: 0.589
[10,     1] loss: 0.573
[11,     1] loss: 0.558
[12,     1] loss: 0.543
[13,     1] loss: 0.526
[14,     1] loss: 0.513
[15,     1] loss: 0.490
[16,     1] loss: 0.471
[17,     1] loss: 0.454
[18,     1] loss: 0.433
[19,     1] loss: 0.411
[20,     1] loss: 0.390
[21,     1] loss: 0.369
[22,     1] loss: 0.345
[23,     1] loss: 0.328
[24,     1] loss: 0.308
[25,     1] loss: 0.289
[26,     1] loss: 0.272
[27,     1] loss: 0.251
[28,     1] loss: 0.233
[29,     1] loss: 0.217
[30,     1] loss: 0.199
[31,     1] loss: 0.185
[32,     1] loss: 0.173
[33,     1] loss: 0.158
[34,     1] loss: 0.148
[35,     1] loss: 0.136
[36,     1] loss: 0.128
[37,     1] loss: 0.116
[38,     1] loss: 0.110
[39,     1] loss: 0.103
[40,     1] loss: 0.094
[41,     1] loss: 0.091
[42,     1] loss: 0.084
[43,     1] loss: 0.080
[44,     1] loss: 0.075
[45,     1] loss: 0.073
[46,     1] loss: 0.067
[47,     1] loss: 0.066
[48,     1] loss: 0.062
[49,     1] loss: 0.062
[50,     1] loss: 0.058
[51,     1] loss: 0.056
[52,     1] loss: 0.054
[53,     1] loss: 0.052
[54,     1] loss: 0.052
[55,     1] loss: 0.050
[56,     1] loss: 0.050
[57,     1] loss: 0.049
[58,     1] loss: 0.047
[59,     1] loss: 0.047
[60,     1] loss: 0.047
[61,     1] loss: 0.047
[62,     1] loss: 0.047
[63,     1] loss: 0.048
[64,     1] loss: 0.046
[65,     1] loss: 0.044
[66,     1] loss: 0.045
[67,     1] loss: 0.043
[68,     1] loss: 0.046
[69,     1] loss: 0.044
[70,     1] loss: 0.044
[71,     1] loss: 0.044
[72,     1] loss: 0.044
[73,     1] loss: 0.045
[74,     1] loss: 0.045
[75,     1] loss: 0.044
[76,     1] loss: 0.043
[77,     1] loss: 0.043
[78,     1] loss: 0.044
[79,     1] loss: 0.043
Early stopping applied (best metric=0.37712615728378296)
Finished Training
Total time taken: 150.25220274925232
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.658
[4,     1] loss: 0.643
[5,     1] loss: 0.632
[6,     1] loss: 0.616
[7,     1] loss: 0.601
[8,     1] loss: 0.588
[9,     1] loss: 0.572
[10,     1] loss: 0.554
[11,     1] loss: 0.538
[12,     1] loss: 0.522
[13,     1] loss: 0.501
[14,     1] loss: 0.478
[15,     1] loss: 0.457
[16,     1] loss: 0.441
[17,     1] loss: 0.417
[18,     1] loss: 0.399
[19,     1] loss: 0.374
[20,     1] loss: 0.353
[21,     1] loss: 0.333
[22,     1] loss: 0.311
[23,     1] loss: 0.291
[24,     1] loss: 0.270
[25,     1] loss: 0.250
[26,     1] loss: 0.232
[27,     1] loss: 0.213
[28,     1] loss: 0.197
[29,     1] loss: 0.180
[30,     1] loss: 0.163
[31,     1] loss: 0.148
[32,     1] loss: 0.136
[33,     1] loss: 0.123
[34,     1] loss: 0.114
[35,     1] loss: 0.105
[36,     1] loss: 0.093
[37,     1] loss: 0.088
[38,     1] loss: 0.081
[39,     1] loss: 0.074
[40,     1] loss: 0.067
[41,     1] loss: 0.064
[42,     1] loss: 0.063
[43,     1] loss: 0.057
[44,     1] loss: 0.054
[45,     1] loss: 0.051
[46,     1] loss: 0.050
[47,     1] loss: 0.048
[48,     1] loss: 0.045
[49,     1] loss: 0.045
[50,     1] loss: 0.044
[51,     1] loss: 0.044
[52,     1] loss: 0.042
[53,     1] loss: 0.042
[54,     1] loss: 0.039
[55,     1] loss: 0.040
[56,     1] loss: 0.040
[57,     1] loss: 0.040
[58,     1] loss: 0.040
[59,     1] loss: 0.039
[60,     1] loss: 0.039
[61,     1] loss: 0.038
[62,     1] loss: 0.039
[63,     1] loss: 0.039
[64,     1] loss: 0.039
[65,     1] loss: 0.039
[66,     1] loss: 0.038
[67,     1] loss: 0.039
[68,     1] loss: 0.039
[69,     1] loss: 0.038
[70,     1] loss: 0.040
[71,     1] loss: 0.039
[72,     1] loss: 0.039
[73,     1] loss: 0.039
[74,     1] loss: 0.039
Early stopping applied (best metric=0.37249356508255005)
Finished Training
Total time taken: 141.1429500579834
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.678
[3,     1] loss: 0.659
[4,     1] loss: 0.644
[5,     1] loss: 0.627
[6,     1] loss: 0.615
[7,     1] loss: 0.599
[8,     1] loss: 0.588
[9,     1] loss: 0.573
[10,     1] loss: 0.558
[11,     1] loss: 0.540
[12,     1] loss: 0.521
[13,     1] loss: 0.503
[14,     1] loss: 0.483
[15,     1] loss: 0.466
[16,     1] loss: 0.444
[17,     1] loss: 0.426
[18,     1] loss: 0.405
[19,     1] loss: 0.381
[20,     1] loss: 0.366
[21,     1] loss: 0.347
[22,     1] loss: 0.331
[23,     1] loss: 0.310
[24,     1] loss: 0.295
[25,     1] loss: 0.275
[26,     1] loss: 0.258
[27,     1] loss: 0.238
[28,     1] loss: 0.226
[29,     1] loss: 0.210
[30,     1] loss: 0.198
[31,     1] loss: 0.186
[32,     1] loss: 0.176
[33,     1] loss: 0.164
[34,     1] loss: 0.155
[35,     1] loss: 0.142
[36,     1] loss: 0.136
[37,     1] loss: 0.126
[38,     1] loss: 0.121
[39,     1] loss: 0.110
[40,     1] loss: 0.105
[41,     1] loss: 0.099
[42,     1] loss: 0.094
[43,     1] loss: 0.089
[44,     1] loss: 0.083
[45,     1] loss: 0.078
[46,     1] loss: 0.073
[47,     1] loss: 0.069
[48,     1] loss: 0.067
[49,     1] loss: 0.064
[50,     1] loss: 0.061
[51,     1] loss: 0.057
[52,     1] loss: 0.055
[53,     1] loss: 0.053
[54,     1] loss: 0.051
[55,     1] loss: 0.050
[56,     1] loss: 0.048
[57,     1] loss: 0.046
[58,     1] loss: 0.046
[59,     1] loss: 0.044
[60,     1] loss: 0.044
[61,     1] loss: 0.041
[62,     1] loss: 0.041
[63,     1] loss: 0.042
[64,     1] loss: 0.039
[65,     1] loss: 0.039
[66,     1] loss: 0.039
[67,     1] loss: 0.038
[68,     1] loss: 0.038
[69,     1] loss: 0.037
[70,     1] loss: 0.038
[71,     1] loss: 0.037
[72,     1] loss: 0.036
[73,     1] loss: 0.037
[74,     1] loss: 0.035
[75,     1] loss: 0.034
[76,     1] loss: 0.036
[77,     1] loss: 0.035
Early stopping applied (best metric=0.39550256729125977)
Finished Training
Total time taken: 147.224769115448
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.659
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.619
[7,     1] loss: 0.606
[8,     1] loss: 0.593
[9,     1] loss: 0.581
[10,     1] loss: 0.566
[11,     1] loss: 0.553
[12,     1] loss: 0.537
[13,     1] loss: 0.522
[14,     1] loss: 0.502
[15,     1] loss: 0.488
[16,     1] loss: 0.470
[17,     1] loss: 0.452
[18,     1] loss: 0.434
[19,     1] loss: 0.415
[20,     1] loss: 0.398
[21,     1] loss: 0.379
[22,     1] loss: 0.362
[23,     1] loss: 0.343
[24,     1] loss: 0.329
[25,     1] loss: 0.312
[26,     1] loss: 0.299
[27,     1] loss: 0.287
[28,     1] loss: 0.273
[29,     1] loss: 0.260
[30,     1] loss: 0.250
[31,     1] loss: 0.238
[32,     1] loss: 0.228
[33,     1] loss: 0.219
[34,     1] loss: 0.207
[35,     1] loss: 0.198
[36,     1] loss: 0.189
[37,     1] loss: 0.182
[38,     1] loss: 0.173
[39,     1] loss: 0.163
[40,     1] loss: 0.156
[41,     1] loss: 0.147
[42,     1] loss: 0.141
[43,     1] loss: 0.132
[44,     1] loss: 0.126
[45,     1] loss: 0.121
[46,     1] loss: 0.113
[47,     1] loss: 0.104
[48,     1] loss: 0.098
[49,     1] loss: 0.090
[50,     1] loss: 0.087
[51,     1] loss: 0.082
[52,     1] loss: 0.079
[53,     1] loss: 0.072
[54,     1] loss: 0.069
[55,     1] loss: 0.065
[56,     1] loss: 0.060
[57,     1] loss: 0.058
[58,     1] loss: 0.055
[59,     1] loss: 0.053
[60,     1] loss: 0.051
[61,     1] loss: 0.049
[62,     1] loss: 0.047
[63,     1] loss: 0.046
[64,     1] loss: 0.044
[65,     1] loss: 0.041
[66,     1] loss: 0.041
[67,     1] loss: 0.041
[68,     1] loss: 0.039
[69,     1] loss: 0.038
[70,     1] loss: 0.037
[71,     1] loss: 0.035
[72,     1] loss: 0.035
[73,     1] loss: 0.035
[74,     1] loss: 0.035
[75,     1] loss: 0.034
Early stopping applied (best metric=0.397983193397522)
Finished Training
Total time taken: 144.03601050376892
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.641
[6,     1] loss: 0.628
[7,     1] loss: 0.616
[8,     1] loss: 0.602
[9,     1] loss: 0.588
[10,     1] loss: 0.571
[11,     1] loss: 0.555
[12,     1] loss: 0.539
[13,     1] loss: 0.521
[14,     1] loss: 0.501
[15,     1] loss: 0.480
[16,     1] loss: 0.464
[17,     1] loss: 0.444
[18,     1] loss: 0.427
[19,     1] loss: 0.407
[20,     1] loss: 0.386
[21,     1] loss: 0.363
[22,     1] loss: 0.347
[23,     1] loss: 0.327
[24,     1] loss: 0.309
[25,     1] loss: 0.290
[26,     1] loss: 0.268
[27,     1] loss: 0.251
[28,     1] loss: 0.236
[29,     1] loss: 0.218
[30,     1] loss: 0.201
[31,     1] loss: 0.189
[32,     1] loss: 0.172
[33,     1] loss: 0.160
[34,     1] loss: 0.145
[35,     1] loss: 0.132
[36,     1] loss: 0.118
[37,     1] loss: 0.110
[38,     1] loss: 0.099
[39,     1] loss: 0.090
[40,     1] loss: 0.085
[41,     1] loss: 0.078
[42,     1] loss: 0.073
[43,     1] loss: 0.068
[44,     1] loss: 0.064
[45,     1] loss: 0.060
[46,     1] loss: 0.056
[47,     1] loss: 0.054
[48,     1] loss: 0.052
[49,     1] loss: 0.050
[50,     1] loss: 0.049
[51,     1] loss: 0.047
[52,     1] loss: 0.045
[53,     1] loss: 0.043
[54,     1] loss: 0.043
[55,     1] loss: 0.042
[56,     1] loss: 0.040
[57,     1] loss: 0.038
[58,     1] loss: 0.039
[59,     1] loss: 0.040
[60,     1] loss: 0.038
[61,     1] loss: 0.037
[62,     1] loss: 0.038
[63,     1] loss: 0.038
[64,     1] loss: 0.037
[65,     1] loss: 0.036
[66,     1] loss: 0.035
[67,     1] loss: 0.037
[68,     1] loss: 0.038
[69,     1] loss: 0.037
[70,     1] loss: 0.037
[71,     1] loss: 0.036
[72,     1] loss: 0.036
[73,     1] loss: 0.037
[74,     1] loss: 0.036
[75,     1] loss: 0.037
[76,     1] loss: 0.037
[77,     1] loss: 0.037
[78,     1] loss: 0.037
[79,     1] loss: 0.036
[80,     1] loss: 0.037
[81,     1] loss: 0.036
[82,     1] loss: 0.037
[83,     1] loss: 0.037
[84,     1] loss: 0.037
[85,     1] loss: 0.035
[86,     1] loss: 0.036
[87,     1] loss: 0.037
[88,     1] loss: 0.036
[89,     1] loss: 0.036
[90,     1] loss: 0.036
[91,     1] loss: 0.035
[92,     1] loss: 0.033
[93,     1] loss: 0.034
[94,     1] loss: 0.034
[95,     1] loss: 0.034
[96,     1] loss: 0.034
[97,     1] loss: 0.034
[98,     1] loss: 0.032
[99,     1] loss: 0.033
[100,     1] loss: 0.032
[101,     1] loss: 0.032
[102,     1] loss: 0.032
[103,     1] loss: 0.032
[104,     1] loss: 0.032
[105,     1] loss: 0.032
[106,     1] loss: 0.031
[107,     1] loss: 0.030
[108,     1] loss: 0.031
[109,     1] loss: 0.031
[110,     1] loss: 0.031
[111,     1] loss: 0.030
[112,     1] loss: 0.030
[113,     1] loss: 0.030
[114,     1] loss: 0.030
[115,     1] loss: 0.029
[116,     1] loss: 0.028
[117,     1] loss: 0.029
[118,     1] loss: 0.028
[119,     1] loss: 0.029
[120,     1] loss: 0.029
[121,     1] loss: 0.029
[122,     1] loss: 0.029
[123,     1] loss: 0.028
[124,     1] loss: 0.029
Early stopping applied (best metric=0.3207879662513733)
Finished Training
Total time taken: 237.39070105552673
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.691
[3,     1] loss: 0.670
[4,     1] loss: 0.656
[5,     1] loss: 0.643
[6,     1] loss: 0.629
[7,     1] loss: 0.613
[8,     1] loss: 0.598
[9,     1] loss: 0.584
[10,     1] loss: 0.562
[11,     1] loss: 0.546
[12,     1] loss: 0.528
[13,     1] loss: 0.505
[14,     1] loss: 0.481
[15,     1] loss: 0.459
[16,     1] loss: 0.436
[17,     1] loss: 0.414
[18,     1] loss: 0.391
[19,     1] loss: 0.368
[20,     1] loss: 0.344
[21,     1] loss: 0.320
[22,     1] loss: 0.300
[23,     1] loss: 0.277
[24,     1] loss: 0.257
[25,     1] loss: 0.236
[26,     1] loss: 0.216
[27,     1] loss: 0.197
[28,     1] loss: 0.178
[29,     1] loss: 0.161
[30,     1] loss: 0.148
[31,     1] loss: 0.134
[32,     1] loss: 0.124
[33,     1] loss: 0.108
[34,     1] loss: 0.101
[35,     1] loss: 0.091
[36,     1] loss: 0.083
[37,     1] loss: 0.076
[38,     1] loss: 0.070
[39,     1] loss: 0.065
[40,     1] loss: 0.060
[41,     1] loss: 0.056
[42,     1] loss: 0.053
[43,     1] loss: 0.050
[44,     1] loss: 0.048
[45,     1] loss: 0.045
[46,     1] loss: 0.045
[47,     1] loss: 0.043
[48,     1] loss: 0.041
[49,     1] loss: 0.040
[50,     1] loss: 0.040
[51,     1] loss: 0.038
[52,     1] loss: 0.038
[53,     1] loss: 0.037
[54,     1] loss: 0.038
[55,     1] loss: 0.037
[56,     1] loss: 0.037
[57,     1] loss: 0.036
[58,     1] loss: 0.036
[59,     1] loss: 0.036
[60,     1] loss: 0.036
[61,     1] loss: 0.037
[62,     1] loss: 0.036
[63,     1] loss: 0.036
[64,     1] loss: 0.036
[65,     1] loss: 0.036
[66,     1] loss: 0.036
[67,     1] loss: 0.037
[68,     1] loss: 0.036
[69,     1] loss: 0.036
[70,     1] loss: 0.036
[71,     1] loss: 0.036
[72,     1] loss: 0.036
[73,     1] loss: 0.037
[74,     1] loss: 0.037
[75,     1] loss: 0.037
[76,     1] loss: 0.036
[77,     1] loss: 0.037
Early stopping applied (best metric=0.3480416536331177)
Finished Training
Total time taken: 148.44231343269348
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.675
[3,     1] loss: 0.653
[4,     1] loss: 0.636
[5,     1] loss: 0.618
[6,     1] loss: 0.602
[7,     1] loss: 0.584
[8,     1] loss: 0.565
[9,     1] loss: 0.545
[10,     1] loss: 0.525
[11,     1] loss: 0.506
[12,     1] loss: 0.481
[13,     1] loss: 0.458
[14,     1] loss: 0.436
[15,     1] loss: 0.412
[16,     1] loss: 0.385
[17,     1] loss: 0.365
[18,     1] loss: 0.337
[19,     1] loss: 0.311
[20,     1] loss: 0.288
[21,     1] loss: 0.266
[22,     1] loss: 0.246
[23,     1] loss: 0.227
[24,     1] loss: 0.206
[25,     1] loss: 0.187
[26,     1] loss: 0.168
[27,     1] loss: 0.155
[28,     1] loss: 0.140
[29,     1] loss: 0.126
[30,     1] loss: 0.113
[31,     1] loss: 0.104
[32,     1] loss: 0.093
[33,     1] loss: 0.087
[34,     1] loss: 0.078
[35,     1] loss: 0.073
[36,     1] loss: 0.068
[37,     1] loss: 0.062
[38,     1] loss: 0.057
[39,     1] loss: 0.055
[40,     1] loss: 0.052
[41,     1] loss: 0.048
[42,     1] loss: 0.046
[43,     1] loss: 0.044
[44,     1] loss: 0.043
[45,     1] loss: 0.040
[46,     1] loss: 0.040
[47,     1] loss: 0.039
[48,     1] loss: 0.036
[49,     1] loss: 0.036
[50,     1] loss: 0.036
[51,     1] loss: 0.035
[52,     1] loss: 0.034
[53,     1] loss: 0.035
[54,     1] loss: 0.034
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.034
[58,     1] loss: 0.033
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.034
[62,     1] loss: 0.034
[63,     1] loss: 0.034
[64,     1] loss: 0.035
[65,     1] loss: 0.034
[66,     1] loss: 0.034
[67,     1] loss: 0.034
[68,     1] loss: 0.034
[69,     1] loss: 0.035
[70,     1] loss: 0.034
[71,     1] loss: 0.035
Early stopping applied (best metric=0.3856984078884125)
Finished Training
Total time taken: 137.38341736793518
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.688
[3,     1] loss: 0.675
[4,     1] loss: 0.664
[5,     1] loss: 0.654
[6,     1] loss: 0.642
[7,     1] loss: 0.631
[8,     1] loss: 0.618
[9,     1] loss: 0.608
[10,     1] loss: 0.593
[11,     1] loss: 0.578
[12,     1] loss: 0.564
[13,     1] loss: 0.551
[14,     1] loss: 0.536
[15,     1] loss: 0.517
[16,     1] loss: 0.497
[17,     1] loss: 0.477
[18,     1] loss: 0.461
[19,     1] loss: 0.439
[20,     1] loss: 0.420
[21,     1] loss: 0.399
[22,     1] loss: 0.378
[23,     1] loss: 0.360
[24,     1] loss: 0.336
[25,     1] loss: 0.318
[26,     1] loss: 0.300
[27,     1] loss: 0.281
[28,     1] loss: 0.260
[29,     1] loss: 0.243
[30,     1] loss: 0.227
[31,     1] loss: 0.212
[32,     1] loss: 0.198
[33,     1] loss: 0.185
[34,     1] loss: 0.174
[35,     1] loss: 0.162
[36,     1] loss: 0.149
[37,     1] loss: 0.139
[38,     1] loss: 0.131
[39,     1] loss: 0.122
[40,     1] loss: 0.115
[41,     1] loss: 0.107
[42,     1] loss: 0.102
[43,     1] loss: 0.097
[44,     1] loss: 0.092
[45,     1] loss: 0.087
[46,     1] loss: 0.084
[47,     1] loss: 0.079
[48,     1] loss: 0.075
[49,     1] loss: 0.074
[50,     1] loss: 0.070
[51,     1] loss: 0.067
[52,     1] loss: 0.066
[53,     1] loss: 0.064
[54,     1] loss: 0.062
[55,     1] loss: 0.060
[56,     1] loss: 0.057
[57,     1] loss: 0.059
[58,     1] loss: 0.058
[59,     1] loss: 0.057
[60,     1] loss: 0.055
[61,     1] loss: 0.054
[62,     1] loss: 0.054
[63,     1] loss: 0.054
[64,     1] loss: 0.052
[65,     1] loss: 0.052
[66,     1] loss: 0.051
[67,     1] loss: 0.051
[68,     1] loss: 0.051
[69,     1] loss: 0.051
[70,     1] loss: 0.050
[71,     1] loss: 0.051
[72,     1] loss: 0.049
[73,     1] loss: 0.051
[74,     1] loss: 0.049
Early stopping applied (best metric=0.4211202561855316)
Finished Training
Total time taken: 143.71423840522766
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.678
[3,     1] loss: 0.655
[4,     1] loss: 0.635
[5,     1] loss: 0.619
[6,     1] loss: 0.598
[7,     1] loss: 0.578
[8,     1] loss: 0.559
[9,     1] loss: 0.537
[10,     1] loss: 0.516
[11,     1] loss: 0.493
[12,     1] loss: 0.469
[13,     1] loss: 0.448
[14,     1] loss: 0.427
[15,     1] loss: 0.405
[16,     1] loss: 0.389
[17,     1] loss: 0.374
[18,     1] loss: 0.363
[19,     1] loss: 0.352
[20,     1] loss: 0.342
[21,     1] loss: 0.334
[22,     1] loss: 0.329
[23,     1] loss: 0.322
[24,     1] loss: 0.318
[25,     1] loss: 0.313
[26,     1] loss: 0.310
[27,     1] loss: 0.306
[28,     1] loss: 0.303
[29,     1] loss: 0.300
[30,     1] loss: 0.297
[31,     1] loss: 0.294
[32,     1] loss: 0.291
[33,     1] loss: 0.288
[34,     1] loss: 0.285
[35,     1] loss: 0.282
[36,     1] loss: 0.278
[37,     1] loss: 0.274
[38,     1] loss: 0.271
[39,     1] loss: 0.266
[40,     1] loss: 0.262
[41,     1] loss: 0.256
[42,     1] loss: 0.251
[43,     1] loss: 0.245
[44,     1] loss: 0.241
[45,     1] loss: 0.236
[46,     1] loss: 0.231
[47,     1] loss: 0.224
[48,     1] loss: 0.218
[49,     1] loss: 0.212
[50,     1] loss: 0.207
[51,     1] loss: 0.201
[52,     1] loss: 0.192
[53,     1] loss: 0.187
[54,     1] loss: 0.182
[55,     1] loss: 0.176
[56,     1] loss: 0.168
[57,     1] loss: 0.162
[58,     1] loss: 0.156
[59,     1] loss: 0.150
[60,     1] loss: 0.145
[61,     1] loss: 0.139
[62,     1] loss: 0.135
[63,     1] loss: 0.131
[64,     1] loss: 0.127
[65,     1] loss: 0.122
Early stopping applied (best metric=0.38173016905784607)
Finished Training
Total time taken: 126.46587681770325
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.671
[4,     1] loss: 0.660
[5,     1] loss: 0.652
[6,     1] loss: 0.642
[7,     1] loss: 0.630
[8,     1] loss: 0.619
[9,     1] loss: 0.606
[10,     1] loss: 0.591
[11,     1] loss: 0.579
[12,     1] loss: 0.563
[13,     1] loss: 0.545
[14,     1] loss: 0.526
[15,     1] loss: 0.505
[16,     1] loss: 0.484
[17,     1] loss: 0.464
[18,     1] loss: 0.441
[19,     1] loss: 0.417
[20,     1] loss: 0.394
[21,     1] loss: 0.371
[22,     1] loss: 0.348
[23,     1] loss: 0.324
[24,     1] loss: 0.303
[25,     1] loss: 0.282
[26,     1] loss: 0.261
[27,     1] loss: 0.243
[28,     1] loss: 0.225
[29,     1] loss: 0.208
[30,     1] loss: 0.190
[31,     1] loss: 0.174
[32,     1] loss: 0.160
[33,     1] loss: 0.149
[34,     1] loss: 0.135
[35,     1] loss: 0.123
[36,     1] loss: 0.112
[37,     1] loss: 0.104
[38,     1] loss: 0.097
[39,     1] loss: 0.092
[40,     1] loss: 0.086
[41,     1] loss: 0.080
[42,     1] loss: 0.074
[43,     1] loss: 0.071
[44,     1] loss: 0.066
[45,     1] loss: 0.062
[46,     1] loss: 0.061
[47,     1] loss: 0.057
[48,     1] loss: 0.055
[49,     1] loss: 0.054
[50,     1] loss: 0.051
[51,     1] loss: 0.050
[52,     1] loss: 0.048
[53,     1] loss: 0.048
[54,     1] loss: 0.048
[55,     1] loss: 0.045
[56,     1] loss: 0.044
[57,     1] loss: 0.045
[58,     1] loss: 0.045
[59,     1] loss: 0.044
[60,     1] loss: 0.044
[61,     1] loss: 0.044
[62,     1] loss: 0.044
[63,     1] loss: 0.043
[64,     1] loss: 0.042
[65,     1] loss: 0.043
[66,     1] loss: 0.043
[67,     1] loss: 0.041
[68,     1] loss: 0.042
[69,     1] loss: 0.043
[70,     1] loss: 0.042
Early stopping applied (best metric=0.430683970451355)
Finished Training
Total time taken: 136.41232585906982
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.688
[3,     1] loss: 0.672
[4,     1] loss: 0.656
[5,     1] loss: 0.642
[6,     1] loss: 0.630
[7,     1] loss: 0.617
[8,     1] loss: 0.601
[9,     1] loss: 0.589
[10,     1] loss: 0.573
[11,     1] loss: 0.557
[12,     1] loss: 0.541
[13,     1] loss: 0.524
[14,     1] loss: 0.503
[15,     1] loss: 0.488
[16,     1] loss: 0.467
[17,     1] loss: 0.447
[18,     1] loss: 0.428
[19,     1] loss: 0.401
[20,     1] loss: 0.382
[21,     1] loss: 0.359
[22,     1] loss: 0.335
[23,     1] loss: 0.315
[24,     1] loss: 0.289
[25,     1] loss: 0.267
[26,     1] loss: 0.248
[27,     1] loss: 0.229
[28,     1] loss: 0.212
[29,     1] loss: 0.196
[30,     1] loss: 0.180
[31,     1] loss: 0.171
[32,     1] loss: 0.154
[33,     1] loss: 0.144
[34,     1] loss: 0.134
[35,     1] loss: 0.124
[36,     1] loss: 0.113
[37,     1] loss: 0.105
[38,     1] loss: 0.098
[39,     1] loss: 0.094
[40,     1] loss: 0.086
[41,     1] loss: 0.081
[42,     1] loss: 0.078
[43,     1] loss: 0.073
[44,     1] loss: 0.069
[45,     1] loss: 0.066
[46,     1] loss: 0.063
[47,     1] loss: 0.061
[48,     1] loss: 0.058
[49,     1] loss: 0.055
[50,     1] loss: 0.054
[51,     1] loss: 0.052
[52,     1] loss: 0.051
[53,     1] loss: 0.049
[54,     1] loss: 0.050
[55,     1] loss: 0.047
[56,     1] loss: 0.047
[57,     1] loss: 0.046
[58,     1] loss: 0.046
[59,     1] loss: 0.044
[60,     1] loss: 0.045
[61,     1] loss: 0.045
[62,     1] loss: 0.046
[63,     1] loss: 0.045
[64,     1] loss: 0.044
[65,     1] loss: 0.044
[66,     1] loss: 0.043
[67,     1] loss: 0.043
[68,     1] loss: 0.044
[69,     1] loss: 0.044
[70,     1] loss: 0.043
[71,     1] loss: 0.044
[72,     1] loss: 0.042
[73,     1] loss: 0.043
[74,     1] loss: 0.042
[75,     1] loss: 0.043
[76,     1] loss: 0.042
Early stopping applied (best metric=0.3974972367286682)
Finished Training
Total time taken: 148.52858972549438
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.697
[3,     1] loss: 0.688
[4,     1] loss: 0.679
[5,     1] loss: 0.670
[6,     1] loss: 0.661
[7,     1] loss: 0.649
[8,     1] loss: 0.637
[9,     1] loss: 0.622
[10,     1] loss: 0.607
[11,     1] loss: 0.592
[12,     1] loss: 0.574
[13,     1] loss: 0.558
[14,     1] loss: 0.539
[15,     1] loss: 0.523
[16,     1] loss: 0.506
[17,     1] loss: 0.488
[18,     1] loss: 0.474
[19,     1] loss: 0.458
[20,     1] loss: 0.444
[21,     1] loss: 0.429
[22,     1] loss: 0.413
[23,     1] loss: 0.400
[24,     1] loss: 0.388
[25,     1] loss: 0.376
[26,     1] loss: 0.364
[27,     1] loss: 0.351
[28,     1] loss: 0.339
[29,     1] loss: 0.329
[30,     1] loss: 0.315
[31,     1] loss: 0.303
[32,     1] loss: 0.290
[33,     1] loss: 0.278
[34,     1] loss: 0.265
[35,     1] loss: 0.255
[36,     1] loss: 0.242
[37,     1] loss: 0.231
[38,     1] loss: 0.220
[39,     1] loss: 0.209
[40,     1] loss: 0.196
[41,     1] loss: 0.186
[42,     1] loss: 0.176
[43,     1] loss: 0.165
[44,     1] loss: 0.155
[45,     1] loss: 0.146
[46,     1] loss: 0.138
[47,     1] loss: 0.129
[48,     1] loss: 0.126
[49,     1] loss: 0.116
[50,     1] loss: 0.108
[51,     1] loss: 0.105
[52,     1] loss: 0.099
[53,     1] loss: 0.094
[54,     1] loss: 0.089
[55,     1] loss: 0.085
[56,     1] loss: 0.082
[57,     1] loss: 0.077
[58,     1] loss: 0.075
[59,     1] loss: 0.072
[60,     1] loss: 0.069
[61,     1] loss: 0.067
[62,     1] loss: 0.065
[63,     1] loss: 0.063
[64,     1] loss: 0.062
[65,     1] loss: 0.060
[66,     1] loss: 0.059
[67,     1] loss: 0.056
[68,     1] loss: 0.055
[69,     1] loss: 0.055
[70,     1] loss: 0.053
[71,     1] loss: 0.052
[72,     1] loss: 0.052
[73,     1] loss: 0.051
[74,     1] loss: 0.051
[75,     1] loss: 0.050
[76,     1] loss: 0.050
[77,     1] loss: 0.049
[78,     1] loss: 0.049
[79,     1] loss: 0.048
[80,     1] loss: 0.047
[81,     1] loss: 0.048
[82,     1] loss: 0.047
[83,     1] loss: 0.047
[84,     1] loss: 0.046
[85,     1] loss: 0.047
[86,     1] loss: 0.047
[87,     1] loss: 0.047
[88,     1] loss: 0.046
[89,     1] loss: 0.047
[90,     1] loss: 0.046
[91,     1] loss: 0.045
[92,     1] loss: 0.045
[93,     1] loss: 0.046
[94,     1] loss: 0.046
[95,     1] loss: 0.045
[96,     1] loss: 0.047
[97,     1] loss: 0.045
[98,     1] loss: 0.044
[99,     1] loss: 0.044
[100,     1] loss: 0.044
[101,     1] loss: 0.045
Early stopping applied (best metric=0.43410220742225647)
Finished Training
Total time taken: 197.2240891456604
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.664
[4,     1] loss: 0.652
[5,     1] loss: 0.639
[6,     1] loss: 0.629
[7,     1] loss: 0.616
[8,     1] loss: 0.601
[9,     1] loss: 0.587
[10,     1] loss: 0.571
[11,     1] loss: 0.556
[12,     1] loss: 0.537
[13,     1] loss: 0.518
[14,     1] loss: 0.501
[15,     1] loss: 0.481
[16,     1] loss: 0.462
[17,     1] loss: 0.440
[18,     1] loss: 0.419
[19,     1] loss: 0.397
[20,     1] loss: 0.376
[21,     1] loss: 0.358
[22,     1] loss: 0.335
[23,     1] loss: 0.316
[24,     1] loss: 0.295
[25,     1] loss: 0.276
[26,     1] loss: 0.257
[27,     1] loss: 0.241
[28,     1] loss: 0.222
[29,     1] loss: 0.203
[30,     1] loss: 0.189
[31,     1] loss: 0.174
[32,     1] loss: 0.160
[33,     1] loss: 0.149
[34,     1] loss: 0.134
[35,     1] loss: 0.126
[36,     1] loss: 0.116
[37,     1] loss: 0.105
[38,     1] loss: 0.098
[39,     1] loss: 0.091
[40,     1] loss: 0.085
[41,     1] loss: 0.080
[42,     1] loss: 0.073
[43,     1] loss: 0.068
[44,     1] loss: 0.065
[45,     1] loss: 0.060
[46,     1] loss: 0.058
[47,     1] loss: 0.055
[48,     1] loss: 0.052
[49,     1] loss: 0.051
[50,     1] loss: 0.049
[51,     1] loss: 0.048
[52,     1] loss: 0.046
[53,     1] loss: 0.044
[54,     1] loss: 0.043
[55,     1] loss: 0.042
[56,     1] loss: 0.041
[57,     1] loss: 0.040
[58,     1] loss: 0.040
[59,     1] loss: 0.040
[60,     1] loss: 0.039
[61,     1] loss: 0.039
[62,     1] loss: 0.039
[63,     1] loss: 0.039
[64,     1] loss: 0.039
[65,     1] loss: 0.039
[66,     1] loss: 0.038
Early stopping applied (best metric=0.48020029067993164)
Finished Training
Total time taken: 129.7676887512207
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.676
[5,     1] loss: 0.667
[6,     1] loss: 0.656
[7,     1] loss: 0.646
[8,     1] loss: 0.637
[9,     1] loss: 0.626
[10,     1] loss: 0.615
[11,     1] loss: 0.602
[12,     1] loss: 0.591
[13,     1] loss: 0.576
[14,     1] loss: 0.562
[15,     1] loss: 0.547
[16,     1] loss: 0.530
[17,     1] loss: 0.514
[18,     1] loss: 0.495
[19,     1] loss: 0.475
[20,     1] loss: 0.456
[21,     1] loss: 0.436
[22,     1] loss: 0.412
[23,     1] loss: 0.389
[24,     1] loss: 0.371
[25,     1] loss: 0.348
[26,     1] loss: 0.327
[27,     1] loss: 0.307
[28,     1] loss: 0.287
[29,     1] loss: 0.267
[30,     1] loss: 0.247
[31,     1] loss: 0.230
[32,     1] loss: 0.207
[33,     1] loss: 0.193
[34,     1] loss: 0.179
[35,     1] loss: 0.166
[36,     1] loss: 0.155
[37,     1] loss: 0.143
[38,     1] loss: 0.132
[39,     1] loss: 0.122
[40,     1] loss: 0.117
[41,     1] loss: 0.109
[42,     1] loss: 0.104
[43,     1] loss: 0.095
[44,     1] loss: 0.092
[45,     1] loss: 0.086
[46,     1] loss: 0.082
[47,     1] loss: 0.077
[48,     1] loss: 0.074
[49,     1] loss: 0.071
[50,     1] loss: 0.066
[51,     1] loss: 0.063
[52,     1] loss: 0.062
[53,     1] loss: 0.059
[54,     1] loss: 0.059
[55,     1] loss: 0.057
[56,     1] loss: 0.054
[57,     1] loss: 0.054
[58,     1] loss: 0.053
[59,     1] loss: 0.052
[60,     1] loss: 0.050
[61,     1] loss: 0.050
[62,     1] loss: 0.048
[63,     1] loss: 0.048
[64,     1] loss: 0.048
[65,     1] loss: 0.047
[66,     1] loss: 0.048
[67,     1] loss: 0.047
[68,     1] loss: 0.047
[69,     1] loss: 0.046
[70,     1] loss: 0.046
[71,     1] loss: 0.047
[72,     1] loss: 0.045
[73,     1] loss: 0.045
[74,     1] loss: 0.045
[75,     1] loss: 0.044
[76,     1] loss: 0.044
[77,     1] loss: 0.045
Early stopping applied (best metric=0.39304402470588684)
Finished Training
Total time taken: 151.3967034816742
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.671
[4,     1] loss: 0.657
[5,     1] loss: 0.644
[6,     1] loss: 0.632
[7,     1] loss: 0.619
[8,     1] loss: 0.605
[9,     1] loss: 0.590
[10,     1] loss: 0.574
[11,     1] loss: 0.559
[12,     1] loss: 0.546
[13,     1] loss: 0.529
[14,     1] loss: 0.510
[15,     1] loss: 0.492
[16,     1] loss: 0.470
[17,     1] loss: 0.449
[18,     1] loss: 0.423
[19,     1] loss: 0.403
[20,     1] loss: 0.383
[21,     1] loss: 0.358
[22,     1] loss: 0.332
[23,     1] loss: 0.308
[24,     1] loss: 0.287
[25,     1] loss: 0.262
[26,     1] loss: 0.238
[27,     1] loss: 0.216
[28,     1] loss: 0.200
[29,     1] loss: 0.178
[30,     1] loss: 0.163
[31,     1] loss: 0.150
[32,     1] loss: 0.133
[33,     1] loss: 0.120
[34,     1] loss: 0.110
[35,     1] loss: 0.100
[36,     1] loss: 0.090
[37,     1] loss: 0.080
[38,     1] loss: 0.075
[39,     1] loss: 0.066
[40,     1] loss: 0.063
[41,     1] loss: 0.055
[42,     1] loss: 0.053
[43,     1] loss: 0.048
[44,     1] loss: 0.045
[45,     1] loss: 0.042
[46,     1] loss: 0.040
[47,     1] loss: 0.039
[48,     1] loss: 0.036
[49,     1] loss: 0.036
[50,     1] loss: 0.034
[51,     1] loss: 0.033
[52,     1] loss: 0.032
[53,     1] loss: 0.031
[54,     1] loss: 0.031
[55,     1] loss: 0.031
[56,     1] loss: 0.030
[57,     1] loss: 0.029
[58,     1] loss: 0.029
[59,     1] loss: 0.029
[60,     1] loss: 0.030
[61,     1] loss: 0.029
[62,     1] loss: 0.029
[63,     1] loss: 0.029
[64,     1] loss: 0.029
[65,     1] loss: 0.029
[66,     1] loss: 0.030
[67,     1] loss: 0.029
[68,     1] loss: 0.030
[69,     1] loss: 0.029
[70,     1] loss: 0.029
[71,     1] loss: 0.030
[72,     1] loss: 0.029
[73,     1] loss: 0.030
[74,     1] loss: 0.031
[75,     1] loss: 0.029
[76,     1] loss: 0.030
Early stopping applied (best metric=0.40882399678230286)
Finished Training
Total time taken: 149.68064284324646
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.668
[3,     1] loss: 0.646
[4,     1] loss: 0.628
[5,     1] loss: 0.610
[6,     1] loss: 0.594
[7,     1] loss: 0.574
[8,     1] loss: 0.553
[9,     1] loss: 0.533
[10,     1] loss: 0.512
[11,     1] loss: 0.490
[12,     1] loss: 0.468
[13,     1] loss: 0.442
[14,     1] loss: 0.417
[15,     1] loss: 0.397
[16,     1] loss: 0.369
[17,     1] loss: 0.345
[18,     1] loss: 0.323
[19,     1] loss: 0.296
[20,     1] loss: 0.273
[21,     1] loss: 0.253
[22,     1] loss: 0.234
[23,     1] loss: 0.212
[24,     1] loss: 0.192
[25,     1] loss: 0.179
[26,     1] loss: 0.162
[27,     1] loss: 0.146
[28,     1] loss: 0.132
[29,     1] loss: 0.119
[30,     1] loss: 0.109
[31,     1] loss: 0.099
[32,     1] loss: 0.090
[33,     1] loss: 0.080
[34,     1] loss: 0.078
[35,     1] loss: 0.070
[36,     1] loss: 0.065
[37,     1] loss: 0.062
[38,     1] loss: 0.057
[39,     1] loss: 0.054
[40,     1] loss: 0.050
[41,     1] loss: 0.049
[42,     1] loss: 0.045
[43,     1] loss: 0.044
[44,     1] loss: 0.043
[45,     1] loss: 0.042
[46,     1] loss: 0.039
[47,     1] loss: 0.040
[48,     1] loss: 0.038
[49,     1] loss: 0.037
[50,     1] loss: 0.037
[51,     1] loss: 0.037
[52,     1] loss: 0.036
[53,     1] loss: 0.036
[54,     1] loss: 0.035
[55,     1] loss: 0.034
[56,     1] loss: 0.036
[57,     1] loss: 0.035
[58,     1] loss: 0.036
[59,     1] loss: 0.035
[60,     1] loss: 0.035
[61,     1] loss: 0.036
[62,     1] loss: 0.036
[63,     1] loss: 0.036
[64,     1] loss: 0.037
[65,     1] loss: 0.036
[66,     1] loss: 0.037
[67,     1] loss: 0.037
[68,     1] loss: 0.037
[69,     1] loss: 0.037
[70,     1] loss: 0.036
[71,     1] loss: 0.037
Early stopping applied (best metric=0.40663695335388184)
Finished Training
Total time taken: 140.44070100784302
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.675
[3,     1] loss: 0.653
[4,     1] loss: 0.632
[5,     1] loss: 0.615
[6,     1] loss: 0.597
[7,     1] loss: 0.576
[8,     1] loss: 0.558
[9,     1] loss: 0.535
[10,     1] loss: 0.516
[11,     1] loss: 0.491
[12,     1] loss: 0.466
[13,     1] loss: 0.439
[14,     1] loss: 0.416
[15,     1] loss: 0.394
[16,     1] loss: 0.371
[17,     1] loss: 0.348
[18,     1] loss: 0.331
[19,     1] loss: 0.305
[20,     1] loss: 0.287
[21,     1] loss: 0.272
[22,     1] loss: 0.256
[23,     1] loss: 0.237
[24,     1] loss: 0.224
[25,     1] loss: 0.211
[26,     1] loss: 0.199
[27,     1] loss: 0.188
[28,     1] loss: 0.176
[29,     1] loss: 0.163
[30,     1] loss: 0.153
[31,     1] loss: 0.144
[32,     1] loss: 0.136
[33,     1] loss: 0.124
[34,     1] loss: 0.118
[35,     1] loss: 0.112
[36,     1] loss: 0.104
[37,     1] loss: 0.099
[38,     1] loss: 0.094
[39,     1] loss: 0.090
[40,     1] loss: 0.085
[41,     1] loss: 0.080
[42,     1] loss: 0.079
[43,     1] loss: 0.076
[44,     1] loss: 0.071
[45,     1] loss: 0.070
[46,     1] loss: 0.067
[47,     1] loss: 0.064
[48,     1] loss: 0.063
[49,     1] loss: 0.061
[50,     1] loss: 0.061
[51,     1] loss: 0.060
[52,     1] loss: 0.058
[53,     1] loss: 0.056
[54,     1] loss: 0.057
[55,     1] loss: 0.055
[56,     1] loss: 0.056
[57,     1] loss: 0.054
[58,     1] loss: 0.054
[59,     1] loss: 0.053
[60,     1] loss: 0.053
[61,     1] loss: 0.053
[62,     1] loss: 0.054
[63,     1] loss: 0.052
[64,     1] loss: 0.052
[65,     1] loss: 0.050
Early stopping applied (best metric=0.4189798831939697)
Finished Training
Total time taken: 129.085289478302
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.696
[3,     1] loss: 0.686
[4,     1] loss: 0.675
[5,     1] loss: 0.665
[6,     1] loss: 0.652
[7,     1] loss: 0.640
[8,     1] loss: 0.627
[9,     1] loss: 0.613
[10,     1] loss: 0.601
[11,     1] loss: 0.585
[12,     1] loss: 0.569
[13,     1] loss: 0.557
[14,     1] loss: 0.538
[15,     1] loss: 0.522
[16,     1] loss: 0.500
[17,     1] loss: 0.482
[18,     1] loss: 0.461
[19,     1] loss: 0.439
[20,     1] loss: 0.417
[21,     1] loss: 0.399
[22,     1] loss: 0.377
[23,     1] loss: 0.358
[24,     1] loss: 0.337
[25,     1] loss: 0.315
[26,     1] loss: 0.296
[27,     1] loss: 0.278
[28,     1] loss: 0.260
[29,     1] loss: 0.242
[30,     1] loss: 0.228
[31,     1] loss: 0.215
[32,     1] loss: 0.204
[33,     1] loss: 0.190
[34,     1] loss: 0.179
[35,     1] loss: 0.168
[36,     1] loss: 0.159
[37,     1] loss: 0.153
[38,     1] loss: 0.144
[39,     1] loss: 0.137
[40,     1] loss: 0.130
[41,     1] loss: 0.125
[42,     1] loss: 0.119
[43,     1] loss: 0.114
[44,     1] loss: 0.108
[45,     1] loss: 0.105
[46,     1] loss: 0.100
[47,     1] loss: 0.096
[48,     1] loss: 0.091
[49,     1] loss: 0.089
[50,     1] loss: 0.088
[51,     1] loss: 0.086
[52,     1] loss: 0.081
[53,     1] loss: 0.079
[54,     1] loss: 0.078
[55,     1] loss: 0.077
[56,     1] loss: 0.074
[57,     1] loss: 0.073
[58,     1] loss: 0.070
[59,     1] loss: 0.071
[60,     1] loss: 0.069
[61,     1] loss: 0.067
[62,     1] loss: 0.067
[63,     1] loss: 0.067
[64,     1] loss: 0.068
[65,     1] loss: 0.066
[66,     1] loss: 0.065
[67,     1] loss: 0.065
[68,     1] loss: 0.063
[69,     1] loss: 0.064
[70,     1] loss: 0.064
[71,     1] loss: 0.063
[72,     1] loss: 0.062
[73,     1] loss: 0.062
[74,     1] loss: 0.063
Early stopping applied (best metric=0.40853098034858704)
Finished Training
Total time taken: 147.0581681728363
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.632
[7,     1] loss: 0.618
[8,     1] loss: 0.603
[9,     1] loss: 0.586
[10,     1] loss: 0.569
[11,     1] loss: 0.552
[12,     1] loss: 0.530
[13,     1] loss: 0.508
[14,     1] loss: 0.486
[15,     1] loss: 0.462
[16,     1] loss: 0.440
[17,     1] loss: 0.417
[18,     1] loss: 0.390
[19,     1] loss: 0.364
[20,     1] loss: 0.338
[21,     1] loss: 0.313
[22,     1] loss: 0.290
[23,     1] loss: 0.268
[24,     1] loss: 0.241
[25,     1] loss: 0.221
[26,     1] loss: 0.199
[27,     1] loss: 0.180
[28,     1] loss: 0.165
[29,     1] loss: 0.150
[30,     1] loss: 0.136
[31,     1] loss: 0.121
[32,     1] loss: 0.108
[33,     1] loss: 0.098
[34,     1] loss: 0.089
[35,     1] loss: 0.082
[36,     1] loss: 0.075
[37,     1] loss: 0.069
[38,     1] loss: 0.062
[39,     1] loss: 0.059
[40,     1] loss: 0.053
[41,     1] loss: 0.049
[42,     1] loss: 0.047
[43,     1] loss: 0.043
[44,     1] loss: 0.042
[45,     1] loss: 0.041
[46,     1] loss: 0.040
[47,     1] loss: 0.038
[48,     1] loss: 0.037
[49,     1] loss: 0.035
[50,     1] loss: 0.036
[51,     1] loss: 0.034
[52,     1] loss: 0.034
[53,     1] loss: 0.033
[54,     1] loss: 0.033
[55,     1] loss: 0.032
[56,     1] loss: 0.034
[57,     1] loss: 0.033
[58,     1] loss: 0.034
[59,     1] loss: 0.034
[60,     1] loss: 0.034
[61,     1] loss: 0.033
[62,     1] loss: 0.033
[63,     1] loss: 0.034
[64,     1] loss: 0.033
[65,     1] loss: 0.033
[66,     1] loss: 0.033
[67,     1] loss: 0.034
[68,     1] loss: 0.034
[69,     1] loss: 0.034
[70,     1] loss: 0.035
[71,     1] loss: 0.034
[72,     1] loss: 0.035
Early stopping applied (best metric=0.3719349801540375)
Finished Training
Total time taken: 143.37111020088196
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.678
[3,     1] loss: 0.663
[4,     1] loss: 0.652
[5,     1] loss: 0.641
[6,     1] loss: 0.630
[7,     1] loss: 0.617
[8,     1] loss: 0.604
[9,     1] loss: 0.589
[10,     1] loss: 0.575
[11,     1] loss: 0.562
[12,     1] loss: 0.544
[13,     1] loss: 0.527
[14,     1] loss: 0.511
[15,     1] loss: 0.491
[16,     1] loss: 0.473
[17,     1] loss: 0.455
[18,     1] loss: 0.432
[19,     1] loss: 0.413
[20,     1] loss: 0.396
[21,     1] loss: 0.375
[22,     1] loss: 0.353
[23,     1] loss: 0.332
[24,     1] loss: 0.313
[25,     1] loss: 0.291
[26,     1] loss: 0.272
[27,     1] loss: 0.249
[28,     1] loss: 0.229
[29,     1] loss: 0.212
[30,     1] loss: 0.196
[31,     1] loss: 0.182
[32,     1] loss: 0.167
[33,     1] loss: 0.153
[34,     1] loss: 0.139
[35,     1] loss: 0.127
[36,     1] loss: 0.117
[37,     1] loss: 0.106
[38,     1] loss: 0.099
[39,     1] loss: 0.091
[40,     1] loss: 0.083
[41,     1] loss: 0.077
[42,     1] loss: 0.071
[43,     1] loss: 0.067
[44,     1] loss: 0.063
[45,     1] loss: 0.058
[46,     1] loss: 0.054
[47,     1] loss: 0.052
[48,     1] loss: 0.050
[49,     1] loss: 0.047
[50,     1] loss: 0.045
[51,     1] loss: 0.043
[52,     1] loss: 0.042
[53,     1] loss: 0.040
[54,     1] loss: 0.040
[55,     1] loss: 0.039
[56,     1] loss: 0.039
[57,     1] loss: 0.038
[58,     1] loss: 0.037
[59,     1] loss: 0.037
[60,     1] loss: 0.036
[61,     1] loss: 0.036
[62,     1] loss: 0.036
[63,     1] loss: 0.036
[64,     1] loss: 0.035
[65,     1] loss: 0.036
[66,     1] loss: 0.035
[67,     1] loss: 0.036
[68,     1] loss: 0.035
[69,     1] loss: 0.035
[70,     1] loss: 0.036
[71,     1] loss: 0.035
[72,     1] loss: 0.036
[73,     1] loss: 0.035
[74,     1] loss: 0.036
[75,     1] loss: 0.035
Early stopping applied (best metric=0.38206470012664795)
Finished Training
Total time taken: 149.7801537513733
{'Hydroxylation-P Validation Accuracy': 0.7935676970712147, 'Hydroxylation-P Validation Sensitivity': 0.7715873015873016, 'Hydroxylation-P Validation Specificity': 0.7982777195870119, 'Hydroxylation-P Validation Precision': 0.45961643411276304, 'Hydroxylation-P AUC ROC': 0.8503195599353014, 'Hydroxylation-P AUC PR': 0.5934041709855182, 'Hydroxylation-P MCC': 0.4781425101395247, 'Hydroxylation-P F1': 0.57223709309877, 'Validation Loss (Hydroxylation-P)': 0.39034610986709595, 'Validation Loss (total)': 0.39034610986709595}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030654097489241612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4113651514,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.455759698093773}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.673
[4,     1] loss: 0.657
[5,     1] loss: 0.640
[6,     1] loss: 0.620
[7,     1] loss: 0.599
[8,     1] loss: 0.572
[9,     1] loss: 0.546
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002150471472497027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 276698085,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.601540453433337}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.632
[7,     1] loss: 0.619
[8,     1] loss: 0.603
[9,     1] loss: 0.585
[10,     1] loss: 0.564
[11,     1] loss: 0.540
[12,     1] loss: 0.518
[13,     1] loss: 0.490
[14,     1] loss: 0.459
[15,     1] loss: 0.429
[16,     1] loss: 0.402
[17,     1] loss: 0.370
[18,     1] loss: 0.338
[19,     1] loss: 0.308
[20,     1] loss: 0.281
[21,     1] loss: 0.249
[22,     1] loss: 0.223
[23,     1] loss: 0.201
[24,     1] loss: 0.177
[25,     1] loss: 0.158
[26,     1] loss: 0.140
[27,     1] loss: 0.124
[28,     1] loss: 0.108
[29,     1] loss: 0.096
[30,     1] loss: 0.083
[31,     1] loss: 0.075
[32,     1] loss: 0.066
[33,     1] loss: 0.059
[34,     1] loss: 0.053
[35,     1] loss: 0.049
[36,     1] loss: 0.045
[37,     1] loss: 0.043
[38,     1] loss: 0.040
[39,     1] loss: 0.037
[40,     1] loss: 0.036
[41,     1] loss: 0.034
[42,     1] loss: 0.034
[43,     1] loss: 0.032
[44,     1] loss: 0.032
[45,     1] loss: 0.032
[46,     1] loss: 0.032
[47,     1] loss: 0.032
[48,     1] loss: 0.031
[49,     1] loss: 0.032
[50,     1] loss: 0.032
[51,     1] loss: 0.032
[52,     1] loss: 0.033
[53,     1] loss: 0.033
[54,     1] loss: 0.034
[55,     1] loss: 0.033
[56,     1] loss: 0.034
[57,     1] loss: 0.034
[58,     1] loss: 0.034
[59,     1] loss: 0.035
[60,     1] loss: 0.033
[61,     1] loss: 0.036
[62,     1] loss: 0.036
[63,     1] loss: 0.035
[64,     1] loss: 0.035
[65,     1] loss: 0.034
[66,     1] loss: 0.034
[67,     1] loss: 0.034
[68,     1] loss: 0.034
[69,     1] loss: 0.033
[70,     1] loss: 0.033
[71,     1] loss: 0.033
[72,     1] loss: 0.034
[73,     1] loss: 0.033
Early stopping applied (best metric=0.3771951198577881)
Finished Training
Total time taken: 144.8436291217804
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.670
[4,     1] loss: 0.657
[5,     1] loss: 0.642
[6,     1] loss: 0.628
[7,     1] loss: 0.609
[8,     1] loss: 0.589
[9,     1] loss: 0.569
[10,     1] loss: 0.546
[11,     1] loss: 0.525
[12,     1] loss: 0.502
[13,     1] loss: 0.479
[14,     1] loss: 0.458
[15,     1] loss: 0.437
[16,     1] loss: 0.416
[17,     1] loss: 0.397
[18,     1] loss: 0.376
[19,     1] loss: 0.359
[20,     1] loss: 0.340
[21,     1] loss: 0.328
[22,     1] loss: 0.318
[23,     1] loss: 0.307
[24,     1] loss: 0.296
[25,     1] loss: 0.287
[26,     1] loss: 0.279
[27,     1] loss: 0.270
[28,     1] loss: 0.262
[29,     1] loss: 0.252
[30,     1] loss: 0.245
[31,     1] loss: 0.238
[32,     1] loss: 0.229
[33,     1] loss: 0.221
[34,     1] loss: 0.212
[35,     1] loss: 0.204
[36,     1] loss: 0.192
[37,     1] loss: 0.184
[38,     1] loss: 0.172
[39,     1] loss: 0.165
[40,     1] loss: 0.160
[41,     1] loss: 0.149
[42,     1] loss: 0.143
[43,     1] loss: 0.138
[44,     1] loss: 0.130
[45,     1] loss: 0.125
[46,     1] loss: 0.120
[47,     1] loss: 0.113
[48,     1] loss: 0.107
[49,     1] loss: 0.103
[50,     1] loss: 0.098
[51,     1] loss: 0.092
[52,     1] loss: 0.087
[53,     1] loss: 0.081
[54,     1] loss: 0.080
[55,     1] loss: 0.076
[56,     1] loss: 0.072
[57,     1] loss: 0.069
[58,     1] loss: 0.065
[59,     1] loss: 0.064
[60,     1] loss: 0.060
[61,     1] loss: 0.058
[62,     1] loss: 0.057
[63,     1] loss: 0.053
Early stopping applied (best metric=0.41055595874786377)
Finished Training
Total time taken: 125.66986465454102
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.653
[4,     1] loss: 0.627
[5,     1] loss: 0.602
[6,     1] loss: 0.577
[7,     1] loss: 0.552
[8,     1] loss: 0.520
[9,     1] loss: 0.493
[10,     1] loss: 0.462
[11,     1] loss: 0.435
[12,     1] loss: 0.409
[13,     1] loss: 0.381
[14,     1] loss: 0.357
[15,     1] loss: 0.331
[16,     1] loss: 0.308
[17,     1] loss: 0.286
[18,     1] loss: 0.265
[19,     1] loss: 0.245
[20,     1] loss: 0.227
[21,     1] loss: 0.209
[22,     1] loss: 0.194
[23,     1] loss: 0.175
[24,     1] loss: 0.152
[25,     1] loss: 0.141
[26,     1] loss: 0.122
[27,     1] loss: 0.106
[28,     1] loss: 0.090
[29,     1] loss: 0.085
[30,     1] loss: 0.074
[31,     1] loss: 0.065
[32,     1] loss: 0.059
[33,     1] loss: 0.053
[34,     1] loss: 0.049
[35,     1] loss: 0.046
[36,     1] loss: 0.041
[37,     1] loss: 0.039
[38,     1] loss: 0.038
[39,     1] loss: 0.037
[40,     1] loss: 0.035
[41,     1] loss: 0.033
[42,     1] loss: 0.033
[43,     1] loss: 0.033
[44,     1] loss: 0.032
[45,     1] loss: 0.032
[46,     1] loss: 0.032
[47,     1] loss: 0.032
[48,     1] loss: 0.031
[49,     1] loss: 0.033
[50,     1] loss: 0.031
[51,     1] loss: 0.032
[52,     1] loss: 0.033
[53,     1] loss: 0.033
[54,     1] loss: 0.033
[55,     1] loss: 0.033
[56,     1] loss: 0.033
[57,     1] loss: 0.034
[58,     1] loss: 0.033
[59,     1] loss: 0.033
[60,     1] loss: 0.033
[61,     1] loss: 0.034
Early stopping applied (best metric=0.4550071954727173)
Finished Training
Total time taken: 122.07781410217285
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.659
[4,     1] loss: 0.644
[5,     1] loss: 0.628
[6,     1] loss: 0.611
[7,     1] loss: 0.593
[8,     1] loss: 0.572
[9,     1] loss: 0.546
[10,     1] loss: 0.522
[11,     1] loss: 0.495
[12,     1] loss: 0.467
[13,     1] loss: 0.441
[14,     1] loss: 0.408
[15,     1] loss: 0.375
[16,     1] loss: 0.348
[17,     1] loss: 0.314
[18,     1] loss: 0.286
[19,     1] loss: 0.261
[20,     1] loss: 0.232
[21,     1] loss: 0.209
[22,     1] loss: 0.182
[23,     1] loss: 0.164
[24,     1] loss: 0.148
[25,     1] loss: 0.132
[26,     1] loss: 0.116
[27,     1] loss: 0.103
[28,     1] loss: 0.090
[29,     1] loss: 0.081
[30,     1] loss: 0.069
[31,     1] loss: 0.060
[32,     1] loss: 0.056
[33,     1] loss: 0.051
[34,     1] loss: 0.046
[35,     1] loss: 0.043
[36,     1] loss: 0.041
[37,     1] loss: 0.038
[38,     1] loss: 0.036
[39,     1] loss: 0.035
[40,     1] loss: 0.034
[41,     1] loss: 0.033
[42,     1] loss: 0.032
[43,     1] loss: 0.033
[44,     1] loss: 0.032
[45,     1] loss: 0.031
[46,     1] loss: 0.032
[47,     1] loss: 0.032
[48,     1] loss: 0.031
[49,     1] loss: 0.032
[50,     1] loss: 0.032
[51,     1] loss: 0.032
[52,     1] loss: 0.033
[53,     1] loss: 0.034
[54,     1] loss: 0.034
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.034
[58,     1] loss: 0.035
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.034
[62,     1] loss: 0.034
[63,     1] loss: 0.035
[64,     1] loss: 0.033
[65,     1] loss: 0.033
Early stopping applied (best metric=0.4055842459201813)
Finished Training
Total time taken: 130.26835179328918
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.676
[4,     1] loss: 0.666
[5,     1] loss: 0.656
[6,     1] loss: 0.645
[7,     1] loss: 0.632
[8,     1] loss: 0.618
[9,     1] loss: 0.602
[10,     1] loss: 0.583
[11,     1] loss: 0.565
[12,     1] loss: 0.544
[13,     1] loss: 0.520
[14,     1] loss: 0.497
[15,     1] loss: 0.473
[16,     1] loss: 0.452
[17,     1] loss: 0.428
[18,     1] loss: 0.408
[19,     1] loss: 0.388
[20,     1] loss: 0.367
[21,     1] loss: 0.344
[22,     1] loss: 0.325
[23,     1] loss: 0.310
[24,     1] loss: 0.294
[25,     1] loss: 0.282
[26,     1] loss: 0.268
[27,     1] loss: 0.256
[28,     1] loss: 0.246
[29,     1] loss: 0.233
[30,     1] loss: 0.221
[31,     1] loss: 0.212
[32,     1] loss: 0.199
[33,     1] loss: 0.189
[34,     1] loss: 0.177
[35,     1] loss: 0.165
[36,     1] loss: 0.155
[37,     1] loss: 0.146
[38,     1] loss: 0.134
[39,     1] loss: 0.122
[40,     1] loss: 0.115
[41,     1] loss: 0.107
[42,     1] loss: 0.096
[43,     1] loss: 0.085
[44,     1] loss: 0.076
[45,     1] loss: 0.068
[46,     1] loss: 0.063
[47,     1] loss: 0.058
[48,     1] loss: 0.055
[49,     1] loss: 0.051
[50,     1] loss: 0.046
[51,     1] loss: 0.043
[52,     1] loss: 0.041
[53,     1] loss: 0.039
[54,     1] loss: 0.036
[55,     1] loss: 0.035
[56,     1] loss: 0.032
[57,     1] loss: 0.031
[58,     1] loss: 0.030
[59,     1] loss: 0.029
[60,     1] loss: 0.027
[61,     1] loss: 0.027
[62,     1] loss: 0.026
[63,     1] loss: 0.026
[64,     1] loss: 0.025
[65,     1] loss: 0.025
[66,     1] loss: 0.024
[67,     1] loss: 0.024
[68,     1] loss: 0.023
Early stopping applied (best metric=0.4325702488422394)
Finished Training
Total time taken: 136.67059779167175
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.682
[3,     1] loss: 0.651
[4,     1] loss: 0.627
[5,     1] loss: 0.604
[6,     1] loss: 0.579
[7,     1] loss: 0.552
[8,     1] loss: 0.526
[9,     1] loss: 0.500
[10,     1] loss: 0.474
[11,     1] loss: 0.449
[12,     1] loss: 0.419
[13,     1] loss: 0.384
[14,     1] loss: 0.356
[15,     1] loss: 0.329
[16,     1] loss: 0.306
[17,     1] loss: 0.284
[18,     1] loss: 0.266
[19,     1] loss: 0.249
[20,     1] loss: 0.235
[21,     1] loss: 0.217
[22,     1] loss: 0.206
[23,     1] loss: 0.191
[24,     1] loss: 0.180
[25,     1] loss: 0.165
[26,     1] loss: 0.151
[27,     1] loss: 0.141
[28,     1] loss: 0.126
[29,     1] loss: 0.118
[30,     1] loss: 0.108
[31,     1] loss: 0.100
[32,     1] loss: 0.091
[33,     1] loss: 0.085
[34,     1] loss: 0.079
[35,     1] loss: 0.072
[36,     1] loss: 0.070
[37,     1] loss: 0.065
[38,     1] loss: 0.063
[39,     1] loss: 0.058
[40,     1] loss: 0.057
[41,     1] loss: 0.054
[42,     1] loss: 0.054
[43,     1] loss: 0.053
[44,     1] loss: 0.052
[45,     1] loss: 0.049
[46,     1] loss: 0.048
[47,     1] loss: 0.047
[48,     1] loss: 0.047
[49,     1] loss: 0.046
[50,     1] loss: 0.046
[51,     1] loss: 0.045
[52,     1] loss: 0.046
[53,     1] loss: 0.046
[54,     1] loss: 0.046
[55,     1] loss: 0.044
[56,     1] loss: 0.044
[57,     1] loss: 0.044
[58,     1] loss: 0.045
[59,     1] loss: 0.045
[60,     1] loss: 0.044
[61,     1] loss: 0.044
[62,     1] loss: 0.044
Early stopping applied (best metric=0.4297427237033844)
Finished Training
Total time taken: 124.83279657363892
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.677
[3,     1] loss: 0.654
[4,     1] loss: 0.635
[5,     1] loss: 0.617
[6,     1] loss: 0.596
[7,     1] loss: 0.574
[8,     1] loss: 0.548
[9,     1] loss: 0.524
[10,     1] loss: 0.492
[11,     1] loss: 0.460
[12,     1] loss: 0.427
[13,     1] loss: 0.391
[14,     1] loss: 0.360
[15,     1] loss: 0.322
[16,     1] loss: 0.290
[17,     1] loss: 0.256
[18,     1] loss: 0.230
[19,     1] loss: 0.204
[20,     1] loss: 0.182
[21,     1] loss: 0.161
[22,     1] loss: 0.139
[23,     1] loss: 0.118
[24,     1] loss: 0.101
[25,     1] loss: 0.087
[26,     1] loss: 0.076
[27,     1] loss: 0.067
[28,     1] loss: 0.060
[29,     1] loss: 0.056
[30,     1] loss: 0.050
[31,     1] loss: 0.046
[32,     1] loss: 0.043
[33,     1] loss: 0.040
[34,     1] loss: 0.038
[35,     1] loss: 0.038
[36,     1] loss: 0.036
[37,     1] loss: 0.035
[38,     1] loss: 0.035
[39,     1] loss: 0.034
[40,     1] loss: 0.033
[41,     1] loss: 0.033
[42,     1] loss: 0.033
[43,     1] loss: 0.034
[44,     1] loss: 0.034
[45,     1] loss: 0.035
[46,     1] loss: 0.035
[47,     1] loss: 0.035
[48,     1] loss: 0.036
[49,     1] loss: 0.036
[50,     1] loss: 0.036
[51,     1] loss: 0.037
[52,     1] loss: 0.037
[53,     1] loss: 0.037
[54,     1] loss: 0.037
[55,     1] loss: 0.038
[56,     1] loss: 0.038
[57,     1] loss: 0.036
[58,     1] loss: 0.037
[59,     1] loss: 0.038
[60,     1] loss: 0.036
[61,     1] loss: 0.037
[62,     1] loss: 0.037
[63,     1] loss: 0.036
[64,     1] loss: 0.036
[65,     1] loss: 0.035
[66,     1] loss: 0.035
[67,     1] loss: 0.035
[68,     1] loss: 0.034
Early stopping applied (best metric=0.34960559010505676)
Finished Training
Total time taken: 137.10505199432373
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.679
[3,     1] loss: 0.652
[4,     1] loss: 0.629
[5,     1] loss: 0.605
[6,     1] loss: 0.581
[7,     1] loss: 0.555
[8,     1] loss: 0.534
[9,     1] loss: 0.503
[10,     1] loss: 0.475
[11,     1] loss: 0.446
[12,     1] loss: 0.416
[13,     1] loss: 0.388
[14,     1] loss: 0.358
[15,     1] loss: 0.326
[16,     1] loss: 0.300
[17,     1] loss: 0.277
[18,     1] loss: 0.251
[19,     1] loss: 0.227
[20,     1] loss: 0.205
[21,     1] loss: 0.186
[22,     1] loss: 0.171
[23,     1] loss: 0.154
[24,     1] loss: 0.139
[25,     1] loss: 0.128
[26,     1] loss: 0.118
[27,     1] loss: 0.105
[28,     1] loss: 0.095
[29,     1] loss: 0.091
[30,     1] loss: 0.080
[31,     1] loss: 0.082
[32,     1] loss: 0.070
[33,     1] loss: 0.069
[34,     1] loss: 0.062
[35,     1] loss: 0.061
[36,     1] loss: 0.058
[37,     1] loss: 0.056
[38,     1] loss: 0.054
[39,     1] loss: 0.053
[40,     1] loss: 0.052
[41,     1] loss: 0.050
[42,     1] loss: 0.048
[43,     1] loss: 0.047
[44,     1] loss: 0.048
[45,     1] loss: 0.047
[46,     1] loss: 0.047
[47,     1] loss: 0.045
[48,     1] loss: 0.045
[49,     1] loss: 0.044
[50,     1] loss: 0.045
[51,     1] loss: 0.044
[52,     1] loss: 0.044
[53,     1] loss: 0.044
[54,     1] loss: 0.044
[55,     1] loss: 0.043
[56,     1] loss: 0.043
[57,     1] loss: 0.043
[58,     1] loss: 0.043
[59,     1] loss: 0.041
[60,     1] loss: 0.042
[61,     1] loss: 0.041
[62,     1] loss: 0.041
[63,     1] loss: 0.041
[64,     1] loss: 0.040
Early stopping applied (best metric=0.38345643877983093)
Finished Training
Total time taken: 129.55827236175537
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.685
[3,     1] loss: 0.664
[4,     1] loss: 0.641
[5,     1] loss: 0.617
[6,     1] loss: 0.595
[7,     1] loss: 0.568
[8,     1] loss: 0.544
[9,     1] loss: 0.513
[10,     1] loss: 0.486
[11,     1] loss: 0.458
[12,     1] loss: 0.421
[13,     1] loss: 0.394
[14,     1] loss: 0.360
[15,     1] loss: 0.332
[16,     1] loss: 0.308
[17,     1] loss: 0.286
[18,     1] loss: 0.261
[19,     1] loss: 0.242
[20,     1] loss: 0.217
[21,     1] loss: 0.202
[22,     1] loss: 0.185
[23,     1] loss: 0.172
[24,     1] loss: 0.157
[25,     1] loss: 0.145
[26,     1] loss: 0.133
[27,     1] loss: 0.124
[28,     1] loss: 0.114
[29,     1] loss: 0.105
[30,     1] loss: 0.097
[31,     1] loss: 0.089
[32,     1] loss: 0.081
[33,     1] loss: 0.075
[34,     1] loss: 0.067
[35,     1] loss: 0.065
[36,     1] loss: 0.061
[37,     1] loss: 0.058
[38,     1] loss: 0.055
[39,     1] loss: 0.053
[40,     1] loss: 0.051
[41,     1] loss: 0.050
[42,     1] loss: 0.048
[43,     1] loss: 0.047
[44,     1] loss: 0.046
[45,     1] loss: 0.046
[46,     1] loss: 0.045
[47,     1] loss: 0.044
[48,     1] loss: 0.044
[49,     1] loss: 0.042
[50,     1] loss: 0.042
[51,     1] loss: 0.041
[52,     1] loss: 0.042
[53,     1] loss: 0.041
[54,     1] loss: 0.041
[55,     1] loss: 0.041
[56,     1] loss: 0.041
[57,     1] loss: 0.040
[58,     1] loss: 0.040
[59,     1] loss: 0.040
[60,     1] loss: 0.040
[61,     1] loss: 0.040
[62,     1] loss: 0.040
[63,     1] loss: 0.039
Early stopping applied (best metric=0.40605443716049194)
Finished Training
Total time taken: 127.90324401855469
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.693
[3,     1] loss: 0.676
[4,     1] loss: 0.660
[5,     1] loss: 0.644
[6,     1] loss: 0.625
[7,     1] loss: 0.605
[8,     1] loss: 0.584
[9,     1] loss: 0.556
[10,     1] loss: 0.531
[11,     1] loss: 0.501
[12,     1] loss: 0.473
[13,     1] loss: 0.445
[14,     1] loss: 0.415
[15,     1] loss: 0.380
[16,     1] loss: 0.346
[17,     1] loss: 0.316
[18,     1] loss: 0.283
[19,     1] loss: 0.253
[20,     1] loss: 0.226
[21,     1] loss: 0.202
[22,     1] loss: 0.180
[23,     1] loss: 0.161
[24,     1] loss: 0.146
[25,     1] loss: 0.131
[26,     1] loss: 0.116
[27,     1] loss: 0.108
[28,     1] loss: 0.108
[29,     1] loss: 0.091
[30,     1] loss: 0.090
[31,     1] loss: 0.076
[32,     1] loss: 0.075
[33,     1] loss: 0.071
[34,     1] loss: 0.066
[35,     1] loss: 0.062
[36,     1] loss: 0.059
[37,     1] loss: 0.059
[38,     1] loss: 0.059
[39,     1] loss: 0.056
[40,     1] loss: 0.056
[41,     1] loss: 0.053
[42,     1] loss: 0.053
[43,     1] loss: 0.053
[44,     1] loss: 0.053
[45,     1] loss: 0.052
[46,     1] loss: 0.054
[47,     1] loss: 0.052
[48,     1] loss: 0.052
[49,     1] loss: 0.052
[50,     1] loss: 0.053
[51,     1] loss: 0.053
[52,     1] loss: 0.052
[53,     1] loss: 0.054
[54,     1] loss: 0.053
[55,     1] loss: 0.052
[56,     1] loss: 0.053
[57,     1] loss: 0.053
[58,     1] loss: 0.053
[59,     1] loss: 0.053
[60,     1] loss: 0.053
[61,     1] loss: 0.053
[62,     1] loss: 0.052
[63,     1] loss: 0.052
[64,     1] loss: 0.052
[65,     1] loss: 0.052
Early stopping applied (best metric=0.412566214799881)
Finished Training
Total time taken: 131.984356880188
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.668
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.622
[7,     1] loss: 0.602
[8,     1] loss: 0.579
[9,     1] loss: 0.559
[10,     1] loss: 0.532
[11,     1] loss: 0.507
[12,     1] loss: 0.477
[13,     1] loss: 0.450
[14,     1] loss: 0.422
[15,     1] loss: 0.396
[16,     1] loss: 0.373
[17,     1] loss: 0.349
[18,     1] loss: 0.326
[19,     1] loss: 0.304
[20,     1] loss: 0.284
[21,     1] loss: 0.262
[22,     1] loss: 0.243
[23,     1] loss: 0.218
[24,     1] loss: 0.203
[25,     1] loss: 0.184
[26,     1] loss: 0.169
[27,     1] loss: 0.150
[28,     1] loss: 0.138
[29,     1] loss: 0.123
[30,     1] loss: 0.112
[31,     1] loss: 0.101
[32,     1] loss: 0.092
[33,     1] loss: 0.087
[34,     1] loss: 0.080
[35,     1] loss: 0.074
[36,     1] loss: 0.068
[37,     1] loss: 0.063
[38,     1] loss: 0.059
[39,     1] loss: 0.058
[40,     1] loss: 0.054
[41,     1] loss: 0.053
[42,     1] loss: 0.052
[43,     1] loss: 0.049
[44,     1] loss: 0.047
[45,     1] loss: 0.047
[46,     1] loss: 0.047
[47,     1] loss: 0.046
[48,     1] loss: 0.045
[49,     1] loss: 0.045
[50,     1] loss: 0.045
[51,     1] loss: 0.045
[52,     1] loss: 0.045
[53,     1] loss: 0.044
[54,     1] loss: 0.044
[55,     1] loss: 0.044
[56,     1] loss: 0.042
[57,     1] loss: 0.044
[58,     1] loss: 0.044
[59,     1] loss: 0.044
[60,     1] loss: 0.044
[61,     1] loss: 0.042
[62,     1] loss: 0.044
Early stopping applied (best metric=0.460178941488266)
Finished Training
Total time taken: 126.23707723617554
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.672
[4,     1] loss: 0.654
[5,     1] loss: 0.640
[6,     1] loss: 0.625
[7,     1] loss: 0.608
[8,     1] loss: 0.590
[9,     1] loss: 0.571
[10,     1] loss: 0.554
[11,     1] loss: 0.535
[12,     1] loss: 0.511
[13,     1] loss: 0.485
[14,     1] loss: 0.457
[15,     1] loss: 0.432
[16,     1] loss: 0.403
[17,     1] loss: 0.377
[18,     1] loss: 0.342
[19,     1] loss: 0.320
[20,     1] loss: 0.290
[21,     1] loss: 0.263
[22,     1] loss: 0.238
[23,     1] loss: 0.212
[24,     1] loss: 0.193
[25,     1] loss: 0.172
[26,     1] loss: 0.157
[27,     1] loss: 0.140
[28,     1] loss: 0.129
[29,     1] loss: 0.117
[30,     1] loss: 0.106
[31,     1] loss: 0.097
[32,     1] loss: 0.089
[33,     1] loss: 0.081
[34,     1] loss: 0.076
[35,     1] loss: 0.071
[36,     1] loss: 0.064
[37,     1] loss: 0.062
[38,     1] loss: 0.058
[39,     1] loss: 0.055
[40,     1] loss: 0.052
[41,     1] loss: 0.052
[42,     1] loss: 0.049
[43,     1] loss: 0.047
[44,     1] loss: 0.046
[45,     1] loss: 0.045
[46,     1] loss: 0.045
[47,     1] loss: 0.043
[48,     1] loss: 0.043
[49,     1] loss: 0.042
[50,     1] loss: 0.043
[51,     1] loss: 0.042
[52,     1] loss: 0.041
[53,     1] loss: 0.041
[54,     1] loss: 0.042
[55,     1] loss: 0.041
[56,     1] loss: 0.042
[57,     1] loss: 0.041
[58,     1] loss: 0.041
[59,     1] loss: 0.040
[60,     1] loss: 0.040
[61,     1] loss: 0.039
[62,     1] loss: 0.040
[63,     1] loss: 0.039
[64,     1] loss: 0.039
[65,     1] loss: 0.040
[66,     1] loss: 0.038
[67,     1] loss: 0.039
[68,     1] loss: 0.039
[69,     1] loss: 0.037
[70,     1] loss: 0.037
Early stopping applied (best metric=0.3475729823112488)
Finished Training
Total time taken: 142.62042260169983
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.636
[6,     1] loss: 0.621
[7,     1] loss: 0.604
[8,     1] loss: 0.584
[9,     1] loss: 0.561
[10,     1] loss: 0.537
[11,     1] loss: 0.511
[12,     1] loss: 0.483
[13,     1] loss: 0.452
[14,     1] loss: 0.420
[15,     1] loss: 0.390
[16,     1] loss: 0.357
[17,     1] loss: 0.321
[18,     1] loss: 0.297
[19,     1] loss: 0.266
[20,     1] loss: 0.239
[21,     1] loss: 0.213
[22,     1] loss: 0.191
[23,     1] loss: 0.170
[24,     1] loss: 0.149
[25,     1] loss: 0.137
[26,     1] loss: 0.120
[27,     1] loss: 0.110
[28,     1] loss: 0.098
[29,     1] loss: 0.087
[30,     1] loss: 0.079
[31,     1] loss: 0.073
[32,     1] loss: 0.067
[33,     1] loss: 0.064
[34,     1] loss: 0.059
[35,     1] loss: 0.057
[36,     1] loss: 0.054
[37,     1] loss: 0.050
[38,     1] loss: 0.049
[39,     1] loss: 0.049
[40,     1] loss: 0.048
[41,     1] loss: 0.046
[42,     1] loss: 0.045
[43,     1] loss: 0.045
[44,     1] loss: 0.044
[45,     1] loss: 0.043
[46,     1] loss: 0.044
[47,     1] loss: 0.043
[48,     1] loss: 0.043
[49,     1] loss: 0.043
[50,     1] loss: 0.043
[51,     1] loss: 0.044
[52,     1] loss: 0.043
[53,     1] loss: 0.044
[54,     1] loss: 0.044
[55,     1] loss: 0.044
[56,     1] loss: 0.044
[57,     1] loss: 0.044
[58,     1] loss: 0.043
[59,     1] loss: 0.044
[60,     1] loss: 0.043
[61,     1] loss: 0.042
[62,     1] loss: 0.042
[63,     1] loss: 0.042
[64,     1] loss: 0.041
[65,     1] loss: 0.041
Early stopping applied (best metric=0.40089476108551025)
Finished Training
Total time taken: 132.81217646598816
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.647
[5,     1] loss: 0.629
[6,     1] loss: 0.610
[7,     1] loss: 0.587
[8,     1] loss: 0.562
[9,     1] loss: 0.539
[10,     1] loss: 0.515
[11,     1] loss: 0.488
[12,     1] loss: 0.463
[13,     1] loss: 0.435
[14,     1] loss: 0.411
[15,     1] loss: 0.391
[16,     1] loss: 0.373
[17,     1] loss: 0.359
[18,     1] loss: 0.346
[19,     1] loss: 0.333
[20,     1] loss: 0.320
[21,     1] loss: 0.308
[22,     1] loss: 0.295
[23,     1] loss: 0.281
[24,     1] loss: 0.265
[25,     1] loss: 0.250
[26,     1] loss: 0.238
[27,     1] loss: 0.221
[28,     1] loss: 0.207
[29,     1] loss: 0.192
[30,     1] loss: 0.180
[31,     1] loss: 0.164
[32,     1] loss: 0.152
[33,     1] loss: 0.140
[34,     1] loss: 0.131
[35,     1] loss: 0.123
[36,     1] loss: 0.114
[37,     1] loss: 0.106
[38,     1] loss: 0.099
[39,     1] loss: 0.094
[40,     1] loss: 0.091
[41,     1] loss: 0.086
[42,     1] loss: 0.082
[43,     1] loss: 0.079
[44,     1] loss: 0.077
[45,     1] loss: 0.074
[46,     1] loss: 0.072
[47,     1] loss: 0.071
[48,     1] loss: 0.070
[49,     1] loss: 0.068
[50,     1] loss: 0.067
[51,     1] loss: 0.066
[52,     1] loss: 0.064
[53,     1] loss: 0.064
[54,     1] loss: 0.063
[55,     1] loss: 0.062
[56,     1] loss: 0.062
[57,     1] loss: 0.061
[58,     1] loss: 0.061
[59,     1] loss: 0.061
[60,     1] loss: 0.060
[61,     1] loss: 0.061
Early stopping applied (best metric=0.4560120403766632)
Finished Training
Total time taken: 125.37449264526367
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.683
[3,     1] loss: 0.663
[4,     1] loss: 0.647
[5,     1] loss: 0.627
[6,     1] loss: 0.610
[7,     1] loss: 0.589
[8,     1] loss: 0.568
[9,     1] loss: 0.548
[10,     1] loss: 0.526
[11,     1] loss: 0.501
[12,     1] loss: 0.472
[13,     1] loss: 0.442
[14,     1] loss: 0.413
[15,     1] loss: 0.384
[16,     1] loss: 0.351
[17,     1] loss: 0.324
[18,     1] loss: 0.293
[19,     1] loss: 0.265
[20,     1] loss: 0.243
[21,     1] loss: 0.217
[22,     1] loss: 0.196
[23,     1] loss: 0.174
[24,     1] loss: 0.157
[25,     1] loss: 0.136
[26,     1] loss: 0.123
[27,     1] loss: 0.108
[28,     1] loss: 0.099
[29,     1] loss: 0.089
[30,     1] loss: 0.080
[31,     1] loss: 0.074
[32,     1] loss: 0.067
[33,     1] loss: 0.062
[34,     1] loss: 0.056
[35,     1] loss: 0.054
[36,     1] loss: 0.050
[37,     1] loss: 0.052
[38,     1] loss: 0.048
[39,     1] loss: 0.048
[40,     1] loss: 0.044
[41,     1] loss: 0.047
[42,     1] loss: 0.045
[43,     1] loss: 0.041
[44,     1] loss: 0.041
[45,     1] loss: 0.041
[46,     1] loss: 0.038
[47,     1] loss: 0.039
[48,     1] loss: 0.038
[49,     1] loss: 0.038
[50,     1] loss: 0.038
[51,     1] loss: 0.038
[52,     1] loss: 0.038
[53,     1] loss: 0.037
[54,     1] loss: 0.035
[55,     1] loss: 0.037
[56,     1] loss: 0.036
[57,     1] loss: 0.036
[58,     1] loss: 0.035
[59,     1] loss: 0.036
[60,     1] loss: 0.034
[61,     1] loss: 0.035
[62,     1] loss: 0.034
[63,     1] loss: 0.035
[64,     1] loss: 0.033
[65,     1] loss: 0.033
[66,     1] loss: 0.033
[67,     1] loss: 0.033
[68,     1] loss: 0.033
[69,     1] loss: 0.032
[70,     1] loss: 0.031
[71,     1] loss: 0.032
[72,     1] loss: 0.032
[73,     1] loss: 0.031
[74,     1] loss: 0.031
[75,     1] loss: 0.030
[76,     1] loss: 0.031
Early stopping applied (best metric=0.34241652488708496)
Finished Training
Total time taken: 155.8368501663208
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.662
[4,     1] loss: 0.646
[5,     1] loss: 0.627
[6,     1] loss: 0.610
[7,     1] loss: 0.590
[8,     1] loss: 0.570
[9,     1] loss: 0.547
[10,     1] loss: 0.524
[11,     1] loss: 0.495
[12,     1] loss: 0.462
[13,     1] loss: 0.433
[14,     1] loss: 0.401
[15,     1] loss: 0.371
[16,     1] loss: 0.336
[17,     1] loss: 0.309
[18,     1] loss: 0.275
[19,     1] loss: 0.254
[20,     1] loss: 0.226
[21,     1] loss: 0.211
[22,     1] loss: 0.184
[23,     1] loss: 0.163
[24,     1] loss: 0.144
[25,     1] loss: 0.129
[26,     1] loss: 0.115
[27,     1] loss: 0.103
[28,     1] loss: 0.090
[29,     1] loss: 0.077
[30,     1] loss: 0.071
[31,     1] loss: 0.064
[32,     1] loss: 0.057
[33,     1] loss: 0.052
[34,     1] loss: 0.049
[35,     1] loss: 0.046
[36,     1] loss: 0.043
[37,     1] loss: 0.042
[38,     1] loss: 0.040
[39,     1] loss: 0.038
[40,     1] loss: 0.037
[41,     1] loss: 0.037
[42,     1] loss: 0.036
[43,     1] loss: 0.036
[44,     1] loss: 0.036
[45,     1] loss: 0.036
[46,     1] loss: 0.035
[47,     1] loss: 0.037
[48,     1] loss: 0.036
[49,     1] loss: 0.035
[50,     1] loss: 0.037
[51,     1] loss: 0.036
[52,     1] loss: 0.036
[53,     1] loss: 0.037
[54,     1] loss: 0.038
[55,     1] loss: 0.038
[56,     1] loss: 0.038
[57,     1] loss: 0.038
[58,     1] loss: 0.038
[59,     1] loss: 0.037
[60,     1] loss: 0.039
[61,     1] loss: 0.037
[62,     1] loss: 0.039
[63,     1] loss: 0.038
[64,     1] loss: 0.037
[65,     1] loss: 0.037
[66,     1] loss: 0.035
[67,     1] loss: 0.036
[68,     1] loss: 0.036
Early stopping applied (best metric=0.3701094686985016)
Finished Training
Total time taken: 139.8866686820984
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.631
[6,     1] loss: 0.614
[7,     1] loss: 0.592
[8,     1] loss: 0.570
[9,     1] loss: 0.544
[10,     1] loss: 0.513
[11,     1] loss: 0.483
[12,     1] loss: 0.450
[13,     1] loss: 0.410
[14,     1] loss: 0.372
[15,     1] loss: 0.337
[16,     1] loss: 0.302
[17,     1] loss: 0.262
[18,     1] loss: 0.229
[19,     1] loss: 0.199
[20,     1] loss: 0.173
[21,     1] loss: 0.151
[22,     1] loss: 0.129
[23,     1] loss: 0.112
[24,     1] loss: 0.096
[25,     1] loss: 0.086
[26,     1] loss: 0.072
[27,     1] loss: 0.065
[28,     1] loss: 0.057
[29,     1] loss: 0.051
[30,     1] loss: 0.047
[31,     1] loss: 0.043
[32,     1] loss: 0.039
[33,     1] loss: 0.036
[34,     1] loss: 0.034
[35,     1] loss: 0.033
[36,     1] loss: 0.031
[37,     1] loss: 0.031
[38,     1] loss: 0.031
[39,     1] loss: 0.030
[40,     1] loss: 0.030
[41,     1] loss: 0.030
[42,     1] loss: 0.029
[43,     1] loss: 0.029
[44,     1] loss: 0.031
[45,     1] loss: 0.032
[46,     1] loss: 0.030
[47,     1] loss: 0.031
[48,     1] loss: 0.032
[49,     1] loss: 0.032
[50,     1] loss: 0.032
[51,     1] loss: 0.033
[52,     1] loss: 0.034
[53,     1] loss: 0.033
[54,     1] loss: 0.034
[55,     1] loss: 0.034
[56,     1] loss: 0.034
[57,     1] loss: 0.035
[58,     1] loss: 0.035
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.034
[62,     1] loss: 0.033
[63,     1] loss: 0.035
[64,     1] loss: 0.033
[65,     1] loss: 0.033
[66,     1] loss: 0.033
Early stopping applied (best metric=0.3494364619255066)
Finished Training
Total time taken: 136.02962183952332
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.652
[5,     1] loss: 0.636
[6,     1] loss: 0.618
[7,     1] loss: 0.602
[8,     1] loss: 0.582
[9,     1] loss: 0.558
[10,     1] loss: 0.533
[11,     1] loss: 0.506
[12,     1] loss: 0.479
[13,     1] loss: 0.451
[14,     1] loss: 0.416
[15,     1] loss: 0.385
[16,     1] loss: 0.354
[17,     1] loss: 0.323
[18,     1] loss: 0.287
[19,     1] loss: 0.258
[20,     1] loss: 0.231
[21,     1] loss: 0.205
[22,     1] loss: 0.187
[23,     1] loss: 0.167
[24,     1] loss: 0.148
[25,     1] loss: 0.133
[26,     1] loss: 0.118
[27,     1] loss: 0.099
[28,     1] loss: 0.088
[29,     1] loss: 0.079
[30,     1] loss: 0.071
[31,     1] loss: 0.064
[32,     1] loss: 0.059
[33,     1] loss: 0.052
[34,     1] loss: 0.049
[35,     1] loss: 0.046
[36,     1] loss: 0.043
[37,     1] loss: 0.040
[38,     1] loss: 0.038
[39,     1] loss: 0.038
[40,     1] loss: 0.037
[41,     1] loss: 0.036
[42,     1] loss: 0.034
[43,     1] loss: 0.034
[44,     1] loss: 0.033
[45,     1] loss: 0.032
[46,     1] loss: 0.033
[47,     1] loss: 0.032
[48,     1] loss: 0.033
[49,     1] loss: 0.034
[50,     1] loss: 0.033
[51,     1] loss: 0.033
[52,     1] loss: 0.035
[53,     1] loss: 0.034
[54,     1] loss: 0.035
[55,     1] loss: 0.034
[56,     1] loss: 0.034
[57,     1] loss: 0.035
[58,     1] loss: 0.035
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.034
[62,     1] loss: 0.034
[63,     1] loss: 0.033
[64,     1] loss: 0.034
[65,     1] loss: 0.033
[66,     1] loss: 0.032
[67,     1] loss: 0.032
[68,     1] loss: 0.031
[69,     1] loss: 0.032
[70,     1] loss: 0.030
[71,     1] loss: 0.030
Early stopping applied (best metric=0.40562230348587036)
Finished Training
Total time taken: 146.74451875686646
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.683
[3,     1] loss: 0.663
[4,     1] loss: 0.648
[5,     1] loss: 0.628
[6,     1] loss: 0.609
[7,     1] loss: 0.588
[8,     1] loss: 0.567
[9,     1] loss: 0.541
[10,     1] loss: 0.515
[11,     1] loss: 0.487
[12,     1] loss: 0.455
[13,     1] loss: 0.425
[14,     1] loss: 0.395
[15,     1] loss: 0.362
[16,     1] loss: 0.335
[17,     1] loss: 0.302
[18,     1] loss: 0.273
[19,     1] loss: 0.249
[20,     1] loss: 0.224
[21,     1] loss: 0.198
[22,     1] loss: 0.179
[23,     1] loss: 0.157
[24,     1] loss: 0.139
[25,     1] loss: 0.124
[26,     1] loss: 0.112
[27,     1] loss: 0.101
[28,     1] loss: 0.092
[29,     1] loss: 0.083
[30,     1] loss: 0.077
[31,     1] loss: 0.071
[32,     1] loss: 0.066
[33,     1] loss: 0.062
[34,     1] loss: 0.060
[35,     1] loss: 0.055
[36,     1] loss: 0.054
[37,     1] loss: 0.051
[38,     1] loss: 0.049
[39,     1] loss: 0.048
[40,     1] loss: 0.047
[41,     1] loss: 0.045
[42,     1] loss: 0.046
[43,     1] loss: 0.045
[44,     1] loss: 0.044
[45,     1] loss: 0.045
[46,     1] loss: 0.044
[47,     1] loss: 0.043
[48,     1] loss: 0.044
[49,     1] loss: 0.044
[50,     1] loss: 0.044
[51,     1] loss: 0.044
[52,     1] loss: 0.045
[53,     1] loss: 0.044
[54,     1] loss: 0.045
[55,     1] loss: 0.044
[56,     1] loss: 0.044
[57,     1] loss: 0.044
[58,     1] loss: 0.044
[59,     1] loss: 0.043
[60,     1] loss: 0.045
[61,     1] loss: 0.045
[62,     1] loss: 0.044
[63,     1] loss: 0.043
[64,     1] loss: 0.044
Early stopping applied (best metric=0.4189048111438751)
Finished Training
Total time taken: 132.51231050491333
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.678
[3,     1] loss: 0.658
[4,     1] loss: 0.641
[5,     1] loss: 0.621
[6,     1] loss: 0.603
[7,     1] loss: 0.581
[8,     1] loss: 0.554
[9,     1] loss: 0.531
[10,     1] loss: 0.501
[11,     1] loss: 0.472
[12,     1] loss: 0.442
[13,     1] loss: 0.412
[14,     1] loss: 0.377
[15,     1] loss: 0.345
[16,     1] loss: 0.318
[17,     1] loss: 0.286
[18,     1] loss: 0.255
[19,     1] loss: 0.232
[20,     1] loss: 0.203
[21,     1] loss: 0.184
[22,     1] loss: 0.163
[23,     1] loss: 0.143
[24,     1] loss: 0.128
[25,     1] loss: 0.112
[26,     1] loss: 0.101
[27,     1] loss: 0.091
[28,     1] loss: 0.083
[29,     1] loss: 0.074
[30,     1] loss: 0.066
[31,     1] loss: 0.060
[32,     1] loss: 0.056
[33,     1] loss: 0.051
[34,     1] loss: 0.046
[35,     1] loss: 0.044
[36,     1] loss: 0.042
[37,     1] loss: 0.041
[38,     1] loss: 0.038
[39,     1] loss: 0.037
[40,     1] loss: 0.036
[41,     1] loss: 0.035
[42,     1] loss: 0.035
[43,     1] loss: 0.035
[44,     1] loss: 0.033
[45,     1] loss: 0.033
[46,     1] loss: 0.034
[47,     1] loss: 0.032
[48,     1] loss: 0.033
[49,     1] loss: 0.034
[50,     1] loss: 0.032
[51,     1] loss: 0.034
[52,     1] loss: 0.034
[53,     1] loss: 0.033
[54,     1] loss: 0.034
[55,     1] loss: 0.034
[56,     1] loss: 0.035
[57,     1] loss: 0.034
[58,     1] loss: 0.034
[59,     1] loss: 0.034
[60,     1] loss: 0.034
[61,     1] loss: 0.036
[62,     1] loss: 0.034
[63,     1] loss: 0.033
[64,     1] loss: 0.033
[65,     1] loss: 0.033
Early stopping applied (best metric=0.41391584277153015)
Finished Training
Total time taken: 134.84558081626892
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.666
[5,     1] loss: 0.654
[6,     1] loss: 0.639
[7,     1] loss: 0.623
[8,     1] loss: 0.603
[9,     1] loss: 0.589
[10,     1] loss: 0.566
[11,     1] loss: 0.544
[12,     1] loss: 0.521
[13,     1] loss: 0.496
[14,     1] loss: 0.469
[15,     1] loss: 0.442
[16,     1] loss: 0.410
[17,     1] loss: 0.385
[18,     1] loss: 0.358
[19,     1] loss: 0.331
[20,     1] loss: 0.307
[21,     1] loss: 0.284
[22,     1] loss: 0.258
[23,     1] loss: 0.238
[24,     1] loss: 0.221
[25,     1] loss: 0.215
[26,     1] loss: 0.187
[27,     1] loss: 0.180
[28,     1] loss: 0.162
[29,     1] loss: 0.148
[30,     1] loss: 0.140
[31,     1] loss: 0.125
[32,     1] loss: 0.118
[33,     1] loss: 0.108
[34,     1] loss: 0.100
[35,     1] loss: 0.092
[36,     1] loss: 0.085
[37,     1] loss: 0.080
[38,     1] loss: 0.072
[39,     1] loss: 0.067
[40,     1] loss: 0.062
[41,     1] loss: 0.059
[42,     1] loss: 0.056
[43,     1] loss: 0.051
[44,     1] loss: 0.049
[45,     1] loss: 0.047
[46,     1] loss: 0.044
[47,     1] loss: 0.041
[48,     1] loss: 0.041
[49,     1] loss: 0.040
[50,     1] loss: 0.039
[51,     1] loss: 0.037
[52,     1] loss: 0.037
[53,     1] loss: 0.037
[54,     1] loss: 0.036
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.034
[58,     1] loss: 0.034
[59,     1] loss: 0.034
[60,     1] loss: 0.034
[61,     1] loss: 0.032
[62,     1] loss: 0.032
[63,     1] loss: 0.032
[64,     1] loss: 0.032
[65,     1] loss: 0.032
[66,     1] loss: 0.031
[67,     1] loss: 0.031
[68,     1] loss: 0.030
[69,     1] loss: 0.030
Early stopping applied (best metric=0.39652273058891296)
Finished Training
Total time taken: 143.24969458580017
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.681
[4,     1] loss: 0.672
[5,     1] loss: 0.663
[6,     1] loss: 0.652
[7,     1] loss: 0.641
[8,     1] loss: 0.627
[9,     1] loss: 0.610
[10,     1] loss: 0.593
[11,     1] loss: 0.573
[12,     1] loss: 0.552
[13,     1] loss: 0.529
[14,     1] loss: 0.506
[15,     1] loss: 0.479
[16,     1] loss: 0.457
[17,     1] loss: 0.432
[18,     1] loss: 0.409
[19,     1] loss: 0.380
[20,     1] loss: 0.362
[21,     1] loss: 0.337
[22,     1] loss: 0.319
[23,     1] loss: 0.294
[24,     1] loss: 0.271
[25,     1] loss: 0.249
[26,     1] loss: 0.229
[27,     1] loss: 0.209
[28,     1] loss: 0.192
[29,     1] loss: 0.176
[30,     1] loss: 0.156
[31,     1] loss: 0.142
[32,     1] loss: 0.127
[33,     1] loss: 0.110
[34,     1] loss: 0.097
[35,     1] loss: 0.090
[36,     1] loss: 0.082
[37,     1] loss: 0.073
[38,     1] loss: 0.067
[39,     1] loss: 0.061
[40,     1] loss: 0.056
[41,     1] loss: 0.053
[42,     1] loss: 0.049
[43,     1] loss: 0.046
[44,     1] loss: 0.044
[45,     1] loss: 0.041
[46,     1] loss: 0.041
[47,     1] loss: 0.039
[48,     1] loss: 0.037
[49,     1] loss: 0.036
[50,     1] loss: 0.035
[51,     1] loss: 0.034
[52,     1] loss: 0.034
[53,     1] loss: 0.033
[54,     1] loss: 0.035
[55,     1] loss: 0.033
[56,     1] loss: 0.034
[57,     1] loss: 0.033
[58,     1] loss: 0.033
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.034
[62,     1] loss: 0.034
[63,     1] loss: 0.033
[64,     1] loss: 0.033
[65,     1] loss: 0.032
[66,     1] loss: 0.034
[67,     1] loss: 0.034
[68,     1] loss: 0.033
[69,     1] loss: 0.032
[70,     1] loss: 0.033
[71,     1] loss: 0.033
[72,     1] loss: 0.033
Early stopping applied (best metric=0.4172433912754059)
Finished Training
Total time taken: 149.8609230518341
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.686
[3,     1] loss: 0.673
[4,     1] loss: 0.661
[5,     1] loss: 0.649
[6,     1] loss: 0.634
[7,     1] loss: 0.617
[8,     1] loss: 0.599
[9,     1] loss: 0.578
[10,     1] loss: 0.556
[11,     1] loss: 0.531
[12,     1] loss: 0.506
[13,     1] loss: 0.479
[14,     1] loss: 0.450
[15,     1] loss: 0.420
[16,     1] loss: 0.384
[17,     1] loss: 0.358
[18,     1] loss: 0.324
[19,     1] loss: 0.291
[20,     1] loss: 0.263
[21,     1] loss: 0.227
[22,     1] loss: 0.203
[23,     1] loss: 0.182
[24,     1] loss: 0.160
[25,     1] loss: 0.144
[26,     1] loss: 0.132
[27,     1] loss: 0.117
[28,     1] loss: 0.101
[29,     1] loss: 0.090
[30,     1] loss: 0.079
[31,     1] loss: 0.071
[32,     1] loss: 0.063
[33,     1] loss: 0.059
[34,     1] loss: 0.052
[35,     1] loss: 0.049
[36,     1] loss: 0.045
[37,     1] loss: 0.043
[38,     1] loss: 0.041
[39,     1] loss: 0.037
[40,     1] loss: 0.037
[41,     1] loss: 0.036
[42,     1] loss: 0.036
[43,     1] loss: 0.034
[44,     1] loss: 0.033
[45,     1] loss: 0.033
[46,     1] loss: 0.033
[47,     1] loss: 0.033
[48,     1] loss: 0.034
[49,     1] loss: 0.034
[50,     1] loss: 0.033
[51,     1] loss: 0.033
[52,     1] loss: 0.033
[53,     1] loss: 0.035
[54,     1] loss: 0.035
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.036
[58,     1] loss: 0.035
[59,     1] loss: 0.034
[60,     1] loss: 0.035
[61,     1] loss: 0.035
[62,     1] loss: 0.035
[63,     1] loss: 0.034
Early stopping applied (best metric=0.46914297342300415)
Finished Training
Total time taken: 131.64537572860718
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.657
[4,     1] loss: 0.640
[5,     1] loss: 0.627
[6,     1] loss: 0.609
[7,     1] loss: 0.591
[8,     1] loss: 0.573
[9,     1] loss: 0.553
[10,     1] loss: 0.530
[11,     1] loss: 0.505
[12,     1] loss: 0.481
[13,     1] loss: 0.453
[14,     1] loss: 0.427
[15,     1] loss: 0.396
[16,     1] loss: 0.372
[17,     1] loss: 0.338
[18,     1] loss: 0.312
[19,     1] loss: 0.284
[20,     1] loss: 0.256
[21,     1] loss: 0.230
[22,     1] loss: 0.204
[23,     1] loss: 0.182
[24,     1] loss: 0.159
[25,     1] loss: 0.143
[26,     1] loss: 0.130
[27,     1] loss: 0.116
[28,     1] loss: 0.104
[29,     1] loss: 0.093
[30,     1] loss: 0.082
[31,     1] loss: 0.073
[32,     1] loss: 0.067
[33,     1] loss: 0.059
[34,     1] loss: 0.054
[35,     1] loss: 0.052
[36,     1] loss: 0.048
[37,     1] loss: 0.044
[38,     1] loss: 0.041
[39,     1] loss: 0.039
[40,     1] loss: 0.037
[41,     1] loss: 0.035
[42,     1] loss: 0.034
[43,     1] loss: 0.033
[44,     1] loss: 0.032
[45,     1] loss: 0.032
[46,     1] loss: 0.031
[47,     1] loss: 0.031
[48,     1] loss: 0.030
[49,     1] loss: 0.030
[50,     1] loss: 0.029
[51,     1] loss: 0.030
[52,     1] loss: 0.029
[53,     1] loss: 0.029
[54,     1] loss: 0.030
[55,     1] loss: 0.029
[56,     1] loss: 0.029
[57,     1] loss: 0.029
[58,     1] loss: 0.028
[59,     1] loss: 0.029
[60,     1] loss: 0.028
[61,     1] loss: 0.028
[62,     1] loss: 0.026
[63,     1] loss: 0.027
[64,     1] loss: 0.027
[65,     1] loss: 0.026
[66,     1] loss: 0.027
[67,     1] loss: 0.026
[68,     1] loss: 0.026
[69,     1] loss: 0.025
[70,     1] loss: 0.025
Early stopping applied (best metric=0.3350512981414795)
Finished Training
Total time taken: 146.49776077270508
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.653
[4,     1] loss: 0.636
[5,     1] loss: 0.615
[6,     1] loss: 0.598
[7,     1] loss: 0.580
[8,     1] loss: 0.559
[9,     1] loss: 0.543
[10,     1] loss: 0.519
[11,     1] loss: 0.494
[12,     1] loss: 0.471
[13,     1] loss: 0.447
[14,     1] loss: 0.424
[15,     1] loss: 0.402
[16,     1] loss: 0.375
[17,     1] loss: 0.352
[18,     1] loss: 0.330
[19,     1] loss: 0.307
[20,     1] loss: 0.287
[21,     1] loss: 0.266
[22,     1] loss: 0.248
[23,     1] loss: 0.232
[24,     1] loss: 0.217
[25,     1] loss: 0.200
[26,     1] loss: 0.185
[27,     1] loss: 0.171
[28,     1] loss: 0.161
[29,     1] loss: 0.149
[30,     1] loss: 0.137
[31,     1] loss: 0.126
[32,     1] loss: 0.116
[33,     1] loss: 0.104
[34,     1] loss: 0.094
[35,     1] loss: 0.085
[36,     1] loss: 0.078
[37,     1] loss: 0.073
[38,     1] loss: 0.066
[39,     1] loss: 0.061
[40,     1] loss: 0.055
[41,     1] loss: 0.052
[42,     1] loss: 0.048
[43,     1] loss: 0.045
[44,     1] loss: 0.044
[45,     1] loss: 0.040
[46,     1] loss: 0.039
[47,     1] loss: 0.038
[48,     1] loss: 0.036
[49,     1] loss: 0.035
[50,     1] loss: 0.034
[51,     1] loss: 0.033
[52,     1] loss: 0.032
[53,     1] loss: 0.031
[54,     1] loss: 0.031
[55,     1] loss: 0.030
[56,     1] loss: 0.031
[57,     1] loss: 0.029
[58,     1] loss: 0.030
[59,     1] loss: 0.030
[60,     1] loss: 0.030
[61,     1] loss: 0.030
[62,     1] loss: 0.031
[63,     1] loss: 0.029
[64,     1] loss: 0.030
[65,     1] loss: 0.029
Early stopping applied (best metric=0.42016297578811646)
Finished Training
Total time taken: 136.27946209907532
{'Hydroxylation-P Validation Accuracy': 0.7834953454139384, 'Hydroxylation-P Validation Sensitivity': 0.7785714285714286, 'Hydroxylation-P Validation Specificity': 0.7846176866676642, 'Hydroxylation-P Validation Precision': 0.4469186083780138, 'Hydroxylation-P AUC ROC': 0.8464929814810856, 'Hydroxylation-P AUC PR': 0.576913095975358, 'Hydroxylation-P MCC': 0.4681977021997112, 'Hydroxylation-P F1': 0.5636675894655017, 'Validation Loss (Hydroxylation-P)': 0.40262102723121646, 'Validation Loss (total)': 0.40262102723121646}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00502664614356173,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2507388274,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.628030476366759}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.643
[4,     1] loss: 0.603
[5,     1] loss: 0.559
[6,     1] loss: 0.512
[7,     1] loss: 0.456
[8,     1] loss: 0.395
[9,     1] loss: 0.337
[10,     1] loss: 0.274
[11,     1] loss: 0.220
[12,     1] loss: 0.202
[13,     1] loss: 0.166
[14,     1] loss: 0.135
[15,     1] loss: 0.116
[16,     1] loss: 0.075
[17,     1] loss: 0.049
[18,     1] loss: 0.042
[19,     1] loss: 0.031
[20,     1] loss: 0.023
[21,     1] loss: 0.017
[22,     1] loss: 0.014
[23,     1] loss: 0.012
[24,     1] loss: 0.010
[25,     1] loss: 0.009
[26,     1] loss: 0.007
[27,     1] loss: 0.007
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00505025094121551,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2970924247,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.252394318986413}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.649
[4,     1] loss: 0.614
[5,     1] loss: 0.577
[6,     1] loss: 0.535
[7,     1] loss: 0.489
[8,     1] loss: 0.435
[9,     1] loss: 0.376
[10,     1] loss: 0.320
[11,     1] loss: 0.271
[12,     1] loss: 0.214
[13,     1] loss: 0.158
[14,     1] loss: 0.124
[15,     1] loss: 0.101
[16,     1] loss: 0.087
[17,     1] loss: 0.064
[18,     1] loss: 0.055
[19,     1] loss: 0.040
[20,     1] loss: 0.038
[21,     1] loss: 0.027
[22,     1] loss: 0.024
[23,     1] loss: 0.019
[24,     1] loss: 0.015
[25,     1] loss: 0.013
[26,     1] loss: 0.012
[27,     1] loss: 0.011
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050727102791582376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1613349319,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.169081888886012}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.649
[4,     1] loss: 0.621
[5,     1] loss: 0.592
[6,     1] loss: 0.558
[7,     1] loss: 0.521
[8,     1] loss: 0.478
[9,     1] loss: 0.435
[10,     1] loss: 0.390
[11,     1] loss: 0.348
[12,     1] loss: 0.312
[13,     1] loss: 0.283
[14,     1] loss: 0.259
[15,     1] loss: 0.237
[16,     1] loss: 0.214
[17,     1] loss: 0.195
[18,     1] loss: 0.181
[19,     1] loss: 0.169
[20,     1] loss: 0.159
[21,     1] loss: 0.152
[22,     1] loss: 0.140
[23,     1] loss: 0.133
[24,     1] loss: 0.127
[25,     1] loss: 0.121
[26,     1] loss: 0.116
[27,     1] loss: 0.112
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00683297452868753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2083057556,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.286488142205073}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.661
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001765260319596609,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3111441877,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.387222739005555}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.689
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002537569886983315,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2334067069,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.81122561579872}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.688
[3,     1] loss: 0.676
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005872061673967272,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1598559307,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.192670629464297}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.644
[4,     1] loss: 0.604
[5,     1] loss: 0.554
[6,     1] loss: 0.494
[7,     1] loss: 0.432
[8,     1] loss: 0.359
[9,     1] loss: 0.293
[10,     1] loss: 0.242
[11,     1] loss: 0.192
[12,     1] loss: 0.154
[13,     1] loss: 0.117
[14,     1] loss: 0.091
[15,     1] loss: 0.078
[16,     1] loss: 0.061
[17,     1] loss: 0.034
[18,     1] loss: 0.031
[19,     1] loss: 0.018
[20,     1] loss: 0.013
[21,     1] loss: 0.010
[22,     1] loss: 0.007
[23,     1] loss: 0.005
[24,     1] loss: 0.004
[25,     1] loss: 0.003
[26,     1] loss: 0.003
[27,     1] loss: 0.002
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015938999223669178,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 621611105,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.240966260169035}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.674
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009871430006391813,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1850145620,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.112230290296734}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.678
[3,     1] loss: 0.635
[4,     1] loss: 0.595
[5,     1] loss: 0.550
[6,     1] loss: 0.507
[7,     1] loss: 0.455
[8,     1] loss: 0.421
[9,     1] loss: 0.441
[10,     1] loss: 0.383
[11,     1] loss: 0.345
[12,     1] loss: 0.325
[13,     1] loss: 0.289
[14,     1] loss: 0.263
[15,     1] loss: 0.223
[16,     1] loss: 0.200
[17,     1] loss: 0.165
[18,     1] loss: 0.147
[19,     1] loss: 0.123
[20,     1] loss: 0.107
[21,     1] loss: 0.092
[22,     1] loss: 0.083
[23,     1] loss: 0.072
[24,     1] loss: 0.065
[25,     1] loss: 0.069
[26,     1] loss: 0.181
[27,     1] loss: 1.105
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0065674456172576945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3929403936,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.857619277872868}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.638
[4,     1] loss: 0.599
[5,     1] loss: 0.553
[6,     1] loss: 0.503
[7,     1] loss: 0.448
[8,     1] loss: 0.388
[9,     1] loss: 0.328
[10,     1] loss: 0.289
[11,     1] loss: 0.257
[12,     1] loss: 0.225
[13,     1] loss: 0.176
[14,     1] loss: 0.160
[15,     1] loss: 0.138
[16,     1] loss: 0.116
[17,     1] loss: 0.099
[18,     1] loss: 0.096
[19,     1] loss: 0.063
[20,     1] loss: 0.057
[21,     1] loss: 0.062
[22,     1] loss: 0.043
[23,     1] loss: 0.042
[24,     1] loss: 0.029
[25,     1] loss: 0.028
[26,     1] loss: 0.022
[27,     1] loss: 0.023
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066195460237929715,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3710663863,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.176872522416845}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.684
[3,     1] loss: 0.642
[4,     1] loss: 0.598
[5,     1] loss: 0.546
[6,     1] loss: 0.499
[7,     1] loss: 0.454
[8,     1] loss: 0.408
[9,     1] loss: 0.364
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009632333655239658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1604728162,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.42135043176229}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.669
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003305243246805048,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1812942362,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.188926192004523}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002782242224425224,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2386771367,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.863764290143328}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015951370650590247,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2765220502,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.28192195720878}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.683
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004866005073036593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3107809628,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.566144961120822}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.672
[3,     1] loss: 0.630
[4,     1] loss: 0.590
[5,     1] loss: 0.539
[6,     1] loss: 0.484
[7,     1] loss: 0.423
[8,     1] loss: 0.364
[9,     1] loss: 0.298
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008570243873555986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3483957866,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.48431117885065}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.630
[4,     1] loss: 0.587
[5,     1] loss: 0.539
[6,     1] loss: 0.493
[7,     1] loss: 0.453
[8,     1] loss: 0.415
[9,     1] loss: 0.379
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004387993700323398,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1204553274,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.9519492527413966}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.670
[3,     1] loss: 0.635
[4,     1] loss: 0.598
[5,     1] loss: 0.565
[6,     1] loss: 0.536
[7,     1] loss: 0.498
[8,     1] loss: 0.454
[9,     1] loss: 0.412
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007057531760612228,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1210410729,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.86866388487798}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.649
[4,     1] loss: 0.612
[5,     1] loss: 0.574
[6,     1] loss: 0.535
[7,     1] loss: 0.485
[8,     1] loss: 0.429
[9,     1] loss: 0.356
[10,     1] loss: 0.295
[11,     1] loss: 0.239
[12,     1] loss: 0.188
[13,     1] loss: 0.150
[14,     1] loss: 0.108
[15,     1] loss: 0.144
[16,     1] loss: 0.123
[17,     1] loss: 0.089
[18,     1] loss: 0.177
[19,     1] loss: 0.072
[20,     1] loss: 0.192
[21,     1] loss: 0.093
[22,     1] loss: 0.108
[23,     1] loss: 0.089
[24,     1] loss: 0.085
[25,     1] loss: 0.082
[26,     1] loss: 0.083
[27,     1] loss: 0.077
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003473188654640956,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 174537607,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.525537134884011}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005393921361134254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2826207877,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.61486146947597}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.651
[4,     1] loss: 0.610
[5,     1] loss: 0.565
[6,     1] loss: 0.506
[7,     1] loss: 0.446
[8,     1] loss: 0.374
[9,     1] loss: 0.305
[10,     1] loss: 0.233
[11,     1] loss: 0.168
[12,     1] loss: 0.119
[13,     1] loss: 0.089
[14,     1] loss: 0.058
[15,     1] loss: 0.038
[16,     1] loss: 0.026
[17,     1] loss: 0.020
[18,     1] loss: 0.014
[19,     1] loss: 0.011
[20,     1] loss: 0.009
[21,     1] loss: 0.007
[22,     1] loss: 0.006
[23,     1] loss: 0.005
[24,     1] loss: 0.005
[25,     1] loss: 0.005
[26,     1] loss: 0.005
[27,     1] loss: 0.005
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003439987715503497,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2956424208,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.825874022468318}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.652
[4,     1] loss: 0.633
[5,     1] loss: 0.608
[6,     1] loss: 0.586
[7,     1] loss: 0.562
[8,     1] loss: 0.529
[9,     1] loss: 0.502
[10,     1] loss: 0.472
[11,     1] loss: 0.441
[12,     1] loss: 0.412
[13,     1] loss: 0.382
[14,     1] loss: 0.352
[15,     1] loss: 0.321
[16,     1] loss: 0.293
[17,     1] loss: 0.266
[18,     1] loss: 0.243
[19,     1] loss: 0.221
[20,     1] loss: 0.199
[21,     1] loss: 0.187
[22,     1] loss: 0.164
[23,     1] loss: 0.162
[24,     1] loss: 0.148
[25,     1] loss: 0.137
[26,     1] loss: 0.127
[27,     1] loss: 0.124
[28,     1] loss: 0.116
[29,     1] loss: 0.112
[30,     1] loss: 0.106
[31,     1] loss: 0.105
[32,     1] loss: 0.102
[33,     1] loss: 0.098
[34,     1] loss: 0.095
[35,     1] loss: 0.096
[36,     1] loss: 0.096
[37,     1] loss: 0.095
[38,     1] loss: 0.094
[39,     1] loss: 0.096
[40,     1] loss: 0.095
[41,     1] loss: 0.096
[42,     1] loss: 0.094
[43,     1] loss: 0.096
[44,     1] loss: 0.096
[45,     1] loss: 0.093
[46,     1] loss: 0.094
[47,     1] loss: 0.093
[48,     1] loss: 0.092
[49,     1] loss: 0.090
[50,     1] loss: 0.086
[51,     1] loss: 0.081
[52,     1] loss: 0.087
[53,     1] loss: 0.278
[54,     1] loss: 0.665
[55,     1] loss: 0.517
[56,     1] loss: 0.428
[57,     1] loss: 0.396
[58,     1] loss: 0.380
[59,     1] loss: 0.374
[60,     1] loss: 0.364
[61,     1] loss: 0.355
[62,     1] loss: 0.340
[63,     1] loss: 0.323
[64,     1] loss: 0.308
[65,     1] loss: 0.293
[66,     1] loss: 0.275
[67,     1] loss: 0.256
[68,     1] loss: 0.240
[69,     1] loss: 0.223
[70,     1] loss: 0.206
[71,     1] loss: 0.189
[72,     1] loss: 0.174
[73,     1] loss: 0.164
[74,     1] loss: 0.147
[75,     1] loss: 0.137
[76,     1] loss: 0.127
[77,     1] loss: 0.118
[78,     1] loss: 0.111
[79,     1] loss: 0.101
[80,     1] loss: 0.096
[81,     1] loss: 0.094
[82,     1] loss: 0.090
[83,     1] loss: 0.086
[84,     1] loss: 0.088
[85,     1] loss: 0.085
[86,     1] loss: 0.086
[87,     1] loss: 0.085
[88,     1] loss: 0.087
[89,     1] loss: 0.087
[90,     1] loss: 0.088
[91,     1] loss: 0.088
[92,     1] loss: 0.087
[93,     1] loss: 0.088
[94,     1] loss: 0.089
[95,     1] loss: 0.088
[96,     1] loss: 0.089
[97,     1] loss: 0.089
[98,     1] loss: 0.103
[99,     1] loss: 0.336
[100,     1] loss: 0.245
[101,     1] loss: 0.584
[102,     1] loss: 0.598
[103,     1] loss: 0.496
[104,     1] loss: 0.443
[105,     1] loss: 0.469
[106,     1] loss: 0.491
[107,     1] loss: 0.491
[108,     1] loss: 0.490
[109,     1] loss: 0.489
[110,     1] loss: 0.488
[111,     1] loss: 0.483
[112,     1] loss: 0.472
[113,     1] loss: 0.457
[114,     1] loss: 0.445
[115,     1] loss: 0.432
[116,     1] loss: 0.417
[117,     1] loss: 0.404
[118,     1] loss: 0.390
[119,     1] loss: 0.376
[120,     1] loss: 0.358
[121,     1] loss: 0.340
[122,     1] loss: 0.324
[123,     1] loss: 0.309
[124,     1] loss: 0.310
[125,     1] loss: 0.274
[126,     1] loss: 0.266
[127,     1] loss: 0.249
[128,     1] loss: 0.229
[129,     1] loss: 0.216
[130,     1] loss: 0.199
[131,     1] loss: 0.188
[132,     1] loss: 0.175
[133,     1] loss: 0.164
[134,     1] loss: 0.153
[135,     1] loss: 0.146
Early stopping applied (best metric=0.31592196226119995)
Finished Training
Total time taken: 287.35388350486755
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.676
[3,     1] loss: 0.649
[4,     1] loss: 0.628
[5,     1] loss: 0.604
[6,     1] loss: 0.580
[7,     1] loss: 0.559
[8,     1] loss: 0.530
[9,     1] loss: 0.505
[10,     1] loss: 0.473
[11,     1] loss: 0.439
[12,     1] loss: 0.407
[13,     1] loss: 0.373
[14,     1] loss: 0.341
[15,     1] loss: 0.309
[16,     1] loss: 0.284
[17,     1] loss: 0.258
[18,     1] loss: 0.237
[19,     1] loss: 0.217
[20,     1] loss: 0.193
[21,     1] loss: 0.177
[22,     1] loss: 0.169
[23,     1] loss: 0.155
[24,     1] loss: 0.145
[25,     1] loss: 0.132
[26,     1] loss: 0.127
[27,     1] loss: 0.120
[28,     1] loss: 0.115
[29,     1] loss: 0.112
[30,     1] loss: 0.110
[31,     1] loss: 0.107
[32,     1] loss: 0.103
[33,     1] loss: 0.102
[34,     1] loss: 0.100
[35,     1] loss: 0.098
[36,     1] loss: 0.097
[37,     1] loss: 0.097
[38,     1] loss: 0.097
[39,     1] loss: 0.097
[40,     1] loss: 0.097
[41,     1] loss: 0.094
[42,     1] loss: 0.095
[43,     1] loss: 0.095
[44,     1] loss: 0.093
[45,     1] loss: 0.092
[46,     1] loss: 0.093
[47,     1] loss: 0.091
[48,     1] loss: 0.089
[49,     1] loss: 0.088
[50,     1] loss: 0.087
[51,     1] loss: 0.084
[52,     1] loss: 0.080
[53,     1] loss: 0.122
[54,     1] loss: 0.462
[55,     1] loss: 0.512
[56,     1] loss: 0.390
[57,     1] loss: 0.392
[58,     1] loss: 0.348
[59,     1] loss: 0.362
[60,     1] loss: 0.353
[61,     1] loss: 0.354
[62,     1] loss: 0.341
[63,     1] loss: 0.326
[64,     1] loss: 0.317
[65,     1] loss: 0.302
[66,     1] loss: 0.288
[67,     1] loss: 0.278
[68,     1] loss: 0.262
[69,     1] loss: 0.244
[70,     1] loss: 0.231
[71,     1] loss: 0.216
[72,     1] loss: 0.200
[73,     1] loss: 0.185
[74,     1] loss: 0.172
[75,     1] loss: 0.156
[76,     1] loss: 0.141
[77,     1] loss: 0.130
[78,     1] loss: 0.122
[79,     1] loss: 0.112
[80,     1] loss: 0.105
[81,     1] loss: 0.102
[82,     1] loss: 0.098
[83,     1] loss: 0.094
[84,     1] loss: 0.094
[85,     1] loss: 0.088
[86,     1] loss: 0.089
[87,     1] loss: 0.087
[88,     1] loss: 0.087
[89,     1] loss: 0.089
[90,     1] loss: 0.088
[91,     1] loss: 0.090
[92,     1] loss: 0.089
[93,     1] loss: 0.091
[94,     1] loss: 0.092
[95,     1] loss: 0.092
[96,     1] loss: 0.093
[97,     1] loss: 0.095
[98,     1] loss: 0.125
[99,     1] loss: 0.453
[100,     1] loss: 0.883
[101,     1] loss: 0.387
[102,     1] loss: 0.499
[103,     1] loss: 0.518
[104,     1] loss: 0.504
[105,     1] loss: 0.497
[106,     1] loss: 0.506
[107,     1] loss: 0.516
Early stopping applied (best metric=0.34921374917030334)
Finished Training
Total time taken: 228.63613653182983
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.649
[5,     1] loss: 0.634
[6,     1] loss: 0.617
[7,     1] loss: 0.596
[8,     1] loss: 0.578
[9,     1] loss: 0.557
[10,     1] loss: 0.528
[11,     1] loss: 0.500
[12,     1] loss: 0.474
[13,     1] loss: 0.440
[14,     1] loss: 0.411
[15,     1] loss: 0.376
[16,     1] loss: 0.342
[17,     1] loss: 0.312
[18,     1] loss: 0.288
[19,     1] loss: 0.266
[20,     1] loss: 0.244
[21,     1] loss: 0.223
[22,     1] loss: 0.209
[23,     1] loss: 0.195
[24,     1] loss: 0.182
[25,     1] loss: 0.178
[26,     1] loss: 0.164
[27,     1] loss: 0.153
[28,     1] loss: 0.151
[29,     1] loss: 0.138
[30,     1] loss: 0.154
[31,     1] loss: 0.138
[32,     1] loss: 0.163
[33,     1] loss: 0.139
[34,     1] loss: 0.162
[35,     1] loss: 0.140
[36,     1] loss: 0.142
[37,     1] loss: 0.140
[38,     1] loss: 0.134
[39,     1] loss: 0.135
[40,     1] loss: 0.132
[41,     1] loss: 0.130
[42,     1] loss: 0.127
[43,     1] loss: 0.127
[44,     1] loss: 0.122
[45,     1] loss: 0.125
[46,     1] loss: 0.120
[47,     1] loss: 0.120
[48,     1] loss: 0.115
[49,     1] loss: 0.112
[50,     1] loss: 0.109
[51,     1] loss: 0.106
[52,     1] loss: 0.101
[53,     1] loss: 0.098
[54,     1] loss: 0.110
[55,     1] loss: 0.204
[56,     1] loss: 0.230
[57,     1] loss: 0.615
[58,     1] loss: 0.316
[59,     1] loss: 0.386
[60,     1] loss: 0.393
[61,     1] loss: 0.370
[62,     1] loss: 0.374
[63,     1] loss: 0.383
[64,     1] loss: 0.371
[65,     1] loss: 0.370
[66,     1] loss: 0.353
[67,     1] loss: 0.336
[68,     1] loss: 0.326
Early stopping applied (best metric=0.39613616466522217)
Finished Training
Total time taken: 146.41066932678223
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.678
[3,     1] loss: 0.655
[4,     1] loss: 0.637
[5,     1] loss: 0.617
[6,     1] loss: 0.596
[7,     1] loss: 0.571
[8,     1] loss: 0.547
[9,     1] loss: 0.522
[10,     1] loss: 0.493
[11,     1] loss: 0.464
[12,     1] loss: 0.434
[13,     1] loss: 0.406
[14,     1] loss: 0.379
[15,     1] loss: 0.348
[16,     1] loss: 0.317
[17,     1] loss: 0.292
[18,     1] loss: 0.271
[19,     1] loss: 0.249
[20,     1] loss: 0.230
[21,     1] loss: 0.213
[22,     1] loss: 0.194
[23,     1] loss: 0.181
[24,     1] loss: 0.168
[25,     1] loss: 0.169
[26,     1] loss: 0.148
[27,     1] loss: 0.141
[28,     1] loss: 0.139
[29,     1] loss: 0.130
[30,     1] loss: 0.126
[31,     1] loss: 0.122
[32,     1] loss: 0.119
[33,     1] loss: 0.115
[34,     1] loss: 0.114
[35,     1] loss: 0.110
[36,     1] loss: 0.111
[37,     1] loss: 0.109
[38,     1] loss: 0.107
[39,     1] loss: 0.105
[40,     1] loss: 0.105
[41,     1] loss: 0.104
[42,     1] loss: 0.105
[43,     1] loss: 0.104
[44,     1] loss: 0.104
[45,     1] loss: 0.101
[46,     1] loss: 0.101
[47,     1] loss: 0.099
[48,     1] loss: 0.096
[49,     1] loss: 0.132
[50,     1] loss: 0.343
[51,     1] loss: 0.584
[52,     1] loss: 0.488
[53,     1] loss: 0.443
[54,     1] loss: 0.426
[55,     1] loss: 0.429
[56,     1] loss: 0.441
[57,     1] loss: 0.444
[58,     1] loss: 0.449
[59,     1] loss: 0.448
[60,     1] loss: 0.446
[61,     1] loss: 0.441
[62,     1] loss: 0.436
[63,     1] loss: 0.423
[64,     1] loss: 0.412
[65,     1] loss: 0.399
[66,     1] loss: 0.387
[67,     1] loss: 0.368
[68,     1] loss: 0.348
[69,     1] loss: 0.329
[70,     1] loss: 0.307
[71,     1] loss: 0.289
[72,     1] loss: 0.265
[73,     1] loss: 0.241
[74,     1] loss: 0.223
[75,     1] loss: 0.200
[76,     1] loss: 0.184
[77,     1] loss: 0.166
[78,     1] loss: 0.148
[79,     1] loss: 0.132
[80,     1] loss: 0.119
[81,     1] loss: 0.110
[82,     1] loss: 0.101
[83,     1] loss: 0.097
[84,     1] loss: 0.090
[85,     1] loss: 0.085
[86,     1] loss: 0.084
[87,     1] loss: 0.083
[88,     1] loss: 0.079
[89,     1] loss: 0.081
[90,     1] loss: 0.081
[91,     1] loss: 0.080
[92,     1] loss: 0.082
[93,     1] loss: 0.087
[94,     1] loss: 0.085
[95,     1] loss: 0.086
[96,     1] loss: 0.088
[97,     1] loss: 0.091
[98,     1] loss: 0.090
[99,     1] loss: 0.091
[100,     1] loss: 0.091
[101,     1] loss: 0.092
[102,     1] loss: 0.092
[103,     1] loss: 0.094
[104,     1] loss: 0.092
[105,     1] loss: 0.092
[106,     1] loss: 0.092
[107,     1] loss: 0.090
[108,     1] loss: 0.094
[109,     1] loss: 0.092
[110,     1] loss: 0.091
[111,     1] loss: 0.091
[112,     1] loss: 0.094
[113,     1] loss: 0.356
[114,     1] loss: 0.476
[115,     1] loss: 0.667
[116,     1] loss: 0.605
[117,     1] loss: 0.549
[118,     1] loss: 0.494
[119,     1] loss: 0.452
[120,     1] loss: 0.456
[121,     1] loss: 0.451
[122,     1] loss: 0.447
Early stopping applied (best metric=0.39496201276779175)
Finished Training
Total time taken: 262.1365542411804
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.656
[4,     1] loss: 0.638
[5,     1] loss: 0.618
[6,     1] loss: 0.598
[7,     1] loss: 0.577
[8,     1] loss: 0.554
[9,     1] loss: 0.527
[10,     1] loss: 0.505
[11,     1] loss: 0.476
[12,     1] loss: 0.450
[13,     1] loss: 0.429
[14,     1] loss: 0.406
[15,     1] loss: 0.383
[16,     1] loss: 0.358
[17,     1] loss: 0.333
[18,     1] loss: 0.314
[19,     1] loss: 0.290
[20,     1] loss: 0.268
[21,     1] loss: 0.247
[22,     1] loss: 0.230
[23,     1] loss: 0.214
[24,     1] loss: 0.199
[25,     1] loss: 0.186
[26,     1] loss: 0.175
[27,     1] loss: 0.166
[28,     1] loss: 0.155
[29,     1] loss: 0.146
[30,     1] loss: 0.140
[31,     1] loss: 0.137
[32,     1] loss: 0.129
[33,     1] loss: 0.128
[34,     1] loss: 0.123
[35,     1] loss: 0.122
[36,     1] loss: 0.120
[37,     1] loss: 0.118
[38,     1] loss: 0.117
[39,     1] loss: 0.115
[40,     1] loss: 0.116
[41,     1] loss: 0.114
[42,     1] loss: 0.114
[43,     1] loss: 0.114
[44,     1] loss: 0.111
[45,     1] loss: 0.115
[46,     1] loss: 0.114
[47,     1] loss: 0.113
[48,     1] loss: 0.112
[49,     1] loss: 0.112
[50,     1] loss: 0.112
[51,     1] loss: 0.110
[52,     1] loss: 0.109
[53,     1] loss: 0.167
[54,     1] loss: 0.324
[55,     1] loss: 0.929
[56,     1] loss: 0.657
[57,     1] loss: 0.508
[58,     1] loss: 0.508
[59,     1] loss: 0.499
[60,     1] loss: 0.512
[61,     1] loss: 0.521
[62,     1] loss: 0.528
[63,     1] loss: 0.530
[64,     1] loss: 0.523
[65,     1] loss: 0.519
[66,     1] loss: 0.514
[67,     1] loss: 0.509
[68,     1] loss: 0.501
[69,     1] loss: 0.491
[70,     1] loss: 0.484
[71,     1] loss: 0.475
[72,     1] loss: 0.465
[73,     1] loss: 0.456
[74,     1] loss: 0.446
[75,     1] loss: 0.439
[76,     1] loss: 0.428
[77,     1] loss: 0.419
[78,     1] loss: 0.408
[79,     1] loss: 0.399
[80,     1] loss: 0.389
[81,     1] loss: 0.382
[82,     1] loss: 0.369
[83,     1] loss: 0.365
[84,     1] loss: 0.353
[85,     1] loss: 0.345
[86,     1] loss: 0.334
[87,     1] loss: 0.333
[88,     1] loss: 0.337
[89,     1] loss: 0.334
[90,     1] loss: 0.326
[91,     1] loss: 0.317
[92,     1] loss: 0.312
[93,     1] loss: 0.299
[94,     1] loss: 0.285
[95,     1] loss: 0.300
[96,     1] loss: 0.274
[97,     1] loss: 0.272
[98,     1] loss: 0.269
[99,     1] loss: 0.270
[100,     1] loss: 0.243
Early stopping applied (best metric=0.3848630487918854)
Finished Training
Total time taken: 216.0828456878662
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.686
[3,     1] loss: 0.670
[4,     1] loss: 0.656
[5,     1] loss: 0.641
[6,     1] loss: 0.623
[7,     1] loss: 0.607
[8,     1] loss: 0.586
[9,     1] loss: 0.563
[10,     1] loss: 0.539
[11,     1] loss: 0.514
[12,     1] loss: 0.492
[13,     1] loss: 0.466
[14,     1] loss: 0.443
[15,     1] loss: 0.421
[16,     1] loss: 0.395
[17,     1] loss: 0.375
[18,     1] loss: 0.354
[19,     1] loss: 0.334
[20,     1] loss: 0.314
[21,     1] loss: 0.292
[22,     1] loss: 0.275
[23,     1] loss: 0.258
[24,     1] loss: 0.240
[25,     1] loss: 0.225
[26,     1] loss: 0.211
[27,     1] loss: 0.200
[28,     1] loss: 0.188
[29,     1] loss: 0.177
[30,     1] loss: 0.165
[31,     1] loss: 0.158
[32,     1] loss: 0.149
[33,     1] loss: 0.141
[34,     1] loss: 0.136
[35,     1] loss: 0.129
[36,     1] loss: 0.126
[37,     1] loss: 0.123
[38,     1] loss: 0.118
[39,     1] loss: 0.115
[40,     1] loss: 0.114
[41,     1] loss: 0.114
[42,     1] loss: 0.111
[43,     1] loss: 0.109
[44,     1] loss: 0.108
[45,     1] loss: 0.109
[46,     1] loss: 0.106
[47,     1] loss: 0.106
[48,     1] loss: 0.104
[49,     1] loss: 0.105
[50,     1] loss: 0.104
[51,     1] loss: 0.104
[52,     1] loss: 0.103
[53,     1] loss: 0.100
[54,     1] loss: 0.099
[55,     1] loss: 0.099
[56,     1] loss: 0.129
[57,     1] loss: 0.324
[58,     1] loss: 0.613
[59,     1] loss: 0.583
[60,     1] loss: 0.524
[61,     1] loss: 0.489
[62,     1] loss: 0.475
[63,     1] loss: 0.482
[64,     1] loss: 0.487
[65,     1] loss: 0.488
[66,     1] loss: 0.490
[67,     1] loss: 0.489
[68,     1] loss: 0.488
[69,     1] loss: 0.483
[70,     1] loss: 0.480
[71,     1] loss: 0.473
[72,     1] loss: 0.466
[73,     1] loss: 0.463
[74,     1] loss: 0.460
[75,     1] loss: 0.451
[76,     1] loss: 0.443
[77,     1] loss: 0.433
[78,     1] loss: 0.422
[79,     1] loss: 0.411
[80,     1] loss: 0.396
[81,     1] loss: 0.386
[82,     1] loss: 0.370
[83,     1] loss: 0.360
[84,     1] loss: 0.350
[85,     1] loss: 0.344
[86,     1] loss: 0.335
[87,     1] loss: 0.326
[88,     1] loss: 0.314
[89,     1] loss: 0.305
[90,     1] loss: 0.292
[91,     1] loss: 0.280
[92,     1] loss: 0.265
[93,     1] loss: 0.252
[94,     1] loss: 0.244
[95,     1] loss: 0.230
[96,     1] loss: 0.217
[97,     1] loss: 0.211
[98,     1] loss: 0.375
[99,     1] loss: 0.383
[100,     1] loss: 0.361
[101,     1] loss: 0.297
[102,     1] loss: 0.306
[103,     1] loss: 0.546
Early stopping applied (best metric=0.3304806053638458)
Finished Training
Total time taken: 222.67287755012512
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.687
[3,     1] loss: 0.674
[4,     1] loss: 0.660
[5,     1] loss: 0.646
[6,     1] loss: 0.631
[7,     1] loss: 0.614
[8,     1] loss: 0.593
[9,     1] loss: 0.572
[10,     1] loss: 0.551
[11,     1] loss: 0.528
[12,     1] loss: 0.508
[13,     1] loss: 0.487
[14,     1] loss: 0.467
[15,     1] loss: 0.444
[16,     1] loss: 0.424
[17,     1] loss: 0.405
[18,     1] loss: 0.389
[19,     1] loss: 0.365
[20,     1] loss: 0.346
[21,     1] loss: 0.331
[22,     1] loss: 0.311
[23,     1] loss: 0.300
[24,     1] loss: 0.301
[25,     1] loss: 0.264
[26,     1] loss: 0.265
[27,     1] loss: 0.264
[28,     1] loss: 0.247
[29,     1] loss: 0.252
[30,     1] loss: 0.222
[31,     1] loss: 0.216
[32,     1] loss: 0.197
[33,     1] loss: 0.194
[34,     1] loss: 0.185
[35,     1] loss: 0.177
[36,     1] loss: 0.165
[37,     1] loss: 0.156
[38,     1] loss: 0.150
[39,     1] loss: 0.139
[40,     1] loss: 0.131
[41,     1] loss: 0.125
[42,     1] loss: 0.119
[43,     1] loss: 0.115
[44,     1] loss: 0.111
[45,     1] loss: 0.109
[46,     1] loss: 0.105
[47,     1] loss: 0.101
[48,     1] loss: 0.098
[49,     1] loss: 0.098
[50,     1] loss: 0.095
[51,     1] loss: 0.093
[52,     1] loss: 0.092
[53,     1] loss: 0.089
[54,     1] loss: 0.090
[55,     1] loss: 0.088
[56,     1] loss: 0.086
[57,     1] loss: 0.086
[58,     1] loss: 0.082
[59,     1] loss: 0.081
[60,     1] loss: 0.080
[61,     1] loss: 0.181
[62,     1] loss: 0.256
[63,     1] loss: 0.366
[64,     1] loss: 0.339
[65,     1] loss: 0.333
[66,     1] loss: 0.321
[67,     1] loss: 0.313
[68,     1] loss: 0.287
[69,     1] loss: 0.280
[70,     1] loss: 0.276
[71,     1] loss: 0.255
[72,     1] loss: 0.236
[73,     1] loss: 0.214
[74,     1] loss: 0.194
[75,     1] loss: 0.177
[76,     1] loss: 0.175
[77,     1] loss: 0.206
[78,     1] loss: 0.191
[79,     1] loss: 0.172
[80,     1] loss: 0.126
[81,     1] loss: 0.131
[82,     1] loss: 0.120
[83,     1] loss: 0.131
[84,     1] loss: 0.112
[85,     1] loss: 0.119
[86,     1] loss: 0.125
[87,     1] loss: 0.115
[88,     1] loss: 0.106
[89,     1] loss: 0.103
[90,     1] loss: 0.118
[91,     1] loss: 0.105
[92,     1] loss: 0.127
[93,     1] loss: 0.192
[94,     1] loss: 0.130
[95,     1] loss: 0.170
[96,     1] loss: 0.143
[97,     1] loss: 0.190
[98,     1] loss: 0.133
Early stopping applied (best metric=0.40108078718185425)
Finished Training
Total time taken: 212.44776248931885
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.677
[4,     1] loss: 0.664
[5,     1] loss: 0.653
[6,     1] loss: 0.640
[7,     1] loss: 0.627
[8,     1] loss: 0.613
[9,     1] loss: 0.598
[10,     1] loss: 0.581
[11,     1] loss: 0.564
[12,     1] loss: 0.540
[13,     1] loss: 0.521
[14,     1] loss: 0.498
[15,     1] loss: 0.476
[16,     1] loss: 0.451
[17,     1] loss: 0.426
[18,     1] loss: 0.403
[19,     1] loss: 0.377
[20,     1] loss: 0.354
[21,     1] loss: 0.333
[22,     1] loss: 0.312
[23,     1] loss: 0.297
[24,     1] loss: 0.274
[25,     1] loss: 0.254
[26,     1] loss: 0.237
[27,     1] loss: 0.222
[28,     1] loss: 0.207
[29,     1] loss: 0.191
[30,     1] loss: 0.183
[31,     1] loss: 0.171
[32,     1] loss: 0.164
[33,     1] loss: 0.155
[34,     1] loss: 0.147
[35,     1] loss: 0.141
[36,     1] loss: 0.137
[37,     1] loss: 0.134
[38,     1] loss: 0.129
[39,     1] loss: 0.124
[40,     1] loss: 0.122
[41,     1] loss: 0.121
[42,     1] loss: 0.118
[43,     1] loss: 0.113
[44,     1] loss: 0.114
[45,     1] loss: 0.110
[46,     1] loss: 0.106
[47,     1] loss: 0.103
[48,     1] loss: 0.101
[49,     1] loss: 0.162
[50,     1] loss: 0.307
[51,     1] loss: 0.490
[52,     1] loss: 0.447
[53,     1] loss: 0.428
[54,     1] loss: 0.409
[55,     1] loss: 0.404
[56,     1] loss: 0.401
[57,     1] loss: 0.402
[58,     1] loss: 0.407
[59,     1] loss: 0.401
[60,     1] loss: 0.391
[61,     1] loss: 0.380
[62,     1] loss: 0.360
[63,     1] loss: 0.343
[64,     1] loss: 0.333
[65,     1] loss: 0.318
[66,     1] loss: 0.305
[67,     1] loss: 0.291
[68,     1] loss: 0.278
[69,     1] loss: 0.269
[70,     1] loss: 0.263
[71,     1] loss: 0.259
[72,     1] loss: 0.243
[73,     1] loss: 0.235
[74,     1] loss: 0.231
[75,     1] loss: 0.222
[76,     1] loss: 0.214
[77,     1] loss: 0.211
[78,     1] loss: 0.209
[79,     1] loss: 0.200
[80,     1] loss: 0.194
[81,     1] loss: 0.189
[82,     1] loss: 0.185
[83,     1] loss: 0.180
[84,     1] loss: 0.174
[85,     1] loss: 0.167
[86,     1] loss: 0.162
[87,     1] loss: 0.154
[88,     1] loss: 0.151
[89,     1] loss: 0.144
[90,     1] loss: 0.141
[91,     1] loss: 0.135
[92,     1] loss: 0.133
[93,     1] loss: 0.128
[94,     1] loss: 0.125
[95,     1] loss: 0.122
[96,     1] loss: 0.119
[97,     1] loss: 0.115
[98,     1] loss: 0.113
[99,     1] loss: 0.111
[100,     1] loss: 0.110
[101,     1] loss: 0.108
[102,     1] loss: 0.107
[103,     1] loss: 0.106
[104,     1] loss: 0.104
[105,     1] loss: 0.105
[106,     1] loss: 0.103
[107,     1] loss: 0.104
[108,     1] loss: 0.104
[109,     1] loss: 0.103
[110,     1] loss: 0.105
[111,     1] loss: 0.108
[112,     1] loss: 0.205
[113,     1] loss: 0.596
[114,     1] loss: 0.563
[115,     1] loss: 0.666
[116,     1] loss: 0.591
[117,     1] loss: 0.522
[118,     1] loss: 0.502
[119,     1] loss: 0.494
[120,     1] loss: 0.494
[121,     1] loss: 0.483
[122,     1] loss: 0.469
Early stopping applied (best metric=0.36143624782562256)
Finished Training
Total time taken: 265.24467301368713
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.685
[3,     1] loss: 0.660
[4,     1] loss: 0.634
[5,     1] loss: 0.610
[6,     1] loss: 0.585
[7,     1] loss: 0.558
[8,     1] loss: 0.528
[9,     1] loss: 0.499
[10,     1] loss: 0.468
[11,     1] loss: 0.433
[12,     1] loss: 0.397
[13,     1] loss: 0.367
[14,     1] loss: 0.340
[15,     1] loss: 0.307
[16,     1] loss: 0.280
[17,     1] loss: 0.258
[18,     1] loss: 0.234
[19,     1] loss: 0.209
[20,     1] loss: 0.192
[21,     1] loss: 0.173
[22,     1] loss: 0.158
[23,     1] loss: 0.146
[24,     1] loss: 0.133
[25,     1] loss: 0.134
[26,     1] loss: 0.138
[27,     1] loss: 0.134
[28,     1] loss: 0.125
[29,     1] loss: 0.114
[30,     1] loss: 0.114
[31,     1] loss: 0.108
[32,     1] loss: 0.105
[33,     1] loss: 0.103
[34,     1] loss: 0.101
[35,     1] loss: 0.098
[36,     1] loss: 0.097
[37,     1] loss: 0.097
[38,     1] loss: 0.098
[39,     1] loss: 0.097
[40,     1] loss: 0.095
[41,     1] loss: 0.096
[42,     1] loss: 0.097
[43,     1] loss: 0.093
[44,     1] loss: 0.095
[45,     1] loss: 0.093
[46,     1] loss: 0.092
[47,     1] loss: 0.090
[48,     1] loss: 0.087
[49,     1] loss: 0.084
[50,     1] loss: 0.081
[51,     1] loss: 0.074
[52,     1] loss: 0.142
[53,     1] loss: 0.342
[54,     1] loss: 0.594
[55,     1] loss: 0.398
[56,     1] loss: 0.391
[57,     1] loss: 0.368
[58,     1] loss: 0.356
[59,     1] loss: 0.385
[60,     1] loss: 0.379
[61,     1] loss: 0.385
[62,     1] loss: 0.375
[63,     1] loss: 0.360
[64,     1] loss: 0.346
[65,     1] loss: 0.328
[66,     1] loss: 0.310
[67,     1] loss: 0.295
[68,     1] loss: 0.279
[69,     1] loss: 0.261
[70,     1] loss: 0.247
[71,     1] loss: 0.232
[72,     1] loss: 0.220
[73,     1] loss: 0.208
[74,     1] loss: 0.198
[75,     1] loss: 0.186
[76,     1] loss: 0.175
[77,     1] loss: 0.166
[78,     1] loss: 0.159
[79,     1] loss: 0.150
[80,     1] loss: 0.144
[81,     1] loss: 0.138
[82,     1] loss: 0.131
[83,     1] loss: 0.125
[84,     1] loss: 0.120
[85,     1] loss: 0.118
[86,     1] loss: 0.117
[87,     1] loss: 0.115
[88,     1] loss: 0.113
[89,     1] loss: 0.114
[90,     1] loss: 0.111
[91,     1] loss: 0.111
[92,     1] loss: 0.112
[93,     1] loss: 0.112
[94,     1] loss: 0.113
[95,     1] loss: 0.115
[96,     1] loss: 0.125
[97,     1] loss: 0.324
[98,     1] loss: 0.200
[99,     1] loss: 0.492
[100,     1] loss: 0.349
[101,     1] loss: 0.295
[102,     1] loss: 0.279
[103,     1] loss: 0.274
[104,     1] loss: 0.263
[105,     1] loss: 0.250
[106,     1] loss: 0.240
[107,     1] loss: 0.228
[108,     1] loss: 0.215
[109,     1] loss: 0.206
[110,     1] loss: 0.195
[111,     1] loss: 0.185
[112,     1] loss: 0.174
[113,     1] loss: 0.167
[114,     1] loss: 0.161
[115,     1] loss: 0.153
[116,     1] loss: 0.150
[117,     1] loss: 0.150
[118,     1] loss: 0.169
[119,     1] loss: 0.152
[120,     1] loss: 0.158
[121,     1] loss: 0.148
[122,     1] loss: 0.146
[123,     1] loss: 0.142
[124,     1] loss: 0.137
[125,     1] loss: 0.133
[126,     1] loss: 0.129
[127,     1] loss: 0.128
[128,     1] loss: 0.127
[129,     1] loss: 0.128
[130,     1] loss: 0.123
[131,     1] loss: 0.124
[132,     1] loss: 0.126
[133,     1] loss: 0.118
[134,     1] loss: 0.119
[135,     1] loss: 0.120
[136,     1] loss: 0.118
[137,     1] loss: 0.117
[138,     1] loss: 0.115
[139,     1] loss: 0.117
[140,     1] loss: 0.134
[141,     1] loss: 0.324
[142,     1] loss: 0.282
[143,     1] loss: 0.488
[144,     1] loss: 0.318
[145,     1] loss: 0.362
[146,     1] loss: 0.299
[147,     1] loss: 0.286
[148,     1] loss: 0.267
[149,     1] loss: 0.245
[150,     1] loss: 0.231
[151,     1] loss: 0.210
[152,     1] loss: 0.197
[153,     1] loss: 0.185
[154,     1] loss: 0.179
[155,     1] loss: 0.170
[156,     1] loss: 0.161
[157,     1] loss: 0.155
[158,     1] loss: 0.147
[159,     1] loss: 0.142
[160,     1] loss: 0.139
[161,     1] loss: 0.136
[162,     1] loss: 0.130
[163,     1] loss: 0.127
[164,     1] loss: 0.128
[165,     1] loss: 0.128
[166,     1] loss: 0.125
[167,     1] loss: 0.123
[168,     1] loss: 0.124
[169,     1] loss: 0.122
[170,     1] loss: 0.120
[171,     1] loss: 0.121
[172,     1] loss: 0.120
[173,     1] loss: 0.120
[174,     1] loss: 0.120
[175,     1] loss: 0.116
[176,     1] loss: 0.123
[177,     1] loss: 0.131
[178,     1] loss: 0.190
[179,     1] loss: 0.166
[180,     1] loss: 0.191
[181,     1] loss: 0.236
[182,     1] loss: 0.298
[183,     1] loss: 0.272
[184,     1] loss: 0.224
[185,     1] loss: 0.221
[186,     1] loss: 0.220
[187,     1] loss: 0.197
[188,     1] loss: 0.186
[189,     1] loss: 0.179
[190,     1] loss: 0.174
[191,     1] loss: 0.168
[192,     1] loss: 0.163
[193,     1] loss: 0.160
[194,     1] loss: 0.155
[195,     1] loss: 0.148
[196,     1] loss: 0.147
Early stopping applied (best metric=0.3972404897212982)
Finished Training
Total time taken: 426.5262989997864
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.653
[5,     1] loss: 0.638
[6,     1] loss: 0.625
[7,     1] loss: 0.606
[8,     1] loss: 0.590
[9,     1] loss: 0.569
[10,     1] loss: 0.548
[11,     1] loss: 0.524
[12,     1] loss: 0.499
[13,     1] loss: 0.470
[14,     1] loss: 0.439
[15,     1] loss: 0.406
[16,     1] loss: 0.370
[17,     1] loss: 0.342
[18,     1] loss: 0.316
[19,     1] loss: 0.289
[20,     1] loss: 0.261
[21,     1] loss: 0.237
[22,     1] loss: 0.215
[23,     1] loss: 0.199
[24,     1] loss: 0.181
[25,     1] loss: 0.173
[26,     1] loss: 0.156
[27,     1] loss: 0.149
[28,     1] loss: 0.138
[29,     1] loss: 0.130
[30,     1] loss: 0.123
[31,     1] loss: 0.122
[32,     1] loss: 0.113
[33,     1] loss: 0.111
[34,     1] loss: 0.107
[35,     1] loss: 0.103
[36,     1] loss: 0.103
[37,     1] loss: 0.097
[38,     1] loss: 0.097
[39,     1] loss: 0.097
[40,     1] loss: 0.096
[41,     1] loss: 0.095
[42,     1] loss: 0.095
[43,     1] loss: 0.095
[44,     1] loss: 0.094
[45,     1] loss: 0.095
[46,     1] loss: 0.092
[47,     1] loss: 0.091
[48,     1] loss: 0.088
[49,     1] loss: 0.086
[50,     1] loss: 0.081
[51,     1] loss: 0.078
[52,     1] loss: 0.077
[53,     1] loss: 0.072
[54,     1] loss: 0.070
[55,     1] loss: 0.068
[56,     1] loss: 0.068
[57,     1] loss: 0.081
[58,     1] loss: 0.384
[59,     1] loss: 0.606
[60,     1] loss: 0.398
[61,     1] loss: 0.390
[62,     1] loss: 0.384
[63,     1] loss: 0.509
[64,     1] loss: 0.424
[65,     1] loss: 0.426
[66,     1] loss: 0.394
[67,     1] loss: 0.405
[68,     1] loss: 0.386
[69,     1] loss: 0.385
[70,     1] loss: 0.379
[71,     1] loss: 0.362
[72,     1] loss: 0.350
[73,     1] loss: 0.336
[74,     1] loss: 0.324
[75,     1] loss: 0.307
[76,     1] loss: 0.288
[77,     1] loss: 0.275
[78,     1] loss: 0.251
[79,     1] loss: 0.232
[80,     1] loss: 0.216
[81,     1] loss: 0.197
[82,     1] loss: 0.175
[83,     1] loss: 0.159
[84,     1] loss: 0.144
[85,     1] loss: 0.132
[86,     1] loss: 0.121
[87,     1] loss: 0.112
[88,     1] loss: 0.106
[89,     1] loss: 0.099
[90,     1] loss: 0.096
[91,     1] loss: 0.091
[92,     1] loss: 0.089
[93,     1] loss: 0.087
[94,     1] loss: 0.087
[95,     1] loss: 0.087
[96,     1] loss: 0.086
[97,     1] loss: 0.085
[98,     1] loss: 0.088
[99,     1] loss: 0.088
[100,     1] loss: 0.090
[101,     1] loss: 0.092
[102,     1] loss: 0.090
[103,     1] loss: 0.090
[104,     1] loss: 0.093
[105,     1] loss: 0.093
[106,     1] loss: 0.095
[107,     1] loss: 0.095
[108,     1] loss: 0.093
[109,     1] loss: 0.098
[110,     1] loss: 0.093
[111,     1] loss: 0.151
[112,     1] loss: 0.363
[113,     1] loss: 0.596
[114,     1] loss: 0.879
[115,     1] loss: 0.518
[116,     1] loss: 0.589
[117,     1] loss: 0.614
[118,     1] loss: 0.624
[119,     1] loss: 0.633
[120,     1] loss: 0.641
[121,     1] loss: 0.647
[122,     1] loss: 0.652
[123,     1] loss: 0.656
[124,     1] loss: 0.661
[125,     1] loss: 0.664
[126,     1] loss: 0.667
[127,     1] loss: 0.669
Early stopping applied (best metric=0.35482606291770935)
Finished Training
Total time taken: 278.1556215286255
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.676
[3,     1] loss: 0.650
[4,     1] loss: 0.629
[5,     1] loss: 0.606
[6,     1] loss: 0.584
[7,     1] loss: 0.556
[8,     1] loss: 0.531
[9,     1] loss: 0.507
[10,     1] loss: 0.484
[11,     1] loss: 0.464
[12,     1] loss: 0.439
[13,     1] loss: 0.417
[14,     1] loss: 0.397
[15,     1] loss: 0.376
[16,     1] loss: 0.363
[17,     1] loss: 0.390
[18,     1] loss: 0.355
[19,     1] loss: 0.353
[20,     1] loss: 0.321
[21,     1] loss: 0.320
[22,     1] loss: 0.307
[23,     1] loss: 0.292
[24,     1] loss: 0.285
[25,     1] loss: 0.272
[26,     1] loss: 0.267
[27,     1] loss: 0.259
[28,     1] loss: 0.248
[29,     1] loss: 0.241
[30,     1] loss: 0.229
[31,     1] loss: 0.223
[32,     1] loss: 0.217
[33,     1] loss: 0.214
[34,     1] loss: 0.205
[35,     1] loss: 0.201
[36,     1] loss: 0.195
[37,     1] loss: 0.190
[38,     1] loss: 0.183
[39,     1] loss: 0.179
[40,     1] loss: 0.173
[41,     1] loss: 0.171
[42,     1] loss: 0.166
[43,     1] loss: 0.162
[44,     1] loss: 0.157
[45,     1] loss: 0.151
[46,     1] loss: 0.148
[47,     1] loss: 0.143
[48,     1] loss: 0.139
[49,     1] loss: 0.134
[50,     1] loss: 0.132
[51,     1] loss: 0.127
[52,     1] loss: 0.122
[53,     1] loss: 0.114
[54,     1] loss: 0.129
[55,     1] loss: 0.344
[56,     1] loss: 0.605
[57,     1] loss: 0.544
[58,     1] loss: 0.476
[59,     1] loss: 0.450
[60,     1] loss: 0.432
[61,     1] loss: 0.409
[62,     1] loss: 0.401
[63,     1] loss: 0.400
[64,     1] loss: 0.389
[65,     1] loss: 0.379
[66,     1] loss: 0.368
[67,     1] loss: 0.350
[68,     1] loss: 0.332
[69,     1] loss: 0.316
[70,     1] loss: 0.299
[71,     1] loss: 0.284
[72,     1] loss: 0.276
[73,     1] loss: 0.271
[74,     1] loss: 0.262
[75,     1] loss: 0.265
[76,     1] loss: 0.275
[77,     1] loss: 0.313
[78,     1] loss: 0.310
[79,     1] loss: 0.320
[80,     1] loss: 0.292
[81,     1] loss: 0.278
[82,     1] loss: 0.289
[83,     1] loss: 0.273
[84,     1] loss: 0.264
[85,     1] loss: 0.256
[86,     1] loss: 0.250
[87,     1] loss: 0.248
[88,     1] loss: 0.239
[89,     1] loss: 0.231
[90,     1] loss: 0.221
[91,     1] loss: 0.217
[92,     1] loss: 0.209
Early stopping applied (best metric=0.3863215446472168)
Finished Training
Total time taken: 202.7787630558014
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.658
[4,     1] loss: 0.640
[5,     1] loss: 0.621
[6,     1] loss: 0.599
[7,     1] loss: 0.579
[8,     1] loss: 0.555
[9,     1] loss: 0.534
[10,     1] loss: 0.506
[11,     1] loss: 0.481
[12,     1] loss: 0.456
[13,     1] loss: 0.434
[14,     1] loss: 0.415
[15,     1] loss: 0.393
[16,     1] loss: 0.379
[17,     1] loss: 0.363
[18,     1] loss: 0.351
[19,     1] loss: 0.340
[20,     1] loss: 0.330
[21,     1] loss: 0.319
[22,     1] loss: 0.308
[23,     1] loss: 0.296
[24,     1] loss: 0.285
[25,     1] loss: 0.273
[26,     1] loss: 0.262
[27,     1] loss: 0.252
[28,     1] loss: 0.239
[29,     1] loss: 0.230
[30,     1] loss: 0.220
[31,     1] loss: 0.212
[32,     1] loss: 0.202
[33,     1] loss: 0.194
[34,     1] loss: 0.188
[35,     1] loss: 0.181
[36,     1] loss: 0.174
[37,     1] loss: 0.168
[38,     1] loss: 0.163
[39,     1] loss: 0.158
[40,     1] loss: 0.153
[41,     1] loss: 0.150
[42,     1] loss: 0.146
[43,     1] loss: 0.145
[44,     1] loss: 0.139
[45,     1] loss: 0.139
[46,     1] loss: 0.134
[47,     1] loss: 0.134
[48,     1] loss: 0.129
[49,     1] loss: 0.122
[50,     1] loss: 0.149
[51,     1] loss: 0.324
[52,     1] loss: 0.830
[53,     1] loss: 0.463
[54,     1] loss: 0.492
[55,     1] loss: 0.458
[56,     1] loss: 0.424
[57,     1] loss: 0.437
[58,     1] loss: 0.451
[59,     1] loss: 0.449
[60,     1] loss: 0.447
[61,     1] loss: 0.442
[62,     1] loss: 0.436
[63,     1] loss: 0.418
[64,     1] loss: 0.403
[65,     1] loss: 0.391
[66,     1] loss: 0.375
[67,     1] loss: 0.360
[68,     1] loss: 0.344
[69,     1] loss: 0.333
[70,     1] loss: 0.324
[71,     1] loss: 0.314
[72,     1] loss: 0.296
[73,     1] loss: 0.285
[74,     1] loss: 0.279
[75,     1] loss: 0.273
[76,     1] loss: 0.260
[77,     1] loss: 0.251
[78,     1] loss: 0.246
[79,     1] loss: 0.241
[80,     1] loss: 0.230
[81,     1] loss: 0.219
[82,     1] loss: 0.204
[83,     1] loss: 0.200
[84,     1] loss: 0.195
[85,     1] loss: 0.186
[86,     1] loss: 0.179
[87,     1] loss: 0.170
[88,     1] loss: 0.163
[89,     1] loss: 0.156
[90,     1] loss: 0.155
[91,     1] loss: 0.149
[92,     1] loss: 0.144
[93,     1] loss: 0.140
[94,     1] loss: 0.134
[95,     1] loss: 0.131
[96,     1] loss: 0.127
[97,     1] loss: 0.128
[98,     1] loss: 0.124
[99,     1] loss: 0.122
[100,     1] loss: 0.119
[101,     1] loss: 0.119
[102,     1] loss: 0.118
[103,     1] loss: 0.116
[104,     1] loss: 0.115
[105,     1] loss: 0.116
[106,     1] loss: 0.117
[107,     1] loss: 0.115
[108,     1] loss: 0.121
[109,     1] loss: 0.138
[110,     1] loss: 0.195
[111,     1] loss: 0.285
[112,     1] loss: 0.789
[113,     1] loss: 0.605
[114,     1] loss: 0.546
[115,     1] loss: 0.523
[116,     1] loss: 0.512
[117,     1] loss: 0.513
[118,     1] loss: 0.505
[119,     1] loss: 0.496
[120,     1] loss: 0.502
[121,     1] loss: 0.508
[122,     1] loss: 0.510
[123,     1] loss: 0.508
[124,     1] loss: 0.504
[125,     1] loss: 0.500
[126,     1] loss: 0.494
[127,     1] loss: 0.488
[128,     1] loss: 0.481
Early stopping applied (best metric=0.35489988327026367)
Finished Training
Total time taken: 281.8691146373749
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.663
[4,     1] loss: 0.647
[5,     1] loss: 0.630
[6,     1] loss: 0.608
[7,     1] loss: 0.584
[8,     1] loss: 0.559
[9,     1] loss: 0.525
[10,     1] loss: 0.496
[11,     1] loss: 0.466
[12,     1] loss: 0.439
[13,     1] loss: 0.416
[14,     1] loss: 0.397
[15,     1] loss: 0.385
[16,     1] loss: 0.373
[17,     1] loss: 0.362
[18,     1] loss: 0.353
[19,     1] loss: 0.344
[20,     1] loss: 0.336
[21,     1] loss: 0.326
[22,     1] loss: 0.316
[23,     1] loss: 0.305
[24,     1] loss: 0.292
[25,     1] loss: 0.281
[26,     1] loss: 0.271
[27,     1] loss: 0.260
[28,     1] loss: 0.248
[29,     1] loss: 0.239
[30,     1] loss: 0.226
[31,     1] loss: 0.247
[32,     1] loss: 0.214
[33,     1] loss: 0.226
[34,     1] loss: 0.245
[35,     1] loss: 0.214
[36,     1] loss: 0.208
[37,     1] loss: 0.202
[38,     1] loss: 0.204
[39,     1] loss: 0.190
[40,     1] loss: 0.188
[41,     1] loss: 0.180
[42,     1] loss: 0.172
[43,     1] loss: 0.168
[44,     1] loss: 0.161
[45,     1] loss: 0.156
[46,     1] loss: 0.153
[47,     1] loss: 0.148
[48,     1] loss: 0.145
[49,     1] loss: 0.141
[50,     1] loss: 0.138
[51,     1] loss: 0.139
[52,     1] loss: 0.137
[53,     1] loss: 0.133
[54,     1] loss: 0.133
[55,     1] loss: 0.132
[56,     1] loss: 0.131
[57,     1] loss: 0.131
[58,     1] loss: 0.130
[59,     1] loss: 0.127
[60,     1] loss: 0.127
[61,     1] loss: 0.145
[62,     1] loss: 0.135
[63,     1] loss: 0.139
[64,     1] loss: 0.130
[65,     1] loss: 0.119
[66,     1] loss: 0.121
[67,     1] loss: 0.170
[68,     1] loss: 0.563
[69,     1] loss: 0.446
[70,     1] loss: 0.306
[71,     1] loss: 0.283
[72,     1] loss: 0.287
[73,     1] loss: 0.254
[74,     1] loss: 0.226
[75,     1] loss: 0.218
[76,     1] loss: 0.213
[77,     1] loss: 0.213
[78,     1] loss: 0.205
[79,     1] loss: 0.194
[80,     1] loss: 0.184
[81,     1] loss: 0.179
[82,     1] loss: 0.170
[83,     1] loss: 0.161
[84,     1] loss: 0.157
[85,     1] loss: 0.149
[86,     1] loss: 0.146
[87,     1] loss: 0.136
[88,     1] loss: 0.132
[89,     1] loss: 0.127
[90,     1] loss: 0.122
[91,     1] loss: 0.117
[92,     1] loss: 0.115
[93,     1] loss: 0.113
[94,     1] loss: 0.107
[95,     1] loss: 0.105
[96,     1] loss: 0.102
[97,     1] loss: 0.098
[98,     1] loss: 0.098
[99,     1] loss: 0.099
[100,     1] loss: 0.099
[101,     1] loss: 0.096
[102,     1] loss: 0.095
[103,     1] loss: 0.093
[104,     1] loss: 0.091
[105,     1] loss: 0.093
[106,     1] loss: 0.091
[107,     1] loss: 0.091
[108,     1] loss: 0.090
[109,     1] loss: 0.091
[110,     1] loss: 0.092
[111,     1] loss: 0.092
[112,     1] loss: 0.089
[113,     1] loss: 0.089
[114,     1] loss: 0.093
[115,     1] loss: 0.091
[116,     1] loss: 0.092
[117,     1] loss: 0.094
[118,     1] loss: 0.093
[119,     1] loss: 0.092
[120,     1] loss: 0.095
[121,     1] loss: 0.143
[122,     1] loss: 0.466
[123,     1] loss: 1.059
[124,     1] loss: 0.646
[125,     1] loss: 0.604
[126,     1] loss: 0.667
[127,     1] loss: 0.649
[128,     1] loss: 0.631
[129,     1] loss: 0.632
[130,     1] loss: 0.641
[131,     1] loss: 0.649
[132,     1] loss: 0.651
[133,     1] loss: 0.656
[134,     1] loss: 0.659
[135,     1] loss: 0.660
[136,     1] loss: 0.662
[137,     1] loss: 0.663
[138,     1] loss: 0.665
[139,     1] loss: 0.667
[140,     1] loss: 0.668
[141,     1] loss: 0.669
[142,     1] loss: 0.672
[143,     1] loss: 0.673
[144,     1] loss: 0.674
[145,     1] loss: 0.676
[146,     1] loss: 0.677
[147,     1] loss: 0.679
[148,     1] loss: 0.680
[149,     1] loss: 0.681
Early stopping applied (best metric=0.309672087430954)
Finished Training
Total time taken: 329.2703580856323
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.653
[4,     1] loss: 0.631
[5,     1] loss: 0.610
[6,     1] loss: 0.589
[7,     1] loss: 0.567
[8,     1] loss: 0.549
[9,     1] loss: 0.522
[10,     1] loss: 0.501
[11,     1] loss: 0.474
[12,     1] loss: 0.447
[13,     1] loss: 0.418
[14,     1] loss: 0.390
[15,     1] loss: 0.360
[16,     1] loss: 0.335
[17,     1] loss: 0.305
[18,     1] loss: 0.280
[19,     1] loss: 0.255
[20,     1] loss: 0.235
[21,     1] loss: 0.234
[22,     1] loss: 0.222
[23,     1] loss: 0.207
[24,     1] loss: 0.188
[25,     1] loss: 0.184
[26,     1] loss: 0.169
[27,     1] loss: 0.163
[28,     1] loss: 0.158
[29,     1] loss: 0.152
[30,     1] loss: 0.147
[31,     1] loss: 0.144
[32,     1] loss: 0.141
[33,     1] loss: 0.137
[34,     1] loss: 0.134
[35,     1] loss: 0.133
[36,     1] loss: 0.132
[37,     1] loss: 0.133
[38,     1] loss: 0.131
[39,     1] loss: 0.129
[40,     1] loss: 0.131
[41,     1] loss: 0.128
[42,     1] loss: 0.131
[43,     1] loss: 0.129
[44,     1] loss: 0.128
[45,     1] loss: 0.128
[46,     1] loss: 0.128
[47,     1] loss: 0.129
[48,     1] loss: 0.126
[49,     1] loss: 0.125
[50,     1] loss: 0.124
[51,     1] loss: 0.121
[52,     1] loss: 0.126
[53,     1] loss: 0.289
[54,     1] loss: 0.217
[55,     1] loss: 0.725
[56,     1] loss: 0.409
[57,     1] loss: 0.431
[58,     1] loss: 0.433
[59,     1] loss: 0.431
[60,     1] loss: 0.435
[61,     1] loss: 0.441
[62,     1] loss: 0.441
[63,     1] loss: 0.439
Early stopping applied (best metric=0.38431230187416077)
Finished Training
Total time taken: 140.61379766464233
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.655
[4,     1] loss: 0.635
[5,     1] loss: 0.617
[6,     1] loss: 0.598
[7,     1] loss: 0.579
[8,     1] loss: 0.559
[9,     1] loss: 0.536
[10,     1] loss: 0.510
[11,     1] loss: 0.487
[12,     1] loss: 0.465
[13,     1] loss: 0.434
[14,     1] loss: 0.409
[15,     1] loss: 0.387
[16,     1] loss: 0.360
[17,     1] loss: 0.335
[18,     1] loss: 0.311
[19,     1] loss: 0.286
[20,     1] loss: 0.270
[21,     1] loss: 0.251
[22,     1] loss: 0.239
[23,     1] loss: 0.229
[24,     1] loss: 0.222
[25,     1] loss: 0.211
[26,     1] loss: 0.186
[27,     1] loss: 0.176
[28,     1] loss: 0.164
[29,     1] loss: 0.158
[30,     1] loss: 0.150
[31,     1] loss: 0.142
[32,     1] loss: 0.135
[33,     1] loss: 0.128
[34,     1] loss: 0.123
[35,     1] loss: 0.119
[36,     1] loss: 0.117
[37,     1] loss: 0.112
[38,     1] loss: 0.111
[39,     1] loss: 0.108
[40,     1] loss: 0.104
[41,     1] loss: 0.104
[42,     1] loss: 0.102
[43,     1] loss: 0.103
[44,     1] loss: 0.101
[45,     1] loss: 0.100
[46,     1] loss: 0.099
[47,     1] loss: 0.097
[48,     1] loss: 0.096
[49,     1] loss: 0.093
[50,     1] loss: 0.090
[51,     1] loss: 0.086
[52,     1] loss: 0.085
[53,     1] loss: 0.081
[54,     1] loss: 0.080
[55,     1] loss: 0.086
[56,     1] loss: 0.287
[57,     1] loss: 0.406
[58,     1] loss: 0.505
[59,     1] loss: 0.466
[60,     1] loss: 0.381
[61,     1] loss: 0.344
[62,     1] loss: 0.319
[63,     1] loss: 0.312
[64,     1] loss: 0.319
[65,     1] loss: 0.308
[66,     1] loss: 0.301
[67,     1] loss: 0.298
[68,     1] loss: 0.269
[69,     1] loss: 0.262
Early stopping applied (best metric=0.39836859703063965)
Finished Training
Total time taken: 154.16225504875183
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.714
[2,     1] loss: 0.696
[3,     1] loss: 0.677
[4,     1] loss: 0.661
[5,     1] loss: 0.641
[6,     1] loss: 0.623
[7,     1] loss: 0.602
[8,     1] loss: 0.584
[9,     1] loss: 0.561
[10,     1] loss: 0.540
[11,     1] loss: 0.515
[12,     1] loss: 0.492
[13,     1] loss: 0.464
[14,     1] loss: 0.438
[15,     1] loss: 0.410
[16,     1] loss: 0.381
[17,     1] loss: 0.351
[18,     1] loss: 0.322
[19,     1] loss: 0.299
[20,     1] loss: 0.286
[21,     1] loss: 0.282
[22,     1] loss: 0.243
[23,     1] loss: 0.247
[24,     1] loss: 0.217
[25,     1] loss: 0.248
[26,     1] loss: 0.196
[27,     1] loss: 0.253
[28,     1] loss: 0.187
[29,     1] loss: 0.202
[30,     1] loss: 0.199
[31,     1] loss: 0.196
[32,     1] loss: 0.183
[33,     1] loss: 0.174
[34,     1] loss: 0.171
[35,     1] loss: 0.164
[36,     1] loss: 0.154
[37,     1] loss: 0.146
[38,     1] loss: 0.141
[39,     1] loss: 0.132
[40,     1] loss: 0.129
[41,     1] loss: 0.125
[42,     1] loss: 0.121
[43,     1] loss: 0.118
[44,     1] loss: 0.114
[45,     1] loss: 0.111
[46,     1] loss: 0.111
[47,     1] loss: 0.109
[48,     1] loss: 0.107
[49,     1] loss: 0.104
[50,     1] loss: 0.102
[51,     1] loss: 0.100
[52,     1] loss: 0.098
[53,     1] loss: 0.098
[54,     1] loss: 0.095
[55,     1] loss: 0.093
[56,     1] loss: 0.090
[57,     1] loss: 0.090
[58,     1] loss: 0.106
[59,     1] loss: 0.183
[60,     1] loss: 0.186
[61,     1] loss: 0.555
[62,     1] loss: 0.337
[63,     1] loss: 0.410
[64,     1] loss: 0.360
[65,     1] loss: 0.349
[66,     1] loss: 0.356
[67,     1] loss: 0.344
[68,     1] loss: 0.329
[69,     1] loss: 0.326
[70,     1] loss: 0.310
[71,     1] loss: 0.295
[72,     1] loss: 0.284
[73,     1] loss: 0.272
[74,     1] loss: 0.258
[75,     1] loss: 0.246
[76,     1] loss: 0.238
[77,     1] loss: 0.224
[78,     1] loss: 0.214
[79,     1] loss: 0.202
[80,     1] loss: 0.194
[81,     1] loss: 0.184
[82,     1] loss: 0.173
[83,     1] loss: 0.166
[84,     1] loss: 0.158
[85,     1] loss: 0.155
[86,     1] loss: 0.148
[87,     1] loss: 0.139
[88,     1] loss: 0.137
[89,     1] loss: 0.137
[90,     1] loss: 0.130
[91,     1] loss: 0.127
[92,     1] loss: 0.126
[93,     1] loss: 0.123
[94,     1] loss: 0.122
[95,     1] loss: 0.121
[96,     1] loss: 0.121
[97,     1] loss: 0.117
[98,     1] loss: 0.118
[99,     1] loss: 0.116
[100,     1] loss: 0.118
[101,     1] loss: 0.117
[102,     1] loss: 0.119
[103,     1] loss: 0.120
[104,     1] loss: 0.188
[105,     1] loss: 0.313
Early stopping applied (best metric=0.3079838454723358)
Finished Training
Total time taken: 234.02508854866028
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.698
[3,     1] loss: 0.681
[4,     1] loss: 0.664
[5,     1] loss: 0.650
[6,     1] loss: 0.633
[7,     1] loss: 0.615
[8,     1] loss: 0.596
[9,     1] loss: 0.578
[10,     1] loss: 0.557
[11,     1] loss: 0.532
[12,     1] loss: 0.506
[13,     1] loss: 0.480
[14,     1] loss: 0.453
[15,     1] loss: 0.424
[16,     1] loss: 0.401
[17,     1] loss: 0.373
[18,     1] loss: 0.352
[19,     1] loss: 0.329
[20,     1] loss: 0.307
[21,     1] loss: 0.286
[22,     1] loss: 0.267
[23,     1] loss: 0.247
[24,     1] loss: 0.233
[25,     1] loss: 0.220
[26,     1] loss: 0.204
[27,     1] loss: 0.188
[28,     1] loss: 0.176
[29,     1] loss: 0.168
[30,     1] loss: 0.158
[31,     1] loss: 0.149
[32,     1] loss: 0.143
[33,     1] loss: 0.137
[34,     1] loss: 0.132
[35,     1] loss: 0.129
[36,     1] loss: 0.124
[37,     1] loss: 0.120
[38,     1] loss: 0.119
[39,     1] loss: 0.116
[40,     1] loss: 0.114
[41,     1] loss: 0.112
[42,     1] loss: 0.111
[43,     1] loss: 0.108
[44,     1] loss: 0.109
[45,     1] loss: 0.105
[46,     1] loss: 0.102
[47,     1] loss: 0.103
[48,     1] loss: 0.110
[49,     1] loss: 0.253
[50,     1] loss: 0.534
[51,     1] loss: 0.327
[52,     1] loss: 0.507
[53,     1] loss: 0.345
[54,     1] loss: 0.400
[55,     1] loss: 0.401
[56,     1] loss: 0.389
[57,     1] loss: 0.396
[58,     1] loss: 0.382
[59,     1] loss: 0.377
[60,     1] loss: 0.374
[61,     1] loss: 0.359
[62,     1] loss: 0.341
[63,     1] loss: 0.330
[64,     1] loss: 0.312
[65,     1] loss: 0.300
[66,     1] loss: 0.285
[67,     1] loss: 0.271
[68,     1] loss: 0.256
[69,     1] loss: 0.243
[70,     1] loss: 0.228
[71,     1] loss: 0.212
[72,     1] loss: 0.198
[73,     1] loss: 0.183
[74,     1] loss: 0.171
[75,     1] loss: 0.160
[76,     1] loss: 0.172
[77,     1] loss: 0.184
[78,     1] loss: 0.186
[79,     1] loss: 0.184
[80,     1] loss: 0.178
[81,     1] loss: 0.172
[82,     1] loss: 0.157
[83,     1] loss: 0.142
[84,     1] loss: 0.150
[85,     1] loss: 0.153
[86,     1] loss: 0.133
[87,     1] loss: 0.135
[88,     1] loss: 0.131
[89,     1] loss: 0.120
[90,     1] loss: 0.118
[91,     1] loss: 0.113
[92,     1] loss: 0.113
[93,     1] loss: 0.110
[94,     1] loss: 0.107
[95,     1] loss: 0.107
[96,     1] loss: 0.106
[97,     1] loss: 0.109
[98,     1] loss: 0.106
[99,     1] loss: 0.111
[100,     1] loss: 0.103
[101,     1] loss: 0.105
[102,     1] loss: 0.106
[103,     1] loss: 0.108
[104,     1] loss: 0.106
[105,     1] loss: 0.105
[106,     1] loss: 0.108
[107,     1] loss: 0.107
[108,     1] loss: 0.110
[109,     1] loss: 0.108
[110,     1] loss: 0.108
[111,     1] loss: 0.111
[112,     1] loss: 0.111
[113,     1] loss: 0.147
[114,     1] loss: 0.371
[115,     1] loss: 0.842
Early stopping applied (best metric=0.4532361626625061)
Finished Training
Total time taken: 256.87419867515564
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.647
[5,     1] loss: 0.630
[6,     1] loss: 0.610
[7,     1] loss: 0.589
[8,     1] loss: 0.563
[9,     1] loss: 0.537
[10,     1] loss: 0.507
[11,     1] loss: 0.475
[12,     1] loss: 0.449
[13,     1] loss: 0.419
[14,     1] loss: 0.392
[15,     1] loss: 0.368
[16,     1] loss: 0.349
[17,     1] loss: 0.331
[18,     1] loss: 0.315
[19,     1] loss: 0.297
[20,     1] loss: 0.278
[21,     1] loss: 0.261
[22,     1] loss: 0.248
[23,     1] loss: 0.233
[24,     1] loss: 0.228
[25,     1] loss: 0.214
[26,     1] loss: 0.201
[27,     1] loss: 0.190
[28,     1] loss: 0.182
[29,     1] loss: 0.172
[30,     1] loss: 0.167
[31,     1] loss: 0.162
[32,     1] loss: 0.153
[33,     1] loss: 0.148
[34,     1] loss: 0.142
[35,     1] loss: 0.137
[36,     1] loss: 0.135
[37,     1] loss: 0.132
[38,     1] loss: 0.129
[39,     1] loss: 0.125
[40,     1] loss: 0.124
[41,     1] loss: 0.121
[42,     1] loss: 0.120
[43,     1] loss: 0.116
[44,     1] loss: 0.115
[45,     1] loss: 0.113
[46,     1] loss: 0.109
[47,     1] loss: 0.105
[48,     1] loss: 0.098
[49,     1] loss: 0.095
[50,     1] loss: 0.121
[51,     1] loss: 0.394
[52,     1] loss: 0.650
[53,     1] loss: 0.552
[54,     1] loss: 0.513
[55,     1] loss: 0.497
[56,     1] loss: 0.492
[57,     1] loss: 0.503
[58,     1] loss: 0.513
[59,     1] loss: 0.521
[60,     1] loss: 0.526
[61,     1] loss: 0.529
[62,     1] loss: 0.534
[63,     1] loss: 0.532
[64,     1] loss: 0.534
[65,     1] loss: 0.529
[66,     1] loss: 0.528
[67,     1] loss: 0.523
[68,     1] loss: 0.519
[69,     1] loss: 0.515
[70,     1] loss: 0.508
[71,     1] loss: 0.502
[72,     1] loss: 0.494
[73,     1] loss: 0.486
[74,     1] loss: 0.477
[75,     1] loss: 0.463
[76,     1] loss: 0.453
[77,     1] loss: 0.436
[78,     1] loss: 0.424
[79,     1] loss: 0.410
[80,     1] loss: 0.390
[81,     1] loss: 0.370
[82,     1] loss: 0.352
[83,     1] loss: 0.336
[84,     1] loss: 0.313
[85,     1] loss: 0.293
[86,     1] loss: 0.273
[87,     1] loss: 0.250
[88,     1] loss: 0.230
[89,     1] loss: 0.209
[90,     1] loss: 0.190
[91,     1] loss: 0.171
[92,     1] loss: 0.155
[93,     1] loss: 0.143
[94,     1] loss: 0.130
[95,     1] loss: 0.119
[96,     1] loss: 0.113
[97,     1] loss: 0.105
Early stopping applied (best metric=0.3204648196697235)
Finished Training
Total time taken: 217.80981159210205
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.701
[3,     1] loss: 0.684
[4,     1] loss: 0.667
[5,     1] loss: 0.648
[6,     1] loss: 0.629
[7,     1] loss: 0.608
[8,     1] loss: 0.588
[9,     1] loss: 0.563
[10,     1] loss: 0.541
[11,     1] loss: 0.514
[12,     1] loss: 0.486
[13,     1] loss: 0.460
[14,     1] loss: 0.430
[15,     1] loss: 0.404
[16,     1] loss: 0.376
[17,     1] loss: 0.354
[18,     1] loss: 0.329
[19,     1] loss: 0.304
[20,     1] loss: 0.284
[21,     1] loss: 0.268
[22,     1] loss: 0.250
[23,     1] loss: 0.234
[24,     1] loss: 0.220
[25,     1] loss: 0.205
[26,     1] loss: 0.191
[27,     1] loss: 0.181
[28,     1] loss: 0.170
[29,     1] loss: 0.162
[30,     1] loss: 0.156
[31,     1] loss: 0.152
[32,     1] loss: 0.145
[33,     1] loss: 0.141
[34,     1] loss: 0.136
[35,     1] loss: 0.137
[36,     1] loss: 0.135
[37,     1] loss: 0.130
[38,     1] loss: 0.128
[39,     1] loss: 0.129
[40,     1] loss: 0.127
[41,     1] loss: 0.125
[42,     1] loss: 0.125
[43,     1] loss: 0.125
[44,     1] loss: 0.123
[45,     1] loss: 0.123
[46,     1] loss: 0.122
[47,     1] loss: 0.122
[48,     1] loss: 0.118
[49,     1] loss: 0.117
[50,     1] loss: 0.182
[51,     1] loss: 0.668
[52,     1] loss: 0.353
[53,     1] loss: 0.347
[54,     1] loss: 0.338
[55,     1] loss: 0.344
[56,     1] loss: 0.323
[57,     1] loss: 0.308
[58,     1] loss: 0.301
[59,     1] loss: 0.280
[60,     1] loss: 0.273
[61,     1] loss: 0.258
[62,     1] loss: 0.238
[63,     1] loss: 0.230
[64,     1] loss: 0.216
[65,     1] loss: 0.200
[66,     1] loss: 0.187
[67,     1] loss: 0.188
[68,     1] loss: 0.209
[69,     1] loss: 0.201
[70,     1] loss: 0.197
[71,     1] loss: 0.189
[72,     1] loss: 0.183
[73,     1] loss: 0.158
[74,     1] loss: 0.164
[75,     1] loss: 0.167
[76,     1] loss: 0.169
[77,     1] loss: 0.151
[78,     1] loss: 0.168
[79,     1] loss: 0.156
[80,     1] loss: 0.161
[81,     1] loss: 0.145
[82,     1] loss: 0.142
[83,     1] loss: 0.139
[84,     1] loss: 0.132
[85,     1] loss: 0.128
[86,     1] loss: 0.124
[87,     1] loss: 0.121
[88,     1] loss: 0.121
[89,     1] loss: 0.117
[90,     1] loss: 0.114
[91,     1] loss: 0.111
[92,     1] loss: 0.112
[93,     1] loss: 0.110
[94,     1] loss: 0.111
[95,     1] loss: 0.108
[96,     1] loss: 0.109
[97,     1] loss: 0.112
[98,     1] loss: 0.110
[99,     1] loss: 0.111
[100,     1] loss: 0.110
[101,     1] loss: 0.110
[102,     1] loss: 0.112
[103,     1] loss: 0.111
[104,     1] loss: 0.111
[105,     1] loss: 0.110
[106,     1] loss: 0.114
[107,     1] loss: 0.115
[108,     1] loss: 0.221
[109,     1] loss: 0.498
[110,     1] loss: 0.794
[111,     1] loss: 0.610
[112,     1] loss: 0.548
[113,     1] loss: 0.588
[114,     1] loss: 0.584
[115,     1] loss: 0.561
[116,     1] loss: 0.553
[117,     1] loss: 0.556
[118,     1] loss: 0.564
[119,     1] loss: 0.567
[120,     1] loss: 0.570
[121,     1] loss: 0.571
[122,     1] loss: 0.572
[123,     1] loss: 0.573
[124,     1] loss: 0.571
[125,     1] loss: 0.567
[126,     1] loss: 0.567
[127,     1] loss: 0.561
[128,     1] loss: 0.560
Early stopping applied (best metric=0.3065948486328125)
Finished Training
Total time taken: 287.6847813129425
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.682
[3,     1] loss: 0.653
[4,     1] loss: 0.629
[5,     1] loss: 0.605
[6,     1] loss: 0.576
[7,     1] loss: 0.551
[8,     1] loss: 0.526
[9,     1] loss: 0.500
[10,     1] loss: 0.471
[11,     1] loss: 0.446
[12,     1] loss: 0.425
[13,     1] loss: 0.402
[14,     1] loss: 0.380
[15,     1] loss: 0.364
[16,     1] loss: 0.345
[17,     1] loss: 0.330
[18,     1] loss: 0.314
[19,     1] loss: 0.302
[20,     1] loss: 0.290
[21,     1] loss: 0.277
[22,     1] loss: 0.261
[23,     1] loss: 0.248
[24,     1] loss: 0.235
[25,     1] loss: 0.220
[26,     1] loss: 0.207
[27,     1] loss: 0.194
[28,     1] loss: 0.182
[29,     1] loss: 0.171
[30,     1] loss: 0.165
[31,     1] loss: 0.156
[32,     1] loss: 0.150
[33,     1] loss: 0.142
[34,     1] loss: 0.138
[35,     1] loss: 0.132
[36,     1] loss: 0.129
[37,     1] loss: 0.126
[38,     1] loss: 0.123
[39,     1] loss: 0.121
[40,     1] loss: 0.119
[41,     1] loss: 0.117
[42,     1] loss: 0.114
[43,     1] loss: 0.111
[44,     1] loss: 0.103
[45,     1] loss: 0.103
[46,     1] loss: 0.196
[47,     1] loss: 0.800
[48,     1] loss: 0.570
[49,     1] loss: 0.372
[50,     1] loss: 0.407
[51,     1] loss: 0.386
[52,     1] loss: 0.397
[53,     1] loss: 0.394
[54,     1] loss: 0.386
[55,     1] loss: 0.381
[56,     1] loss: 0.368
[57,     1] loss: 0.360
[58,     1] loss: 0.349
[59,     1] loss: 0.336
[60,     1] loss: 0.324
[61,     1] loss: 0.311
[62,     1] loss: 0.302
[63,     1] loss: 0.294
[64,     1] loss: 0.283
[65,     1] loss: 0.272
[66,     1] loss: 0.263
[67,     1] loss: 0.249
[68,     1] loss: 0.236
[69,     1] loss: 0.224
[70,     1] loss: 0.209
[71,     1] loss: 0.201
[72,     1] loss: 0.186
[73,     1] loss: 0.174
[74,     1] loss: 0.160
[75,     1] loss: 0.150
[76,     1] loss: 0.140
[77,     1] loss: 0.134
[78,     1] loss: 0.129
[79,     1] loss: 0.116
[80,     1] loss: 0.131
[81,     1] loss: 0.124
[82,     1] loss: 0.112
[83,     1] loss: 0.132
[84,     1] loss: 0.131
[85,     1] loss: 0.122
[86,     1] loss: 0.113
[87,     1] loss: 0.112
[88,     1] loss: 0.114
[89,     1] loss: 0.107
[90,     1] loss: 0.105
[91,     1] loss: 0.103
[92,     1] loss: 0.094
[93,     1] loss: 0.092
[94,     1] loss: 0.089
[95,     1] loss: 0.086
[96,     1] loss: 0.085
[97,     1] loss: 0.086
[98,     1] loss: 0.085
[99,     1] loss: 0.084
[100,     1] loss: 0.085
[101,     1] loss: 0.086
[102,     1] loss: 0.088
[103,     1] loss: 0.088
[104,     1] loss: 0.088
[105,     1] loss: 0.090
[106,     1] loss: 0.094
[107,     1] loss: 0.091
[108,     1] loss: 0.094
[109,     1] loss: 0.096
[110,     1] loss: 0.095
[111,     1] loss: 0.095
[112,     1] loss: 0.095
[113,     1] loss: 0.094
[114,     1] loss: 0.096
[115,     1] loss: 0.096
[116,     1] loss: 0.094
[117,     1] loss: 0.093
[118,     1] loss: 0.093
[119,     1] loss: 0.093
[120,     1] loss: 0.095
[121,     1] loss: 0.096
[122,     1] loss: 0.097
[123,     1] loss: 0.151
[124,     1] loss: 0.360
[125,     1] loss: 0.412
[126,     1] loss: 0.609
[127,     1] loss: 0.603
[128,     1] loss: 0.569
[129,     1] loss: 0.532
[130,     1] loss: 0.485
[131,     1] loss: 0.434
[132,     1] loss: 0.440
[133,     1] loss: 0.478
[134,     1] loss: 0.480
[135,     1] loss: 0.455
Early stopping applied (best metric=0.4235893785953522)
Finished Training
Total time taken: 304.431964635849
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.676
[3,     1] loss: 0.653
[4,     1] loss: 0.632
[5,     1] loss: 0.609
[6,     1] loss: 0.582
[7,     1] loss: 0.555
[8,     1] loss: 0.527
[9,     1] loss: 0.497
[10,     1] loss: 0.466
[11,     1] loss: 0.439
[12,     1] loss: 0.415
[13,     1] loss: 0.396
[14,     1] loss: 0.379
[15,     1] loss: 0.372
[16,     1] loss: 0.363
[17,     1] loss: 0.355
[18,     1] loss: 0.347
[19,     1] loss: 0.340
[20,     1] loss: 0.334
[21,     1] loss: 0.328
[22,     1] loss: 0.322
[23,     1] loss: 0.315
[24,     1] loss: 0.309
[25,     1] loss: 0.301
[26,     1] loss: 0.292
[27,     1] loss: 0.283
[28,     1] loss: 0.273
[29,     1] loss: 0.261
[30,     1] loss: 0.250
[31,     1] loss: 0.241
[32,     1] loss: 0.232
[33,     1] loss: 0.221
[34,     1] loss: 0.209
[35,     1] loss: 0.199
[36,     1] loss: 0.189
[37,     1] loss: 0.180
[38,     1] loss: 0.173
[39,     1] loss: 0.164
[40,     1] loss: 0.156
[41,     1] loss: 0.150
[42,     1] loss: 0.144
[43,     1] loss: 0.137
[44,     1] loss: 0.135
[45,     1] loss: 0.130
[46,     1] loss: 0.127
[47,     1] loss: 0.123
[48,     1] loss: 0.119
[49,     1] loss: 0.115
[50,     1] loss: 0.107
[51,     1] loss: 0.108
[52,     1] loss: 0.246
[53,     1] loss: 0.201
[54,     1] loss: 0.337
[55,     1] loss: 0.508
[56,     1] loss: 0.464
[57,     1] loss: 0.426
[58,     1] loss: 0.422
[59,     1] loss: 0.443
[60,     1] loss: 0.453
[61,     1] loss: 0.461
[62,     1] loss: 0.460
[63,     1] loss: 0.454
[64,     1] loss: 0.444
[65,     1] loss: 0.431
[66,     1] loss: 0.416
[67,     1] loss: 0.399
[68,     1] loss: 0.380
[69,     1] loss: 0.361
[70,     1] loss: 0.341
[71,     1] loss: 0.320
[72,     1] loss: 0.296
[73,     1] loss: 0.266
[74,     1] loss: 0.245
[75,     1] loss: 0.220
[76,     1] loss: 0.200
[77,     1] loss: 0.181
[78,     1] loss: 0.178
[79,     1] loss: 0.151
[80,     1] loss: 0.143
[81,     1] loss: 0.135
[82,     1] loss: 0.128
[83,     1] loss: 0.122
[84,     1] loss: 0.119
[85,     1] loss: 0.114
[86,     1] loss: 0.112
[87,     1] loss: 0.110
[88,     1] loss: 0.108
[89,     1] loss: 0.105
[90,     1] loss: 0.106
[91,     1] loss: 0.106
[92,     1] loss: 0.106
[93,     1] loss: 0.106
[94,     1] loss: 0.106
[95,     1] loss: 0.106
[96,     1] loss: 0.108
[97,     1] loss: 0.108
[98,     1] loss: 0.107
[99,     1] loss: 0.108
[100,     1] loss: 0.106
[101,     1] loss: 0.104
[102,     1] loss: 0.104
[103,     1] loss: 0.102
[104,     1] loss: 0.101
[105,     1] loss: 0.102
[106,     1] loss: 0.099
[107,     1] loss: 0.099
[108,     1] loss: 0.099
[109,     1] loss: 0.098
[110,     1] loss: 0.097
[111,     1] loss: 0.095
[112,     1] loss: 0.095
[113,     1] loss: 0.094
[114,     1] loss: 0.095
[115,     1] loss: 0.099
[116,     1] loss: 0.095
[117,     1] loss: 0.114
[118,     1] loss: 0.554
[119,     1] loss: 1.180
[120,     1] loss: 0.684
Early stopping applied (best metric=0.4183201491832733)
Finished Training
Total time taken: 271.3513331413269
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.668
[4,     1] loss: 0.650
[5,     1] loss: 0.630
[6,     1] loss: 0.614
[7,     1] loss: 0.592
[8,     1] loss: 0.571
[9,     1] loss: 0.549
[10,     1] loss: 0.523
[11,     1] loss: 0.498
[12,     1] loss: 0.474
[13,     1] loss: 0.456
[14,     1] loss: 0.430
[15,     1] loss: 0.407
[16,     1] loss: 0.386
[17,     1] loss: 0.366
[18,     1] loss: 0.341
[19,     1] loss: 0.322
[20,     1] loss: 0.307
[21,     1] loss: 0.290
[22,     1] loss: 0.274
[23,     1] loss: 0.259
[24,     1] loss: 0.244
[25,     1] loss: 0.231
[26,     1] loss: 0.216
[27,     1] loss: 0.203
[28,     1] loss: 0.188
[29,     1] loss: 0.175
[30,     1] loss: 0.163
[31,     1] loss: 0.152
[32,     1] loss: 0.144
[33,     1] loss: 0.135
[34,     1] loss: 0.129
[35,     1] loss: 0.119
[36,     1] loss: 0.112
[37,     1] loss: 0.108
[38,     1] loss: 0.101
[39,     1] loss: 0.096
[40,     1] loss: 0.093
[41,     1] loss: 0.087
[42,     1] loss: 0.084
[43,     1] loss: 0.080
[44,     1] loss: 0.078
[45,     1] loss: 0.075
[46,     1] loss: 0.071
[47,     1] loss: 0.065
[48,     1] loss: 0.063
[49,     1] loss: 0.060
[50,     1] loss: 0.058
[51,     1] loss: 0.056
[52,     1] loss: 0.056
[53,     1] loss: 0.056
[54,     1] loss: 0.054
[55,     1] loss: 0.057
[56,     1] loss: 0.056
[57,     1] loss: 0.055
[58,     1] loss: 0.055
[59,     1] loss: 0.056
[60,     1] loss: 0.056
[61,     1] loss: 0.057
[62,     1] loss: 0.056
[63,     1] loss: 0.057
[64,     1] loss: 0.064
[65,     1] loss: 0.402
[66,     1] loss: 0.518
[67,     1] loss: 0.822
[68,     1] loss: 0.485
[69,     1] loss: 0.488
[70,     1] loss: 0.542
[71,     1] loss: 0.537
Early stopping applied (best metric=0.4026094377040863)
Finished Training
Total time taken: 161.83310961723328
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.682
[3,     1] loss: 0.661
[4,     1] loss: 0.645
[5,     1] loss: 0.626
[6,     1] loss: 0.610
[7,     1] loss: 0.590
[8,     1] loss: 0.573
[9,     1] loss: 0.549
[10,     1] loss: 0.527
[11,     1] loss: 0.505
[12,     1] loss: 0.480
[13,     1] loss: 0.451
[14,     1] loss: 0.428
[15,     1] loss: 0.399
[16,     1] loss: 0.374
[17,     1] loss: 0.348
[18,     1] loss: 0.321
[19,     1] loss: 0.298
[20,     1] loss: 0.279
[21,     1] loss: 0.260
[22,     1] loss: 0.247
[23,     1] loss: 0.234
[24,     1] loss: 0.214
[25,     1] loss: 0.204
[26,     1] loss: 0.195
[27,     1] loss: 0.183
[28,     1] loss: 0.173
[29,     1] loss: 0.166
[30,     1] loss: 0.159
[31,     1] loss: 0.153
[32,     1] loss: 0.146
[33,     1] loss: 0.142
[34,     1] loss: 0.137
[35,     1] loss: 0.134
[36,     1] loss: 0.132
[37,     1] loss: 0.130
[38,     1] loss: 0.126
[39,     1] loss: 0.125
[40,     1] loss: 0.122
[41,     1] loss: 0.122
[42,     1] loss: 0.121
[43,     1] loss: 0.118
[44,     1] loss: 0.113
[45,     1] loss: 0.113
[46,     1] loss: 0.108
[47,     1] loss: 0.107
[48,     1] loss: 0.106
[49,     1] loss: 0.187
[50,     1] loss: 0.495
[51,     1] loss: 0.372
[52,     1] loss: 0.403
[53,     1] loss: 0.360
[54,     1] loss: 0.348
[55,     1] loss: 0.324
[56,     1] loss: 0.320
[57,     1] loss: 0.310
[58,     1] loss: 0.305
[59,     1] loss: 0.301
[60,     1] loss: 0.293
[61,     1] loss: 0.280
[62,     1] loss: 0.266
[63,     1] loss: 0.256
[64,     1] loss: 0.240
[65,     1] loss: 0.226
[66,     1] loss: 0.220
[67,     1] loss: 0.203
[68,     1] loss: 0.197
[69,     1] loss: 0.184
[70,     1] loss: 0.176
[71,     1] loss: 0.167
[72,     1] loss: 0.159
[73,     1] loss: 0.154
[74,     1] loss: 0.149
[75,     1] loss: 0.140
[76,     1] loss: 0.139
[77,     1] loss: 0.136
[78,     1] loss: 0.134
[79,     1] loss: 0.130
[80,     1] loss: 0.127
[81,     1] loss: 0.128
[82,     1] loss: 0.126
[83,     1] loss: 0.124
[84,     1] loss: 0.126
[85,     1] loss: 0.124
[86,     1] loss: 0.124
[87,     1] loss: 0.125
[88,     1] loss: 0.122
[89,     1] loss: 0.122
[90,     1] loss: 0.121
[91,     1] loss: 0.124
[92,     1] loss: 0.123
[93,     1] loss: 0.122
[94,     1] loss: 0.119
[95,     1] loss: 0.122
[96,     1] loss: 0.125
[97,     1] loss: 0.150
[98,     1] loss: 0.393
Early stopping applied (best metric=0.3415548801422119)
Finished Training
Total time taken: 223.43239307403564
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.654
[4,     1] loss: 0.633
[5,     1] loss: 0.614
[6,     1] loss: 0.589
[7,     1] loss: 0.564
[8,     1] loss: 0.537
[9,     1] loss: 0.510
[10,     1] loss: 0.478
[11,     1] loss: 0.445
[12,     1] loss: 0.413
[13,     1] loss: 0.384
[14,     1] loss: 0.354
[15,     1] loss: 0.328
[16,     1] loss: 0.308
[17,     1] loss: 0.289
[18,     1] loss: 0.275
[19,     1] loss: 0.257
[20,     1] loss: 0.244
[21,     1] loss: 0.229
[22,     1] loss: 0.219
[23,     1] loss: 0.208
[24,     1] loss: 0.196
[25,     1] loss: 0.187
[26,     1] loss: 0.183
[27,     1] loss: 0.174
[28,     1] loss: 0.165
[29,     1] loss: 0.160
[30,     1] loss: 0.150
[31,     1] loss: 0.149
[32,     1] loss: 0.143
[33,     1] loss: 0.139
[34,     1] loss: 0.134
[35,     1] loss: 0.134
[36,     1] loss: 0.128
[37,     1] loss: 0.127
[38,     1] loss: 0.125
[39,     1] loss: 0.127
[40,     1] loss: 0.126
[41,     1] loss: 0.126
[42,     1] loss: 0.122
[43,     1] loss: 0.123
[44,     1] loss: 0.121
[45,     1] loss: 0.121
[46,     1] loss: 0.116
[47,     1] loss: 0.115
[48,     1] loss: 0.115
[49,     1] loss: 0.113
[50,     1] loss: 0.105
[51,     1] loss: 0.098
[52,     1] loss: 0.091
[53,     1] loss: 0.118
[54,     1] loss: 0.386
[55,     1] loss: 0.549
[56,     1] loss: 0.448
[57,     1] loss: 0.476
[58,     1] loss: 0.442
[59,     1] loss: 0.419
[60,     1] loss: 0.408
[61,     1] loss: 0.401
[62,     1] loss: 0.399
[63,     1] loss: 0.391
[64,     1] loss: 0.381
[65,     1] loss: 0.367
[66,     1] loss: 0.356
[67,     1] loss: 0.338
[68,     1] loss: 0.328
[69,     1] loss: 0.307
[70,     1] loss: 0.286
[71,     1] loss: 0.265
[72,     1] loss: 0.247
[73,     1] loss: 0.238
[74,     1] loss: 0.225
[75,     1] loss: 0.211
[76,     1] loss: 0.199
[77,     1] loss: 0.186
[78,     1] loss: 0.175
[79,     1] loss: 0.166
[80,     1] loss: 0.160
[81,     1] loss: 0.152
[82,     1] loss: 0.146
[83,     1] loss: 0.144
[84,     1] loss: 0.137
[85,     1] loss: 0.131
[86,     1] loss: 0.128
[87,     1] loss: 0.128
[88,     1] loss: 0.123
[89,     1] loss: 0.121
[90,     1] loss: 0.120
[91,     1] loss: 0.119
[92,     1] loss: 0.117
[93,     1] loss: 0.118
[94,     1] loss: 0.116
[95,     1] loss: 0.115
[96,     1] loss: 0.113
[97,     1] loss: 0.113
[98,     1] loss: 0.114
[99,     1] loss: 0.111
[100,     1] loss: 0.112
[101,     1] loss: 0.112
[102,     1] loss: 0.113
[103,     1] loss: 0.109
[104,     1] loss: 0.111
[105,     1] loss: 0.159
[106,     1] loss: 0.307
[107,     1] loss: 0.372
[108,     1] loss: 0.958
[109,     1] loss: 0.899
[110,     1] loss: 0.687
[111,     1] loss: 0.596
[112,     1] loss: 0.626
[113,     1] loss: 0.653
[114,     1] loss: 0.652
[115,     1] loss: 0.638
[116,     1] loss: 0.629
[117,     1] loss: 0.629
[118,     1] loss: 0.629
[119,     1] loss: 0.633
[120,     1] loss: 0.637
[121,     1] loss: 0.640
[122,     1] loss: 0.642
[123,     1] loss: 0.641
[124,     1] loss: 0.641
[125,     1] loss: 0.641
[126,     1] loss: 0.640
[127,     1] loss: 0.639
[128,     1] loss: 0.638
[129,     1] loss: 0.636
[130,     1] loss: 0.634
[131,     1] loss: 0.633
[132,     1] loss: 0.629
[133,     1] loss: 0.625
[134,     1] loss: 0.621
[135,     1] loss: 0.616
[136,     1] loss: 0.611
[137,     1] loss: 0.603
[138,     1] loss: 0.596
[139,     1] loss: 0.586
[140,     1] loss: 0.577
[141,     1] loss: 0.565
[142,     1] loss: 0.551
[143,     1] loss: 0.546
[144,     1] loss: 0.532
[145,     1] loss: 0.512
[146,     1] loss: 0.494
[147,     1] loss: 0.475
[148,     1] loss: 0.457
[149,     1] loss: 0.444
[150,     1] loss: 0.429
[151,     1] loss: 0.417
[152,     1] loss: 0.405
[153,     1] loss: 0.397
Early stopping applied (best metric=0.3861583173274994)
Finished Training
Total time taken: 348.289293050766
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.679
[3,     1] loss: 0.649
[4,     1] loss: 0.629
[5,     1] loss: 0.606
[6,     1] loss: 0.588
[7,     1] loss: 0.565
[8,     1] loss: 0.546
[9,     1] loss: 0.520
[10,     1] loss: 0.500
[11,     1] loss: 0.476
[12,     1] loss: 0.452
[13,     1] loss: 0.427
[14,     1] loss: 0.401
[15,     1] loss: 0.381
[16,     1] loss: 0.367
[17,     1] loss: 0.354
[18,     1] loss: 0.342
[19,     1] loss: 0.329
[20,     1] loss: 0.316
[21,     1] loss: 0.305
[22,     1] loss: 0.294
[23,     1] loss: 0.283
[24,     1] loss: 0.273
[25,     1] loss: 0.260
[26,     1] loss: 0.253
[27,     1] loss: 0.243
[28,     1] loss: 0.231
[29,     1] loss: 0.221
[30,     1] loss: 0.209
[31,     1] loss: 0.202
[32,     1] loss: 0.191
[33,     1] loss: 0.181
[34,     1] loss: 0.174
[35,     1] loss: 0.168
[36,     1] loss: 0.158
[37,     1] loss: 0.153
[38,     1] loss: 0.148
[39,     1] loss: 0.142
[40,     1] loss: 0.139
[41,     1] loss: 0.134
[42,     1] loss: 0.133
[43,     1] loss: 0.129
[44,     1] loss: 0.127
[45,     1] loss: 0.124
[46,     1] loss: 0.123
[47,     1] loss: 0.120
[48,     1] loss: 0.121
[49,     1] loss: 0.130
[50,     1] loss: 0.127
[51,     1] loss: 0.125
[52,     1] loss: 0.118
[53,     1] loss: 0.118
[54,     1] loss: 0.110
[55,     1] loss: 0.210
[56,     1] loss: 0.276
[57,     1] loss: 0.666
[58,     1] loss: 0.314
[59,     1] loss: 0.336
[60,     1] loss: 0.320
[61,     1] loss: 0.317
[62,     1] loss: 0.333
[63,     1] loss: 0.337
[64,     1] loss: 0.326
[65,     1] loss: 0.311
[66,     1] loss: 0.294
[67,     1] loss: 0.284
[68,     1] loss: 0.271
[69,     1] loss: 0.260
[70,     1] loss: 0.242
[71,     1] loss: 0.231
[72,     1] loss: 0.219
[73,     1] loss: 0.206
[74,     1] loss: 0.197
[75,     1] loss: 0.188
[76,     1] loss: 0.181
[77,     1] loss: 0.172
[78,     1] loss: 0.165
[79,     1] loss: 0.156
[80,     1] loss: 0.151
[81,     1] loss: 0.148
[82,     1] loss: 0.141
[83,     1] loss: 0.137
[84,     1] loss: 0.133
[85,     1] loss: 0.129
[86,     1] loss: 0.126
[87,     1] loss: 0.123
[88,     1] loss: 0.121
[89,     1] loss: 0.126
[90,     1] loss: 0.179
[91,     1] loss: 0.182
[92,     1] loss: 0.200
[93,     1] loss: 0.186
[94,     1] loss: 0.199
[95,     1] loss: 0.235
[96,     1] loss: 0.187
[97,     1] loss: 0.202
[98,     1] loss: 0.182
[99,     1] loss: 0.201
[100,     1] loss: 0.180
[101,     1] loss: 0.234
[102,     1] loss: 0.312
[103,     1] loss: 0.192
[104,     1] loss: 0.199
[105,     1] loss: 0.209
[106,     1] loss: 0.189
[107,     1] loss: 0.186
[108,     1] loss: 0.182
[109,     1] loss: 0.180
[110,     1] loss: 0.175
[111,     1] loss: 0.170
[112,     1] loss: 0.163
[113,     1] loss: 0.158
[114,     1] loss: 0.150
[115,     1] loss: 0.147
[116,     1] loss: 0.141
[117,     1] loss: 0.138
[118,     1] loss: 0.135
[119,     1] loss: 0.130
[120,     1] loss: 0.126
[121,     1] loss: 0.124
[122,     1] loss: 0.120
[123,     1] loss: 0.121
[124,     1] loss: 0.120
[125,     1] loss: 0.124
[126,     1] loss: 0.125
[127,     1] loss: 0.122
[128,     1] loss: 0.123
[129,     1] loss: 0.119
[130,     1] loss: 0.120
[131,     1] loss: 0.118
[132,     1] loss: 0.119
[133,     1] loss: 0.116
[134,     1] loss: 0.120
[135,     1] loss: 0.116
[136,     1] loss: 0.114
[137,     1] loss: 0.116
[138,     1] loss: 0.113
[139,     1] loss: 0.114
[140,     1] loss: 0.112
[141,     1] loss: 0.114
[142,     1] loss: 0.113
[143,     1] loss: 0.112
[144,     1] loss: 0.112
[145,     1] loss: 0.118
[146,     1] loss: 0.155
[147,     1] loss: 0.259
[148,     1] loss: 0.404
[149,     1] loss: 0.609
[150,     1] loss: 0.443
[151,     1] loss: 0.407
[152,     1] loss: 0.402
[153,     1] loss: 0.344
[154,     1] loss: 0.375
[155,     1] loss: 0.349
[156,     1] loss: 0.342
Early stopping applied (best metric=0.42116230726242065)
Finished Training
Total time taken: 389.6362953186035
{'Hydroxylation-P Validation Accuracy': 0.7928940561392823, 'Hydroxylation-P Validation Sensitivity': 0.7875555555555556, 'Hydroxylation-P Validation Specificity': 0.7940879844381266, 'Hydroxylation-P Validation Precision': 0.4603359693794531, 'Hydroxylation-P AUC ROC': 0.8521833353521966, 'Hydroxylation-P AUC PR': 0.5988705806616252, 'Hydroxylation-P MCC': 0.4856531068955069, 'Hydroxylation-P F1': 0.576887440975439, 'Validation Loss (Hydroxylation-P)': 0.37205638766288757, 'Validation Loss (total)': 0.37205638766288757}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055686284980377195,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 647223014,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.322280960109705}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.672
[3,     1] loss: 0.642
[4,     1] loss: 0.613
[5,     1] loss: 0.580
[6,     1] loss: 0.542
[7,     1] loss: 0.500
[8,     1] loss: 0.449
[9,     1] loss: 0.393
[10,     1] loss: 0.343
[11,     1] loss: 0.301
[12,     1] loss: 0.278
[13,     1] loss: 0.239
[14,     1] loss: 0.226
[15,     1] loss: 0.183
[16,     1] loss: 0.169
[17,     1] loss: 0.147
[18,     1] loss: 0.126
[19,     1] loss: 0.120
[20,     1] loss: 0.115
[21,     1] loss: 0.095
[22,     1] loss: 0.105
[23,     1] loss: 0.096
[24,     1] loss: 0.096
[25,     1] loss: 0.080
[26,     1] loss: 0.088
[27,     1] loss: 0.076
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005996232480079312,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2916407748,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.0024913426961275}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00014589666023061797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2684006831,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.503170266849345}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030788384527115917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4023658219,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.09980995885396}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.676
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00206458792349886,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2401649717,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8713496066645394}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.688
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005662806312898122,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4135968484,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.894225924467705}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.666
[4,     1] loss: 0.652
[5,     1] loss: 0.641
[6,     1] loss: 0.626
[7,     1] loss: 0.611
[8,     1] loss: 0.599
[9,     1] loss: 0.586
[10,     1] loss: 0.573
[11,     1] loss: 0.560
[12,     1] loss: 0.549
[13,     1] loss: 0.539
[14,     1] loss: 0.533
[15,     1] loss: 0.524
[16,     1] loss: 0.522
[17,     1] loss: 0.504
[18,     1] loss: 0.497
[19,     1] loss: 0.494
[20,     1] loss: 0.487
[21,     1] loss: 0.501
[22,     1] loss: 0.471
[23,     1] loss: 0.479
[24,     1] loss: 0.505
[25,     1] loss: 0.483
[26,     1] loss: 0.483
[27,     1] loss: 0.475
[28,     1] loss: 0.476
[29,     1] loss: 0.470
[30,     1] loss: 0.463
[31,     1] loss: 0.456
[32,     1] loss: 0.452
[33,     1] loss: 0.466
[34,     1] loss: 0.441
[35,     1] loss: 0.456
[36,     1] loss: 0.469
[37,     1] loss: 0.438
[38,     1] loss: 0.444
[39,     1] loss: 0.479
[40,     1] loss: 0.438
[41,     1] loss: 0.438
[42,     1] loss: 0.444
[43,     1] loss: 0.431
[44,     1] loss: 0.420
[45,     1] loss: 0.418
[46,     1] loss: 0.422
[47,     1] loss: 0.423
[48,     1] loss: 0.435
[49,     1] loss: 0.416
[50,     1] loss: 0.407
[51,     1] loss: 0.420
[52,     1] loss: 0.407
[53,     1] loss: 0.394
[54,     1] loss: 0.388
[55,     1] loss: 0.383
[56,     1] loss: 0.378
[57,     1] loss: 0.369
[58,     1] loss: 0.349
[59,     1] loss: 0.340
[60,     1] loss: 0.339
[61,     1] loss: 0.320
[62,     1] loss: 0.319
[63,     1] loss: 0.326
[64,     1] loss: 0.362
[65,     1] loss: 0.345
[66,     1] loss: 0.327
[67,     1] loss: 0.335
[68,     1] loss: 0.321
[69,     1] loss: 0.305
[70,     1] loss: 0.305
[71,     1] loss: 0.289
[72,     1] loss: 0.283
[73,     1] loss: 0.265
[74,     1] loss: 0.247
[75,     1] loss: 0.237
[76,     1] loss: 0.231
[77,     1] loss: 0.218
[78,     1] loss: 0.214
[79,     1] loss: 0.204
[80,     1] loss: 0.210
[81,     1] loss: 0.241
[82,     1] loss: 0.240
[83,     1] loss: 0.228
[84,     1] loss: 0.246
[85,     1] loss: 0.295
[86,     1] loss: 0.637
[87,     1] loss: 0.445
[88,     1] loss: 0.580
[89,     1] loss: 0.476
[90,     1] loss: 0.489
[91,     1] loss: 0.514
[92,     1] loss: 0.520
[93,     1] loss: 0.520
[94,     1] loss: 0.522
[95,     1] loss: 0.525
[96,     1] loss: 0.523
[97,     1] loss: 0.522
[98,     1] loss: 0.521
[99,     1] loss: 0.521
[100,     1] loss: 0.520
[101,     1] loss: 0.517
[102,     1] loss: 0.515
[103,     1] loss: 0.515
[104,     1] loss: 0.520
[105,     1] loss: 0.515
[106,     1] loss: 0.513
[107,     1] loss: 0.502
[108,     1] loss: 0.498
[109,     1] loss: 0.519
[110,     1] loss: 0.508
[111,     1] loss: 0.499
[112,     1] loss: 0.493
[113,     1] loss: 0.493
[114,     1] loss: 0.516
[115,     1] loss: 0.521
[116,     1] loss: 0.507
[117,     1] loss: 0.494
[118,     1] loss: 0.491
[119,     1] loss: 0.483
[120,     1] loss: 0.480
[121,     1] loss: 0.479
[122,     1] loss: 0.499
[123,     1] loss: 0.497
[124,     1] loss: 0.491
Early stopping applied (best metric=0.36775895953178406)
Finished Training
Total time taken: 300.0103735923767
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.691
[3,     1] loss: 0.679
[4,     1] loss: 0.670
[5,     1] loss: 0.661
[6,     1] loss: 0.653
[7,     1] loss: 0.643
[8,     1] loss: 0.634
[9,     1] loss: 0.624
[10,     1] loss: 0.616
[11,     1] loss: 0.606
[12,     1] loss: 0.596
[13,     1] loss: 0.589
[14,     1] loss: 0.580
[15,     1] loss: 0.574
[16,     1] loss: 0.569
[17,     1] loss: 0.561
[18,     1] loss: 0.556
[19,     1] loss: 0.551
[20,     1] loss: 0.545
[21,     1] loss: 0.544
[22,     1] loss: 0.572
[23,     1] loss: 0.545
[24,     1] loss: 0.555
[25,     1] loss: 0.549
[26,     1] loss: 0.546
[27,     1] loss: 0.539
[28,     1] loss: 0.532
[29,     1] loss: 0.526
[30,     1] loss: 0.520
[31,     1] loss: 0.515
[32,     1] loss: 0.512
[33,     1] loss: 0.511
[34,     1] loss: 0.505
[35,     1] loss: 0.497
[36,     1] loss: 0.493
[37,     1] loss: 0.486
[38,     1] loss: 0.480
[39,     1] loss: 0.474
[40,     1] loss: 0.480
[41,     1] loss: 0.579
[42,     1] loss: 0.565
[43,     1] loss: 0.545
[44,     1] loss: 0.535
[45,     1] loss: 0.527
[46,     1] loss: 0.525
[47,     1] loss: 0.513
[48,     1] loss: 0.505
[49,     1] loss: 0.500
[50,     1] loss: 0.501
[51,     1] loss: 0.490
[52,     1] loss: 0.483
[53,     1] loss: 0.472
[54,     1] loss: 0.458
[55,     1] loss: 0.448
[56,     1] loss: 0.434
[57,     1] loss: 0.425
[58,     1] loss: 0.431
[59,     1] loss: 0.414
[60,     1] loss: 0.403
[61,     1] loss: 0.389
[62,     1] loss: 0.376
[63,     1] loss: 0.365
[64,     1] loss: 0.355
[65,     1] loss: 0.346
[66,     1] loss: 0.339
[67,     1] loss: 0.338
[68,     1] loss: 0.334
[69,     1] loss: 0.328
[70,     1] loss: 0.319
[71,     1] loss: 0.330
[72,     1] loss: 0.342
[73,     1] loss: 0.614
[74,     1] loss: 0.609
[75,     1] loss: 0.588
[76,     1] loss: 0.577
[77,     1] loss: 0.578
[78,     1] loss: 0.578
[79,     1] loss: 0.585
[80,     1] loss: 0.593
[81,     1] loss: 0.602
[82,     1] loss: 0.607
[83,     1] loss: 0.612
[84,     1] loss: 0.615
[85,     1] loss: 0.620
[86,     1] loss: 0.621
[87,     1] loss: 0.625
[88,     1] loss: 0.627
[89,     1] loss: 0.629
[90,     1] loss: 0.631
[91,     1] loss: 0.632
[92,     1] loss: 0.635
[93,     1] loss: 0.636
[94,     1] loss: 0.636
[95,     1] loss: 0.637
[96,     1] loss: 0.638
[97,     1] loss: 0.638
[98,     1] loss: 0.638
[99,     1] loss: 0.638
[100,     1] loss: 0.639
[101,     1] loss: 0.639
[102,     1] loss: 0.638
[103,     1] loss: 0.637
[104,     1] loss: 0.637
[105,     1] loss: 0.635
[106,     1] loss: 0.634
[107,     1] loss: 0.632
[108,     1] loss: 0.632
[109,     1] loss: 0.630
[110,     1] loss: 0.627
[111,     1] loss: 0.623
[112,     1] loss: 0.622
[113,     1] loss: 0.619
[114,     1] loss: 0.621
[115,     1] loss: 0.618
[116,     1] loss: 0.613
[117,     1] loss: 0.604
[118,     1] loss: 0.599
[119,     1] loss: 0.593
[120,     1] loss: 0.586
[121,     1] loss: 0.577
Early stopping applied (best metric=0.43697428703308105)
Finished Training
Total time taken: 299.6284520626068
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.678
[3,     1] loss: 0.656
[4,     1] loss: 0.642
[5,     1] loss: 0.630
[6,     1] loss: 0.618
[7,     1] loss: 0.608
[8,     1] loss: 0.597
[9,     1] loss: 0.588
[10,     1] loss: 0.581
[11,     1] loss: 0.570
[12,     1] loss: 0.561
[13,     1] loss: 0.552
[14,     1] loss: 0.543
[15,     1] loss: 0.536
[16,     1] loss: 0.527
[17,     1] loss: 0.517
[18,     1] loss: 0.507
[19,     1] loss: 0.497
[20,     1] loss: 0.486
[21,     1] loss: 0.476
[22,     1] loss: 0.463
[23,     1] loss: 0.452
[24,     1] loss: 0.446
[25,     1] loss: 0.449
[26,     1] loss: 0.493
[27,     1] loss: 0.479
[28,     1] loss: 0.467
[29,     1] loss: 0.463
[30,     1] loss: 0.448
[31,     1] loss: 0.431
[32,     1] loss: 0.420
[33,     1] loss: 0.407
[34,     1] loss: 0.398
[35,     1] loss: 0.383
[36,     1] loss: 0.376
[37,     1] loss: 0.367
[38,     1] loss: 0.372
[39,     1] loss: 0.359
[40,     1] loss: 0.346
[41,     1] loss: 0.336
[42,     1] loss: 0.326
[43,     1] loss: 0.325
[44,     1] loss: 0.317
[45,     1] loss: 0.331
[46,     1] loss: 0.334
[47,     1] loss: 0.355
[48,     1] loss: 0.365
[49,     1] loss: 0.347
[50,     1] loss: 0.348
[51,     1] loss: 0.340
[52,     1] loss: 0.338
[53,     1] loss: 0.341
[54,     1] loss: 0.330
[55,     1] loss: 0.328
[56,     1] loss: 0.333
[57,     1] loss: 0.314
[58,     1] loss: 0.318
[59,     1] loss: 0.341
[60,     1] loss: 0.332
[61,     1] loss: 0.329
[62,     1] loss: 0.320
[63,     1] loss: 0.323
[64,     1] loss: 0.382
[65,     1] loss: 0.367
[66,     1] loss: 0.414
[67,     1] loss: 0.375
[68,     1] loss: 0.381
[69,     1] loss: 0.378
[70,     1] loss: 0.375
[71,     1] loss: 0.376
[72,     1] loss: 0.378
[73,     1] loss: 0.392
[74,     1] loss: 0.386
[75,     1] loss: 0.383
[76,     1] loss: 0.380
[77,     1] loss: 0.381
[78,     1] loss: 0.375
[79,     1] loss: 0.371
[80,     1] loss: 0.367
[81,     1] loss: 0.364
[82,     1] loss: 0.371
[83,     1] loss: 0.428
[84,     1] loss: 0.421
[85,     1] loss: 0.476
[86,     1] loss: 0.541
[87,     1] loss: 0.530
[88,     1] loss: 0.525
[89,     1] loss: 0.524
[90,     1] loss: 0.528
[91,     1] loss: 0.531
[92,     1] loss: 0.535
[93,     1] loss: 0.539
[94,     1] loss: 0.541
[95,     1] loss: 0.544
[96,     1] loss: 0.551
[97,     1] loss: 0.553
[98,     1] loss: 0.555
[99,     1] loss: 0.561
[100,     1] loss: 0.558
[101,     1] loss: 0.556
[102,     1] loss: 0.556
[103,     1] loss: 0.560
[104,     1] loss: 0.579
[105,     1] loss: 0.573
[106,     1] loss: 0.568
[107,     1] loss: 0.567
Early stopping applied (best metric=0.37054702639579773)
Finished Training
Total time taken: 249.02417302131653
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.687
[3,     1] loss: 0.674
[4,     1] loss: 0.666
[5,     1] loss: 0.659
[6,     1] loss: 0.651
[7,     1] loss: 0.643
[8,     1] loss: 0.635
[9,     1] loss: 0.627
[10,     1] loss: 0.618
[11,     1] loss: 0.608
[12,     1] loss: 0.598
[13,     1] loss: 0.590
[14,     1] loss: 0.580
[15,     1] loss: 0.569
[16,     1] loss: 0.559
[17,     1] loss: 0.548
[18,     1] loss: 0.542
[19,     1] loss: 0.548
[20,     1] loss: 0.531
[21,     1] loss: 0.525
[22,     1] loss: 0.516
[23,     1] loss: 0.498
[24,     1] loss: 0.493
[25,     1] loss: 0.508
[26,     1] loss: 0.486
[27,     1] loss: 0.483
[28,     1] loss: 0.518
[29,     1] loss: 0.489
[30,     1] loss: 0.478
[31,     1] loss: 0.488
[32,     1] loss: 0.480
[33,     1] loss: 0.462
[34,     1] loss: 0.456
[35,     1] loss: 0.448
[36,     1] loss: 0.450
[37,     1] loss: 0.481
[38,     1] loss: 0.454
[39,     1] loss: 0.472
[40,     1] loss: 0.486
[41,     1] loss: 0.474
[42,     1] loss: 0.467
[43,     1] loss: 0.460
[44,     1] loss: 0.449
[45,     1] loss: 0.438
[46,     1] loss: 0.427
[47,     1] loss: 0.437
[48,     1] loss: 0.422
[49,     1] loss: 0.430
[50,     1] loss: 0.417
[51,     1] loss: 0.426
[52,     1] loss: 0.441
[53,     1] loss: 0.411
[54,     1] loss: 0.417
[55,     1] loss: 0.437
[56,     1] loss: 0.417
[57,     1] loss: 0.402
[58,     1] loss: 0.390
[59,     1] loss: 0.415
[60,     1] loss: 0.422
[61,     1] loss: 0.390
[62,     1] loss: 0.394
[63,     1] loss: 0.377
[64,     1] loss: 0.361
[65,     1] loss: 0.346
[66,     1] loss: 0.341
[67,     1] loss: 0.357
[68,     1] loss: 0.335
[69,     1] loss: 0.319
[70,     1] loss: 0.305
[71,     1] loss: 0.302
[72,     1] loss: 0.286
[73,     1] loss: 0.286
[74,     1] loss: 0.262
[75,     1] loss: 0.252
[76,     1] loss: 0.244
[77,     1] loss: 0.237
[78,     1] loss: 0.223
[79,     1] loss: 0.236
[80,     1] loss: 0.354
[81,     1] loss: 0.694
[82,     1] loss: 0.638
[83,     1] loss: 0.593
[84,     1] loss: 0.568
[85,     1] loss: 0.569
[86,     1] loss: 0.567
[87,     1] loss: 0.574
[88,     1] loss: 0.581
[89,     1] loss: 0.584
[90,     1] loss: 0.588
[91,     1] loss: 0.591
[92,     1] loss: 0.591
[93,     1] loss: 0.590
[94,     1] loss: 0.588
[95,     1] loss: 0.585
[96,     1] loss: 0.580
[97,     1] loss: 0.574
[98,     1] loss: 0.569
[99,     1] loss: 0.561
[100,     1] loss: 0.560
[101,     1] loss: 0.569
[102,     1] loss: 0.555
[103,     1] loss: 0.550
[104,     1] loss: 0.538
[105,     1] loss: 0.526
[106,     1] loss: 0.516
[107,     1] loss: 0.504
[108,     1] loss: 0.514
[109,     1] loss: 0.561
[110,     1] loss: 0.544
[111,     1] loss: 0.540
[112,     1] loss: 0.534
[113,     1] loss: 0.524
[114,     1] loss: 0.510
[115,     1] loss: 0.500
[116,     1] loss: 0.487
[117,     1] loss: 0.477
[118,     1] loss: 0.468
[119,     1] loss: 0.460
[120,     1] loss: 0.457
[121,     1] loss: 0.460
[122,     1] loss: 0.513
[123,     1] loss: 0.534
[124,     1] loss: 0.510
[125,     1] loss: 0.509
[126,     1] loss: 0.501
[127,     1] loss: 0.493
Early stopping applied (best metric=0.37267664074897766)
Finished Training
Total time taken: 287.9904611110687
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.694
[3,     1] loss: 0.682
[4,     1] loss: 0.674
[5,     1] loss: 0.666
[6,     1] loss: 0.658
[7,     1] loss: 0.650
[8,     1] loss: 0.641
[9,     1] loss: 0.632
[10,     1] loss: 0.624
[11,     1] loss: 0.613
[12,     1] loss: 0.601
[13,     1] loss: 0.587
[14,     1] loss: 0.575
[15,     1] loss: 0.562
[16,     1] loss: 0.555
[17,     1] loss: 0.537
[18,     1] loss: 0.530
[19,     1] loss: 0.510
[20,     1] loss: 0.498
[21,     1] loss: 0.497
[22,     1] loss: 0.532
[23,     1] loss: 0.506
[24,     1] loss: 0.506
[25,     1] loss: 0.491
[26,     1] loss: 0.489
[27,     1] loss: 0.482
[28,     1] loss: 0.487
[29,     1] loss: 0.488
[30,     1] loss: 0.469
[31,     1] loss: 0.480
[32,     1] loss: 0.491
[33,     1] loss: 0.468
[34,     1] loss: 0.471
[35,     1] loss: 0.464
[36,     1] loss: 0.452
[37,     1] loss: 0.445
[38,     1] loss: 0.449
[39,     1] loss: 0.446
[40,     1] loss: 0.469
[41,     1] loss: 0.439
[42,     1] loss: 0.471
[43,     1] loss: 0.510
[44,     1] loss: 0.483
[45,     1] loss: 0.480
[46,     1] loss: 0.468
[47,     1] loss: 0.464
[48,     1] loss: 0.455
[49,     1] loss: 0.446
[50,     1] loss: 0.449
[51,     1] loss: 0.456
[52,     1] loss: 0.476
[53,     1] loss: 0.452
[54,     1] loss: 0.459
[55,     1] loss: 0.455
[56,     1] loss: 0.449
[57,     1] loss: 0.439
[58,     1] loss: 0.434
[59,     1] loss: 0.433
[60,     1] loss: 0.460
[61,     1] loss: 0.438
[62,     1] loss: 0.441
[63,     1] loss: 0.441
[64,     1] loss: 0.483
[65,     1] loss: 0.459
[66,     1] loss: 0.462
[67,     1] loss: 0.452
[68,     1] loss: 0.450
[69,     1] loss: 0.449
[70,     1] loss: 0.460
[71,     1] loss: 0.444
[72,     1] loss: 0.441
[73,     1] loss: 0.436
[74,     1] loss: 0.436
[75,     1] loss: 0.459
[76,     1] loss: 0.438
[77,     1] loss: 0.434
[78,     1] loss: 0.431
[79,     1] loss: 0.447
[80,     1] loss: 0.427
[81,     1] loss: 0.422
[82,     1] loss: 0.424
[83,     1] loss: 0.444
[84,     1] loss: 0.434
[85,     1] loss: 0.505
[86,     1] loss: 0.474
[87,     1] loss: 0.477
[88,     1] loss: 0.509
[89,     1] loss: 0.500
[90,     1] loss: 0.493
[91,     1] loss: 0.478
[92,     1] loss: 0.462
[93,     1] loss: 0.445
[94,     1] loss: 0.439
[95,     1] loss: 0.428
[96,     1] loss: 0.469
[97,     1] loss: 0.424
[98,     1] loss: 0.400
[99,     1] loss: 0.376
[100,     1] loss: 0.359
[101,     1] loss: 0.341
[102,     1] loss: 0.324
[103,     1] loss: 0.302
[104,     1] loss: 0.290
[105,     1] loss: 0.267
[106,     1] loss: 0.255
[107,     1] loss: 0.272
[108,     1] loss: 0.233
[109,     1] loss: 0.267
[110,     1] loss: 0.302
[111,     1] loss: 0.374
[112,     1] loss: 0.619
[113,     1] loss: 0.530
[114,     1] loss: 0.499
[115,     1] loss: 0.504
[116,     1] loss: 0.521
[117,     1] loss: 0.524
[118,     1] loss: 0.528
[119,     1] loss: 0.531
[120,     1] loss: 0.534
[121,     1] loss: 0.532
[122,     1] loss: 0.532
[123,     1] loss: 0.532
[124,     1] loss: 0.531
[125,     1] loss: 0.532
[126,     1] loss: 0.529
[127,     1] loss: 0.528
[128,     1] loss: 0.526
[129,     1] loss: 0.525
[130,     1] loss: 0.521
[131,     1] loss: 0.525
[132,     1] loss: 0.530
[133,     1] loss: 0.518
[134,     1] loss: 0.521
[135,     1] loss: 0.530
[136,     1] loss: 0.512
[137,     1] loss: 0.511
[138,     1] loss: 0.518
[139,     1] loss: 0.502
[140,     1] loss: 0.496
[141,     1] loss: 0.499
[142,     1] loss: 0.487
[143,     1] loss: 0.479
[144,     1] loss: 0.469
[145,     1] loss: 0.484
[146,     1] loss: 0.473
[147,     1] loss: 0.476
[148,     1] loss: 0.452
[149,     1] loss: 0.460
[150,     1] loss: 0.491
[151,     1] loss: 0.466
[152,     1] loss: 0.453
[153,     1] loss: 0.448
[154,     1] loss: 0.433
[155,     1] loss: 0.427
[156,     1] loss: 0.424
[157,     1] loss: 0.434
[158,     1] loss: 0.483
[159,     1] loss: 0.435
[160,     1] loss: 0.434
[161,     1] loss: 0.440
[162,     1] loss: 0.423
[163,     1] loss: 0.416
[164,     1] loss: 0.412
Early stopping applied (best metric=0.4101388454437256)
Finished Training
Total time taken: 371.2642025947571
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.639
[6,     1] loss: 0.626
[7,     1] loss: 0.613
[8,     1] loss: 0.599
[9,     1] loss: 0.585
[10,     1] loss: 0.574
[11,     1] loss: 0.559
[12,     1] loss: 0.547
[13,     1] loss: 0.535
[14,     1] loss: 0.524
[15,     1] loss: 0.511
[16,     1] loss: 0.499
[17,     1] loss: 0.489
[18,     1] loss: 0.477
[19,     1] loss: 0.468
[20,     1] loss: 0.457
[21,     1] loss: 0.446
[22,     1] loss: 0.436
[23,     1] loss: 0.429
[24,     1] loss: 0.421
[25,     1] loss: 0.413
[26,     1] loss: 0.405
[27,     1] loss: 0.452
[28,     1] loss: 0.536
[29,     1] loss: 0.507
[30,     1] loss: 0.496
[31,     1] loss: 0.499
[32,     1] loss: 0.484
[33,     1] loss: 0.475
[34,     1] loss: 0.465
[35,     1] loss: 0.458
[36,     1] loss: 0.448
[37,     1] loss: 0.438
[38,     1] loss: 0.428
[39,     1] loss: 0.416
[40,     1] loss: 0.404
[41,     1] loss: 0.394
[42,     1] loss: 0.384
[43,     1] loss: 0.374
[44,     1] loss: 0.361
[45,     1] loss: 0.351
[46,     1] loss: 0.340
[47,     1] loss: 0.329
[48,     1] loss: 0.325
[49,     1] loss: 0.335
[50,     1] loss: 0.392
[51,     1] loss: 0.542
[52,     1] loss: 0.485
[53,     1] loss: 0.504
[54,     1] loss: 0.490
[55,     1] loss: 0.496
[56,     1] loss: 0.497
[57,     1] loss: 0.497
[58,     1] loss: 0.495
[59,     1] loss: 0.487
[60,     1] loss: 0.481
[61,     1] loss: 0.471
[62,     1] loss: 0.460
[63,     1] loss: 0.451
[64,     1] loss: 0.442
[65,     1] loss: 0.429
[66,     1] loss: 0.420
[67,     1] loss: 0.413
[68,     1] loss: 0.402
[69,     1] loss: 0.395
[70,     1] loss: 0.406
[71,     1] loss: 0.384
[72,     1] loss: 0.392
[73,     1] loss: 0.441
[74,     1] loss: 0.555
[75,     1] loss: 0.465
[76,     1] loss: 0.464
[77,     1] loss: 0.473
[78,     1] loss: 0.459
[79,     1] loss: 0.456
[80,     1] loss: 0.450
[81,     1] loss: 0.441
[82,     1] loss: 0.437
[83,     1] loss: 0.430
[84,     1] loss: 0.422
[85,     1] loss: 0.417
[86,     1] loss: 0.411
[87,     1] loss: 0.422
[88,     1] loss: 0.412
[89,     1] loss: 0.404
[90,     1] loss: 0.399
[91,     1] loss: 0.411
[92,     1] loss: 0.403
[93,     1] loss: 0.419
[94,     1] loss: 0.403
[95,     1] loss: 0.395
[96,     1] loss: 0.393
[97,     1] loss: 0.405
[98,     1] loss: 0.395
[99,     1] loss: 0.401
[100,     1] loss: 0.389
[101,     1] loss: 0.383
[102,     1] loss: 0.396
[103,     1] loss: 0.435
Early stopping applied (best metric=0.4313739538192749)
Finished Training
Total time taken: 234.6836757659912
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.659
[4,     1] loss: 0.644
[5,     1] loss: 0.631
[6,     1] loss: 0.619
[7,     1] loss: 0.606
[8,     1] loss: 0.596
[9,     1] loss: 0.584
[10,     1] loss: 0.572
[11,     1] loss: 0.560
[12,     1] loss: 0.550
[13,     1] loss: 0.539
[14,     1] loss: 0.530
[15,     1] loss: 0.519
[16,     1] loss: 0.510
[17,     1] loss: 0.504
[18,     1] loss: 0.503
[19,     1] loss: 0.491
[20,     1] loss: 0.483
[21,     1] loss: 0.476
[22,     1] loss: 0.471
[23,     1] loss: 0.464
[24,     1] loss: 0.452
[25,     1] loss: 0.447
[26,     1] loss: 0.436
[27,     1] loss: 0.429
[28,     1] loss: 0.421
[29,     1] loss: 0.416
[30,     1] loss: 0.439
[31,     1] loss: 0.520
[32,     1] loss: 0.487
[33,     1] loss: 0.493
[34,     1] loss: 0.491
[35,     1] loss: 0.477
[36,     1] loss: 0.469
[37,     1] loss: 0.459
[38,     1] loss: 0.444
[39,     1] loss: 0.430
[40,     1] loss: 0.417
[41,     1] loss: 0.402
[42,     1] loss: 0.387
[43,     1] loss: 0.374
[44,     1] loss: 0.360
[45,     1] loss: 0.345
[46,     1] loss: 0.332
[47,     1] loss: 0.315
[48,     1] loss: 0.313
[49,     1] loss: 0.324
[50,     1] loss: 0.315
[51,     1] loss: 0.353
[52,     1] loss: 0.416
[53,     1] loss: 0.397
[54,     1] loss: 0.382
[55,     1] loss: 0.366
[56,     1] loss: 0.360
[57,     1] loss: 0.357
[58,     1] loss: 0.343
[59,     1] loss: 0.334
[60,     1] loss: 0.322
[61,     1] loss: 0.315
[62,     1] loss: 0.306
[63,     1] loss: 0.301
[64,     1] loss: 0.296
[65,     1] loss: 0.288
[66,     1] loss: 0.299
[67,     1] loss: 0.337
[68,     1] loss: 0.307
[69,     1] loss: 0.314
[70,     1] loss: 0.313
[71,     1] loss: 0.308
[72,     1] loss: 0.322
[73,     1] loss: 0.316
[74,     1] loss: 0.313
[75,     1] loss: 0.303
[76,     1] loss: 0.299
[77,     1] loss: 0.292
[78,     1] loss: 0.295
[79,     1] loss: 0.323
[80,     1] loss: 0.314
[81,     1] loss: 0.402
[82,     1] loss: 0.381
[83,     1] loss: 0.475
[84,     1] loss: 0.406
[85,     1] loss: 0.493
[86,     1] loss: 0.427
[87,     1] loss: 0.425
[88,     1] loss: 0.431
[89,     1] loss: 0.427
[90,     1] loss: 0.423
[91,     1] loss: 0.423
[92,     1] loss: 0.423
[93,     1] loss: 0.416
[94,     1] loss: 0.411
[95,     1] loss: 0.406
[96,     1] loss: 0.403
[97,     1] loss: 0.407
[98,     1] loss: 0.422
[99,     1] loss: 0.413
[100,     1] loss: 0.434
[101,     1] loss: 0.422
[102,     1] loss: 0.415
[103,     1] loss: 0.420
[104,     1] loss: 0.420
[105,     1] loss: 0.415
[106,     1] loss: 0.414
[107,     1] loss: 0.421
[108,     1] loss: 0.419
[109,     1] loss: 0.421
[110,     1] loss: 0.419
[111,     1] loss: 0.413
[112,     1] loss: 0.419
[113,     1] loss: 0.463
[114,     1] loss: 0.448
[115,     1] loss: 0.436
[116,     1] loss: 0.435
[117,     1] loss: 0.439
[118,     1] loss: 0.435
[119,     1] loss: 0.432
[120,     1] loss: 0.437
Early stopping applied (best metric=0.40149325132369995)
Finished Training
Total time taken: 273.87119913101196
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.674
[3,     1] loss: 0.655
[4,     1] loss: 0.639
[5,     1] loss: 0.626
[6,     1] loss: 0.614
[7,     1] loss: 0.603
[8,     1] loss: 0.592
[9,     1] loss: 0.581
[10,     1] loss: 0.573
[11,     1] loss: 0.565
[12,     1] loss: 0.558
[13,     1] loss: 0.551
[14,     1] loss: 0.544
[15,     1] loss: 0.534
[16,     1] loss: 0.528
[17,     1] loss: 0.520
[18,     1] loss: 0.512
[19,     1] loss: 0.504
[20,     1] loss: 0.496
[21,     1] loss: 0.497
[22,     1] loss: 0.482
[23,     1] loss: 0.496
[24,     1] loss: 0.525
[25,     1] loss: 0.530
[26,     1] loss: 0.517
[27,     1] loss: 0.505
[28,     1] loss: 0.501
[29,     1] loss: 0.493
[30,     1] loss: 0.481
[31,     1] loss: 0.472
[32,     1] loss: 0.472
[33,     1] loss: 0.462
[34,     1] loss: 0.457
[35,     1] loss: 0.444
[36,     1] loss: 0.445
[37,     1] loss: 0.431
[38,     1] loss: 0.423
[39,     1] loss: 0.425
[40,     1] loss: 0.440
[41,     1] loss: 0.474
[42,     1] loss: 0.513
[43,     1] loss: 0.523
[44,     1] loss: 0.524
[45,     1] loss: 0.523
[46,     1] loss: 0.524
[47,     1] loss: 0.520
[48,     1] loss: 0.516
[49,     1] loss: 0.508
[50,     1] loss: 0.501
[51,     1] loss: 0.494
[52,     1] loss: 0.486
[53,     1] loss: 0.479
[54,     1] loss: 0.474
[55,     1] loss: 0.469
[56,     1] loss: 0.479
[57,     1] loss: 0.468
[58,     1] loss: 0.487
[59,     1] loss: 0.473
[60,     1] loss: 0.474
[61,     1] loss: 0.491
[62,     1] loss: 0.473
[63,     1] loss: 0.470
[64,     1] loss: 0.472
[65,     1] loss: 0.487
[66,     1] loss: 0.470
[67,     1] loss: 0.479
[68,     1] loss: 0.497
[69,     1] loss: 0.472
[70,     1] loss: 0.489
[71,     1] loss: 0.499
[72,     1] loss: 0.486
[73,     1] loss: 0.480
[74,     1] loss: 0.479
[75,     1] loss: 0.482
[76,     1] loss: 0.473
[77,     1] loss: 0.474
[78,     1] loss: 0.475
[79,     1] loss: 0.467
[80,     1] loss: 0.465
[81,     1] loss: 0.469
[82,     1] loss: 0.465
[83,     1] loss: 0.472
Early stopping applied (best metric=0.4270269572734833)
Finished Training
Total time taken: 190.57928657531738
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.692
[3,     1] loss: 0.673
[4,     1] loss: 0.662
[5,     1] loss: 0.652
[6,     1] loss: 0.641
[7,     1] loss: 0.632
[8,     1] loss: 0.621
[9,     1] loss: 0.611
[10,     1] loss: 0.601
[11,     1] loss: 0.591
[12,     1] loss: 0.581
[13,     1] loss: 0.572
[14,     1] loss: 0.559
[15,     1] loss: 0.548
[16,     1] loss: 0.538
[17,     1] loss: 0.531
[18,     1] loss: 0.543
[19,     1] loss: 0.529
[20,     1] loss: 0.524
[21,     1] loss: 0.519
[22,     1] loss: 0.506
[23,     1] loss: 0.497
[24,     1] loss: 0.495
[25,     1] loss: 0.503
[26,     1] loss: 0.530
[27,     1] loss: 0.516
[28,     1] loss: 0.508
[29,     1] loss: 0.497
[30,     1] loss: 0.495
[31,     1] loss: 0.492
[32,     1] loss: 0.498
[33,     1] loss: 0.517
[34,     1] loss: 0.495
[35,     1] loss: 0.503
[36,     1] loss: 0.505
[37,     1] loss: 0.494
[38,     1] loss: 0.488
[39,     1] loss: 0.480
[40,     1] loss: 0.491
[41,     1] loss: 0.470
[42,     1] loss: 0.468
[43,     1] loss: 0.484
[44,     1] loss: 0.459
[45,     1] loss: 0.468
[46,     1] loss: 0.496
[47,     1] loss: 0.470
[48,     1] loss: 0.477
[49,     1] loss: 0.476
[50,     1] loss: 0.466
[51,     1] loss: 0.463
[52,     1] loss: 0.466
[53,     1] loss: 0.455
[54,     1] loss: 0.453
[55,     1] loss: 0.458
[56,     1] loss: 0.453
[57,     1] loss: 0.458
[58,     1] loss: 0.460
[59,     1] loss: 0.488
[60,     1] loss: 0.489
[61,     1] loss: 0.485
[62,     1] loss: 0.467
[63,     1] loss: 0.461
[64,     1] loss: 0.467
[65,     1] loss: 0.468
[66,     1] loss: 0.473
[67,     1] loss: 0.458
[68,     1] loss: 0.457
[69,     1] loss: 0.457
[70,     1] loss: 0.459
[71,     1] loss: 0.467
[72,     1] loss: 0.453
[73,     1] loss: 0.454
[74,     1] loss: 0.454
[75,     1] loss: 0.457
[76,     1] loss: 0.471
[77,     1] loss: 0.517
[78,     1] loss: 0.488
[79,     1] loss: 0.483
[80,     1] loss: 0.490
[81,     1] loss: 0.479
[82,     1] loss: 0.473
[83,     1] loss: 0.467
[84,     1] loss: 0.464
[85,     1] loss: 0.461
[86,     1] loss: 0.469
[87,     1] loss: 0.461
[88,     1] loss: 0.458
[89,     1] loss: 0.458
[90,     1] loss: 0.461
[91,     1] loss: 0.457
[92,     1] loss: 0.458
[93,     1] loss: 0.457
[94,     1] loss: 0.468
[95,     1] loss: 0.473
[96,     1] loss: 0.506
[97,     1] loss: 0.480
[98,     1] loss: 0.478
[99,     1] loss: 0.470
[100,     1] loss: 0.466
[101,     1] loss: 0.473
[102,     1] loss: 0.472
[103,     1] loss: 0.482
[104,     1] loss: 0.466
[105,     1] loss: 0.474
[106,     1] loss: 0.496
[107,     1] loss: 0.468
[108,     1] loss: 0.476
[109,     1] loss: 0.497
[110,     1] loss: 0.474
[111,     1] loss: 0.478
[112,     1] loss: 0.483
[113,     1] loss: 0.474
[114,     1] loss: 0.470
[115,     1] loss: 0.471
[116,     1] loss: 0.468
[117,     1] loss: 0.474
[118,     1] loss: 0.467
[119,     1] loss: 0.468
[120,     1] loss: 0.466
[121,     1] loss: 0.475
[122,     1] loss: 0.474
[123,     1] loss: 0.493
[124,     1] loss: 0.480
[125,     1] loss: 0.481
[126,     1] loss: 0.475
[127,     1] loss: 0.471
[128,     1] loss: 0.471
Early stopping applied (best metric=0.441177099943161)
Finished Training
Total time taken: 295.7089343070984
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.669
[4,     1] loss: 0.660
[5,     1] loss: 0.650
[6,     1] loss: 0.640
[7,     1] loss: 0.630
[8,     1] loss: 0.621
[9,     1] loss: 0.612
[10,     1] loss: 0.601
[11,     1] loss: 0.593
[12,     1] loss: 0.582
[13,     1] loss: 0.575
[14,     1] loss: 0.564
[15,     1] loss: 0.556
[16,     1] loss: 0.546
[17,     1] loss: 0.536
[18,     1] loss: 0.528
[19,     1] loss: 0.521
[20,     1] loss: 0.508
[21,     1] loss: 0.496
[22,     1] loss: 0.486
[23,     1] loss: 0.482
[24,     1] loss: 0.482
[25,     1] loss: 0.521
[26,     1] loss: 0.504
[27,     1] loss: 0.489
[28,     1] loss: 0.477
[29,     1] loss: 0.467
[30,     1] loss: 0.452
[31,     1] loss: 0.456
[32,     1] loss: 0.433
[33,     1] loss: 0.415
[34,     1] loss: 0.402
[35,     1] loss: 0.401
[36,     1] loss: 0.409
[37,     1] loss: 0.438
[38,     1] loss: 0.413
[39,     1] loss: 0.431
[40,     1] loss: 0.447
[41,     1] loss: 0.477
[42,     1] loss: 0.496
[43,     1] loss: 0.464
[44,     1] loss: 0.471
[45,     1] loss: 0.467
[46,     1] loss: 0.466
[47,     1] loss: 0.457
[48,     1] loss: 0.451
[49,     1] loss: 0.446
[50,     1] loss: 0.436
[51,     1] loss: 0.429
[52,     1] loss: 0.421
[53,     1] loss: 0.412
[54,     1] loss: 0.403
[55,     1] loss: 0.395
[56,     1] loss: 0.399
[57,     1] loss: 0.396
[58,     1] loss: 0.395
[59,     1] loss: 0.383
[60,     1] loss: 0.377
[61,     1] loss: 0.368
[62,     1] loss: 0.361
[63,     1] loss: 0.372
[64,     1] loss: 0.411
[65,     1] loss: 0.525
[66,     1] loss: 0.509
[67,     1] loss: 0.500
[68,     1] loss: 0.490
[69,     1] loss: 0.488
[70,     1] loss: 0.490
[71,     1] loss: 0.495
[72,     1] loss: 0.490
[73,     1] loss: 0.489
[74,     1] loss: 0.484
[75,     1] loss: 0.478
[76,     1] loss: 0.473
[77,     1] loss: 0.466
[78,     1] loss: 0.465
[79,     1] loss: 0.474
[80,     1] loss: 0.464
[81,     1] loss: 0.453
[82,     1] loss: 0.443
[83,     1] loss: 0.451
[84,     1] loss: 0.505
[85,     1] loss: 0.477
[86,     1] loss: 0.486
[87,     1] loss: 0.477
Early stopping applied (best metric=0.42744988203048706)
Finished Training
Total time taken: 209.73522663116455
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.702
[3,     1] loss: 0.691
[4,     1] loss: 0.682
[5,     1] loss: 0.673
[6,     1] loss: 0.664
[7,     1] loss: 0.655
[8,     1] loss: 0.644
[9,     1] loss: 0.635
[10,     1] loss: 0.623
[11,     1] loss: 0.612
[12,     1] loss: 0.602
[13,     1] loss: 0.591
[14,     1] loss: 0.582
[15,     1] loss: 0.570
[16,     1] loss: 0.566
[17,     1] loss: 0.567
[18,     1] loss: 0.553
[19,     1] loss: 0.558
[20,     1] loss: 0.546
[21,     1] loss: 0.545
[22,     1] loss: 0.531
[23,     1] loss: 0.533
[24,     1] loss: 0.528
[25,     1] loss: 0.517
[26,     1] loss: 0.509
[27,     1] loss: 0.505
[28,     1] loss: 0.500
[29,     1] loss: 0.492
[30,     1] loss: 0.485
[31,     1] loss: 0.477
[32,     1] loss: 0.468
[33,     1] loss: 0.460
[34,     1] loss: 0.452
[35,     1] loss: 0.442
[36,     1] loss: 0.431
[37,     1] loss: 0.424
[38,     1] loss: 0.429
[39,     1] loss: 0.459
[40,     1] loss: 0.534
[41,     1] loss: 0.498
[42,     1] loss: 0.477
[43,     1] loss: 0.481
[44,     1] loss: 0.456
[45,     1] loss: 0.450
[46,     1] loss: 0.443
[47,     1] loss: 0.429
[48,     1] loss: 0.415
[49,     1] loss: 0.403
[50,     1] loss: 0.389
[51,     1] loss: 0.377
[52,     1] loss: 0.364
[53,     1] loss: 0.351
[54,     1] loss: 0.346
[55,     1] loss: 0.334
[56,     1] loss: 0.330
[57,     1] loss: 0.322
[58,     1] loss: 0.314
[59,     1] loss: 0.306
[60,     1] loss: 0.303
[61,     1] loss: 0.330
[62,     1] loss: 0.433
[63,     1] loss: 0.526
[64,     1] loss: 0.517
[65,     1] loss: 0.524
[66,     1] loss: 0.524
[67,     1] loss: 0.531
[68,     1] loss: 0.539
[69,     1] loss: 0.545
[70,     1] loss: 0.556
[71,     1] loss: 0.562
[72,     1] loss: 0.569
[73,     1] loss: 0.572
[74,     1] loss: 0.576
[75,     1] loss: 0.578
[76,     1] loss: 0.580
[77,     1] loss: 0.581
[78,     1] loss: 0.580
[79,     1] loss: 0.580
[80,     1] loss: 0.579
[81,     1] loss: 0.577
[82,     1] loss: 0.574
[83,     1] loss: 0.572
[84,     1] loss: 0.572
[85,     1] loss: 0.569
[86,     1] loss: 0.567
[87,     1] loss: 0.569
[88,     1] loss: 0.563
[89,     1] loss: 0.553
[90,     1] loss: 0.549
[91,     1] loss: 0.549
[92,     1] loss: 0.564
[93,     1] loss: 0.547
[94,     1] loss: 0.540
[95,     1] loss: 0.539
[96,     1] loss: 0.531
[97,     1] loss: 0.524
[98,     1] loss: 0.523
[99,     1] loss: 0.527
[100,     1] loss: 0.522
[101,     1] loss: 0.516
[102,     1] loss: 0.511
[103,     1] loss: 0.506
[104,     1] loss: 0.510
[105,     1] loss: 0.524
[106,     1] loss: 0.516
Early stopping applied (best metric=0.4470609128475189)
Finished Training
Total time taken: 251.01933360099792
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.696
[3,     1] loss: 0.680
[4,     1] loss: 0.669
[5,     1] loss: 0.660
[6,     1] loss: 0.648
[7,     1] loss: 0.638
[8,     1] loss: 0.628
[9,     1] loss: 0.617
[10,     1] loss: 0.607
[11,     1] loss: 0.597
[12,     1] loss: 0.587
[13,     1] loss: 0.575
[14,     1] loss: 0.560
[15,     1] loss: 0.548
[16,     1] loss: 0.536
[17,     1] loss: 0.531
[18,     1] loss: 0.532
[19,     1] loss: 0.536
[20,     1] loss: 0.518
[21,     1] loss: 0.524
[22,     1] loss: 0.509
[23,     1] loss: 0.502
[24,     1] loss: 0.492
[25,     1] loss: 0.490
[26,     1] loss: 0.481
[27,     1] loss: 0.477
[28,     1] loss: 0.466
[29,     1] loss: 0.463
[30,     1] loss: 0.459
[31,     1] loss: 0.479
[32,     1] loss: 0.450
[33,     1] loss: 0.448
[34,     1] loss: 0.464
[35,     1] loss: 0.441
[36,     1] loss: 0.437
[37,     1] loss: 0.437
[38,     1] loss: 0.461
[39,     1] loss: 0.495
[40,     1] loss: 0.460
[41,     1] loss: 0.468
[42,     1] loss: 0.467
[43,     1] loss: 0.460
[44,     1] loss: 0.460
[45,     1] loss: 0.460
[46,     1] loss: 0.475
[47,     1] loss: 0.461
[48,     1] loss: 0.450
[49,     1] loss: 0.445
[50,     1] loss: 0.453
[51,     1] loss: 0.442
[52,     1] loss: 0.458
[53,     1] loss: 0.441
[54,     1] loss: 0.436
[55,     1] loss: 0.442
[56,     1] loss: 0.454
[57,     1] loss: 0.514
[58,     1] loss: 0.465
[59,     1] loss: 0.484
[60,     1] loss: 0.470
[61,     1] loss: 0.461
[62,     1] loss: 0.456
[63,     1] loss: 0.456
[64,     1] loss: 0.449
[65,     1] loss: 0.451
[66,     1] loss: 0.447
[67,     1] loss: 0.451
[68,     1] loss: 0.441
[69,     1] loss: 0.440
[70,     1] loss: 0.436
[71,     1] loss: 0.439
[72,     1] loss: 0.446
[73,     1] loss: 0.504
[74,     1] loss: 0.460
[75,     1] loss: 0.455
[76,     1] loss: 0.454
[77,     1] loss: 0.448
[78,     1] loss: 0.460
[79,     1] loss: 0.450
[80,     1] loss: 0.453
[81,     1] loss: 0.447
[82,     1] loss: 0.443
[83,     1] loss: 0.442
[84,     1] loss: 0.449
[85,     1] loss: 0.450
[86,     1] loss: 0.448
[87,     1] loss: 0.442
[88,     1] loss: 0.445
[89,     1] loss: 0.447
[90,     1] loss: 0.457
[91,     1] loss: 0.448
[92,     1] loss: 0.451
[93,     1] loss: 0.451
[94,     1] loss: 0.466
[95,     1] loss: 0.462
[96,     1] loss: 0.451
[97,     1] loss: 0.454
[98,     1] loss: 0.461
[99,     1] loss: 0.462
[100,     1] loss: 0.482
[101,     1] loss: 0.452
[102,     1] loss: 0.452
[103,     1] loss: 0.444
[104,     1] loss: 0.443
[105,     1] loss: 0.445
[106,     1] loss: 0.460
[107,     1] loss: 0.478
Early stopping applied (best metric=0.44329750537872314)
Finished Training
Total time taken: 253.3675172328949
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.687
[3,     1] loss: 0.665
[4,     1] loss: 0.651
[5,     1] loss: 0.639
[6,     1] loss: 0.628
[7,     1] loss: 0.618
[8,     1] loss: 0.611
[9,     1] loss: 0.603
[10,     1] loss: 0.596
[11,     1] loss: 0.588
[12,     1] loss: 0.582
[13,     1] loss: 0.574
[14,     1] loss: 0.568
[15,     1] loss: 0.560
[16,     1] loss: 0.555
[17,     1] loss: 0.550
[18,     1] loss: 0.552
[19,     1] loss: 0.537
[20,     1] loss: 0.536
[21,     1] loss: 0.545
[22,     1] loss: 0.526
[23,     1] loss: 0.546
[24,     1] loss: 0.539
[25,     1] loss: 0.539
[26,     1] loss: 0.527
[27,     1] loss: 0.519
[28,     1] loss: 0.515
[29,     1] loss: 0.506
[30,     1] loss: 0.499
[31,     1] loss: 0.498
[32,     1] loss: 0.507
[33,     1] loss: 0.523
[34,     1] loss: 0.512
[35,     1] loss: 0.513
[36,     1] loss: 0.509
[37,     1] loss: 0.499
[38,     1] loss: 0.493
[39,     1] loss: 0.487
[40,     1] loss: 0.480
[41,     1] loss: 0.478
[42,     1] loss: 0.488
[43,     1] loss: 0.488
[44,     1] loss: 0.476
[45,     1] loss: 0.474
[46,     1] loss: 0.470
[47,     1] loss: 0.464
[48,     1] loss: 0.464
[49,     1] loss: 0.457
[50,     1] loss: 0.458
[51,     1] loss: 0.445
[52,     1] loss: 0.437
[53,     1] loss: 0.432
[54,     1] loss: 0.440
[55,     1] loss: 0.464
[56,     1] loss: 0.572
[57,     1] loss: 0.494
[58,     1] loss: 0.499
[59,     1] loss: 0.492
[60,     1] loss: 0.488
[61,     1] loss: 0.481
[62,     1] loss: 0.473
[63,     1] loss: 0.464
[64,     1] loss: 0.457
[65,     1] loss: 0.443
[66,     1] loss: 0.435
[67,     1] loss: 0.426
[68,     1] loss: 0.421
[69,     1] loss: 0.419
[70,     1] loss: 0.434
[71,     1] loss: 0.416
[72,     1] loss: 0.409
[73,     1] loss: 0.401
[74,     1] loss: 0.386
[75,     1] loss: 0.372
[76,     1] loss: 0.355
[77,     1] loss: 0.345
[78,     1] loss: 0.343
[79,     1] loss: 0.389
[80,     1] loss: 0.387
[81,     1] loss: 0.426
[82,     1] loss: 0.426
[83,     1] loss: 0.472
[84,     1] loss: 0.506
[85,     1] loss: 0.477
[86,     1] loss: 0.460
[87,     1] loss: 0.469
[88,     1] loss: 0.473
[89,     1] loss: 0.470
[90,     1] loss: 0.467
[91,     1] loss: 0.464
[92,     1] loss: 0.456
[93,     1] loss: 0.454
[94,     1] loss: 0.446
[95,     1] loss: 0.441
[96,     1] loss: 0.434
[97,     1] loss: 0.427
[98,     1] loss: 0.436
[99,     1] loss: 0.425
[100,     1] loss: 0.418
[101,     1] loss: 0.414
[102,     1] loss: 0.432
[103,     1] loss: 0.421
[104,     1] loss: 0.421
[105,     1] loss: 0.406
[106,     1] loss: 0.404
[107,     1] loss: 0.440
[108,     1] loss: 0.448
[109,     1] loss: 0.508
[110,     1] loss: 0.465
[111,     1] loss: 0.459
[112,     1] loss: 0.457
[113,     1] loss: 0.453
[114,     1] loss: 0.449
[115,     1] loss: 0.447
[116,     1] loss: 0.443
[117,     1] loss: 0.446
[118,     1] loss: 0.457
[119,     1] loss: 0.456
[120,     1] loss: 0.462
[121,     1] loss: 0.470
[122,     1] loss: 0.470
[123,     1] loss: 0.477
[124,     1] loss: 0.473
[125,     1] loss: 0.472
Early stopping applied (best metric=0.40724503993988037)
Finished Training
Total time taken: 296.5368330478668
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.678
[3,     1] loss: 0.654
[4,     1] loss: 0.635
[5,     1] loss: 0.614
[6,     1] loss: 0.596
[7,     1] loss: 0.576
[8,     1] loss: 0.560
[9,     1] loss: 0.544
[10,     1] loss: 0.526
[11,     1] loss: 0.508
[12,     1] loss: 0.494
[13,     1] loss: 0.481
[14,     1] loss: 0.470
[15,     1] loss: 0.464
[16,     1] loss: 0.459
[17,     1] loss: 0.447
[18,     1] loss: 0.445
[19,     1] loss: 0.459
[20,     1] loss: 0.438
[21,     1] loss: 0.443
[22,     1] loss: 0.443
[23,     1] loss: 0.441
[24,     1] loss: 0.436
[25,     1] loss: 0.436
[26,     1] loss: 0.441
[27,     1] loss: 0.441
[28,     1] loss: 0.443
[29,     1] loss: 0.438
[30,     1] loss: 0.437
[31,     1] loss: 0.441
[32,     1] loss: 0.427
[33,     1] loss: 0.423
[34,     1] loss: 0.421
[35,     1] loss: 0.423
[36,     1] loss: 0.417
[37,     1] loss: 0.414
[38,     1] loss: 0.412
[39,     1] loss: 0.410
[40,     1] loss: 0.405
[41,     1] loss: 0.407
[42,     1] loss: 0.470
[43,     1] loss: 0.590
[44,     1] loss: 0.603
[45,     1] loss: 0.606
[46,     1] loss: 0.602
[47,     1] loss: 0.601
[48,     1] loss: 0.600
[49,     1] loss: 0.598
[50,     1] loss: 0.596
[51,     1] loss: 0.595
[52,     1] loss: 0.590
[53,     1] loss: 0.585
[54,     1] loss: 0.580
[55,     1] loss: 0.575
[56,     1] loss: 0.567
[57,     1] loss: 0.561
[58,     1] loss: 0.555
[59,     1] loss: 0.548
[60,     1] loss: 0.553
[61,     1] loss: 0.546
[62,     1] loss: 0.538
[63,     1] loss: 0.531
[64,     1] loss: 0.522
[65,     1] loss: 0.512
[66,     1] loss: 0.498
[67,     1] loss: 0.487
[68,     1] loss: 0.475
[69,     1] loss: 0.464
[70,     1] loss: 0.474
[71,     1] loss: 0.532
[72,     1] loss: 0.625
[73,     1] loss: 0.631
[74,     1] loss: 0.624
[75,     1] loss: 0.617
[76,     1] loss: 0.611
[77,     1] loss: 0.608
[78,     1] loss: 0.603
[79,     1] loss: 0.601
Early stopping applied (best metric=0.448575884103775)
Finished Training
Total time taken: 186.1579568386078
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.698
[3,     1] loss: 0.686
[4,     1] loss: 0.675
[5,     1] loss: 0.666
[6,     1] loss: 0.658
[7,     1] loss: 0.650
[8,     1] loss: 0.642
[9,     1] loss: 0.633
[10,     1] loss: 0.623
[11,     1] loss: 0.616
[12,     1] loss: 0.606
[13,     1] loss: 0.596
[14,     1] loss: 0.585
[15,     1] loss: 0.573
[16,     1] loss: 0.560
[17,     1] loss: 0.546
[18,     1] loss: 0.532
[19,     1] loss: 0.520
[20,     1] loss: 0.516
[21,     1] loss: 0.497
[22,     1] loss: 0.480
[23,     1] loss: 0.465
[24,     1] loss: 0.449
[25,     1] loss: 0.452
[26,     1] loss: 0.514
[27,     1] loss: 0.473
[28,     1] loss: 0.478
[29,     1] loss: 0.490
[30,     1] loss: 0.467
[31,     1] loss: 0.460
[32,     1] loss: 0.450
[33,     1] loss: 0.440
[34,     1] loss: 0.431
[35,     1] loss: 0.422
[36,     1] loss: 0.412
[37,     1] loss: 0.410
[38,     1] loss: 0.435
[39,     1] loss: 0.402
[40,     1] loss: 0.419
[41,     1] loss: 0.422
[42,     1] loss: 0.410
[43,     1] loss: 0.404
[44,     1] loss: 0.396
[45,     1] loss: 0.395
[46,     1] loss: 0.390
[47,     1] loss: 0.393
[48,     1] loss: 0.393
[49,     1] loss: 0.415
[50,     1] loss: 0.394
[51,     1] loss: 0.390
[52,     1] loss: 0.388
[53,     1] loss: 0.382
[54,     1] loss: 0.385
[55,     1] loss: 0.409
[56,     1] loss: 0.526
[57,     1] loss: 0.519
[58,     1] loss: 0.500
[59,     1] loss: 0.492
[60,     1] loss: 0.483
[61,     1] loss: 0.475
[62,     1] loss: 0.463
[63,     1] loss: 0.456
[64,     1] loss: 0.455
[65,     1] loss: 0.450
[66,     1] loss: 0.469
[67,     1] loss: 0.444
[68,     1] loss: 0.449
[69,     1] loss: 0.438
[70,     1] loss: 0.428
[71,     1] loss: 0.425
[72,     1] loss: 0.429
[73,     1] loss: 0.422
[74,     1] loss: 0.411
[75,     1] loss: 0.405
[76,     1] loss: 0.400
[77,     1] loss: 0.407
[78,     1] loss: 0.460
[79,     1] loss: 0.437
[80,     1] loss: 0.423
[81,     1] loss: 0.425
[82,     1] loss: 0.416
[83,     1] loss: 0.414
[84,     1] loss: 0.427
[85,     1] loss: 0.408
[86,     1] loss: 0.403
[87,     1] loss: 0.401
[88,     1] loss: 0.398
[89,     1] loss: 0.415
[90,     1] loss: 0.430
[91,     1] loss: 0.525
[92,     1] loss: 0.518
[93,     1] loss: 0.485
[94,     1] loss: 0.490
[95,     1] loss: 0.479
[96,     1] loss: 0.462
Early stopping applied (best metric=0.42537713050842285)
Finished Training
Total time taken: 226.17121052742004
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.696
[3,     1] loss: 0.680
[4,     1] loss: 0.672
[5,     1] loss: 0.665
[6,     1] loss: 0.658
[7,     1] loss: 0.651
[8,     1] loss: 0.644
[9,     1] loss: 0.637
[10,     1] loss: 0.630
[11,     1] loss: 0.622
[12,     1] loss: 0.614
[13,     1] loss: 0.605
[14,     1] loss: 0.596
[15,     1] loss: 0.586
[16,     1] loss: 0.576
[17,     1] loss: 0.565
[18,     1] loss: 0.554
[19,     1] loss: 0.541
[20,     1] loss: 0.531
[21,     1] loss: 0.516
[22,     1] loss: 0.500
[23,     1] loss: 0.485
[24,     1] loss: 0.470
[25,     1] loss: 0.453
[26,     1] loss: 0.439
[27,     1] loss: 0.427
[28,     1] loss: 0.450
[29,     1] loss: 0.671
[30,     1] loss: 0.617
[31,     1] loss: 0.562
[32,     1] loss: 0.547
[33,     1] loss: 0.558
[34,     1] loss: 0.562
[35,     1] loss: 0.560
[36,     1] loss: 0.556
[37,     1] loss: 0.550
[38,     1] loss: 0.544
[39,     1] loss: 0.536
[40,     1] loss: 0.527
[41,     1] loss: 0.517
[42,     1] loss: 0.513
[43,     1] loss: 0.523
[44,     1] loss: 0.509
[45,     1] loss: 0.503
[46,     1] loss: 0.483
[47,     1] loss: 0.469
[48,     1] loss: 0.456
[49,     1] loss: 0.448
[50,     1] loss: 0.432
[51,     1] loss: 0.430
[52,     1] loss: 0.412
[53,     1] loss: 0.426
[54,     1] loss: 0.468
[55,     1] loss: 0.508
[56,     1] loss: 0.492
[57,     1] loss: 0.494
[58,     1] loss: 0.484
[59,     1] loss: 0.479
[60,     1] loss: 0.471
[61,     1] loss: 0.463
[62,     1] loss: 0.456
[63,     1] loss: 0.442
[64,     1] loss: 0.427
[65,     1] loss: 0.422
[66,     1] loss: 0.450
[67,     1] loss: 0.419
[68,     1] loss: 0.416
[69,     1] loss: 0.422
[70,     1] loss: 0.403
[71,     1] loss: 0.388
[72,     1] loss: 0.389
[73,     1] loss: 0.415
[74,     1] loss: 0.403
[75,     1] loss: 0.404
[76,     1] loss: 0.414
[77,     1] loss: 0.392
[78,     1] loss: 0.382
[79,     1] loss: 0.372
[80,     1] loss: 0.362
[81,     1] loss: 0.364
[82,     1] loss: 0.375
[83,     1] loss: 0.359
[84,     1] loss: 0.365
[85,     1] loss: 0.360
[86,     1] loss: 0.365
[87,     1] loss: 0.353
[88,     1] loss: 0.348
[89,     1] loss: 0.349
[90,     1] loss: 0.360
[91,     1] loss: 0.358
[92,     1] loss: 0.406
[93,     1] loss: 0.370
[94,     1] loss: 0.389
[95,     1] loss: 0.386
[96,     1] loss: 0.382
[97,     1] loss: 0.376
[98,     1] loss: 0.370
[99,     1] loss: 0.368
[100,     1] loss: 0.363
[101,     1] loss: 0.367
[102,     1] loss: 0.381
[103,     1] loss: 0.362
[104,     1] loss: 0.366
[105,     1] loss: 0.376
[106,     1] loss: 0.361
[107,     1] loss: 0.385
[108,     1] loss: 0.393
[109,     1] loss: 0.374
[110,     1] loss: 0.385
[111,     1] loss: 0.406
[112,     1] loss: 0.383
[113,     1] loss: 0.374
[114,     1] loss: 0.371
[115,     1] loss: 0.363
[116,     1] loss: 0.367
[117,     1] loss: 0.378
[118,     1] loss: 0.396
[119,     1] loss: 0.397
[120,     1] loss: 0.388
[121,     1] loss: 0.388
[122,     1] loss: 0.380
[123,     1] loss: 0.377
[124,     1] loss: 0.379
[125,     1] loss: 0.417
[126,     1] loss: 0.412
[127,     1] loss: 0.423
[128,     1] loss: 0.399
[129,     1] loss: 0.410
[130,     1] loss: 0.418
[131,     1] loss: 0.406
[132,     1] loss: 0.427
[133,     1] loss: 0.428
[134,     1] loss: 0.424
[135,     1] loss: 0.423
[136,     1] loss: 0.416
[137,     1] loss: 0.414
[138,     1] loss: 0.412
[139,     1] loss: 0.411
[140,     1] loss: 0.408
[141,     1] loss: 0.412
[142,     1] loss: 0.421
[143,     1] loss: 0.445
[144,     1] loss: 0.446
[145,     1] loss: 0.447
[146,     1] loss: 0.449
[147,     1] loss: 0.452
Early stopping applied (best metric=0.4028571844100952)
Finished Training
Total time taken: 345.4418787956238
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.680
[3,     1] loss: 0.666
[4,     1] loss: 0.657
[5,     1] loss: 0.648
[6,     1] loss: 0.638
[7,     1] loss: 0.627
[8,     1] loss: 0.617
[9,     1] loss: 0.607
[10,     1] loss: 0.597
[11,     1] loss: 0.587
[12,     1] loss: 0.578
[13,     1] loss: 0.566
[14,     1] loss: 0.556
[15,     1] loss: 0.544
[16,     1] loss: 0.532
[17,     1] loss: 0.521
[18,     1] loss: 0.512
[19,     1] loss: 0.505
[20,     1] loss: 0.498
[21,     1] loss: 0.481
[22,     1] loss: 0.467
[23,     1] loss: 0.457
[24,     1] loss: 0.446
[25,     1] loss: 0.439
[26,     1] loss: 0.424
[27,     1] loss: 0.414
[28,     1] loss: 0.396
[29,     1] loss: 0.407
[30,     1] loss: 0.390
[31,     1] loss: 0.372
[32,     1] loss: 0.368
[33,     1] loss: 0.375
[34,     1] loss: 0.446
[35,     1] loss: 0.531
[36,     1] loss: 0.521
[37,     1] loss: 0.508
[38,     1] loss: 0.529
[39,     1] loss: 0.539
[40,     1] loss: 0.544
[41,     1] loss: 0.546
[42,     1] loss: 0.545
[43,     1] loss: 0.543
[44,     1] loss: 0.540
[45,     1] loss: 0.535
[46,     1] loss: 0.531
[47,     1] loss: 0.524
[48,     1] loss: 0.517
[49,     1] loss: 0.512
[50,     1] loss: 0.506
[51,     1] loss: 0.499
[52,     1] loss: 0.494
[53,     1] loss: 0.486
[54,     1] loss: 0.484
[55,     1] loss: 0.502
[56,     1] loss: 0.489
[57,     1] loss: 0.485
[58,     1] loss: 0.485
[59,     1] loss: 0.472
[60,     1] loss: 0.466
[61,     1] loss: 0.462
[62,     1] loss: 0.473
[63,     1] loss: 0.459
[64,     1] loss: 0.448
[65,     1] loss: 0.442
[66,     1] loss: 0.446
[67,     1] loss: 0.455
[68,     1] loss: 0.482
[69,     1] loss: 0.514
[70,     1] loss: 0.515
[71,     1] loss: 0.497
[72,     1] loss: 0.494
[73,     1] loss: 0.482
[74,     1] loss: 0.477
[75,     1] loss: 0.470
[76,     1] loss: 0.466
[77,     1] loss: 0.462
[78,     1] loss: 0.457
[79,     1] loss: 0.452
[80,     1] loss: 0.448
Early stopping applied (best metric=0.40365171432495117)
Finished Training
Total time taken: 189.8751482963562
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.661
[4,     1] loss: 0.646
[5,     1] loss: 0.632
[6,     1] loss: 0.620
[7,     1] loss: 0.606
[8,     1] loss: 0.595
[9,     1] loss: 0.583
[10,     1] loss: 0.570
[11,     1] loss: 0.560
[12,     1] loss: 0.547
[13,     1] loss: 0.536
[14,     1] loss: 0.523
[15,     1] loss: 0.511
[16,     1] loss: 0.502
[17,     1] loss: 0.494
[18,     1] loss: 0.483
[19,     1] loss: 0.471
[20,     1] loss: 0.463
[21,     1] loss: 0.455
[22,     1] loss: 0.443
[23,     1] loss: 0.434
[24,     1] loss: 0.425
[25,     1] loss: 0.417
[26,     1] loss: 0.409
[27,     1] loss: 0.399
[28,     1] loss: 0.387
[29,     1] loss: 0.380
[30,     1] loss: 0.373
[31,     1] loss: 0.372
[32,     1] loss: 0.544
[33,     1] loss: 0.643
[34,     1] loss: 0.547
[35,     1] loss: 0.583
[36,     1] loss: 0.579
[37,     1] loss: 0.574
[38,     1] loss: 0.582
[39,     1] loss: 0.585
[40,     1] loss: 0.583
[41,     1] loss: 0.586
[42,     1] loss: 0.587
[43,     1] loss: 0.586
[44,     1] loss: 0.587
[45,     1] loss: 0.585
[46,     1] loss: 0.584
[47,     1] loss: 0.582
[48,     1] loss: 0.579
[49,     1] loss: 0.577
[50,     1] loss: 0.575
[51,     1] loss: 0.572
[52,     1] loss: 0.568
[53,     1] loss: 0.566
[54,     1] loss: 0.563
[55,     1] loss: 0.566
[56,     1] loss: 0.564
[57,     1] loss: 0.563
[58,     1] loss: 0.559
[59,     1] loss: 0.554
[60,     1] loss: 0.550
[61,     1] loss: 0.546
[62,     1] loss: 0.537
[63,     1] loss: 0.532
[64,     1] loss: 0.525
[65,     1] loss: 0.514
[66,     1] loss: 0.505
[67,     1] loss: 0.494
[68,     1] loss: 0.489
[69,     1] loss: 0.478
[70,     1] loss: 0.479
[71,     1] loss: 0.519
[72,     1] loss: 0.491
[73,     1] loss: 0.469
[74,     1] loss: 0.457
[75,     1] loss: 0.452
[76,     1] loss: 0.441
[77,     1] loss: 0.422
[78,     1] loss: 0.408
[79,     1] loss: 0.391
[80,     1] loss: 0.378
[81,     1] loss: 0.362
[82,     1] loss: 0.347
[83,     1] loss: 0.334
[84,     1] loss: 0.321
[85,     1] loss: 0.319
[86,     1] loss: 0.374
[87,     1] loss: 0.547
[88,     1] loss: 0.452
[89,     1] loss: 0.444
[90,     1] loss: 0.426
[91,     1] loss: 0.410
[92,     1] loss: 0.396
[93,     1] loss: 0.383
[94,     1] loss: 0.374
[95,     1] loss: 0.361
[96,     1] loss: 0.353
[97,     1] loss: 0.343
[98,     1] loss: 0.333
[99,     1] loss: 0.323
[100,     1] loss: 0.344
[101,     1] loss: 0.346
[102,     1] loss: 0.354
[103,     1] loss: 0.329
[104,     1] loss: 0.334
[105,     1] loss: 0.320
[106,     1] loss: 0.316
[107,     1] loss: 0.303
[108,     1] loss: 0.299
[109,     1] loss: 0.292
[110,     1] loss: 0.287
[111,     1] loss: 0.305
[112,     1] loss: 0.306
[113,     1] loss: 0.313
[114,     1] loss: 0.306
[115,     1] loss: 0.300
[116,     1] loss: 0.293
[117,     1] loss: 0.297
[118,     1] loss: 0.292
[119,     1] loss: 0.310
[120,     1] loss: 0.364
[121,     1] loss: 0.457
[122,     1] loss: 0.415
[123,     1] loss: 0.413
[124,     1] loss: 0.394
[125,     1] loss: 0.387
[126,     1] loss: 0.382
[127,     1] loss: 0.383
[128,     1] loss: 0.373
[129,     1] loss: 0.366
[130,     1] loss: 0.361
[131,     1] loss: 0.361
[132,     1] loss: 0.355
[133,     1] loss: 0.353
[134,     1] loss: 0.360
[135,     1] loss: 0.351
[136,     1] loss: 0.345
[137,     1] loss: 0.341
[138,     1] loss: 0.340
[139,     1] loss: 0.371
[140,     1] loss: 0.350
[141,     1] loss: 0.347
[142,     1] loss: 0.347
[143,     1] loss: 0.376
[144,     1] loss: 0.365
[145,     1] loss: 0.361
[146,     1] loss: 0.385
[147,     1] loss: 0.444
[148,     1] loss: 0.404
[149,     1] loss: 0.410
[150,     1] loss: 0.413
[151,     1] loss: 0.398
[152,     1] loss: 0.394
[153,     1] loss: 0.384
[154,     1] loss: 0.379
[155,     1] loss: 0.372
[156,     1] loss: 0.369
[157,     1] loss: 0.368
[158,     1] loss: 0.364
[159,     1] loss: 0.364
[160,     1] loss: 0.377
[161,     1] loss: 0.362
[162,     1] loss: 0.360
[163,     1] loss: 0.354
Early stopping applied (best metric=0.4088498055934906)
Finished Training
Total time taken: 386.58898067474365
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.716
[2,     1] loss: 0.693
[3,     1] loss: 0.677
[4,     1] loss: 0.665
[5,     1] loss: 0.653
[6,     1] loss: 0.644
[7,     1] loss: 0.633
[8,     1] loss: 0.623
[9,     1] loss: 0.614
[10,     1] loss: 0.600
[11,     1] loss: 0.589
[12,     1] loss: 0.580
[13,     1] loss: 0.567
[14,     1] loss: 0.555
[15,     1] loss: 0.544
[16,     1] loss: 0.533
[17,     1] loss: 0.525
[18,     1] loss: 0.515
[19,     1] loss: 0.508
[20,     1] loss: 0.497
[21,     1] loss: 0.492
[22,     1] loss: 0.512
[23,     1] loss: 0.491
[24,     1] loss: 0.485
[25,     1] loss: 0.483
[26,     1] loss: 0.474
[27,     1] loss: 0.469
[28,     1] loss: 0.458
[29,     1] loss: 0.452
[30,     1] loss: 0.447
[31,     1] loss: 0.441
[32,     1] loss: 0.468
[33,     1] loss: 0.525
[34,     1] loss: 0.513
[35,     1] loss: 0.510
[36,     1] loss: 0.503
[37,     1] loss: 0.493
[38,     1] loss: 0.481
[39,     1] loss: 0.473
[40,     1] loss: 0.465
[41,     1] loss: 0.460
[42,     1] loss: 0.458
[43,     1] loss: 0.467
[44,     1] loss: 0.464
[45,     1] loss: 0.462
[46,     1] loss: 0.467
[47,     1] loss: 0.460
[48,     1] loss: 0.453
[49,     1] loss: 0.447
[50,     1] loss: 0.445
[51,     1] loss: 0.439
[52,     1] loss: 0.436
[53,     1] loss: 0.431
[54,     1] loss: 0.431
[55,     1] loss: 0.430
[56,     1] loss: 0.427
[57,     1] loss: 0.424
[58,     1] loss: 0.423
[59,     1] loss: 0.423
[60,     1] loss: 0.417
[61,     1] loss: 0.417
[62,     1] loss: 0.414
[63,     1] loss: 0.412
[64,     1] loss: 0.419
[65,     1] loss: 0.553
[66,     1] loss: 0.540
[67,     1] loss: 0.543
[68,     1] loss: 0.526
[69,     1] loss: 0.517
[70,     1] loss: 0.507
[71,     1] loss: 0.500
[72,     1] loss: 0.492
[73,     1] loss: 0.478
[74,     1] loss: 0.468
[75,     1] loss: 0.454
[76,     1] loss: 0.440
[77,     1] loss: 0.430
[78,     1] loss: 0.413
[79,     1] loss: 0.398
[80,     1] loss: 0.380
[81,     1] loss: 0.361
[82,     1] loss: 0.344
[83,     1] loss: 0.326
[84,     1] loss: 0.309
[85,     1] loss: 0.295
[86,     1] loss: 0.290
[87,     1] loss: 0.330
[88,     1] loss: 0.738
[89,     1] loss: 0.652
[90,     1] loss: 0.583
[91,     1] loss: 0.540
[92,     1] loss: 0.553
[93,     1] loss: 0.565
[94,     1] loss: 0.574
[95,     1] loss: 0.577
[96,     1] loss: 0.581
[97,     1] loss: 0.583
[98,     1] loss: 0.585
[99,     1] loss: 0.588
[100,     1] loss: 0.590
[101,     1] loss: 0.592
[102,     1] loss: 0.594
[103,     1] loss: 0.596
[104,     1] loss: 0.596
[105,     1] loss: 0.599
[106,     1] loss: 0.600
[107,     1] loss: 0.603
[108,     1] loss: 0.603
[109,     1] loss: 0.605
[110,     1] loss: 0.606
[111,     1] loss: 0.607
[112,     1] loss: 0.614
[113,     1] loss: 0.614
[114,     1] loss: 0.611
[115,     1] loss: 0.610
[116,     1] loss: 0.612
[117,     1] loss: 0.612
[118,     1] loss: 0.625
[119,     1] loss: 0.622
[120,     1] loss: 0.619
[121,     1] loss: 0.618
[122,     1] loss: 0.615
[123,     1] loss: 0.614
[124,     1] loss: 0.613
[125,     1] loss: 0.611
[126,     1] loss: 0.611
[127,     1] loss: 0.610
[128,     1] loss: 0.609
[129,     1] loss: 0.616
[130,     1] loss: 0.614
[131,     1] loss: 0.611
[132,     1] loss: 0.610
[133,     1] loss: 0.608
[134,     1] loss: 0.606
Early stopping applied (best metric=0.3992803394794464)
Finished Training
Total time taken: 322.98617577552795
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.662
[5,     1] loss: 0.656
[6,     1] loss: 0.650
[7,     1] loss: 0.643
[8,     1] loss: 0.635
[9,     1] loss: 0.629
[10,     1] loss: 0.622
[11,     1] loss: 0.613
[12,     1] loss: 0.605
[13,     1] loss: 0.595
[14,     1] loss: 0.587
[15,     1] loss: 0.576
[16,     1] loss: 0.566
[17,     1] loss: 0.556
[18,     1] loss: 0.546
[19,     1] loss: 0.535
[20,     1] loss: 0.522
[21,     1] loss: 0.511
[22,     1] loss: 0.499
[23,     1] loss: 0.486
[24,     1] loss: 0.473
[25,     1] loss: 0.463
[26,     1] loss: 0.461
[27,     1] loss: 0.579
[28,     1] loss: 0.509
[29,     1] loss: 0.556
[30,     1] loss: 0.530
[31,     1] loss: 0.532
[32,     1] loss: 0.528
[33,     1] loss: 0.521
[34,     1] loss: 0.514
[35,     1] loss: 0.503
[36,     1] loss: 0.492
[37,     1] loss: 0.481
[38,     1] loss: 0.468
[39,     1] loss: 0.458
[40,     1] loss: 0.445
[41,     1] loss: 0.433
[42,     1] loss: 0.421
[43,     1] loss: 0.409
[44,     1] loss: 0.399
[45,     1] loss: 0.385
[46,     1] loss: 0.374
[47,     1] loss: 0.363
[48,     1] loss: 0.353
[49,     1] loss: 0.416
[50,     1] loss: 0.494
[51,     1] loss: 0.561
[52,     1] loss: 0.487
[53,     1] loss: 0.514
[54,     1] loss: 0.518
[55,     1] loss: 0.514
[56,     1] loss: 0.508
[57,     1] loss: 0.497
[58,     1] loss: 0.490
[59,     1] loss: 0.482
[60,     1] loss: 0.476
[61,     1] loss: 0.470
[62,     1] loss: 0.465
[63,     1] loss: 0.458
[64,     1] loss: 0.457
[65,     1] loss: 0.467
[66,     1] loss: 0.471
[67,     1] loss: 0.475
[68,     1] loss: 0.468
[69,     1] loss: 0.461
[70,     1] loss: 0.456
[71,     1] loss: 0.450
[72,     1] loss: 0.442
[73,     1] loss: 0.434
[74,     1] loss: 0.430
[75,     1] loss: 0.425
[76,     1] loss: 0.426
[77,     1] loss: 0.446
[78,     1] loss: 0.483
[79,     1] loss: 0.455
[80,     1] loss: 0.441
[81,     1] loss: 0.436
[82,     1] loss: 0.432
[83,     1] loss: 0.424
[84,     1] loss: 0.418
[85,     1] loss: 0.422
[86,     1] loss: 0.414
[87,     1] loss: 0.421
[88,     1] loss: 0.419
[89,     1] loss: 0.416
[90,     1] loss: 0.417
[91,     1] loss: 0.414
[92,     1] loss: 0.404
[93,     1] loss: 0.406
[94,     1] loss: 0.415
[95,     1] loss: 0.403
[96,     1] loss: 0.397
[97,     1] loss: 0.405
[98,     1] loss: 0.393
[99,     1] loss: 0.390
[100,     1] loss: 0.404
[101,     1] loss: 0.435
[102,     1] loss: 0.441
[103,     1] loss: 0.432
[104,     1] loss: 0.420
[105,     1] loss: 0.418
[106,     1] loss: 0.413
[107,     1] loss: 0.410
[108,     1] loss: 0.402
[109,     1] loss: 0.398
[110,     1] loss: 0.397
[111,     1] loss: 0.407
[112,     1] loss: 0.432
[113,     1] loss: 0.422
[114,     1] loss: 0.430
[115,     1] loss: 0.429
[116,     1] loss: 0.424
[117,     1] loss: 0.417
[118,     1] loss: 0.414
[119,     1] loss: 0.413
[120,     1] loss: 0.411
[121,     1] loss: 0.408
[122,     1] loss: 0.404
[123,     1] loss: 0.403
[124,     1] loss: 0.402
[125,     1] loss: 0.402
[126,     1] loss: 0.405
[127,     1] loss: 0.415
[128,     1] loss: 0.404
[129,     1] loss: 0.413
[130,     1] loss: 0.416
[131,     1] loss: 0.412
[132,     1] loss: 0.407
[133,     1] loss: 0.404
[134,     1] loss: 0.398
[135,     1] loss: 0.400
[136,     1] loss: 0.424
[137,     1] loss: 0.489
[138,     1] loss: 0.527
[139,     1] loss: 0.500
[140,     1] loss: 0.477
[141,     1] loss: 0.485
[142,     1] loss: 0.475
[143,     1] loss: 0.476
[144,     1] loss: 0.473
[145,     1] loss: 0.473
[146,     1] loss: 0.471
[147,     1] loss: 0.468
[148,     1] loss: 0.470
[149,     1] loss: 0.471
[150,     1] loss: 0.473
[151,     1] loss: 0.489
[152,     1] loss: 0.490
[153,     1] loss: 0.490
[154,     1] loss: 0.488
Early stopping applied (best metric=0.3963213860988617)
Finished Training
Total time taken: 385.29733991622925
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.661
[5,     1] loss: 0.649
[6,     1] loss: 0.639
[7,     1] loss: 0.627
[8,     1] loss: 0.618
[9,     1] loss: 0.607
[10,     1] loss: 0.599
[11,     1] loss: 0.588
[12,     1] loss: 0.579
[13,     1] loss: 0.571
[14,     1] loss: 0.559
[15,     1] loss: 0.549
[16,     1] loss: 0.537
[17,     1] loss: 0.528
[18,     1] loss: 0.518
[19,     1] loss: 0.514
[20,     1] loss: 0.528
[21,     1] loss: 0.504
[22,     1] loss: 0.513
[23,     1] loss: 0.514
[24,     1] loss: 0.499
[25,     1] loss: 0.490
[26,     1] loss: 0.480
[27,     1] loss: 0.470
[28,     1] loss: 0.459
[29,     1] loss: 0.460
[30,     1] loss: 0.451
[31,     1] loss: 0.452
[32,     1] loss: 0.432
[33,     1] loss: 0.426
[34,     1] loss: 0.442
[35,     1] loss: 0.421
[36,     1] loss: 0.429
[37,     1] loss: 0.417
[38,     1] loss: 0.413
[39,     1] loss: 0.440
[40,     1] loss: 0.458
[41,     1] loss: 0.444
[42,     1] loss: 0.449
[43,     1] loss: 0.455
[44,     1] loss: 0.437
[45,     1] loss: 0.429
[46,     1] loss: 0.420
[47,     1] loss: 0.423
[48,     1] loss: 0.448
[49,     1] loss: 0.436
[50,     1] loss: 0.432
[51,     1] loss: 0.439
[52,     1] loss: 0.420
[53,     1] loss: 0.421
[54,     1] loss: 0.419
[55,     1] loss: 0.408
[56,     1] loss: 0.401
[57,     1] loss: 0.392
[58,     1] loss: 0.400
[59,     1] loss: 0.378
[60,     1] loss: 0.379
[61,     1] loss: 0.390
[62,     1] loss: 0.374
[63,     1] loss: 0.369
[64,     1] loss: 0.375
[65,     1] loss: 0.357
[66,     1] loss: 0.352
[67,     1] loss: 0.381
[68,     1] loss: 0.472
[69,     1] loss: 0.661
[70,     1] loss: 0.556
[71,     1] loss: 0.520
[72,     1] loss: 0.542
[73,     1] loss: 0.540
[74,     1] loss: 0.546
[75,     1] loss: 0.549
[76,     1] loss: 0.548
[77,     1] loss: 0.547
[78,     1] loss: 0.544
[79,     1] loss: 0.542
[80,     1] loss: 0.539
[81,     1] loss: 0.536
[82,     1] loss: 0.534
[83,     1] loss: 0.530
[84,     1] loss: 0.526
[85,     1] loss: 0.524
[86,     1] loss: 0.524
[87,     1] loss: 0.539
[88,     1] loss: 0.545
[89,     1] loss: 0.541
[90,     1] loss: 0.532
[91,     1] loss: 0.531
[92,     1] loss: 0.526
[93,     1] loss: 0.522
[94,     1] loss: 0.516
[95,     1] loss: 0.513
[96,     1] loss: 0.505
[97,     1] loss: 0.502
[98,     1] loss: 0.506
[99,     1] loss: 0.549
[100,     1] loss: 0.595
[101,     1] loss: 0.578
[102,     1] loss: 0.563
[103,     1] loss: 0.560
[104,     1] loss: 0.554
[105,     1] loss: 0.551
[106,     1] loss: 0.549
[107,     1] loss: 0.542
[108,     1] loss: 0.539
[109,     1] loss: 0.539
[110,     1] loss: 0.540
[111,     1] loss: 0.546
[112,     1] loss: 0.542
[113,     1] loss: 0.537
[114,     1] loss: 0.548
[115,     1] loss: 0.542
Early stopping applied (best metric=0.3962080776691437)
Finished Training
Total time taken: 275.7872676849365
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.658
[5,     1] loss: 0.647
[6,     1] loss: 0.637
[7,     1] loss: 0.627
[8,     1] loss: 0.617
[9,     1] loss: 0.607
[10,     1] loss: 0.597
[11,     1] loss: 0.590
[12,     1] loss: 0.582
[13,     1] loss: 0.574
[14,     1] loss: 0.564
[15,     1] loss: 0.555
[16,     1] loss: 0.548
[17,     1] loss: 0.557
[18,     1] loss: 0.539
[19,     1] loss: 0.565
[20,     1] loss: 0.557
[21,     1] loss: 0.561
[22,     1] loss: 0.546
[23,     1] loss: 0.546
[24,     1] loss: 0.537
[25,     1] loss: 0.531
[26,     1] loss: 0.524
[27,     1] loss: 0.518
[28,     1] loss: 0.523
[29,     1] loss: 0.530
[30,     1] loss: 0.518
[31,     1] loss: 0.515
[32,     1] loss: 0.509
[33,     1] loss: 0.500
[34,     1] loss: 0.498
[35,     1] loss: 0.504
[36,     1] loss: 0.523
[37,     1] loss: 0.510
[38,     1] loss: 0.504
[39,     1] loss: 0.494
[40,     1] loss: 0.489
[41,     1] loss: 0.484
[42,     1] loss: 0.480
[43,     1] loss: 0.499
[44,     1] loss: 0.496
[45,     1] loss: 0.493
[46,     1] loss: 0.482
[47,     1] loss: 0.473
[48,     1] loss: 0.472
[49,     1] loss: 0.466
[50,     1] loss: 0.469
[51,     1] loss: 0.449
[52,     1] loss: 0.446
[53,     1] loss: 0.457
[54,     1] loss: 0.437
[55,     1] loss: 0.427
[56,     1] loss: 0.413
[57,     1] loss: 0.406
[58,     1] loss: 0.416
[59,     1] loss: 0.412
[60,     1] loss: 0.410
[61,     1] loss: 0.440
[62,     1] loss: 0.407
[63,     1] loss: 0.403
[64,     1] loss: 0.383
[65,     1] loss: 0.368
[66,     1] loss: 0.355
[67,     1] loss: 0.341
[68,     1] loss: 0.342
[69,     1] loss: 0.327
[70,     1] loss: 0.364
[71,     1] loss: 0.352
[72,     1] loss: 0.348
[73,     1] loss: 0.367
[74,     1] loss: 0.352
[75,     1] loss: 0.350
[76,     1] loss: 0.345
[77,     1] loss: 0.336
[78,     1] loss: 0.330
[79,     1] loss: 0.326
[80,     1] loss: 0.321
[81,     1] loss: 0.313
[82,     1] loss: 0.309
[83,     1] loss: 0.304
[84,     1] loss: 0.319
[85,     1] loss: 0.356
[86,     1] loss: 0.480
[87,     1] loss: 0.378
[88,     1] loss: 0.401
[89,     1] loss: 0.391
[90,     1] loss: 0.404
[91,     1] loss: 0.400
[92,     1] loss: 0.404
[93,     1] loss: 0.396
[94,     1] loss: 0.395
[95,     1] loss: 0.390
[96,     1] loss: 0.396
[97,     1] loss: 0.391
[98,     1] loss: 0.439
[99,     1] loss: 0.418
[100,     1] loss: 0.424
[101,     1] loss: 0.430
[102,     1] loss: 0.428
[103,     1] loss: 0.425
[104,     1] loss: 0.428
[105,     1] loss: 0.423
[106,     1] loss: 0.425
[107,     1] loss: 0.425
[108,     1] loss: 0.427
[109,     1] loss: 0.426
[110,     1] loss: 0.427
[111,     1] loss: 0.436
[112,     1] loss: 0.449
[113,     1] loss: 0.487
[114,     1] loss: 0.586
[115,     1] loss: 0.558
[116,     1] loss: 0.552
[117,     1] loss: 0.537
[118,     1] loss: 0.531
[119,     1] loss: 0.530
[120,     1] loss: 0.526
[121,     1] loss: 0.527
[122,     1] loss: 0.521
[123,     1] loss: 0.518
[124,     1] loss: 0.519
[125,     1] loss: 0.511
[126,     1] loss: 0.508
[127,     1] loss: 0.516
Early stopping applied (best metric=0.42223185300827026)
Finished Training
Total time taken: 304.22497963905334
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.674
[4,     1] loss: 0.669
[5,     1] loss: 0.663
[6,     1] loss: 0.659
[7,     1] loss: 0.653
[8,     1] loss: 0.649
[9,     1] loss: 0.643
[10,     1] loss: 0.639
[11,     1] loss: 0.633
[12,     1] loss: 0.628
[13,     1] loss: 0.621
[14,     1] loss: 0.615
[15,     1] loss: 0.606
[16,     1] loss: 0.600
[17,     1] loss: 0.591
[18,     1] loss: 0.582
[19,     1] loss: 0.573
[20,     1] loss: 0.564
[21,     1] loss: 0.554
[22,     1] loss: 0.544
[23,     1] loss: 0.533
[24,     1] loss: 0.529
[25,     1] loss: 0.527
[26,     1] loss: 0.549
[27,     1] loss: 0.527
[28,     1] loss: 0.520
[29,     1] loss: 0.508
[30,     1] loss: 0.492
[31,     1] loss: 0.484
[32,     1] loss: 0.469
[33,     1] loss: 0.459
[34,     1] loss: 0.460
[35,     1] loss: 0.436
[36,     1] loss: 0.428
[37,     1] loss: 0.418
[38,     1] loss: 0.401
[39,     1] loss: 0.404
[40,     1] loss: 0.383
[41,     1] loss: 0.370
[42,     1] loss: 0.364
[43,     1] loss: 0.446
[44,     1] loss: 0.598
[45,     1] loss: 0.577
[46,     1] loss: 0.536
[47,     1] loss: 0.523
[48,     1] loss: 0.529
[49,     1] loss: 0.531
[50,     1] loss: 0.534
[51,     1] loss: 0.529
[52,     1] loss: 0.526
[53,     1] loss: 0.523
[54,     1] loss: 0.518
[55,     1] loss: 0.515
[56,     1] loss: 0.508
[57,     1] loss: 0.504
[58,     1] loss: 0.500
[59,     1] loss: 0.495
[60,     1] loss: 0.504
[61,     1] loss: 0.529
[62,     1] loss: 0.571
[63,     1] loss: 0.532
[64,     1] loss: 0.544
[65,     1] loss: 0.531
[66,     1] loss: 0.530
[67,     1] loss: 0.525
[68,     1] loss: 0.521
[69,     1] loss: 0.516
[70,     1] loss: 0.511
[71,     1] loss: 0.505
[72,     1] loss: 0.498
[73,     1] loss: 0.503
[74,     1] loss: 0.524
[75,     1] loss: 0.523
[76,     1] loss: 0.526
[77,     1] loss: 0.501
[78,     1] loss: 0.503
[79,     1] loss: 0.495
[80,     1] loss: 0.488
[81,     1] loss: 0.480
[82,     1] loss: 0.471
[83,     1] loss: 0.480
[84,     1] loss: 0.460
[85,     1] loss: 0.460
[86,     1] loss: 0.506
Early stopping applied (best metric=0.44110387563705444)
Finished Training
Total time taken: 207.72768688201904
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.685
[3,     1] loss: 0.667
[4,     1] loss: 0.653
[5,     1] loss: 0.640
[6,     1] loss: 0.625
[7,     1] loss: 0.611
[8,     1] loss: 0.600
[9,     1] loss: 0.584
[10,     1] loss: 0.570
[11,     1] loss: 0.555
[12,     1] loss: 0.540
[13,     1] loss: 0.527
[14,     1] loss: 0.514
[15,     1] loss: 0.501
[16,     1] loss: 0.492
[17,     1] loss: 0.486
[18,     1] loss: 0.491
[19,     1] loss: 0.471
[20,     1] loss: 0.465
[21,     1] loss: 0.459
[22,     1] loss: 0.453
[23,     1] loss: 0.446
[24,     1] loss: 0.441
[25,     1] loss: 0.435
[26,     1] loss: 0.429
[27,     1] loss: 0.425
[28,     1] loss: 0.419
[29,     1] loss: 0.430
[30,     1] loss: 0.509
[31,     1] loss: 0.544
[32,     1] loss: 0.541
[33,     1] loss: 0.545
[34,     1] loss: 0.526
[35,     1] loss: 0.517
[36,     1] loss: 0.508
[37,     1] loss: 0.503
[38,     1] loss: 0.494
[39,     1] loss: 0.484
[40,     1] loss: 0.475
[41,     1] loss: 0.467
[42,     1] loss: 0.462
[43,     1] loss: 0.455
[44,     1] loss: 0.448
[45,     1] loss: 0.444
[46,     1] loss: 0.443
[47,     1] loss: 0.438
[48,     1] loss: 0.434
[49,     1] loss: 0.434
[50,     1] loss: 0.429
[51,     1] loss: 0.423
[52,     1] loss: 0.424
[53,     1] loss: 0.431
[54,     1] loss: 0.474
[55,     1] loss: 0.488
[56,     1] loss: 0.462
[57,     1] loss: 0.459
[58,     1] loss: 0.441
[59,     1] loss: 0.433
[60,     1] loss: 0.425
[61,     1] loss: 0.420
[62,     1] loss: 0.430
[63,     1] loss: 0.441
[64,     1] loss: 0.455
[65,     1] loss: 0.440
[66,     1] loss: 0.430
[67,     1] loss: 0.418
[68,     1] loss: 0.406
[69,     1] loss: 0.395
[70,     1] loss: 0.385
[71,     1] loss: 0.375
[72,     1] loss: 0.363
[73,     1] loss: 0.352
[74,     1] loss: 0.340
[75,     1] loss: 0.327
[76,     1] loss: 0.312
[77,     1] loss: 0.300
[78,     1] loss: 0.286
[79,     1] loss: 0.269
[80,     1] loss: 0.259
[81,     1] loss: 0.245
[82,     1] loss: 0.237
[83,     1] loss: 0.229
[84,     1] loss: 0.239
[85,     1] loss: 0.293
[86,     1] loss: 0.561
[87,     1] loss: 0.438
[88,     1] loss: 0.429
[89,     1] loss: 0.444
[90,     1] loss: 0.410
[91,     1] loss: 0.448
[92,     1] loss: 0.427
[93,     1] loss: 0.430
[94,     1] loss: 0.412
[95,     1] loss: 0.402
[96,     1] loss: 0.390
[97,     1] loss: 0.382
[98,     1] loss: 0.370
[99,     1] loss: 0.358
[100,     1] loss: 0.345
[101,     1] loss: 0.332
[102,     1] loss: 0.334
[103,     1] loss: 0.314
[104,     1] loss: 0.342
[105,     1] loss: 0.337
[106,     1] loss: 0.348
[107,     1] loss: 0.331
[108,     1] loss: 0.326
[109,     1] loss: 0.319
[110,     1] loss: 0.304
[111,     1] loss: 0.296
[112,     1] loss: 0.291
[113,     1] loss: 0.289
[114,     1] loss: 0.293
[115,     1] loss: 0.270
[116,     1] loss: 0.274
[117,     1] loss: 0.283
[118,     1] loss: 0.288
[119,     1] loss: 0.291
[120,     1] loss: 0.300
[121,     1] loss: 0.325
[122,     1] loss: 0.317
[123,     1] loss: 0.324
[124,     1] loss: 0.305
[125,     1] loss: 0.298
[126,     1] loss: 0.384
[127,     1] loss: 0.526
[128,     1] loss: 0.503
[129,     1] loss: 0.511
[130,     1] loss: 0.479
[131,     1] loss: 0.470
[132,     1] loss: 0.469
[133,     1] loss: 0.474
[134,     1] loss: 0.477
[135,     1] loss: 0.475
[136,     1] loss: 0.474
[137,     1] loss: 0.468
[138,     1] loss: 0.466
[139,     1] loss: 0.461
[140,     1] loss: 0.457
[141,     1] loss: 0.453
[142,     1] loss: 0.460
[143,     1] loss: 0.472
[144,     1] loss: 0.469
[145,     1] loss: 0.457
[146,     1] loss: 0.464
[147,     1] loss: 0.457
[148,     1] loss: 0.451
[149,     1] loss: 0.450
[150,     1] loss: 0.460
[151,     1] loss: 0.452
[152,     1] loss: 0.463
[153,     1] loss: 0.446
[154,     1] loss: 0.457
[155,     1] loss: 0.466
[156,     1] loss: 0.454
[157,     1] loss: 0.474
[158,     1] loss: 0.499
[159,     1] loss: 0.485
[160,     1] loss: 0.496
[161,     1] loss: 0.495
[162,     1] loss: 0.473
[163,     1] loss: 0.472
[164,     1] loss: 0.473
[165,     1] loss: 0.472
[166,     1] loss: 0.473
[167,     1] loss: 0.474
[168,     1] loss: 0.477
Early stopping applied (best metric=0.39760977029800415)
Finished Training
Total time taken: 404.7773687839508
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.697
[3,     1] loss: 0.684
[4,     1] loss: 0.673
[5,     1] loss: 0.665
[6,     1] loss: 0.655
[7,     1] loss: 0.645
[8,     1] loss: 0.637
[9,     1] loss: 0.628
[10,     1] loss: 0.621
[11,     1] loss: 0.614
[12,     1] loss: 0.606
[13,     1] loss: 0.599
[14,     1] loss: 0.592
[15,     1] loss: 0.583
[16,     1] loss: 0.574
[17,     1] loss: 0.564
[18,     1] loss: 0.554
[19,     1] loss: 0.543
[20,     1] loss: 0.531
[21,     1] loss: 0.517
[22,     1] loss: 0.507
[23,     1] loss: 0.494
[24,     1] loss: 0.508
[25,     1] loss: 0.544
[26,     1] loss: 0.522
[27,     1] loss: 0.511
[28,     1] loss: 0.497
[29,     1] loss: 0.485
[30,     1] loss: 0.472
[31,     1] loss: 0.460
[32,     1] loss: 0.445
[33,     1] loss: 0.436
[34,     1] loss: 0.424
[35,     1] loss: 0.419
[36,     1] loss: 0.401
[37,     1] loss: 0.391
[38,     1] loss: 0.377
[39,     1] loss: 0.376
[40,     1] loss: 0.373
[41,     1] loss: 0.395
[42,     1] loss: 0.433
[43,     1] loss: 0.433
[44,     1] loss: 0.445
[45,     1] loss: 0.472
[46,     1] loss: 0.471
[47,     1] loss: 0.453
[48,     1] loss: 0.453
[49,     1] loss: 0.449
[50,     1] loss: 0.436
[51,     1] loss: 0.429
[52,     1] loss: 0.422
[53,     1] loss: 0.412
[54,     1] loss: 0.403
[55,     1] loss: 0.391
[56,     1] loss: 0.384
[57,     1] loss: 0.385
[58,     1] loss: 0.431
[59,     1] loss: 0.419
[60,     1] loss: 0.416
[61,     1] loss: 0.403
[62,     1] loss: 0.402
[63,     1] loss: 0.391
[64,     1] loss: 0.386
[65,     1] loss: 0.378
[66,     1] loss: 0.373
[67,     1] loss: 0.395
[68,     1] loss: 0.389
[69,     1] loss: 0.373
[70,     1] loss: 0.372
[71,     1] loss: 0.368
[72,     1] loss: 0.389
[73,     1] loss: 0.374
[74,     1] loss: 0.372
[75,     1] loss: 0.366
[76,     1] loss: 0.354
[77,     1] loss: 0.350
[78,     1] loss: 0.347
[79,     1] loss: 0.380
[80,     1] loss: 0.469
[81,     1] loss: 0.638
[82,     1] loss: 0.544
[83,     1] loss: 0.526
[84,     1] loss: 0.538
[85,     1] loss: 0.544
[86,     1] loss: 0.548
[87,     1] loss: 0.545
[88,     1] loss: 0.541
[89,     1] loss: 0.538
[90,     1] loss: 0.538
[91,     1] loss: 0.538
[92,     1] loss: 0.538
[93,     1] loss: 0.538
[94,     1] loss: 0.539
[95,     1] loss: 0.541
[96,     1] loss: 0.542
[97,     1] loss: 0.544
[98,     1] loss: 0.547
[99,     1] loss: 0.545
[100,     1] loss: 0.551
[101,     1] loss: 0.554
[102,     1] loss: 0.564
[103,     1] loss: 0.565
[104,     1] loss: 0.560
[105,     1] loss: 0.560
[106,     1] loss: 0.558
[107,     1] loss: 0.557
[108,     1] loss: 0.563
[109,     1] loss: 0.574
[110,     1] loss: 0.587
[111,     1] loss: 0.582
[112,     1] loss: 0.579
[113,     1] loss: 0.578
[114,     1] loss: 0.578
[115,     1] loss: 0.577
[116,     1] loss: 0.577
[117,     1] loss: 0.579
[118,     1] loss: 0.584
[119,     1] loss: 0.582
[120,     1] loss: 0.585
[121,     1] loss: 0.588
[122,     1] loss: 0.587
[123,     1] loss: 0.588
[124,     1] loss: 0.591
[125,     1] loss: 0.591
[126,     1] loss: 0.590
[127,     1] loss: 0.590
Early stopping applied (best metric=0.3939439356327057)
Finished Training
Total time taken: 307.84115839004517
{'Hydroxylation-P Validation Accuracy': 0.7786782193797269, 'Hydroxylation-P Validation Sensitivity': 0.7626349206349207, 'Hydroxylation-P Validation Specificity': 0.7821696842735298, 'Hydroxylation-P Validation Precision': 0.45823091566381363, 'Hydroxylation-P AUC ROC': 0.8469759078353273, 'Hydroxylation-P AUC PR': 0.6136705441849851, 'Hydroxylation-P MCC': 0.4649683585793647, 'Hydroxylation-P F1': 0.5622646998337826, 'Validation Loss (Hydroxylation-P)': 0.41280925273895264, 'Validation Loss (total)': 0.41280925273895264}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009590116407343299,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2851551079,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.930616687760736}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.671
[3,     1] loss: 0.619
[4,     1] loss: 0.568
[5,     1] loss: 0.507
[6,     1] loss: 0.442
[7,     1] loss: 0.377
[8,     1] loss: 0.332
[9,     1] loss: 0.305
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005988780325311416,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3000354741,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.194131945498285}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.674
[3,     1] loss: 0.654
[4,     1] loss: 0.639
[5,     1] loss: 0.627
[6,     1] loss: 0.614
[7,     1] loss: 0.603
[8,     1] loss: 0.593
[9,     1] loss: 0.580
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005418093677927163,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2494129887,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.147028505350852}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.697
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004194999013396996,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1664157982,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.314163493137176}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028755426672541803,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 171191836,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.85988768046418}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.695
[3,     1] loss: 0.681
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00440499775780798,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2352365414,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.079475168941059}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.679
[3,     1] loss: 0.654
[4,     1] loss: 0.631
[5,     1] loss: 0.606
[6,     1] loss: 0.576
[7,     1] loss: 0.552
[8,     1] loss: 0.522
[9,     1] loss: 0.494
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00550568266205203,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 553781494,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.7526881579406526}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.676
[3,     1] loss: 0.630
[4,     1] loss: 0.569
[5,     1] loss: 0.505
[6,     1] loss: 0.431
[7,     1] loss: 0.354
[8,     1] loss: 0.287
[9,     1] loss: 0.212
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006935284930681024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3631575050,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.913755452347935}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.663
[3,     1] loss: 0.625
[4,     1] loss: 0.589
[5,     1] loss: 0.556
[6,     1] loss: 0.522
[7,     1] loss: 0.482
[8,     1] loss: 0.443
[9,     1] loss: 0.400
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035828677684670946,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2278757201,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.869819563576304}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.678
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005327954301141848,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3852642185,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.9225115941463695}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.691
[3,     1] loss: 0.660
[4,     1] loss: 0.624
[5,     1] loss: 0.583
[6,     1] loss: 0.537
[7,     1] loss: 0.485
[8,     1] loss: 0.427
[9,     1] loss: 0.376
[10,     1] loss: 0.314
[11,     1] loss: 0.260
[12,     1] loss: 0.209
[13,     1] loss: 0.153
[14,     1] loss: 0.155
[15,     1] loss: 0.175
[16,     1] loss: 0.099
[17,     1] loss: 0.108
[18,     1] loss: 0.083
[19,     1] loss: 0.063
[20,     1] loss: 0.047
[21,     1] loss: 0.037
[22,     1] loss: 0.029
[23,     1] loss: 0.027
[24,     1] loss: 0.018
[25,     1] loss: 0.016
[26,     1] loss: 0.014
[27,     1] loss: 0.011
[28,     1] loss: 0.010
[29,     1] loss: 0.010
[30,     1] loss: 0.010
[31,     1] loss: 0.008
[32,     1] loss: 0.008
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
[60,     1] loss: 0.005
Early stopping applied (best metric=0.33580389618873596)
Finished Training
Total time taken: 147.20036602020264
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.662
[4,     1] loss: 0.634
[5,     1] loss: 0.599
[6,     1] loss: 0.560
[7,     1] loss: 0.507
[8,     1] loss: 0.449
[9,     1] loss: 0.392
[10,     1] loss: 0.336
[11,     1] loss: 0.290
[12,     1] loss: 0.260
[13,     1] loss: 0.234
[14,     1] loss: 0.174
[15,     1] loss: 0.161
[16,     1] loss: 0.122
[17,     1] loss: 0.094
[18,     1] loss: 0.078
[19,     1] loss: 0.064
[20,     1] loss: 0.053
[21,     1] loss: 0.047
[22,     1] loss: 0.036
[23,     1] loss: 0.032
[24,     1] loss: 0.027
[25,     1] loss: 0.022
[26,     1] loss: 0.020
[27,     1] loss: 0.019
[28,     1] loss: 0.018
[29,     1] loss: 0.016
[30,     1] loss: 0.015
[31,     1] loss: 0.015
[32,     1] loss: 0.014
[33,     1] loss: 0.014
[34,     1] loss: 0.013
[35,     1] loss: 0.013
[36,     1] loss: 0.012
[37,     1] loss: 0.012
[38,     1] loss: 0.012
[39,     1] loss: 0.012
[40,     1] loss: 0.012
[41,     1] loss: 0.012
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
Early stopping applied (best metric=0.45037680864334106)
Finished Training
Total time taken: 138.20608067512512
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.667
[3,     1] loss: 0.625
[4,     1] loss: 0.577
[5,     1] loss: 0.524
[6,     1] loss: 0.464
[7,     1] loss: 0.395
[8,     1] loss: 0.328
[9,     1] loss: 0.258
[10,     1] loss: 0.186
[11,     1] loss: 0.134
[12,     1] loss: 0.095
[13,     1] loss: 0.068
[14,     1] loss: 0.047
[15,     1] loss: 0.036
[16,     1] loss: 0.024
[17,     1] loss: 0.018
[18,     1] loss: 0.011
[19,     1] loss: 0.009
[20,     1] loss: 0.006
[21,     1] loss: 0.005
[22,     1] loss: 0.004
[23,     1] loss: 0.004
[24,     1] loss: 0.004
[25,     1] loss: 0.003
[26,     1] loss: 0.004
[27,     1] loss: 0.004
[28,     1] loss: 0.004
[29,     1] loss: 0.004
[30,     1] loss: 0.005
[31,     1] loss: 0.005
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.007
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.009
[38,     1] loss: 0.010
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.009
[44,     1] loss: 0.008
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.006
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.31139522790908813)
Finished Training
Total time taken: 148.72768878936768
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.673
[3,     1] loss: 0.638
[4,     1] loss: 0.601
[5,     1] loss: 0.556
[6,     1] loss: 0.503
[7,     1] loss: 0.442
[8,     1] loss: 0.374
[9,     1] loss: 0.308
[10,     1] loss: 0.247
[11,     1] loss: 0.196
[12,     1] loss: 0.156
[13,     1] loss: 0.128
[14,     1] loss: 0.111
[15,     1] loss: 0.149
[16,     1] loss: 0.189
[17,     1] loss: 0.096
[18,     1] loss: 0.134
[19,     1] loss: 0.051
[20,     1] loss: 0.062
[21,     1] loss: 0.060
[22,     1] loss: 0.041
[23,     1] loss: 0.035
[24,     1] loss: 0.034
[25,     1] loss: 0.032
[26,     1] loss: 0.030
[27,     1] loss: 0.027
[28,     1] loss: 0.024
[29,     1] loss: 0.022
[30,     1] loss: 0.019
[31,     1] loss: 0.018
[32,     1] loss: 0.017
[33,     1] loss: 0.016
[34,     1] loss: 0.014
[35,     1] loss: 0.012
[36,     1] loss: 0.013
[37,     1] loss: 0.012
[38,     1] loss: 0.011
[39,     1] loss: 0.012
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.011
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.008
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.008
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.008
[63,     1] loss: 0.008
[64,     1] loss: 0.007
[65,     1] loss: 0.008
Early stopping applied (best metric=0.4052540361881256)
Finished Training
Total time taken: 159.9837303161621
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.672
[3,     1] loss: 0.636
[4,     1] loss: 0.596
[5,     1] loss: 0.553
[6,     1] loss: 0.513
[7,     1] loss: 0.469
[8,     1] loss: 0.418
[9,     1] loss: 0.371
[10,     1] loss: 0.323
[11,     1] loss: 0.277
[12,     1] loss: 0.232
[13,     1] loss: 0.188
[14,     1] loss: 0.162
[15,     1] loss: 0.133
[16,     1] loss: 0.107
[17,     1] loss: 0.091
[18,     1] loss: 0.106
[19,     1] loss: 0.095
[20,     1] loss: 0.096
[21,     1] loss: 0.087
[22,     1] loss: 0.064
[23,     1] loss: 0.052
[24,     1] loss: 0.048
[25,     1] loss: 0.042
[26,     1] loss: 0.037
[27,     1] loss: 0.032
[28,     1] loss: 0.029
[29,     1] loss: 0.025
[30,     1] loss: 0.022
[31,     1] loss: 0.018
[32,     1] loss: 0.017
[33,     1] loss: 0.016
[34,     1] loss: 0.015
[35,     1] loss: 0.014
[36,     1] loss: 0.014
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.010
[43,     1] loss: 0.009
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.007
[59,     1] loss: 0.007
[60,     1] loss: 0.007
[61,     1] loss: 0.007
[62,     1] loss: 0.006
Early stopping applied (best metric=0.34168535470962524)
Finished Training
Total time taken: 152.84065628051758
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.665
[3,     1] loss: 0.624
[4,     1] loss: 0.581
[5,     1] loss: 0.529
[6,     1] loss: 0.471
[7,     1] loss: 0.404
[8,     1] loss: 0.337
[9,     1] loss: 0.271
[10,     1] loss: 0.211
[11,     1] loss: 0.171
[12,     1] loss: 0.136
[13,     1] loss: 0.104
[14,     1] loss: 0.069
[15,     1] loss: 0.069
[16,     1] loss: 0.050
[17,     1] loss: 0.045
[18,     1] loss: 0.027
[19,     1] loss: 0.030
[20,     1] loss: 0.020
[21,     1] loss: 0.015
[22,     1] loss: 0.014
[23,     1] loss: 0.010
[24,     1] loss: 0.008
[25,     1] loss: 0.007
[26,     1] loss: 0.007
[27,     1] loss: 0.006
[28,     1] loss: 0.006
[29,     1] loss: 0.006
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
Early stopping applied (best metric=0.415394127368927)
Finished Training
Total time taken: 144.08524107933044
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.672
[3,     1] loss: 0.630
[4,     1] loss: 0.580
[5,     1] loss: 0.526
[6,     1] loss: 0.465
[7,     1] loss: 0.396
[8,     1] loss: 0.327
[9,     1] loss: 0.271
[10,     1] loss: 0.218
[11,     1] loss: 0.174
[12,     1] loss: 0.145
[13,     1] loss: 0.128
[14,     1] loss: 0.104
[15,     1] loss: 0.107
[16,     1] loss: 0.085
[17,     1] loss: 0.069
[18,     1] loss: 0.063
[19,     1] loss: 0.056
[20,     1] loss: 0.060
[21,     1] loss: 0.039
[22,     1] loss: 0.047
[23,     1] loss: 0.030
[24,     1] loss: 0.034
[25,     1] loss: 0.022
[26,     1] loss: 0.030
[27,     1] loss: 0.018
[28,     1] loss: 0.019
[29,     1] loss: 0.017
[30,     1] loss: 0.015
[31,     1] loss: 0.014
[32,     1] loss: 0.013
[33,     1] loss: 0.014
[34,     1] loss: 0.013
[35,     1] loss: 0.011
[36,     1] loss: 0.012
[37,     1] loss: 0.011
[38,     1] loss: 0.011
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.009
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.008
Early stopping applied (best metric=0.4730588495731354)
Finished Training
Total time taken: 138.92659759521484
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.684
[3,     1] loss: 0.644
[4,     1] loss: 0.602
[5,     1] loss: 0.556
[6,     1] loss: 0.509
[7,     1] loss: 0.452
[8,     1] loss: 0.387
[9,     1] loss: 0.327
[10,     1] loss: 0.266
[11,     1] loss: 0.210
[12,     1] loss: 0.169
[13,     1] loss: 0.140
[14,     1] loss: 0.098
[15,     1] loss: 0.083
[16,     1] loss: 0.100
[17,     1] loss: 0.055
[18,     1] loss: 0.049
[19,     1] loss: 0.033
[20,     1] loss: 0.025
[21,     1] loss: 0.022
[22,     1] loss: 0.016
[23,     1] loss: 0.014
[24,     1] loss: 0.012
[25,     1] loss: 0.010
[26,     1] loss: 0.009
[27,     1] loss: 0.009
[28,     1] loss: 0.008
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.007
[32,     1] loss: 0.007
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.009
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.009
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.006
[53,     1] loss: 0.007
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.006
Early stopping applied (best metric=0.4031641483306885)
Finished Training
Total time taken: 143.89338779449463
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.677
[3,     1] loss: 0.631
[4,     1] loss: 0.592
[5,     1] loss: 0.552
[6,     1] loss: 0.511
[7,     1] loss: 0.469
[8,     1] loss: 0.419
[9,     1] loss: 0.368
[10,     1] loss: 0.322
[11,     1] loss: 0.289
[12,     1] loss: 0.254
[13,     1] loss: 0.225
[14,     1] loss: 0.201
[15,     1] loss: 0.176
[16,     1] loss: 0.153
[17,     1] loss: 0.134
[18,     1] loss: 0.114
[19,     1] loss: 0.103
[20,     1] loss: 0.092
[21,     1] loss: 0.080
[22,     1] loss: 0.074
[23,     1] loss: 0.070
[24,     1] loss: 0.064
[25,     1] loss: 0.058
[26,     1] loss: 0.051
[27,     1] loss: 0.050
[28,     1] loss: 0.041
[29,     1] loss: 0.031
[30,     1] loss: 0.029
[31,     1] loss: 0.024
[32,     1] loss: 0.023
[33,     1] loss: 0.020
[34,     1] loss: 0.021
[35,     1] loss: 0.019
[36,     1] loss: 0.019
[37,     1] loss: 0.019
[38,     1] loss: 0.019
[39,     1] loss: 0.018
[40,     1] loss: 0.018
[41,     1] loss: 0.018
[42,     1] loss: 0.017
[43,     1] loss: 0.018
[44,     1] loss: 0.018
[45,     1] loss: 0.017
[46,     1] loss: 0.016
[47,     1] loss: 0.016
[48,     1] loss: 0.016
[49,     1] loss: 0.016
[50,     1] loss: 0.016
[51,     1] loss: 0.015
[52,     1] loss: 0.015
[53,     1] loss: 0.015
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.014
[57,     1] loss: 0.014
[58,     1] loss: 0.014
Early stopping applied (best metric=0.42764830589294434)
Finished Training
Total time taken: 144.31527423858643
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.664
[3,     1] loss: 0.610
[4,     1] loss: 0.564
[5,     1] loss: 0.505
[6,     1] loss: 0.459
[7,     1] loss: 0.415
[8,     1] loss: 0.368
[9,     1] loss: 0.321
[10,     1] loss: 0.276
[11,     1] loss: 0.230
[12,     1] loss: 0.188
[13,     1] loss: 0.152
[14,     1] loss: 0.127
[15,     1] loss: 0.115
[16,     1] loss: 0.089
[17,     1] loss: 0.069
[18,     1] loss: 0.055
[19,     1] loss: 0.051
[20,     1] loss: 0.036
[21,     1] loss: 0.035
[22,     1] loss: 0.027
[23,     1] loss: 0.021
[24,     1] loss: 0.021
[25,     1] loss: 0.014
[26,     1] loss: 0.011
[27,     1] loss: 0.012
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.008
[32,     1] loss: 0.007
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.009
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.006
Early stopping applied (best metric=0.45309537649154663)
Finished Training
Total time taken: 144.41308665275574
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.694
[3,     1] loss: 0.670
[4,     1] loss: 0.640
[5,     1] loss: 0.607
[6,     1] loss: 0.566
[7,     1] loss: 0.520
[8,     1] loss: 0.469
[9,     1] loss: 0.414
[10,     1] loss: 0.355
[11,     1] loss: 0.292
[12,     1] loss: 0.251
[13,     1] loss: 0.205
[14,     1] loss: 0.166
[15,     1] loss: 0.140
[16,     1] loss: 0.113
[17,     1] loss: 0.094
[18,     1] loss: 0.064
[19,     1] loss: 0.044
[20,     1] loss: 0.038
[21,     1] loss: 0.028
[22,     1] loss: 0.021
[23,     1] loss: 0.016
[24,     1] loss: 0.014
[25,     1] loss: 0.011
[26,     1] loss: 0.010
[27,     1] loss: 0.008
[28,     1] loss: 0.007
[29,     1] loss: 0.006
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.009
[41,     1] loss: 0.009
[42,     1] loss: 0.009
[43,     1] loss: 0.010
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.008
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
Early stopping applied (best metric=0.4233608841896057)
Finished Training
Total time taken: 146.97526216506958
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.665
[3,     1] loss: 0.607
[4,     1] loss: 0.563
[5,     1] loss: 0.519
[6,     1] loss: 0.483
[7,     1] loss: 0.432
[8,     1] loss: 0.383
[9,     1] loss: 0.337
[10,     1] loss: 0.308
[11,     1] loss: 0.282
[12,     1] loss: 0.255
[13,     1] loss: 0.229
[14,     1] loss: 0.203
[15,     1] loss: 0.179
[16,     1] loss: 0.157
[17,     1] loss: 0.139
[18,     1] loss: 0.123
[19,     1] loss: 0.102
[20,     1] loss: 0.096
[21,     1] loss: 0.081
[22,     1] loss: 0.061
[23,     1] loss: 0.049
[24,     1] loss: 0.040
[25,     1] loss: 0.034
[26,     1] loss: 0.027
[27,     1] loss: 0.020
[28,     1] loss: 0.017
[29,     1] loss: 0.015
[30,     1] loss: 0.014
[31,     1] loss: 0.011
[32,     1] loss: 0.010
[33,     1] loss: 0.010
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.009
[37,     1] loss: 0.009
[38,     1] loss: 0.008
[39,     1] loss: 0.009
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.009
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.007
[58,     1] loss: 0.007
[59,     1] loss: 0.008
[60,     1] loss: 0.007
[61,     1] loss: 0.007
[62,     1] loss: 0.007
[63,     1] loss: 0.008
Early stopping applied (best metric=0.42675113677978516)
Finished Training
Total time taken: 157.1110863685608
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.668
[4,     1] loss: 0.643
[5,     1] loss: 0.607
[6,     1] loss: 0.566
[7,     1] loss: 0.520
[8,     1] loss: 0.466
[9,     1] loss: 0.402
[10,     1] loss: 0.340
[11,     1] loss: 0.275
[12,     1] loss: 0.218
[13,     1] loss: 0.161
[14,     1] loss: 0.132
[15,     1] loss: 0.104
[16,     1] loss: 0.077
[17,     1] loss: 0.069
[18,     1] loss: 0.047
[19,     1] loss: 0.036
[20,     1] loss: 0.025
[21,     1] loss: 0.021
[22,     1] loss: 0.014
[23,     1] loss: 0.011
[24,     1] loss: 0.009
[25,     1] loss: 0.006
[26,     1] loss: 0.009
[27,     1] loss: 0.011
[28,     1] loss: 0.006
[29,     1] loss: 0.016
[30,     1] loss: 0.086
[31,     1] loss: 0.152
[32,     1] loss: 0.011
[33,     1] loss: 0.043
[34,     1] loss: 0.074
[35,     1] loss: 0.023
[36,     1] loss: 0.035
[37,     1] loss: 0.030
[38,     1] loss: 0.032
[39,     1] loss: 0.032
[40,     1] loss: 0.030
[41,     1] loss: 0.027
[42,     1] loss: 0.026
[43,     1] loss: 0.021
[44,     1] loss: 0.019
[45,     1] loss: 0.016
[46,     1] loss: 0.014
[47,     1] loss: 0.012
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.009
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.008
[58,     1] loss: 0.007
[59,     1] loss: 0.008
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.009
Early stopping applied (best metric=0.2697927951812744)
Finished Training
Total time taken: 154.87848544120789
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.661
[3,     1] loss: 0.605
[4,     1] loss: 0.560
[5,     1] loss: 0.532
[6,     1] loss: 0.495
[7,     1] loss: 0.465
[8,     1] loss: 0.417
[9,     1] loss: 0.372
[10,     1] loss: 0.343
[11,     1] loss: 0.325
[12,     1] loss: 0.308
[13,     1] loss: 0.291
[14,     1] loss: 0.273
[15,     1] loss: 0.247
[16,     1] loss: 0.223
[17,     1] loss: 0.199
[18,     1] loss: 0.171
[19,     1] loss: 0.147
[20,     1] loss: 0.126
[21,     1] loss: 0.106
[22,     1] loss: 0.089
[23,     1] loss: 0.074
[24,     1] loss: 0.064
[25,     1] loss: 0.055
[26,     1] loss: 0.047
[27,     1] loss: 0.043
[28,     1] loss: 0.035
[29,     1] loss: 0.033
[30,     1] loss: 0.063
[31,     1] loss: 0.059
[32,     1] loss: 0.082
[33,     1] loss: 0.063
[34,     1] loss: 0.074
[35,     1] loss: 0.058
[36,     1] loss: 0.082
[37,     1] loss: 0.063
[38,     1] loss: 0.093
[39,     1] loss: 0.068
[40,     1] loss: 0.070
[41,     1] loss: 0.100
[42,     1] loss: 0.077
[43,     1] loss: 0.063
[44,     1] loss: 0.066
[45,     1] loss: 0.061
[46,     1] loss: 0.057
[47,     1] loss: 0.042
[48,     1] loss: 0.033
[49,     1] loss: 0.026
[50,     1] loss: 0.021
[51,     1] loss: 0.016
[52,     1] loss: 0.013
[53,     1] loss: 0.011
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
Early stopping applied (best metric=0.4609183967113495)
Finished Training
Total time taken: 148.16270470619202
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.647
[4,     1] loss: 0.609
[5,     1] loss: 0.562
[6,     1] loss: 0.510
[7,     1] loss: 0.453
[8,     1] loss: 0.391
[9,     1] loss: 0.335
[10,     1] loss: 0.281
[11,     1] loss: 0.228
[12,     1] loss: 0.181
[13,     1] loss: 0.138
[14,     1] loss: 0.130
[15,     1] loss: 0.109
[16,     1] loss: 0.094
[17,     1] loss: 0.087
[18,     1] loss: 0.072
[19,     1] loss: 0.053
[20,     1] loss: 0.044
[21,     1] loss: 0.035
[22,     1] loss: 0.027
[23,     1] loss: 0.022
[24,     1] loss: 0.020
[25,     1] loss: 0.017
[26,     1] loss: 0.015
[27,     1] loss: 0.014
[28,     1] loss: 0.013
[29,     1] loss: 0.011
[30,     1] loss: 0.010
[31,     1] loss: 0.010
[32,     1] loss: 0.009
[33,     1] loss: 0.009
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.009
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.009
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.007
[51,     1] loss: 0.008
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
[60,     1] loss: 0.006
Early stopping applied (best metric=0.4480420649051666)
Finished Training
Total time taken: 150.34093141555786
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.615
[5,     1] loss: 0.568
[6,     1] loss: 0.511
[7,     1] loss: 0.445
[8,     1] loss: 0.372
[9,     1] loss: 0.293
[10,     1] loss: 0.232
[11,     1] loss: 0.195
[12,     1] loss: 0.168
[13,     1] loss: 0.133
[14,     1] loss: 0.094
[15,     1] loss: 0.069
[16,     1] loss: 0.053
[17,     1] loss: 0.050
[18,     1] loss: 0.034
[19,     1] loss: 0.027
[20,     1] loss: 0.019
[21,     1] loss: 0.015
[22,     1] loss: 0.011
[23,     1] loss: 0.008
[24,     1] loss: 0.007
[25,     1] loss: 0.007
[26,     1] loss: 0.006
[27,     1] loss: 0.006
[28,     1] loss: 0.006
[29,     1] loss: 0.006
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.007
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.009
[42,     1] loss: 0.009
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.3632250130176544)
Finished Training
Total time taken: 150.7301890850067
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.679
[3,     1] loss: 0.636
[4,     1] loss: 0.591
[5,     1] loss: 0.547
[6,     1] loss: 0.506
[7,     1] loss: 0.466
[8,     1] loss: 0.403
[9,     1] loss: 0.354
[10,     1] loss: 0.304
[11,     1] loss: 0.260
[12,     1] loss: 0.216
[13,     1] loss: 0.195
[14,     1] loss: 0.166
[15,     1] loss: 0.148
[16,     1] loss: 0.131
[17,     1] loss: 0.113
[18,     1] loss: 0.093
[19,     1] loss: 0.094
[20,     1] loss: 0.065
[21,     1] loss: 0.056
[22,     1] loss: 0.064
[23,     1] loss: 0.042
[24,     1] loss: 0.119
[25,     1] loss: 0.042
[26,     1] loss: 0.081
[27,     1] loss: 0.058
[28,     1] loss: 0.043
[29,     1] loss: 0.043
[30,     1] loss: 0.042
[31,     1] loss: 0.040
[32,     1] loss: 0.036
[33,     1] loss: 0.033
[34,     1] loss: 0.030
[35,     1] loss: 0.027
[36,     1] loss: 0.025
[37,     1] loss: 0.022
[38,     1] loss: 0.021
[39,     1] loss: 0.020
[40,     1] loss: 0.018
[41,     1] loss: 0.016
[42,     1] loss: 0.015
[43,     1] loss: 0.014
[44,     1] loss: 0.014
[45,     1] loss: 0.013
[46,     1] loss: 0.012
[47,     1] loss: 0.012
[48,     1] loss: 0.012
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.009
[59,     1] loss: 0.010
[60,     1] loss: 0.009
[61,     1] loss: 0.009
[62,     1] loss: 0.008
Early stopping applied (best metric=0.3234716057777405)
Finished Training
Total time taken: 155.72175073623657
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.653
[4,     1] loss: 0.615
[5,     1] loss: 0.576
[6,     1] loss: 0.531
[7,     1] loss: 0.472
[8,     1] loss: 0.411
[9,     1] loss: 0.351
[10,     1] loss: 0.290
[11,     1] loss: 0.248
[12,     1] loss: 0.197
[13,     1] loss: 0.154
[14,     1] loss: 0.112
[15,     1] loss: 0.078
[16,     1] loss: 0.082
[17,     1] loss: 0.044
[18,     1] loss: 0.053
[19,     1] loss: 0.029
[20,     1] loss: 0.065
[21,     1] loss: 0.020
[22,     1] loss: 0.018
[23,     1] loss: 0.018
[24,     1] loss: 0.017
[25,     1] loss: 0.012
[26,     1] loss: 0.012
[27,     1] loss: 0.010
[28,     1] loss: 0.010
[29,     1] loss: 0.010
[30,     1] loss: 0.009
[31,     1] loss: 0.009
[32,     1] loss: 0.009
[33,     1] loss: 0.009
[34,     1] loss: 0.009
[35,     1] loss: 0.009
[36,     1] loss: 0.009
[37,     1] loss: 0.010
[38,     1] loss: 0.010
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.009
[42,     1] loss: 0.009
[43,     1] loss: 0.008
[44,     1] loss: 0.009
[45,     1] loss: 0.008
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
[61,     1] loss: 0.005
Early stopping applied (best metric=0.3692418336868286)
Finished Training
Total time taken: 153.9492826461792
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.681
[3,     1] loss: 0.650
[4,     1] loss: 0.618
[5,     1] loss: 0.582
[6,     1] loss: 0.552
[7,     1] loss: 0.516
[8,     1] loss: 0.490
[9,     1] loss: 0.465
[10,     1] loss: 0.433
[11,     1] loss: 0.405
[12,     1] loss: 0.381
[13,     1] loss: 0.385
[14,     1] loss: 0.367
[15,     1] loss: 0.343
[16,     1] loss: 0.325
[17,     1] loss: 0.307
[18,     1] loss: 0.290
[19,     1] loss: 0.277
[20,     1] loss: 0.254
[21,     1] loss: 0.238
[22,     1] loss: 0.217
[23,     1] loss: 0.197
[24,     1] loss: 0.176
[25,     1] loss: 0.158
[26,     1] loss: 0.137
[27,     1] loss: 0.119
[28,     1] loss: 0.107
[29,     1] loss: 0.092
[30,     1] loss: 0.077
[31,     1] loss: 0.059
[32,     1] loss: 0.045
[33,     1] loss: 0.032
[34,     1] loss: 0.065
[35,     1] loss: 0.017
[36,     1] loss: 0.019
[37,     1] loss: 0.016
[38,     1] loss: 0.015
[39,     1] loss: 0.013
[40,     1] loss: 0.012
[41,     1] loss: 0.011
[42,     1] loss: 0.012
[43,     1] loss: 0.010
[44,     1] loss: 0.039
[45,     1] loss: 0.009
[46,     1] loss: 0.036
[47,     1] loss: 0.056
[48,     1] loss: 0.169
[49,     1] loss: 0.244
[50,     1] loss: 0.237
[51,     1] loss: 0.138
[52,     1] loss: 0.134
[53,     1] loss: 0.147
[54,     1] loss: 0.124
[55,     1] loss: 0.110
[56,     1] loss: 0.092
[57,     1] loss: 0.067
[58,     1] loss: 0.053
[59,     1] loss: 0.041
[60,     1] loss: 0.029
[61,     1] loss: 0.021
[62,     1] loss: 0.016
[63,     1] loss: 0.012
[64,     1] loss: 0.010
[65,     1] loss: 0.009
[66,     1] loss: 0.007
[67,     1] loss: 0.006
[68,     1] loss: 0.006
[69,     1] loss: 0.006
[70,     1] loss: 0.006
[71,     1] loss: 0.006
[72,     1] loss: 0.006
[73,     1] loss: 0.007
[74,     1] loss: 0.008
[75,     1] loss: 0.009
[76,     1] loss: 0.009
[77,     1] loss: 0.010
[78,     1] loss: 0.011
[79,     1] loss: 0.012
[80,     1] loss: 0.013
[81,     1] loss: 0.013
[82,     1] loss: 0.013
[83,     1] loss: 0.013
[84,     1] loss: 0.013
[85,     1] loss: 0.013
[86,     1] loss: 0.012
[87,     1] loss: 0.012
[88,     1] loss: 0.011
[89,     1] loss: 0.011
[90,     1] loss: 0.010
[91,     1] loss: 0.009
[92,     1] loss: 0.009
[93,     1] loss: 0.009
[94,     1] loss: 0.009
[95,     1] loss: 0.009
[96,     1] loss: 0.009
[97,     1] loss: 0.009
[98,     1] loss: 0.009
[99,     1] loss: 0.009
[100,     1] loss: 0.008
[101,     1] loss: 0.009
[102,     1] loss: 0.009
[103,     1] loss: 0.009
[104,     1] loss: 0.009
[105,     1] loss: 0.009
Early stopping applied (best metric=0.37311744689941406)
Finished Training
Total time taken: 263.4779050350189
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.693
[3,     1] loss: 0.670
[4,     1] loss: 0.643
[5,     1] loss: 0.605
[6,     1] loss: 0.565
[7,     1] loss: 0.520
[8,     1] loss: 0.474
[9,     1] loss: 0.421
[10,     1] loss: 0.366
[11,     1] loss: 0.312
[12,     1] loss: 0.253
[13,     1] loss: 0.208
[14,     1] loss: 0.170
[15,     1] loss: 0.141
[16,     1] loss: 0.110
[17,     1] loss: 0.084
[18,     1] loss: 0.066
[19,     1] loss: 0.052
[20,     1] loss: 0.039
[21,     1] loss: 0.031
[22,     1] loss: 0.026
[23,     1] loss: 0.022
[24,     1] loss: 0.018
[25,     1] loss: 0.014
[26,     1] loss: 0.012
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.005
[33,     1] loss: 0.005
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.007
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.004
[55,     1] loss: 0.005
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
[60,     1] loss: 0.004
[61,     1] loss: 0.003
Early stopping applied (best metric=0.3802374005317688)
Finished Training
Total time taken: 154.2137315273285
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.677
[3,     1] loss: 0.628
[4,     1] loss: 0.585
[5,     1] loss: 0.533
[6,     1] loss: 0.487
[7,     1] loss: 0.437
[8,     1] loss: 0.381
[9,     1] loss: 0.317
[10,     1] loss: 0.257
[11,     1] loss: 0.196
[12,     1] loss: 0.157
[13,     1] loss: 0.126
[14,     1] loss: 0.100
[15,     1] loss: 0.069
[16,     1] loss: 0.087
[17,     1] loss: 0.042
[18,     1] loss: 0.084
[19,     1] loss: 0.048
[20,     1] loss: 0.110
[21,     1] loss: 0.044
[22,     1] loss: 0.035
[23,     1] loss: 0.088
[24,     1] loss: 0.050
[25,     1] loss: 0.049
[26,     1] loss: 0.056
[27,     1] loss: 0.032
[28,     1] loss: 0.039
[29,     1] loss: 0.033
[30,     1] loss: 0.033
[31,     1] loss: 0.034
[32,     1] loss: 0.030
[33,     1] loss: 0.028
[34,     1] loss: 0.027
[35,     1] loss: 0.024
[36,     1] loss: 0.021
[37,     1] loss: 0.018
[38,     1] loss: 0.018
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.012
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.011
[57,     1] loss: 0.010
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.010
Early stopping applied (best metric=0.3584362268447876)
Finished Training
Total time taken: 152.11033654212952
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.644
[4,     1] loss: 0.609
[5,     1] loss: 0.566
[6,     1] loss: 0.518
[7,     1] loss: 0.465
[8,     1] loss: 0.410
[9,     1] loss: 0.349
[10,     1] loss: 0.289
[11,     1] loss: 0.241
[12,     1] loss: 0.197
[13,     1] loss: 0.162
[14,     1] loss: 0.135
[15,     1] loss: 0.115
[16,     1] loss: 0.106
[17,     1] loss: 0.098
[18,     1] loss: 0.088
[19,     1] loss: 0.083
[20,     1] loss: 0.072
[21,     1] loss: 0.060
[22,     1] loss: 0.058
[23,     1] loss: 0.051
[24,     1] loss: 0.047
[25,     1] loss: 0.044
[26,     1] loss: 0.040
[27,     1] loss: 0.036
[28,     1] loss: 0.030
[29,     1] loss: 0.025
[30,     1] loss: 0.023
[31,     1] loss: 0.021
[32,     1] loss: 0.019
[33,     1] loss: 0.017
[34,     1] loss: 0.016
[35,     1] loss: 0.015
[36,     1] loss: 0.015
[37,     1] loss: 0.015
[38,     1] loss: 0.013
[39,     1] loss: 0.013
[40,     1] loss: 0.013
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.009
[58,     1] loss: 0.008
Early stopping applied (best metric=0.3905152976512909)
Finished Training
Total time taken: 147.26140213012695
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.663
[4,     1] loss: 0.641
[5,     1] loss: 0.613
[6,     1] loss: 0.584
[7,     1] loss: 0.550
[8,     1] loss: 0.508
[9,     1] loss: 0.468
[10,     1] loss: 0.428
[11,     1] loss: 0.380
[12,     1] loss: 0.343
[13,     1] loss: 0.307
[14,     1] loss: 0.265
[15,     1] loss: 0.218
[16,     1] loss: 0.186
[17,     1] loss: 0.164
[18,     1] loss: 0.145
[19,     1] loss: 0.124
[20,     1] loss: 0.114
[21,     1] loss: 0.105
[22,     1] loss: 0.099
[23,     1] loss: 0.081
[24,     1] loss: 0.063
[25,     1] loss: 0.061
[26,     1] loss: 0.101
[27,     1] loss: 0.025
[28,     1] loss: 0.264
[29,     1] loss: 0.043
[30,     1] loss: 0.087
[31,     1] loss: 0.153
[32,     1] loss: 0.057
[33,     1] loss: 0.071
[34,     1] loss: 0.066
[35,     1] loss: 0.059
[36,     1] loss: 0.055
[37,     1] loss: 0.049
[38,     1] loss: 0.044
[39,     1] loss: 0.039
[40,     1] loss: 0.033
[41,     1] loss: 0.029
[42,     1] loss: 0.026
[43,     1] loss: 0.023
[44,     1] loss: 0.020
[45,     1] loss: 0.018
[46,     1] loss: 0.017
[47,     1] loss: 0.015
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.013
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.013
[54,     1] loss: 0.013
[55,     1] loss: 0.013
[56,     1] loss: 0.014
[57,     1] loss: 0.014
[58,     1] loss: 0.014
Early stopping applied (best metric=0.43427643179893494)
Finished Training
Total time taken: 147.53771495819092
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.641
[4,     1] loss: 0.602
[5,     1] loss: 0.555
[6,     1] loss: 0.497
[7,     1] loss: 0.445
[8,     1] loss: 0.375
[9,     1] loss: 0.311
[10,     1] loss: 0.247
[11,     1] loss: 0.191
[12,     1] loss: 0.150
[13,     1] loss: 0.118
[14,     1] loss: 0.112
[15,     1] loss: 0.096
[16,     1] loss: 0.072
[17,     1] loss: 0.051
[18,     1] loss: 0.045
[19,     1] loss: 0.036
[20,     1] loss: 0.029
[21,     1] loss: 0.022
[22,     1] loss: 0.020
[23,     1] loss: 0.016
[24,     1] loss: 0.014
[25,     1] loss: 0.013
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.007
[32,     1] loss: 0.008
[33,     1] loss: 0.007
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.008
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.008
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.007
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.005
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
Early stopping applied (best metric=0.4039890766143799)
Finished Training
Total time taken: 150.15071201324463
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.680
[3,     1] loss: 0.653
[4,     1] loss: 0.622
[5,     1] loss: 0.583
[6,     1] loss: 0.546
[7,     1] loss: 0.509
[8,     1] loss: 0.463
[9,     1] loss: 0.416
[10,     1] loss: 0.359
[11,     1] loss: 0.297
[12,     1] loss: 0.240
[13,     1] loss: 0.194
[14,     1] loss: 0.151
[15,     1] loss: 0.124
[16,     1] loss: 0.087
[17,     1] loss: 0.066
[18,     1] loss: 0.069
[19,     1] loss: 0.046
[20,     1] loss: 0.027
[21,     1] loss: 0.026
[22,     1] loss: 0.068
[23,     1] loss: 0.013
[24,     1] loss: 0.117
[25,     1] loss: 0.013
[26,     1] loss: 0.141
[27,     1] loss: 0.022
[28,     1] loss: 0.029
[29,     1] loss: 0.021
[30,     1] loss: 0.030
[31,     1] loss: 0.037
[32,     1] loss: 0.029
[33,     1] loss: 0.025
[34,     1] loss: 0.023
[35,     1] loss: 0.022
[36,     1] loss: 0.020
[37,     1] loss: 0.018
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.013
[41,     1] loss: 0.013
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.008
[59,     1] loss: 0.008
Early stopping applied (best metric=0.384567528963089)
Finished Training
Total time taken: 150.51800632476807
{'Hydroxylation-P Validation Accuracy': 0.6826239683264809, 'Hydroxylation-P Validation Sensitivity': 0.8784761904761905, 'Hydroxylation-P Validation Specificity': 0.6405461619033368, 'Hydroxylation-P Validation Precision': 0.3550742826855436, 'Hydroxylation-P AUC ROC': 0.8492181298675389, 'Hydroxylation-P AUC PR': 0.5531103914540296, 'Hydroxylation-P MCC': 0.4049578769882312, 'Hydroxylation-P F1': 0.5018754807644796, 'Validation Loss (Hydroxylation-P)': 0.39307277083396913, 'Validation Loss (total)': 0.39307277083396913}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004546424875628879,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 793889317,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.00089376078614}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.654
[5,     1] loss: 0.631
[6,     1] loss: 0.609
[7,     1] loss: 0.585
[8,     1] loss: 0.564
[9,     1] loss: 0.537
[10,     1] loss: 0.508
[11,     1] loss: 0.480
[12,     1] loss: 0.454
[13,     1] loss: 0.431
[14,     1] loss: 0.400
[15,     1] loss: 0.378
[16,     1] loss: 0.356
[17,     1] loss: 0.334
[18,     1] loss: 0.315
[19,     1] loss: 0.291
[20,     1] loss: 0.273
[21,     1] loss: 0.257
[22,     1] loss: 0.243
[23,     1] loss: 0.228
[24,     1] loss: 0.215
[25,     1] loss: 0.204
[26,     1] loss: 0.197
[27,     1] loss: 0.189
[28,     1] loss: 0.182
[29,     1] loss: 0.174
[30,     1] loss: 0.169
[31,     1] loss: 0.161
[32,     1] loss: 0.157
[33,     1] loss: 0.152
[34,     1] loss: 0.148
[35,     1] loss: 0.147
[36,     1] loss: 0.191
[37,     1] loss: 0.305
[38,     1] loss: 0.375
[39,     1] loss: 0.269
[40,     1] loss: 0.282
[41,     1] loss: 0.268
[42,     1] loss: 0.259
[43,     1] loss: 0.258
[44,     1] loss: 0.245
[45,     1] loss: 0.235
[46,     1] loss: 0.222
[47,     1] loss: 0.209
[48,     1] loss: 0.193
[49,     1] loss: 0.180
[50,     1] loss: 0.171
[51,     1] loss: 0.165
[52,     1] loss: 0.154
[53,     1] loss: 0.148
[54,     1] loss: 0.137
[55,     1] loss: 0.132
[56,     1] loss: 0.132
[57,     1] loss: 0.170
[58,     1] loss: 0.160
[59,     1] loss: 0.155
[60,     1] loss: 0.150
[61,     1] loss: 0.171
[62,     1] loss: 0.146
[63,     1] loss: 0.152
[64,     1] loss: 0.174
[65,     1] loss: 0.160
[66,     1] loss: 0.172
[67,     1] loss: 0.206
[68,     1] loss: 0.150
[69,     1] loss: 0.164
[70,     1] loss: 0.152
[71,     1] loss: 0.157
[72,     1] loss: 0.153
[73,     1] loss: 0.153
[74,     1] loss: 0.140
[75,     1] loss: 0.142
[76,     1] loss: 0.143
[77,     1] loss: 0.135
[78,     1] loss: 0.133
[79,     1] loss: 0.132
[80,     1] loss: 0.131
[81,     1] loss: 0.128
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 8.008977930582493e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4006256780,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.574641632271316}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.699
[3,     1] loss: 0.693
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004722369111329757,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2468651394,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.06843804829164}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.678
[3,     1] loss: 0.645
[4,     1] loss: 0.611
[5,     1] loss: 0.570
[6,     1] loss: 0.528
[7,     1] loss: 0.476
[8,     1] loss: 0.422
[9,     1] loss: 0.364
[10,     1] loss: 0.301
[11,     1] loss: 0.232
[12,     1] loss: 0.171
[13,     1] loss: 0.121
[14,     1] loss: 0.090
[15,     1] loss: 0.076
[16,     1] loss: 0.059
[17,     1] loss: 0.045
[18,     1] loss: 0.041
[19,     1] loss: 0.033
[20,     1] loss: 0.026
[21,     1] loss: 0.021
[22,     1] loss: 0.019
[23,     1] loss: 0.015
[24,     1] loss: 0.013
[25,     1] loss: 0.011
[26,     1] loss: 0.011
[27,     1] loss: 0.010
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005231354317708464,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2857094796,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.962202397384146}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.659
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0040343870801005256,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2803991193,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.778573564575916}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.698
[3,     1] loss: 0.678
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006225190130416997,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 507866497,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.074716655379533}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.683
[3,     1] loss: 0.641
[4,     1] loss: 0.595
[5,     1] loss: 0.549
[6,     1] loss: 0.499
[7,     1] loss: 0.440
[8,     1] loss: 0.385
[9,     1] loss: 0.323
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007126842808739811,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1127926753,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.551170921632043}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.687
[3,     1] loss: 0.661
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005282869392379883,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2904005365,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.822990088287252}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.679
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005699399025537084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1082884009,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.6079315952875834}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.662
[3,     1] loss: 0.602
[4,     1] loss: 0.544
[5,     1] loss: 0.476
[6,     1] loss: 0.409
[7,     1] loss: 0.345
[8,     1] loss: 0.274
[9,     1] loss: 0.218
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033429612032165497,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1415838669,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.668640737166278}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.688
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037393088025817387,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1420238978,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.604044729497385}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.663
[4,     1] loss: 0.644
[5,     1] loss: 0.630
[6,     1] loss: 0.611
[7,     1] loss: 0.596
[8,     1] loss: 0.578
[9,     1] loss: 0.556
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 1.8761459489923926e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1810463131,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.031998426915408}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.700
[3,     1] loss: 0.697
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00811764809042816,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3543853611,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.184758355272988}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.639
[4,     1] loss: 0.592
[5,     1] loss: 0.546
[6,     1] loss: 0.485
[7,     1] loss: 0.437
[8,     1] loss: 0.385
[9,     1] loss: 0.319
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009452744138721323,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 65094091,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.037156145972074395}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.686
[3,     1] loss: 0.640
[4,     1] loss: 0.564
[5,     1] loss: 0.488
[6,     1] loss: 0.439
[7,     1] loss: 0.374
[8,     1] loss: 0.310
[9,     1] loss: 0.259
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033868951813555525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2381902845,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.622993501097579}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005937241118333672,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2435154913,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.1500359231775867}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.651
[4,     1] loss: 0.618
[5,     1] loss: 0.576
[6,     1] loss: 0.528
[7,     1] loss: 0.472
[8,     1] loss: 0.409
[9,     1] loss: 0.336
[10,     1] loss: 0.260
[11,     1] loss: 0.188
[12,     1] loss: 0.139
[13,     1] loss: 0.116
[14,     1] loss: 0.072
[15,     1] loss: 0.052
[16,     1] loss: 0.037
[17,     1] loss: 0.026
[18,     1] loss: 0.020
[19,     1] loss: 0.017
[20,     1] loss: 0.012
[21,     1] loss: 0.010
[22,     1] loss: 0.009
[23,     1] loss: 0.007
[24,     1] loss: 0.006
[25,     1] loss: 0.006
[26,     1] loss: 0.005
[27,     1] loss: 0.004
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036944256533006283,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 986629492,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.09645074158281}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.686
[3,     1] loss: 0.670
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021830138876672723,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1530818952,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.907381189214904}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005171885216167762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 825124781,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.515079215303545}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.687
[3,     1] loss: 0.650
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004525445463679816,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2350456068,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.71709972614533}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.684
[3,     1] loss: 0.653
[4,     1] loss: 0.620
[5,     1] loss: 0.581
[6,     1] loss: 0.540
[7,     1] loss: 0.491
[8,     1] loss: 0.434
[9,     1] loss: 0.380
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005626864841256702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3572507340,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.931811397670694}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.658
[4,     1] loss: 0.629
[5,     1] loss: 0.589
[6,     1] loss: 0.543
[7,     1] loss: 0.491
[8,     1] loss: 0.436
[9,     1] loss: 0.388
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005505164659213187,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3439937263,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.9030966487764}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.654
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005253646224401767,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3068055041,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.882418797785277}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.665
[3,     1] loss: 0.622
[4,     1] loss: 0.590
[5,     1] loss: 0.550
[6,     1] loss: 0.515
[7,     1] loss: 0.478
[8,     1] loss: 0.452
[9,     1] loss: 0.426
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0063763584724104375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3509810548,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.409926446268802}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.665
[4,     1] loss: 0.648
[5,     1] loss: 0.626
[6,     1] loss: 0.605
[7,     1] loss: 0.579
[8,     1] loss: 0.552
[9,     1] loss: 0.521
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008968446201701064,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1279087796,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.730291684570778}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007553046526519502,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3928742178,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.39938589679968}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.679
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033115490161930595,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2119871078,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.088705711675942}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.661
[4,     1] loss: 0.641
[5,     1] loss: 0.623
[6,     1] loss: 0.603
[7,     1] loss: 0.582
[8,     1] loss: 0.561
[9,     1] loss: 0.538
[10,     1] loss: 0.513
[11,     1] loss: 0.491
[12,     1] loss: 0.463
[13,     1] loss: 0.437
[14,     1] loss: 0.411
[15,     1] loss: 0.390
[16,     1] loss: 0.367
[17,     1] loss: 0.344
[18,     1] loss: 0.323
[19,     1] loss: 0.305
[20,     1] loss: 0.286
[21,     1] loss: 0.272
[22,     1] loss: 0.255
[23,     1] loss: 0.242
[24,     1] loss: 0.230
[25,     1] loss: 0.216
[26,     1] loss: 0.207
[27,     1] loss: 0.203
[28,     1] loss: 0.195
[29,     1] loss: 0.186
[30,     1] loss: 0.181
[31,     1] loss: 0.179
[32,     1] loss: 0.173
[33,     1] loss: 0.171
[34,     1] loss: 0.169
[35,     1] loss: 0.164
[36,     1] loss: 0.164
[37,     1] loss: 0.161
[38,     1] loss: 0.158
[39,     1] loss: 0.159
[40,     1] loss: 0.157
[41,     1] loss: 0.158
[42,     1] loss: 0.155
[43,     1] loss: 0.155
[44,     1] loss: 0.153
[45,     1] loss: 0.153
[46,     1] loss: 0.150
[47,     1] loss: 0.149
[48,     1] loss: 0.146
[49,     1] loss: 0.328
[50,     1] loss: 0.418
[51,     1] loss: 0.332
[52,     1] loss: 0.350
[53,     1] loss: 0.317
[54,     1] loss: 0.294
[55,     1] loss: 0.289
[56,     1] loss: 0.276
[57,     1] loss: 0.272
[58,     1] loss: 0.262
[59,     1] loss: 0.249
[60,     1] loss: 0.240
[61,     1] loss: 0.226
[62,     1] loss: 0.215
[63,     1] loss: 0.198
[64,     1] loss: 0.189
[65,     1] loss: 0.195
[66,     1] loss: 0.181
[67,     1] loss: 0.164
[68,     1] loss: 0.170
[69,     1] loss: 0.150
[70,     1] loss: 0.144
[71,     1] loss: 0.142
[72,     1] loss: 0.136
[73,     1] loss: 0.134
[74,     1] loss: 0.131
[75,     1] loss: 0.127
[76,     1] loss: 0.124
[77,     1] loss: 0.125
[78,     1] loss: 0.126
[79,     1] loss: 0.123
[80,     1] loss: 0.122
[81,     1] loss: 0.125
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006959789767494987,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 569390868,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.660861015903983}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.699
[3,     1] loss: 0.680
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009821939799621518,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3093327731,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.311512048032284}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.669
[3,     1] loss: 0.600
[4,     1] loss: 0.569
[5,     1] loss: 0.532
[6,     1] loss: 0.490
[7,     1] loss: 0.437
[8,     1] loss: 0.385
[9,     1] loss: 0.319
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005274998483854343,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3274922105,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.882215858005782}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.689
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004743823944668483,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2927550134,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.901342036640551}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.670
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006913853824627121,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3961555946,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.033529966873786}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.697
[3,     1] loss: 0.684
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003222593612061842,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1179115717,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.960569308838195}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0041485797030368365,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 447910220,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.186057347559395}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.677
[3,     1] loss: 0.651
[4,     1] loss: 0.630
[5,     1] loss: 0.611
[6,     1] loss: 0.589
[7,     1] loss: 0.565
[8,     1] loss: 0.541
[9,     1] loss: 0.515
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064989648034176175,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4206067226,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.433626793601798}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.646
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052787638851268135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4209606500,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.688170662623068}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.691
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009306131882789823,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2930842702,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.51757055174986}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.657
[4,     1] loss: 0.634
[5,     1] loss: 0.606
[6,     1] loss: 0.576
[7,     1] loss: 0.547
[8,     1] loss: 0.511
[9,     1] loss: 0.479
[10,     1] loss: 0.454
[11,     1] loss: 0.447
[12,     1] loss: 0.424
[13,     1] loss: 0.402
[14,     1] loss: 0.368
[15,     1] loss: 0.380
[16,     1] loss: 0.402
[17,     1] loss: 0.365
[18,     1] loss: 0.350
[19,     1] loss: 0.333
[20,     1] loss: 0.314
[21,     1] loss: 0.292
[22,     1] loss: 0.266
[23,     1] loss: 0.241
[24,     1] loss: 0.218
[25,     1] loss: 0.249
[26,     1] loss: 0.254
[27,     1] loss: 0.239
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007317650438133441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1829735342,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.6057118030145805}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.671
[4,     1] loss: 0.658
[5,     1] loss: 0.642
[6,     1] loss: 0.629
[7,     1] loss: 0.615
[8,     1] loss: 0.598
[9,     1] loss: 0.586
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006862547666302921,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2307655318,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.412108457349092}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.698
[3,     1] loss: 0.684
[4,     1] loss: 0.671
[5,     1] loss: 0.657
[6,     1] loss: 0.641
[7,     1] loss: 0.625
[8,     1] loss: 0.606
[9,     1] loss: 0.585
[10,     1] loss: 0.564
[11,     1] loss: 0.544
[12,     1] loss: 0.525
[13,     1] loss: 0.516
[14,     1] loss: 0.493
[15,     1] loss: 0.483
[16,     1] loss: 0.475
[17,     1] loss: 0.460
[18,     1] loss: 0.446
[19,     1] loss: 0.433
[20,     1] loss: 0.422
[21,     1] loss: 0.411
[22,     1] loss: 0.400
[23,     1] loss: 0.387
[24,     1] loss: 0.376
[25,     1] loss: 0.365
[26,     1] loss: 0.349
[27,     1] loss: 0.336
[28,     1] loss: 0.325
[29,     1] loss: 0.313
[30,     1] loss: 0.307
[31,     1] loss: 0.335
[32,     1] loss: 0.341
[33,     1] loss: 0.484
[34,     1] loss: 0.520
[35,     1] loss: 0.457
[36,     1] loss: 0.455
[37,     1] loss: 0.459
[38,     1] loss: 0.451
[39,     1] loss: 0.449
[40,     1] loss: 0.441
[41,     1] loss: 0.428
[42,     1] loss: 0.412
[43,     1] loss: 0.392
[44,     1] loss: 0.375
[45,     1] loss: 0.354
[46,     1] loss: 0.336
[47,     1] loss: 0.320
[48,     1] loss: 0.298
[49,     1] loss: 0.283
[50,     1] loss: 0.270
[51,     1] loss: 0.253
[52,     1] loss: 0.242
[53,     1] loss: 0.237
[54,     1] loss: 0.226
[55,     1] loss: 0.215
[56,     1] loss: 0.207
[57,     1] loss: 0.289
[58,     1] loss: 0.301
[59,     1] loss: 0.683
[60,     1] loss: 0.488
[61,     1] loss: 0.608
[62,     1] loss: 0.596
[63,     1] loss: 0.551
[64,     1] loss: 0.549
[65,     1] loss: 0.555
[66,     1] loss: 0.562
[67,     1] loss: 0.570
[68,     1] loss: 0.576
[69,     1] loss: 0.574
[70,     1] loss: 0.576
[71,     1] loss: 0.574
[72,     1] loss: 0.573
[73,     1] loss: 0.570
[74,     1] loss: 0.567
[75,     1] loss: 0.563
[76,     1] loss: 0.559
[77,     1] loss: 0.554
[78,     1] loss: 0.547
[79,     1] loss: 0.543
[80,     1] loss: 0.534
[81,     1] loss: 0.528
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006487277074237548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2497213206,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.687199294271089}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.682
[3,     1] loss: 0.634
[4,     1] loss: 0.580
[5,     1] loss: 0.518
[6,     1] loss: 0.461
[7,     1] loss: 0.410
[8,     1] loss: 0.336
[9,     1] loss: 0.274
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005022007638836614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3801520142,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.530025026346538}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014652823067572373,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 941275021,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.717462783067788}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.673
[3,     1] loss: 0.650
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005291602921331905,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 282368308,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.024304824507745}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.647
[4,     1] loss: 0.611
[5,     1] loss: 0.566
[6,     1] loss: 0.520
[7,     1] loss: 0.465
[8,     1] loss: 0.402
[9,     1] loss: 0.340
[10,     1] loss: 0.289
[11,     1] loss: 0.235
[12,     1] loss: 0.190
[13,     1] loss: 0.154
[14,     1] loss: 0.121
[15,     1] loss: 0.095
[16,     1] loss: 0.077
[17,     1] loss: 0.082
[18,     1] loss: 0.072
[19,     1] loss: 0.044
[20,     1] loss: 0.038
[21,     1] loss: 0.021
[22,     1] loss: 0.017
[23,     1] loss: 0.013
[24,     1] loss: 0.010
[25,     1] loss: 0.008
[26,     1] loss: 0.007
[27,     1] loss: 0.006
[28,     1] loss: 0.005
[29,     1] loss: 0.004
[30,     1] loss: 0.004
[31,     1] loss: 0.004
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.004
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.005
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.006
[45,     1] loss: 0.007
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.004
[61,     1] loss: 0.004
[62,     1] loss: 0.005
Early stopping applied (best metric=0.4142817258834839)
Finished Training
Total time taken: 161.88965892791748
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.677
[4,     1] loss: 0.656
[5,     1] loss: 0.629
[6,     1] loss: 0.592
[7,     1] loss: 0.548
[8,     1] loss: 0.496
[9,     1] loss: 0.434
[10,     1] loss: 0.370
[11,     1] loss: 0.302
[12,     1] loss: 0.242
[13,     1] loss: 0.187
[14,     1] loss: 0.142
[15,     1] loss: 0.106
[16,     1] loss: 0.080
[17,     1] loss: 0.062
[18,     1] loss: 0.065
[19,     1] loss: 0.041
[20,     1] loss: 0.033
[21,     1] loss: 0.022
[22,     1] loss: 0.019
[23,     1] loss: 0.013
[24,     1] loss: 0.010
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.004
[28,     1] loss: 0.003
[29,     1] loss: 0.003
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.003
[34,     1] loss: 0.003
[35,     1] loss: 0.004
[36,     1] loss: 0.004
[37,     1] loss: 0.005
[38,     1] loss: 0.005
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.42384862899780273)
Finished Training
Total time taken: 156.88396692276
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.692
[3,     1] loss: 0.664
[4,     1] loss: 0.630
[5,     1] loss: 0.585
[6,     1] loss: 0.533
[7,     1] loss: 0.474
[8,     1] loss: 0.417
[9,     1] loss: 0.362
[10,     1] loss: 0.299
[11,     1] loss: 0.243
[12,     1] loss: 0.195
[13,     1] loss: 0.151
[14,     1] loss: 0.123
[15,     1] loss: 0.102
[16,     1] loss: 0.070
[17,     1] loss: 0.061
[18,     1] loss: 0.045
[19,     1] loss: 0.026
[20,     1] loss: 0.016
[21,     1] loss: 0.010
[22,     1] loss: 0.011
[23,     1] loss: 0.007
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.005
[27,     1] loss: 0.005
[28,     1] loss: 0.004
[29,     1] loss: 0.003
[30,     1] loss: 0.004
[31,     1] loss: 0.004
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.004
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.005
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.007
[44,     1] loss: 0.006
[45,     1] loss: 0.007
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.003
[59,     1] loss: 0.003
Early stopping applied (best metric=0.42029881477355957)
Finished Training
Total time taken: 155.04601073265076
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.639
[4,     1] loss: 0.600
[5,     1] loss: 0.554
[6,     1] loss: 0.504
[7,     1] loss: 0.450
[8,     1] loss: 0.397
[9,     1] loss: 0.344
[10,     1] loss: 0.302
[11,     1] loss: 0.254
[12,     1] loss: 0.222
[13,     1] loss: 0.165
[14,     1] loss: 0.155
[15,     1] loss: 0.161
[16,     1] loss: 0.106
[17,     1] loss: 0.094
[18,     1] loss: 0.075
[19,     1] loss: 0.053
[20,     1] loss: 0.049
[21,     1] loss: 0.043
[22,     1] loss: 0.034
[23,     1] loss: 0.024
[24,     1] loss: 0.018
[25,     1] loss: 0.015
[26,     1] loss: 0.011
[27,     1] loss: 0.008
[28,     1] loss: 0.008
[29,     1] loss: 0.006
[30,     1] loss: 0.005
[31,     1] loss: 0.004
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.004
[35,     1] loss: 0.004
[36,     1] loss: 0.004
[37,     1] loss: 0.004
[38,     1] loss: 0.004
[39,     1] loss: 0.005
[40,     1] loss: 0.005
[41,     1] loss: 0.005
[42,     1] loss: 0.005
[43,     1] loss: 0.005
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.005
[47,     1] loss: 0.005
[48,     1] loss: 0.005
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
Early stopping applied (best metric=0.4152894914150238)
Finished Training
Total time taken: 152.31306743621826
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.669
[3,     1] loss: 0.622
[4,     1] loss: 0.576
[5,     1] loss: 0.526
[6,     1] loss: 0.480
[7,     1] loss: 0.439
[8,     1] loss: 0.387
[9,     1] loss: 0.342
[10,     1] loss: 0.286
[11,     1] loss: 0.241
[12,     1] loss: 0.185
[13,     1] loss: 0.165
[14,     1] loss: 0.131
[15,     1] loss: 0.102
[16,     1] loss: 0.073
[17,     1] loss: 0.049
[18,     1] loss: 0.032
[19,     1] loss: 0.031
[20,     1] loss: 0.017
[21,     1] loss: 0.012
[22,     1] loss: 0.010
[23,     1] loss: 0.010
[24,     1] loss: 0.007
[25,     1] loss: 0.005
[26,     1] loss: 0.004
[27,     1] loss: 0.003
[28,     1] loss: 0.004
[29,     1] loss: 0.004
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.004
[35,     1] loss: 0.004
[36,     1] loss: 0.004
[37,     1] loss: 0.005
[38,     1] loss: 0.005
[39,     1] loss: 0.005
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.007
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.005
[48,     1] loss: 0.006
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.004
[53,     1] loss: 0.004
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.003
[58,     1] loss: 0.004
[59,     1] loss: 0.003
[60,     1] loss: 0.003
[61,     1] loss: 0.003
[62,     1] loss: 0.003
Early stopping applied (best metric=0.3475643992424011)
Finished Training
Total time taken: 162.65683460235596
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.678
[4,     1] loss: 0.660
[5,     1] loss: 0.635
[6,     1] loss: 0.600
[7,     1] loss: 0.560
[8,     1] loss: 0.510
[9,     1] loss: 0.452
[10,     1] loss: 0.405
[11,     1] loss: 0.359
[12,     1] loss: 0.324
[13,     1] loss: 0.283
[14,     1] loss: 0.246
[15,     1] loss: 0.193
[16,     1] loss: 0.163
[17,     1] loss: 0.135
[18,     1] loss: 0.111
[19,     1] loss: 0.096
[20,     1] loss: 0.072
[21,     1] loss: 0.058
[22,     1] loss: 0.055
[23,     1] loss: 0.033
[24,     1] loss: 0.027
[25,     1] loss: 0.015
[26,     1] loss: 0.063
[27,     1] loss: 0.046
[28,     1] loss: 0.011
[29,     1] loss: 0.017
[30,     1] loss: 0.012
[31,     1] loss: 0.019
[32,     1] loss: 0.014
[33,     1] loss: 0.014
[34,     1] loss: 0.012
[35,     1] loss: 0.012
[36,     1] loss: 0.012
[37,     1] loss: 0.010
[38,     1] loss: 0.011
[39,     1] loss: 0.011
[40,     1] loss: 0.012
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.012
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.010
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.007
[58,     1] loss: 0.008
[59,     1] loss: 0.007
[60,     1] loss: 0.007
[61,     1] loss: 0.007
Early stopping applied (best metric=0.42118847370147705)
Finished Training
Total time taken: 160.57570457458496
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.674
[3,     1] loss: 0.642
[4,     1] loss: 0.603
[5,     1] loss: 0.558
[6,     1] loss: 0.510
[7,     1] loss: 0.454
[8,     1] loss: 0.391
[9,     1] loss: 0.326
[10,     1] loss: 0.254
[11,     1] loss: 0.202
[12,     1] loss: 0.154
[13,     1] loss: 0.122
[14,     1] loss: 0.095
[15,     1] loss: 0.070
[16,     1] loss: 0.062
[17,     1] loss: 0.040
[18,     1] loss: 0.034
[19,     1] loss: 0.028
[20,     1] loss: 0.020
[21,     1] loss: 0.017
[22,     1] loss: 0.013
[23,     1] loss: 0.010
[24,     1] loss: 0.008
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.004
[28,     1] loss: 0.004
[29,     1] loss: 0.004
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.003
[34,     1] loss: 0.003
[35,     1] loss: 0.004
[36,     1] loss: 0.004
[37,     1] loss: 0.004
[38,     1] loss: 0.005
[39,     1] loss: 0.005
[40,     1] loss: 0.005
[41,     1] loss: 0.006
[42,     1] loss: 0.005
[43,     1] loss: 0.005
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.005
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.004
[54,     1] loss: 0.004
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
[60,     1] loss: 0.003
Early stopping applied (best metric=0.36810794472694397)
Finished Training
Total time taken: 158.02295064926147
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.673
[3,     1] loss: 0.634
[4,     1] loss: 0.591
[5,     1] loss: 0.540
[6,     1] loss: 0.477
[7,     1] loss: 0.414
[8,     1] loss: 0.348
[9,     1] loss: 0.277
[10,     1] loss: 0.226
[11,     1] loss: 0.200
[12,     1] loss: 0.164
[13,     1] loss: 0.136
[14,     1] loss: 0.109
[15,     1] loss: 0.082
[16,     1] loss: 0.113
[17,     1] loss: 0.057
[18,     1] loss: 0.049
[19,     1] loss: 0.036
[20,     1] loss: 0.031
[21,     1] loss: 0.023
[22,     1] loss: 0.023
[23,     1] loss: 0.017
[24,     1] loss: 0.015
[25,     1] loss: 0.012
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.008
[32,     1] loss: 0.007
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.007
[36,     1] loss: 0.008
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.006
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.005
[56,     1] loss: 0.005
Early stopping applied (best metric=0.4293232858181)
Finished Training
Total time taken: 149.67923641204834
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.663
[4,     1] loss: 0.638
[5,     1] loss: 0.612
[6,     1] loss: 0.581
[7,     1] loss: 0.541
[8,     1] loss: 0.504
[9,     1] loss: 0.465
[10,     1] loss: 0.424
[11,     1] loss: 0.383
[12,     1] loss: 0.348
[13,     1] loss: 0.315
[14,     1] loss: 0.299
[15,     1] loss: 0.280
[16,     1] loss: 0.263
[17,     1] loss: 0.238
[18,     1] loss: 0.210
[19,     1] loss: 0.209
[20,     1] loss: 0.167
[21,     1] loss: 0.152
[22,     1] loss: 0.141
[23,     1] loss: 0.126
[24,     1] loss: 0.117
[25,     1] loss: 0.111
[26,     1] loss: 0.101
[27,     1] loss: 0.093
[28,     1] loss: 0.081
[29,     1] loss: 0.069
[30,     1] loss: 0.053
[31,     1] loss: 0.042
[32,     1] loss: 0.044
[33,     1] loss: 0.034
[34,     1] loss: 0.029
[35,     1] loss: 0.023
[36,     1] loss: 0.018
[37,     1] loss: 0.018
[38,     1] loss: 0.017
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.012
[43,     1] loss: 0.012
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.009
Early stopping applied (best metric=0.4084588885307312)
Finished Training
Total time taken: 150.65924835205078
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.653
[5,     1] loss: 0.627
[6,     1] loss: 0.593
[7,     1] loss: 0.552
[8,     1] loss: 0.506
[9,     1] loss: 0.441
[10,     1] loss: 0.377
[11,     1] loss: 0.314
[12,     1] loss: 0.255
[13,     1] loss: 0.206
[14,     1] loss: 0.167
[15,     1] loss: 0.131
[16,     1] loss: 0.109
[17,     1] loss: 0.092
[18,     1] loss: 0.067
[19,     1] loss: 0.057
[20,     1] loss: 0.050
[21,     1] loss: 0.039
[22,     1] loss: 0.035
[23,     1] loss: 0.035
[24,     1] loss: 0.028
[25,     1] loss: 0.045
[26,     1] loss: 0.036
[27,     1] loss: 0.027
[28,     1] loss: 0.029
[29,     1] loss: 0.027
[30,     1] loss: 0.042
[31,     1] loss: 0.055
[32,     1] loss: 0.022
[33,     1] loss: 0.020
[34,     1] loss: 0.032
[35,     1] loss: 0.021
[36,     1] loss: 0.018
[37,     1] loss: 0.020
[38,     1] loss: 0.018
[39,     1] loss: 0.021
[40,     1] loss: 0.019
[41,     1] loss: 0.018
[42,     1] loss: 0.018
[43,     1] loss: 0.018
[44,     1] loss: 0.017
[45,     1] loss: 0.017
[46,     1] loss: 0.016
[47,     1] loss: 0.015
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.013
[51,     1] loss: 0.012
[52,     1] loss: 0.012
[53,     1] loss: 0.012
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.011
[58,     1] loss: 0.011
[59,     1] loss: 0.010
[60,     1] loss: 0.010
[61,     1] loss: 0.010
[62,     1] loss: 0.010
Early stopping applied (best metric=0.3507023751735687)
Finished Training
Total time taken: 163.97595071792603
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.647
[4,     1] loss: 0.607
[5,     1] loss: 0.556
[6,     1] loss: 0.503
[7,     1] loss: 0.442
[8,     1] loss: 0.376
[9,     1] loss: 0.315
[10,     1] loss: 0.252
[11,     1] loss: 0.202
[12,     1] loss: 0.166
[13,     1] loss: 0.147
[14,     1] loss: 0.097
[15,     1] loss: 0.074
[16,     1] loss: 0.064
[17,     1] loss: 0.079
[18,     1] loss: 0.047
[19,     1] loss: 0.061
[20,     1] loss: 0.038
[21,     1] loss: 0.030
[22,     1] loss: 0.025
[23,     1] loss: 0.019
[24,     1] loss: 0.017
[25,     1] loss: 0.013
[26,     1] loss: 0.012
[27,     1] loss: 0.009
[28,     1] loss: 0.007
[29,     1] loss: 0.007
[30,     1] loss: 0.006
[31,     1] loss: 0.005
[32,     1] loss: 0.005
[33,     1] loss: 0.005
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.006
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.006
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.004
Early stopping applied (best metric=0.3769858777523041)
Finished Training
Total time taken: 156.41720485687256
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.688
[3,     1] loss: 0.649
[4,     1] loss: 0.608
[5,     1] loss: 0.557
[6,     1] loss: 0.518
[7,     1] loss: 0.467
[8,     1] loss: 0.422
[9,     1] loss: 0.361
[10,     1] loss: 0.299
[11,     1] loss: 0.244
[12,     1] loss: 0.196
[13,     1] loss: 0.147
[14,     1] loss: 0.105
[15,     1] loss: 0.085
[16,     1] loss: 0.067
[17,     1] loss: 0.050
[18,     1] loss: 0.038
[19,     1] loss: 0.032
[20,     1] loss: 0.051
[21,     1] loss: 0.061
[22,     1] loss: 0.024
[23,     1] loss: 0.018
[24,     1] loss: 0.021
[25,     1] loss: 0.018
[26,     1] loss: 0.013
[27,     1] loss: 0.010
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.007
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.005
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.004
[60,     1] loss: 0.004
Early stopping applied (best metric=0.40023273229599)
Finished Training
Total time taken: 159.40101742744446
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.661
[4,     1] loss: 0.622
[5,     1] loss: 0.586
[6,     1] loss: 0.534
[7,     1] loss: 0.483
[8,     1] loss: 0.438
[9,     1] loss: 0.383
[10,     1] loss: 0.332
[11,     1] loss: 0.278
[12,     1] loss: 0.236
[13,     1] loss: 0.191
[14,     1] loss: 0.146
[15,     1] loss: 0.111
[16,     1] loss: 0.089
[17,     1] loss: 0.068
[18,     1] loss: 0.049
[19,     1] loss: 0.033
[20,     1] loss: 0.024
[21,     1] loss: 0.018
[22,     1] loss: 0.013
[23,     1] loss: 0.010
[24,     1] loss: 0.007
[25,     1] loss: 0.006
[26,     1] loss: 0.005
[27,     1] loss: 0.004
[28,     1] loss: 0.004
[29,     1] loss: 0.003
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.003
[34,     1] loss: 0.003
[35,     1] loss: 0.003
[36,     1] loss: 0.004
[37,     1] loss: 0.004
[38,     1] loss: 0.005
[39,     1] loss: 0.005
[40,     1] loss: 0.005
[41,     1] loss: 0.006
[42,     1] loss: 0.007
[43,     1] loss: 0.006
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
Early stopping applied (best metric=0.43723249435424805)
Finished Training
Total time taken: 151.5702736377716
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.671
[3,     1] loss: 0.634
[4,     1] loss: 0.592
[5,     1] loss: 0.551
[6,     1] loss: 0.507
[7,     1] loss: 0.464
[8,     1] loss: 0.420
[9,     1] loss: 0.372
[10,     1] loss: 0.315
[11,     1] loss: 0.249
[12,     1] loss: 0.211
[13,     1] loss: 0.164
[14,     1] loss: 0.126
[15,     1] loss: 0.104
[16,     1] loss: 0.084
[17,     1] loss: 0.075
[18,     1] loss: 0.067
[19,     1] loss: 0.066
[20,     1] loss: 0.052
[21,     1] loss: 0.038
[22,     1] loss: 0.026
[23,     1] loss: 0.015
[24,     1] loss: 0.012
[25,     1] loss: 0.010
[26,     1] loss: 0.009
[27,     1] loss: 0.008
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.007
[31,     1] loss: 0.006
[32,     1] loss: 0.007
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.006
[36,     1] loss: 0.006
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
Early stopping applied (best metric=0.3785168528556824)
Finished Training
Total time taken: 154.9140591621399
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.680
[3,     1] loss: 0.640
[4,     1] loss: 0.593
[5,     1] loss: 0.543
[6,     1] loss: 0.502
[7,     1] loss: 0.458
[8,     1] loss: 0.402
[9,     1] loss: 0.350
[10,     1] loss: 0.292
[11,     1] loss: 0.247
[12,     1] loss: 0.203
[13,     1] loss: 0.170
[14,     1] loss: 0.128
[15,     1] loss: 0.103
[16,     1] loss: 0.087
[17,     1] loss: 0.072
[18,     1] loss: 0.063
[19,     1] loss: 0.059
[20,     1] loss: 0.050
[21,     1] loss: 0.045
[22,     1] loss: 0.035
[23,     1] loss: 0.029
[24,     1] loss: 0.025
[25,     1] loss: 0.022
[26,     1] loss: 0.017
[27,     1] loss: 0.017
[28,     1] loss: 0.014
[29,     1] loss: 0.014
[30,     1] loss: 0.011
[31,     1] loss: 0.011
[32,     1] loss: 0.011
[33,     1] loss: 0.010
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.007
[39,     1] loss: 0.008
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.007
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.42442384362220764)
Finished Training
Total time taken: 160.0051646232605
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.651
[4,     1] loss: 0.615
[5,     1] loss: 0.574
[6,     1] loss: 0.528
[7,     1] loss: 0.475
[8,     1] loss: 0.425
[9,     1] loss: 0.374
[10,     1] loss: 0.315
[11,     1] loss: 0.267
[12,     1] loss: 0.215
[13,     1] loss: 0.192
[14,     1] loss: 0.154
[15,     1] loss: 0.121
[16,     1] loss: 0.098
[17,     1] loss: 0.073
[18,     1] loss: 0.056
[19,     1] loss: 0.043
[20,     1] loss: 0.033
[21,     1] loss: 0.027
[22,     1] loss: 0.022
[23,     1] loss: 0.018
[24,     1] loss: 0.016
[25,     1] loss: 0.014
[26,     1] loss: 0.012
[27,     1] loss: 0.010
[28,     1] loss: 0.009
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.007
[32,     1] loss: 0.007
[33,     1] loss: 0.007
[34,     1] loss: 0.007
[35,     1] loss: 0.007
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.009
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.007
Early stopping applied (best metric=0.4064170718193054)
Finished Training
Total time taken: 155.3723292350769
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.649
[4,     1] loss: 0.613
[5,     1] loss: 0.570
[6,     1] loss: 0.514
[7,     1] loss: 0.442
[8,     1] loss: 0.369
[9,     1] loss: 0.291
[10,     1] loss: 0.228
[11,     1] loss: 0.190
[12,     1] loss: 0.159
[13,     1] loss: 0.146
[14,     1] loss: 0.137
[15,     1] loss: 0.112
[16,     1] loss: 0.093
[17,     1] loss: 0.067
[18,     1] loss: 0.062
[19,     1] loss: 0.100
[20,     1] loss: 0.073
[21,     1] loss: 0.054
[22,     1] loss: 0.039
[23,     1] loss: 0.033
[24,     1] loss: 0.033
[25,     1] loss: 0.024
[26,     1] loss: 0.021
[27,     1] loss: 0.019
[28,     1] loss: 0.016
[29,     1] loss: 0.013
[30,     1] loss: 0.013
[31,     1] loss: 0.012
[32,     1] loss: 0.011
[33,     1] loss: 0.010
[34,     1] loss: 0.010
[35,     1] loss: 0.009
[36,     1] loss: 0.008
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.006
[44,     1] loss: 0.006
[45,     1] loss: 0.005
[46,     1] loss: 0.006
[47,     1] loss: 0.005
[48,     1] loss: 0.006
[49,     1] loss: 0.005
[50,     1] loss: 0.005
[51,     1] loss: 0.006
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.006
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
Early stopping applied (best metric=0.4555356502532959)
Finished Training
Total time taken: 159.54770684242249
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.653
[4,     1] loss: 0.621
[5,     1] loss: 0.579
[6,     1] loss: 0.518
[7,     1] loss: 0.450
[8,     1] loss: 0.388
[9,     1] loss: 0.330
[10,     1] loss: 0.263
[11,     1] loss: 0.220
[12,     1] loss: 0.172
[13,     1] loss: 0.139
[14,     1] loss: 0.114
[15,     1] loss: 0.089
[16,     1] loss: 0.073
[17,     1] loss: 0.076
[18,     1] loss: 0.055
[19,     1] loss: 0.043
[20,     1] loss: 0.039
[21,     1] loss: 0.026
[22,     1] loss: 0.020
[23,     1] loss: 0.017
[24,     1] loss: 0.014
[25,     1] loss: 0.011
[26,     1] loss: 0.009
[27,     1] loss: 0.008
[28,     1] loss: 0.007
[29,     1] loss: 0.007
[30,     1] loss: 0.006
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.006
[34,     1] loss: 0.006
[35,     1] loss: 0.006
[36,     1] loss: 0.006
[37,     1] loss: 0.006
[38,     1] loss: 0.006
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.008
[43,     1] loss: 0.008
[44,     1] loss: 0.007
[45,     1] loss: 0.008
[46,     1] loss: 0.007
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.006
[52,     1] loss: 0.007
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.006
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
Early stopping applied (best metric=0.34993433952331543)
Finished Training
Total time taken: 163.4692771434784
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.671
[3,     1] loss: 0.629
[4,     1] loss: 0.582
[5,     1] loss: 0.533
[6,     1] loss: 0.477
[7,     1] loss: 0.426
[8,     1] loss: 0.362
[9,     1] loss: 0.294
[10,     1] loss: 0.239
[11,     1] loss: 0.183
[12,     1] loss: 0.152
[13,     1] loss: 0.123
[14,     1] loss: 0.101
[15,     1] loss: 0.073
[16,     1] loss: 0.056
[17,     1] loss: 0.046
[18,     1] loss: 0.030
[19,     1] loss: 0.039
[20,     1] loss: 0.025
[21,     1] loss: 0.023
[22,     1] loss: 0.012
[23,     1] loss: 0.012
[24,     1] loss: 0.009
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.005
[28,     1] loss: 0.005
[29,     1] loss: 0.004
[30,     1] loss: 0.004
[31,     1] loss: 0.005
[32,     1] loss: 0.004
[33,     1] loss: 0.004
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.005
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.005
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.004
[56,     1] loss: 0.004
[57,     1] loss: 0.004
[58,     1] loss: 0.004
[59,     1] loss: 0.004
[60,     1] loss: 0.004
Early stopping applied (best metric=0.32249733805656433)
Finished Training
Total time taken: 163.63141703605652
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.688
[3,     1] loss: 0.659
[4,     1] loss: 0.628
[5,     1] loss: 0.591
[6,     1] loss: 0.548
[7,     1] loss: 0.495
[8,     1] loss: 0.434
[9,     1] loss: 0.378
[10,     1] loss: 0.315
[11,     1] loss: 0.260
[12,     1] loss: 0.213
[13,     1] loss: 0.171
[14,     1] loss: 0.148
[15,     1] loss: 0.112
[16,     1] loss: 0.094
[17,     1] loss: 0.101
[18,     1] loss: 0.053
[19,     1] loss: 0.040
[20,     1] loss: 0.061
[21,     1] loss: 0.036
[22,     1] loss: 0.028
[23,     1] loss: 0.020
[24,     1] loss: 0.021
[25,     1] loss: 0.027
[26,     1] loss: 0.020
[27,     1] loss: 0.013
[28,     1] loss: 0.012
[29,     1] loss: 0.012
[30,     1] loss: 0.010
[31,     1] loss: 0.009
[32,     1] loss: 0.008
[33,     1] loss: 0.008
[34,     1] loss: 0.007
[35,     1] loss: 0.008
[36,     1] loss: 0.007
[37,     1] loss: 0.007
[38,     1] loss: 0.007
[39,     1] loss: 0.007
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.007
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.006
[46,     1] loss: 0.006
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
Early stopping applied (best metric=0.36440667510032654)
Finished Training
Total time taken: 162.44451570510864
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.679
[3,     1] loss: 0.645
[4,     1] loss: 0.612
[5,     1] loss: 0.575
[6,     1] loss: 0.543
[7,     1] loss: 0.508
[8,     1] loss: 0.477
[9,     1] loss: 0.447
[10,     1] loss: 0.420
[11,     1] loss: 0.391
[12,     1] loss: 0.370
[13,     1] loss: 0.374
[14,     1] loss: 0.354
[15,     1] loss: 0.340
[16,     1] loss: 0.320
[17,     1] loss: 0.307
[18,     1] loss: 0.302
[19,     1] loss: 0.288
[20,     1] loss: 0.278
[21,     1] loss: 0.269
[22,     1] loss: 0.256
[23,     1] loss: 0.243
[24,     1] loss: 0.226
[25,     1] loss: 0.210
[26,     1] loss: 0.194
[27,     1] loss: 0.179
[28,     1] loss: 0.160
[29,     1] loss: 0.146
[30,     1] loss: 0.126
[31,     1] loss: 0.116
[32,     1] loss: 0.109
[33,     1] loss: 0.090
[34,     1] loss: 0.060
[35,     1] loss: 0.046
[36,     1] loss: 0.039
[37,     1] loss: 0.022
[38,     1] loss: 0.017
[39,     1] loss: 0.013
[40,     1] loss: 0.012
[41,     1] loss: 0.012
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.018
[47,     1] loss: 0.062
[48,     1] loss: 0.156
[49,     1] loss: 0.036
[50,     1] loss: 0.122
[51,     1] loss: 0.065
[52,     1] loss: 0.037
[53,     1] loss: 0.040
[54,     1] loss: 0.041
[55,     1] loss: 0.041
[56,     1] loss: 0.038
[57,     1] loss: 0.032
Early stopping applied (best metric=0.40070778131484985)
Finished Training
Total time taken: 162.19048357009888
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.670
[3,     1] loss: 0.633
[4,     1] loss: 0.596
[5,     1] loss: 0.552
[6,     1] loss: 0.519
[7,     1] loss: 0.488
[8,     1] loss: 0.450
[9,     1] loss: 0.431
[10,     1] loss: 0.426
[11,     1] loss: 0.391
[12,     1] loss: 0.374
[13,     1] loss: 0.355
[14,     1] loss: 0.348
[15,     1] loss: 0.340
[16,     1] loss: 0.332
[17,     1] loss: 0.323
[18,     1] loss: 0.315
[19,     1] loss: 0.304
[20,     1] loss: 0.293
[21,     1] loss: 0.280
[22,     1] loss: 0.265
[23,     1] loss: 0.246
[24,     1] loss: 0.228
[25,     1] loss: 0.207
[26,     1] loss: 0.185
[27,     1] loss: 0.164
[28,     1] loss: 0.147
[29,     1] loss: 0.126
[30,     1] loss: 0.109
[31,     1] loss: 0.090
[32,     1] loss: 0.066
[33,     1] loss: 0.047
[34,     1] loss: 0.051
[35,     1] loss: 0.106
[36,     1] loss: 0.090
[37,     1] loss: 0.036
[38,     1] loss: 0.160
[39,     1] loss: 0.033
[40,     1] loss: 0.028
[41,     1] loss: 0.035
[42,     1] loss: 0.026
[43,     1] loss: 0.026
[44,     1] loss: 0.026
[45,     1] loss: 0.024
[46,     1] loss: 0.022
[47,     1] loss: 0.020
[48,     1] loss: 0.019
[49,     1] loss: 0.017
[50,     1] loss: 0.017
[51,     1] loss: 0.016
[52,     1] loss: 0.015
[53,     1] loss: 0.014
[54,     1] loss: 0.013
Early stopping applied (best metric=0.44298118352890015)
Finished Training
Total time taken: 151.49501395225525
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.653
[5,     1] loss: 0.630
[6,     1] loss: 0.597
[7,     1] loss: 0.553
[8,     1] loss: 0.506
[9,     1] loss: 0.458
[10,     1] loss: 0.405
[11,     1] loss: 0.353
[12,     1] loss: 0.310
[13,     1] loss: 0.253
[14,     1] loss: 0.205
[15,     1] loss: 0.150
[16,     1] loss: 0.109
[17,     1] loss: 0.101
[18,     1] loss: 0.051
[19,     1] loss: 0.073
[20,     1] loss: 0.033
[21,     1] loss: 0.075
[22,     1] loss: 0.079
[23,     1] loss: 0.025
[24,     1] loss: 0.073
[25,     1] loss: 0.025
[26,     1] loss: 0.020
[27,     1] loss: 0.018
[28,     1] loss: 0.017
[29,     1] loss: 0.018
[30,     1] loss: 0.016
[31,     1] loss: 0.013
[32,     1] loss: 0.013
[33,     1] loss: 0.013
[34,     1] loss: 0.013
[35,     1] loss: 0.012
[36,     1] loss: 0.011
[37,     1] loss: 0.010
[38,     1] loss: 0.010
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.008
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.007
[46,     1] loss: 0.007
[47,     1] loss: 0.007
[48,     1] loss: 0.006
[49,     1] loss: 0.007
[50,     1] loss: 0.006
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.005
[61,     1] loss: 0.005
Early stopping applied (best metric=0.35167014598846436)
Finished Training
Total time taken: 166.94328498840332
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.693
[3,     1] loss: 0.661
[4,     1] loss: 0.627
[5,     1] loss: 0.582
[6,     1] loss: 0.528
[7,     1] loss: 0.461
[8,     1] loss: 0.387
[9,     1] loss: 0.318
[10,     1] loss: 0.261
[11,     1] loss: 0.213
[12,     1] loss: 0.168
[13,     1] loss: 0.139
[14,     1] loss: 0.120
[15,     1] loss: 0.086
[16,     1] loss: 0.074
[17,     1] loss: 0.063
[18,     1] loss: 0.075
[19,     1] loss: 0.052
[20,     1] loss: 0.052
[21,     1] loss: 0.034
[22,     1] loss: 0.030
[23,     1] loss: 0.023
[24,     1] loss: 0.019
[25,     1] loss: 0.014
[26,     1] loss: 0.011
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.007
[31,     1] loss: 0.006
[32,     1] loss: 0.006
[33,     1] loss: 0.005
[34,     1] loss: 0.005
[35,     1] loss: 0.005
[36,     1] loss: 0.005
[37,     1] loss: 0.006
[38,     1] loss: 0.006
[39,     1] loss: 0.006
[40,     1] loss: 0.006
[41,     1] loss: 0.006
[42,     1] loss: 0.006
[43,     1] loss: 0.007
[44,     1] loss: 0.007
[45,     1] loss: 0.006
[46,     1] loss: 0.007
[47,     1] loss: 0.006
[48,     1] loss: 0.006
[49,     1] loss: 0.006
[50,     1] loss: 0.006
[51,     1] loss: 0.005
[52,     1] loss: 0.005
[53,     1] loss: 0.005
[54,     1] loss: 0.005
[55,     1] loss: 0.005
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
[59,     1] loss: 0.005
[60,     1] loss: 0.004
Early stopping applied (best metric=0.3801476061344147)
Finished Training
Total time taken: 170.03794646263123
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.674
[3,     1] loss: 0.630
[4,     1] loss: 0.579
[5,     1] loss: 0.524
[6,     1] loss: 0.465
[7,     1] loss: 0.386
[8,     1] loss: 0.312
[9,     1] loss: 0.246
[10,     1] loss: 0.199
[11,     1] loss: 0.158
[12,     1] loss: 0.121
[13,     1] loss: 0.088
[14,     1] loss: 0.058
[15,     1] loss: 0.049
[16,     1] loss: 0.038
[17,     1] loss: 0.047
[18,     1] loss: 0.046
[19,     1] loss: 0.031
[20,     1] loss: 0.020
[21,     1] loss: 0.025
[22,     1] loss: 0.016
[23,     1] loss: 0.013
[24,     1] loss: 0.011
[25,     1] loss: 0.009
[26,     1] loss: 0.010
[27,     1] loss: 0.018
[28,     1] loss: 0.034
[29,     1] loss: 0.039
[30,     1] loss: 0.008
[31,     1] loss: 0.009
[32,     1] loss: 0.024
[33,     1] loss: 0.012
[34,     1] loss: 0.016
[35,     1] loss: 0.012
[36,     1] loss: 0.012
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.013
[40,     1] loss: 0.011
[41,     1] loss: 0.012
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.005
Early stopping applied (best metric=0.4496520161628723)
Finished Training
Total time taken: 166.00717782974243
{'Hydroxylation-P Validation Accuracy': 0.686878757423481, 'Hydroxylation-P Validation Sensitivity': 0.8737777777777778, 'Hydroxylation-P Validation Specificity': 0.6467140505760886, 'Hydroxylation-P Validation Precision': 0.356472135044211, 'Hydroxylation-P AUC ROC': 0.8456837319994585, 'Hydroxylation-P AUC PR': 0.562636920090774, 'Hydroxylation-P MCC': 0.4060393791418142, 'Hydroxylation-P F1': 0.5030431918235704, 'Validation Loss (Hydroxylation-P)': 0.3976162254810333, 'Validation Loss (total)': 0.3976162254810333}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005299665950943373,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3584168024,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.685045817630637}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0046331968742274075,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 982242011,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.77007456894077}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.667
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006200334992578366,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 93804072,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.326611294904307}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.680
[3,     1] loss: 0.638
[4,     1] loss: 0.599
[5,     1] loss: 0.559
[6,     1] loss: 0.513
[7,     1] loss: 0.465
[8,     1] loss: 0.410
[9,     1] loss: 0.353
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0070696472069292715,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1884434848,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.814392040650173}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.651
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004249290683399387,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 680395730,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.643363964159789}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.647
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008196804666938583,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2112321535,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.265860621741034}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.711
[2,     1] loss: 0.699
[3,     1] loss: 0.680
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007165753201128437,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4278081516,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.123903076460204}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.697
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005362906915699763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1166559545,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.5243381906794644}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.679
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009372858761883119,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3357662593,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.35120513398524}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.655
[4,     1] loss: 0.634
[5,     1] loss: 0.609
[6,     1] loss: 0.588
[7,     1] loss: 0.566
[8,     1] loss: 0.547
[9,     1] loss: 0.527
[10,     1] loss: 0.505
[11,     1] loss: 0.487
[12,     1] loss: 0.474
[13,     1] loss: 0.478
[14,     1] loss: 0.447
[15,     1] loss: 0.462
[16,     1] loss: 0.431
[17,     1] loss: 0.416
[18,     1] loss: 0.393
[19,     1] loss: 0.374
[20,     1] loss: 0.357
[21,     1] loss: 0.340
[22,     1] loss: 0.360
[23,     1] loss: 0.391
[24,     1] loss: 0.473
[25,     1] loss: 0.489
[26,     1] loss: 0.504
[27,     1] loss: 0.503
[28,     1] loss: 0.496
[29,     1] loss: 0.488
[30,     1] loss: 0.475
[31,     1] loss: 0.464
[32,     1] loss: 0.448
[33,     1] loss: 0.441
[34,     1] loss: 0.433
[35,     1] loss: 0.446
[36,     1] loss: 0.433
[37,     1] loss: 0.405
[38,     1] loss: 0.394
[39,     1] loss: 0.389
[40,     1] loss: 0.358
[41,     1] loss: 0.379
[42,     1] loss: 0.426
[43,     1] loss: 0.378
[44,     1] loss: 0.350
[45,     1] loss: 0.332
[46,     1] loss: 0.307
[47,     1] loss: 0.286
[48,     1] loss: 0.280
[49,     1] loss: 0.316
[50,     1] loss: 0.361
[51,     1] loss: 0.301
[52,     1] loss: 0.272
[53,     1] loss: 0.266
[54,     1] loss: 0.243
[55,     1] loss: 0.229
[56,     1] loss: 0.239
[57,     1] loss: 0.281
[58,     1] loss: 0.232
[59,     1] loss: 0.219
[60,     1] loss: 0.239
[61,     1] loss: 0.197
[62,     1] loss: 0.227
[63,     1] loss: 0.284
[64,     1] loss: 0.301
[65,     1] loss: 0.290
[66,     1] loss: 0.280
[67,     1] loss: 0.277
[68,     1] loss: 0.266
[69,     1] loss: 0.256
[70,     1] loss: 0.252
[71,     1] loss: 0.254
[72,     1] loss: 0.294
[73,     1] loss: 0.344
[74,     1] loss: 0.567
[75,     1] loss: 0.508
[76,     1] loss: 0.532
[77,     1] loss: 0.537
[78,     1] loss: 0.522
[79,     1] loss: 0.507
[80,     1] loss: 0.492
[81,     1] loss: 0.482
[82,     1] loss: 0.473
[83,     1] loss: 0.462
[84,     1] loss: 0.450
[85,     1] loss: 0.436
[86,     1] loss: 0.424
[87,     1] loss: 0.437
[88,     1] loss: 0.403
[89,     1] loss: 0.417
[90,     1] loss: 0.447
[91,     1] loss: 0.426
[92,     1] loss: 0.415
[93,     1] loss: 0.378
[94,     1] loss: 0.353
[95,     1] loss: 0.337
[96,     1] loss: 0.338
[97,     1] loss: 0.359
[98,     1] loss: 0.448
[99,     1] loss: 0.408
[100,     1] loss: 0.382
[101,     1] loss: 0.356
[102,     1] loss: 0.333
[103,     1] loss: 0.308
[104,     1] loss: 0.290
[105,     1] loss: 0.311
[106,     1] loss: 0.402
[107,     1] loss: 0.358
Early stopping applied (best metric=0.3708333373069763)
Finished Training
Total time taken: 301.91400051116943
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.658
[5,     1] loss: 0.644
[6,     1] loss: 0.630
[7,     1] loss: 0.615
[8,     1] loss: 0.600
[9,     1] loss: 0.583
[10,     1] loss: 0.566
[11,     1] loss: 0.548
[12,     1] loss: 0.527
[13,     1] loss: 0.508
[14,     1] loss: 0.497
[15,     1] loss: 0.491
[16,     1] loss: 0.464
[17,     1] loss: 0.444
[18,     1] loss: 0.422
[19,     1] loss: 0.401
[20,     1] loss: 0.374
[21,     1] loss: 0.351
[22,     1] loss: 0.333
[23,     1] loss: 0.309
[24,     1] loss: 0.288
[25,     1] loss: 0.268
[26,     1] loss: 0.244
[27,     1] loss: 0.227
[28,     1] loss: 0.261
[29,     1] loss: 0.415
[30,     1] loss: 0.537
[31,     1] loss: 0.704
[32,     1] loss: 0.560
[33,     1] loss: 0.528
[34,     1] loss: 0.565
[35,     1] loss: 0.575
[36,     1] loss: 0.582
[37,     1] loss: 0.591
[38,     1] loss: 0.590
[39,     1] loss: 0.588
[40,     1] loss: 0.585
[41,     1] loss: 0.583
[42,     1] loss: 0.579
[43,     1] loss: 0.574
[44,     1] loss: 0.569
[45,     1] loss: 0.564
[46,     1] loss: 0.556
[47,     1] loss: 0.548
[48,     1] loss: 0.538
[49,     1] loss: 0.529
[50,     1] loss: 0.517
[51,     1] loss: 0.512
[52,     1] loss: 0.504
[53,     1] loss: 0.526
[54,     1] loss: 0.513
[55,     1] loss: 0.484
[56,     1] loss: 0.463
[57,     1] loss: 0.445
[58,     1] loss: 0.438
[59,     1] loss: 0.424
[60,     1] loss: 0.446
[61,     1] loss: 0.424
[62,     1] loss: 0.463
[63,     1] loss: 0.431
[64,     1] loss: 0.410
[65,     1] loss: 0.381
[66,     1] loss: 0.362
[67,     1] loss: 0.340
[68,     1] loss: 0.326
[69,     1] loss: 0.309
[70,     1] loss: 0.287
[71,     1] loss: 0.270
[72,     1] loss: 0.249
[73,     1] loss: 0.227
[74,     1] loss: 0.217
[75,     1] loss: 0.208
[76,     1] loss: 0.243
Early stopping applied (best metric=0.3870077431201935)
Finished Training
Total time taken: 214.7941243648529
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.665
[5,     1] loss: 0.654
[6,     1] loss: 0.642
[7,     1] loss: 0.630
[8,     1] loss: 0.617
[9,     1] loss: 0.604
[10,     1] loss: 0.588
[11,     1] loss: 0.570
[12,     1] loss: 0.553
[13,     1] loss: 0.534
[14,     1] loss: 0.518
[15,     1] loss: 0.518
[16,     1] loss: 0.505
[17,     1] loss: 0.493
[18,     1] loss: 0.477
[19,     1] loss: 0.459
[20,     1] loss: 0.444
[21,     1] loss: 0.455
[22,     1] loss: 0.421
[23,     1] loss: 0.403
[24,     1] loss: 0.407
[25,     1] loss: 0.402
[26,     1] loss: 0.467
[27,     1] loss: 0.439
[28,     1] loss: 0.463
[29,     1] loss: 0.450
[30,     1] loss: 0.465
[31,     1] loss: 0.477
[32,     1] loss: 0.461
[33,     1] loss: 0.480
[34,     1] loss: 0.475
[35,     1] loss: 0.450
[36,     1] loss: 0.431
[37,     1] loss: 0.419
[38,     1] loss: 0.409
[39,     1] loss: 0.416
[40,     1] loss: 0.441
[41,     1] loss: 0.409
[42,     1] loss: 0.439
[43,     1] loss: 0.494
[44,     1] loss: 0.465
[45,     1] loss: 0.466
[46,     1] loss: 0.453
[47,     1] loss: 0.444
[48,     1] loss: 0.439
[49,     1] loss: 0.437
[50,     1] loss: 0.447
[51,     1] loss: 0.423
[52,     1] loss: 0.426
[53,     1] loss: 0.429
[54,     1] loss: 0.412
[55,     1] loss: 0.418
[56,     1] loss: 0.442
[57,     1] loss: 0.414
[58,     1] loss: 0.417
[59,     1] loss: 0.432
[60,     1] loss: 0.414
[61,     1] loss: 0.412
[62,     1] loss: 0.412
[63,     1] loss: 0.448
[64,     1] loss: 0.420
[65,     1] loss: 0.428
[66,     1] loss: 0.433
[67,     1] loss: 0.474
[68,     1] loss: 0.575
[69,     1] loss: 0.594
[70,     1] loss: 0.607
[71,     1] loss: 0.599
[72,     1] loss: 0.592
[73,     1] loss: 0.589
[74,     1] loss: 0.584
[75,     1] loss: 0.582
[76,     1] loss: 0.580
[77,     1] loss: 0.576
[78,     1] loss: 0.570
[79,     1] loss: 0.566
[80,     1] loss: 0.561
[81,     1] loss: 0.559
Early stopping applied (best metric=0.41286250948905945)
Finished Training
Total time taken: 233.54398679733276
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.696
[3,     1] loss: 0.681
[4,     1] loss: 0.668
[5,     1] loss: 0.656
[6,     1] loss: 0.642
[7,     1] loss: 0.625
[8,     1] loss: 0.608
[9,     1] loss: 0.587
[10,     1] loss: 0.566
[11,     1] loss: 0.539
[12,     1] loss: 0.509
[13,     1] loss: 0.479
[14,     1] loss: 0.447
[15,     1] loss: 0.421
[16,     1] loss: 0.466
[17,     1] loss: 0.408
[18,     1] loss: 0.412
[19,     1] loss: 0.435
[20,     1] loss: 0.424
[21,     1] loss: 0.386
[22,     1] loss: 0.378
[23,     1] loss: 0.346
[24,     1] loss: 0.312
[25,     1] loss: 0.282
[26,     1] loss: 0.259
[27,     1] loss: 0.238
[28,     1] loss: 0.238
[29,     1] loss: 0.206
[30,     1] loss: 0.193
[31,     1] loss: 0.181
[32,     1] loss: 0.223
[33,     1] loss: 0.376
[34,     1] loss: 0.970
[35,     1] loss: 0.584
[36,     1] loss: 0.582
[37,     1] loss: 0.621
[38,     1] loss: 0.637
[39,     1] loss: 0.646
[40,     1] loss: 0.651
[41,     1] loss: 0.655
[42,     1] loss: 0.660
[43,     1] loss: 0.663
[44,     1] loss: 0.666
[45,     1] loss: 0.669
[46,     1] loss: 0.672
[47,     1] loss: 0.675
[48,     1] loss: 0.676
[49,     1] loss: 0.679
[50,     1] loss: 0.681
[51,     1] loss: 0.683
[52,     1] loss: 0.685
[53,     1] loss: 0.686
[54,     1] loss: 0.688
[55,     1] loss: 0.689
[56,     1] loss: 0.690
[57,     1] loss: 0.690
[58,     1] loss: 0.691
[59,     1] loss: 0.692
[60,     1] loss: 0.692
[61,     1] loss: 0.692
[62,     1] loss: 0.693
[63,     1] loss: 0.693
[64,     1] loss: 0.693
[65,     1] loss: 0.693
[66,     1] loss: 0.693
[67,     1] loss: 0.693
[68,     1] loss: 0.693
[69,     1] loss: 0.693
[70,     1] loss: 0.693
[71,     1] loss: 0.693
[72,     1] loss: 0.693
[73,     1] loss: 0.693
Early stopping applied (best metric=0.36291617155075073)
Finished Training
Total time taken: 204.29276204109192
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.663
[4,     1] loss: 0.647
[5,     1] loss: 0.632
[6,     1] loss: 0.614
[7,     1] loss: 0.597
[8,     1] loss: 0.577
[9,     1] loss: 0.556
[10,     1] loss: 0.537
[11,     1] loss: 0.510
[12,     1] loss: 0.488
[13,     1] loss: 0.473
[14,     1] loss: 0.451
[15,     1] loss: 0.433
[16,     1] loss: 0.403
[17,     1] loss: 0.390
[18,     1] loss: 0.365
[19,     1] loss: 0.341
[20,     1] loss: 0.322
[21,     1] loss: 0.296
[22,     1] loss: 0.270
[23,     1] loss: 0.247
[24,     1] loss: 0.224
[25,     1] loss: 0.209
[26,     1] loss: 0.274
[27,     1] loss: 0.726
[28,     1] loss: 0.648
[29,     1] loss: 0.582
[30,     1] loss: 0.580
[31,     1] loss: 0.591
[32,     1] loss: 0.598
[33,     1] loss: 0.600
[34,     1] loss: 0.600
[35,     1] loss: 0.599
[36,     1] loss: 0.594
[37,     1] loss: 0.590
[38,     1] loss: 0.582
[39,     1] loss: 0.574
[40,     1] loss: 0.564
[41,     1] loss: 0.557
[42,     1] loss: 0.556
[43,     1] loss: 0.542
[44,     1] loss: 0.529
[45,     1] loss: 0.514
[46,     1] loss: 0.503
[47,     1] loss: 0.532
[48,     1] loss: 0.511
[49,     1] loss: 0.489
[50,     1] loss: 0.477
[51,     1] loss: 0.471
[52,     1] loss: 0.474
[53,     1] loss: 0.467
[54,     1] loss: 0.446
[55,     1] loss: 0.435
[56,     1] loss: 0.426
[57,     1] loss: 0.416
[58,     1] loss: 0.409
[59,     1] loss: 0.402
[60,     1] loss: 0.426
[61,     1] loss: 0.439
[62,     1] loss: 0.470
[63,     1] loss: 0.588
[64,     1] loss: 0.541
[65,     1] loss: 0.514
[66,     1] loss: 0.493
[67,     1] loss: 0.485
[68,     1] loss: 0.473
[69,     1] loss: 0.460
[70,     1] loss: 0.452
Early stopping applied (best metric=0.43594586849212646)
Finished Training
Total time taken: 197.74436140060425
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.676
[5,     1] loss: 0.666
[6,     1] loss: 0.655
[7,     1] loss: 0.642
[8,     1] loss: 0.629
[9,     1] loss: 0.611
[10,     1] loss: 0.594
[11,     1] loss: 0.573
[12,     1] loss: 0.553
[13,     1] loss: 0.527
[14,     1] loss: 0.506
[15,     1] loss: 0.542
[16,     1] loss: 0.506
[17,     1] loss: 0.501
[18,     1] loss: 0.473
[19,     1] loss: 0.447
[20,     1] loss: 0.424
[21,     1] loss: 0.396
[22,     1] loss: 0.416
[23,     1] loss: 0.470
[24,     1] loss: 0.407
[25,     1] loss: 0.410
[26,     1] loss: 0.468
[27,     1] loss: 0.428
[28,     1] loss: 0.413
[29,     1] loss: 0.389
[30,     1] loss: 0.365
[31,     1] loss: 0.339
[32,     1] loss: 0.310
[33,     1] loss: 0.285
[34,     1] loss: 0.265
[35,     1] loss: 0.273
[36,     1] loss: 0.234
[37,     1] loss: 0.218
[38,     1] loss: 0.215
[39,     1] loss: 0.197
[40,     1] loss: 0.218
[41,     1] loss: 0.208
[42,     1] loss: 0.195
[43,     1] loss: 0.206
[44,     1] loss: 0.498
[45,     1] loss: 0.400
[46,     1] loss: 0.539
[47,     1] loss: 0.480
[48,     1] loss: 0.480
[49,     1] loss: 0.504
[50,     1] loss: 0.497
[51,     1] loss: 0.499
[52,     1] loss: 0.487
[53,     1] loss: 0.475
[54,     1] loss: 0.462
[55,     1] loss: 0.447
[56,     1] loss: 0.431
[57,     1] loss: 0.417
[58,     1] loss: 0.397
[59,     1] loss: 0.379
[60,     1] loss: 0.360
[61,     1] loss: 0.340
[62,     1] loss: 0.328
[63,     1] loss: 0.365
[64,     1] loss: 0.442
[65,     1] loss: 0.799
[66,     1] loss: 0.776
[67,     1] loss: 0.734
[68,     1] loss: 0.708
[69,     1] loss: 0.697
[70,     1] loss: 0.693
[71,     1] loss: 0.692
[72,     1] loss: 0.692
[73,     1] loss: 0.692
[74,     1] loss: 0.693
[75,     1] loss: 0.693
[76,     1] loss: 0.693
[77,     1] loss: 0.693
[78,     1] loss: 0.693
[79,     1] loss: 0.693
[80,     1] loss: 0.693
[81,     1] loss: 0.693
[82,     1] loss: 0.693
[83,     1] loss: 0.693
[84,     1] loss: 0.693
[85,     1] loss: 0.693
[86,     1] loss: 0.693
[87,     1] loss: 0.693
[88,     1] loss: 0.693
[89,     1] loss: 0.693
[90,     1] loss: 0.693
Early stopping applied (best metric=0.36621275544166565)
Finished Training
Total time taken: 253.25965762138367
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.664
[4,     1] loss: 0.649
[5,     1] loss: 0.634
[6,     1] loss: 0.618
[7,     1] loss: 0.600
[8,     1] loss: 0.583
[9,     1] loss: 0.564
[10,     1] loss: 0.541
[11,     1] loss: 0.522
[12,     1] loss: 0.498
[13,     1] loss: 0.479
[14,     1] loss: 0.456
[15,     1] loss: 0.430
[16,     1] loss: 0.405
[17,     1] loss: 0.375
[18,     1] loss: 0.350
[19,     1] loss: 0.324
[20,     1] loss: 0.295
[21,     1] loss: 0.268
[22,     1] loss: 0.260
[23,     1] loss: 0.507
[24,     1] loss: 0.466
[25,     1] loss: 0.420
[26,     1] loss: 0.428
[27,     1] loss: 0.449
[28,     1] loss: 0.436
[29,     1] loss: 0.425
[30,     1] loss: 0.406
[31,     1] loss: 0.385
[32,     1] loss: 0.362
[33,     1] loss: 0.339
[34,     1] loss: 0.321
[35,     1] loss: 0.296
[36,     1] loss: 0.279
[37,     1] loss: 0.265
[38,     1] loss: 0.365
[39,     1] loss: 0.671
[40,     1] loss: 0.533
[41,     1] loss: 0.482
[42,     1] loss: 0.486
[43,     1] loss: 0.499
[44,     1] loss: 0.506
[45,     1] loss: 0.502
[46,     1] loss: 0.490
[47,     1] loss: 0.474
[48,     1] loss: 0.467
[49,     1] loss: 0.454
[50,     1] loss: 0.443
[51,     1] loss: 0.439
[52,     1] loss: 0.429
[53,     1] loss: 0.443
[54,     1] loss: 0.423
[55,     1] loss: 0.410
[56,     1] loss: 0.393
[57,     1] loss: 0.379
[58,     1] loss: 0.428
[59,     1] loss: 0.436
[60,     1] loss: 0.406
[61,     1] loss: 0.370
[62,     1] loss: 0.349
[63,     1] loss: 0.337
[64,     1] loss: 0.376
[65,     1] loss: 0.464
[66,     1] loss: 0.484
[67,     1] loss: 0.437
[68,     1] loss: 0.409
[69,     1] loss: 0.384
[70,     1] loss: 0.382
[71,     1] loss: 0.372
[72,     1] loss: 0.353
[73,     1] loss: 0.347
[74,     1] loss: 0.333
[75,     1] loss: 0.323
[76,     1] loss: 0.319
[77,     1] loss: 0.336
[78,     1] loss: 0.305
[79,     1] loss: 0.292
[80,     1] loss: 0.280
[81,     1] loss: 0.269
[82,     1] loss: 0.271
[83,     1] loss: 0.372
[84,     1] loss: 0.386
[85,     1] loss: 0.315
[86,     1] loss: 0.304
[87,     1] loss: 0.287
[88,     1] loss: 0.280
[89,     1] loss: 0.280
[90,     1] loss: 0.322
[91,     1] loss: 0.328
[92,     1] loss: 0.323
[93,     1] loss: 0.303
[94,     1] loss: 0.290
[95,     1] loss: 0.280
[96,     1] loss: 0.271
[97,     1] loss: 0.264
[98,     1] loss: 0.256
[99,     1] loss: 0.268
[100,     1] loss: 0.328
[101,     1] loss: 0.344
[102,     1] loss: 0.380
[103,     1] loss: 0.342
[104,     1] loss: 0.324
[105,     1] loss: 0.306
[106,     1] loss: 0.297
[107,     1] loss: 0.288
[108,     1] loss: 0.282
[109,     1] loss: 0.307
[110,     1] loss: 0.283
[111,     1] loss: 0.280
[112,     1] loss: 0.288
[113,     1] loss: 0.306
[114,     1] loss: 0.291
[115,     1] loss: 0.277
[116,     1] loss: 0.273
[117,     1] loss: 0.280
[118,     1] loss: 0.274
[119,     1] loss: 0.303
[120,     1] loss: 0.271
[121,     1] loss: 0.285
[122,     1] loss: 0.351
[123,     1] loss: 0.396
[124,     1] loss: 0.403
[125,     1] loss: 0.379
[126,     1] loss: 0.403
[127,     1] loss: 0.392
[128,     1] loss: 0.369
[129,     1] loss: 0.358
[130,     1] loss: 0.342
[131,     1] loss: 0.337
[132,     1] loss: 0.328
[133,     1] loss: 0.322
[134,     1] loss: 0.311
[135,     1] loss: 0.305
[136,     1] loss: 0.319
[137,     1] loss: 0.403
[138,     1] loss: 0.380
[139,     1] loss: 0.358
[140,     1] loss: 0.344
[141,     1] loss: 0.333
[142,     1] loss: 0.326
[143,     1] loss: 0.311
[144,     1] loss: 0.308
[145,     1] loss: 0.304
[146,     1] loss: 0.316
[147,     1] loss: 0.300
[148,     1] loss: 0.290
[149,     1] loss: 0.291
[150,     1] loss: 0.318
[151,     1] loss: 0.452
[152,     1] loss: 0.430
[153,     1] loss: 0.390
[154,     1] loss: 0.362
[155,     1] loss: 0.360
[156,     1] loss: 0.355
[157,     1] loss: 0.344
[158,     1] loss: 0.333
[159,     1] loss: 0.325
[160,     1] loss: 0.320
[161,     1] loss: 0.316
[162,     1] loss: 0.338
[163,     1] loss: 0.317
[164,     1] loss: 0.316
[165,     1] loss: 0.316
[166,     1] loss: 0.328
[167,     1] loss: 0.321
[168,     1] loss: 0.314
[169,     1] loss: 0.316
[170,     1] loss: 0.310
[171,     1] loss: 0.294
[172,     1] loss: 0.295
[173,     1] loss: 0.296
Early stopping applied (best metric=0.3557361662387848)
Finished Training
Total time taken: 503.822429895401
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.667
[3,     1] loss: 0.623
[4,     1] loss: 0.587
[5,     1] loss: 0.557
[6,     1] loss: 0.533
[7,     1] loss: 0.511
[8,     1] loss: 0.495
[9,     1] loss: 0.519
[10,     1] loss: 0.498
[11,     1] loss: 0.490
[12,     1] loss: 0.478
[13,     1] loss: 0.472
[14,     1] loss: 0.461
[15,     1] loss: 0.450
[16,     1] loss: 0.456
[17,     1] loss: 0.499
[18,     1] loss: 0.460
[19,     1] loss: 0.467
[20,     1] loss: 0.451
[21,     1] loss: 0.436
[22,     1] loss: 0.426
[23,     1] loss: 0.411
[24,     1] loss: 0.416
[25,     1] loss: 0.525
[26,     1] loss: 0.461
[27,     1] loss: 0.487
[28,     1] loss: 0.485
[29,     1] loss: 0.457
[30,     1] loss: 0.442
[31,     1] loss: 0.434
[32,     1] loss: 0.454
[33,     1] loss: 0.446
[34,     1] loss: 0.416
[35,     1] loss: 0.406
[36,     1] loss: 0.397
[37,     1] loss: 0.416
[38,     1] loss: 0.425
[39,     1] loss: 0.510
[40,     1] loss: 0.450
[41,     1] loss: 0.471
[42,     1] loss: 0.478
[43,     1] loss: 0.458
[44,     1] loss: 0.457
[45,     1] loss: 0.456
[46,     1] loss: 0.453
[47,     1] loss: 0.435
[48,     1] loss: 0.422
[49,     1] loss: 0.413
[50,     1] loss: 0.423
[51,     1] loss: 0.432
[52,     1] loss: 0.401
[53,     1] loss: 0.391
[54,     1] loss: 0.385
[55,     1] loss: 0.380
[56,     1] loss: 0.414
[57,     1] loss: 0.399
[58,     1] loss: 0.456
[59,     1] loss: 0.416
[60,     1] loss: 0.414
[61,     1] loss: 0.419
[62,     1] loss: 0.415
[63,     1] loss: 0.436
[64,     1] loss: 0.410
[65,     1] loss: 0.409
[66,     1] loss: 0.405
[67,     1] loss: 0.418
[68,     1] loss: 0.429
[69,     1] loss: 0.410
[70,     1] loss: 0.389
[71,     1] loss: 0.382
[72,     1] loss: 0.370
[73,     1] loss: 0.353
[74,     1] loss: 0.545
[75,     1] loss: 0.502
[76,     1] loss: 0.477
[77,     1] loss: 0.536
[78,     1] loss: 0.500
[79,     1] loss: 0.479
[80,     1] loss: 0.456
[81,     1] loss: 0.416
[82,     1] loss: 0.380
[83,     1] loss: 0.336
[84,     1] loss: 0.309
[85,     1] loss: 0.349
[86,     1] loss: 0.564
[87,     1] loss: 0.530
[88,     1] loss: 0.513
[89,     1] loss: 0.478
[90,     1] loss: 0.497
[91,     1] loss: 0.502
[92,     1] loss: 0.499
[93,     1] loss: 0.481
[94,     1] loss: 0.462
[95,     1] loss: 0.440
[96,     1] loss: 0.416
[97,     1] loss: 0.395
[98,     1] loss: 0.411
[99,     1] loss: 0.382
[100,     1] loss: 0.360
[101,     1] loss: 0.348
[102,     1] loss: 0.329
[103,     1] loss: 0.330
[104,     1] loss: 0.304
[105,     1] loss: 0.279
[106,     1] loss: 0.262
[107,     1] loss: 0.247
[108,     1] loss: 0.240
[109,     1] loss: 0.353
[110,     1] loss: 0.369
[111,     1] loss: 1.072
[112,     1] loss: 0.528
[113,     1] loss: 0.608
[114,     1] loss: 0.649
[115,     1] loss: 0.649
[116,     1] loss: 0.651
[117,     1] loss: 0.660
[118,     1] loss: 0.665
[119,     1] loss: 0.670
[120,     1] loss: 0.673
[121,     1] loss: 0.676
[122,     1] loss: 0.679
[123,     1] loss: 0.681
[124,     1] loss: 0.684
[125,     1] loss: 0.686
[126,     1] loss: 0.687
[127,     1] loss: 0.689
[128,     1] loss: 0.690
[129,     1] loss: 0.691
[130,     1] loss: 0.691
[131,     1] loss: 0.692
[132,     1] loss: 0.692
Early stopping applied (best metric=0.4243670105934143)
Finished Training
Total time taken: 375.5909173488617
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.654
[4,     1] loss: 0.630
[5,     1] loss: 0.601
[6,     1] loss: 0.578
[7,     1] loss: 0.550
[8,     1] loss: 0.521
[9,     1] loss: 0.496
[10,     1] loss: 0.520
[11,     1] loss: 0.496
[12,     1] loss: 0.483
[13,     1] loss: 0.467
[14,     1] loss: 0.453
[15,     1] loss: 0.432
[16,     1] loss: 0.418
[17,     1] loss: 0.406
[18,     1] loss: 0.397
[19,     1] loss: 0.391
[20,     1] loss: 0.386
[21,     1] loss: 0.426
[22,     1] loss: 0.458
[23,     1] loss: 0.589
[24,     1] loss: 0.584
[25,     1] loss: 0.527
[26,     1] loss: 0.505
[27,     1] loss: 0.487
[28,     1] loss: 0.462
[29,     1] loss: 0.444
[30,     1] loss: 0.425
[31,     1] loss: 0.410
[32,     1] loss: 0.405
[33,     1] loss: 0.397
[34,     1] loss: 0.395
[35,     1] loss: 0.385
[36,     1] loss: 0.392
[37,     1] loss: 0.431
[38,     1] loss: 0.490
[39,     1] loss: 0.441
[40,     1] loss: 0.436
[41,     1] loss: 0.413
[42,     1] loss: 0.405
[43,     1] loss: 0.395
[44,     1] loss: 0.384
[45,     1] loss: 0.382
[46,     1] loss: 0.417
[47,     1] loss: 0.417
[48,     1] loss: 0.406
[49,     1] loss: 0.402
[50,     1] loss: 0.397
[51,     1] loss: 0.389
[52,     1] loss: 0.381
[53,     1] loss: 0.376
[54,     1] loss: 0.413
[55,     1] loss: 0.441
[56,     1] loss: 0.418
[57,     1] loss: 0.402
[58,     1] loss: 0.402
Early stopping applied (best metric=0.4722931683063507)
Finished Training
Total time taken: 165.644948720932
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.668
[4,     1] loss: 0.651
[5,     1] loss: 0.627
[6,     1] loss: 0.602
[7,     1] loss: 0.571
[8,     1] loss: 0.541
[9,     1] loss: 0.524
[10,     1] loss: 0.526
[11,     1] loss: 0.483
[12,     1] loss: 0.476
[13,     1] loss: 0.456
[14,     1] loss: 0.433
[15,     1] loss: 0.416
[16,     1] loss: 0.399
[17,     1] loss: 0.386
[18,     1] loss: 0.380
[19,     1] loss: 0.362
[20,     1] loss: 0.353
[21,     1] loss: 0.342
[22,     1] loss: 0.352
[23,     1] loss: 0.397
[24,     1] loss: 0.664
[25,     1] loss: 0.500
[26,     1] loss: 0.560
[27,     1] loss: 0.548
[28,     1] loss: 0.553
[29,     1] loss: 0.559
[30,     1] loss: 0.554
[31,     1] loss: 0.549
[32,     1] loss: 0.538
[33,     1] loss: 0.523
[34,     1] loss: 0.504
[35,     1] loss: 0.485
[36,     1] loss: 0.458
[37,     1] loss: 0.432
[38,     1] loss: 0.407
[39,     1] loss: 0.377
[40,     1] loss: 0.350
[41,     1] loss: 0.328
[42,     1] loss: 0.303
[43,     1] loss: 0.278
[44,     1] loss: 0.258
[45,     1] loss: 0.240
[46,     1] loss: 0.224
[47,     1] loss: 0.215
[48,     1] loss: 0.257
[49,     1] loss: 0.544
[50,     1] loss: 0.853
[51,     1] loss: 0.634
[52,     1] loss: 0.670
[53,     1] loss: 0.676
[54,     1] loss: 0.671
[55,     1] loss: 0.670
[56,     1] loss: 0.673
[57,     1] loss: 0.677
[58,     1] loss: 0.680
[59,     1] loss: 0.683
[60,     1] loss: 0.685
[61,     1] loss: 0.687
[62,     1] loss: 0.688
[63,     1] loss: 0.689
[64,     1] loss: 0.690
[65,     1] loss: 0.691
[66,     1] loss: 0.692
[67,     1] loss: 0.692
[68,     1] loss: 0.692
[69,     1] loss: 0.693
[70,     1] loss: 0.693
[71,     1] loss: 0.693
[72,     1] loss: 0.693
[73,     1] loss: 0.693
[74,     1] loss: 0.693
[75,     1] loss: 0.693
[76,     1] loss: 0.693
[77,     1] loss: 0.693
[78,     1] loss: 0.693
[79,     1] loss: 0.693
[80,     1] loss: 0.693
[81,     1] loss: 0.693
[82,     1] loss: 0.693
[83,     1] loss: 0.693
[84,     1] loss: 0.693
[85,     1] loss: 0.693
[86,     1] loss: 0.693
[87,     1] loss: 0.693
[88,     1] loss: 0.693
[89,     1] loss: 0.693
[90,     1] loss: 0.693
[91,     1] loss: 0.693
[92,     1] loss: 0.693
[93,     1] loss: 0.693
[94,     1] loss: 0.693
[95,     1] loss: 0.693
[96,     1] loss: 0.693
Early stopping applied (best metric=0.3462589979171753)
Finished Training
Total time taken: 272.05584239959717
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.686
[3,     1] loss: 0.660
[4,     1] loss: 0.639
[5,     1] loss: 0.612
[6,     1] loss: 0.585
[7,     1] loss: 0.563
[8,     1] loss: 0.543
[9,     1] loss: 0.523
[10,     1] loss: 0.510
[11,     1] loss: 0.504
[12,     1] loss: 0.473
[13,     1] loss: 0.462
[14,     1] loss: 0.480
[15,     1] loss: 0.439
[16,     1] loss: 0.427
[17,     1] loss: 0.429
[18,     1] loss: 0.415
[19,     1] loss: 0.443
[20,     1] loss: 0.405
[21,     1] loss: 0.401
[22,     1] loss: 0.390
[23,     1] loss: 0.378
[24,     1] loss: 0.413
[25,     1] loss: 0.433
[26,     1] loss: 0.493
[27,     1] loss: 0.439
[28,     1] loss: 0.514
[29,     1] loss: 0.509
[30,     1] loss: 0.515
[31,     1] loss: 0.506
[32,     1] loss: 0.487
[33,     1] loss: 0.469
[34,     1] loss: 0.448
[35,     1] loss: 0.424
[36,     1] loss: 0.404
[37,     1] loss: 0.387
[38,     1] loss: 0.370
[39,     1] loss: 0.402
[40,     1] loss: 0.477
[41,     1] loss: 0.443
[42,     1] loss: 0.425
[43,     1] loss: 0.418
[44,     1] loss: 0.422
[45,     1] loss: 0.381
[46,     1] loss: 0.374
[47,     1] loss: 0.362
[48,     1] loss: 0.333
[49,     1] loss: 0.307
[50,     1] loss: 0.285
[51,     1] loss: 0.294
[52,     1] loss: 0.300
[53,     1] loss: 0.304
[54,     1] loss: 0.617
[55,     1] loss: 0.507
[56,     1] loss: 0.529
[57,     1] loss: 0.499
[58,     1] loss: 0.494
[59,     1] loss: 0.500
[60,     1] loss: 0.502
[61,     1] loss: 0.496
[62,     1] loss: 0.486
[63,     1] loss: 0.471
[64,     1] loss: 0.450
[65,     1] loss: 0.427
[66,     1] loss: 0.398
[67,     1] loss: 0.369
[68,     1] loss: 0.366
[69,     1] loss: 0.337
[70,     1] loss: 0.458
[71,     1] loss: 0.419
[72,     1] loss: 0.405
[73,     1] loss: 0.350
[74,     1] loss: 0.329
[75,     1] loss: 0.311
[76,     1] loss: 0.293
[77,     1] loss: 0.293
[78,     1] loss: 0.259
[79,     1] loss: 0.239
[80,     1] loss: 0.233
[81,     1] loss: 0.257
[82,     1] loss: 0.334
[83,     1] loss: 0.435
[84,     1] loss: 0.407
[85,     1] loss: 0.388
[86,     1] loss: 0.369
[87,     1] loss: 0.346
[88,     1] loss: 0.326
[89,     1] loss: 0.305
[90,     1] loss: 0.287
[91,     1] loss: 0.265
[92,     1] loss: 0.256
[93,     1] loss: 0.314
[94,     1] loss: 0.296
[95,     1] loss: 0.270
[96,     1] loss: 0.254
[97,     1] loss: 0.249
[98,     1] loss: 0.247
[99,     1] loss: 0.251
[100,     1] loss: 0.225
[101,     1] loss: 0.209
[102,     1] loss: 0.211
[103,     1] loss: 0.262
[104,     1] loss: 0.379
[105,     1] loss: 0.422
[106,     1] loss: 0.345
[107,     1] loss: 0.299
[108,     1] loss: 0.292
[109,     1] loss: 0.297
[110,     1] loss: 0.285
[111,     1] loss: 0.283
[112,     1] loss: 0.269
[113,     1] loss: 0.252
[114,     1] loss: 0.250
[115,     1] loss: 0.268
[116,     1] loss: 0.243
[117,     1] loss: 0.236
[118,     1] loss: 0.236
[119,     1] loss: 0.269
[120,     1] loss: 0.260
[121,     1] loss: 0.310
[122,     1] loss: 0.319
[123,     1] loss: 0.280
[124,     1] loss: 0.256
[125,     1] loss: 0.248
[126,     1] loss: 0.236
[127,     1] loss: 0.234
[128,     1] loss: 0.261
[129,     1] loss: 0.300
[130,     1] loss: 0.277
[131,     1] loss: 0.268
[132,     1] loss: 0.268
[133,     1] loss: 0.249
[134,     1] loss: 0.236
[135,     1] loss: 0.243
[136,     1] loss: 0.268
[137,     1] loss: 0.243
[138,     1] loss: 0.239
[139,     1] loss: 0.249
[140,     1] loss: 0.248
[141,     1] loss: 0.369
[142,     1] loss: 0.382
[143,     1] loss: 0.314
[144,     1] loss: 0.295
[145,     1] loss: 0.301
[146,     1] loss: 0.322
[147,     1] loss: 0.300
[148,     1] loss: 0.276
[149,     1] loss: 0.258
[150,     1] loss: 0.250
[151,     1] loss: 0.295
Early stopping applied (best metric=0.34162968397140503)
Finished Training
Total time taken: 428.9646599292755
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.658
[4,     1] loss: 0.639
[5,     1] loss: 0.617
[6,     1] loss: 0.596
[7,     1] loss: 0.570
[8,     1] loss: 0.546
[9,     1] loss: 0.517
[10,     1] loss: 0.489
[11,     1] loss: 0.466
[12,     1] loss: 0.436
[13,     1] loss: 0.411
[14,     1] loss: 0.385
[15,     1] loss: 0.445
[16,     1] loss: 0.504
[17,     1] loss: 0.498
[18,     1] loss: 0.491
[19,     1] loss: 0.488
[20,     1] loss: 0.483
[21,     1] loss: 0.469
[22,     1] loss: 0.451
[23,     1] loss: 0.429
[24,     1] loss: 0.403
[25,     1] loss: 0.376
[26,     1] loss: 0.350
[27,     1] loss: 0.327
[28,     1] loss: 0.387
[29,     1] loss: 0.503
[30,     1] loss: 0.441
[31,     1] loss: 0.423
[32,     1] loss: 0.406
[33,     1] loss: 0.390
[34,     1] loss: 0.380
[35,     1] loss: 0.413
[36,     1] loss: 0.372
[37,     1] loss: 0.361
[38,     1] loss: 0.331
[39,     1] loss: 0.314
[40,     1] loss: 0.289
[41,     1] loss: 0.265
[42,     1] loss: 0.246
[43,     1] loss: 0.239
[44,     1] loss: 0.250
[45,     1] loss: 0.263
[46,     1] loss: 0.268
[47,     1] loss: 0.343
[48,     1] loss: 0.687
[49,     1] loss: 0.577
[50,     1] loss: 0.560
[51,     1] loss: 0.583
[52,     1] loss: 0.591
[53,     1] loss: 0.586
[54,     1] loss: 0.583
[55,     1] loss: 0.583
[56,     1] loss: 0.582
[57,     1] loss: 0.581
[58,     1] loss: 0.576
[59,     1] loss: 0.570
[60,     1] loss: 0.564
[61,     1] loss: 0.555
[62,     1] loss: 0.545
[63,     1] loss: 0.533
[64,     1] loss: 0.521
[65,     1] loss: 0.510
[66,     1] loss: 0.498
[67,     1] loss: 0.530
[68,     1] loss: 0.498
[69,     1] loss: 0.481
[70,     1] loss: 0.481
[71,     1] loss: 0.450
[72,     1] loss: 0.433
[73,     1] loss: 0.418
[74,     1] loss: 0.473
Early stopping applied (best metric=0.394147127866745)
Finished Training
Total time taken: 207.4325873851776
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.692
[3,     1] loss: 0.670
[4,     1] loss: 0.650
[5,     1] loss: 0.630
[6,     1] loss: 0.607
[7,     1] loss: 0.588
[8,     1] loss: 0.567
[9,     1] loss: 0.543
[10,     1] loss: 0.521
[11,     1] loss: 0.499
[12,     1] loss: 0.469
[13,     1] loss: 0.457
[14,     1] loss: 0.494
[15,     1] loss: 0.430
[16,     1] loss: 0.445
[17,     1] loss: 0.418
[18,     1] loss: 0.410
[19,     1] loss: 0.374
[20,     1] loss: 0.351
[21,     1] loss: 0.331
[22,     1] loss: 0.307
[23,     1] loss: 0.286
[24,     1] loss: 0.262
[25,     1] loss: 0.243
[26,     1] loss: 0.225
[27,     1] loss: 0.213
[28,     1] loss: 0.196
[29,     1] loss: 0.182
[30,     1] loss: 0.170
[31,     1] loss: 0.209
[32,     1] loss: 0.699
[33,     1] loss: 0.567
[34,     1] loss: 0.545
[35,     1] loss: 0.512
[36,     1] loss: 0.533
[37,     1] loss: 0.524
[38,     1] loss: 0.529
[39,     1] loss: 0.530
[40,     1] loss: 0.524
[41,     1] loss: 0.517
[42,     1] loss: 0.507
[43,     1] loss: 0.494
[44,     1] loss: 0.478
[45,     1] loss: 0.466
[46,     1] loss: 0.452
[47,     1] loss: 0.439
[48,     1] loss: 0.432
[49,     1] loss: 0.438
[50,     1] loss: 0.437
[51,     1] loss: 0.425
[52,     1] loss: 0.417
[53,     1] loss: 0.406
[54,     1] loss: 0.402
[55,     1] loss: 0.397
[56,     1] loss: 0.394
[57,     1] loss: 0.394
[58,     1] loss: 0.392
[59,     1] loss: 0.392
[60,     1] loss: 0.393
[61,     1] loss: 0.393
[62,     1] loss: 0.393
[63,     1] loss: 0.395
[64,     1] loss: 0.403
[65,     1] loss: 0.485
[66,     1] loss: 0.743
[67,     1] loss: 0.648
[68,     1] loss: 0.678
[69,     1] loss: 0.687
[70,     1] loss: 0.688
[71,     1] loss: 0.687
[72,     1] loss: 0.687
[73,     1] loss: 0.687
[74,     1] loss: 0.688
[75,     1] loss: 0.689
[76,     1] loss: 0.690
[77,     1] loss: 0.691
Early stopping applied (best metric=0.3968595266342163)
Finished Training
Total time taken: 217.4245150089264
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.678
[3,     1] loss: 0.655
[4,     1] loss: 0.633
[5,     1] loss: 0.615
[6,     1] loss: 0.590
[7,     1] loss: 0.569
[8,     1] loss: 0.546
[9,     1] loss: 0.525
[10,     1] loss: 0.503
[11,     1] loss: 0.471
[12,     1] loss: 0.447
[13,     1] loss: 0.427
[14,     1] loss: 0.433
[15,     1] loss: 0.391
[16,     1] loss: 0.378
[17,     1] loss: 0.358
[18,     1] loss: 0.330
[19,     1] loss: 0.306
[20,     1] loss: 0.289
[21,     1] loss: 0.266
[22,     1] loss: 0.258
[23,     1] loss: 0.243
[24,     1] loss: 0.247
[25,     1] loss: 0.219
[26,     1] loss: 0.195
[27,     1] loss: 0.193
[28,     1] loss: 0.292
[29,     1] loss: 0.764
[30,     1] loss: 0.530
[31,     1] loss: 0.583
[32,     1] loss: 0.584
[33,     1] loss: 0.582
[34,     1] loss: 0.595
[35,     1] loss: 0.608
[36,     1] loss: 0.613
[37,     1] loss: 0.618
[38,     1] loss: 0.621
[39,     1] loss: 0.622
[40,     1] loss: 0.620
[41,     1] loss: 0.621
[42,     1] loss: 0.619
[43,     1] loss: 0.618
[44,     1] loss: 0.616
[45,     1] loss: 0.613
[46,     1] loss: 0.608
[47,     1] loss: 0.602
[48,     1] loss: 0.599
[49,     1] loss: 0.592
[50,     1] loss: 0.588
[51,     1] loss: 0.578
[52,     1] loss: 0.571
[53,     1] loss: 0.559
[54,     1] loss: 0.555
[55,     1] loss: 0.552
[56,     1] loss: 0.565
[57,     1] loss: 0.559
[58,     1] loss: 0.532
[59,     1] loss: 0.520
[60,     1] loss: 0.508
[61,     1] loss: 0.509
[62,     1] loss: 0.495
[63,     1] loss: 0.481
[64,     1] loss: 0.470
[65,     1] loss: 0.459
[66,     1] loss: 0.451
[67,     1] loss: 0.443
[68,     1] loss: 0.436
[69,     1] loss: 0.441
[70,     1] loss: 0.459
[71,     1] loss: 0.526
[72,     1] loss: 0.599
Early stopping applied (best metric=0.37285280227661133)
Finished Training
Total time taken: 204.59407329559326
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.683
[3,     1] loss: 0.658
[4,     1] loss: 0.635
[5,     1] loss: 0.612
[6,     1] loss: 0.592
[7,     1] loss: 0.571
[8,     1] loss: 0.550
[9,     1] loss: 0.531
[10,     1] loss: 0.510
[11,     1] loss: 0.487
[12,     1] loss: 0.461
[13,     1] loss: 0.441
[14,     1] loss: 0.417
[15,     1] loss: 0.395
[16,     1] loss: 0.373
[17,     1] loss: 0.357
[18,     1] loss: 0.353
[19,     1] loss: 0.398
[20,     1] loss: 0.361
[21,     1] loss: 0.365
[22,     1] loss: 0.351
[23,     1] loss: 0.329
[24,     1] loss: 0.309
[25,     1] loss: 0.284
[26,     1] loss: 0.263
[27,     1] loss: 0.253
[28,     1] loss: 0.232
[29,     1] loss: 0.231
[30,     1] loss: 0.213
[31,     1] loss: 0.207
[32,     1] loss: 0.215
[33,     1] loss: 0.203
[34,     1] loss: 0.195
[35,     1] loss: 0.188
[36,     1] loss: 0.186
[37,     1] loss: 0.271
[38,     1] loss: 0.823
[39,     1] loss: 0.659
[40,     1] loss: 0.618
[41,     1] loss: 0.612
[42,     1] loss: 0.624
[43,     1] loss: 0.635
[44,     1] loss: 0.642
[45,     1] loss: 0.647
[46,     1] loss: 0.651
[47,     1] loss: 0.654
[48,     1] loss: 0.657
[49,     1] loss: 0.660
[50,     1] loss: 0.663
[51,     1] loss: 0.665
[52,     1] loss: 0.667
[53,     1] loss: 0.669
[54,     1] loss: 0.672
[55,     1] loss: 0.674
[56,     1] loss: 0.676
[57,     1] loss: 0.678
[58,     1] loss: 0.679
[59,     1] loss: 0.681
[60,     1] loss: 0.682
[61,     1] loss: 0.684
[62,     1] loss: 0.685
[63,     1] loss: 0.686
[64,     1] loss: 0.688
[65,     1] loss: 0.689
[66,     1] loss: 0.689
[67,     1] loss: 0.690
[68,     1] loss: 0.691
[69,     1] loss: 0.691
[70,     1] loss: 0.692
[71,     1] loss: 0.692
[72,     1] loss: 0.692
[73,     1] loss: 0.693
[74,     1] loss: 0.693
[75,     1] loss: 0.693
[76,     1] loss: 0.693
[77,     1] loss: 0.693
[78,     1] loss: 0.693
[79,     1] loss: 0.693
[80,     1] loss: 0.693
[81,     1] loss: 0.693
[82,     1] loss: 0.693
[83,     1] loss: 0.693
Early stopping applied (best metric=0.36707159876823425)
Finished Training
Total time taken: 235.75838208198547
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.659
[4,     1] loss: 0.640
[5,     1] loss: 0.617
[6,     1] loss: 0.595
[7,     1] loss: 0.575
[8,     1] loss: 0.551
[9,     1] loss: 0.528
[10,     1] loss: 0.506
[11,     1] loss: 0.483
[12,     1] loss: 0.458
[13,     1] loss: 0.436
[14,     1] loss: 0.413
[15,     1] loss: 0.428
[16,     1] loss: 0.411
[17,     1] loss: 0.416
[18,     1] loss: 0.401
[19,     1] loss: 0.396
[20,     1] loss: 0.373
[21,     1] loss: 0.357
[22,     1] loss: 0.337
[23,     1] loss: 0.324
[24,     1] loss: 0.307
[25,     1] loss: 0.321
[26,     1] loss: 0.288
[27,     1] loss: 0.273
[28,     1] loss: 0.269
[29,     1] loss: 0.253
[30,     1] loss: 0.290
[31,     1] loss: 0.654
[32,     1] loss: 0.467
[33,     1] loss: 0.476
[34,     1] loss: 0.507
[35,     1] loss: 0.528
[36,     1] loss: 0.539
[37,     1] loss: 0.540
[38,     1] loss: 0.537
[39,     1] loss: 0.530
[40,     1] loss: 0.522
[41,     1] loss: 0.509
[42,     1] loss: 0.496
[43,     1] loss: 0.483
[44,     1] loss: 0.470
[45,     1] loss: 0.457
[46,     1] loss: 0.443
[47,     1] loss: 0.476
[48,     1] loss: 0.483
[49,     1] loss: 0.443
[50,     1] loss: 0.433
[51,     1] loss: 0.403
[52,     1] loss: 0.389
[53,     1] loss: 0.369
[54,     1] loss: 0.345
[55,     1] loss: 0.331
[56,     1] loss: 0.356
[57,     1] loss: 0.459
[58,     1] loss: 0.779
[59,     1] loss: 0.731
[60,     1] loss: 0.638
[61,     1] loss: 0.598
[62,     1] loss: 0.616
[63,     1] loss: 0.619
[64,     1] loss: 0.622
[65,     1] loss: 0.626
Early stopping applied (best metric=0.43135514855384827)
Finished Training
Total time taken: 185.5023787021637
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.664
[5,     1] loss: 0.647
[6,     1] loss: 0.630
[7,     1] loss: 0.609
[8,     1] loss: 0.587
[9,     1] loss: 0.564
[10,     1] loss: 0.540
[11,     1] loss: 0.524
[12,     1] loss: 0.542
[13,     1] loss: 0.489
[14,     1] loss: 0.516
[15,     1] loss: 0.488
[16,     1] loss: 0.502
[17,     1] loss: 0.465
[18,     1] loss: 0.447
[19,     1] loss: 0.420
[20,     1] loss: 0.401
[21,     1] loss: 0.378
[22,     1] loss: 0.360
[23,     1] loss: 0.346
[24,     1] loss: 0.331
[25,     1] loss: 0.318
[26,     1] loss: 0.302
[27,     1] loss: 0.288
[28,     1] loss: 0.279
[29,     1] loss: 0.261
[30,     1] loss: 0.247
[31,     1] loss: 0.262
[32,     1] loss: 0.311
[33,     1] loss: 0.563
[34,     1] loss: 0.551
[35,     1] loss: 0.543
[36,     1] loss: 0.503
[37,     1] loss: 0.499
[38,     1] loss: 0.512
[39,     1] loss: 0.499
[40,     1] loss: 0.475
[41,     1] loss: 0.454
[42,     1] loss: 0.430
[43,     1] loss: 0.406
[44,     1] loss: 0.377
[45,     1] loss: 0.349
[46,     1] loss: 0.323
[47,     1] loss: 0.297
[48,     1] loss: 0.273
[49,     1] loss: 0.250
[50,     1] loss: 0.230
[51,     1] loss: 0.272
[52,     1] loss: 0.287
[53,     1] loss: 0.419
[54,     1] loss: 0.689
[55,     1] loss: 0.610
[56,     1] loss: 0.529
[57,     1] loss: 0.512
[58,     1] loss: 0.524
[59,     1] loss: 0.533
[60,     1] loss: 0.524
[61,     1] loss: 0.514
[62,     1] loss: 0.509
[63,     1] loss: 0.502
[64,     1] loss: 0.504
[65,     1] loss: 0.495
[66,     1] loss: 0.491
[67,     1] loss: 0.489
[68,     1] loss: 0.484
[69,     1] loss: 0.488
[70,     1] loss: 0.485
[71,     1] loss: 0.541
[72,     1] loss: 0.492
[73,     1] loss: 0.492
[74,     1] loss: 0.503
[75,     1] loss: 0.592
[76,     1] loss: 0.585
[77,     1] loss: 0.590
[78,     1] loss: 0.577
[79,     1] loss: 0.580
[80,     1] loss: 0.581
[81,     1] loss: 0.584
[82,     1] loss: 0.585
[83,     1] loss: 0.583
[84,     1] loss: 0.579
[85,     1] loss: 0.575
[86,     1] loss: 0.572
[87,     1] loss: 0.568
[88,     1] loss: 0.561
[89,     1] loss: 0.558
[90,     1] loss: 0.556
[91,     1] loss: 0.565
[92,     1] loss: 0.554
[93,     1] loss: 0.545
[94,     1] loss: 0.544
[95,     1] loss: 0.568
[96,     1] loss: 0.572
[97,     1] loss: 0.604
[98,     1] loss: 0.581
[99,     1] loss: 0.571
Early stopping applied (best metric=0.38071000576019287)
Finished Training
Total time taken: 281.8461136817932
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.649
[6,     1] loss: 0.632
[7,     1] loss: 0.617
[8,     1] loss: 0.599
[9,     1] loss: 0.579
[10,     1] loss: 0.557
[11,     1] loss: 0.535
[12,     1] loss: 0.505
[13,     1] loss: 0.477
[14,     1] loss: 0.468
[15,     1] loss: 0.523
[16,     1] loss: 0.494
[17,     1] loss: 0.481
[18,     1] loss: 0.461
[19,     1] loss: 0.440
[20,     1] loss: 0.415
[21,     1] loss: 0.385
[22,     1] loss: 0.366
[23,     1] loss: 0.415
[24,     1] loss: 0.538
[25,     1] loss: 0.512
[26,     1] loss: 0.496
[27,     1] loss: 0.505
[28,     1] loss: 0.504
[29,     1] loss: 0.496
[30,     1] loss: 0.481
[31,     1] loss: 0.463
[32,     1] loss: 0.437
[33,     1] loss: 0.412
[34,     1] loss: 0.398
[35,     1] loss: 0.371
[36,     1] loss: 0.383
[37,     1] loss: 0.368
[38,     1] loss: 0.336
[39,     1] loss: 0.303
[40,     1] loss: 0.275
[41,     1] loss: 0.246
[42,     1] loss: 0.220
[43,     1] loss: 0.222
[44,     1] loss: 0.335
[45,     1] loss: 1.082
[46,     1] loss: 0.754
[47,     1] loss: 0.620
[48,     1] loss: 0.631
[49,     1] loss: 0.650
[50,     1] loss: 0.654
[51,     1] loss: 0.656
[52,     1] loss: 0.657
[53,     1] loss: 0.659
[54,     1] loss: 0.661
[55,     1] loss: 0.663
[56,     1] loss: 0.664
[57,     1] loss: 0.666
[58,     1] loss: 0.668
[59,     1] loss: 0.669
[60,     1] loss: 0.671
[61,     1] loss: 0.673
[62,     1] loss: 0.674
[63,     1] loss: 0.676
[64,     1] loss: 0.677
[65,     1] loss: 0.679
[66,     1] loss: 0.680
[67,     1] loss: 0.682
[68,     1] loss: 0.683
[69,     1] loss: 0.685
[70,     1] loss: 0.686
[71,     1] loss: 0.687
[72,     1] loss: 0.688
[73,     1] loss: 0.689
[74,     1] loss: 0.690
[75,     1] loss: 0.691
[76,     1] loss: 0.691
[77,     1] loss: 0.692
[78,     1] loss: 0.692
[79,     1] loss: 0.692
[80,     1] loss: 0.693
[81,     1] loss: 0.693
[82,     1] loss: 0.693
[83,     1] loss: 0.693
[84,     1] loss: 0.693
[85,     1] loss: 0.693
[86,     1] loss: 0.693
[87,     1] loss: 0.693
[88,     1] loss: 0.693
[89,     1] loss: 0.693
[90,     1] loss: 0.693
Early stopping applied (best metric=0.3689330220222473)
Finished Training
Total time taken: 256.966082572937
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.652
[4,     1] loss: 0.629
[5,     1] loss: 0.605
[6,     1] loss: 0.579
[7,     1] loss: 0.556
[8,     1] loss: 0.535
[9,     1] loss: 0.511
[10,     1] loss: 0.488
[11,     1] loss: 0.466
[12,     1] loss: 0.438
[13,     1] loss: 0.416
[14,     1] loss: 0.394
[15,     1] loss: 0.376
[16,     1] loss: 0.347
[17,     1] loss: 0.319
[18,     1] loss: 0.294
[19,     1] loss: 0.269
[20,     1] loss: 0.248
[21,     1] loss: 0.225
[22,     1] loss: 0.209
[23,     1] loss: 0.238
[24,     1] loss: 0.448
[25,     1] loss: 0.430
[26,     1] loss: 0.421
[27,     1] loss: 0.454
[28,     1] loss: 0.468
[29,     1] loss: 0.481
[30,     1] loss: 0.484
[31,     1] loss: 0.473
[32,     1] loss: 0.458
[33,     1] loss: 0.440
[34,     1] loss: 0.419
[35,     1] loss: 0.395
[36,     1] loss: 0.373
[37,     1] loss: 0.346
[38,     1] loss: 0.324
[39,     1] loss: 0.302
[40,     1] loss: 0.305
[41,     1] loss: 0.306
[42,     1] loss: 0.433
[43,     1] loss: 0.383
[44,     1] loss: 0.353
[45,     1] loss: 0.343
[46,     1] loss: 0.320
[47,     1] loss: 0.305
[48,     1] loss: 0.300
[49,     1] loss: 0.363
[50,     1] loss: 0.432
[51,     1] loss: 0.344
[52,     1] loss: 0.343
[53,     1] loss: 0.327
[54,     1] loss: 0.316
[55,     1] loss: 0.296
[56,     1] loss: 0.278
[57,     1] loss: 0.259
[58,     1] loss: 0.239
[59,     1] loss: 0.231
[60,     1] loss: 0.230
[61,     1] loss: 0.271
[62,     1] loss: 0.281
[63,     1] loss: 0.243
[64,     1] loss: 0.236
[65,     1] loss: 0.241
[66,     1] loss: 0.233
[67,     1] loss: 0.212
[68,     1] loss: 0.203
[69,     1] loss: 0.195
[70,     1] loss: 0.189
[71,     1] loss: 0.189
[72,     1] loss: 0.197
[73,     1] loss: 0.194
[74,     1] loss: 0.230
[75,     1] loss: 0.392
[76,     1] loss: 0.554
[77,     1] loss: 0.472
[78,     1] loss: 0.479
[79,     1] loss: 0.497
[80,     1] loss: 0.506
[81,     1] loss: 0.509
[82,     1] loss: 0.505
[83,     1] loss: 0.501
[84,     1] loss: 0.497
[85,     1] loss: 0.492
[86,     1] loss: 0.487
[87,     1] loss: 0.477
[88,     1] loss: 0.463
[89,     1] loss: 0.451
[90,     1] loss: 0.436
[91,     1] loss: 0.456
[92,     1] loss: 0.420
[93,     1] loss: 0.415
[94,     1] loss: 0.442
[95,     1] loss: 0.405
Early stopping applied (best metric=0.38129091262817383)
Finished Training
Total time taken: 277.2027106285095
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.686
[3,     1] loss: 0.659
[4,     1] loss: 0.637
[5,     1] loss: 0.614
[6,     1] loss: 0.591
[7,     1] loss: 0.569
[8,     1] loss: 0.546
[9,     1] loss: 0.525
[10,     1] loss: 0.500
[11,     1] loss: 0.474
[12,     1] loss: 0.448
[13,     1] loss: 0.423
[14,     1] loss: 0.405
[15,     1] loss: 0.396
[16,     1] loss: 0.376
[17,     1] loss: 0.352
[18,     1] loss: 0.330
[19,     1] loss: 0.307
[20,     1] loss: 0.283
[21,     1] loss: 0.257
[22,     1] loss: 0.233
[23,     1] loss: 0.210
[24,     1] loss: 0.226
[25,     1] loss: 0.366
[26,     1] loss: 0.806
[27,     1] loss: 0.570
[28,     1] loss: 0.587
[29,     1] loss: 0.579
[30,     1] loss: 0.604
[31,     1] loss: 0.621
[32,     1] loss: 0.631
[33,     1] loss: 0.634
[34,     1] loss: 0.636
[35,     1] loss: 0.635
[36,     1] loss: 0.634
[37,     1] loss: 0.631
[38,     1] loss: 0.630
[39,     1] loss: 0.627
[40,     1] loss: 0.626
[41,     1] loss: 0.623
[42,     1] loss: 0.620
[43,     1] loss: 0.615
[44,     1] loss: 0.612
[45,     1] loss: 0.606
[46,     1] loss: 0.601
[47,     1] loss: 0.594
[48,     1] loss: 0.585
[49,     1] loss: 0.579
[50,     1] loss: 0.569
[51,     1] loss: 0.558
[52,     1] loss: 0.549
[53,     1] loss: 0.554
[54,     1] loss: 0.579
[55,     1] loss: 0.553
[56,     1] loss: 0.538
[57,     1] loss: 0.524
[58,     1] loss: 0.521
[59,     1] loss: 0.517
[60,     1] loss: 0.496
[61,     1] loss: 0.483
[62,     1] loss: 0.477
[63,     1] loss: 0.475
[64,     1] loss: 0.466
[65,     1] loss: 0.459
[66,     1] loss: 0.449
[67,     1] loss: 0.446
[68,     1] loss: 0.436
[69,     1] loss: 0.446
[70,     1] loss: 0.486
Early stopping applied (best metric=0.3957359194755554)
Finished Training
Total time taken: 206.16497778892517
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.693
[3,     1] loss: 0.675
[4,     1] loss: 0.660
[5,     1] loss: 0.643
[6,     1] loss: 0.627
[7,     1] loss: 0.608
[8,     1] loss: 0.590
[9,     1] loss: 0.570
[10,     1] loss: 0.548
[11,     1] loss: 0.529
[12,     1] loss: 0.502
[13,     1] loss: 0.481
[14,     1] loss: 0.487
[15,     1] loss: 0.452
[16,     1] loss: 0.438
[17,     1] loss: 0.420
[18,     1] loss: 0.397
[19,     1] loss: 0.379
[20,     1] loss: 0.373
[21,     1] loss: 0.363
[22,     1] loss: 0.425
[23,     1] loss: 0.506
[24,     1] loss: 0.421
[25,     1] loss: 0.414
[26,     1] loss: 0.418
[27,     1] loss: 0.410
[28,     1] loss: 0.383
[29,     1] loss: 0.367
[30,     1] loss: 0.335
[31,     1] loss: 0.309
[32,     1] loss: 0.289
[33,     1] loss: 0.279
[34,     1] loss: 0.338
[35,     1] loss: 0.658
[36,     1] loss: 0.461
[37,     1] loss: 0.479
[38,     1] loss: 0.489
[39,     1] loss: 0.500
[40,     1] loss: 0.501
[41,     1] loss: 0.497
[42,     1] loss: 0.488
[43,     1] loss: 0.480
[44,     1] loss: 0.469
[45,     1] loss: 0.459
[46,     1] loss: 0.449
[47,     1] loss: 0.447
[48,     1] loss: 0.487
[49,     1] loss: 0.469
[50,     1] loss: 0.465
[51,     1] loss: 0.442
[52,     1] loss: 0.431
[53,     1] loss: 0.415
[54,     1] loss: 0.408
[55,     1] loss: 0.390
[56,     1] loss: 0.406
[57,     1] loss: 0.408
[58,     1] loss: 0.433
[59,     1] loss: 0.406
[60,     1] loss: 0.400
[61,     1] loss: 0.383
[62,     1] loss: 0.366
[63,     1] loss: 0.350
[64,     1] loss: 0.343
[65,     1] loss: 0.332
[66,     1] loss: 0.343
[67,     1] loss: 0.384
[68,     1] loss: 0.483
[69,     1] loss: 0.423
[70,     1] loss: 0.442
[71,     1] loss: 0.393
[72,     1] loss: 0.388
[73,     1] loss: 0.373
Early stopping applied (best metric=0.38321128487586975)
Finished Training
Total time taken: 215.10419058799744
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.646
[4,     1] loss: 0.623
[5,     1] loss: 0.601
[6,     1] loss: 0.579
[7,     1] loss: 0.556
[8,     1] loss: 0.535
[9,     1] loss: 0.514
[10,     1] loss: 0.490
[11,     1] loss: 0.473
[12,     1] loss: 0.455
[13,     1] loss: 0.437
[14,     1] loss: 0.411
[15,     1] loss: 0.376
[16,     1] loss: 0.370
[17,     1] loss: 0.504
[18,     1] loss: 0.446
[19,     1] loss: 0.467
[20,     1] loss: 0.456
[21,     1] loss: 0.454
[22,     1] loss: 0.438
[23,     1] loss: 0.418
[24,     1] loss: 0.385
[25,     1] loss: 0.350
[26,     1] loss: 0.329
[27,     1] loss: 0.328
[28,     1] loss: 0.443
[29,     1] loss: 0.391
[30,     1] loss: 0.351
[31,     1] loss: 0.337
[32,     1] loss: 0.335
[33,     1] loss: 0.436
[34,     1] loss: 0.337
[35,     1] loss: 0.371
[36,     1] loss: 0.333
[37,     1] loss: 0.305
[38,     1] loss: 0.299
[39,     1] loss: 0.271
[40,     1] loss: 0.253
[41,     1] loss: 0.235
[42,     1] loss: 0.217
[43,     1] loss: 0.200
[44,     1] loss: 0.187
[45,     1] loss: 0.174
[46,     1] loss: 0.204
[47,     1] loss: 0.327
[48,     1] loss: 1.014
[49,     1] loss: 0.709
[50,     1] loss: 0.605
[51,     1] loss: 0.622
[52,     1] loss: 0.607
[53,     1] loss: 0.612
[54,     1] loss: 0.617
[55,     1] loss: 0.622
[56,     1] loss: 0.620
[57,     1] loss: 0.618
[58,     1] loss: 0.614
[59,     1] loss: 0.609
[60,     1] loss: 0.605
[61,     1] loss: 0.596
[62,     1] loss: 0.588
[63,     1] loss: 0.578
[64,     1] loss: 0.567
[65,     1] loss: 0.555
[66,     1] loss: 0.544
[67,     1] loss: 0.559
[68,     1] loss: 0.549
[69,     1] loss: 0.527
[70,     1] loss: 0.509
[71,     1] loss: 0.495
[72,     1] loss: 0.489
[73,     1] loss: 0.533
[74,     1] loss: 0.524
[75,     1] loss: 0.502
[76,     1] loss: 0.485
[77,     1] loss: 0.468
[78,     1] loss: 0.457
[79,     1] loss: 0.444
[80,     1] loss: 0.436
[81,     1] loss: 0.430
[82,     1] loss: 0.445
[83,     1] loss: 0.441
Early stopping applied (best metric=0.3797343969345093)
Finished Training
Total time taken: 245.31664657592773
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.667
[4,     1] loss: 0.649
[5,     1] loss: 0.630
[6,     1] loss: 0.610
[7,     1] loss: 0.587
[8,     1] loss: 0.561
[9,     1] loss: 0.535
[10,     1] loss: 0.511
[11,     1] loss: 0.484
[12,     1] loss: 0.462
[13,     1] loss: 0.476
[14,     1] loss: 0.485
[15,     1] loss: 0.434
[16,     1] loss: 0.450
[17,     1] loss: 0.422
[18,     1] loss: 0.419
[19,     1] loss: 0.397
[20,     1] loss: 0.387
[21,     1] loss: 0.377
[22,     1] loss: 0.366
[23,     1] loss: 0.359
[24,     1] loss: 0.351
[25,     1] loss: 0.342
[26,     1] loss: 0.336
[27,     1] loss: 0.326
[28,     1] loss: 0.317
[29,     1] loss: 0.309
[30,     1] loss: 0.298
[31,     1] loss: 0.288
[32,     1] loss: 0.276
[33,     1] loss: 0.264
[34,     1] loss: 0.254
[35,     1] loss: 0.389
[36,     1] loss: 0.796
[37,     1] loss: 0.695
[38,     1] loss: 0.663
[39,     1] loss: 0.655
[40,     1] loss: 0.655
[41,     1] loss: 0.658
[42,     1] loss: 0.661
[43,     1] loss: 0.663
[44,     1] loss: 0.665
[45,     1] loss: 0.667
[46,     1] loss: 0.669
[47,     1] loss: 0.670
[48,     1] loss: 0.672
[49,     1] loss: 0.673
[50,     1] loss: 0.674
[51,     1] loss: 0.675
[52,     1] loss: 0.677
[53,     1] loss: 0.678
[54,     1] loss: 0.679
[55,     1] loss: 0.680
[56,     1] loss: 0.681
[57,     1] loss: 0.682
[58,     1] loss: 0.684
[59,     1] loss: 0.685
[60,     1] loss: 0.685
[61,     1] loss: 0.686
[62,     1] loss: 0.687
[63,     1] loss: 0.688
[64,     1] loss: 0.689
[65,     1] loss: 0.690
[66,     1] loss: 0.690
[67,     1] loss: 0.691
[68,     1] loss: 0.691
[69,     1] loss: 0.692
[70,     1] loss: 0.692
[71,     1] loss: 0.692
[72,     1] loss: 0.692
[73,     1] loss: 0.693
[74,     1] loss: 0.693
[75,     1] loss: 0.693
[76,     1] loss: 0.693
[77,     1] loss: 0.693
[78,     1] loss: 0.693
[79,     1] loss: 0.693
[80,     1] loss: 0.693
[81,     1] loss: 0.693
[82,     1] loss: 0.693
[83,     1] loss: 0.693
Early stopping applied (best metric=0.42367205023765564)
Finished Training
Total time taken: 247.91353917121887
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.664
[4,     1] loss: 0.649
[5,     1] loss: 0.633
[6,     1] loss: 0.618
[7,     1] loss: 0.604
[8,     1] loss: 0.588
[9,     1] loss: 0.572
[10,     1] loss: 0.555
[11,     1] loss: 0.538
[12,     1] loss: 0.514
[13,     1] loss: 0.492
[14,     1] loss: 0.473
[15,     1] loss: 0.449
[16,     1] loss: 0.453
[17,     1] loss: 0.504
[18,     1] loss: 0.473
[19,     1] loss: 0.462
[20,     1] loss: 0.448
[21,     1] loss: 0.435
[22,     1] loss: 0.411
[23,     1] loss: 0.404
[24,     1] loss: 0.367
[25,     1] loss: 0.346
[26,     1] loss: 0.324
[27,     1] loss: 0.306
[28,     1] loss: 0.310
[29,     1] loss: 0.281
[30,     1] loss: 0.268
[31,     1] loss: 0.258
[32,     1] loss: 0.277
[33,     1] loss: 0.269
[34,     1] loss: 0.347
[35,     1] loss: 0.377
[36,     1] loss: 0.451
[37,     1] loss: 0.404
[38,     1] loss: 0.396
[39,     1] loss: 0.395
[40,     1] loss: 0.391
[41,     1] loss: 0.371
[42,     1] loss: 0.349
[43,     1] loss: 0.324
[44,     1] loss: 0.303
[45,     1] loss: 0.286
[46,     1] loss: 0.269
[47,     1] loss: 0.262
[48,     1] loss: 0.247
[49,     1] loss: 0.258
[50,     1] loss: 0.234
[51,     1] loss: 0.233
[52,     1] loss: 0.243
[53,     1] loss: 0.237
[54,     1] loss: 0.305
[55,     1] loss: 0.843
[56,     1] loss: 0.658
[57,     1] loss: 0.594
[58,     1] loss: 0.581
[59,     1] loss: 0.600
[60,     1] loss: 0.616
[61,     1] loss: 0.623
[62,     1] loss: 0.627
[63,     1] loss: 0.630
[64,     1] loss: 0.633
[65,     1] loss: 0.637
[66,     1] loss: 0.638
[67,     1] loss: 0.640
[68,     1] loss: 0.643
[69,     1] loss: 0.644
[70,     1] loss: 0.646
[71,     1] loss: 0.648
[72,     1] loss: 0.649
[73,     1] loss: 0.649
[74,     1] loss: 0.650
[75,     1] loss: 0.650
[76,     1] loss: 0.649
[77,     1] loss: 0.649
[78,     1] loss: 0.648
[79,     1] loss: 0.648
[80,     1] loss: 0.647
[81,     1] loss: 0.647
[82,     1] loss: 0.646
Early stopping applied (best metric=0.38879236578941345)
Finished Training
Total time taken: 244.62258625030518
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.691
[3,     1] loss: 0.672
[4,     1] loss: 0.656
[5,     1] loss: 0.643
[6,     1] loss: 0.628
[7,     1] loss: 0.616
[8,     1] loss: 0.607
[9,     1] loss: 0.593
[10,     1] loss: 0.581
[11,     1] loss: 0.565
[12,     1] loss: 0.551
[13,     1] loss: 0.535
[14,     1] loss: 0.520
[15,     1] loss: 0.501
[16,     1] loss: 0.490
[17,     1] loss: 0.501
[18,     1] loss: 0.532
[19,     1] loss: 0.477
[20,     1] loss: 0.500
[21,     1] loss: 0.480
[22,     1] loss: 0.472
[23,     1] loss: 0.452
[24,     1] loss: 0.433
[25,     1] loss: 0.419
[26,     1] loss: 0.407
[27,     1] loss: 0.400
[28,     1] loss: 0.401
[29,     1] loss: 0.561
[30,     1] loss: 0.486
[31,     1] loss: 0.518
[32,     1] loss: 0.478
[33,     1] loss: 0.476
[34,     1] loss: 0.467
[35,     1] loss: 0.456
[36,     1] loss: 0.447
[37,     1] loss: 0.430
[38,     1] loss: 0.416
[39,     1] loss: 0.402
[40,     1] loss: 0.389
[41,     1] loss: 0.378
[42,     1] loss: 0.374
[43,     1] loss: 0.377
[44,     1] loss: 0.417
[45,     1] loss: 0.610
[46,     1] loss: 0.554
[47,     1] loss: 0.537
[48,     1] loss: 0.536
[49,     1] loss: 0.535
[50,     1] loss: 0.519
[51,     1] loss: 0.509
[52,     1] loss: 0.492
[53,     1] loss: 0.470
[54,     1] loss: 0.455
[55,     1] loss: 0.434
[56,     1] loss: 0.410
[57,     1] loss: 0.393
[58,     1] loss: 0.372
[59,     1] loss: 0.350
[60,     1] loss: 0.340
[61,     1] loss: 0.318
[62,     1] loss: 0.332
[63,     1] loss: 0.347
[64,     1] loss: 0.632
[65,     1] loss: 0.613
[66,     1] loss: 0.591
[67,     1] loss: 0.567
[68,     1] loss: 0.557
[69,     1] loss: 0.524
[70,     1] loss: 0.517
[71,     1] loss: 0.502
[72,     1] loss: 0.478
[73,     1] loss: 0.462
[74,     1] loss: 0.449
[75,     1] loss: 0.431
[76,     1] loss: 0.411
[77,     1] loss: 0.391
[78,     1] loss: 0.434
[79,     1] loss: 0.408
[80,     1] loss: 0.573
[81,     1] loss: 0.525
[82,     1] loss: 0.480
[83,     1] loss: 0.495
[84,     1] loss: 0.463
[85,     1] loss: 0.461
[86,     1] loss: 0.443
[87,     1] loss: 0.428
[88,     1] loss: 0.409
[89,     1] loss: 0.394
[90,     1] loss: 0.375
[91,     1] loss: 0.363
[92,     1] loss: 0.354
[93,     1] loss: 0.359
[94,     1] loss: 0.353
[95,     1] loss: 0.344
[96,     1] loss: 0.333
[97,     1] loss: 0.320
[98,     1] loss: 0.307
[99,     1] loss: 0.292
[100,     1] loss: 0.283
[101,     1] loss: 0.276
[102,     1] loss: 0.295
[103,     1] loss: 0.547
[104,     1] loss: 0.452
[105,     1] loss: 0.432
[106,     1] loss: 0.401
[107,     1] loss: 0.418
[108,     1] loss: 0.386
[109,     1] loss: 0.381
[110,     1] loss: 0.376
[111,     1] loss: 0.357
[112,     1] loss: 0.344
[113,     1] loss: 0.335
[114,     1] loss: 0.329
[115,     1] loss: 0.319
[116,     1] loss: 0.311
[117,     1] loss: 0.303
[118,     1] loss: 0.295
[119,     1] loss: 0.286
[120,     1] loss: 0.278
[121,     1] loss: 0.280
[122,     1] loss: 0.305
[123,     1] loss: 0.310
[124,     1] loss: 0.435
[125,     1] loss: 0.575
[126,     1] loss: 0.474
[127,     1] loss: 0.444
[128,     1] loss: 0.434
[129,     1] loss: 0.435
[130,     1] loss: 0.427
[131,     1] loss: 0.416
[132,     1] loss: 0.401
[133,     1] loss: 0.396
[134,     1] loss: 0.384
[135,     1] loss: 0.379
[136,     1] loss: 0.365
[137,     1] loss: 0.357
[138,     1] loss: 0.352
[139,     1] loss: 0.336
[140,     1] loss: 0.351
[141,     1] loss: 0.377
[142,     1] loss: 0.391
[143,     1] loss: 0.390
[144,     1] loss: 0.469
[145,     1] loss: 0.463
[146,     1] loss: 0.431
[147,     1] loss: 0.420
[148,     1] loss: 0.411
[149,     1] loss: 0.401
[150,     1] loss: 0.398
[151,     1] loss: 0.397
[152,     1] loss: 0.390
[153,     1] loss: 0.387
[154,     1] loss: 0.383
[155,     1] loss: 0.380
[156,     1] loss: 0.371
[157,     1] loss: 0.366
[158,     1] loss: 0.359
[159,     1] loss: 0.359
[160,     1] loss: 0.365
[161,     1] loss: 0.375
[162,     1] loss: 0.399
[163,     1] loss: 0.583
[164,     1] loss: 0.547
[165,     1] loss: 0.519
[166,     1] loss: 0.490
[167,     1] loss: 0.476
[168,     1] loss: 0.457
[169,     1] loss: 0.449
Early stopping applied (best metric=0.41537290811538696)
Finished Training
Total time taken: 498.2855453491211
{'Hydroxylation-P Validation Accuracy': 0.7778859651794325, 'Hydroxylation-P Validation Sensitivity': 0.794, 'Hydroxylation-P Validation Specificity': 0.7743244052072422, 'Hydroxylation-P Validation Precision': 0.4517018298287394, 'Hydroxylation-P AUC ROC': 0.8477231805885105, 'Hydroxylation-P AUC PR': 0.5957725853951729, 'Hydroxylation-P MCC': 0.4751758250360618, 'Hydroxylation-P F1': 0.5668375820086103, 'Validation Loss (Hydroxylation-P)': 0.39023209929466246, 'Validation Loss (total)': 0.39023209929466246}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004316435146527725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2962707121,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.4183868007453633}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.645
[4,     1] loss: 0.606
[5,     1] loss: 0.565
[6,     1] loss: 0.527
[7,     1] loss: 0.487
[8,     1] loss: 0.457
[9,     1] loss: 0.421
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006502865680794888,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3176084396,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.29169022851557}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.645
[4,     1] loss: 0.597
[5,     1] loss: 0.538
[6,     1] loss: 0.479
[7,     1] loss: 0.412
[8,     1] loss: 0.342
[9,     1] loss: 0.270
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033320102222297292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 554146274,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.244896074481975}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.665
[4,     1] loss: 0.650
[5,     1] loss: 0.632
[6,     1] loss: 0.612
[7,     1] loss: 0.592
[8,     1] loss: 0.570
[9,     1] loss: 0.546
[10,     1] loss: 0.522
[11,     1] loss: 0.500
[12,     1] loss: 0.484
[13,     1] loss: 0.465
[14,     1] loss: 0.443
[15,     1] loss: 0.432
[16,     1] loss: 0.417
[17,     1] loss: 0.403
[18,     1] loss: 0.394
[19,     1] loss: 0.382
[20,     1] loss: 0.377
[21,     1] loss: 0.369
[22,     1] loss: 0.364
[23,     1] loss: 0.360
[24,     1] loss: 0.357
[25,     1] loss: 0.352
[26,     1] loss: 0.346
[27,     1] loss: 0.345
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005938748644193733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2150301907,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.4103707726923345}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.670
[3,     1] loss: 0.630
[4,     1] loss: 0.581
[5,     1] loss: 0.525
[6,     1] loss: 0.463
[7,     1] loss: 0.395
[8,     1] loss: 0.324
[9,     1] loss: 0.265
[10,     1] loss: 0.214
[11,     1] loss: 0.178
[12,     1] loss: 0.124
[13,     1] loss: 0.105
[14,     1] loss: 0.062
[15,     1] loss: 0.041
[16,     1] loss: 0.037
[17,     1] loss: 0.022
[18,     1] loss: 0.018
[19,     1] loss: 0.012
[20,     1] loss: 0.009
[21,     1] loss: 0.008
[22,     1] loss: 0.007
[23,     1] loss: 0.006
[24,     1] loss: 0.004
[25,     1] loss: 0.004
[26,     1] loss: 0.004
[27,     1] loss: 0.003
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003698365205259346,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 162732658,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.21456616828979}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002145618233360698,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3507837276,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.559889584861002}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.674
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005544635158265688,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2626734190,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.69933858404305}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.664
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.625
[7,     1] loss: 0.611
[8,     1] loss: 0.595
[9,     1] loss: 0.578
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050630736667748285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2782925575,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.998080937249078}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.679
[3,     1] loss: 0.646
[4,     1] loss: 0.609
[5,     1] loss: 0.567
[6,     1] loss: 0.527
[7,     1] loss: 0.487
[8,     1] loss: 0.440
[9,     1] loss: 0.383
[10,     1] loss: 0.322
[11,     1] loss: 0.270
[12,     1] loss: 0.219
[13,     1] loss: 0.179
[14,     1] loss: 0.146
[15,     1] loss: 0.123
[16,     1] loss: 0.105
[17,     1] loss: 0.098
[18,     1] loss: 0.098
[19,     1] loss: 0.092
[20,     1] loss: 0.076
[21,     1] loss: 0.061
[22,     1] loss: 0.053
[23,     1] loss: 0.038
[24,     1] loss: 0.037
[25,     1] loss: 0.029
[26,     1] loss: 0.025
[27,     1] loss: 0.023
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009537100245003831,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1112729626,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.118178925655542}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.663
[4,     1] loss: 0.646
[5,     1] loss: 0.629
[6,     1] loss: 0.616
[7,     1] loss: 0.602
[8,     1] loss: 0.591
[9,     1] loss: 0.578
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0045071612728796616,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2490354156,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.167523325255587}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.669
[4,     1] loss: 0.659
[5,     1] loss: 0.646
[6,     1] loss: 0.635
[7,     1] loss: 0.622
[8,     1] loss: 0.610
[9,     1] loss: 0.597
[10,     1] loss: 0.581
[11,     1] loss: 0.568
[12,     1] loss: 0.552
[13,     1] loss: 0.537
[14,     1] loss: 0.523
[15,     1] loss: 0.510
[16,     1] loss: 0.497
[17,     1] loss: 0.486
[18,     1] loss: 0.474
[19,     1] loss: 0.464
[20,     1] loss: 0.452
[21,     1] loss: 0.442
[22,     1] loss: 0.434
[23,     1] loss: 0.424
[24,     1] loss: 0.414
[25,     1] loss: 0.408
[26,     1] loss: 0.399
[27,     1] loss: 0.391
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008108846186636313,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1056497358,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.338704291874409}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004673624529057046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1621556250,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.02409532454552}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.690
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005490794493521451,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4283648169,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.114581441155932}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.656
[4,     1] loss: 0.628
[5,     1] loss: 0.597
[6,     1] loss: 0.559
[7,     1] loss: 0.514
[8,     1] loss: 0.464
[9,     1] loss: 0.400
[10,     1] loss: 0.344
[11,     1] loss: 0.286
[12,     1] loss: 0.239
[13,     1] loss: 0.206
[14,     1] loss: 0.202
[15,     1] loss: 0.158
[16,     1] loss: 0.137
[17,     1] loss: 0.121
[18,     1] loss: 0.097
[19,     1] loss: 0.082
[20,     1] loss: 0.079
[21,     1] loss: 0.061
[22,     1] loss: 0.047
[23,     1] loss: 0.047
[24,     1] loss: 0.033
[25,     1] loss: 0.040
[26,     1] loss: 0.029
[27,     1] loss: 0.031
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005371544347109041,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4113138627,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.667701229779185}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.663
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006791236527719059,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3300152830,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.335392113709627}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.685
[3,     1] loss: 0.644
[4,     1] loss: 0.604
[5,     1] loss: 0.560
[6,     1] loss: 0.514
[7,     1] loss: 0.458
[8,     1] loss: 0.404
[9,     1] loss: 0.344
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006515188960028519,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1245669314,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.981176217513638}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.637
[4,     1] loss: 0.592
[5,     1] loss: 0.546
[6,     1] loss: 0.488
[7,     1] loss: 0.427
[8,     1] loss: 0.361
[9,     1] loss: 0.298
[10,     1] loss: 0.232
[11,     1] loss: 0.181
[12,     1] loss: 0.213
[13,     1] loss: 0.148
[14,     1] loss: 0.221
[15,     1] loss: 0.105
[16,     1] loss: 0.196
[17,     1] loss: 0.099
[18,     1] loss: 0.134
[19,     1] loss: 0.101
[20,     1] loss: 0.078
[21,     1] loss: 0.079
[22,     1] loss: 0.070
[23,     1] loss: 0.060
[24,     1] loss: 0.050
[25,     1] loss: 0.045
[26,     1] loss: 0.041
[27,     1] loss: 0.036
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009518912871551105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 600541673,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.741724034673222}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.669
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007809644955962262,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1293060592,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5012744995592211}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.671
[3,     1] loss: 0.611
[4,     1] loss: 0.570
[5,     1] loss: 0.524
[6,     1] loss: 0.501
[7,     1] loss: 0.451
[8,     1] loss: 0.409
[9,     1] loss: 0.368
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005306056863184055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3558865064,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.833058262388185}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.670
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004726015268007751,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1449905311,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.74866505357504}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.675
[3,     1] loss: 0.641
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005657497423040732,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3234399102,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.047644628416359}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.675
[3,     1] loss: 0.632
[4,     1] loss: 0.586
[5,     1] loss: 0.536
[6,     1] loss: 0.491
[7,     1] loss: 0.455
[8,     1] loss: 0.418
[9,     1] loss: 0.392
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006916535896583559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1350425946,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.41444601515114}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.688
[3,     1] loss: 0.665
[4,     1] loss: 0.649
[5,     1] loss: 0.632
[6,     1] loss: 0.613
[7,     1] loss: 0.592
[8,     1] loss: 0.564
[9,     1] loss: 0.539
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006073911028855987,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1178628940,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.166483120249837}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.630
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00399190599903561,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3074025049,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.302174893342858}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.690
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004166917032754512,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 771571732,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.707488453760392}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007294666944568674,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2439523303,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.230726746707843}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.674
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047691800692332646,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2706964374,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.416046134081924}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00924865973622633,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2976669715,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.90105232884347}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.651
[4,     1] loss: 0.621
[5,     1] loss: 0.588
[6,     1] loss: 0.562
[7,     1] loss: 0.531
[8,     1] loss: 0.505
[9,     1] loss: 0.475
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018436419363125583,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2794083034,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.54474014803961}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.695
[3,     1] loss: 0.681
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031493447125921586,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2744172334,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.8568681162970002}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.671
[4,     1] loss: 0.644
[5,     1] loss: 0.616
[6,     1] loss: 0.580
[7,     1] loss: 0.539
[8,     1] loss: 0.491
[9,     1] loss: 0.445
[10,     1] loss: 0.394
[11,     1] loss: 0.346
[12,     1] loss: 0.295
[13,     1] loss: 0.252
[14,     1] loss: 0.196
[15,     1] loss: 0.160
[16,     1] loss: 0.114
[17,     1] loss: 0.084
[18,     1] loss: 0.061
[19,     1] loss: 0.043
[20,     1] loss: 0.028
[21,     1] loss: 0.019
[22,     1] loss: 0.013
[23,     1] loss: 0.010
[24,     1] loss: 0.007
[25,     1] loss: 0.005
[26,     1] loss: 0.004
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.000
[38,     1] loss: 0.001
[39,     1] loss: 0.000
[40,     1] loss: 0.001
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.000
[51,     1] loss: 0.000
[52,     1] loss: 0.000
[53,     1] loss: 0.001
[54,     1] loss: 0.000
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.41485098004341125)
Finished Training
Total time taken: 174.40428376197815
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.673
[3,     1] loss: 0.637
[4,     1] loss: 0.605
[5,     1] loss: 0.561
[6,     1] loss: 0.519
[7,     1] loss: 0.474
[8,     1] loss: 0.429
[9,     1] loss: 0.379
[10,     1] loss: 0.329
[11,     1] loss: 0.282
[12,     1] loss: 0.251
[13,     1] loss: 0.209
[14,     1] loss: 0.180
[15,     1] loss: 0.148
[16,     1] loss: 0.117
[17,     1] loss: 0.093
[18,     1] loss: 0.083
[19,     1] loss: 0.069
[20,     1] loss: 0.053
[21,     1] loss: 0.042
[22,     1] loss: 0.028
[23,     1] loss: 0.023
[24,     1] loss: 0.019
[25,     1] loss: 0.014
[26,     1] loss: 0.013
[27,     1] loss: 0.010
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.006
[31,     1] loss: 0.005
[32,     1] loss: 0.005
[33,     1] loss: 0.004
[34,     1] loss: 0.004
[35,     1] loss: 0.003
[36,     1] loss: 0.003
[37,     1] loss: 0.003
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.001
[55,     1] loss: 0.002
[56,     1] loss: 0.001
[57,     1] loss: 0.001
Early stopping applied (best metric=0.43452146649360657)
Finished Training
Total time taken: 168.49535655975342
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.657
[4,     1] loss: 0.626
[5,     1] loss: 0.590
[6,     1] loss: 0.552
[7,     1] loss: 0.508
[8,     1] loss: 0.465
[9,     1] loss: 0.421
[10,     1] loss: 0.375
[11,     1] loss: 0.341
[12,     1] loss: 0.305
[13,     1] loss: 0.273
[14,     1] loss: 0.245
[15,     1] loss: 0.219
[16,     1] loss: 0.192
[17,     1] loss: 0.171
[18,     1] loss: 0.143
[19,     1] loss: 0.114
[20,     1] loss: 0.094
[21,     1] loss: 0.071
[22,     1] loss: 0.056
[23,     1] loss: 0.042
[24,     1] loss: 0.034
[25,     1] loss: 0.025
[26,     1] loss: 0.020
[27,     1] loss: 0.016
[28,     1] loss: 0.012
[29,     1] loss: 0.010
[30,     1] loss: 0.008
[31,     1] loss: 0.007
[32,     1] loss: 0.005
[33,     1] loss: 0.005
[34,     1] loss: 0.004
[35,     1] loss: 0.004
[36,     1] loss: 0.003
[37,     1] loss: 0.003
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.001
[44,     1] loss: 0.002
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.39351925253868103)
Finished Training
Total time taken: 179.6597921848297
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.649
[4,     1] loss: 0.618
[5,     1] loss: 0.585
[6,     1] loss: 0.542
[7,     1] loss: 0.497
[8,     1] loss: 0.445
[9,     1] loss: 0.392
[10,     1] loss: 0.337
[11,     1] loss: 0.282
[12,     1] loss: 0.227
[13,     1] loss: 0.186
[14,     1] loss: 0.147
[15,     1] loss: 0.122
[16,     1] loss: 0.096
[17,     1] loss: 0.067
[18,     1] loss: 0.057
[19,     1] loss: 0.040
[20,     1] loss: 0.029
[21,     1] loss: 0.023
[22,     1] loss: 0.018
[23,     1] loss: 0.013
[24,     1] loss: 0.011
[25,     1] loss: 0.008
[26,     1] loss: 0.006
[27,     1] loss: 0.005
[28,     1] loss: 0.004
[29,     1] loss: 0.004
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.42886674404144287)
Finished Training
Total time taken: 178.06972789764404
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.644
[4,     1] loss: 0.613
[5,     1] loss: 0.576
[6,     1] loss: 0.539
[7,     1] loss: 0.491
[8,     1] loss: 0.441
[9,     1] loss: 0.398
[10,     1] loss: 0.339
[11,     1] loss: 0.293
[12,     1] loss: 0.233
[13,     1] loss: 0.187
[14,     1] loss: 0.142
[15,     1] loss: 0.099
[16,     1] loss: 0.075
[17,     1] loss: 0.053
[18,     1] loss: 0.045
[19,     1] loss: 0.032
[20,     1] loss: 0.025
[21,     1] loss: 0.019
[22,     1] loss: 0.015
[23,     1] loss: 0.012
[24,     1] loss: 0.009
[25,     1] loss: 0.008
[26,     1] loss: 0.006
[27,     1] loss: 0.005
[28,     1] loss: 0.004
[29,     1] loss: 0.004
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.38096240162849426)
Finished Training
Total time taken: 182.20898509025574
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.661
[4,     1] loss: 0.641
[5,     1] loss: 0.617
[6,     1] loss: 0.594
[7,     1] loss: 0.565
[8,     1] loss: 0.537
[9,     1] loss: 0.503
[10,     1] loss: 0.470
[11,     1] loss: 0.435
[12,     1] loss: 0.404
[13,     1] loss: 0.365
[14,     1] loss: 0.322
[15,     1] loss: 0.285
[16,     1] loss: 0.244
[17,     1] loss: 0.204
[18,     1] loss: 0.166
[19,     1] loss: 0.139
[20,     1] loss: 0.111
[21,     1] loss: 0.095
[22,     1] loss: 0.074
[23,     1] loss: 0.061
[24,     1] loss: 0.046
[25,     1] loss: 0.032
[26,     1] loss: 0.022
[27,     1] loss: 0.018
[28,     1] loss: 0.014
[29,     1] loss: 0.009
[30,     1] loss: 0.007
[31,     1] loss: 0.005
[32,     1] loss: 0.003
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.000
[51,     1] loss: 0.000
[52,     1] loss: 0.000
[53,     1] loss: 0.000
[54,     1] loss: 0.000
[55,     1] loss: 0.000
[56,     1] loss: 0.000
[57,     1] loss: 0.000
[58,     1] loss: 0.000
[59,     1] loss: 0.000
Early stopping applied (best metric=0.4184496998786926)
Finished Training
Total time taken: 173.73199033737183
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.675
[3,     1] loss: 0.644
[4,     1] loss: 0.614
[5,     1] loss: 0.577
[6,     1] loss: 0.532
[7,     1] loss: 0.482
[8,     1] loss: 0.437
[9,     1] loss: 0.383
[10,     1] loss: 0.321
[11,     1] loss: 0.265
[12,     1] loss: 0.217
[13,     1] loss: 0.171
[14,     1] loss: 0.130
[15,     1] loss: 0.095
[16,     1] loss: 0.069
[17,     1] loss: 0.045
[18,     1] loss: 0.033
[19,     1] loss: 0.023
[20,     1] loss: 0.017
[21,     1] loss: 0.013
[22,     1] loss: 0.010
[23,     1] loss: 0.008
[24,     1] loss: 0.006
[25,     1] loss: 0.005
[26,     1] loss: 0.004
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.002
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.000
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.3681691884994507)
Finished Training
Total time taken: 176.92299699783325
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.675
[3,     1] loss: 0.634
[4,     1] loss: 0.596
[5,     1] loss: 0.553
[6,     1] loss: 0.502
[7,     1] loss: 0.456
[8,     1] loss: 0.415
[9,     1] loss: 0.374
[10,     1] loss: 0.338
[11,     1] loss: 0.312
[12,     1] loss: 0.290
[13,     1] loss: 0.267
[14,     1] loss: 0.249
[15,     1] loss: 0.228
[16,     1] loss: 0.206
[17,     1] loss: 0.185
[18,     1] loss: 0.160
[19,     1] loss: 0.136
[20,     1] loss: 0.116
[21,     1] loss: 0.098
[22,     1] loss: 0.078
[23,     1] loss: 0.067
[24,     1] loss: 0.050
[25,     1] loss: 0.041
[26,     1] loss: 0.036
[27,     1] loss: 0.029
[28,     1] loss: 0.025
[29,     1] loss: 0.022
[30,     1] loss: 0.020
[31,     1] loss: 0.019
[32,     1] loss: 0.016
[33,     1] loss: 0.012
[34,     1] loss: 0.011
[35,     1] loss: 0.009
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.007
[39,     1] loss: 0.006
[40,     1] loss: 0.005
[41,     1] loss: 0.005
[42,     1] loss: 0.004
[43,     1] loss: 0.004
[44,     1] loss: 0.004
[45,     1] loss: 0.003
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.003
[49,     1] loss: 0.002
[50,     1] loss: 0.003
[51,     1] loss: 0.003
[52,     1] loss: 0.003
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.002
[59,     1] loss: 0.002
Early stopping applied (best metric=0.4169824719429016)
Finished Training
Total time taken: 174.40707063674927
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.690
[3,     1] loss: 0.665
[4,     1] loss: 0.633
[5,     1] loss: 0.596
[6,     1] loss: 0.553
[7,     1] loss: 0.501
[8,     1] loss: 0.445
[9,     1] loss: 0.405
[10,     1] loss: 0.336
[11,     1] loss: 0.288
[12,     1] loss: 0.230
[13,     1] loss: 0.189
[14,     1] loss: 0.147
[15,     1] loss: 0.108
[16,     1] loss: 0.082
[17,     1] loss: 0.062
[18,     1] loss: 0.045
[19,     1] loss: 0.033
[20,     1] loss: 0.024
[21,     1] loss: 0.019
[22,     1] loss: 0.015
[23,     1] loss: 0.011
[24,     1] loss: 0.009
[25,     1] loss: 0.007
[26,     1] loss: 0.006
[27,     1] loss: 0.005
[28,     1] loss: 0.004
[29,     1] loss: 0.003
[30,     1] loss: 0.003
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.36132174730300903)
Finished Training
Total time taken: 179.98299098014832
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.691
[3,     1] loss: 0.669
[4,     1] loss: 0.646
[5,     1] loss: 0.619
[6,     1] loss: 0.584
[7,     1] loss: 0.545
[8,     1] loss: 0.504
[9,     1] loss: 0.456
[10,     1] loss: 0.402
[11,     1] loss: 0.349
[12,     1] loss: 0.293
[13,     1] loss: 0.241
[14,     1] loss: 0.186
[15,     1] loss: 0.148
[16,     1] loss: 0.111
[17,     1] loss: 0.082
[18,     1] loss: 0.059
[19,     1] loss: 0.042
[20,     1] loss: 0.030
[21,     1] loss: 0.021
[22,     1] loss: 0.016
[23,     1] loss: 0.012
[24,     1] loss: 0.009
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.004
[28,     1] loss: 0.004
[29,     1] loss: 0.003
[30,     1] loss: 0.002
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.000
[47,     1] loss: 0.001
[48,     1] loss: 0.000
[49,     1] loss: 0.001
[50,     1] loss: 0.000
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
[62,     1] loss: 0.001
[63,     1] loss: 0.001
Early stopping applied (best metric=0.34878870844841003)
Finished Training
Total time taken: 186.14687728881836
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.675
[3,     1] loss: 0.631
[4,     1] loss: 0.590
[5,     1] loss: 0.543
[6,     1] loss: 0.497
[7,     1] loss: 0.451
[8,     1] loss: 0.402
[9,     1] loss: 0.343
[10,     1] loss: 0.297
[11,     1] loss: 0.254
[12,     1] loss: 0.214
[13,     1] loss: 0.179
[14,     1] loss: 0.147
[15,     1] loss: 0.121
[16,     1] loss: 0.096
[17,     1] loss: 0.080
[18,     1] loss: 0.068
[19,     1] loss: 0.071
[20,     1] loss: 0.041
[21,     1] loss: 0.040
[22,     1] loss: 0.028
[23,     1] loss: 0.022
[24,     1] loss: 0.014
[25,     1] loss: 0.011
[26,     1] loss: 0.009
[27,     1] loss: 0.007
[28,     1] loss: 0.005
[29,     1] loss: 0.005
[30,     1] loss: 0.003
[31,     1] loss: 0.003
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.40129950642585754)
Finished Training
Total time taken: 177.71078085899353
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.678
[4,     1] loss: 0.658
[5,     1] loss: 0.634
[6,     1] loss: 0.605
[7,     1] loss: 0.570
[8,     1] loss: 0.531
[9,     1] loss: 0.486
[10,     1] loss: 0.439
[11,     1] loss: 0.386
[12,     1] loss: 0.328
[13,     1] loss: 0.268
[14,     1] loss: 0.213
[15,     1] loss: 0.170
[16,     1] loss: 0.134
[17,     1] loss: 0.102
[18,     1] loss: 0.083
[19,     1] loss: 0.068
[20,     1] loss: 0.057
[21,     1] loss: 0.048
[22,     1] loss: 0.038
[23,     1] loss: 0.035
[24,     1] loss: 0.025
[25,     1] loss: 0.047
[26,     1] loss: 0.019
[27,     1] loss: 0.022
[28,     1] loss: 0.030
[29,     1] loss: 0.020
[30,     1] loss: 0.014
[31,     1] loss: 0.014
[32,     1] loss: 0.014
[33,     1] loss: 0.014
[34,     1] loss: 0.013
[35,     1] loss: 0.012
[36,     1] loss: 0.012
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.008
[40,     1] loss: 0.007
[41,     1] loss: 0.007
[42,     1] loss: 0.006
[43,     1] loss: 0.004
[44,     1] loss: 0.004
[45,     1] loss: 0.004
[46,     1] loss: 0.003
[47,     1] loss: 0.003
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.002
[52,     1] loss: 0.002
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
[62,     1] loss: 0.002
Early stopping applied (best metric=0.41022786498069763)
Finished Training
Total time taken: 184.00805377960205
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.666
[4,     1] loss: 0.642
[5,     1] loss: 0.609
[6,     1] loss: 0.568
[7,     1] loss: 0.529
[8,     1] loss: 0.478
[9,     1] loss: 0.425
[10,     1] loss: 0.367
[11,     1] loss: 0.308
[12,     1] loss: 0.256
[13,     1] loss: 0.209
[14,     1] loss: 0.171
[15,     1] loss: 0.139
[16,     1] loss: 0.110
[17,     1] loss: 0.087
[18,     1] loss: 0.071
[19,     1] loss: 0.061
[20,     1] loss: 0.047
[21,     1] loss: 0.035
[22,     1] loss: 0.030
[23,     1] loss: 0.022
[24,     1] loss: 0.019
[25,     1] loss: 0.015
[26,     1] loss: 0.011
[27,     1] loss: 0.009
[28,     1] loss: 0.008
[29,     1] loss: 0.006
[30,     1] loss: 0.006
[31,     1] loss: 0.005
[32,     1] loss: 0.004
[33,     1] loss: 0.003
[34,     1] loss: 0.003
[35,     1] loss: 0.003
[36,     1] loss: 0.003
[37,     1] loss: 0.002
[38,     1] loss: 0.002
[39,     1] loss: 0.002
[40,     1] loss: 0.002
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.002
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.3846794366836548)
Finished Training
Total time taken: 181.25490927696228
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.668
[4,     1] loss: 0.648
[5,     1] loss: 0.627
[6,     1] loss: 0.600
[7,     1] loss: 0.566
[8,     1] loss: 0.533
[9,     1] loss: 0.496
[10,     1] loss: 0.462
[11,     1] loss: 0.427
[12,     1] loss: 0.392
[13,     1] loss: 0.362
[14,     1] loss: 0.336
[15,     1] loss: 0.296
[16,     1] loss: 0.268
[17,     1] loss: 0.243
[18,     1] loss: 0.216
[19,     1] loss: 0.196
[20,     1] loss: 0.172
[21,     1] loss: 0.153
[22,     1] loss: 0.135
[23,     1] loss: 0.112
[24,     1] loss: 0.095
[25,     1] loss: 0.073
[26,     1] loss: 0.061
[27,     1] loss: 0.052
[28,     1] loss: 0.043
[29,     1] loss: 0.037
[30,     1] loss: 0.030
[31,     1] loss: 0.020
[32,     1] loss: 0.014
[33,     1] loss: 0.011
[34,     1] loss: 0.009
[35,     1] loss: 0.007
[36,     1] loss: 0.006
[37,     1] loss: 0.005
[38,     1] loss: 0.004
[39,     1] loss: 0.003
[40,     1] loss: 0.003
[41,     1] loss: 0.003
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.001
[46,     1] loss: 0.002
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.4237808883190155)
Finished Training
Total time taken: 178.44742941856384
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.663
[3,     1] loss: 0.626
[4,     1] loss: 0.587
[5,     1] loss: 0.549
[6,     1] loss: 0.513
[7,     1] loss: 0.473
[8,     1] loss: 0.443
[9,     1] loss: 0.412
[10,     1] loss: 0.388
[11,     1] loss: 0.347
[12,     1] loss: 0.318
[13,     1] loss: 0.298
[14,     1] loss: 0.274
[15,     1] loss: 0.255
[16,     1] loss: 0.230
[17,     1] loss: 0.210
[18,     1] loss: 0.188
[19,     1] loss: 0.170
[20,     1] loss: 0.150
[21,     1] loss: 0.133
[22,     1] loss: 0.118
[23,     1] loss: 0.102
[24,     1] loss: 0.085
[25,     1] loss: 0.079
[26,     1] loss: 0.073
[27,     1] loss: 0.065
[28,     1] loss: 0.062
[29,     1] loss: 0.052
[30,     1] loss: 0.038
[31,     1] loss: 0.027
[32,     1] loss: 0.018
[33,     1] loss: 0.014
[34,     1] loss: 0.018
[35,     1] loss: 0.009
[36,     1] loss: 0.007
[37,     1] loss: 0.006
[38,     1] loss: 0.005
[39,     1] loss: 0.004
[40,     1] loss: 0.004
[41,     1] loss: 0.003
[42,     1] loss: 0.003
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
Early stopping applied (best metric=0.45289546251296997)
Finished Training
Total time taken: 164.20350456237793
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.662
[4,     1] loss: 0.638
[5,     1] loss: 0.607
[6,     1] loss: 0.574
[7,     1] loss: 0.535
[8,     1] loss: 0.499
[9,     1] loss: 0.453
[10,     1] loss: 0.408
[11,     1] loss: 0.365
[12,     1] loss: 0.317
[13,     1] loss: 0.270
[14,     1] loss: 0.225
[15,     1] loss: 0.167
[16,     1] loss: 0.136
[17,     1] loss: 0.108
[18,     1] loss: 0.096
[19,     1] loss: 0.062
[20,     1] loss: 0.056
[21,     1] loss: 0.037
[22,     1] loss: 0.029
[23,     1] loss: 0.021
[24,     1] loss: 0.015
[25,     1] loss: 0.013
[26,     1] loss: 0.010
[27,     1] loss: 0.008
[28,     1] loss: 0.006
[29,     1] loss: 0.005
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.002
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.42783495783805847)
Finished Training
Total time taken: 178.81182551383972
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.698
[3,     1] loss: 0.679
[4,     1] loss: 0.658
[5,     1] loss: 0.630
[6,     1] loss: 0.600
[7,     1] loss: 0.563
[8,     1] loss: 0.521
[9,     1] loss: 0.477
[10,     1] loss: 0.425
[11,     1] loss: 0.371
[12,     1] loss: 0.313
[13,     1] loss: 0.251
[14,     1] loss: 0.200
[15,     1] loss: 0.166
[16,     1] loss: 0.130
[17,     1] loss: 0.105
[18,     1] loss: 0.070
[19,     1] loss: 0.054
[20,     1] loss: 0.041
[21,     1] loss: 0.034
[22,     1] loss: 0.021
[23,     1] loss: 0.016
[24,     1] loss: 0.014
[25,     1] loss: 0.010
[26,     1] loss: 0.008
[27,     1] loss: 0.007
[28,     1] loss: 0.006
[29,     1] loss: 0.004
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.4046093225479126)
Finished Training
Total time taken: 182.15126776695251
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.689
[3,     1] loss: 0.662
[4,     1] loss: 0.635
[5,     1] loss: 0.606
[6,     1] loss: 0.572
[7,     1] loss: 0.531
[8,     1] loss: 0.486
[9,     1] loss: 0.435
[10,     1] loss: 0.373
[11,     1] loss: 0.324
[12,     1] loss: 0.272
[13,     1] loss: 0.223
[14,     1] loss: 0.181
[15,     1] loss: 0.142
[16,     1] loss: 0.102
[17,     1] loss: 0.073
[18,     1] loss: 0.055
[19,     1] loss: 0.036
[20,     1] loss: 0.028
[21,     1] loss: 0.021
[22,     1] loss: 0.016
[23,     1] loss: 0.013
[24,     1] loss: 0.010
[25,     1] loss: 0.008
[26,     1] loss: 0.006
[27,     1] loss: 0.005
[28,     1] loss: 0.004
[29,     1] loss: 0.003
[30,     1] loss: 0.003
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.000
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.37253835797309875)
Finished Training
Total time taken: 179.65778923034668
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.646
[5,     1] loss: 0.616
[6,     1] loss: 0.579
[7,     1] loss: 0.534
[8,     1] loss: 0.485
[9,     1] loss: 0.430
[10,     1] loss: 0.371
[11,     1] loss: 0.315
[12,     1] loss: 0.260
[13,     1] loss: 0.216
[14,     1] loss: 0.170
[15,     1] loss: 0.139
[16,     1] loss: 0.099
[17,     1] loss: 0.080
[18,     1] loss: 0.059
[19,     1] loss: 0.044
[20,     1] loss: 0.032
[21,     1] loss: 0.021
[22,     1] loss: 0.015
[23,     1] loss: 0.012
[24,     1] loss: 0.009
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.005
[28,     1] loss: 0.003
[29,     1] loss: 0.003
[30,     1] loss: 0.002
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.000
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.4067361056804657)
Finished Training
Total time taken: 182.50016617774963
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.662
[4,     1] loss: 0.640
[5,     1] loss: 0.615
[6,     1] loss: 0.585
[7,     1] loss: 0.548
[8,     1] loss: 0.510
[9,     1] loss: 0.467
[10,     1] loss: 0.426
[11,     1] loss: 0.377
[12,     1] loss: 0.326
[13,     1] loss: 0.279
[14,     1] loss: 0.235
[15,     1] loss: 0.190
[16,     1] loss: 0.153
[17,     1] loss: 0.118
[18,     1] loss: 0.089
[19,     1] loss: 0.063
[20,     1] loss: 0.043
[21,     1] loss: 0.032
[22,     1] loss: 0.024
[23,     1] loss: 0.019
[24,     1] loss: 0.015
[25,     1] loss: 0.012
[26,     1] loss: 0.009
[27,     1] loss: 0.007
[28,     1] loss: 0.006
[29,     1] loss: 0.005
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.002
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
Early stopping applied (best metric=0.37792643904685974)
Finished Training
Total time taken: 182.99286127090454
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.648
[4,     1] loss: 0.616
[5,     1] loss: 0.580
[6,     1] loss: 0.539
[7,     1] loss: 0.500
[8,     1] loss: 0.468
[9,     1] loss: 0.436
[10,     1] loss: 0.416
[11,     1] loss: 0.404
[12,     1] loss: 0.390
[13,     1] loss: 0.378
[14,     1] loss: 0.366
[15,     1] loss: 0.357
[16,     1] loss: 0.352
[17,     1] loss: 0.346
[18,     1] loss: 0.338
[19,     1] loss: 0.326
[20,     1] loss: 0.317
[21,     1] loss: 0.315
[22,     1] loss: 0.293
[23,     1] loss: 0.290
[24,     1] loss: 0.271
[25,     1] loss: 0.253
[26,     1] loss: 0.239
[27,     1] loss: 0.220
[28,     1] loss: 0.200
[29,     1] loss: 0.181
[30,     1] loss: 0.161
[31,     1] loss: 0.144
[32,     1] loss: 0.130
[33,     1] loss: 0.115
[34,     1] loss: 0.099
[35,     1] loss: 0.092
[36,     1] loss: 0.081
[37,     1] loss: 0.075
[38,     1] loss: 0.068
[39,     1] loss: 0.061
[40,     1] loss: 0.057
[41,     1] loss: 0.053
[42,     1] loss: 0.048
[43,     1] loss: 0.041
[44,     1] loss: 0.034
[45,     1] loss: 0.030
[46,     1] loss: 0.027
[47,     1] loss: 0.024
[48,     1] loss: 0.023
[49,     1] loss: 0.021
[50,     1] loss: 0.019
[51,     1] loss: 0.017
[52,     1] loss: 0.017
[53,     1] loss: 0.015
[54,     1] loss: 0.014
[55,     1] loss: 0.013
[56,     1] loss: 0.013
Early stopping applied (best metric=0.5083820223808289)
Finished Training
Total time taken: 168.15035343170166
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.658
[4,     1] loss: 0.633
[5,     1] loss: 0.603
[6,     1] loss: 0.568
[7,     1] loss: 0.525
[8,     1] loss: 0.480
[9,     1] loss: 0.428
[10,     1] loss: 0.375
[11,     1] loss: 0.324
[12,     1] loss: 0.278
[13,     1] loss: 0.233
[14,     1] loss: 0.198
[15,     1] loss: 0.168
[16,     1] loss: 0.130
[17,     1] loss: 0.100
[18,     1] loss: 0.075
[19,     1] loss: 0.062
[20,     1] loss: 0.050
[21,     1] loss: 0.033
[22,     1] loss: 0.029
[23,     1] loss: 0.019
[24,     1] loss: 0.023
[25,     1] loss: 0.013
[26,     1] loss: 0.013
[27,     1] loss: 0.009
[28,     1] loss: 0.008
[29,     1] loss: 0.007
[30,     1] loss: 0.007
[31,     1] loss: 0.006
[32,     1] loss: 0.005
[33,     1] loss: 0.005
[34,     1] loss: 0.004
[35,     1] loss: 0.004
[36,     1] loss: 0.004
[37,     1] loss: 0.003
[38,     1] loss: 0.003
[39,     1] loss: 0.003
[40,     1] loss: 0.003
[41,     1] loss: 0.002
[42,     1] loss: 0.002
[43,     1] loss: 0.002
[44,     1] loss: 0.002
[45,     1] loss: 0.002
[46,     1] loss: 0.002
[47,     1] loss: 0.002
[48,     1] loss: 0.002
[49,     1] loss: 0.002
[50,     1] loss: 0.002
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.002
[54,     1] loss: 0.002
[55,     1] loss: 0.002
[56,     1] loss: 0.002
[57,     1] loss: 0.002
[58,     1] loss: 0.001
[59,     1] loss: 0.002
[60,     1] loss: 0.001
Early stopping applied (best metric=0.4214588701725006)
Finished Training
Total time taken: 180.88428044319153
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.649
[4,     1] loss: 0.620
[5,     1] loss: 0.581
[6,     1] loss: 0.546
[7,     1] loss: 0.498
[8,     1] loss: 0.453
[9,     1] loss: 0.404
[10,     1] loss: 0.354
[11,     1] loss: 0.300
[12,     1] loss: 0.254
[13,     1] loss: 0.205
[14,     1] loss: 0.169
[15,     1] loss: 0.142
[16,     1] loss: 0.119
[17,     1] loss: 0.092
[18,     1] loss: 0.076
[19,     1] loss: 0.061
[20,     1] loss: 0.048
[21,     1] loss: 0.040
[22,     1] loss: 0.038
[23,     1] loss: 0.032
[24,     1] loss: 0.024
[25,     1] loss: 0.019
[26,     1] loss: 0.014
[27,     1] loss: 0.007
[28,     1] loss: 0.006
[29,     1] loss: 0.005
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
[62,     1] loss: 0.001
Early stopping applied (best metric=0.35386401414871216)
Finished Training
Total time taken: 186.28520679473877
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.647
[4,     1] loss: 0.616
[5,     1] loss: 0.580
[6,     1] loss: 0.541
[7,     1] loss: 0.490
[8,     1] loss: 0.444
[9,     1] loss: 0.395
[10,     1] loss: 0.346
[11,     1] loss: 0.298
[12,     1] loss: 0.253
[13,     1] loss: 0.213
[14,     1] loss: 0.177
[15,     1] loss: 0.153
[16,     1] loss: 0.129
[17,     1] loss: 0.108
[18,     1] loss: 0.086
[19,     1] loss: 0.064
[20,     1] loss: 0.048
[21,     1] loss: 0.039
[22,     1] loss: 0.027
[23,     1] loss: 0.022
[24,     1] loss: 0.015
[25,     1] loss: 0.013
[26,     1] loss: 0.009
[27,     1] loss: 0.007
[28,     1] loss: 0.006
[29,     1] loss: 0.005
[30,     1] loss: 0.004
[31,     1] loss: 0.003
[32,     1] loss: 0.003
[33,     1] loss: 0.002
[34,     1] loss: 0.002
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
[62,     1] loss: 0.001
[63,     1] loss: 0.001
Early stopping applied (best metric=0.36030179262161255)
Finished Training
Total time taken: 189.46353673934937
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.650
[4,     1] loss: 0.619
[5,     1] loss: 0.585
[6,     1] loss: 0.547
[7,     1] loss: 0.502
[8,     1] loss: 0.453
[9,     1] loss: 0.403
[10,     1] loss: 0.348
[11,     1] loss: 0.295
[12,     1] loss: 0.244
[13,     1] loss: 0.201
[14,     1] loss: 0.162
[15,     1] loss: 0.121
[16,     1] loss: 0.097
[17,     1] loss: 0.068
[18,     1] loss: 0.044
[19,     1] loss: 0.030
[20,     1] loss: 0.019
[21,     1] loss: 0.015
[22,     1] loss: 0.011
[23,     1] loss: 0.008
[24,     1] loss: 0.005
[25,     1] loss: 0.004
[26,     1] loss: 0.003
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.002
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.000
[37,     1] loss: 0.000
[38,     1] loss: 0.000
[39,     1] loss: 0.000
[40,     1] loss: 0.000
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.000
[51,     1] loss: 0.000
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.000
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
Early stopping applied (best metric=0.43132004141807556)
Finished Training
Total time taken: 181.09342098236084
{'Hydroxylation-P Validation Accuracy': 0.6922186792548601, 'Hydroxylation-P Validation Sensitivity': 0.8388571428571429, 'Hydroxylation-P Validation Specificity': 0.6607526559928176, 'Hydroxylation-P Validation Precision': 0.35093164570031754, 'Hydroxylation-P AUC ROC': 0.8394268209553261, 'Hydroxylation-P AUC PR': 0.5741843272521965, 'Hydroxylation-P MCC': 0.38821743034668943, 'Hydroxylation-P F1': 0.49309125046591507, 'Validation Loss (Hydroxylation-P)': 0.4041715097427368, 'Validation Loss (total)': 0.4041715097427368}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005616695604014276,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1236546198,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.170969032910264}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.678
[3,     1] loss: 0.641
[4,     1] loss: 0.605
[5,     1] loss: 0.562
[6,     1] loss: 0.510
[7,     1] loss: 0.452
[8,     1] loss: 0.384
[9,     1] loss: 0.320
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008668202936141388,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3472119238,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.167071649076256}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.671
[3,     1] loss: 0.626
[4,     1] loss: 0.576
[5,     1] loss: 0.517
[6,     1] loss: 0.460
[7,     1] loss: 0.398
[8,     1] loss: 0.339
[9,     1] loss: 0.269
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005688621131173559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 514582066,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.684635487061758}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.663
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004083918851558113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1086788065,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.206590270528078}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.682
[3,     1] loss: 0.648
[4,     1] loss: 0.610
[5,     1] loss: 0.566
[6,     1] loss: 0.519
[7,     1] loss: 0.467
[8,     1] loss: 0.416
[9,     1] loss: 0.365
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0072064223932083475,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2954420726,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.162609710673972}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.687
[3,     1] loss: 0.646
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005724074776817277,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1444443790,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.5653037608460973}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.661
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006149452423904201,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2493226035,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.389502563787726}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.650
[4,     1] loss: 0.616
[5,     1] loss: 0.574
[6,     1] loss: 0.527
[7,     1] loss: 0.473
[8,     1] loss: 0.416
[9,     1] loss: 0.358
[10,     1] loss: 0.295
[11,     1] loss: 0.245
[12,     1] loss: 0.195
[13,     1] loss: 0.161
[14,     1] loss: 0.162
[15,     1] loss: 0.131
[16,     1] loss: 0.118
[17,     1] loss: 0.084
[18,     1] loss: 0.058
[19,     1] loss: 0.053
[20,     1] loss: 0.035
[21,     1] loss: 0.034
[22,     1] loss: 0.023
[23,     1] loss: 0.024
[24,     1] loss: 0.019
[25,     1] loss: 0.016
[26,     1] loss: 0.016
[27,     1] loss: 0.012
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009349561913612345,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2660698806,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.6201080535436}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.694
[3,     1] loss: 0.683
[4,     1] loss: 0.673
[5,     1] loss: 0.658
[6,     1] loss: 0.643
[7,     1] loss: 0.626
[8,     1] loss: 0.608
[9,     1] loss: 0.593
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0045646505814939675,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2139422520,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.200195848304972}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.683
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006247194555210305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2692121248,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.968095524150339}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.647
[4,     1] loss: 0.605
[5,     1] loss: 0.558
[6,     1] loss: 0.502
[7,     1] loss: 0.431
[8,     1] loss: 0.362
[9,     1] loss: 0.305
[10,     1] loss: 0.237
[11,     1] loss: 0.199
[12,     1] loss: 0.155
[13,     1] loss: 0.141
[14,     1] loss: 0.120
[15,     1] loss: 0.090
[16,     1] loss: 0.061
[17,     1] loss: 0.048
[18,     1] loss: 0.029
[19,     1] loss: 0.069
[20,     1] loss: 0.094
[21,     1] loss: 0.025
[22,     1] loss: 0.124
[23,     1] loss: 0.055
[24,     1] loss: 0.089
[25,     1] loss: 0.042
[26,     1] loss: 0.042
[27,     1] loss: 0.056
[28,     1] loss: 0.044
[29,     1] loss: 0.039
[30,     1] loss: 0.035
[31,     1] loss: 0.032
[32,     1] loss: 0.030
[33,     1] loss: 0.028
[34,     1] loss: 0.027
[35,     1] loss: 0.023
[36,     1] loss: 0.020
[37,     1] loss: 0.018
[38,     1] loss: 0.016
[39,     1] loss: 0.014
[40,     1] loss: 0.013
[41,     1] loss: 0.013
[42,     1] loss: 0.012
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.012
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.012
[52,     1] loss: 0.012
[53,     1] loss: 0.012
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.011
[57,     1] loss: 0.011
[58,     1] loss: 0.011
Early stopping applied (best metric=0.37956103682518005)
Finished Training
Total time taken: 175.53739404678345
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.662
[3,     1] loss: 0.597
[4,     1] loss: 0.539
[5,     1] loss: 0.490
[6,     1] loss: 0.455
[7,     1] loss: 0.409
[8,     1] loss: 0.366
[9,     1] loss: 0.339
[10,     1] loss: 0.303
[11,     1] loss: 0.269
[12,     1] loss: 0.244
[13,     1] loss: 0.216
[14,     1] loss: 0.191
[15,     1] loss: 0.165
[16,     1] loss: 0.144
[17,     1] loss: 0.127
[18,     1] loss: 0.113
[19,     1] loss: 0.099
[20,     1] loss: 0.093
[21,     1] loss: 0.085
[22,     1] loss: 0.077
[23,     1] loss: 0.069
[24,     1] loss: 0.060
[25,     1] loss: 0.061
[26,     1] loss: 0.039
[27,     1] loss: 0.042
[28,     1] loss: 0.038
[29,     1] loss: 0.027
[30,     1] loss: 0.033
[31,     1] loss: 0.027
[32,     1] loss: 0.027
[33,     1] loss: 0.024
[34,     1] loss: 0.021
[35,     1] loss: 0.028
[36,     1] loss: 0.043
[37,     1] loss: 0.033
[38,     1] loss: 0.031
[39,     1] loss: 0.032
[40,     1] loss: 0.029
[41,     1] loss: 0.030
[42,     1] loss: 0.038
[43,     1] loss: 0.023
[44,     1] loss: 0.045
[45,     1] loss: 0.035
[46,     1] loss: 0.033
[47,     1] loss: 0.043
[48,     1] loss: 0.020
[49,     1] loss: 0.033
[50,     1] loss: 0.021
[51,     1] loss: 0.022
[52,     1] loss: 0.020
[53,     1] loss: 0.016
[54,     1] loss: 0.015
[55,     1] loss: 0.016
[56,     1] loss: 0.015
[57,     1] loss: 0.014
[58,     1] loss: 0.013
[59,     1] loss: 0.013
[60,     1] loss: 0.013
Early stopping applied (best metric=0.47844579815864563)
Finished Training
Total time taken: 181.91545939445496
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.676
[3,     1] loss: 0.637
[4,     1] loss: 0.594
[5,     1] loss: 0.549
[6,     1] loss: 0.504
[7,     1] loss: 0.455
[8,     1] loss: 0.406
[9,     1] loss: 0.359
[10,     1] loss: 0.299
[11,     1] loss: 0.243
[12,     1] loss: 0.203
[13,     1] loss: 0.169
[14,     1] loss: 0.134
[15,     1] loss: 0.119
[16,     1] loss: 0.093
[17,     1] loss: 0.067
[18,     1] loss: 0.069
[19,     1] loss: 0.054
[20,     1] loss: 0.088
[21,     1] loss: 0.285
[22,     1] loss: 0.043
[23,     1] loss: 0.160
[24,     1] loss: 0.129
[25,     1] loss: 0.080
[26,     1] loss: 0.080
[27,     1] loss: 0.092
[28,     1] loss: 0.084
[29,     1] loss: 0.077
[30,     1] loss: 0.072
[31,     1] loss: 0.064
[32,     1] loss: 0.052
[33,     1] loss: 0.043
[34,     1] loss: 0.037
[35,     1] loss: 0.031
[36,     1] loss: 0.027
[37,     1] loss: 0.023
[38,     1] loss: 0.021
[39,     1] loss: 0.018
[40,     1] loss: 0.017
[41,     1] loss: 0.016
[42,     1] loss: 0.016
[43,     1] loss: 0.015
[44,     1] loss: 0.014
[45,     1] loss: 0.015
[46,     1] loss: 0.015
[47,     1] loss: 0.016
[48,     1] loss: 0.016
[49,     1] loss: 0.017
[50,     1] loss: 0.017
[51,     1] loss: 0.017
[52,     1] loss: 0.018
[53,     1] loss: 0.018
[54,     1] loss: 0.018
[55,     1] loss: 0.017
[56,     1] loss: 0.016
[57,     1] loss: 0.015
[58,     1] loss: 0.016
[59,     1] loss: 0.015
Early stopping applied (best metric=0.36828935146331787)
Finished Training
Total time taken: 179.00022506713867
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.687
[3,     1] loss: 0.651
[4,     1] loss: 0.614
[5,     1] loss: 0.571
[6,     1] loss: 0.515
[7,     1] loss: 0.454
[8,     1] loss: 0.389
[9,     1] loss: 0.317
[10,     1] loss: 0.257
[11,     1] loss: 0.234
[12,     1] loss: 0.189
[13,     1] loss: 0.147
[14,     1] loss: 0.122
[15,     1] loss: 0.101
[16,     1] loss: 0.081
[17,     1] loss: 0.062
[18,     1] loss: 0.068
[19,     1] loss: 0.114
[20,     1] loss: 0.051
[21,     1] loss: 0.164
[22,     1] loss: 0.043
[23,     1] loss: 0.128
[24,     1] loss: 0.072
[25,     1] loss: 0.180
[26,     1] loss: 0.053
[27,     1] loss: 0.081
[28,     1] loss: 0.090
[29,     1] loss: 0.079
[30,     1] loss: 0.066
[31,     1] loss: 0.069
[32,     1] loss: 0.057
[33,     1] loss: 0.049
[34,     1] loss: 0.040
[35,     1] loss: 0.034
[36,     1] loss: 0.027
[37,     1] loss: 0.024
[38,     1] loss: 0.021
[39,     1] loss: 0.017
[40,     1] loss: 0.016
[41,     1] loss: 0.015
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.013
[46,     1] loss: 0.013
[47,     1] loss: 0.014
[48,     1] loss: 0.014
[49,     1] loss: 0.015
[50,     1] loss: 0.016
[51,     1] loss: 0.017
[52,     1] loss: 0.016
[53,     1] loss: 0.018
[54,     1] loss: 0.018
[55,     1] loss: 0.017
[56,     1] loss: 0.017
Early stopping applied (best metric=0.4317944347858429)
Finished Training
Total time taken: 170.1955828666687
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.678
[3,     1] loss: 0.630
[4,     1] loss: 0.583
[5,     1] loss: 0.529
[6,     1] loss: 0.476
[7,     1] loss: 0.424
[8,     1] loss: 0.375
[9,     1] loss: 0.325
[10,     1] loss: 0.285
[11,     1] loss: 0.263
[12,     1] loss: 0.206
[13,     1] loss: 0.184
[14,     1] loss: 0.134
[15,     1] loss: 0.120
[16,     1] loss: 0.107
[17,     1] loss: 0.092
[18,     1] loss: 0.090
[19,     1] loss: 0.070
[20,     1] loss: 0.057
[21,     1] loss: 0.042
[22,     1] loss: 0.037
[23,     1] loss: 0.033
[24,     1] loss: 0.027
[25,     1] loss: 0.024
[26,     1] loss: 0.021
[27,     1] loss: 0.018
[28,     1] loss: 0.017
[29,     1] loss: 0.017
[30,     1] loss: 0.016
[31,     1] loss: 0.015
[32,     1] loss: 0.015
[33,     1] loss: 0.015
[34,     1] loss: 0.015
[35,     1] loss: 0.015
[36,     1] loss: 0.014
[37,     1] loss: 0.014
[38,     1] loss: 0.014
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.015
[43,     1] loss: 0.014
[44,     1] loss: 0.013
[45,     1] loss: 0.013
[46,     1] loss: 0.013
[47,     1] loss: 0.012
[48,     1] loss: 0.013
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.010
[55,     1] loss: 0.010
Early stopping applied (best metric=0.44885900616645813)
Finished Training
Total time taken: 167.80990171432495
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.690
[3,     1] loss: 0.669
[4,     1] loss: 0.639
[5,     1] loss: 0.603
[6,     1] loss: 0.556
[7,     1] loss: 0.503
[8,     1] loss: 0.444
[9,     1] loss: 0.413
[10,     1] loss: 0.352
[11,     1] loss: 0.328
[12,     1] loss: 0.292
[13,     1] loss: 0.260
[14,     1] loss: 0.221
[15,     1] loss: 0.200
[16,     1] loss: 0.169
[17,     1] loss: 0.148
[18,     1] loss: 0.132
[19,     1] loss: 0.122
[20,     1] loss: 0.114
[21,     1] loss: 0.103
[22,     1] loss: 0.089
[23,     1] loss: 0.076
[24,     1] loss: 0.071
[25,     1] loss: 0.069
[26,     1] loss: 0.061
[27,     1] loss: 0.055
[28,     1] loss: 0.049
[29,     1] loss: 0.046
[30,     1] loss: 0.041
[31,     1] loss: 0.038
[32,     1] loss: 0.036
[33,     1] loss: 0.032
[34,     1] loss: 0.029
[35,     1] loss: 0.027
[36,     1] loss: 0.026
[37,     1] loss: 0.025
[38,     1] loss: 0.025
[39,     1] loss: 0.024
[40,     1] loss: 0.022
[41,     1] loss: 0.022
[42,     1] loss: 0.022
[43,     1] loss: 0.020
[44,     1] loss: 0.019
[45,     1] loss: 0.019
[46,     1] loss: 0.018
[47,     1] loss: 0.018
[48,     1] loss: 0.016
[49,     1] loss: 0.017
[50,     1] loss: 0.016
[51,     1] loss: 0.015
[52,     1] loss: 0.015
[53,     1] loss: 0.014
[54,     1] loss: 0.014
[55,     1] loss: 0.013
[56,     1] loss: 0.012
[57,     1] loss: 0.011
[58,     1] loss: 0.010
[59,     1] loss: 0.009
[60,     1] loss: 0.008
[61,     1] loss: 0.007
[62,     1] loss: 0.006
Early stopping applied (best metric=0.4041818082332611)
Finished Training
Total time taken: 188.8249225616455
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.683
[3,     1] loss: 0.643
[4,     1] loss: 0.595
[5,     1] loss: 0.553
[6,     1] loss: 0.517
[7,     1] loss: 0.480
[8,     1] loss: 0.434
[9,     1] loss: 0.392
[10,     1] loss: 0.347
[11,     1] loss: 0.309
[12,     1] loss: 0.288
[13,     1] loss: 0.257
[14,     1] loss: 0.227
[15,     1] loss: 0.203
[16,     1] loss: 0.180
[17,     1] loss: 0.163
[18,     1] loss: 0.149
[19,     1] loss: 0.130
[20,     1] loss: 0.114
[21,     1] loss: 0.098
[22,     1] loss: 0.076
[23,     1] loss: 0.081
[24,     1] loss: 0.109
[25,     1] loss: 0.086
[26,     1] loss: 0.073
[27,     1] loss: 0.067
[28,     1] loss: 0.059
[29,     1] loss: 0.057
[30,     1] loss: 0.048
[31,     1] loss: 0.044
[32,     1] loss: 0.037
[33,     1] loss: 0.032
[34,     1] loss: 0.028
[35,     1] loss: 0.026
[36,     1] loss: 0.022
[37,     1] loss: 0.020
[38,     1] loss: 0.017
[39,     1] loss: 0.016
[40,     1] loss: 0.014
[41,     1] loss: 0.013
[42,     1] loss: 0.013
[43,     1] loss: 0.012
[44,     1] loss: 0.012
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.012
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.012
[56,     1] loss: 0.010
[57,     1] loss: 0.011
[58,     1] loss: 0.010
[59,     1] loss: 0.009
Early stopping applied (best metric=0.42846229672431946)
Finished Training
Total time taken: 180.44688892364502
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.683
[3,     1] loss: 0.643
[4,     1] loss: 0.596
[5,     1] loss: 0.553
[6,     1] loss: 0.503
[7,     1] loss: 0.460
[8,     1] loss: 0.407
[9,     1] loss: 0.355
[10,     1] loss: 0.292
[11,     1] loss: 0.246
[12,     1] loss: 0.201
[13,     1] loss: 0.168
[14,     1] loss: 0.140
[15,     1] loss: 0.117
[16,     1] loss: 0.088
[17,     1] loss: 0.080
[18,     1] loss: 0.088
[19,     1] loss: 0.067
[20,     1] loss: 0.048
[21,     1] loss: 0.061
[22,     1] loss: 0.041
[23,     1] loss: 0.037
[24,     1] loss: 0.033
[25,     1] loss: 0.030
[26,     1] loss: 0.028
[27,     1] loss: 0.023
[28,     1] loss: 0.030
[29,     1] loss: 0.019
[30,     1] loss: 0.028
[31,     1] loss: 0.019
[32,     1] loss: 0.030
[33,     1] loss: 0.018
[34,     1] loss: 0.017
[35,     1] loss: 0.016
[36,     1] loss: 0.016
[37,     1] loss: 0.015
[38,     1] loss: 0.015
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.012
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.008
[59,     1] loss: 0.008
Early stopping applied (best metric=0.41316184401512146)
Finished Training
Total time taken: 180.33435678482056
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.697
[3,     1] loss: 0.671
[4,     1] loss: 0.643
[5,     1] loss: 0.609
[6,     1] loss: 0.571
[7,     1] loss: 0.522
[8,     1] loss: 0.464
[9,     1] loss: 0.402
[10,     1] loss: 0.337
[11,     1] loss: 0.281
[12,     1] loss: 0.227
[13,     1] loss: 0.184
[14,     1] loss: 0.144
[15,     1] loss: 0.139
[16,     1] loss: 0.116
[17,     1] loss: 0.086
[18,     1] loss: 0.094
[19,     1] loss: 0.055
[20,     1] loss: 0.045
[21,     1] loss: 0.047
[22,     1] loss: 0.036
[23,     1] loss: 0.037
[24,     1] loss: 0.022
[25,     1] loss: 0.023
[26,     1] loss: 0.019
[27,     1] loss: 0.018
[28,     1] loss: 0.017
[29,     1] loss: 0.015
[30,     1] loss: 0.014
[31,     1] loss: 0.014
[32,     1] loss: 0.013
[33,     1] loss: 0.013
[34,     1] loss: 0.013
[35,     1] loss: 0.013
[36,     1] loss: 0.012
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.012
[40,     1] loss: 0.013
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.012
[44,     1] loss: 0.011
[45,     1] loss: 0.012
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.008
[59,     1] loss: 0.008
[60,     1] loss: 0.007
[61,     1] loss: 0.007
[62,     1] loss: 0.007
[63,     1] loss: 0.007
Early stopping applied (best metric=0.40562745928764343)
Finished Training
Total time taken: 192.66527128219604
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.684
[3,     1] loss: 0.641
[4,     1] loss: 0.592
[5,     1] loss: 0.539
[6,     1] loss: 0.475
[7,     1] loss: 0.407
[8,     1] loss: 0.345
[9,     1] loss: 0.288
[10,     1] loss: 0.227
[11,     1] loss: 0.204
[12,     1] loss: 0.201
[13,     1] loss: 0.130
[14,     1] loss: 0.137
[15,     1] loss: 0.120
[16,     1] loss: 0.096
[17,     1] loss: 0.077
[18,     1] loss: 0.075
[19,     1] loss: 0.061
[20,     1] loss: 0.058
[21,     1] loss: 0.045
[22,     1] loss: 0.041
[23,     1] loss: 0.038
[24,     1] loss: 0.031
[25,     1] loss: 0.027
[26,     1] loss: 0.023
[27,     1] loss: 0.021
[28,     1] loss: 0.018
[29,     1] loss: 0.017
[30,     1] loss: 0.016
[31,     1] loss: 0.015
[32,     1] loss: 0.014
[33,     1] loss: 0.013
[34,     1] loss: 0.013
[35,     1] loss: 0.012
[36,     1] loss: 0.012
[37,     1] loss: 0.011
[38,     1] loss: 0.011
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.007
Early stopping applied (best metric=0.41314613819122314)
Finished Training
Total time taken: 174.75509095191956
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.634
[4,     1] loss: 0.590
[5,     1] loss: 0.533
[6,     1] loss: 0.471
[7,     1] loss: 0.394
[8,     1] loss: 0.330
[9,     1] loss: 0.273
[10,     1] loss: 0.227
[11,     1] loss: 0.167
[12,     1] loss: 0.139
[13,     1] loss: 0.110
[14,     1] loss: 0.090
[15,     1] loss: 0.085
[16,     1] loss: 0.088
[17,     1] loss: 0.085
[18,     1] loss: 0.074
[19,     1] loss: 0.074
[20,     1] loss: 0.066
[21,     1] loss: 0.063
[22,     1] loss: 0.056
[23,     1] loss: 0.059
[24,     1] loss: 0.052
[25,     1] loss: 0.055
[26,     1] loss: 0.050
[27,     1] loss: 0.046
[28,     1] loss: 0.044
[29,     1] loss: 0.041
[30,     1] loss: 0.038
[31,     1] loss: 0.036
[32,     1] loss: 0.033
[33,     1] loss: 0.031
[34,     1] loss: 0.027
[35,     1] loss: 0.024
[36,     1] loss: 0.021
[37,     1] loss: 0.018
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.014
[41,     1] loss: 0.012
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.006
[57,     1] loss: 0.006
[58,     1] loss: 0.006
[59,     1] loss: 0.006
Early stopping applied (best metric=0.3969179391860962)
Finished Training
Total time taken: 181.01665377616882
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.649
[4,     1] loss: 0.609
[5,     1] loss: 0.564
[6,     1] loss: 0.516
[7,     1] loss: 0.453
[8,     1] loss: 0.392
[9,     1] loss: 0.332
[10,     1] loss: 0.309
[11,     1] loss: 0.236
[12,     1] loss: 0.211
[13,     1] loss: 0.168
[14,     1] loss: 0.136
[15,     1] loss: 0.113
[16,     1] loss: 0.097
[17,     1] loss: 0.079
[18,     1] loss: 0.064
[19,     1] loss: 0.050
[20,     1] loss: 0.041
[21,     1] loss: 0.030
[22,     1] loss: 0.025
[23,     1] loss: 0.021
[24,     1] loss: 0.016
[25,     1] loss: 0.015
[26,     1] loss: 0.014
[27,     1] loss: 0.012
[28,     1] loss: 0.011
[29,     1] loss: 0.011
[30,     1] loss: 0.011
[31,     1] loss: 0.011
[32,     1] loss: 0.011
[33,     1] loss: 0.011
[34,     1] loss: 0.011
[35,     1] loss: 0.011
[36,     1] loss: 0.011
[37,     1] loss: 0.011
[38,     1] loss: 0.011
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.007
Early stopping applied (best metric=0.41635552048683167)
Finished Training
Total time taken: 178.40494537353516
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.676
[3,     1] loss: 0.645
[4,     1] loss: 0.613
[5,     1] loss: 0.576
[6,     1] loss: 0.540
[7,     1] loss: 0.514
[8,     1] loss: 0.484
[9,     1] loss: 0.462
[10,     1] loss: 0.429
[11,     1] loss: 0.401
[12,     1] loss: 0.364
[13,     1] loss: 0.335
[14,     1] loss: 0.308
[15,     1] loss: 0.335
[16,     1] loss: 0.255
[17,     1] loss: 0.244
[18,     1] loss: 0.205
[19,     1] loss: 0.190
[20,     1] loss: 0.164
[21,     1] loss: 0.155
[22,     1] loss: 0.119
[23,     1] loss: 0.107
[24,     1] loss: 0.078
[25,     1] loss: 0.065
[26,     1] loss: 0.048
[27,     1] loss: 0.036
[28,     1] loss: 0.029
[29,     1] loss: 0.023
[30,     1] loss: 0.019
[31,     1] loss: 0.016
[32,     1] loss: 0.014
[33,     1] loss: 0.012
[34,     1] loss: 0.011
[35,     1] loss: 0.010
[36,     1] loss: 0.009
[37,     1] loss: 0.008
[38,     1] loss: 0.008
[39,     1] loss: 0.008
[40,     1] loss: 0.008
[41,     1] loss: 0.008
[42,     1] loss: 0.009
[43,     1] loss: 0.008
[44,     1] loss: 0.009
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
Early stopping applied (best metric=0.4491945207118988)
Finished Training
Total time taken: 169.2929871082306
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.645
[4,     1] loss: 0.609
[5,     1] loss: 0.568
[6,     1] loss: 0.518
[7,     1] loss: 0.480
[8,     1] loss: 0.445
[9,     1] loss: 0.411
[10,     1] loss: 0.371
[11,     1] loss: 0.335
[12,     1] loss: 0.315
[13,     1] loss: 0.288
[14,     1] loss: 0.259
[15,     1] loss: 0.236
[16,     1] loss: 0.220
[17,     1] loss: 0.199
[18,     1] loss: 0.184
[19,     1] loss: 0.162
[20,     1] loss: 0.142
[21,     1] loss: 0.128
[22,     1] loss: 0.112
[23,     1] loss: 0.093
[24,     1] loss: 0.089
[25,     1] loss: 0.105
[26,     1] loss: 0.071
[27,     1] loss: 0.122
[28,     1] loss: 0.121
[29,     1] loss: 0.073
[30,     1] loss: 0.105
[31,     1] loss: 0.067
[32,     1] loss: 0.070
[33,     1] loss: 0.055
[34,     1] loss: 0.052
[35,     1] loss: 0.052
[36,     1] loss: 0.047
[37,     1] loss: 0.044
[38,     1] loss: 0.039
[39,     1] loss: 0.035
[40,     1] loss: 0.033
[41,     1] loss: 0.027
[42,     1] loss: 0.026
[43,     1] loss: 0.023
[44,     1] loss: 0.022
[45,     1] loss: 0.020
[46,     1] loss: 0.019
[47,     1] loss: 0.018
[48,     1] loss: 0.017
[49,     1] loss: 0.016
[50,     1] loss: 0.016
[51,     1] loss: 0.016
[52,     1] loss: 0.016
[53,     1] loss: 0.015
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.015
[57,     1] loss: 0.014
[58,     1] loss: 0.014
[59,     1] loss: 0.014
[60,     1] loss: 0.013
[61,     1] loss: 0.013
[62,     1] loss: 0.014
[63,     1] loss: 0.014
[64,     1] loss: 0.014
[65,     1] loss: 0.014
[66,     1] loss: 0.014
[67,     1] loss: 0.015
[68,     1] loss: 0.014
[69,     1] loss: 0.014
[70,     1] loss: 0.015
[71,     1] loss: 0.014
[72,     1] loss: 0.014
[73,     1] loss: 0.013
[74,     1] loss: 0.013
[75,     1] loss: 0.013
[76,     1] loss: 0.013
[77,     1] loss: 0.013
[78,     1] loss: 0.013
[79,     1] loss: 0.013
[80,     1] loss: 0.048
[81,     1] loss: 0.436
[82,     1] loss: 1.544
[83,     1] loss: 0.632
[84,     1] loss: 0.663
[85,     1] loss: 0.663
[86,     1] loss: 0.667
[87,     1] loss: 0.671
[88,     1] loss: 0.676
[89,     1] loss: 0.678
[90,     1] loss: 0.679
[91,     1] loss: 0.681
[92,     1] loss: 0.681
[93,     1] loss: 0.682
[94,     1] loss: 0.683
[95,     1] loss: 0.684
[96,     1] loss: 0.685
[97,     1] loss: 0.685
[98,     1] loss: 0.686
[99,     1] loss: 0.687
[100,     1] loss: 0.687
[101,     1] loss: 0.688
[102,     1] loss: 0.688
[103,     1] loss: 0.689
[104,     1] loss: 0.689
[105,     1] loss: 0.689
[106,     1] loss: 0.690
[107,     1] loss: 0.690
[108,     1] loss: 0.690
[109,     1] loss: 0.691
[110,     1] loss: 0.691
[111,     1] loss: 0.691
[112,     1] loss: 0.691
[113,     1] loss: 0.691
[114,     1] loss: 0.692
[115,     1] loss: 0.692
Early stopping applied (best metric=0.39227956533432007)
Finished Training
Total time taken: 351.5689935684204
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.687
[3,     1] loss: 0.646
[4,     1] loss: 0.599
[5,     1] loss: 0.548
[6,     1] loss: 0.490
[7,     1] loss: 0.437
[8,     1] loss: 0.379
[9,     1] loss: 0.330
[10,     1] loss: 0.281
[11,     1] loss: 0.225
[12,     1] loss: 0.184
[13,     1] loss: 0.146
[14,     1] loss: 0.110
[15,     1] loss: 0.105
[16,     1] loss: 0.092
[17,     1] loss: 0.069
[18,     1] loss: 0.048
[19,     1] loss: 0.038
[20,     1] loss: 0.030
[21,     1] loss: 0.026
[22,     1] loss: 0.022
[23,     1] loss: 0.018
[24,     1] loss: 0.016
[25,     1] loss: 0.014
[26,     1] loss: 0.013
[27,     1] loss: 0.012
[28,     1] loss: 0.011
[29,     1] loss: 0.011
[30,     1] loss: 0.011
[31,     1] loss: 0.011
[32,     1] loss: 0.010
[33,     1] loss: 0.011
[34,     1] loss: 0.010
[35,     1] loss: 0.010
[36,     1] loss: 0.011
[37,     1] loss: 0.011
[38,     1] loss: 0.010
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.009
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.007
[50,     1] loss: 0.007
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
Early stopping applied (best metric=0.44170302152633667)
Finished Training
Total time taken: 175.90593838691711
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.665
[4,     1] loss: 0.637
[5,     1] loss: 0.602
[6,     1] loss: 0.565
[7,     1] loss: 0.530
[8,     1] loss: 0.487
[9,     1] loss: 0.441
[10,     1] loss: 0.392
[11,     1] loss: 0.348
[12,     1] loss: 0.302
[13,     1] loss: 0.253
[14,     1] loss: 0.245
[15,     1] loss: 0.214
[16,     1] loss: 0.169
[17,     1] loss: 0.216
[18,     1] loss: 0.129
[19,     1] loss: 0.127
[20,     1] loss: 0.104
[21,     1] loss: 0.086
[22,     1] loss: 0.078
[23,     1] loss: 0.073
[24,     1] loss: 0.060
[25,     1] loss: 0.052
[26,     1] loss: 0.043
[27,     1] loss: 0.040
[28,     1] loss: 0.037
[29,     1] loss: 0.033
[30,     1] loss: 0.031
[31,     1] loss: 0.030
[32,     1] loss: 0.029
[33,     1] loss: 0.028
[34,     1] loss: 0.027
[35,     1] loss: 0.027
[36,     1] loss: 0.026
[37,     1] loss: 0.025
[38,     1] loss: 0.025
[39,     1] loss: 0.025
[40,     1] loss: 0.024
[41,     1] loss: 0.024
[42,     1] loss: 0.024
[43,     1] loss: 0.023
[44,     1] loss: 0.022
[45,     1] loss: 0.020
[46,     1] loss: 0.017
[47,     1] loss: 0.014
[48,     1] loss: 0.013
[49,     1] loss: 0.012
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.008
[60,     1] loss: 0.008
[61,     1] loss: 0.008
[62,     1] loss: 0.009
Early stopping applied (best metric=0.40819936990737915)
Finished Training
Total time taken: 191.19569778442383
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.695
[3,     1] loss: 0.662
[4,     1] loss: 0.622
[5,     1] loss: 0.572
[6,     1] loss: 0.508
[7,     1] loss: 0.431
[8,     1] loss: 0.355
[9,     1] loss: 0.306
[10,     1] loss: 0.285
[11,     1] loss: 0.201
[12,     1] loss: 0.202
[13,     1] loss: 0.163
[14,     1] loss: 0.141
[15,     1] loss: 0.124
[16,     1] loss: 0.096
[17,     1] loss: 0.076
[18,     1] loss: 0.063
[19,     1] loss: 0.050
[20,     1] loss: 0.039
[21,     1] loss: 0.034
[22,     1] loss: 0.032
[23,     1] loss: 0.029
[24,     1] loss: 0.027
[25,     1] loss: 0.025
[26,     1] loss: 0.024
[27,     1] loss: 0.023
[28,     1] loss: 0.021
[29,     1] loss: 0.020
[30,     1] loss: 0.018
[31,     1] loss: 0.017
[32,     1] loss: 0.015
[33,     1] loss: 0.013
[34,     1] loss: 0.012
[35,     1] loss: 0.013
[36,     1] loss: 0.012
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.013
[40,     1] loss: 0.012
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.011
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.007
[59,     1] loss: 0.008
Early stopping applied (best metric=0.396729975938797)
Finished Training
Total time taken: 182.5182716846466
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.667
[4,     1] loss: 0.635
[5,     1] loss: 0.591
[6,     1] loss: 0.541
[7,     1] loss: 0.485
[8,     1] loss: 0.430
[9,     1] loss: 0.386
[10,     1] loss: 0.341
[11,     1] loss: 0.283
[12,     1] loss: 0.258
[13,     1] loss: 0.222
[14,     1] loss: 0.189
[15,     1] loss: 0.157
[16,     1] loss: 0.132
[17,     1] loss: 0.111
[18,     1] loss: 0.102
[19,     1] loss: 0.088
[20,     1] loss: 0.070
[21,     1] loss: 0.069
[22,     1] loss: 0.053
[23,     1] loss: 0.063
[24,     1] loss: 0.132
[25,     1] loss: 0.180
[26,     1] loss: 0.060
[27,     1] loss: 0.070
[28,     1] loss: 0.106
[29,     1] loss: 0.072
[30,     1] loss: 0.080
[31,     1] loss: 0.079
[32,     1] loss: 0.073
[33,     1] loss: 0.072
[34,     1] loss: 0.067
[35,     1] loss: 0.061
[36,     1] loss: 0.055
[37,     1] loss: 0.050
[38,     1] loss: 0.046
[39,     1] loss: 0.043
[40,     1] loss: 0.040
[41,     1] loss: 0.039
[42,     1] loss: 0.036
[43,     1] loss: 0.032
[44,     1] loss: 0.032
[45,     1] loss: 0.031
[46,     1] loss: 0.029
[47,     1] loss: 0.028
[48,     1] loss: 0.027
[49,     1] loss: 0.026
[50,     1] loss: 0.027
[51,     1] loss: 0.026
[52,     1] loss: 0.026
[53,     1] loss: 0.026
[54,     1] loss: 0.027
[55,     1] loss: 0.026
[56,     1] loss: 0.025
[57,     1] loss: 0.025
[58,     1] loss: 0.023
Early stopping applied (best metric=0.42213451862335205)
Finished Training
Total time taken: 179.7729456424713
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.662
[4,     1] loss: 0.628
[5,     1] loss: 0.586
[6,     1] loss: 0.542
[7,     1] loss: 0.490
[8,     1] loss: 0.430
[9,     1] loss: 0.361
[10,     1] loss: 0.289
[11,     1] loss: 0.238
[12,     1] loss: 0.194
[13,     1] loss: 0.159
[14,     1] loss: 0.129
[15,     1] loss: 0.095
[16,     1] loss: 0.084
[17,     1] loss: 0.068
[18,     1] loss: 0.056
[19,     1] loss: 0.055
[20,     1] loss: 0.044
[21,     1] loss: 0.032
[22,     1] loss: 0.032
[23,     1] loss: 0.023
[24,     1] loss: 0.025
[25,     1] loss: 0.020
[26,     1] loss: 0.020
[27,     1] loss: 0.017
[28,     1] loss: 0.018
[29,     1] loss: 0.017
[30,     1] loss: 0.016
[31,     1] loss: 0.016
[32,     1] loss: 0.016
[33,     1] loss: 0.015
[34,     1] loss: 0.014
[35,     1] loss: 0.015
[36,     1] loss: 0.013
[37,     1] loss: 0.014
[38,     1] loss: 0.014
[39,     1] loss: 0.013
[40,     1] loss: 0.013
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.012
[44,     1] loss: 0.012
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.011
[49,     1] loss: 0.010
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.010
[55,     1] loss: 0.010
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.009
[60,     1] loss: 0.009
Early stopping applied (best metric=0.3863134980201721)
Finished Training
Total time taken: 186.1835470199585
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.657
[4,     1] loss: 0.622
[5,     1] loss: 0.580
[6,     1] loss: 0.530
[7,     1] loss: 0.476
[8,     1] loss: 0.422
[9,     1] loss: 0.361
[10,     1] loss: 0.295
[11,     1] loss: 0.237
[12,     1] loss: 0.212
[13,     1] loss: 0.170
[14,     1] loss: 0.150
[15,     1] loss: 0.114
[16,     1] loss: 0.094
[17,     1] loss: 0.085
[18,     1] loss: 0.066
[19,     1] loss: 0.050
[20,     1] loss: 0.044
[21,     1] loss: 0.041
[22,     1] loss: 0.038
[23,     1] loss: 0.033
[24,     1] loss: 0.026
[25,     1] loss: 0.020
[26,     1] loss: 0.017
[27,     1] loss: 0.014
[28,     1] loss: 0.013
[29,     1] loss: 0.012
[30,     1] loss: 0.011
[31,     1] loss: 0.011
[32,     1] loss: 0.011
[33,     1] loss: 0.011
[34,     1] loss: 0.010
[35,     1] loss: 0.011
[36,     1] loss: 0.011
[37,     1] loss: 0.011
[38,     1] loss: 0.010
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.008
[51,     1] loss: 0.008
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.006
Early stopping applied (best metric=0.4247313141822815)
Finished Training
Total time taken: 180.21335434913635
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.696
[3,     1] loss: 0.681
[4,     1] loss: 0.660
[5,     1] loss: 0.637
[6,     1] loss: 0.609
[7,     1] loss: 0.572
[8,     1] loss: 0.529
[9,     1] loss: 0.485
[10,     1] loss: 0.444
[11,     1] loss: 0.404
[12,     1] loss: 0.376
[13,     1] loss: 0.342
[14,     1] loss: 0.285
[15,     1] loss: 0.276
[16,     1] loss: 0.242
[17,     1] loss: 0.227
[18,     1] loss: 0.160
[19,     1] loss: 0.176
[20,     1] loss: 0.121
[21,     1] loss: 0.125
[22,     1] loss: 0.083
[23,     1] loss: 0.083
[24,     1] loss: 0.061
[25,     1] loss: 0.046
[26,     1] loss: 0.038
[27,     1] loss: 0.032
[28,     1] loss: 0.025
[29,     1] loss: 0.023
[30,     1] loss: 0.019
[31,     1] loss: 0.016
[32,     1] loss: 0.015
[33,     1] loss: 0.013
[34,     1] loss: 0.011
[35,     1] loss: 0.011
[36,     1] loss: 0.010
[37,     1] loss: 0.010
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.009
[41,     1] loss: 0.008
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.009
[47,     1] loss: 0.010
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.007
[58,     1] loss: 0.008
[59,     1] loss: 0.007
[60,     1] loss: 0.006
[61,     1] loss: 0.006
[62,     1] loss: 0.006
[63,     1] loss: 0.006
[64,     1] loss: 0.006
[65,     1] loss: 0.005
Early stopping applied (best metric=0.40389594435691833)
Finished Training
Total time taken: 201.82953000068665
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.679
[3,     1] loss: 0.631
[4,     1] loss: 0.583
[5,     1] loss: 0.542
[6,     1] loss: 0.502
[7,     1] loss: 0.462
[8,     1] loss: 0.405
[9,     1] loss: 0.357
[10,     1] loss: 0.313
[11,     1] loss: 0.273
[12,     1] loss: 0.246
[13,     1] loss: 0.219
[14,     1] loss: 0.197
[15,     1] loss: 0.171
[16,     1] loss: 0.145
[17,     1] loss: 0.135
[18,     1] loss: 0.111
[19,     1] loss: 0.099
[20,     1] loss: 0.086
[21,     1] loss: 0.080
[22,     1] loss: 0.071
[23,     1] loss: 0.064
[24,     1] loss: 0.059
[25,     1] loss: 0.056
[26,     1] loss: 0.058
[27,     1] loss: 0.071
[28,     1] loss: 0.097
[29,     1] loss: 0.063
[30,     1] loss: 0.069
[31,     1] loss: 0.055
[32,     1] loss: 0.063
[33,     1] loss: 0.059
[34,     1] loss: 0.054
[35,     1] loss: 0.053
[36,     1] loss: 0.049
[37,     1] loss: 0.047
[38,     1] loss: 0.043
[39,     1] loss: 0.040
[40,     1] loss: 0.035
[41,     1] loss: 0.031
[42,     1] loss: 0.028
[43,     1] loss: 0.025
[44,     1] loss: 0.024
[45,     1] loss: 0.021
[46,     1] loss: 0.020
[47,     1] loss: 0.018
[48,     1] loss: 0.018
[49,     1] loss: 0.017
[50,     1] loss: 0.016
[51,     1] loss: 0.016
[52,     1] loss: 0.015
[53,     1] loss: 0.015
[54,     1] loss: 0.015
[55,     1] loss: 0.015
[56,     1] loss: 0.014
[57,     1] loss: 0.014
Early stopping applied (best metric=0.4470149278640747)
Finished Training
Total time taken: 177.46279454231262
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.632
[4,     1] loss: 0.591
[5,     1] loss: 0.540
[6,     1] loss: 0.490
[7,     1] loss: 0.435
[8,     1] loss: 0.370
[9,     1] loss: 0.300
[10,     1] loss: 0.237
[11,     1] loss: 0.180
[12,     1] loss: 0.140
[13,     1] loss: 0.134
[14,     1] loss: 0.117
[15,     1] loss: 0.107
[16,     1] loss: 0.073
[17,     1] loss: 0.071
[18,     1] loss: 0.063
[19,     1] loss: 0.060
[20,     1] loss: 0.055
[21,     1] loss: 0.053
[22,     1] loss: 0.046
[23,     1] loss: 0.042
[24,     1] loss: 0.039
[25,     1] loss: 0.035
[26,     1] loss: 0.034
[27,     1] loss: 0.032
[28,     1] loss: 0.030
[29,     1] loss: 0.027
[30,     1] loss: 0.026
[31,     1] loss: 0.022
[32,     1] loss: 0.023
[33,     1] loss: 0.020
[34,     1] loss: 0.016
[35,     1] loss: 0.016
[36,     1] loss: 0.014
[37,     1] loss: 0.013
[38,     1] loss: 0.012
[39,     1] loss: 0.011
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.009
[43,     1] loss: 0.009
[44,     1] loss: 0.008
[45,     1] loss: 0.008
[46,     1] loss: 0.008
[47,     1] loss: 0.008
[48,     1] loss: 0.007
[49,     1] loss: 0.008
[50,     1] loss: 0.007
[51,     1] loss: 0.006
[52,     1] loss: 0.006
[53,     1] loss: 0.006
[54,     1] loss: 0.006
[55,     1] loss: 0.006
[56,     1] loss: 0.005
[57,     1] loss: 0.005
[58,     1] loss: 0.004
[59,     1] loss: 0.004
[60,     1] loss: 0.004
Early stopping applied (best metric=0.36268702149391174)
Finished Training
Total time taken: 186.7987322807312
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.682
[3,     1] loss: 0.640
[4,     1] loss: 0.595
[5,     1] loss: 0.559
[6,     1] loss: 0.521
[7,     1] loss: 0.492
[8,     1] loss: 0.462
[9,     1] loss: 0.435
[10,     1] loss: 0.431
[11,     1] loss: 0.387
[12,     1] loss: 0.401
[13,     1] loss: 0.375
[14,     1] loss: 0.389
[15,     1] loss: 0.364
[16,     1] loss: 0.342
[17,     1] loss: 0.335
[18,     1] loss: 0.337
[19,     1] loss: 0.323
[20,     1] loss: 0.318
[21,     1] loss: 0.317
[22,     1] loss: 0.316
[23,     1] loss: 0.316
[24,     1] loss: 0.317
[25,     1] loss: 0.316
[26,     1] loss: 0.316
[27,     1] loss: 0.315
[28,     1] loss: 0.316
[29,     1] loss: 0.316
[30,     1] loss: 0.316
[31,     1] loss: 0.316
[32,     1] loss: 0.318
[33,     1] loss: 0.318
[34,     1] loss: 0.318
[35,     1] loss: 0.321
[36,     1] loss: 0.414
[37,     1] loss: 0.325
[38,     1] loss: 0.390
[39,     1] loss: 0.332
[40,     1] loss: 0.350
[41,     1] loss: 0.322
[42,     1] loss: 0.351
[43,     1] loss: 0.337
[44,     1] loss: 0.328
[45,     1] loss: 0.324
[46,     1] loss: 0.327
[47,     1] loss: 0.323
[48,     1] loss: 0.321
[49,     1] loss: 0.322
[50,     1] loss: 0.320
[51,     1] loss: 0.322
[52,     1] loss: 0.322
[53,     1] loss: 0.322
[54,     1] loss: 0.323
[55,     1] loss: 0.324
[56,     1] loss: 0.321
[57,     1] loss: 0.321
[58,     1] loss: 0.321
[59,     1] loss: 0.321
[60,     1] loss: 0.322
[61,     1] loss: 0.322
[62,     1] loss: 0.323
[63,     1] loss: 0.321
[64,     1] loss: 0.320
[65,     1] loss: 0.321
[66,     1] loss: 0.320
[67,     1] loss: 0.320
[68,     1] loss: 0.321
[69,     1] loss: 0.324
[70,     1] loss: 0.329
[71,     1] loss: 0.435
[72,     1] loss: 0.467
[73,     1] loss: 0.406
[74,     1] loss: 0.555
[75,     1] loss: 0.516
[76,     1] loss: 0.478
[77,     1] loss: 0.464
[78,     1] loss: 0.471
[79,     1] loss: 0.450
[80,     1] loss: 0.442
[81,     1] loss: 0.436
[82,     1] loss: 0.416
[83,     1] loss: 0.411
[84,     1] loss: 0.398
[85,     1] loss: 0.401
[86,     1] loss: 0.393
[87,     1] loss: 0.399
[88,     1] loss: 0.394
[89,     1] loss: 0.408
[90,     1] loss: 0.485
[91,     1] loss: 0.501
[92,     1] loss: 0.514
[93,     1] loss: 0.489
[94,     1] loss: 0.496
Early stopping applied (best metric=0.42550697922706604)
Finished Training
Total time taken: 292.1573634147644
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.687
[3,     1] loss: 0.661
[4,     1] loss: 0.632
[5,     1] loss: 0.588
[6,     1] loss: 0.533
[7,     1] loss: 0.473
[8,     1] loss: 0.406
[9,     1] loss: 0.343
[10,     1] loss: 0.285
[11,     1] loss: 0.234
[12,     1] loss: 0.185
[13,     1] loss: 0.145
[14,     1] loss: 0.109
[15,     1] loss: 0.117
[16,     1] loss: 0.220
[17,     1] loss: 0.297
[18,     1] loss: 0.160
[19,     1] loss: 0.160
[20,     1] loss: 0.156
[21,     1] loss: 0.140
[22,     1] loss: 0.121
[23,     1] loss: 0.113
[24,     1] loss: 0.114
[25,     1] loss: 0.111
[26,     1] loss: 0.107
[27,     1] loss: 0.096
[28,     1] loss: 0.088
[29,     1] loss: 0.076
[30,     1] loss: 0.065
[31,     1] loss: 0.055
[32,     1] loss: 0.047
[33,     1] loss: 0.039
[34,     1] loss: 0.035
[35,     1] loss: 0.030
[36,     1] loss: 0.027
[37,     1] loss: 0.024
[38,     1] loss: 0.022
[39,     1] loss: 0.020
[40,     1] loss: 0.019
[41,     1] loss: 0.019
[42,     1] loss: 0.019
[43,     1] loss: 0.019
[44,     1] loss: 0.018
[45,     1] loss: 0.020
[46,     1] loss: 0.020
[47,     1] loss: 0.020
[48,     1] loss: 0.020
[49,     1] loss: 0.021
[50,     1] loss: 0.021
[51,     1] loss: 0.021
[52,     1] loss: 0.020
[53,     1] loss: 0.019
[54,     1] loss: 0.020
[55,     1] loss: 0.019
[56,     1] loss: 0.019
[57,     1] loss: 0.017
[58,     1] loss: 0.018
[59,     1] loss: 0.017
[60,     1] loss: 0.016
[61,     1] loss: 0.016
Early stopping applied (best metric=0.40768757462501526)
Finished Training
Total time taken: 190.7650270462036
{'Hydroxylation-P Validation Accuracy': 0.6956035125120552, 'Hydroxylation-P Validation Sensitivity': 0.8634920634920635, 'Hydroxylation-P Validation Specificity': 0.6595181804578782, 'Hydroxylation-P Validation Precision': 0.36427878272069153, 'Hydroxylation-P AUC ROC': 0.8388950107949809, 'Hydroxylation-P AUC PR': 0.571998328105871, 'Hydroxylation-P MCC': 0.41072892667245353, 'Hydroxylation-P F1': 0.5077574380181972, 'Validation Loss (Hydroxylation-P)': 0.4141152346134186, 'Validation Loss (total)': 0.4141152346134186}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007018145219792012,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3750003836,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.280341812459064}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007109028671170954,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1703478049,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.015912474948051}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.674
[3,     1] loss: 0.629
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027209750225540056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2150070097,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.260367166255673}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.659
[5,     1] loss: 0.645
[6,     1] loss: 0.632
[7,     1] loss: 0.618
[8,     1] loss: 0.599
[9,     1] loss: 0.584
[10,     1] loss: 0.568
[11,     1] loss: 0.551
[12,     1] loss: 0.531
[13,     1] loss: 0.511
[14,     1] loss: 0.493
[15,     1] loss: 0.474
[16,     1] loss: 0.458
[17,     1] loss: 0.448
[18,     1] loss: 0.424
[19,     1] loss: 0.414
[20,     1] loss: 0.391
[21,     1] loss: 0.382
[22,     1] loss: 0.366
[23,     1] loss: 0.350
[24,     1] loss: 0.338
[25,     1] loss: 0.326
[26,     1] loss: 0.314
[27,     1] loss: 0.307
[28,     1] loss: 0.297
[29,     1] loss: 0.284
[30,     1] loss: 0.279
[31,     1] loss: 0.268
[32,     1] loss: 0.257
[33,     1] loss: 0.249
[34,     1] loss: 0.239
[35,     1] loss: 0.233
[36,     1] loss: 0.222
[37,     1] loss: 0.211
[38,     1] loss: 0.204
[39,     1] loss: 0.193
[40,     1] loss: 0.185
[41,     1] loss: 0.176
[42,     1] loss: 0.168
[43,     1] loss: 0.160
[44,     1] loss: 0.149
[45,     1] loss: 0.143
[46,     1] loss: 0.134
[47,     1] loss: 0.128
[48,     1] loss: 0.122
[49,     1] loss: 0.116
[50,     1] loss: 0.113
[51,     1] loss: 0.108
[52,     1] loss: 0.104
[53,     1] loss: 0.100
[54,     1] loss: 0.096
[55,     1] loss: 0.094
[56,     1] loss: 0.091
[57,     1] loss: 0.088
[58,     1] loss: 0.084
[59,     1] loss: 0.082
[60,     1] loss: 0.088
[61,     1] loss: 0.310
[62,     1] loss: 0.491
[63,     1] loss: 0.261
[64,     1] loss: 0.393
[65,     1] loss: 0.431
[66,     1] loss: 0.459
[67,     1] loss: 0.457
[68,     1] loss: 0.453
[69,     1] loss: 0.444
[70,     1] loss: 0.436
[71,     1] loss: 0.433
[72,     1] loss: 0.427
[73,     1] loss: 0.421
[74,     1] loss: 0.413
[75,     1] loss: 0.404
[76,     1] loss: 0.393
[77,     1] loss: 0.379
[78,     1] loss: 0.369
[79,     1] loss: 0.352
[80,     1] loss: 0.336
[81,     1] loss: 0.320
[82,     1] loss: 0.303
[83,     1] loss: 0.287
Early stopping applied (best metric=0.31740710139274597)
Finished Training
Total time taken: 259.4628870487213
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.671
[3,     1] loss: 0.646
[4,     1] loss: 0.626
[5,     1] loss: 0.610
[6,     1] loss: 0.592
[7,     1] loss: 0.572
[8,     1] loss: 0.551
[9,     1] loss: 0.531
[10,     1] loss: 0.509
[11,     1] loss: 0.484
[12,     1] loss: 0.462
[13,     1] loss: 0.439
[14,     1] loss: 0.415
[15,     1] loss: 0.393
[16,     1] loss: 0.371
[17,     1] loss: 0.353
[18,     1] loss: 0.340
[19,     1] loss: 0.321
[20,     1] loss: 0.306
[21,     1] loss: 0.295
[22,     1] loss: 0.283
[23,     1] loss: 0.271
[24,     1] loss: 0.262
[25,     1] loss: 0.246
[26,     1] loss: 0.237
[27,     1] loss: 0.230
[28,     1] loss: 0.226
[29,     1] loss: 0.219
[30,     1] loss: 0.213
[31,     1] loss: 0.209
[32,     1] loss: 0.201
[33,     1] loss: 0.195
[34,     1] loss: 0.191
[35,     1] loss: 0.188
[36,     1] loss: 0.183
[37,     1] loss: 0.177
[38,     1] loss: 0.176
[39,     1] loss: 0.175
[40,     1] loss: 0.173
[41,     1] loss: 0.171
[42,     1] loss: 0.168
[43,     1] loss: 0.168
[44,     1] loss: 0.166
[45,     1] loss: 0.164
[46,     1] loss: 0.164
[47,     1] loss: 0.163
[48,     1] loss: 0.161
[49,     1] loss: 0.160
[50,     1] loss: 0.155
[51,     1] loss: 0.155
[52,     1] loss: 0.152
[53,     1] loss: 0.151
[54,     1] loss: 0.149
[55,     1] loss: 0.149
[56,     1] loss: 0.145
[57,     1] loss: 0.187
[58,     1] loss: 0.452
[59,     1] loss: 0.383
[60,     1] loss: 0.592
[61,     1] loss: 0.379
[62,     1] loss: 0.449
[63,     1] loss: 0.472
[64,     1] loss: 0.474
[65,     1] loss: 0.469
[66,     1] loss: 0.469
[67,     1] loss: 0.471
[68,     1] loss: 0.472
[69,     1] loss: 0.475
[70,     1] loss: 0.476
[71,     1] loss: 0.475
[72,     1] loss: 0.469
[73,     1] loss: 0.464
[74,     1] loss: 0.459
Early stopping applied (best metric=0.4110625684261322)
Finished Training
Total time taken: 231.47008609771729
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.672
[4,     1] loss: 0.659
[5,     1] loss: 0.647
[6,     1] loss: 0.633
[7,     1] loss: 0.618
[8,     1] loss: 0.605
[9,     1] loss: 0.589
[10,     1] loss: 0.572
[11,     1] loss: 0.554
[12,     1] loss: 0.536
[13,     1] loss: 0.515
[14,     1] loss: 0.495
[15,     1] loss: 0.476
[16,     1] loss: 0.454
[17,     1] loss: 0.432
[18,     1] loss: 0.413
[19,     1] loss: 0.392
[20,     1] loss: 0.371
[21,     1] loss: 0.350
[22,     1] loss: 0.334
[23,     1] loss: 0.315
[24,     1] loss: 0.296
[25,     1] loss: 0.281
[26,     1] loss: 0.264
[27,     1] loss: 0.252
[28,     1] loss: 0.239
[29,     1] loss: 0.227
[30,     1] loss: 0.214
[31,     1] loss: 0.203
[32,     1] loss: 0.194
[33,     1] loss: 0.184
[34,     1] loss: 0.179
[35,     1] loss: 0.171
[36,     1] loss: 0.167
[37,     1] loss: 0.162
[38,     1] loss: 0.158
[39,     1] loss: 0.153
[40,     1] loss: 0.150
[41,     1] loss: 0.147
[42,     1] loss: 0.145
[43,     1] loss: 0.143
[44,     1] loss: 0.141
[45,     1] loss: 0.140
[46,     1] loss: 0.138
[47,     1] loss: 0.140
[48,     1] loss: 0.136
[49,     1] loss: 0.137
[50,     1] loss: 0.136
[51,     1] loss: 0.137
[52,     1] loss: 0.133
[53,     1] loss: 0.139
[54,     1] loss: 0.210
[55,     1] loss: 0.542
[56,     1] loss: 0.419
[57,     1] loss: 0.361
[58,     1] loss: 0.332
[59,     1] loss: 0.315
[60,     1] loss: 0.304
[61,     1] loss: 0.300
[62,     1] loss: 0.290
[63,     1] loss: 0.283
[64,     1] loss: 0.277
[65,     1] loss: 0.268
[66,     1] loss: 0.260
[67,     1] loss: 0.253
[68,     1] loss: 0.243
[69,     1] loss: 0.236
[70,     1] loss: 0.228
[71,     1] loss: 0.216
[72,     1] loss: 0.208
[73,     1] loss: 0.200
[74,     1] loss: 0.188
[75,     1] loss: 0.183
[76,     1] loss: 0.175
[77,     1] loss: 0.171
[78,     1] loss: 0.162
[79,     1] loss: 0.153
[80,     1] loss: 0.150
[81,     1] loss: 0.144
[82,     1] loss: 0.140
[83,     1] loss: 0.132
[84,     1] loss: 0.128
[85,     1] loss: 0.134
[86,     1] loss: 0.134
[87,     1] loss: 0.164
[88,     1] loss: 0.147
[89,     1] loss: 0.134
[90,     1] loss: 0.137
[91,     1] loss: 0.146
[92,     1] loss: 0.142
[93,     1] loss: 0.133
[94,     1] loss: 0.128
[95,     1] loss: 0.123
[96,     1] loss: 0.120
[97,     1] loss: 0.120
[98,     1] loss: 0.116
[99,     1] loss: 0.114
[100,     1] loss: 0.110
[101,     1] loss: 0.115
[102,     1] loss: 0.113
[103,     1] loss: 0.113
[104,     1] loss: 0.114
[105,     1] loss: 0.113
[106,     1] loss: 0.112
[107,     1] loss: 0.112
[108,     1] loss: 0.112
[109,     1] loss: 0.115
[110,     1] loss: 0.115
[111,     1] loss: 0.118
[112,     1] loss: 0.114
[113,     1] loss: 0.116
[114,     1] loss: 0.115
[115,     1] loss: 0.116
[116,     1] loss: 0.118
[117,     1] loss: 0.119
[118,     1] loss: 0.118
[119,     1] loss: 0.119
[120,     1] loss: 0.117
[121,     1] loss: 0.131
[122,     1] loss: 0.289
[123,     1] loss: 0.366
[124,     1] loss: 1.054
[125,     1] loss: 1.010
[126,     1] loss: 0.903
[127,     1] loss: 0.805
[128,     1] loss: 0.731
[129,     1] loss: 0.698
[130,     1] loss: 0.681
[131,     1] loss: 0.673
[132,     1] loss: 0.672
Early stopping applied (best metric=0.3185412883758545)
Finished Training
Total time taken: 433.40486907958984
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.656
[4,     1] loss: 0.641
[5,     1] loss: 0.626
[6,     1] loss: 0.614
[7,     1] loss: 0.597
[8,     1] loss: 0.584
[9,     1] loss: 0.565
[10,     1] loss: 0.549
[11,     1] loss: 0.529
[12,     1] loss: 0.509
[13,     1] loss: 0.489
[14,     1] loss: 0.468
[15,     1] loss: 0.441
[16,     1] loss: 0.420
[17,     1] loss: 0.401
[18,     1] loss: 0.384
[19,     1] loss: 0.375
[20,     1] loss: 0.349
[21,     1] loss: 0.334
[22,     1] loss: 0.314
[23,     1] loss: 0.304
[24,     1] loss: 0.290
[25,     1] loss: 0.281
[26,     1] loss: 0.269
[27,     1] loss: 0.261
[28,     1] loss: 0.254
[29,     1] loss: 0.250
[30,     1] loss: 0.240
[31,     1] loss: 0.235
[32,     1] loss: 0.229
[33,     1] loss: 0.221
[34,     1] loss: 0.216
[35,     1] loss: 0.211
[36,     1] loss: 0.207
[37,     1] loss: 0.203
[38,     1] loss: 0.200
[39,     1] loss: 0.197
[40,     1] loss: 0.191
[41,     1] loss: 0.191
[42,     1] loss: 0.186
[43,     1] loss: 0.182
[44,     1] loss: 0.181
[45,     1] loss: 0.175
[46,     1] loss: 0.175
[47,     1] loss: 0.170
[48,     1] loss: 0.169
[49,     1] loss: 0.166
[50,     1] loss: 0.164
[51,     1] loss: 0.161
[52,     1] loss: 0.157
[53,     1] loss: 0.156
[54,     1] loss: 0.153
[55,     1] loss: 0.149
[56,     1] loss: 0.145
[57,     1] loss: 0.140
[58,     1] loss: 0.136
[59,     1] loss: 0.194
[60,     1] loss: 0.323
[61,     1] loss: 0.485
[62,     1] loss: 0.420
[63,     1] loss: 0.397
[64,     1] loss: 0.378
[65,     1] loss: 0.367
[66,     1] loss: 0.334
[67,     1] loss: 0.323
[68,     1] loss: 0.311
[69,     1] loss: 0.304
[70,     1] loss: 0.293
[71,     1] loss: 0.283
[72,     1] loss: 0.269
[73,     1] loss: 0.260
[74,     1] loss: 0.247
[75,     1] loss: 0.240
[76,     1] loss: 0.231
[77,     1] loss: 0.223
[78,     1] loss: 0.216
[79,     1] loss: 0.209
[80,     1] loss: 0.201
[81,     1] loss: 0.196
[82,     1] loss: 0.190
[83,     1] loss: 0.184
[84,     1] loss: 0.181
[85,     1] loss: 0.178
[86,     1] loss: 0.174
[87,     1] loss: 0.171
[88,     1] loss: 0.167
[89,     1] loss: 0.165
[90,     1] loss: 0.163
[91,     1] loss: 0.159
[92,     1] loss: 0.157
[93,     1] loss: 0.155
[94,     1] loss: 0.152
[95,     1] loss: 0.150
[96,     1] loss: 0.149
[97,     1] loss: 0.147
[98,     1] loss: 0.146
Early stopping applied (best metric=0.39335861802101135)
Finished Training
Total time taken: 324.6551842689514
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.664
[4,     1] loss: 0.647
[5,     1] loss: 0.633
[6,     1] loss: 0.619
[7,     1] loss: 0.603
[8,     1] loss: 0.586
[9,     1] loss: 0.566
[10,     1] loss: 0.546
[11,     1] loss: 0.526
[12,     1] loss: 0.499
[13,     1] loss: 0.476
[14,     1] loss: 0.452
[15,     1] loss: 0.425
[16,     1] loss: 0.400
[17,     1] loss: 0.378
[18,     1] loss: 0.357
[19,     1] loss: 0.334
[20,     1] loss: 0.314
[21,     1] loss: 0.296
[22,     1] loss: 0.277
[23,     1] loss: 0.263
[24,     1] loss: 0.244
[25,     1] loss: 0.231
[26,     1] loss: 0.217
[27,     1] loss: 0.207
[28,     1] loss: 0.192
[29,     1] loss: 0.182
[30,     1] loss: 0.173
[31,     1] loss: 0.166
[32,     1] loss: 0.160
[33,     1] loss: 0.155
[34,     1] loss: 0.148
[35,     1] loss: 0.143
[36,     1] loss: 0.139
[37,     1] loss: 0.134
[38,     1] loss: 0.131
[39,     1] loss: 0.129
[40,     1] loss: 0.129
[41,     1] loss: 0.126
[42,     1] loss: 0.125
[43,     1] loss: 0.122
[44,     1] loss: 0.122
[45,     1] loss: 0.122
[46,     1] loss: 0.120
[47,     1] loss: 0.119
[48,     1] loss: 0.118
[49,     1] loss: 0.120
[50,     1] loss: 0.121
[51,     1] loss: 0.120
[52,     1] loss: 0.121
[53,     1] loss: 0.132
[54,     1] loss: 0.223
[55,     1] loss: 0.487
[56,     1] loss: 0.464
[57,     1] loss: 0.438
[58,     1] loss: 0.400
[59,     1] loss: 0.375
[60,     1] loss: 0.366
[61,     1] loss: 0.354
[62,     1] loss: 0.342
[63,     1] loss: 0.327
[64,     1] loss: 0.307
[65,     1] loss: 0.293
[66,     1] loss: 0.271
[67,     1] loss: 0.258
[68,     1] loss: 0.244
[69,     1] loss: 0.230
[70,     1] loss: 0.216
[71,     1] loss: 0.203
[72,     1] loss: 0.192
[73,     1] loss: 0.178
[74,     1] loss: 0.171
[75,     1] loss: 0.161
[76,     1] loss: 0.150
[77,     1] loss: 0.142
[78,     1] loss: 0.135
[79,     1] loss: 0.130
[80,     1] loss: 0.123
[81,     1] loss: 0.120
[82,     1] loss: 0.115
[83,     1] loss: 0.112
[84,     1] loss: 0.109
[85,     1] loss: 0.108
[86,     1] loss: 0.105
[87,     1] loss: 0.106
[88,     1] loss: 0.105
[89,     1] loss: 0.103
[90,     1] loss: 0.105
[91,     1] loss: 0.103
[92,     1] loss: 0.103
[93,     1] loss: 0.105
[94,     1] loss: 0.104
[95,     1] loss: 0.106
[96,     1] loss: 0.104
[97,     1] loss: 0.106
[98,     1] loss: 0.104
[99,     1] loss: 0.115
[100,     1] loss: 0.223
[101,     1] loss: 0.395
[102,     1] loss: 0.473
[103,     1] loss: 0.426
[104,     1] loss: 0.384
[105,     1] loss: 0.357
[106,     1] loss: 0.348
[107,     1] loss: 0.335
[108,     1] loss: 0.330
[109,     1] loss: 0.316
[110,     1] loss: 0.298
[111,     1] loss: 0.290
[112,     1] loss: 0.281
[113,     1] loss: 0.274
[114,     1] loss: 0.264
[115,     1] loss: 0.257
[116,     1] loss: 0.251
[117,     1] loss: 0.243
[118,     1] loss: 0.233
[119,     1] loss: 0.223
[120,     1] loss: 0.221
[121,     1] loss: 0.220
[122,     1] loss: 0.206
[123,     1] loss: 0.203
[124,     1] loss: 0.197
[125,     1] loss: 0.192
[126,     1] loss: 0.185
[127,     1] loss: 0.183
[128,     1] loss: 0.177
[129,     1] loss: 0.173
[130,     1] loss: 0.173
[131,     1] loss: 0.168
[132,     1] loss: 0.168
[133,     1] loss: 0.164
[134,     1] loss: 0.161
Early stopping applied (best metric=0.3151315450668335)
Finished Training
Total time taken: 428.1103632450104
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.684
[3,     1] loss: 0.662
[4,     1] loss: 0.646
[5,     1] loss: 0.631
[6,     1] loss: 0.617
[7,     1] loss: 0.600
[8,     1] loss: 0.581
[9,     1] loss: 0.562
[10,     1] loss: 0.547
[11,     1] loss: 0.524
[12,     1] loss: 0.504
[13,     1] loss: 0.484
[14,     1] loss: 0.471
[15,     1] loss: 0.449
[16,     1] loss: 0.428
[17,     1] loss: 0.410
[18,     1] loss: 0.395
[19,     1] loss: 0.390
[20,     1] loss: 0.391
[21,     1] loss: 0.403
[22,     1] loss: 0.367
[23,     1] loss: 0.380
[24,     1] loss: 0.362
[25,     1] loss: 0.367
[26,     1] loss: 0.358
[27,     1] loss: 0.354
[28,     1] loss: 0.355
[29,     1] loss: 0.354
[30,     1] loss: 0.352
[31,     1] loss: 0.351
[32,     1] loss: 0.351
[33,     1] loss: 0.351
[34,     1] loss: 0.350
[35,     1] loss: 0.350
[36,     1] loss: 0.350
[37,     1] loss: 0.350
[38,     1] loss: 0.350
[39,     1] loss: 0.349
[40,     1] loss: 0.349
[41,     1] loss: 0.349
[42,     1] loss: 0.349
[43,     1] loss: 0.350
[44,     1] loss: 0.348
[45,     1] loss: 0.348
[46,     1] loss: 0.347
[47,     1] loss: 0.346
[48,     1] loss: 0.345
[49,     1] loss: 0.344
[50,     1] loss: 0.367
[51,     1] loss: 0.447
[52,     1] loss: 0.370
[53,     1] loss: 0.430
[54,     1] loss: 0.437
[55,     1] loss: 0.415
[56,     1] loss: 0.405
[57,     1] loss: 0.382
[58,     1] loss: 0.373
[59,     1] loss: 0.359
[60,     1] loss: 0.352
[61,     1] loss: 0.341
[62,     1] loss: 0.334
[63,     1] loss: 0.327
[64,     1] loss: 0.323
[65,     1] loss: 0.318
[66,     1] loss: 0.315
[67,     1] loss: 0.309
[68,     1] loss: 0.306
[69,     1] loss: 0.302
[70,     1] loss: 0.296
[71,     1] loss: 0.292
[72,     1] loss: 0.285
[73,     1] loss: 0.279
[74,     1] loss: 0.276
[75,     1] loss: 0.269
[76,     1] loss: 0.264
[77,     1] loss: 0.256
[78,     1] loss: 0.249
[79,     1] loss: 0.242
[80,     1] loss: 0.234
[81,     1] loss: 0.224
[82,     1] loss: 0.215
[83,     1] loss: 0.207
[84,     1] loss: 0.197
[85,     1] loss: 0.187
[86,     1] loss: 0.177
[87,     1] loss: 0.168
[88,     1] loss: 0.159
[89,     1] loss: 0.151
[90,     1] loss: 0.145
[91,     1] loss: 0.139
[92,     1] loss: 0.133
[93,     1] loss: 0.350
[94,     1] loss: 0.500
[95,     1] loss: 0.448
[96,     1] loss: 0.325
[97,     1] loss: 0.295
[98,     1] loss: 0.292
[99,     1] loss: 0.308
[100,     1] loss: 0.304
[101,     1] loss: 0.303
[102,     1] loss: 0.317
[103,     1] loss: 0.276
[104,     1] loss: 0.335
[105,     1] loss: 0.270
[106,     1] loss: 0.328
[107,     1] loss: 0.268
[108,     1] loss: 0.249
[109,     1] loss: 0.267
[110,     1] loss: 0.263
[111,     1] loss: 0.239
[112,     1] loss: 0.235
[113,     1] loss: 0.236
[114,     1] loss: 0.224
[115,     1] loss: 0.214
[116,     1] loss: 0.230
[117,     1] loss: 0.213
[118,     1] loss: 0.204
[119,     1] loss: 0.201
[120,     1] loss: 0.201
[121,     1] loss: 0.198
[122,     1] loss: 0.193
[123,     1] loss: 0.191
[124,     1] loss: 0.191
[125,     1] loss: 0.187
[126,     1] loss: 0.187
[127,     1] loss: 0.185
[128,     1] loss: 0.184
[129,     1] loss: 0.180
[130,     1] loss: 0.179
[131,     1] loss: 0.176
[132,     1] loss: 0.175
[133,     1] loss: 0.175
[134,     1] loss: 0.176
[135,     1] loss: 0.173
[136,     1] loss: 0.171
[137,     1] loss: 0.174
[138,     1] loss: 0.173
[139,     1] loss: 0.170
[140,     1] loss: 0.172
[141,     1] loss: 0.171
[142,     1] loss: 0.171
[143,     1] loss: 0.171
[144,     1] loss: 0.171
[145,     1] loss: 0.170
[146,     1] loss: 0.168
[147,     1] loss: 0.167
[148,     1] loss: 0.167
[149,     1] loss: 0.170
[150,     1] loss: 0.168
[151,     1] loss: 0.167
[152,     1] loss: 0.168
[153,     1] loss: 0.167
[154,     1] loss: 0.164
[155,     1] loss: 0.166
[156,     1] loss: 0.165
[157,     1] loss: 0.167
[158,     1] loss: 0.166
[159,     1] loss: 0.165
[160,     1] loss: 0.165
[161,     1] loss: 0.166
[162,     1] loss: 0.172
[163,     1] loss: 0.335
[164,     1] loss: 0.622
[165,     1] loss: 0.473
[166,     1] loss: 0.446
[167,     1] loss: 0.400
[168,     1] loss: 0.409
Early stopping applied (best metric=0.37336885929107666)
Finished Training
Total time taken: 547.3780291080475
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.667
[3,     1] loss: 0.642
[4,     1] loss: 0.623
[5,     1] loss: 0.606
[6,     1] loss: 0.587
[7,     1] loss: 0.568
[8,     1] loss: 0.548
[9,     1] loss: 0.527
[10,     1] loss: 0.504
[11,     1] loss: 0.480
[12,     1] loss: 0.457
[13,     1] loss: 0.433
[14,     1] loss: 0.414
[15,     1] loss: 0.391
[16,     1] loss: 0.374
[17,     1] loss: 0.353
[18,     1] loss: 0.337
[19,     1] loss: 0.320
[20,     1] loss: 0.306
[21,     1] loss: 0.294
[22,     1] loss: 0.281
[23,     1] loss: 0.270
[24,     1] loss: 0.258
[25,     1] loss: 0.249
[26,     1] loss: 0.240
[27,     1] loss: 0.228
[28,     1] loss: 0.222
[29,     1] loss: 0.214
[30,     1] loss: 0.209
[31,     1] loss: 0.201
[32,     1] loss: 0.196
[33,     1] loss: 0.192
[34,     1] loss: 0.186
[35,     1] loss: 0.185
[36,     1] loss: 0.180
[37,     1] loss: 0.183
[38,     1] loss: 0.176
[39,     1] loss: 0.172
[40,     1] loss: 0.171
[41,     1] loss: 0.169
[42,     1] loss: 0.167
[43,     1] loss: 0.165
[44,     1] loss: 0.165
[45,     1] loss: 0.161
[46,     1] loss: 0.163
[47,     1] loss: 0.162
[48,     1] loss: 0.161
[49,     1] loss: 0.158
[50,     1] loss: 0.157
[51,     1] loss: 0.156
[52,     1] loss: 0.155
[53,     1] loss: 0.149
[54,     1] loss: 0.142
[55,     1] loss: 0.157
[56,     1] loss: 0.356
[57,     1] loss: 0.567
[58,     1] loss: 0.496
[59,     1] loss: 0.433
[60,     1] loss: 0.487
[61,     1] loss: 0.410
[62,     1] loss: 0.445
[63,     1] loss: 0.451
[64,     1] loss: 0.434
[65,     1] loss: 0.430
[66,     1] loss: 0.436
[67,     1] loss: 0.425
[68,     1] loss: 0.407
[69,     1] loss: 0.399
[70,     1] loss: 0.388
[71,     1] loss: 0.373
[72,     1] loss: 0.362
[73,     1] loss: 0.355
[74,     1] loss: 0.341
[75,     1] loss: 0.329
[76,     1] loss: 0.320
[77,     1] loss: 0.306
[78,     1] loss: 0.292
[79,     1] loss: 0.280
[80,     1] loss: 0.270
[81,     1] loss: 0.257
[82,     1] loss: 0.246
[83,     1] loss: 0.231
[84,     1] loss: 0.226
[85,     1] loss: 0.212
[86,     1] loss: 0.204
[87,     1] loss: 0.196
[88,     1] loss: 0.189
[89,     1] loss: 0.185
[90,     1] loss: 0.209
[91,     1] loss: 0.200
[92,     1] loss: 0.186
[93,     1] loss: 0.181
[94,     1] loss: 0.174
[95,     1] loss: 0.172
[96,     1] loss: 0.166
[97,     1] loss: 0.163
[98,     1] loss: 0.162
[99,     1] loss: 0.157
[100,     1] loss: 0.153
[101,     1] loss: 0.149
[102,     1] loss: 0.147
[103,     1] loss: 0.145
[104,     1] loss: 0.143
[105,     1] loss: 0.143
[106,     1] loss: 0.140
[107,     1] loss: 0.137
[108,     1] loss: 0.139
[109,     1] loss: 0.137
[110,     1] loss: 0.139
[111,     1] loss: 0.134
[112,     1] loss: 0.136
[113,     1] loss: 0.137
[114,     1] loss: 0.136
[115,     1] loss: 0.135
[116,     1] loss: 0.134
[117,     1] loss: 0.133
[118,     1] loss: 0.136
[119,     1] loss: 0.137
[120,     1] loss: 0.137
[121,     1] loss: 0.135
[122,     1] loss: 0.134
[123,     1] loss: 0.138
[124,     1] loss: 0.137
[125,     1] loss: 0.158
[126,     1] loss: 0.390
[127,     1] loss: 0.484
[128,     1] loss: 0.891
[129,     1] loss: 0.884
[130,     1] loss: 0.832
[131,     1] loss: 0.775
[132,     1] loss: 0.729
[133,     1] loss: 0.699
[134,     1] loss: 0.687
[135,     1] loss: 0.680
[136,     1] loss: 0.680
[137,     1] loss: 0.681
[138,     1] loss: 0.682
[139,     1] loss: 0.684
[140,     1] loss: 0.686
[141,     1] loss: 0.687
[142,     1] loss: 0.688
[143,     1] loss: 0.689
[144,     1] loss: 0.689
[145,     1] loss: 0.690
[146,     1] loss: 0.690
[147,     1] loss: 0.691
[148,     1] loss: 0.691
[149,     1] loss: 0.691
[150,     1] loss: 0.691
[151,     1] loss: 0.692
[152,     1] loss: 0.692
[153,     1] loss: 0.692
[154,     1] loss: 0.692
[155,     1] loss: 0.692
[156,     1] loss: 0.692
[157,     1] loss: 0.692
[158,     1] loss: 0.693
[159,     1] loss: 0.693
[160,     1] loss: 0.693
[161,     1] loss: 0.693
[162,     1] loss: 0.693
[163,     1] loss: 0.693
[164,     1] loss: 0.693
Early stopping applied (best metric=0.37919551134109497)
Finished Training
Total time taken: 598.4243838787079
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.695
[3,     1] loss: 0.674
[4,     1] loss: 0.658
[5,     1] loss: 0.642
[6,     1] loss: 0.622
[7,     1] loss: 0.607
[8,     1] loss: 0.586
[9,     1] loss: 0.567
[10,     1] loss: 0.545
[11,     1] loss: 0.524
[12,     1] loss: 0.499
[13,     1] loss: 0.474
[14,     1] loss: 0.450
[15,     1] loss: 0.428
[16,     1] loss: 0.403
[17,     1] loss: 0.382
[18,     1] loss: 0.357
[19,     1] loss: 0.335
[20,     1] loss: 0.314
[21,     1] loss: 0.292
[22,     1] loss: 0.279
[23,     1] loss: 0.261
[24,     1] loss: 0.247
[25,     1] loss: 0.230
[26,     1] loss: 0.220
[27,     1] loss: 0.213
[28,     1] loss: 0.199
[29,     1] loss: 0.199
[30,     1] loss: 0.185
[31,     1] loss: 0.177
[32,     1] loss: 0.170
[33,     1] loss: 0.169
[34,     1] loss: 0.161
[35,     1] loss: 0.157
[36,     1] loss: 0.153
[37,     1] loss: 0.151
[38,     1] loss: 0.146
[39,     1] loss: 0.143
[40,     1] loss: 0.142
[41,     1] loss: 0.141
[42,     1] loss: 0.139
[43,     1] loss: 0.139
[44,     1] loss: 0.136
[45,     1] loss: 0.137
[46,     1] loss: 0.134
[47,     1] loss: 0.135
[48,     1] loss: 0.135
[49,     1] loss: 0.131
[50,     1] loss: 0.132
[51,     1] loss: 0.131
[52,     1] loss: 0.132
[53,     1] loss: 0.131
[54,     1] loss: 0.129
[55,     1] loss: 0.141
[56,     1] loss: 0.226
[57,     1] loss: 0.419
[58,     1] loss: 0.281
[59,     1] loss: 0.293
[60,     1] loss: 0.277
[61,     1] loss: 0.275
[62,     1] loss: 0.275
[63,     1] loss: 0.257
[64,     1] loss: 0.256
[65,     1] loss: 0.250
[66,     1] loss: 0.244
[67,     1] loss: 0.237
[68,     1] loss: 0.231
[69,     1] loss: 0.223
[70,     1] loss: 0.216
[71,     1] loss: 0.211
[72,     1] loss: 0.205
[73,     1] loss: 0.197
[74,     1] loss: 0.190
[75,     1] loss: 0.186
[76,     1] loss: 0.185
[77,     1] loss: 0.177
[78,     1] loss: 0.171
[79,     1] loss: 0.167
[80,     1] loss: 0.160
[81,     1] loss: 0.156
[82,     1] loss: 0.150
[83,     1] loss: 0.149
[84,     1] loss: 0.152
[85,     1] loss: 0.186
[86,     1] loss: 0.149
[87,     1] loss: 0.190
[88,     1] loss: 0.187
[89,     1] loss: 0.201
[90,     1] loss: 0.181
[91,     1] loss: 0.183
[92,     1] loss: 0.182
[93,     1] loss: 0.180
[94,     1] loss: 0.171
[95,     1] loss: 0.170
[96,     1] loss: 0.162
[97,     1] loss: 0.156
[98,     1] loss: 0.149
[99,     1] loss: 0.144
[100,     1] loss: 0.141
[101,     1] loss: 0.145
[102,     1] loss: 0.146
[103,     1] loss: 0.150
[104,     1] loss: 0.151
[105,     1] loss: 0.164
[106,     1] loss: 0.146
[107,     1] loss: 0.148
[108,     1] loss: 0.145
[109,     1] loss: 0.143
[110,     1] loss: 0.145
[111,     1] loss: 0.143
[112,     1] loss: 0.144
[113,     1] loss: 0.140
Early stopping applied (best metric=0.3839585781097412)
Finished Training
Total time taken: 416.74877190589905
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.661
[5,     1] loss: 0.645
[6,     1] loss: 0.630
[7,     1] loss: 0.618
[8,     1] loss: 0.600
[9,     1] loss: 0.582
[10,     1] loss: 0.564
[11,     1] loss: 0.544
[12,     1] loss: 0.520
[13,     1] loss: 0.500
[14,     1] loss: 0.476
[15,     1] loss: 0.456
[16,     1] loss: 0.435
[17,     1] loss: 0.419
[18,     1] loss: 0.402
[19,     1] loss: 0.390
[20,     1] loss: 0.394
[21,     1] loss: 0.375
[22,     1] loss: 0.358
[23,     1] loss: 0.347
[24,     1] loss: 0.339
[25,     1] loss: 0.333
[26,     1] loss: 0.325
[27,     1] loss: 0.319
[28,     1] loss: 0.315
[29,     1] loss: 0.309
[30,     1] loss: 0.303
[31,     1] loss: 0.299
[32,     1] loss: 0.296
[33,     1] loss: 0.291
[34,     1] loss: 0.286
[35,     1] loss: 0.282
[36,     1] loss: 0.277
[37,     1] loss: 0.272
[38,     1] loss: 0.267
[39,     1] loss: 0.266
[40,     1] loss: 0.261
[41,     1] loss: 0.259
[42,     1] loss: 0.255
[43,     1] loss: 0.253
[44,     1] loss: 0.249
[45,     1] loss: 0.246
[46,     1] loss: 0.243
[47,     1] loss: 0.239
[48,     1] loss: 0.233
[49,     1] loss: 0.233
[50,     1] loss: 0.232
[51,     1] loss: 0.226
[52,     1] loss: 0.224
[53,     1] loss: 0.220
[54,     1] loss: 0.218
[55,     1] loss: 0.212
[56,     1] loss: 0.209
[57,     1] loss: 0.205
[58,     1] loss: 0.201
[59,     1] loss: 0.198
[60,     1] loss: 0.201
[61,     1] loss: 0.326
[62,     1] loss: 0.408
[63,     1] loss: 0.478
[64,     1] loss: 0.395
[65,     1] loss: 0.372
[66,     1] loss: 0.384
[67,     1] loss: 0.345
[68,     1] loss: 0.340
[69,     1] loss: 0.339
[70,     1] loss: 0.337
[71,     1] loss: 0.329
[72,     1] loss: 0.320
[73,     1] loss: 0.312
[74,     1] loss: 0.302
[75,     1] loss: 0.291
[76,     1] loss: 0.281
[77,     1] loss: 0.272
[78,     1] loss: 0.266
[79,     1] loss: 0.258
[80,     1] loss: 0.251
[81,     1] loss: 0.248
[82,     1] loss: 0.242
[83,     1] loss: 0.241
[84,     1] loss: 0.236
[85,     1] loss: 0.236
[86,     1] loss: 0.232
[87,     1] loss: 0.230
[88,     1] loss: 0.228
[89,     1] loss: 0.229
[90,     1] loss: 0.227
[91,     1] loss: 0.223
[92,     1] loss: 0.226
[93,     1] loss: 0.225
[94,     1] loss: 0.223
[95,     1] loss: 0.221
[96,     1] loss: 0.221
[97,     1] loss: 0.220
[98,     1] loss: 0.219
[99,     1] loss: 0.217
[100,     1] loss: 0.217
[101,     1] loss: 0.217
[102,     1] loss: 0.214
[103,     1] loss: 0.213
[104,     1] loss: 0.225
[105,     1] loss: 0.336
[106,     1] loss: 0.428
[107,     1] loss: 0.464
[108,     1] loss: 0.470
[109,     1] loss: 0.443
[110,     1] loss: 0.411
[111,     1] loss: 0.403
[112,     1] loss: 0.388
[113,     1] loss: 0.376
[114,     1] loss: 0.364
[115,     1] loss: 0.356
[116,     1] loss: 0.354
[117,     1] loss: 0.334
[118,     1] loss: 0.325
[119,     1] loss: 0.314
[120,     1] loss: 0.303
[121,     1] loss: 0.301
[122,     1] loss: 0.296
[123,     1] loss: 0.290
[124,     1] loss: 0.283
[125,     1] loss: 0.276
[126,     1] loss: 0.273
[127,     1] loss: 0.268
[128,     1] loss: 0.264
[129,     1] loss: 0.261
[130,     1] loss: 0.260
[131,     1] loss: 0.256
[132,     1] loss: 0.257
[133,     1] loss: 0.253
[134,     1] loss: 0.254
[135,     1] loss: 0.252
[136,     1] loss: 0.254
Early stopping applied (best metric=0.3664892613887787)
Finished Training
Total time taken: 504.07294368743896
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.694
[3,     1] loss: 0.675
[4,     1] loss: 0.663
[5,     1] loss: 0.651
[6,     1] loss: 0.639
[7,     1] loss: 0.628
[8,     1] loss: 0.615
[9,     1] loss: 0.599
[10,     1] loss: 0.585
[11,     1] loss: 0.571
[12,     1] loss: 0.554
[13,     1] loss: 0.536
[14,     1] loss: 0.518
[15,     1] loss: 0.498
[16,     1] loss: 0.476
[17,     1] loss: 0.455
[18,     1] loss: 0.436
[19,     1] loss: 0.415
[20,     1] loss: 0.401
[21,     1] loss: 0.387
[22,     1] loss: 0.369
[23,     1] loss: 0.352
[24,     1] loss: 0.338
[25,     1] loss: 0.320
[26,     1] loss: 0.310
[27,     1] loss: 0.293
[28,     1] loss: 0.283
[29,     1] loss: 0.273
[30,     1] loss: 0.264
[31,     1] loss: 0.257
[32,     1] loss: 0.249
[33,     1] loss: 0.241
[34,     1] loss: 0.233
[35,     1] loss: 0.227
[36,     1] loss: 0.221
[37,     1] loss: 0.215
[38,     1] loss: 0.210
[39,     1] loss: 0.205
[40,     1] loss: 0.202
[41,     1] loss: 0.198
[42,     1] loss: 0.194
[43,     1] loss: 0.191
[44,     1] loss: 0.187
[45,     1] loss: 0.181
[46,     1] loss: 0.180
[47,     1] loss: 0.176
[48,     1] loss: 0.173
[49,     1] loss: 0.171
[50,     1] loss: 0.167
[51,     1] loss: 0.162
[52,     1] loss: 0.161
[53,     1] loss: 0.159
[54,     1] loss: 0.151
[55,     1] loss: 0.149
[56,     1] loss: 0.145
[57,     1] loss: 0.143
[58,     1] loss: 0.140
[59,     1] loss: 0.138
[60,     1] loss: 0.334
[61,     1] loss: 0.452
[62,     1] loss: 0.363
[63,     1] loss: 0.333
[64,     1] loss: 0.422
[65,     1] loss: 0.369
[66,     1] loss: 0.373
[67,     1] loss: 0.362
[68,     1] loss: 0.352
[69,     1] loss: 0.348
[70,     1] loss: 0.345
[71,     1] loss: 0.336
[72,     1] loss: 0.328
[73,     1] loss: 0.316
[74,     1] loss: 0.307
[75,     1] loss: 0.296
[76,     1] loss: 0.287
[77,     1] loss: 0.277
[78,     1] loss: 0.266
[79,     1] loss: 0.257
[80,     1] loss: 0.248
[81,     1] loss: 0.240
[82,     1] loss: 0.232
[83,     1] loss: 0.223
[84,     1] loss: 0.219
[85,     1] loss: 0.215
[86,     1] loss: 0.210
[87,     1] loss: 0.206
[88,     1] loss: 0.201
[89,     1] loss: 0.197
[90,     1] loss: 0.194
[91,     1] loss: 0.191
[92,     1] loss: 0.188
[93,     1] loss: 0.185
[94,     1] loss: 0.181
[95,     1] loss: 0.181
[96,     1] loss: 0.179
[97,     1] loss: 0.176
[98,     1] loss: 0.175
[99,     1] loss: 0.174
[100,     1] loss: 0.169
[101,     1] loss: 0.170
[102,     1] loss: 0.169
[103,     1] loss: 0.168
[104,     1] loss: 0.166
[105,     1] loss: 0.164
[106,     1] loss: 0.158
[107,     1] loss: 0.161
[108,     1] loss: 0.159
[109,     1] loss: 0.155
[110,     1] loss: 0.157
[111,     1] loss: 0.155
Early stopping applied (best metric=0.2905440330505371)
Finished Training
Total time taken: 412.0384588241577
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.652
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.607
[8,     1] loss: 0.589
[9,     1] loss: 0.569
[10,     1] loss: 0.550
[11,     1] loss: 0.528
[12,     1] loss: 0.505
[13,     1] loss: 0.481
[14,     1] loss: 0.457
[15,     1] loss: 0.434
[16,     1] loss: 0.411
[17,     1] loss: 0.389
[18,     1] loss: 0.368
[19,     1] loss: 0.343
[20,     1] loss: 0.324
[21,     1] loss: 0.305
[22,     1] loss: 0.283
[23,     1] loss: 0.264
[24,     1] loss: 0.249
[25,     1] loss: 0.236
[26,     1] loss: 0.219
[27,     1] loss: 0.211
[28,     1] loss: 0.198
[29,     1] loss: 0.187
[30,     1] loss: 0.180
[31,     1] loss: 0.173
[32,     1] loss: 0.165
[33,     1] loss: 0.159
[34,     1] loss: 0.154
[35,     1] loss: 0.149
[36,     1] loss: 0.146
[37,     1] loss: 0.141
[38,     1] loss: 0.140
[39,     1] loss: 0.136
[40,     1] loss: 0.137
[41,     1] loss: 0.134
[42,     1] loss: 0.134
[43,     1] loss: 0.131
[44,     1] loss: 0.132
[45,     1] loss: 0.130
[46,     1] loss: 0.129
[47,     1] loss: 0.128
[48,     1] loss: 0.129
[49,     1] loss: 0.130
[50,     1] loss: 0.129
[51,     1] loss: 0.126
[52,     1] loss: 0.125
[53,     1] loss: 0.126
[54,     1] loss: 0.122
[55,     1] loss: 0.126
[56,     1] loss: 0.306
[57,     1] loss: 0.429
[58,     1] loss: 0.349
[59,     1] loss: 0.303
[60,     1] loss: 0.334
[61,     1] loss: 0.314
Early stopping applied (best metric=0.4783807694911957)
Finished Training
Total time taken: 236.2590548992157
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.688
[3,     1] loss: 0.672
[4,     1] loss: 0.659
[5,     1] loss: 0.643
[6,     1] loss: 0.627
[7,     1] loss: 0.612
[8,     1] loss: 0.591
[9,     1] loss: 0.568
[10,     1] loss: 0.548
[11,     1] loss: 0.522
[12,     1] loss: 0.499
[13,     1] loss: 0.471
[14,     1] loss: 0.446
[15,     1] loss: 0.422
[16,     1] loss: 0.396
[17,     1] loss: 0.370
[18,     1] loss: 0.348
[19,     1] loss: 0.326
[20,     1] loss: 0.303
[21,     1] loss: 0.287
[22,     1] loss: 0.269
[23,     1] loss: 0.250
[24,     1] loss: 0.235
[25,     1] loss: 0.224
[26,     1] loss: 0.209
[27,     1] loss: 0.196
[28,     1] loss: 0.191
[29,     1] loss: 0.175
[30,     1] loss: 0.170
[31,     1] loss: 0.164
[32,     1] loss: 0.156
[33,     1] loss: 0.150
[34,     1] loss: 0.145
[35,     1] loss: 0.143
[36,     1] loss: 0.138
[37,     1] loss: 0.137
[38,     1] loss: 0.134
[39,     1] loss: 0.132
[40,     1] loss: 0.130
[41,     1] loss: 0.127
[42,     1] loss: 0.126
[43,     1] loss: 0.126
[44,     1] loss: 0.126
[45,     1] loss: 0.126
[46,     1] loss: 0.126
[47,     1] loss: 0.124
[48,     1] loss: 0.123
[49,     1] loss: 0.124
[50,     1] loss: 0.123
[51,     1] loss: 0.120
[52,     1] loss: 0.121
[53,     1] loss: 0.121
[54,     1] loss: 0.117
[55,     1] loss: 0.116
[56,     1] loss: 0.128
[57,     1] loss: 0.305
[58,     1] loss: 0.631
[59,     1] loss: 0.500
[60,     1] loss: 0.487
[61,     1] loss: 0.463
[62,     1] loss: 0.451
[63,     1] loss: 0.457
[64,     1] loss: 0.458
[65,     1] loss: 0.461
[66,     1] loss: 0.461
[67,     1] loss: 0.452
[68,     1] loss: 0.449
[69,     1] loss: 0.441
[70,     1] loss: 0.430
[71,     1] loss: 0.417
[72,     1] loss: 0.404
[73,     1] loss: 0.393
[74,     1] loss: 0.380
[75,     1] loss: 0.368
[76,     1] loss: 0.357
[77,     1] loss: 0.341
[78,     1] loss: 0.328
[79,     1] loss: 0.315
[80,     1] loss: 0.300
[81,     1] loss: 0.284
[82,     1] loss: 0.267
[83,     1] loss: 0.252
[84,     1] loss: 0.233
[85,     1] loss: 0.218
[86,     1] loss: 0.202
[87,     1] loss: 0.191
[88,     1] loss: 0.183
[89,     1] loss: 0.217
[90,     1] loss: 0.164
[91,     1] loss: 0.265
[92,     1] loss: 0.222
[93,     1] loss: 0.228
[94,     1] loss: 0.222
[95,     1] loss: 0.206
[96,     1] loss: 0.205
[97,     1] loss: 0.185
[98,     1] loss: 0.181
[99,     1] loss: 0.177
[100,     1] loss: 0.170
[101,     1] loss: 0.161
[102,     1] loss: 0.158
[103,     1] loss: 0.151
[104,     1] loss: 0.144
[105,     1] loss: 0.138
[106,     1] loss: 0.136
[107,     1] loss: 0.128
[108,     1] loss: 0.127
[109,     1] loss: 0.124
[110,     1] loss: 0.122
[111,     1] loss: 0.117
[112,     1] loss: 0.118
[113,     1] loss: 0.119
[114,     1] loss: 0.118
[115,     1] loss: 0.119
[116,     1] loss: 0.117
[117,     1] loss: 0.123
[118,     1] loss: 0.152
[119,     1] loss: 0.278
[120,     1] loss: 0.210
[121,     1] loss: 0.376
[122,     1] loss: 0.268
[123,     1] loss: 0.300
[124,     1] loss: 0.295
[125,     1] loss: 0.252
[126,     1] loss: 0.233
[127,     1] loss: 0.229
[128,     1] loss: 0.214
[129,     1] loss: 0.209
[130,     1] loss: 0.206
[131,     1] loss: 0.199
[132,     1] loss: 0.191
[133,     1] loss: 0.188
[134,     1] loss: 0.182
[135,     1] loss: 0.172
[136,     1] loss: 0.172
[137,     1] loss: 0.162
[138,     1] loss: 0.160
[139,     1] loss: 0.155
Early stopping applied (best metric=0.38862183690071106)
Finished Training
Total time taken: 530.4688341617584
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.661
[4,     1] loss: 0.643
[5,     1] loss: 0.630
[6,     1] loss: 0.615
[7,     1] loss: 0.600
[8,     1] loss: 0.585
[9,     1] loss: 0.565
[10,     1] loss: 0.550
[11,     1] loss: 0.532
[12,     1] loss: 0.513
[13,     1] loss: 0.493
[14,     1] loss: 0.474
[15,     1] loss: 0.453
[16,     1] loss: 0.439
[17,     1] loss: 0.424
[18,     1] loss: 0.405
[19,     1] loss: 0.390
[20,     1] loss: 0.381
[21,     1] loss: 0.369
[22,     1] loss: 0.354
[23,     1] loss: 0.344
[24,     1] loss: 0.334
[25,     1] loss: 0.325
[26,     1] loss: 0.317
[27,     1] loss: 0.306
[28,     1] loss: 0.299
[29,     1] loss: 0.289
[30,     1] loss: 0.283
[31,     1] loss: 0.276
[32,     1] loss: 0.269
[33,     1] loss: 0.264
[34,     1] loss: 0.254
[35,     1] loss: 0.248
[36,     1] loss: 0.237
[37,     1] loss: 0.232
[38,     1] loss: 0.224
[39,     1] loss: 0.218
[40,     1] loss: 0.208
[41,     1] loss: 0.202
[42,     1] loss: 0.197
[43,     1] loss: 0.190
[44,     1] loss: 0.184
[45,     1] loss: 0.180
[46,     1] loss: 0.175
[47,     1] loss: 0.170
[48,     1] loss: 0.166
[49,     1] loss: 0.159
[50,     1] loss: 0.156
[51,     1] loss: 0.152
[52,     1] loss: 0.148
[53,     1] loss: 0.145
[54,     1] loss: 0.140
[55,     1] loss: 0.137
[56,     1] loss: 0.134
[57,     1] loss: 0.130
[58,     1] loss: 0.128
[59,     1] loss: 0.122
[60,     1] loss: 0.122
[61,     1] loss: 0.291
[62,     1] loss: 0.512
[63,     1] loss: 0.302
[64,     1] loss: 0.392
[65,     1] loss: 0.329
[66,     1] loss: 0.346
[67,     1] loss: 0.334
[68,     1] loss: 0.350
[69,     1] loss: 0.340
[70,     1] loss: 0.346
[71,     1] loss: 0.337
[72,     1] loss: 0.332
[73,     1] loss: 0.326
[74,     1] loss: 0.316
[75,     1] loss: 0.308
[76,     1] loss: 0.297
[77,     1] loss: 0.288
[78,     1] loss: 0.276
[79,     1] loss: 0.267
[80,     1] loss: 0.258
Early stopping applied (best metric=0.42736345529556274)
Finished Training
Total time taken: 304.12538623809814
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.656
[5,     1] loss: 0.637
[6,     1] loss: 0.618
[7,     1] loss: 0.599
[8,     1] loss: 0.573
[9,     1] loss: 0.551
[10,     1] loss: 0.526
[11,     1] loss: 0.502
[12,     1] loss: 0.478
[13,     1] loss: 0.456
[14,     1] loss: 0.438
[15,     1] loss: 0.421
[16,     1] loss: 0.406
[17,     1] loss: 0.395
[18,     1] loss: 0.384
[19,     1] loss: 0.372
[20,     1] loss: 0.361
[21,     1] loss: 0.347
[22,     1] loss: 0.337
[23,     1] loss: 0.327
[24,     1] loss: 0.319
[25,     1] loss: 0.309
[26,     1] loss: 0.300
[27,     1] loss: 0.288
[28,     1] loss: 0.277
[29,     1] loss: 0.266
[30,     1] loss: 0.258
[31,     1] loss: 0.244
[32,     1] loss: 0.233
[33,     1] loss: 0.222
[34,     1] loss: 0.213
[35,     1] loss: 0.201
[36,     1] loss: 0.197
[37,     1] loss: 0.187
[38,     1] loss: 0.180
[39,     1] loss: 0.174
[40,     1] loss: 0.168
[41,     1] loss: 0.161
[42,     1] loss: 0.161
[43,     1] loss: 0.154
[44,     1] loss: 0.152
[45,     1] loss: 0.147
[46,     1] loss: 0.144
[47,     1] loss: 0.142
[48,     1] loss: 0.139
[49,     1] loss: 0.138
[50,     1] loss: 0.135
[51,     1] loss: 0.133
[52,     1] loss: 0.133
[53,     1] loss: 0.132
[54,     1] loss: 0.129
[55,     1] loss: 0.129
[56,     1] loss: 0.129
[57,     1] loss: 0.128
[58,     1] loss: 0.123
[59,     1] loss: 0.165
[60,     1] loss: 0.238
[61,     1] loss: 0.806
[62,     1] loss: 0.595
[63,     1] loss: 0.433
[64,     1] loss: 0.468
[65,     1] loss: 0.428
[66,     1] loss: 0.430
[67,     1] loss: 0.446
[68,     1] loss: 0.451
[69,     1] loss: 0.436
[70,     1] loss: 0.422
[71,     1] loss: 0.422
[72,     1] loss: 0.403
[73,     1] loss: 0.394
[74,     1] loss: 0.388
[75,     1] loss: 0.373
[76,     1] loss: 0.364
[77,     1] loss: 0.355
[78,     1] loss: 0.346
[79,     1] loss: 0.334
[80,     1] loss: 0.326
[81,     1] loss: 0.319
[82,     1] loss: 0.310
[83,     1] loss: 0.304
Early stopping applied (best metric=0.3945707082748413)
Finished Training
Total time taken: 302.9128794670105
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.670
[4,     1] loss: 0.656
[5,     1] loss: 0.642
[6,     1] loss: 0.628
[7,     1] loss: 0.614
[8,     1] loss: 0.599
[9,     1] loss: 0.582
[10,     1] loss: 0.565
[11,     1] loss: 0.547
[12,     1] loss: 0.529
[13,     1] loss: 0.509
[14,     1] loss: 0.485
[15,     1] loss: 0.464
[16,     1] loss: 0.444
[17,     1] loss: 0.424
[18,     1] loss: 0.407
[19,     1] loss: 0.388
[20,     1] loss: 0.371
[21,     1] loss: 0.354
[22,     1] loss: 0.338
[23,     1] loss: 0.322
[24,     1] loss: 0.308
[25,     1] loss: 0.294
[26,     1] loss: 0.281
[27,     1] loss: 0.267
[28,     1] loss: 0.252
[29,     1] loss: 0.241
[30,     1] loss: 0.229
[31,     1] loss: 0.218
[32,     1] loss: 0.209
[33,     1] loss: 0.200
[34,     1] loss: 0.194
[35,     1] loss: 0.185
[36,     1] loss: 0.176
[37,     1] loss: 0.171
[38,     1] loss: 0.164
[39,     1] loss: 0.161
[40,     1] loss: 0.155
[41,     1] loss: 0.152
[42,     1] loss: 0.149
[43,     1] loss: 0.146
[44,     1] loss: 0.145
[45,     1] loss: 0.164
[46,     1] loss: 0.151
[47,     1] loss: 0.152
[48,     1] loss: 0.149
[49,     1] loss: 0.148
[50,     1] loss: 0.147
[51,     1] loss: 0.144
[52,     1] loss: 0.149
[53,     1] loss: 0.139
[54,     1] loss: 0.129
[55,     1] loss: 0.126
[56,     1] loss: 0.119
[57,     1] loss: 0.117
[58,     1] loss: 0.252
[59,     1] loss: 0.216
[60,     1] loss: 0.477
[61,     1] loss: 0.373
[62,     1] loss: 0.331
[63,     1] loss: 0.301
[64,     1] loss: 0.299
[65,     1] loss: 0.301
[66,     1] loss: 0.296
[67,     1] loss: 0.286
[68,     1] loss: 0.274
[69,     1] loss: 0.268
[70,     1] loss: 0.255
[71,     1] loss: 0.246
[72,     1] loss: 0.233
[73,     1] loss: 0.226
[74,     1] loss: 0.214
[75,     1] loss: 0.211
[76,     1] loss: 0.198
[77,     1] loss: 0.191
[78,     1] loss: 0.185
[79,     1] loss: 0.175
[80,     1] loss: 0.170
[81,     1] loss: 0.162
[82,     1] loss: 0.160
[83,     1] loss: 0.153
[84,     1] loss: 0.146
[85,     1] loss: 0.146
[86,     1] loss: 0.142
[87,     1] loss: 0.138
[88,     1] loss: 0.137
[89,     1] loss: 0.135
[90,     1] loss: 0.133
[91,     1] loss: 0.133
[92,     1] loss: 0.129
[93,     1] loss: 0.129
[94,     1] loss: 0.130
[95,     1] loss: 0.131
[96,     1] loss: 0.131
[97,     1] loss: 0.129
[98,     1] loss: 0.134
[99,     1] loss: 0.131
[100,     1] loss: 0.134
[101,     1] loss: 0.131
[102,     1] loss: 0.131
[103,     1] loss: 0.130
[104,     1] loss: 0.131
[105,     1] loss: 0.130
[106,     1] loss: 0.133
[107,     1] loss: 0.131
[108,     1] loss: 0.132
[109,     1] loss: 0.167
[110,     1] loss: 0.343
[111,     1] loss: 0.349
[112,     1] loss: 0.545
[113,     1] loss: 0.453
[114,     1] loss: 0.411
[115,     1] loss: 0.349
[116,     1] loss: 0.346
[117,     1] loss: 0.332
[118,     1] loss: 0.300
[119,     1] loss: 0.293
[120,     1] loss: 0.280
[121,     1] loss: 0.267
[122,     1] loss: 0.260
[123,     1] loss: 0.252
[124,     1] loss: 0.245
[125,     1] loss: 0.239
[126,     1] loss: 0.231
[127,     1] loss: 0.225
[128,     1] loss: 0.219
[129,     1] loss: 0.209
[130,     1] loss: 0.203
[131,     1] loss: 0.197
[132,     1] loss: 0.193
[133,     1] loss: 0.188
[134,     1] loss: 0.184
[135,     1] loss: 0.179
[136,     1] loss: 0.178
[137,     1] loss: 0.174
[138,     1] loss: 0.166
[139,     1] loss: 0.168
[140,     1] loss: 0.166
[141,     1] loss: 0.163
[142,     1] loss: 0.161
Early stopping applied (best metric=0.33116257190704346)
Finished Training
Total time taken: 484.6910557746887
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.686
[3,     1] loss: 0.672
[4,     1] loss: 0.659
[5,     1] loss: 0.649
[6,     1] loss: 0.637
[7,     1] loss: 0.624
[8,     1] loss: 0.608
[9,     1] loss: 0.595
[10,     1] loss: 0.579
[11,     1] loss: 0.561
[12,     1] loss: 0.543
[13,     1] loss: 0.523
[14,     1] loss: 0.501
[15,     1] loss: 0.478
[16,     1] loss: 0.461
[17,     1] loss: 0.439
[18,     1] loss: 0.421
[19,     1] loss: 0.405
[20,     1] loss: 0.390
[21,     1] loss: 0.379
[22,     1] loss: 0.367
[23,     1] loss: 0.359
[24,     1] loss: 0.348
[25,     1] loss: 0.338
[26,     1] loss: 0.332
[27,     1] loss: 0.319
[28,     1] loss: 0.313
[29,     1] loss: 0.307
[30,     1] loss: 0.302
[31,     1] loss: 0.296
[32,     1] loss: 0.290
[33,     1] loss: 0.285
[34,     1] loss: 0.282
[35,     1] loss: 0.274
[36,     1] loss: 0.271
[37,     1] loss: 0.267
[38,     1] loss: 0.262
[39,     1] loss: 0.261
[40,     1] loss: 0.255
[41,     1] loss: 0.252
[42,     1] loss: 0.247
[43,     1] loss: 0.245
[44,     1] loss: 0.240
[45,     1] loss: 0.237
[46,     1] loss: 0.233
[47,     1] loss: 0.232
[48,     1] loss: 0.229
[49,     1] loss: 0.224
[50,     1] loss: 0.221
[51,     1] loss: 0.221
[52,     1] loss: 0.218
[53,     1] loss: 0.218
[54,     1] loss: 0.235
[55,     1] loss: 0.225
[56,     1] loss: 0.225
[57,     1] loss: 0.221
[58,     1] loss: 0.220
[59,     1] loss: 0.217
[60,     1] loss: 0.214
[61,     1] loss: 0.209
[62,     1] loss: 0.203
[63,     1] loss: 0.198
Early stopping applied (best metric=0.4836486279964447)
Finished Training
Total time taken: 211.25574827194214
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.665
[5,     1] loss: 0.652
[6,     1] loss: 0.640
[7,     1] loss: 0.627
[8,     1] loss: 0.614
[9,     1] loss: 0.599
[10,     1] loss: 0.581
[11,     1] loss: 0.567
[12,     1] loss: 0.547
[13,     1] loss: 0.527
[14,     1] loss: 0.509
[15,     1] loss: 0.487
[16,     1] loss: 0.462
[17,     1] loss: 0.439
[18,     1] loss: 0.416
[19,     1] loss: 0.394
[20,     1] loss: 0.370
[21,     1] loss: 0.345
[22,     1] loss: 0.325
[23,     1] loss: 0.306
[24,     1] loss: 0.285
[25,     1] loss: 0.269
[26,     1] loss: 0.250
[27,     1] loss: 0.236
[28,     1] loss: 0.223
[29,     1] loss: 0.206
[30,     1] loss: 0.195
[31,     1] loss: 0.186
[32,     1] loss: 0.176
[33,     1] loss: 0.167
[34,     1] loss: 0.160
[35,     1] loss: 0.151
[36,     1] loss: 0.148
[37,     1] loss: 0.142
[38,     1] loss: 0.136
[39,     1] loss: 0.133
[40,     1] loss: 0.127
[41,     1] loss: 0.128
[42,     1] loss: 0.124
[43,     1] loss: 0.121
[44,     1] loss: 0.119
[45,     1] loss: 0.118
[46,     1] loss: 0.116
[47,     1] loss: 0.114
[48,     1] loss: 0.111
[49,     1] loss: 0.108
[50,     1] loss: 0.107
[51,     1] loss: 0.102
[52,     1] loss: 0.116
[53,     1] loss: 0.343
[54,     1] loss: 0.380
[55,     1] loss: 0.259
[56,     1] loss: 0.288
[57,     1] loss: 0.264
[58,     1] loss: 0.265
[59,     1] loss: 0.252
[60,     1] loss: 0.239
[61,     1] loss: 0.234
[62,     1] loss: 0.215
[63,     1] loss: 0.205
[64,     1] loss: 0.196
[65,     1] loss: 0.184
[66,     1] loss: 0.177
[67,     1] loss: 0.170
[68,     1] loss: 0.159
[69,     1] loss: 0.154
[70,     1] loss: 0.145
[71,     1] loss: 0.140
[72,     1] loss: 0.134
[73,     1] loss: 0.130
[74,     1] loss: 0.125
[75,     1] loss: 0.123
[76,     1] loss: 0.121
[77,     1] loss: 0.120
[78,     1] loss: 0.117
[79,     1] loss: 0.114
[80,     1] loss: 0.114
[81,     1] loss: 0.113
[82,     1] loss: 0.112
[83,     1] loss: 0.111
[84,     1] loss: 0.114
[85,     1] loss: 0.117
[86,     1] loss: 0.114
[87,     1] loss: 0.114
[88,     1] loss: 0.113
[89,     1] loss: 0.113
[90,     1] loss: 0.118
[91,     1] loss: 0.115
[92,     1] loss: 0.118
[93,     1] loss: 0.117
[94,     1] loss: 0.116
[95,     1] loss: 0.116
[96,     1] loss: 0.118
[97,     1] loss: 0.116
[98,     1] loss: 0.116
[99,     1] loss: 0.118
[100,     1] loss: 0.118
[101,     1] loss: 0.122
[102,     1] loss: 0.204
[103,     1] loss: 0.512
[104,     1] loss: 0.464
[105,     1] loss: 0.551
[106,     1] loss: 0.449
[107,     1] loss: 0.470
[108,     1] loss: 0.467
[109,     1] loss: 0.461
[110,     1] loss: 0.447
[111,     1] loss: 0.442
[112,     1] loss: 0.434
[113,     1] loss: 0.431
[114,     1] loss: 0.420
[115,     1] loss: 0.410
[116,     1] loss: 0.396
[117,     1] loss: 0.385
Early stopping applied (best metric=0.3903586268424988)
Finished Training
Total time taken: 395.37980914115906
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.664
[4,     1] loss: 0.640
[5,     1] loss: 0.619
[6,     1] loss: 0.593
[7,     1] loss: 0.575
[8,     1] loss: 0.552
[9,     1] loss: 0.529
[10,     1] loss: 0.509
[11,     1] loss: 0.483
[12,     1] loss: 0.464
[13,     1] loss: 0.440
[14,     1] loss: 0.427
[15,     1] loss: 0.420
[16,     1] loss: 0.407
[17,     1] loss: 0.398
[18,     1] loss: 0.392
[19,     1] loss: 0.386
[20,     1] loss: 0.381
[21,     1] loss: 0.377
[22,     1] loss: 0.375
[23,     1] loss: 0.372
[24,     1] loss: 0.371
[25,     1] loss: 0.369
[26,     1] loss: 0.368
[27,     1] loss: 0.367
[28,     1] loss: 0.367
[29,     1] loss: 0.367
[30,     1] loss: 0.366
[31,     1] loss: 0.367
[32,     1] loss: 0.366
[33,     1] loss: 0.365
[34,     1] loss: 0.365
[35,     1] loss: 0.364
[36,     1] loss: 0.364
[37,     1] loss: 0.364
[38,     1] loss: 0.360
[39,     1] loss: 0.359
[40,     1] loss: 0.359
[41,     1] loss: 0.356
[42,     1] loss: 0.354
[43,     1] loss: 0.354
[44,     1] loss: 0.352
[45,     1] loss: 0.349
[46,     1] loss: 0.346
[47,     1] loss: 0.341
[48,     1] loss: 0.335
[49,     1] loss: 0.329
[50,     1] loss: 0.324
[51,     1] loss: 0.316
[52,     1] loss: 0.306
[53,     1] loss: 0.304
[54,     1] loss: 0.308
[55,     1] loss: 0.317
[56,     1] loss: 0.318
[57,     1] loss: 0.298
[58,     1] loss: 0.281
[59,     1] loss: 0.294
[60,     1] loss: 0.270
[61,     1] loss: 0.274
[62,     1] loss: 0.243
[63,     1] loss: 0.240
[64,     1] loss: 0.224
[65,     1] loss: 0.215
[66,     1] loss: 0.199
[67,     1] loss: 0.186
[68,     1] loss: 0.171
[69,     1] loss: 0.157
[70,     1] loss: 0.155
[71,     1] loss: 0.161
[72,     1] loss: 0.137
[73,     1] loss: 0.131
[74,     1] loss: 0.120
[75,     1] loss: 0.119
[76,     1] loss: 0.110
[77,     1] loss: 0.107
[78,     1] loss: 0.101
[79,     1] loss: 0.098
[80,     1] loss: 0.094
[81,     1] loss: 0.092
[82,     1] loss: 0.089
[83,     1] loss: 0.087
[84,     1] loss: 0.087
[85,     1] loss: 0.085
[86,     1] loss: 0.086
[87,     1] loss: 0.086
[88,     1] loss: 0.088
[89,     1] loss: 0.088
[90,     1] loss: 0.088
[91,     1] loss: 0.090
[92,     1] loss: 0.089
[93,     1] loss: 0.091
[94,     1] loss: 0.089
[95,     1] loss: 0.090
[96,     1] loss: 0.091
[97,     1] loss: 0.095
[98,     1] loss: 0.095
[99,     1] loss: 0.095
[100,     1] loss: 0.097
[101,     1] loss: 0.096
[102,     1] loss: 0.098
[103,     1] loss: 0.098
[104,     1] loss: 0.098
[105,     1] loss: 0.098
[106,     1] loss: 0.100
[107,     1] loss: 0.100
[108,     1] loss: 0.100
[109,     1] loss: 0.099
[110,     1] loss: 0.098
[111,     1] loss: 0.100
[112,     1] loss: 0.101
[113,     1] loss: 0.102
[114,     1] loss: 0.102
[115,     1] loss: 0.100
[116,     1] loss: 0.102
[117,     1] loss: 0.102
[118,     1] loss: 0.105
[119,     1] loss: 0.105
[120,     1] loss: 0.105
[121,     1] loss: 0.108
[122,     1] loss: 0.175
[123,     1] loss: 0.268
[124,     1] loss: 0.938
[125,     1] loss: 0.707
[126,     1] loss: 0.590
[127,     1] loss: 0.545
[128,     1] loss: 0.530
[129,     1] loss: 0.537
[130,     1] loss: 0.544
[131,     1] loss: 0.556
[132,     1] loss: 0.565
[133,     1] loss: 0.572
[134,     1] loss: 0.577
[135,     1] loss: 0.581
[136,     1] loss: 0.586
Early stopping applied (best metric=0.35726398229599)
Finished Training
Total time taken: 468.2259702682495
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.691
[3,     1] loss: 0.674
[4,     1] loss: 0.660
[5,     1] loss: 0.647
[6,     1] loss: 0.634
[7,     1] loss: 0.621
[8,     1] loss: 0.605
[9,     1] loss: 0.589
[10,     1] loss: 0.575
[11,     1] loss: 0.559
[12,     1] loss: 0.538
[13,     1] loss: 0.520
[14,     1] loss: 0.503
[15,     1] loss: 0.480
[16,     1] loss: 0.458
[17,     1] loss: 0.435
[18,     1] loss: 0.411
[19,     1] loss: 0.392
[20,     1] loss: 0.369
[21,     1] loss: 0.347
[22,     1] loss: 0.332
[23,     1] loss: 0.310
[24,     1] loss: 0.296
[25,     1] loss: 0.277
[26,     1] loss: 0.259
[27,     1] loss: 0.245
[28,     1] loss: 0.227
[29,     1] loss: 0.216
[30,     1] loss: 0.204
[31,     1] loss: 0.190
[32,     1] loss: 0.181
[33,     1] loss: 0.172
[34,     1] loss: 0.165
[35,     1] loss: 0.159
[36,     1] loss: 0.154
[37,     1] loss: 0.148
[38,     1] loss: 0.145
[39,     1] loss: 0.138
[40,     1] loss: 0.137
[41,     1] loss: 0.134
[42,     1] loss: 0.133
[43,     1] loss: 0.129
[44,     1] loss: 0.127
[45,     1] loss: 0.126
[46,     1] loss: 0.125
[47,     1] loss: 0.123
[48,     1] loss: 0.122
[49,     1] loss: 0.121
[50,     1] loss: 0.119
[51,     1] loss: 0.117
[52,     1] loss: 0.116
[53,     1] loss: 0.224
[54,     1] loss: 0.171
[55,     1] loss: 0.624
[56,     1] loss: 0.280
[57,     1] loss: 0.377
[58,     1] loss: 0.282
[59,     1] loss: 0.305
[60,     1] loss: 0.294
[61,     1] loss: 0.285
[62,     1] loss: 0.282
[63,     1] loss: 0.273
[64,     1] loss: 0.267
[65,     1] loss: 0.259
[66,     1] loss: 0.250
[67,     1] loss: 0.243
[68,     1] loss: 0.232
[69,     1] loss: 0.222
[70,     1] loss: 0.216
Early stopping applied (best metric=0.402140736579895)
Finished Training
Total time taken: 239.10414671897888
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.685
[3,     1] loss: 0.668
[4,     1] loss: 0.651
[5,     1] loss: 0.636
[6,     1] loss: 0.620
[7,     1] loss: 0.602
[8,     1] loss: 0.583
[9,     1] loss: 0.562
[10,     1] loss: 0.541
[11,     1] loss: 0.520
[12,     1] loss: 0.498
[13,     1] loss: 0.474
[14,     1] loss: 0.454
[15,     1] loss: 0.427
[16,     1] loss: 0.410
[17,     1] loss: 0.394
[18,     1] loss: 0.374
[19,     1] loss: 0.369
[20,     1] loss: 0.349
[21,     1] loss: 0.340
[22,     1] loss: 0.328
[23,     1] loss: 0.319
[24,     1] loss: 0.310
[25,     1] loss: 0.302
[26,     1] loss: 0.293
[27,     1] loss: 0.289
[28,     1] loss: 0.282
[29,     1] loss: 0.277
[30,     1] loss: 0.269
[31,     1] loss: 0.265
[32,     1] loss: 0.258
[33,     1] loss: 0.253
[34,     1] loss: 0.248
[35,     1] loss: 0.243
[36,     1] loss: 0.236
[37,     1] loss: 0.233
[38,     1] loss: 0.229
[39,     1] loss: 0.222
[40,     1] loss: 0.216
[41,     1] loss: 0.210
[42,     1] loss: 0.203
[43,     1] loss: 0.198
[44,     1] loss: 0.193
[45,     1] loss: 0.187
[46,     1] loss: 0.183
[47,     1] loss: 0.176
[48,     1] loss: 0.170
[49,     1] loss: 0.162
[50,     1] loss: 0.158
[51,     1] loss: 0.154
[52,     1] loss: 0.147
[53,     1] loss: 0.142
[54,     1] loss: 0.136
[55,     1] loss: 0.132
[56,     1] loss: 0.135
[57,     1] loss: 0.293
[58,     1] loss: 0.468
[59,     1] loss: 0.378
[60,     1] loss: 0.460
[61,     1] loss: 0.389
[62,     1] loss: 0.374
[63,     1] loss: 0.363
[64,     1] loss: 0.350
[65,     1] loss: 0.343
[66,     1] loss: 0.331
[67,     1] loss: 0.320
[68,     1] loss: 0.306
[69,     1] loss: 0.291
[70,     1] loss: 0.279
[71,     1] loss: 0.268
[72,     1] loss: 0.251
[73,     1] loss: 0.238
[74,     1] loss: 0.227
[75,     1] loss: 0.215
[76,     1] loss: 0.207
[77,     1] loss: 0.197
[78,     1] loss: 0.191
[79,     1] loss: 0.181
[80,     1] loss: 0.175
[81,     1] loss: 0.169
[82,     1] loss: 0.163
[83,     1] loss: 0.160
[84,     1] loss: 0.157
[85,     1] loss: 0.153
[86,     1] loss: 0.153
[87,     1] loss: 0.148
[88,     1] loss: 0.144
[89,     1] loss: 0.145
[90,     1] loss: 0.142
[91,     1] loss: 0.140
[92,     1] loss: 0.139
[93,     1] loss: 0.139
[94,     1] loss: 0.139
[95,     1] loss: 0.137
[96,     1] loss: 0.136
[97,     1] loss: 0.135
[98,     1] loss: 0.134
[99,     1] loss: 0.131
[100,     1] loss: 0.131
[101,     1] loss: 0.131
[102,     1] loss: 0.131
[103,     1] loss: 0.130
[104,     1] loss: 0.128
[105,     1] loss: 0.128
[106,     1] loss: 0.127
[107,     1] loss: 0.127
[108,     1] loss: 0.127
[109,     1] loss: 0.129
[110,     1] loss: 0.221
[111,     1] loss: 0.543
[112,     1] loss: 0.667
[113,     1] loss: 0.472
[114,     1] loss: 0.495
[115,     1] loss: 0.510
[116,     1] loss: 0.508
[117,     1] loss: 0.497
[118,     1] loss: 0.493
[119,     1] loss: 0.492
[120,     1] loss: 0.488
[121,     1] loss: 0.486
[122,     1] loss: 0.480
[123,     1] loss: 0.470
[124,     1] loss: 0.463
[125,     1] loss: 0.453
[126,     1] loss: 0.441
[127,     1] loss: 0.433
[128,     1] loss: 0.419
Early stopping applied (best metric=0.3248751759529114)
Finished Training
Total time taken: 412.324102640152
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.692
[3,     1] loss: 0.666
[4,     1] loss: 0.649
[5,     1] loss: 0.628
[6,     1] loss: 0.612
[7,     1] loss: 0.596
[8,     1] loss: 0.579
[9,     1] loss: 0.561
[10,     1] loss: 0.540
[11,     1] loss: 0.520
[12,     1] loss: 0.499
[13,     1] loss: 0.480
[14,     1] loss: 0.463
[15,     1] loss: 0.441
[16,     1] loss: 0.421
[17,     1] loss: 0.401
[18,     1] loss: 0.385
[19,     1] loss: 0.374
[20,     1] loss: 0.369
[21,     1] loss: 0.359
[22,     1] loss: 0.341
[23,     1] loss: 0.336
[24,     1] loss: 0.324
[25,     1] loss: 0.325
[26,     1] loss: 0.313
[27,     1] loss: 0.311
[28,     1] loss: 0.300
[29,     1] loss: 0.296
[30,     1] loss: 0.289
[31,     1] loss: 0.284
[32,     1] loss: 0.276
[33,     1] loss: 0.269
[34,     1] loss: 0.261
[35,     1] loss: 0.258
[36,     1] loss: 0.249
[37,     1] loss: 0.243
[38,     1] loss: 0.239
[39,     1] loss: 0.230
[40,     1] loss: 0.224
[41,     1] loss: 0.215
[42,     1] loss: 0.209
[43,     1] loss: 0.202
[44,     1] loss: 0.192
[45,     1] loss: 0.189
[46,     1] loss: 0.182
[47,     1] loss: 0.173
[48,     1] loss: 0.168
[49,     1] loss: 0.160
[50,     1] loss: 0.155
[51,     1] loss: 0.149
[52,     1] loss: 0.142
[53,     1] loss: 0.138
[54,     1] loss: 0.130
[55,     1] loss: 0.124
[56,     1] loss: 0.119
[57,     1] loss: 0.112
[58,     1] loss: 0.106
[59,     1] loss: 0.103
[60,     1] loss: 0.252
[61,     1] loss: 0.794
[62,     1] loss: 0.426
[63,     1] loss: 0.313
[64,     1] loss: 0.361
[65,     1] loss: 0.347
[66,     1] loss: 0.358
[67,     1] loss: 0.352
[68,     1] loss: 0.344
[69,     1] loss: 0.336
[70,     1] loss: 0.328
[71,     1] loss: 0.314
[72,     1] loss: 0.300
[73,     1] loss: 0.286
[74,     1] loss: 0.271
[75,     1] loss: 0.257
[76,     1] loss: 0.240
[77,     1] loss: 0.219
[78,     1] loss: 0.204
[79,     1] loss: 0.192
[80,     1] loss: 0.180
[81,     1] loss: 0.169
[82,     1] loss: 0.154
[83,     1] loss: 0.146
[84,     1] loss: 0.137
[85,     1] loss: 0.130
[86,     1] loss: 0.121
[87,     1] loss: 0.117
[88,     1] loss: 0.113
[89,     1] loss: 0.108
[90,     1] loss: 0.107
[91,     1] loss: 0.104
[92,     1] loss: 0.102
[93,     1] loss: 0.101
[94,     1] loss: 0.100
[95,     1] loss: 0.101
[96,     1] loss: 0.100
[97,     1] loss: 0.099
[98,     1] loss: 0.102
[99,     1] loss: 0.103
[100,     1] loss: 0.105
Early stopping applied (best metric=0.33458927273750305)
Finished Training
Total time taken: 321.5139720439911
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.679
[4,     1] loss: 0.671
[5,     1] loss: 0.660
[6,     1] loss: 0.648
[7,     1] loss: 0.631
[8,     1] loss: 0.613
[9,     1] loss: 0.589
[10,     1] loss: 0.565
[11,     1] loss: 0.541
[12,     1] loss: 0.519
[13,     1] loss: 0.497
[14,     1] loss: 0.477
[15,     1] loss: 0.460
[16,     1] loss: 0.443
[17,     1] loss: 0.431
[18,     1] loss: 0.424
[19,     1] loss: 0.415
[20,     1] loss: 0.409
[21,     1] loss: 0.403
[22,     1] loss: 0.399
[23,     1] loss: 0.396
[24,     1] loss: 0.393
[25,     1] loss: 0.390
[26,     1] loss: 0.389
[27,     1] loss: 0.387
[28,     1] loss: 0.385
[29,     1] loss: 0.384
[30,     1] loss: 0.383
[31,     1] loss: 0.383
[32,     1] loss: 0.385
[33,     1] loss: 0.382
[34,     1] loss: 0.381
[35,     1] loss: 0.381
[36,     1] loss: 0.382
[37,     1] loss: 0.382
[38,     1] loss: 0.382
[39,     1] loss: 0.382
[40,     1] loss: 0.381
[41,     1] loss: 0.383
[42,     1] loss: 0.393
[43,     1] loss: 0.390
[44,     1] loss: 0.384
[45,     1] loss: 0.388
[46,     1] loss: 0.387
[47,     1] loss: 0.384
[48,     1] loss: 0.384
[49,     1] loss: 0.382
[50,     1] loss: 0.380
[51,     1] loss: 0.379
[52,     1] loss: 0.377
[53,     1] loss: 0.376
[54,     1] loss: 0.375
[55,     1] loss: 0.376
[56,     1] loss: 0.381
[57,     1] loss: 0.377
[58,     1] loss: 0.385
[59,     1] loss: 0.378
[60,     1] loss: 0.381
[61,     1] loss: 0.375
[62,     1] loss: 0.383
[63,     1] loss: 0.383
[64,     1] loss: 0.383
[65,     1] loss: 0.384
[66,     1] loss: 0.383
[67,     1] loss: 0.380
[68,     1] loss: 0.379
[69,     1] loss: 0.376
[70,     1] loss: 0.375
[71,     1] loss: 0.374
[72,     1] loss: 0.373
[73,     1] loss: 0.373
[74,     1] loss: 0.373
[75,     1] loss: 0.373
[76,     1] loss: 0.374
[77,     1] loss: 0.373
[78,     1] loss: 0.371
[79,     1] loss: 0.373
[80,     1] loss: 0.394
[81,     1] loss: 0.393
[82,     1] loss: 0.408
[83,     1] loss: 0.405
[84,     1] loss: 0.399
[85,     1] loss: 0.390
[86,     1] loss: 0.383
[87,     1] loss: 0.376
[88,     1] loss: 0.376
[89,     1] loss: 0.384
[90,     1] loss: 0.392
[91,     1] loss: 0.386
[92,     1] loss: 0.386
[93,     1] loss: 0.386
[94,     1] loss: 0.383
Early stopping applied (best metric=0.45044204592704773)
Finished Training
Total time taken: 302.91878151893616
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.677
[3,     1] loss: 0.659
[4,     1] loss: 0.646
[5,     1] loss: 0.634
[6,     1] loss: 0.622
[7,     1] loss: 0.610
[8,     1] loss: 0.597
[9,     1] loss: 0.586
[10,     1] loss: 0.571
[11,     1] loss: 0.558
[12,     1] loss: 0.541
[13,     1] loss: 0.526
[14,     1] loss: 0.507
[15,     1] loss: 0.489
[16,     1] loss: 0.469
[17,     1] loss: 0.450
[18,     1] loss: 0.429
[19,     1] loss: 0.413
[20,     1] loss: 0.399
[21,     1] loss: 0.387
[22,     1] loss: 0.368
[23,     1] loss: 0.347
[24,     1] loss: 0.337
[25,     1] loss: 0.319
[26,     1] loss: 0.308
[27,     1] loss: 0.295
[28,     1] loss: 0.285
[29,     1] loss: 0.270
[30,     1] loss: 0.260
[31,     1] loss: 0.250
[32,     1] loss: 0.238
[33,     1] loss: 0.229
[34,     1] loss: 0.221
[35,     1] loss: 0.212
[36,     1] loss: 0.207
[37,     1] loss: 0.198
[38,     1] loss: 0.193
[39,     1] loss: 0.187
[40,     1] loss: 0.183
[41,     1] loss: 0.177
[42,     1] loss: 0.171
[43,     1] loss: 0.168
[44,     1] loss: 0.164
[45,     1] loss: 0.162
[46,     1] loss: 0.158
[47,     1] loss: 0.156
[48,     1] loss: 0.153
[49,     1] loss: 0.150
[50,     1] loss: 0.146
[51,     1] loss: 0.142
[52,     1] loss: 0.150
[53,     1] loss: 0.272
[54,     1] loss: 0.299
[55,     1] loss: 0.326
[56,     1] loss: 0.284
[57,     1] loss: 0.276
[58,     1] loss: 0.282
[59,     1] loss: 0.285
[60,     1] loss: 0.279
[61,     1] loss: 0.270
[62,     1] loss: 0.260
[63,     1] loss: 0.251
[64,     1] loss: 0.243
[65,     1] loss: 0.236
[66,     1] loss: 0.227
[67,     1] loss: 0.218
[68,     1] loss: 0.207
[69,     1] loss: 0.202
[70,     1] loss: 0.192
[71,     1] loss: 0.187
[72,     1] loss: 0.178
[73,     1] loss: 0.171
[74,     1] loss: 0.167
[75,     1] loss: 0.160
[76,     1] loss: 0.153
[77,     1] loss: 0.148
[78,     1] loss: 0.142
[79,     1] loss: 0.138
[80,     1] loss: 0.136
Early stopping applied (best metric=0.3869951665401459)
Finished Training
Total time taken: 259.82886147499084
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.675
[4,     1] loss: 0.661
[5,     1] loss: 0.645
[6,     1] loss: 0.633
[7,     1] loss: 0.620
[8,     1] loss: 0.601
[9,     1] loss: 0.587
[10,     1] loss: 0.567
[11,     1] loss: 0.545
[12,     1] loss: 0.524
[13,     1] loss: 0.501
[14,     1] loss: 0.477
[15,     1] loss: 0.449
[16,     1] loss: 0.423
[17,     1] loss: 0.398
[18,     1] loss: 0.382
[19,     1] loss: 0.366
[20,     1] loss: 0.342
[21,     1] loss: 0.333
[22,     1] loss: 0.305
[23,     1] loss: 0.294
[24,     1] loss: 0.279
[25,     1] loss: 0.269
[26,     1] loss: 0.257
[27,     1] loss: 0.246
[28,     1] loss: 0.235
[29,     1] loss: 0.230
[30,     1] loss: 0.223
[31,     1] loss: 0.221
[32,     1] loss: 0.213
[33,     1] loss: 0.209
[34,     1] loss: 0.205
[35,     1] loss: 0.203
[36,     1] loss: 0.199
[37,     1] loss: 0.196
[38,     1] loss: 0.194
[39,     1] loss: 0.194
[40,     1] loss: 0.191
[41,     1] loss: 0.189
[42,     1] loss: 0.189
[43,     1] loss: 0.184
[44,     1] loss: 0.184
[45,     1] loss: 0.184
[46,     1] loss: 0.182
[47,     1] loss: 0.189
[48,     1] loss: 0.206
[49,     1] loss: 0.234
[50,     1] loss: 0.215
[51,     1] loss: 0.231
[52,     1] loss: 0.229
[53,     1] loss: 0.217
[54,     1] loss: 0.221
[55,     1] loss: 0.210
[56,     1] loss: 0.210
[57,     1] loss: 0.202
[58,     1] loss: 0.195
[59,     1] loss: 0.188
[60,     1] loss: 0.182
[61,     1] loss: 0.176
[62,     1] loss: 0.171
[63,     1] loss: 0.172
[64,     1] loss: 0.198
[65,     1] loss: 0.227
[66,     1] loss: 0.194
[67,     1] loss: 0.218
[68,     1] loss: 0.203
[69,     1] loss: 0.223
Early stopping applied (best metric=0.45903992652893066)
Finished Training
Total time taken: 225.95401310920715
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.666
[4,     1] loss: 0.653
[5,     1] loss: 0.640
[6,     1] loss: 0.628
[7,     1] loss: 0.615
[8,     1] loss: 0.603
[9,     1] loss: 0.587
[10,     1] loss: 0.572
[11,     1] loss: 0.555
[12,     1] loss: 0.541
[13,     1] loss: 0.523
[14,     1] loss: 0.507
[15,     1] loss: 0.493
[16,     1] loss: 0.474
[17,     1] loss: 0.456
[18,     1] loss: 0.443
[19,     1] loss: 0.436
[20,     1] loss: 0.422
[21,     1] loss: 0.402
[22,     1] loss: 0.395
[23,     1] loss: 0.386
[24,     1] loss: 0.380
[25,     1] loss: 0.373
[26,     1] loss: 0.366
[27,     1] loss: 0.360
[28,     1] loss: 0.355
[29,     1] loss: 0.350
[30,     1] loss: 0.344
[31,     1] loss: 0.339
[32,     1] loss: 0.333
[33,     1] loss: 0.328
[34,     1] loss: 0.323
[35,     1] loss: 0.316
[36,     1] loss: 0.309
[37,     1] loss: 0.303
[38,     1] loss: 0.295
[39,     1] loss: 0.288
[40,     1] loss: 0.279
[41,     1] loss: 0.269
[42,     1] loss: 0.260
[43,     1] loss: 0.249
[44,     1] loss: 0.236
[45,     1] loss: 0.221
[46,     1] loss: 0.214
[47,     1] loss: 0.217
[48,     1] loss: 0.305
[49,     1] loss: 0.258
[50,     1] loss: 0.244
[51,     1] loss: 0.262
[52,     1] loss: 0.225
[53,     1] loss: 0.224
[54,     1] loss: 0.203
[55,     1] loss: 0.182
[56,     1] loss: 0.173
[57,     1] loss: 0.161
[58,     1] loss: 0.152
[59,     1] loss: 0.139
[60,     1] loss: 0.128
[61,     1] loss: 0.120
[62,     1] loss: 0.111
[63,     1] loss: 0.101
Early stopping applied (best metric=0.46040675044059753)
Finished Training
Total time taken: 206.75694012641907
{'Hydroxylation-P Validation Accuracy': 0.8046526064666768, 'Hydroxylation-P Validation Sensitivity': 0.7649523809523809, 'Hydroxylation-P Validation Specificity': 0.813212629058806, 'Hydroxylation-P Validation Precision': 0.4790016981732297, 'Hydroxylation-P AUC ROC': 0.8396228418945013, 'Hydroxylation-P AUC PR': 0.601239650290141, 'Hydroxylation-P MCC': 0.49262653359590314, 'Hydroxylation-P F1': 0.5856615865056727, 'Validation Loss (Hydroxylation-P)': 0.38475668072700503, 'Validation Loss (total)': 0.38475668072700503}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037397210602859122,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1046177137,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.697098714926167}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.693
[3,     1] loss: 0.673
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0058528330067113875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1011547566,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.077258115583201}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.658
[4,     1] loss: 0.630
[5,     1] loss: 0.597
[6,     1] loss: 0.561
[7,     1] loss: 0.523
[8,     1] loss: 0.483
[9,     1] loss: 0.437
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004189208247452992,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3905525096,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.94312682533366}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.690
[3,     1] loss: 0.677
[4,     1] loss: 0.664
[5,     1] loss: 0.653
[6,     1] loss: 0.638
[7,     1] loss: 0.624
[8,     1] loss: 0.608
[9,     1] loss: 0.590
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009879713784741785,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4264621337,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.956989593529388}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.662
[4,     1] loss: 0.626
[5,     1] loss: 0.581
[6,     1] loss: 0.526
[7,     1] loss: 0.472
[8,     1] loss: 0.420
[9,     1] loss: 0.392
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005629059818367188,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 255854220,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.059455044520064}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.670
[3,     1] loss: 0.636
[4,     1] loss: 0.612
[5,     1] loss: 0.587
[6,     1] loss: 0.565
[7,     1] loss: 0.540
[8,     1] loss: 0.517
[9,     1] loss: 0.492
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022667743302114953,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 699322115,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.772023936100375}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.663
[3,     1] loss: 0.635
[4,     1] loss: 0.618
[5,     1] loss: 0.600
[6,     1] loss: 0.581
[7,     1] loss: 0.566
[8,     1] loss: 0.544
[9,     1] loss: 0.525
[10,     1] loss: 0.506
[11,     1] loss: 0.487
[12,     1] loss: 0.468
[13,     1] loss: 0.445
[14,     1] loss: 0.423
[15,     1] loss: 0.401
[16,     1] loss: 0.385
[17,     1] loss: 0.367
[18,     1] loss: 0.355
[19,     1] loss: 0.338
[20,     1] loss: 0.323
[21,     1] loss: 0.305
[22,     1] loss: 0.294
[23,     1] loss: 0.279
[24,     1] loss: 0.269
[25,     1] loss: 0.258
[26,     1] loss: 0.247
[27,     1] loss: 0.238
[28,     1] loss: 0.228
[29,     1] loss: 0.221
[30,     1] loss: 0.210
[31,     1] loss: 0.202
[32,     1] loss: 0.195
[33,     1] loss: 0.187
[34,     1] loss: 0.178
[35,     1] loss: 0.173
[36,     1] loss: 0.166
[37,     1] loss: 0.160
[38,     1] loss: 0.154
[39,     1] loss: 0.147
[40,     1] loss: 0.142
[41,     1] loss: 0.138
[42,     1] loss: 0.135
[43,     1] loss: 0.127
[44,     1] loss: 0.127
[45,     1] loss: 0.124
[46,     1] loss: 0.120
[47,     1] loss: 0.115
[48,     1] loss: 0.113
[49,     1] loss: 0.112
[50,     1] loss: 0.108
[51,     1] loss: 0.107
[52,     1] loss: 0.104
[53,     1] loss: 0.104
[54,     1] loss: 0.103
[55,     1] loss: 0.100
[56,     1] loss: 0.100
[57,     1] loss: 0.098
[58,     1] loss: 0.097
[59,     1] loss: 0.096
[60,     1] loss: 0.096
[61,     1] loss: 0.094
[62,     1] loss: 0.093
[63,     1] loss: 0.092
[64,     1] loss: 0.089
[65,     1] loss: 0.088
[66,     1] loss: 0.088
[67,     1] loss: 0.086
[68,     1] loss: 0.085
[69,     1] loss: 0.118
[70,     1] loss: 0.292
[71,     1] loss: 0.636
[72,     1] loss: 0.398
[73,     1] loss: 0.529
[74,     1] loss: 0.431
[75,     1] loss: 0.374
[76,     1] loss: 0.395
[77,     1] loss: 0.368
[78,     1] loss: 0.410
[79,     1] loss: 0.369
Early stopping applied (best metric=0.34869059920310974)
Finished Training
Total time taken: 258.3740568161011
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.657
[4,     1] loss: 0.645
[5,     1] loss: 0.630
[6,     1] loss: 0.614
[7,     1] loss: 0.599
[8,     1] loss: 0.582
[9,     1] loss: 0.561
[10,     1] loss: 0.545
[11,     1] loss: 0.524
[12,     1] loss: 0.501
[13,     1] loss: 0.474
[14,     1] loss: 0.457
[15,     1] loss: 0.438
[16,     1] loss: 0.417
[17,     1] loss: 0.396
[18,     1] loss: 0.379
[19,     1] loss: 0.363
[20,     1] loss: 0.347
[21,     1] loss: 0.333
[22,     1] loss: 0.320
[23,     1] loss: 0.306
[24,     1] loss: 0.291
[25,     1] loss: 0.278
[26,     1] loss: 0.268
[27,     1] loss: 0.250
[28,     1] loss: 0.241
[29,     1] loss: 0.228
[30,     1] loss: 0.219
[31,     1] loss: 0.208
[32,     1] loss: 0.198
[33,     1] loss: 0.192
[34,     1] loss: 0.182
[35,     1] loss: 0.178
[36,     1] loss: 0.171
[37,     1] loss: 0.166
[38,     1] loss: 0.163
[39,     1] loss: 0.158
[40,     1] loss: 0.155
[41,     1] loss: 0.152
[42,     1] loss: 0.152
[43,     1] loss: 0.146
[44,     1] loss: 0.145
[45,     1] loss: 0.142
[46,     1] loss: 0.141
[47,     1] loss: 0.139
[48,     1] loss: 0.140
[49,     1] loss: 0.139
[50,     1] loss: 0.138
[51,     1] loss: 0.137
[52,     1] loss: 0.138
[53,     1] loss: 0.138
[54,     1] loss: 0.138
[55,     1] loss: 0.137
[56,     1] loss: 0.136
[57,     1] loss: 0.138
[58,     1] loss: 0.137
[59,     1] loss: 0.137
[60,     1] loss: 0.138
[61,     1] loss: 0.136
[62,     1] loss: 0.136
[63,     1] loss: 0.138
[64,     1] loss: 0.137
[65,     1] loss: 0.134
[66,     1] loss: 0.139
[67,     1] loss: 0.241
[68,     1] loss: 0.819
[69,     1] loss: 0.717
[70,     1] loss: 0.563
[71,     1] loss: 0.442
[72,     1] loss: 0.423
[73,     1] loss: 0.430
[74,     1] loss: 0.430
[75,     1] loss: 0.428
[76,     1] loss: 0.421
[77,     1] loss: 0.412
[78,     1] loss: 0.403
[79,     1] loss: 0.390
[80,     1] loss: 0.376
[81,     1] loss: 0.363
[82,     1] loss: 0.351
[83,     1] loss: 0.339
[84,     1] loss: 0.324
[85,     1] loss: 0.308
[86,     1] loss: 0.298
[87,     1] loss: 0.287
[88,     1] loss: 0.277
[89,     1] loss: 0.266
[90,     1] loss: 0.255
[91,     1] loss: 0.244
[92,     1] loss: 0.232
[93,     1] loss: 0.219
[94,     1] loss: 0.206
[95,     1] loss: 0.199
[96,     1] loss: 0.190
[97,     1] loss: 0.190
[98,     1] loss: 0.220
[99,     1] loss: 0.215
[100,     1] loss: 0.228
[101,     1] loss: 0.210
[102,     1] loss: 0.208
Early stopping applied (best metric=0.30157721042633057)
Finished Training
Total time taken: 333.65900135040283
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.680
[4,     1] loss: 0.674
[5,     1] loss: 0.667
[6,     1] loss: 0.661
[7,     1] loss: 0.654
[8,     1] loss: 0.647
[9,     1] loss: 0.638
[10,     1] loss: 0.630
[11,     1] loss: 0.617
[12,     1] loss: 0.607
[13,     1] loss: 0.593
[14,     1] loss: 0.578
[15,     1] loss: 0.563
[16,     1] loss: 0.547
[17,     1] loss: 0.526
[18,     1] loss: 0.506
[19,     1] loss: 0.485
[20,     1] loss: 0.463
[21,     1] loss: 0.440
[22,     1] loss: 0.418
[23,     1] loss: 0.397
[24,     1] loss: 0.376
[25,     1] loss: 0.351
[26,     1] loss: 0.332
[27,     1] loss: 0.312
[28,     1] loss: 0.301
[29,     1] loss: 0.275
[30,     1] loss: 0.261
[31,     1] loss: 0.244
[32,     1] loss: 0.230
[33,     1] loss: 0.212
[34,     1] loss: 0.204
[35,     1] loss: 0.188
[36,     1] loss: 0.176
[37,     1] loss: 0.165
[38,     1] loss: 0.155
[39,     1] loss: 0.150
[40,     1] loss: 0.141
[41,     1] loss: 0.135
[42,     1] loss: 0.131
[43,     1] loss: 0.127
[44,     1] loss: 0.122
[45,     1] loss: 0.118
[46,     1] loss: 0.116
[47,     1] loss: 0.114
[48,     1] loss: 0.113
[49,     1] loss: 0.111
[50,     1] loss: 0.109
[51,     1] loss: 0.107
[52,     1] loss: 0.108
[53,     1] loss: 0.107
[54,     1] loss: 0.106
[55,     1] loss: 0.108
[56,     1] loss: 0.105
[57,     1] loss: 0.106
[58,     1] loss: 0.108
[59,     1] loss: 0.106
[60,     1] loss: 0.105
[61,     1] loss: 0.106
[62,     1] loss: 0.105
[63,     1] loss: 0.105
[64,     1] loss: 0.141
[65,     1] loss: 0.326
[66,     1] loss: 0.527
[67,     1] loss: 0.453
[68,     1] loss: 0.400
[69,     1] loss: 0.377
[70,     1] loss: 0.371
[71,     1] loss: 0.364
[72,     1] loss: 0.357
[73,     1] loss: 0.345
[74,     1] loss: 0.330
[75,     1] loss: 0.314
[76,     1] loss: 0.304
[77,     1] loss: 0.292
[78,     1] loss: 0.280
[79,     1] loss: 0.265
[80,     1] loss: 0.251
[81,     1] loss: 0.237
[82,     1] loss: 0.226
[83,     1] loss: 0.214
[84,     1] loss: 0.202
[85,     1] loss: 0.193
[86,     1] loss: 0.180
[87,     1] loss: 0.167
[88,     1] loss: 0.159
[89,     1] loss: 0.155
[90,     1] loss: 0.143
[91,     1] loss: 0.135
[92,     1] loss: 0.130
[93,     1] loss: 0.124
[94,     1] loss: 0.119
[95,     1] loss: 0.111
[96,     1] loss: 0.111
[97,     1] loss: 0.110
[98,     1] loss: 0.106
[99,     1] loss: 0.104
[100,     1] loss: 0.098
[101,     1] loss: 0.098
[102,     1] loss: 0.099
[103,     1] loss: 0.096
[104,     1] loss: 0.098
[105,     1] loss: 0.099
[106,     1] loss: 0.098
[107,     1] loss: 0.097
[108,     1] loss: 0.096
[109,     1] loss: 0.100
[110,     1] loss: 0.098
[111,     1] loss: 0.099
[112,     1] loss: 0.100
[113,     1] loss: 0.099
[114,     1] loss: 0.099
[115,     1] loss: 0.099
[116,     1] loss: 0.100
[117,     1] loss: 0.100
[118,     1] loss: 0.102
[119,     1] loss: 0.096
[120,     1] loss: 0.099
[121,     1] loss: 0.100
[122,     1] loss: 0.099
[123,     1] loss: 0.099
[124,     1] loss: 0.098
[125,     1] loss: 0.098
[126,     1] loss: 0.098
[127,     1] loss: 0.098
[128,     1] loss: 0.096
[129,     1] loss: 0.095
[130,     1] loss: 0.096
[131,     1] loss: 0.094
[132,     1] loss: 0.094
[133,     1] loss: 0.095
[134,     1] loss: 0.111
[135,     1] loss: 0.511
[136,     1] loss: 1.020
[137,     1] loss: 0.664
[138,     1] loss: 0.623
[139,     1] loss: 0.579
[140,     1] loss: 0.496
[141,     1] loss: 0.504
[142,     1] loss: 0.524
[143,     1] loss: 0.532
[144,     1] loss: 0.537
[145,     1] loss: 0.537
[146,     1] loss: 0.539
[147,     1] loss: 0.541
Early stopping applied (best metric=0.3738105297088623)
Finished Training
Total time taken: 479.4540967941284
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.660
[4,     1] loss: 0.647
[5,     1] loss: 0.636
[6,     1] loss: 0.622
[7,     1] loss: 0.611
[8,     1] loss: 0.599
[9,     1] loss: 0.582
[10,     1] loss: 0.565
[11,     1] loss: 0.550
[12,     1] loss: 0.532
[13,     1] loss: 0.517
[14,     1] loss: 0.496
[15,     1] loss: 0.479
[16,     1] loss: 0.462
[17,     1] loss: 0.447
[18,     1] loss: 0.435
[19,     1] loss: 0.417
[20,     1] loss: 0.400
[21,     1] loss: 0.389
[22,     1] loss: 0.376
[23,     1] loss: 0.366
[24,     1] loss: 0.356
[25,     1] loss: 0.346
[26,     1] loss: 0.336
[27,     1] loss: 0.328
[28,     1] loss: 0.322
[29,     1] loss: 0.315
[30,     1] loss: 0.308
[31,     1] loss: 0.302
[32,     1] loss: 0.296
[33,     1] loss: 0.289
[34,     1] loss: 0.285
[35,     1] loss: 0.279
[36,     1] loss: 0.272
[37,     1] loss: 0.264
[38,     1] loss: 0.256
[39,     1] loss: 0.249
[40,     1] loss: 0.243
[41,     1] loss: 0.236
[42,     1] loss: 0.230
[43,     1] loss: 0.223
[44,     1] loss: 0.218
[45,     1] loss: 0.210
[46,     1] loss: 0.205
[47,     1] loss: 0.196
[48,     1] loss: 0.190
[49,     1] loss: 0.184
[50,     1] loss: 0.176
[51,     1] loss: 0.170
[52,     1] loss: 0.161
[53,     1] loss: 0.154
[54,     1] loss: 0.148
[55,     1] loss: 0.140
[56,     1] loss: 0.137
[57,     1] loss: 0.132
[58,     1] loss: 0.128
[59,     1] loss: 0.124
[60,     1] loss: 0.121
[61,     1] loss: 0.115
[62,     1] loss: 0.112
[63,     1] loss: 0.109
[64,     1] loss: 0.104
[65,     1] loss: 0.103
[66,     1] loss: 0.100
[67,     1] loss: 0.096
[68,     1] loss: 0.092
[69,     1] loss: 0.122
[70,     1] loss: 0.291
[71,     1] loss: 0.522
[72,     1] loss: 0.405
[73,     1] loss: 0.399
[74,     1] loss: 0.431
[75,     1] loss: 0.394
[76,     1] loss: 0.396
[77,     1] loss: 0.379
[78,     1] loss: 0.380
[79,     1] loss: 0.376
[80,     1] loss: 0.374
[81,     1] loss: 0.363
[82,     1] loss: 0.356
[83,     1] loss: 0.344
[84,     1] loss: 0.335
[85,     1] loss: 0.321
[86,     1] loss: 0.307
[87,     1] loss: 0.291
[88,     1] loss: 0.275
[89,     1] loss: 0.262
[90,     1] loss: 0.250
[91,     1] loss: 0.238
[92,     1] loss: 0.225
[93,     1] loss: 0.216
[94,     1] loss: 0.203
[95,     1] loss: 0.194
[96,     1] loss: 0.182
[97,     1] loss: 0.175
[98,     1] loss: 0.165
[99,     1] loss: 0.160
[100,     1] loss: 0.152
[101,     1] loss: 0.146
[102,     1] loss: 0.140
[103,     1] loss: 0.135
[104,     1] loss: 0.130
[105,     1] loss: 0.127
[106,     1] loss: 0.122
[107,     1] loss: 0.121
[108,     1] loss: 0.117
[109,     1] loss: 0.115
[110,     1] loss: 0.114
[111,     1] loss: 0.110
[112,     1] loss: 0.109
[113,     1] loss: 0.109
[114,     1] loss: 0.106
[115,     1] loss: 0.106
[116,     1] loss: 0.105
[117,     1] loss: 0.106
[118,     1] loss: 0.103
[119,     1] loss: 0.104
[120,     1] loss: 0.103
[121,     1] loss: 0.104
[122,     1] loss: 0.101
[123,     1] loss: 0.101
[124,     1] loss: 0.101
[125,     1] loss: 0.099
[126,     1] loss: 0.102
[127,     1] loss: 0.099
[128,     1] loss: 0.101
[129,     1] loss: 0.099
[130,     1] loss: 0.100
[131,     1] loss: 0.099
[132,     1] loss: 0.099
[133,     1] loss: 0.100
[134,     1] loss: 0.190
[135,     1] loss: 0.270
[136,     1] loss: 0.343
[137,     1] loss: 0.230
[138,     1] loss: 0.327
[139,     1] loss: 0.355
[140,     1] loss: 0.293
[141,     1] loss: 0.301
[142,     1] loss: 0.286
[143,     1] loss: 0.249
[144,     1] loss: 0.217
[145,     1] loss: 0.219
[146,     1] loss: 0.206
[147,     1] loss: 0.198
[148,     1] loss: 0.185
[149,     1] loss: 0.169
[150,     1] loss: 0.163
[151,     1] loss: 0.160
[152,     1] loss: 0.156
[153,     1] loss: 0.148
[154,     1] loss: 0.140
[155,     1] loss: 0.138
[156,     1] loss: 0.131
[157,     1] loss: 0.130
[158,     1] loss: 0.126
[159,     1] loss: 0.123
[160,     1] loss: 0.122
[161,     1] loss: 0.120
[162,     1] loss: 0.120
[163,     1] loss: 0.120
[164,     1] loss: 0.122
[165,     1] loss: 0.121
[166,     1] loss: 0.119
[167,     1] loss: 0.121
[168,     1] loss: 0.121
[169,     1] loss: 0.124
[170,     1] loss: 0.123
[171,     1] loss: 0.123
[172,     1] loss: 0.125
[173,     1] loss: 0.124
[174,     1] loss: 0.123
[175,     1] loss: 0.124
[176,     1] loss: 0.123
[177,     1] loss: 0.124
[178,     1] loss: 0.123
[179,     1] loss: 0.122
[180,     1] loss: 0.126
[181,     1] loss: 0.124
[182,     1] loss: 0.125
[183,     1] loss: 0.123
[184,     1] loss: 0.124
[185,     1] loss: 0.125
[186,     1] loss: 0.129
[187,     1] loss: 0.267
[188,     1] loss: 0.347
[189,     1] loss: 0.748
[190,     1] loss: 0.571
[191,     1] loss: 0.563
Early stopping applied (best metric=0.4141988158226013)
Finished Training
Total time taken: 624.7662301063538
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.663
[4,     1] loss: 0.650
[5,     1] loss: 0.639
[6,     1] loss: 0.625
[7,     1] loss: 0.614
[8,     1] loss: 0.601
[9,     1] loss: 0.588
[10,     1] loss: 0.572
[11,     1] loss: 0.558
[12,     1] loss: 0.541
[13,     1] loss: 0.525
[14,     1] loss: 0.503
[15,     1] loss: 0.483
[16,     1] loss: 0.465
[17,     1] loss: 0.448
[18,     1] loss: 0.425
[19,     1] loss: 0.405
[20,     1] loss: 0.385
[21,     1] loss: 0.370
[22,     1] loss: 0.349
[23,     1] loss: 0.332
[24,     1] loss: 0.313
[25,     1] loss: 0.298
[26,     1] loss: 0.281
[27,     1] loss: 0.269
[28,     1] loss: 0.250
[29,     1] loss: 0.240
[30,     1] loss: 0.226
[31,     1] loss: 0.215
[32,     1] loss: 0.205
[33,     1] loss: 0.196
[34,     1] loss: 0.187
[35,     1] loss: 0.178
[36,     1] loss: 0.174
[37,     1] loss: 0.167
[38,     1] loss: 0.162
[39,     1] loss: 0.156
[40,     1] loss: 0.153
[41,     1] loss: 0.151
[42,     1] loss: 0.146
[43,     1] loss: 0.143
[44,     1] loss: 0.142
[45,     1] loss: 0.139
[46,     1] loss: 0.140
[47,     1] loss: 0.139
[48,     1] loss: 0.135
[49,     1] loss: 0.136
[50,     1] loss: 0.136
[51,     1] loss: 0.134
[52,     1] loss: 0.134
[53,     1] loss: 0.134
[54,     1] loss: 0.135
[55,     1] loss: 0.136
[56,     1] loss: 0.138
[57,     1] loss: 0.156
[58,     1] loss: 0.195
[59,     1] loss: 0.210
[60,     1] loss: 0.488
[61,     1] loss: 0.307
[62,     1] loss: 0.270
[63,     1] loss: 0.307
[64,     1] loss: 0.252
[65,     1] loss: 0.250
[66,     1] loss: 0.256
[67,     1] loss: 0.246
[68,     1] loss: 0.236
[69,     1] loss: 0.234
[70,     1] loss: 0.227
[71,     1] loss: 0.223
[72,     1] loss: 0.220
[73,     1] loss: 0.217
[74,     1] loss: 0.212
[75,     1] loss: 0.211
[76,     1] loss: 0.206
[77,     1] loss: 0.201
[78,     1] loss: 0.198
[79,     1] loss: 0.194
[80,     1] loss: 0.190
[81,     1] loss: 0.187
[82,     1] loss: 0.181
[83,     1] loss: 0.178
[84,     1] loss: 0.171
[85,     1] loss: 0.165
[86,     1] loss: 0.157
[87,     1] loss: 0.153
[88,     1] loss: 0.150
[89,     1] loss: 0.146
[90,     1] loss: 0.140
[91,     1] loss: 0.138
[92,     1] loss: 0.136
[93,     1] loss: 0.131
[94,     1] loss: 0.129
[95,     1] loss: 0.127
Early stopping applied (best metric=0.3520335853099823)
Finished Training
Total time taken: 312.57292580604553
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.678
[3,     1] loss: 0.663
[4,     1] loss: 0.649
[5,     1] loss: 0.638
[6,     1] loss: 0.624
[7,     1] loss: 0.612
[8,     1] loss: 0.598
[9,     1] loss: 0.584
[10,     1] loss: 0.566
[11,     1] loss: 0.549
[12,     1] loss: 0.528
[13,     1] loss: 0.510
[14,     1] loss: 0.486
[15,     1] loss: 0.463
[16,     1] loss: 0.442
[17,     1] loss: 0.422
[18,     1] loss: 0.401
[19,     1] loss: 0.377
[20,     1] loss: 0.356
[21,     1] loss: 0.337
[22,     1] loss: 0.314
[23,     1] loss: 0.296
[24,     1] loss: 0.278
[25,     1] loss: 0.260
[26,     1] loss: 0.244
[27,     1] loss: 0.230
[28,     1] loss: 0.215
[29,     1] loss: 0.205
[30,     1] loss: 0.194
[31,     1] loss: 0.186
[32,     1] loss: 0.177
[33,     1] loss: 0.170
[34,     1] loss: 0.164
[35,     1] loss: 0.160
[36,     1] loss: 0.154
[37,     1] loss: 0.149
[38,     1] loss: 0.145
[39,     1] loss: 0.143
[40,     1] loss: 0.139
[41,     1] loss: 0.138
[42,     1] loss: 0.134
[43,     1] loss: 0.134
[44,     1] loss: 0.131
[45,     1] loss: 0.132
[46,     1] loss: 0.131
[47,     1] loss: 0.129
[48,     1] loss: 0.130
[49,     1] loss: 0.130
[50,     1] loss: 0.129
[51,     1] loss: 0.129
[52,     1] loss: 0.127
[53,     1] loss: 0.130
[54,     1] loss: 0.129
[55,     1] loss: 0.130
[56,     1] loss: 0.132
[57,     1] loss: 0.164
[58,     1] loss: 0.184
[59,     1] loss: 0.231
[60,     1] loss: 0.389
[61,     1] loss: 0.295
[62,     1] loss: 0.286
[63,     1] loss: 0.281
[64,     1] loss: 0.249
[65,     1] loss: 0.256
[66,     1] loss: 0.257
[67,     1] loss: 0.245
[68,     1] loss: 0.240
[69,     1] loss: 0.238
[70,     1] loss: 0.232
[71,     1] loss: 0.226
[72,     1] loss: 0.219
[73,     1] loss: 0.213
[74,     1] loss: 0.210
[75,     1] loss: 0.205
[76,     1] loss: 0.198
[77,     1] loss: 0.192
[78,     1] loss: 0.188
[79,     1] loss: 0.180
[80,     1] loss: 0.172
[81,     1] loss: 0.167
[82,     1] loss: 0.162
[83,     1] loss: 0.155
[84,     1] loss: 0.150
[85,     1] loss: 0.144
[86,     1] loss: 0.139
[87,     1] loss: 0.138
[88,     1] loss: 0.134
[89,     1] loss: 0.130
[90,     1] loss: 0.126
[91,     1] loss: 0.124
[92,     1] loss: 0.121
[93,     1] loss: 0.117
[94,     1] loss: 0.117
[95,     1] loss: 0.115
[96,     1] loss: 0.115
[97,     1] loss: 0.113
[98,     1] loss: 0.114
[99,     1] loss: 0.112
[100,     1] loss: 0.112
[101,     1] loss: 0.111
[102,     1] loss: 0.110
[103,     1] loss: 0.110
[104,     1] loss: 0.112
[105,     1] loss: 0.110
[106,     1] loss: 0.110
[107,     1] loss: 0.112
[108,     1] loss: 0.110
[109,     1] loss: 0.112
[110,     1] loss: 0.113
Early stopping applied (best metric=0.41657745838165283)
Finished Training
Total time taken: 363.09177350997925
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.662
[4,     1] loss: 0.647
[5,     1] loss: 0.633
[6,     1] loss: 0.620
[7,     1] loss: 0.605
[8,     1] loss: 0.588
[9,     1] loss: 0.573
[10,     1] loss: 0.557
[11,     1] loss: 0.538
[12,     1] loss: 0.521
[13,     1] loss: 0.497
[14,     1] loss: 0.476
[15,     1] loss: 0.456
[16,     1] loss: 0.433
[17,     1] loss: 0.412
[18,     1] loss: 0.391
[19,     1] loss: 0.369
[20,     1] loss: 0.348
[21,     1] loss: 0.331
[22,     1] loss: 0.313
[23,     1] loss: 0.293
[24,     1] loss: 0.279
[25,     1] loss: 0.260
[26,     1] loss: 0.243
[27,     1] loss: 0.231
[28,     1] loss: 0.218
[29,     1] loss: 0.205
[30,     1] loss: 0.195
[31,     1] loss: 0.183
[32,     1] loss: 0.176
[33,     1] loss: 0.165
[34,     1] loss: 0.158
[35,     1] loss: 0.155
[36,     1] loss: 0.146
[37,     1] loss: 0.141
[38,     1] loss: 0.137
[39,     1] loss: 0.132
[40,     1] loss: 0.130
[41,     1] loss: 0.127
[42,     1] loss: 0.125
[43,     1] loss: 0.123
[44,     1] loss: 0.120
[45,     1] loss: 0.120
[46,     1] loss: 0.116
[47,     1] loss: 0.115
[48,     1] loss: 0.116
[49,     1] loss: 0.117
[50,     1] loss: 0.115
[51,     1] loss: 0.115
[52,     1] loss: 0.116
[53,     1] loss: 0.114
[54,     1] loss: 0.113
[55,     1] loss: 0.114
[56,     1] loss: 0.112
[57,     1] loss: 0.113
[58,     1] loss: 0.127
[59,     1] loss: 0.210
[60,     1] loss: 0.609
[61,     1] loss: 0.422
[62,     1] loss: 0.286
[63,     1] loss: 0.305
[64,     1] loss: 0.288
[65,     1] loss: 0.270
[66,     1] loss: 0.267
[67,     1] loss: 0.257
[68,     1] loss: 0.244
[69,     1] loss: 0.233
[70,     1] loss: 0.221
[71,     1] loss: 0.212
[72,     1] loss: 0.208
[73,     1] loss: 0.197
[74,     1] loss: 0.189
[75,     1] loss: 0.183
[76,     1] loss: 0.178
[77,     1] loss: 0.166
[78,     1] loss: 0.162
[79,     1] loss: 0.155
[80,     1] loss: 0.150
[81,     1] loss: 0.145
[82,     1] loss: 0.138
[83,     1] loss: 0.132
[84,     1] loss: 0.126
[85,     1] loss: 0.121
[86,     1] loss: 0.120
[87,     1] loss: 0.113
[88,     1] loss: 0.111
[89,     1] loss: 0.111
[90,     1] loss: 0.105
[91,     1] loss: 0.106
[92,     1] loss: 0.104
[93,     1] loss: 0.103
[94,     1] loss: 0.100
[95,     1] loss: 0.101
[96,     1] loss: 0.100
[97,     1] loss: 0.100
[98,     1] loss: 0.100
[99,     1] loss: 0.101
[100,     1] loss: 0.099
[101,     1] loss: 0.100
[102,     1] loss: 0.101
[103,     1] loss: 0.102
[104,     1] loss: 0.102
[105,     1] loss: 0.103
[106,     1] loss: 0.100
[107,     1] loss: 0.102
[108,     1] loss: 0.103
[109,     1] loss: 0.103
[110,     1] loss: 0.103
[111,     1] loss: 0.104
[112,     1] loss: 0.103
[113,     1] loss: 0.102
[114,     1] loss: 0.102
[115,     1] loss: 0.102
[116,     1] loss: 0.106
[117,     1] loss: 0.103
[118,     1] loss: 0.101
[119,     1] loss: 0.104
[120,     1] loss: 0.104
[121,     1] loss: 0.104
[122,     1] loss: 0.142
[123,     1] loss: 0.292
Early stopping applied (best metric=0.3927801549434662)
Finished Training
Total time taken: 406.2501711845398
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.665
[5,     1] loss: 0.657
[6,     1] loss: 0.648
[7,     1] loss: 0.639
[8,     1] loss: 0.629
[9,     1] loss: 0.618
[10,     1] loss: 0.606
[11,     1] loss: 0.595
[12,     1] loss: 0.582
[13,     1] loss: 0.568
[14,     1] loss: 0.552
[15,     1] loss: 0.536
[16,     1] loss: 0.522
[17,     1] loss: 0.506
[18,     1] loss: 0.485
[19,     1] loss: 0.466
[20,     1] loss: 0.447
[21,     1] loss: 0.430
[22,     1] loss: 0.412
[23,     1] loss: 0.396
[24,     1] loss: 0.380
[25,     1] loss: 0.364
[26,     1] loss: 0.351
[27,     1] loss: 0.337
[28,     1] loss: 0.324
[29,     1] loss: 0.310
[30,     1] loss: 0.298
[31,     1] loss: 0.288
[32,     1] loss: 0.279
[33,     1] loss: 0.269
[34,     1] loss: 0.259
[35,     1] loss: 0.255
[36,     1] loss: 0.248
[37,     1] loss: 0.241
[38,     1] loss: 0.234
[39,     1] loss: 0.229
[40,     1] loss: 0.227
[41,     1] loss: 0.222
[42,     1] loss: 0.216
[43,     1] loss: 0.213
[44,     1] loss: 0.210
[45,     1] loss: 0.207
[46,     1] loss: 0.206
[47,     1] loss: 0.203
[48,     1] loss: 0.200
[49,     1] loss: 0.199
[50,     1] loss: 0.197
[51,     1] loss: 0.196
[52,     1] loss: 0.196
[53,     1] loss: 0.195
[54,     1] loss: 0.194
[55,     1] loss: 0.191
[56,     1] loss: 0.190
[57,     1] loss: 0.189
[58,     1] loss: 0.187
[59,     1] loss: 0.183
[60,     1] loss: 0.184
[61,     1] loss: 0.180
[62,     1] loss: 0.177
[63,     1] loss: 0.175
[64,     1] loss: 0.268
[65,     1] loss: 0.551
[66,     1] loss: 0.528
[67,     1] loss: 0.498
[68,     1] loss: 0.472
[69,     1] loss: 0.438
[70,     1] loss: 0.414
[71,     1] loss: 0.405
[72,     1] loss: 0.396
[73,     1] loss: 0.387
[74,     1] loss: 0.377
[75,     1] loss: 0.369
[76,     1] loss: 0.358
[77,     1] loss: 0.351
[78,     1] loss: 0.338
[79,     1] loss: 0.327
[80,     1] loss: 0.320
[81,     1] loss: 0.308
[82,     1] loss: 0.297
[83,     1] loss: 0.289
[84,     1] loss: 0.281
[85,     1] loss: 0.275
[86,     1] loss: 0.267
[87,     1] loss: 0.260
[88,     1] loss: 0.261
[89,     1] loss: 0.253
[90,     1] loss: 0.250
[91,     1] loss: 0.244
[92,     1] loss: 0.245
[93,     1] loss: 0.237
[94,     1] loss: 0.235
[95,     1] loss: 0.236
[96,     1] loss: 0.231
[97,     1] loss: 0.230
[98,     1] loss: 0.228
[99,     1] loss: 0.226
[100,     1] loss: 0.223
[101,     1] loss: 0.222
[102,     1] loss: 0.222
[103,     1] loss: 0.220
[104,     1] loss: 0.219
[105,     1] loss: 0.216
[106,     1] loss: 0.218
[107,     1] loss: 0.215
[108,     1] loss: 0.212
[109,     1] loss: 0.211
[110,     1] loss: 0.209
[111,     1] loss: 0.210
[112,     1] loss: 0.207
[113,     1] loss: 0.206
[114,     1] loss: 0.206
[115,     1] loss: 0.203
[116,     1] loss: 0.203
[117,     1] loss: 0.198
[118,     1] loss: 0.199
[119,     1] loss: 0.198
[120,     1] loss: 0.195
[121,     1] loss: 0.193
[122,     1] loss: 0.194
[123,     1] loss: 0.193
[124,     1] loss: 0.189
[125,     1] loss: 0.191
[126,     1] loss: 0.190
[127,     1] loss: 0.187
[128,     1] loss: 0.186
[129,     1] loss: 0.184
Early stopping applied (best metric=0.42366406321525574)
Finished Training
Total time taken: 426.82699608802795
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.660
[4,     1] loss: 0.647
[5,     1] loss: 0.635
[6,     1] loss: 0.623
[7,     1] loss: 0.609
[8,     1] loss: 0.597
[9,     1] loss: 0.582
[10,     1] loss: 0.566
[11,     1] loss: 0.548
[12,     1] loss: 0.532
[13,     1] loss: 0.514
[14,     1] loss: 0.492
[15,     1] loss: 0.470
[16,     1] loss: 0.449
[17,     1] loss: 0.424
[18,     1] loss: 0.400
[19,     1] loss: 0.375
[20,     1] loss: 0.352
[21,     1] loss: 0.329
[22,     1] loss: 0.310
[23,     1] loss: 0.290
[24,     1] loss: 0.271
[25,     1] loss: 0.252
[26,     1] loss: 0.233
[27,     1] loss: 0.222
[28,     1] loss: 0.209
[29,     1] loss: 0.199
[30,     1] loss: 0.187
[31,     1] loss: 0.179
[32,     1] loss: 0.168
[33,     1] loss: 0.164
[34,     1] loss: 0.157
[35,     1] loss: 0.152
[36,     1] loss: 0.147
[37,     1] loss: 0.140
[38,     1] loss: 0.139
[39,     1] loss: 0.135
[40,     1] loss: 0.131
[41,     1] loss: 0.128
[42,     1] loss: 0.125
[43,     1] loss: 0.127
[44,     1] loss: 0.123
[45,     1] loss: 0.122
[46,     1] loss: 0.121
[47,     1] loss: 0.120
[48,     1] loss: 0.119
[49,     1] loss: 0.120
[50,     1] loss: 0.120
[51,     1] loss: 0.119
[52,     1] loss: 0.119
[53,     1] loss: 0.118
[54,     1] loss: 0.116
[55,     1] loss: 0.118
[56,     1] loss: 0.118
[57,     1] loss: 0.116
[58,     1] loss: 0.119
[59,     1] loss: 0.118
[60,     1] loss: 0.118
[61,     1] loss: 0.116
[62,     1] loss: 0.137
[63,     1] loss: 0.262
[64,     1] loss: 0.485
[65,     1] loss: 0.415
[66,     1] loss: 0.383
[67,     1] loss: 0.354
Early stopping applied (best metric=0.4387454688549042)
Finished Training
Total time taken: 223.36445140838623
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.665
[4,     1] loss: 0.652
[5,     1] loss: 0.641
[6,     1] loss: 0.631
[7,     1] loss: 0.618
[8,     1] loss: 0.606
[9,     1] loss: 0.595
[10,     1] loss: 0.583
[11,     1] loss: 0.569
[12,     1] loss: 0.557
[13,     1] loss: 0.542
[14,     1] loss: 0.527
[15,     1] loss: 0.512
[16,     1] loss: 0.494
[17,     1] loss: 0.476
[18,     1] loss: 0.457
[19,     1] loss: 0.438
[20,     1] loss: 0.418
[21,     1] loss: 0.405
[22,     1] loss: 0.378
[23,     1] loss: 0.361
[24,     1] loss: 0.343
[25,     1] loss: 0.325
[26,     1] loss: 0.305
[27,     1] loss: 0.290
[28,     1] loss: 0.272
[29,     1] loss: 0.255
[30,     1] loss: 0.241
[31,     1] loss: 0.227
[32,     1] loss: 0.215
[33,     1] loss: 0.202
[34,     1] loss: 0.189
[35,     1] loss: 0.176
[36,     1] loss: 0.166
[37,     1] loss: 0.156
[38,     1] loss: 0.148
[39,     1] loss: 0.140
[40,     1] loss: 0.135
[41,     1] loss: 0.127
[42,     1] loss: 0.122
[43,     1] loss: 0.116
[44,     1] loss: 0.115
[45,     1] loss: 0.109
[46,     1] loss: 0.105
[47,     1] loss: 0.104
[48,     1] loss: 0.100
[49,     1] loss: 0.098
[50,     1] loss: 0.097
[51,     1] loss: 0.096
[52,     1] loss: 0.095
[53,     1] loss: 0.093
[54,     1] loss: 0.093
[55,     1] loss: 0.091
[56,     1] loss: 0.091
[57,     1] loss: 0.091
[58,     1] loss: 0.091
[59,     1] loss: 0.090
[60,     1] loss: 0.087
[61,     1] loss: 0.088
[62,     1] loss: 0.086
[63,     1] loss: 0.086
[64,     1] loss: 0.081
[65,     1] loss: 0.080
[66,     1] loss: 0.078
[67,     1] loss: 0.077
[68,     1] loss: 0.094
[69,     1] loss: 0.367
[70,     1] loss: 0.451
[71,     1] loss: 0.449
[72,     1] loss: 0.383
[73,     1] loss: 0.317
[74,     1] loss: 0.309
[75,     1] loss: 0.315
[76,     1] loss: 0.296
[77,     1] loss: 0.281
[78,     1] loss: 0.262
Early stopping applied (best metric=0.3976747989654541)
Finished Training
Total time taken: 259.98047828674316
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.692
[3,     1] loss: 0.677
[4,     1] loss: 0.664
[5,     1] loss: 0.651
[6,     1] loss: 0.638
[7,     1] loss: 0.625
[8,     1] loss: 0.611
[9,     1] loss: 0.594
[10,     1] loss: 0.578
[11,     1] loss: 0.561
[12,     1] loss: 0.545
[13,     1] loss: 0.529
[14,     1] loss: 0.507
[15,     1] loss: 0.491
[16,     1] loss: 0.469
[17,     1] loss: 0.452
[18,     1] loss: 0.432
[19,     1] loss: 0.416
[20,     1] loss: 0.407
[21,     1] loss: 0.401
[22,     1] loss: 0.382
[23,     1] loss: 0.380
[24,     1] loss: 0.367
[25,     1] loss: 0.362
[26,     1] loss: 0.349
[27,     1] loss: 0.349
[28,     1] loss: 0.337
[29,     1] loss: 0.338
[30,     1] loss: 0.325
[31,     1] loss: 0.319
[32,     1] loss: 0.312
[33,     1] loss: 0.305
[34,     1] loss: 0.300
[35,     1] loss: 0.294
[36,     1] loss: 0.288
[37,     1] loss: 0.284
[38,     1] loss: 0.278
[39,     1] loss: 0.272
[40,     1] loss: 0.269
[41,     1] loss: 0.265
[42,     1] loss: 0.257
[43,     1] loss: 0.252
[44,     1] loss: 0.246
[45,     1] loss: 0.241
[46,     1] loss: 0.234
[47,     1] loss: 0.228
[48,     1] loss: 0.224
[49,     1] loss: 0.219
[50,     1] loss: 0.213
[51,     1] loss: 0.204
[52,     1] loss: 0.198
[53,     1] loss: 0.194
[54,     1] loss: 0.187
[55,     1] loss: 0.181
[56,     1] loss: 0.174
[57,     1] loss: 0.169
[58,     1] loss: 0.162
[59,     1] loss: 0.158
[60,     1] loss: 0.152
[61,     1] loss: 0.146
[62,     1] loss: 0.143
[63,     1] loss: 0.137
[64,     1] loss: 0.130
[65,     1] loss: 0.124
[66,     1] loss: 0.119
[67,     1] loss: 0.114
[68,     1] loss: 0.112
[69,     1] loss: 0.204
[70,     1] loss: 0.462
[71,     1] loss: 0.349
[72,     1] loss: 0.344
[73,     1] loss: 0.338
[74,     1] loss: 0.322
[75,     1] loss: 0.294
[76,     1] loss: 0.284
[77,     1] loss: 0.275
[78,     1] loss: 0.254
[79,     1] loss: 0.264
[80,     1] loss: 0.247
[81,     1] loss: 0.246
[82,     1] loss: 0.229
[83,     1] loss: 0.213
[84,     1] loss: 0.196
[85,     1] loss: 0.189
[86,     1] loss: 0.180
[87,     1] loss: 0.167
[88,     1] loss: 0.161
[89,     1] loss: 0.154
[90,     1] loss: 0.147
[91,     1] loss: 0.141
[92,     1] loss: 0.135
[93,     1] loss: 0.131
[94,     1] loss: 0.127
[95,     1] loss: 0.123
[96,     1] loss: 0.121
[97,     1] loss: 0.118
[98,     1] loss: 0.117
[99,     1] loss: 0.117
[100,     1] loss: 0.115
[101,     1] loss: 0.115
[102,     1] loss: 0.115
[103,     1] loss: 0.115
[104,     1] loss: 0.113
[105,     1] loss: 0.113
[106,     1] loss: 0.112
[107,     1] loss: 0.112
[108,     1] loss: 0.114
[109,     1] loss: 0.113
[110,     1] loss: 0.112
[111,     1] loss: 0.115
[112,     1] loss: 0.115
[113,     1] loss: 0.114
[114,     1] loss: 0.114
[115,     1] loss: 0.114
[116,     1] loss: 0.116
[117,     1] loss: 0.116
[118,     1] loss: 0.112
[119,     1] loss: 0.113
[120,     1] loss: 0.115
[121,     1] loss: 0.113
[122,     1] loss: 0.116
[123,     1] loss: 0.112
[124,     1] loss: 0.113
[125,     1] loss: 0.111
[126,     1] loss: 0.115
[127,     1] loss: 0.113
[128,     1] loss: 0.113
[129,     1] loss: 0.113
[130,     1] loss: 0.112
[131,     1] loss: 0.110
[132,     1] loss: 0.115
[133,     1] loss: 0.114
[134,     1] loss: 0.112
[135,     1] loss: 0.113
[136,     1] loss: 0.112
[137,     1] loss: 0.115
[138,     1] loss: 0.114
[139,     1] loss: 0.150
[140,     1] loss: 0.709
[141,     1] loss: 0.497
Early stopping applied (best metric=0.33952897787094116)
Finished Training
Total time taken: 469.55792903900146
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.678
[3,     1] loss: 0.654
[4,     1] loss: 0.637
[5,     1] loss: 0.620
[6,     1] loss: 0.602
[7,     1] loss: 0.585
[8,     1] loss: 0.569
[9,     1] loss: 0.550
[10,     1] loss: 0.532
[11,     1] loss: 0.510
[12,     1] loss: 0.492
[13,     1] loss: 0.471
[14,     1] loss: 0.445
[15,     1] loss: 0.426
[16,     1] loss: 0.404
[17,     1] loss: 0.381
[18,     1] loss: 0.356
[19,     1] loss: 0.334
[20,     1] loss: 0.314
[21,     1] loss: 0.299
[22,     1] loss: 0.280
[23,     1] loss: 0.261
[24,     1] loss: 0.249
[25,     1] loss: 0.232
[26,     1] loss: 0.218
[27,     1] loss: 0.205
[28,     1] loss: 0.192
[29,     1] loss: 0.181
[30,     1] loss: 0.172
[31,     1] loss: 0.166
[32,     1] loss: 0.155
[33,     1] loss: 0.148
[34,     1] loss: 0.147
[35,     1] loss: 0.140
[36,     1] loss: 0.135
[37,     1] loss: 0.131
[38,     1] loss: 0.128
[39,     1] loss: 0.127
[40,     1] loss: 0.123
[41,     1] loss: 0.122
[42,     1] loss: 0.121
[43,     1] loss: 0.120
[44,     1] loss: 0.121
[45,     1] loss: 0.118
[46,     1] loss: 0.118
[47,     1] loss: 0.118
[48,     1] loss: 0.118
[49,     1] loss: 0.118
[50,     1] loss: 0.115
[51,     1] loss: 0.117
[52,     1] loss: 0.118
[53,     1] loss: 0.119
[54,     1] loss: 0.121
[55,     1] loss: 0.118
[56,     1] loss: 0.117
[57,     1] loss: 0.118
[58,     1] loss: 0.119
[59,     1] loss: 0.120
[60,     1] loss: 0.135
[61,     1] loss: 0.199
[62,     1] loss: 0.206
[63,     1] loss: 0.401
[64,     1] loss: 0.266
[65,     1] loss: 0.303
[66,     1] loss: 0.269
[67,     1] loss: 0.264
[68,     1] loss: 0.258
[69,     1] loss: 0.243
Early stopping applied (best metric=0.4114129841327667)
Finished Training
Total time taken: 231.20627307891846
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.678
[3,     1] loss: 0.658
[4,     1] loss: 0.643
[5,     1] loss: 0.627
[6,     1] loss: 0.613
[7,     1] loss: 0.598
[8,     1] loss: 0.582
[9,     1] loss: 0.563
[10,     1] loss: 0.546
[11,     1] loss: 0.526
[12,     1] loss: 0.506
[13,     1] loss: 0.480
[14,     1] loss: 0.456
[15,     1] loss: 0.430
[16,     1] loss: 0.412
[17,     1] loss: 0.387
[18,     1] loss: 0.364
[19,     1] loss: 0.343
[20,     1] loss: 0.323
[21,     1] loss: 0.302
[22,     1] loss: 0.282
[23,     1] loss: 0.263
[24,     1] loss: 0.247
[25,     1] loss: 0.232
[26,     1] loss: 0.217
[27,     1] loss: 0.203
[28,     1] loss: 0.194
[29,     1] loss: 0.183
[30,     1] loss: 0.171
[31,     1] loss: 0.162
[32,     1] loss: 0.156
[33,     1] loss: 0.147
[34,     1] loss: 0.142
[35,     1] loss: 0.137
[36,     1] loss: 0.133
[37,     1] loss: 0.129
[38,     1] loss: 0.127
[39,     1] loss: 0.123
[40,     1] loss: 0.122
[41,     1] loss: 0.120
[42,     1] loss: 0.119
[43,     1] loss: 0.119
[44,     1] loss: 0.117
[45,     1] loss: 0.118
[46,     1] loss: 0.117
[47,     1] loss: 0.116
[48,     1] loss: 0.117
[49,     1] loss: 0.117
[50,     1] loss: 0.116
[51,     1] loss: 0.117
[52,     1] loss: 0.116
[53,     1] loss: 0.117
[54,     1] loss: 0.119
[55,     1] loss: 0.119
[56,     1] loss: 0.117
[57,     1] loss: 0.115
[58,     1] loss: 0.118
[59,     1] loss: 0.119
[60,     1] loss: 0.117
[61,     1] loss: 0.119
[62,     1] loss: 0.118
[63,     1] loss: 0.122
[64,     1] loss: 0.143
[65,     1] loss: 0.208
[66,     1] loss: 0.473
Early stopping applied (best metric=0.4286971390247345)
Finished Training
Total time taken: 221.5791938304901
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.665
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.627
[7,     1] loss: 0.615
[8,     1] loss: 0.603
[9,     1] loss: 0.591
[10,     1] loss: 0.578
[11,     1] loss: 0.564
[12,     1] loss: 0.551
[13,     1] loss: 0.534
[14,     1] loss: 0.518
[15,     1] loss: 0.499
[16,     1] loss: 0.481
[17,     1] loss: 0.463
[18,     1] loss: 0.441
[19,     1] loss: 0.439
[20,     1] loss: 0.408
[21,     1] loss: 0.401
[22,     1] loss: 0.389
[23,     1] loss: 0.366
[24,     1] loss: 0.360
[25,     1] loss: 0.345
[26,     1] loss: 0.335
[27,     1] loss: 0.327
[28,     1] loss: 0.321
[29,     1] loss: 0.312
[30,     1] loss: 0.307
[31,     1] loss: 0.299
[32,     1] loss: 0.290
[33,     1] loss: 0.284
[34,     1] loss: 0.278
[35,     1] loss: 0.272
[36,     1] loss: 0.268
[37,     1] loss: 0.265
[38,     1] loss: 0.258
[39,     1] loss: 0.251
[40,     1] loss: 0.245
[41,     1] loss: 0.241
[42,     1] loss: 0.234
[43,     1] loss: 0.229
[44,     1] loss: 0.227
[45,     1] loss: 0.220
[46,     1] loss: 0.213
[47,     1] loss: 0.210
[48,     1] loss: 0.206
[49,     1] loss: 0.203
[50,     1] loss: 0.197
[51,     1] loss: 0.193
[52,     1] loss: 0.189
[53,     1] loss: 0.184
[54,     1] loss: 0.180
[55,     1] loss: 0.177
[56,     1] loss: 0.171
[57,     1] loss: 0.168
[58,     1] loss: 0.164
[59,     1] loss: 0.159
[60,     1] loss: 0.156
[61,     1] loss: 0.152
[62,     1] loss: 0.150
[63,     1] loss: 0.144
[64,     1] loss: 0.138
[65,     1] loss: 0.137
[66,     1] loss: 0.129
[67,     1] loss: 0.130
[68,     1] loss: 0.341
[69,     1] loss: 0.443
[70,     1] loss: 0.296
[71,     1] loss: 0.286
[72,     1] loss: 0.287
[73,     1] loss: 0.291
[74,     1] loss: 0.274
[75,     1] loss: 0.262
[76,     1] loss: 0.258
[77,     1] loss: 0.247
[78,     1] loss: 0.236
[79,     1] loss: 0.226
[80,     1] loss: 0.215
[81,     1] loss: 0.203
[82,     1] loss: 0.192
[83,     1] loss: 0.180
[84,     1] loss: 0.168
[85,     1] loss: 0.158
[86,     1] loss: 0.150
[87,     1] loss: 0.145
[88,     1] loss: 0.141
[89,     1] loss: 0.136
[90,     1] loss: 0.131
[91,     1] loss: 0.127
[92,     1] loss: 0.124
[93,     1] loss: 0.122
[94,     1] loss: 0.119
[95,     1] loss: 0.120
[96,     1] loss: 0.117
[97,     1] loss: 0.116
[98,     1] loss: 0.116
Early stopping applied (best metric=0.3913857936859131)
Finished Training
Total time taken: 327.9343173503876
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.683
[3,     1] loss: 0.659
[4,     1] loss: 0.640
[5,     1] loss: 0.620
[6,     1] loss: 0.599
[7,     1] loss: 0.577
[8,     1] loss: 0.557
[9,     1] loss: 0.535
[10,     1] loss: 0.514
[11,     1] loss: 0.489
[12,     1] loss: 0.466
[13,     1] loss: 0.446
[14,     1] loss: 0.421
[15,     1] loss: 0.404
[16,     1] loss: 0.389
[17,     1] loss: 0.376
[18,     1] loss: 0.368
[19,     1] loss: 0.361
[20,     1] loss: 0.353
[21,     1] loss: 0.346
[22,     1] loss: 0.339
[23,     1] loss: 0.335
[24,     1] loss: 0.332
[25,     1] loss: 0.328
[26,     1] loss: 0.325
[27,     1] loss: 0.322
[28,     1] loss: 0.320
[29,     1] loss: 0.316
[30,     1] loss: 0.313
[31,     1] loss: 0.310
[32,     1] loss: 0.304
[33,     1] loss: 0.302
[34,     1] loss: 0.299
[35,     1] loss: 0.295
[36,     1] loss: 0.292
[37,     1] loss: 0.288
[38,     1] loss: 0.284
[39,     1] loss: 0.279
[40,     1] loss: 0.276
[41,     1] loss: 0.271
[42,     1] loss: 0.267
[43,     1] loss: 0.263
[44,     1] loss: 0.258
[45,     1] loss: 0.255
[46,     1] loss: 0.250
[47,     1] loss: 0.245
[48,     1] loss: 0.242
[49,     1] loss: 0.239
[50,     1] loss: 0.234
[51,     1] loss: 0.231
[52,     1] loss: 0.228
[53,     1] loss: 0.220
[54,     1] loss: 0.218
[55,     1] loss: 0.215
[56,     1] loss: 0.211
[57,     1] loss: 0.208
[58,     1] loss: 0.206
[59,     1] loss: 0.203
[60,     1] loss: 0.199
[61,     1] loss: 0.197
[62,     1] loss: 0.194
[63,     1] loss: 0.192
[64,     1] loss: 0.191
[65,     1] loss: 0.188
[66,     1] loss: 0.186
[67,     1] loss: 0.205
[68,     1] loss: 0.204
[69,     1] loss: 0.290
[70,     1] loss: 0.290
[71,     1] loss: 0.383
[72,     1] loss: 0.317
[73,     1] loss: 0.377
[74,     1] loss: 0.337
[75,     1] loss: 0.333
[76,     1] loss: 0.316
[77,     1] loss: 0.293
[78,     1] loss: 0.289
[79,     1] loss: 0.281
[80,     1] loss: 0.268
[81,     1] loss: 0.259
[82,     1] loss: 0.258
[83,     1] loss: 0.250
[84,     1] loss: 0.246
[85,     1] loss: 0.240
[86,     1] loss: 0.237
[87,     1] loss: 0.230
[88,     1] loss: 0.226
[89,     1] loss: 0.221
[90,     1] loss: 0.215
[91,     1] loss: 0.213
[92,     1] loss: 0.207
[93,     1] loss: 0.206
[94,     1] loss: 0.198
[95,     1] loss: 0.195
[96,     1] loss: 0.189
[97,     1] loss: 0.187
[98,     1] loss: 0.181
[99,     1] loss: 0.177
[100,     1] loss: 0.174
[101,     1] loss: 0.173
[102,     1] loss: 0.167
[103,     1] loss: 0.165
[104,     1] loss: 0.163
[105,     1] loss: 0.161
[106,     1] loss: 0.157
[107,     1] loss: 0.156
[108,     1] loss: 0.156
[109,     1] loss: 0.149
[110,     1] loss: 0.151
[111,     1] loss: 0.151
[112,     1] loss: 0.148
[113,     1] loss: 0.148
[114,     1] loss: 0.149
[115,     1] loss: 0.147
[116,     1] loss: 0.144
[117,     1] loss: 0.145
[118,     1] loss: 0.146
[119,     1] loss: 0.144
[120,     1] loss: 0.144
[121,     1] loss: 0.145
[122,     1] loss: 0.143
[123,     1] loss: 0.144
[124,     1] loss: 0.144
[125,     1] loss: 0.141
[126,     1] loss: 0.141
[127,     1] loss: 0.143
[128,     1] loss: 0.141
[129,     1] loss: 0.142
[130,     1] loss: 0.144
[131,     1] loss: 0.141
[132,     1] loss: 0.142
[133,     1] loss: 0.142
[134,     1] loss: 0.143
[135,     1] loss: 0.145
[136,     1] loss: 0.144
[137,     1] loss: 0.144
[138,     1] loss: 0.147
[139,     1] loss: 0.146
[140,     1] loss: 0.181
[141,     1] loss: 0.238
[142,     1] loss: 0.562
[143,     1] loss: 0.592
[144,     1] loss: 0.498
[145,     1] loss: 0.449
[146,     1] loss: 0.408
[147,     1] loss: 0.390
[148,     1] loss: 0.387
[149,     1] loss: 0.383
[150,     1] loss: 0.388
[151,     1] loss: 0.379
[152,     1] loss: 0.383
[153,     1] loss: 0.382
[154,     1] loss: 0.378
[155,     1] loss: 0.417
[156,     1] loss: 0.385
[157,     1] loss: 0.407
[158,     1] loss: 0.381
[159,     1] loss: 0.395
[160,     1] loss: 0.385
[161,     1] loss: 0.370
[162,     1] loss: 0.370
[163,     1] loss: 0.365
[164,     1] loss: 0.355
[165,     1] loss: 0.353
Early stopping applied (best metric=0.40888598561286926)
Finished Training
Total time taken: 551.5084249973297
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.702
[3,     1] loss: 0.692
[4,     1] loss: 0.681
[5,     1] loss: 0.674
[6,     1] loss: 0.666
[7,     1] loss: 0.658
[8,     1] loss: 0.650
[9,     1] loss: 0.641
[10,     1] loss: 0.629
[11,     1] loss: 0.618
[12,     1] loss: 0.603
[13,     1] loss: 0.588
[14,     1] loss: 0.571
[15,     1] loss: 0.555
[16,     1] loss: 0.535
[17,     1] loss: 0.518
[18,     1] loss: 0.497
[19,     1] loss: 0.475
[20,     1] loss: 0.457
[21,     1] loss: 0.440
[22,     1] loss: 0.427
[23,     1] loss: 0.408
[24,     1] loss: 0.392
[25,     1] loss: 0.374
[26,     1] loss: 0.366
[27,     1] loss: 0.351
[28,     1] loss: 0.341
[29,     1] loss: 0.333
[30,     1] loss: 0.325
[31,     1] loss: 0.314
[32,     1] loss: 0.309
[33,     1] loss: 0.301
[34,     1] loss: 0.293
[35,     1] loss: 0.286
[36,     1] loss: 0.281
[37,     1] loss: 0.273
[38,     1] loss: 0.266
[39,     1] loss: 0.259
[40,     1] loss: 0.252
[41,     1] loss: 0.245
[42,     1] loss: 0.236
[43,     1] loss: 0.230
[44,     1] loss: 0.220
[45,     1] loss: 0.214
[46,     1] loss: 0.209
[47,     1] loss: 0.196
[48,     1] loss: 0.191
[49,     1] loss: 0.181
[50,     1] loss: 0.176
[51,     1] loss: 0.165
[52,     1] loss: 0.158
[53,     1] loss: 0.155
[54,     1] loss: 0.149
[55,     1] loss: 0.142
[56,     1] loss: 0.135
[57,     1] loss: 0.130
[58,     1] loss: 0.122
[59,     1] loss: 0.120
[60,     1] loss: 0.113
[61,     1] loss: 0.109
[62,     1] loss: 0.104
[63,     1] loss: 0.100
[64,     1] loss: 0.096
[65,     1] loss: 0.092
[66,     1] loss: 0.090
[67,     1] loss: 0.085
[68,     1] loss: 0.085
[69,     1] loss: 0.081
[70,     1] loss: 0.085
[71,     1] loss: 0.593
[72,     1] loss: 0.280
[73,     1] loss: 0.339
[74,     1] loss: 0.310
[75,     1] loss: 0.294
[76,     1] loss: 0.291
[77,     1] loss: 0.292
[78,     1] loss: 0.286
[79,     1] loss: 0.288
[80,     1] loss: 0.285
[81,     1] loss: 0.274
[82,     1] loss: 0.261
[83,     1] loss: 0.248
[84,     1] loss: 0.241
[85,     1] loss: 0.227
[86,     1] loss: 0.220
[87,     1] loss: 0.208
[88,     1] loss: 0.199
[89,     1] loss: 0.193
[90,     1] loss: 0.181
[91,     1] loss: 0.171
[92,     1] loss: 0.161
[93,     1] loss: 0.152
[94,     1] loss: 0.140
[95,     1] loss: 0.133
[96,     1] loss: 0.128
[97,     1] loss: 0.126
[98,     1] loss: 0.121
[99,     1] loss: 0.117
[100,     1] loss: 0.113
[101,     1] loss: 0.110
[102,     1] loss: 0.107
[103,     1] loss: 0.105
[104,     1] loss: 0.104
[105,     1] loss: 0.103
[106,     1] loss: 0.100
[107,     1] loss: 0.100
[108,     1] loss: 0.100
[109,     1] loss: 0.102
[110,     1] loss: 0.100
[111,     1] loss: 0.099
[112,     1] loss: 0.103
[113,     1] loss: 0.100
[114,     1] loss: 0.101
[115,     1] loss: 0.099
[116,     1] loss: 0.101
[117,     1] loss: 0.099
[118,     1] loss: 0.101
[119,     1] loss: 0.100
[120,     1] loss: 0.105
[121,     1] loss: 0.101
[122,     1] loss: 0.104
[123,     1] loss: 0.103
[124,     1] loss: 0.103
[125,     1] loss: 0.102
[126,     1] loss: 0.102
[127,     1] loss: 0.102
[128,     1] loss: 0.101
[129,     1] loss: 0.103
[130,     1] loss: 0.103
[131,     1] loss: 0.102
[132,     1] loss: 0.105
[133,     1] loss: 0.104
[134,     1] loss: 0.103
[135,     1] loss: 0.104
[136,     1] loss: 0.105
[137,     1] loss: 0.136
[138,     1] loss: 0.333
[139,     1] loss: 0.639
[140,     1] loss: 0.347
[141,     1] loss: 0.824
[142,     1] loss: 0.482
Early stopping applied (best metric=0.33691340684890747)
Finished Training
Total time taken: 476.28321528434753
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.657
[5,     1] loss: 0.640
[6,     1] loss: 0.625
[7,     1] loss: 0.610
[8,     1] loss: 0.590
[9,     1] loss: 0.578
[10,     1] loss: 0.558
[11,     1] loss: 0.541
[12,     1] loss: 0.519
[13,     1] loss: 0.500
[14,     1] loss: 0.480
[15,     1] loss: 0.458
[16,     1] loss: 0.436
[17,     1] loss: 0.415
[18,     1] loss: 0.397
[19,     1] loss: 0.375
[20,     1] loss: 0.359
[21,     1] loss: 0.344
[22,     1] loss: 0.327
[23,     1] loss: 0.316
[24,     1] loss: 0.304
[25,     1] loss: 0.294
[26,     1] loss: 0.284
[27,     1] loss: 0.277
[28,     1] loss: 0.267
[29,     1] loss: 0.261
[30,     1] loss: 0.252
[31,     1] loss: 0.245
[32,     1] loss: 0.239
[33,     1] loss: 0.232
[34,     1] loss: 0.224
[35,     1] loss: 0.220
[36,     1] loss: 0.211
[37,     1] loss: 0.205
[38,     1] loss: 0.202
[39,     1] loss: 0.198
[40,     1] loss: 0.191
[41,     1] loss: 0.186
[42,     1] loss: 0.183
[43,     1] loss: 0.177
[44,     1] loss: 0.175
[45,     1] loss: 0.170
[46,     1] loss: 0.168
[47,     1] loss: 0.162
[48,     1] loss: 0.161
[49,     1] loss: 0.155
[50,     1] loss: 0.154
[51,     1] loss: 0.151
[52,     1] loss: 0.148
[53,     1] loss: 0.144
[54,     1] loss: 0.143
[55,     1] loss: 0.141
[56,     1] loss: 0.137
[57,     1] loss: 0.136
[58,     1] loss: 0.131
[59,     1] loss: 0.130
[60,     1] loss: 0.127
[61,     1] loss: 0.122
[62,     1] loss: 0.121
[63,     1] loss: 0.121
[64,     1] loss: 0.117
[65,     1] loss: 0.117
[66,     1] loss: 0.113
[67,     1] loss: 0.119
[68,     1] loss: 0.514
[69,     1] loss: 0.312
[70,     1] loss: 0.476
[71,     1] loss: 0.310
[72,     1] loss: 0.305
[73,     1] loss: 0.337
[74,     1] loss: 0.308
[75,     1] loss: 0.326
[76,     1] loss: 0.304
[77,     1] loss: 0.290
[78,     1] loss: 0.291
[79,     1] loss: 0.281
[80,     1] loss: 0.272
[81,     1] loss: 0.263
[82,     1] loss: 0.254
[83,     1] loss: 0.243
[84,     1] loss: 0.233
[85,     1] loss: 0.223
[86,     1] loss: 0.213
[87,     1] loss: 0.204
[88,     1] loss: 0.194
[89,     1] loss: 0.183
[90,     1] loss: 0.177
[91,     1] loss: 0.168
[92,     1] loss: 0.162
[93,     1] loss: 0.154
[94,     1] loss: 0.149
[95,     1] loss: 0.144
[96,     1] loss: 0.141
[97,     1] loss: 0.139
[98,     1] loss: 0.138
[99,     1] loss: 0.134
[100,     1] loss: 0.133
[101,     1] loss: 0.129
[102,     1] loss: 0.128
[103,     1] loss: 0.126
[104,     1] loss: 0.128
[105,     1] loss: 0.126
[106,     1] loss: 0.125
[107,     1] loss: 0.124
[108,     1] loss: 0.127
[109,     1] loss: 0.123
[110,     1] loss: 0.124
[111,     1] loss: 0.126
[112,     1] loss: 0.126
[113,     1] loss: 0.125
[114,     1] loss: 0.125
[115,     1] loss: 0.125
[116,     1] loss: 0.125
[117,     1] loss: 0.124
[118,     1] loss: 0.127
[119,     1] loss: 0.126
[120,     1] loss: 0.125
[121,     1] loss: 0.127
[122,     1] loss: 0.125
[123,     1] loss: 0.127
[124,     1] loss: 0.125
[125,     1] loss: 0.126
[126,     1] loss: 0.127
[127,     1] loss: 0.129
[128,     1] loss: 0.304
[129,     1] loss: 0.530
[130,     1] loss: 0.334
[131,     1] loss: 0.573
[132,     1] loss: 0.437
[133,     1] loss: 0.482
[134,     1] loss: 0.492
[135,     1] loss: 0.488
[136,     1] loss: 0.481
[137,     1] loss: 0.478
[138,     1] loss: 0.478
Early stopping applied (best metric=0.3595049977302551)
Finished Training
Total time taken: 463.92794966697693
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.669
[4,     1] loss: 0.658
[5,     1] loss: 0.645
[6,     1] loss: 0.633
[7,     1] loss: 0.621
[8,     1] loss: 0.607
[9,     1] loss: 0.593
[10,     1] loss: 0.573
[11,     1] loss: 0.555
[12,     1] loss: 0.536
[13,     1] loss: 0.516
[14,     1] loss: 0.494
[15,     1] loss: 0.472
[16,     1] loss: 0.448
[17,     1] loss: 0.424
[18,     1] loss: 0.401
[19,     1] loss: 0.376
[20,     1] loss: 0.356
[21,     1] loss: 0.338
[22,     1] loss: 0.315
[23,     1] loss: 0.298
[24,     1] loss: 0.279
[25,     1] loss: 0.266
[26,     1] loss: 0.250
[27,     1] loss: 0.234
[28,     1] loss: 0.221
[29,     1] loss: 0.207
[30,     1] loss: 0.197
[31,     1] loss: 0.187
[32,     1] loss: 0.176
[33,     1] loss: 0.169
[34,     1] loss: 0.157
[35,     1] loss: 0.154
[36,     1] loss: 0.149
[37,     1] loss: 0.142
[38,     1] loss: 0.139
[39,     1] loss: 0.132
[40,     1] loss: 0.132
[41,     1] loss: 0.126
[42,     1] loss: 0.122
[43,     1] loss: 0.122
[44,     1] loss: 0.120
[45,     1] loss: 0.118
[46,     1] loss: 0.118
[47,     1] loss: 0.116
[48,     1] loss: 0.116
[49,     1] loss: 0.116
[50,     1] loss: 0.113
[51,     1] loss: 0.114
[52,     1] loss: 0.115
[53,     1] loss: 0.114
[54,     1] loss: 0.113
[55,     1] loss: 0.113
[56,     1] loss: 0.112
[57,     1] loss: 0.112
[58,     1] loss: 0.110
[59,     1] loss: 0.116
[60,     1] loss: 0.247
[61,     1] loss: 0.424
[62,     1] loss: 0.338
[63,     1] loss: 0.267
[64,     1] loss: 0.240
[65,     1] loss: 0.243
[66,     1] loss: 0.235
[67,     1] loss: 0.229
[68,     1] loss: 0.222
[69,     1] loss: 0.207
[70,     1] loss: 0.199
[71,     1] loss: 0.194
[72,     1] loss: 0.186
[73,     1] loss: 0.180
[74,     1] loss: 0.171
[75,     1] loss: 0.163
Early stopping applied (best metric=0.36276546120643616)
Finished Training
Total time taken: 254.1606616973877
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.659
[4,     1] loss: 0.642
[5,     1] loss: 0.627
[6,     1] loss: 0.609
[7,     1] loss: 0.592
[8,     1] loss: 0.575
[9,     1] loss: 0.554
[10,     1] loss: 0.536
[11,     1] loss: 0.514
[12,     1] loss: 0.493
[13,     1] loss: 0.472
[14,     1] loss: 0.453
[15,     1] loss: 0.433
[16,     1] loss: 0.417
[17,     1] loss: 0.398
[18,     1] loss: 0.386
[19,     1] loss: 0.374
[20,     1] loss: 0.360
[21,     1] loss: 0.348
[22,     1] loss: 0.339
[23,     1] loss: 0.332
[24,     1] loss: 0.324
[25,     1] loss: 0.316
[26,     1] loss: 0.311
[27,     1] loss: 0.305
[28,     1] loss: 0.297
[29,     1] loss: 0.292
[30,     1] loss: 0.283
[31,     1] loss: 0.276
[32,     1] loss: 0.269
[33,     1] loss: 0.261
[34,     1] loss: 0.251
[35,     1] loss: 0.248
[36,     1] loss: 0.239
[37,     1] loss: 0.232
[38,     1] loss: 0.227
[39,     1] loss: 0.220
[40,     1] loss: 0.215
[41,     1] loss: 0.207
[42,     1] loss: 0.200
[43,     1] loss: 0.192
[44,     1] loss: 0.186
[45,     1] loss: 0.182
[46,     1] loss: 0.176
[47,     1] loss: 0.169
[48,     1] loss: 0.163
[49,     1] loss: 0.158
[50,     1] loss: 0.151
[51,     1] loss: 0.146
[52,     1] loss: 0.141
[53,     1] loss: 0.137
[54,     1] loss: 0.132
[55,     1] loss: 0.128
[56,     1] loss: 0.126
[57,     1] loss: 0.123
[58,     1] loss: 0.120
[59,     1] loss: 0.118
[60,     1] loss: 0.115
[61,     1] loss: 0.113
[62,     1] loss: 0.111
[63,     1] loss: 0.108
[64,     1] loss: 0.106
[65,     1] loss: 0.103
[66,     1] loss: 0.102
[67,     1] loss: 0.099
[68,     1] loss: 0.096
[69,     1] loss: 0.093
[70,     1] loss: 0.091
[71,     1] loss: 0.091
[72,     1] loss: 0.165
[73,     1] loss: 0.245
[74,     1] loss: 0.484
[75,     1] loss: 0.281
[76,     1] loss: 0.448
[77,     1] loss: 0.342
[78,     1] loss: 0.351
[79,     1] loss: 0.334
[80,     1] loss: 0.333
[81,     1] loss: 0.331
[82,     1] loss: 0.325
[83,     1] loss: 0.317
[84,     1] loss: 0.310
[85,     1] loss: 0.304
[86,     1] loss: 0.294
[87,     1] loss: 0.275
[88,     1] loss: 0.261
[89,     1] loss: 0.245
[90,     1] loss: 0.235
[91,     1] loss: 0.221
[92,     1] loss: 0.208
[93,     1] loss: 0.196
[94,     1] loss: 0.187
[95,     1] loss: 0.174
[96,     1] loss: 0.171
[97,     1] loss: 0.160
[98,     1] loss: 0.153
[99,     1] loss: 0.147
[100,     1] loss: 0.142
[101,     1] loss: 0.136
[102,     1] loss: 0.131
[103,     1] loss: 0.129
[104,     1] loss: 0.126
[105,     1] loss: 0.123
[106,     1] loss: 0.122
[107,     1] loss: 0.118
[108,     1] loss: 0.121
[109,     1] loss: 0.116
[110,     1] loss: 0.116
[111,     1] loss: 0.116
[112,     1] loss: 0.114
[113,     1] loss: 0.115
[114,     1] loss: 0.114
[115,     1] loss: 0.116
[116,     1] loss: 0.115
[117,     1] loss: 0.115
[118,     1] loss: 0.116
[119,     1] loss: 0.115
[120,     1] loss: 0.113
[121,     1] loss: 0.115
[122,     1] loss: 0.115
[123,     1] loss: 0.115
[124,     1] loss: 0.115
[125,     1] loss: 0.114
[126,     1] loss: 0.114
[127,     1] loss: 0.113
[128,     1] loss: 0.113
[129,     1] loss: 0.114
[130,     1] loss: 0.113
[131,     1] loss: 0.113
[132,     1] loss: 0.111
[133,     1] loss: 0.113
[134,     1] loss: 0.115
[135,     1] loss: 0.115
[136,     1] loss: 0.112
[137,     1] loss: 0.116
[138,     1] loss: 0.114
[139,     1] loss: 0.117
[140,     1] loss: 0.142
[141,     1] loss: 0.587
[142,     1] loss: 0.461
[143,     1] loss: 0.545
[144,     1] loss: 0.440
[145,     1] loss: 0.503
Early stopping applied (best metric=0.31724387407302856)
Finished Training
Total time taken: 489.3354878425598
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.678
[4,     1] loss: 0.667
[5,     1] loss: 0.656
[6,     1] loss: 0.644
[7,     1] loss: 0.633
[8,     1] loss: 0.622
[9,     1] loss: 0.607
[10,     1] loss: 0.592
[11,     1] loss: 0.578
[12,     1] loss: 0.560
[13,     1] loss: 0.542
[14,     1] loss: 0.525
[15,     1] loss: 0.504
[16,     1] loss: 0.485
[17,     1] loss: 0.465
[18,     1] loss: 0.443
[19,     1] loss: 0.422
[20,     1] loss: 0.404
[21,     1] loss: 0.383
[22,     1] loss: 0.365
[23,     1] loss: 0.344
[24,     1] loss: 0.325
[25,     1] loss: 0.310
[26,     1] loss: 0.289
[27,     1] loss: 0.274
[28,     1] loss: 0.255
[29,     1] loss: 0.242
[30,     1] loss: 0.227
[31,     1] loss: 0.216
[32,     1] loss: 0.201
[33,     1] loss: 0.193
[34,     1] loss: 0.183
[35,     1] loss: 0.174
[36,     1] loss: 0.162
[37,     1] loss: 0.156
[38,     1] loss: 0.149
[39,     1] loss: 0.143
[40,     1] loss: 0.139
[41,     1] loss: 0.134
[42,     1] loss: 0.129
[43,     1] loss: 0.126
[44,     1] loss: 0.120
[45,     1] loss: 0.117
[46,     1] loss: 0.117
[47,     1] loss: 0.113
[48,     1] loss: 0.110
[49,     1] loss: 0.108
[50,     1] loss: 0.108
[51,     1] loss: 0.105
[52,     1] loss: 0.106
[53,     1] loss: 0.103
[54,     1] loss: 0.105
[55,     1] loss: 0.103
[56,     1] loss: 0.102
[57,     1] loss: 0.102
[58,     1] loss: 0.101
[59,     1] loss: 0.102
[60,     1] loss: 0.104
[61,     1] loss: 0.249
[62,     1] loss: 0.449
[63,     1] loss: 0.393
[64,     1] loss: 0.349
[65,     1] loss: 0.315
[66,     1] loss: 0.300
[67,     1] loss: 0.282
[68,     1] loss: 0.260
[69,     1] loss: 0.241
[70,     1] loss: 0.227
[71,     1] loss: 0.213
[72,     1] loss: 0.196
[73,     1] loss: 0.189
[74,     1] loss: 0.174
[75,     1] loss: 0.165
[76,     1] loss: 0.156
[77,     1] loss: 0.145
[78,     1] loss: 0.138
[79,     1] loss: 0.132
[80,     1] loss: 0.125
[81,     1] loss: 0.120
[82,     1] loss: 0.114
[83,     1] loss: 0.110
[84,     1] loss: 0.108
[85,     1] loss: 0.104
[86,     1] loss: 0.099
[87,     1] loss: 0.096
[88,     1] loss: 0.098
[89,     1] loss: 0.095
[90,     1] loss: 0.093
[91,     1] loss: 0.092
[92,     1] loss: 0.090
[93,     1] loss: 0.091
[94,     1] loss: 0.091
[95,     1] loss: 0.090
[96,     1] loss: 0.092
[97,     1] loss: 0.091
[98,     1] loss: 0.092
[99,     1] loss: 0.092
[100,     1] loss: 0.094
[101,     1] loss: 0.093
[102,     1] loss: 0.091
[103,     1] loss: 0.094
[104,     1] loss: 0.092
[105,     1] loss: 0.093
[106,     1] loss: 0.091
[107,     1] loss: 0.092
[108,     1] loss: 0.094
[109,     1] loss: 0.093
[110,     1] loss: 0.094
[111,     1] loss: 0.091
[112,     1] loss: 0.092
[113,     1] loss: 0.090
[114,     1] loss: 0.092
[115,     1] loss: 0.092
[116,     1] loss: 0.092
[117,     1] loss: 0.096
[118,     1] loss: 0.326
Early stopping applied (best metric=0.3722466230392456)
Finished Training
Total time taken: 399.3824362754822
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.678
[4,     1] loss: 0.669
[5,     1] loss: 0.659
[6,     1] loss: 0.651
[7,     1] loss: 0.641
[8,     1] loss: 0.630
[9,     1] loss: 0.619
[10,     1] loss: 0.607
[11,     1] loss: 0.594
[12,     1] loss: 0.579
[13,     1] loss: 0.565
[14,     1] loss: 0.548
[15,     1] loss: 0.529
[16,     1] loss: 0.512
[17,     1] loss: 0.491
[18,     1] loss: 0.474
[19,     1] loss: 0.454
[20,     1] loss: 0.431
[21,     1] loss: 0.411
[22,     1] loss: 0.393
[23,     1] loss: 0.372
[24,     1] loss: 0.353
[25,     1] loss: 0.333
[26,     1] loss: 0.314
[27,     1] loss: 0.298
[28,     1] loss: 0.277
[29,     1] loss: 0.260
[30,     1] loss: 0.245
[31,     1] loss: 0.234
[32,     1] loss: 0.218
[33,     1] loss: 0.206
[34,     1] loss: 0.195
[35,     1] loss: 0.186
[36,     1] loss: 0.174
[37,     1] loss: 0.166
[38,     1] loss: 0.160
[39,     1] loss: 0.156
[40,     1] loss: 0.148
[41,     1] loss: 0.143
[42,     1] loss: 0.138
[43,     1] loss: 0.133
[44,     1] loss: 0.133
[45,     1] loss: 0.127
[46,     1] loss: 0.123
[47,     1] loss: 0.123
[48,     1] loss: 0.122
[49,     1] loss: 0.123
[50,     1] loss: 0.119
[51,     1] loss: 0.119
[52,     1] loss: 0.119
[53,     1] loss: 0.119
[54,     1] loss: 0.119
[55,     1] loss: 0.118
[56,     1] loss: 0.118
[57,     1] loss: 0.118
[58,     1] loss: 0.117
[59,     1] loss: 0.119
[60,     1] loss: 0.117
[61,     1] loss: 0.119
[62,     1] loss: 0.118
[63,     1] loss: 0.119
[64,     1] loss: 0.141
[65,     1] loss: 0.268
[66,     1] loss: 0.518
[67,     1] loss: 0.475
[68,     1] loss: 0.374
[69,     1] loss: 0.374
[70,     1] loss: 0.311
[71,     1] loss: 0.297
[72,     1] loss: 0.280
[73,     1] loss: 0.279
[74,     1] loss: 0.268
[75,     1] loss: 0.263
[76,     1] loss: 0.256
[77,     1] loss: 0.251
[78,     1] loss: 0.243
[79,     1] loss: 0.235
[80,     1] loss: 0.227
[81,     1] loss: 0.222
[82,     1] loss: 0.213
[83,     1] loss: 0.205
[84,     1] loss: 0.200
[85,     1] loss: 0.191
[86,     1] loss: 0.181
[87,     1] loss: 0.173
[88,     1] loss: 0.160
[89,     1] loss: 0.150
[90,     1] loss: 0.140
[91,     1] loss: 0.135
[92,     1] loss: 0.127
[93,     1] loss: 0.124
[94,     1] loss: 0.115
[95,     1] loss: 0.115
[96,     1] loss: 0.112
[97,     1] loss: 0.110
[98,     1] loss: 0.106
[99,     1] loss: 0.105
[100,     1] loss: 0.103
[101,     1] loss: 0.102
[102,     1] loss: 0.103
[103,     1] loss: 0.100
[104,     1] loss: 0.101
[105,     1] loss: 0.102
[106,     1] loss: 0.101
[107,     1] loss: 0.101
[108,     1] loss: 0.101
[109,     1] loss: 0.103
[110,     1] loss: 0.102
[111,     1] loss: 0.101
[112,     1] loss: 0.103
[113,     1] loss: 0.105
[114,     1] loss: 0.104
[115,     1] loss: 0.106
[116,     1] loss: 0.105
[117,     1] loss: 0.106
[118,     1] loss: 0.108
[119,     1] loss: 0.105
[120,     1] loss: 0.107
[121,     1] loss: 0.110
[122,     1] loss: 0.126
[123,     1] loss: 0.369
[124,     1] loss: 0.442
[125,     1] loss: 0.682
[126,     1] loss: 0.343
Early stopping applied (best metric=0.3863333761692047)
Finished Training
Total time taken: 427.092821598053
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.667
[4,     1] loss: 0.656
[5,     1] loss: 0.645
[6,     1] loss: 0.634
[7,     1] loss: 0.619
[8,     1] loss: 0.605
[9,     1] loss: 0.587
[10,     1] loss: 0.572
[11,     1] loss: 0.553
[12,     1] loss: 0.534
[13,     1] loss: 0.508
[14,     1] loss: 0.489
[15,     1] loss: 0.464
[16,     1] loss: 0.440
[17,     1] loss: 0.418
[18,     1] loss: 0.390
[19,     1] loss: 0.371
[20,     1] loss: 0.343
[21,     1] loss: 0.320
[22,     1] loss: 0.300
[23,     1] loss: 0.279
[24,     1] loss: 0.264
[25,     1] loss: 0.248
[26,     1] loss: 0.230
[27,     1] loss: 0.221
[28,     1] loss: 0.209
[29,     1] loss: 0.196
[30,     1] loss: 0.186
[31,     1] loss: 0.175
[32,     1] loss: 0.167
[33,     1] loss: 0.157
[34,     1] loss: 0.151
[35,     1] loss: 0.146
[36,     1] loss: 0.140
[37,     1] loss: 0.135
[38,     1] loss: 0.130
[39,     1] loss: 0.129
[40,     1] loss: 0.124
[41,     1] loss: 0.122
[42,     1] loss: 0.121
[43,     1] loss: 0.119
[44,     1] loss: 0.117
[45,     1] loss: 0.116
[46,     1] loss: 0.116
[47,     1] loss: 0.113
[48,     1] loss: 0.115
[49,     1] loss: 0.114
[50,     1] loss: 0.113
[51,     1] loss: 0.115
[52,     1] loss: 0.116
[53,     1] loss: 0.114
[54,     1] loss: 0.115
[55,     1] loss: 0.114
[56,     1] loss: 0.114
[57,     1] loss: 0.113
[58,     1] loss: 0.112
[59,     1] loss: 0.112
[60,     1] loss: 0.110
[61,     1] loss: 0.108
[62,     1] loss: 0.107
[63,     1] loss: 0.106
[64,     1] loss: 0.127
[65,     1] loss: 0.390
[66,     1] loss: 0.614
[67,     1] loss: 0.566
[68,     1] loss: 0.507
[69,     1] loss: 0.481
[70,     1] loss: 0.470
[71,     1] loss: 0.467
[72,     1] loss: 0.470
[73,     1] loss: 0.480
[74,     1] loss: 0.483
[75,     1] loss: 0.490
[76,     1] loss: 0.491
[77,     1] loss: 0.490
[78,     1] loss: 0.487
[79,     1] loss: 0.488
[80,     1] loss: 0.482
[81,     1] loss: 0.480
[82,     1] loss: 0.475
[83,     1] loss: 0.471
[84,     1] loss: 0.466
[85,     1] loss: 0.460
[86,     1] loss: 0.455
[87,     1] loss: 0.449
[88,     1] loss: 0.442
[89,     1] loss: 0.439
[90,     1] loss: 0.429
[91,     1] loss: 0.424
[92,     1] loss: 0.418
[93,     1] loss: 0.411
[94,     1] loss: 0.402
[95,     1] loss: 0.396
[96,     1] loss: 0.389
[97,     1] loss: 0.377
[98,     1] loss: 0.366
[99,     1] loss: 0.355
[100,     1] loss: 0.337
[101,     1] loss: 0.320
[102,     1] loss: 0.305
[103,     1] loss: 0.284
[104,     1] loss: 0.266
[105,     1] loss: 0.244
[106,     1] loss: 0.227
[107,     1] loss: 0.213
[108,     1] loss: 0.198
[109,     1] loss: 0.185
[110,     1] loss: 0.171
[111,     1] loss: 0.158
[112,     1] loss: 0.149
[113,     1] loss: 0.139
[114,     1] loss: 0.130
[115,     1] loss: 0.121
[116,     1] loss: 0.116
[117,     1] loss: 0.109
[118,     1] loss: 0.104
[119,     1] loss: 0.100
[120,     1] loss: 0.097
[121,     1] loss: 0.097
[122,     1] loss: 0.095
[123,     1] loss: 0.094
[124,     1] loss: 0.093
[125,     1] loss: 0.093
[126,     1] loss: 0.094
[127,     1] loss: 0.093
[128,     1] loss: 0.093
[129,     1] loss: 0.095
[130,     1] loss: 0.096
[131,     1] loss: 0.097
[132,     1] loss: 0.100
[133,     1] loss: 0.289
[134,     1] loss: 0.262
[135,     1] loss: 0.300
[136,     1] loss: 0.543
[137,     1] loss: 0.347
[138,     1] loss: 0.541
[139,     1] loss: 0.331
[140,     1] loss: 0.388
[141,     1] loss: 0.364
[142,     1] loss: 0.334
[143,     1] loss: 0.355
[144,     1] loss: 0.332
[145,     1] loss: 0.315
[146,     1] loss: 0.316
[147,     1] loss: 0.307
[148,     1] loss: 0.290
[149,     1] loss: 0.282
[150,     1] loss: 0.271
[151,     1] loss: 0.262
[152,     1] loss: 0.253
[153,     1] loss: 0.243
[154,     1] loss: 0.233
[155,     1] loss: 0.223
[156,     1] loss: 0.216
[157,     1] loss: 0.206
[158,     1] loss: 0.196
Early stopping applied (best metric=0.3584059476852417)
Finished Training
Total time taken: 536.0783941745758
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.688
[3,     1] loss: 0.672
[4,     1] loss: 0.659
[5,     1] loss: 0.648
[6,     1] loss: 0.636
[7,     1] loss: 0.623
[8,     1] loss: 0.610
[9,     1] loss: 0.597
[10,     1] loss: 0.581
[11,     1] loss: 0.567
[12,     1] loss: 0.551
[13,     1] loss: 0.535
[14,     1] loss: 0.516
[15,     1] loss: 0.498
[16,     1] loss: 0.481
[17,     1] loss: 0.461
[18,     1] loss: 0.443
[19,     1] loss: 0.423
[20,     1] loss: 0.401
[21,     1] loss: 0.384
[22,     1] loss: 0.364
[23,     1] loss: 0.344
[24,     1] loss: 0.328
[25,     1] loss: 0.308
[26,     1] loss: 0.290
[27,     1] loss: 0.282
[28,     1] loss: 0.259
[29,     1] loss: 0.259
[30,     1] loss: 0.241
[31,     1] loss: 0.233
[32,     1] loss: 0.212
[33,     1] loss: 0.203
[34,     1] loss: 0.193
[35,     1] loss: 0.183
[36,     1] loss: 0.175
[37,     1] loss: 0.166
[38,     1] loss: 0.158
[39,     1] loss: 0.152
[40,     1] loss: 0.147
[41,     1] loss: 0.144
[42,     1] loss: 0.137
[43,     1] loss: 0.134
[44,     1] loss: 0.128
[45,     1] loss: 0.128
[46,     1] loss: 0.124
[47,     1] loss: 0.122
[48,     1] loss: 0.119
[49,     1] loss: 0.118
[50,     1] loss: 0.116
[51,     1] loss: 0.117
[52,     1] loss: 0.116
[53,     1] loss: 0.112
[54,     1] loss: 0.114
[55,     1] loss: 0.113
[56,     1] loss: 0.112
[57,     1] loss: 0.111
[58,     1] loss: 0.108
[59,     1] loss: 0.105
[60,     1] loss: 0.102
[61,     1] loss: 0.101
[62,     1] loss: 0.097
[63,     1] loss: 0.096
[64,     1] loss: 0.092
Early stopping applied (best metric=0.47261279821395874)
Finished Training
Total time taken: 219.91010403633118
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.662
[4,     1] loss: 0.645
[5,     1] loss: 0.630
[6,     1] loss: 0.616
[7,     1] loss: 0.601
[8,     1] loss: 0.586
[9,     1] loss: 0.570
[10,     1] loss: 0.551
[11,     1] loss: 0.532
[12,     1] loss: 0.512
[13,     1] loss: 0.492
[14,     1] loss: 0.470
[15,     1] loss: 0.446
[16,     1] loss: 0.425
[17,     1] loss: 0.406
[18,     1] loss: 0.382
[19,     1] loss: 0.359
[20,     1] loss: 0.337
[21,     1] loss: 0.315
[22,     1] loss: 0.295
[23,     1] loss: 0.279
[24,     1] loss: 0.266
[25,     1] loss: 0.248
[26,     1] loss: 0.230
[27,     1] loss: 0.214
[28,     1] loss: 0.204
[29,     1] loss: 0.194
[30,     1] loss: 0.183
[31,     1] loss: 0.172
[32,     1] loss: 0.165
[33,     1] loss: 0.157
[34,     1] loss: 0.151
[35,     1] loss: 0.143
[36,     1] loss: 0.139
[37,     1] loss: 0.135
[38,     1] loss: 0.129
[39,     1] loss: 0.129
[40,     1] loss: 0.126
[41,     1] loss: 0.125
[42,     1] loss: 0.121
[43,     1] loss: 0.119
[44,     1] loss: 0.117
[45,     1] loss: 0.116
[46,     1] loss: 0.117
[47,     1] loss: 0.115
[48,     1] loss: 0.114
[49,     1] loss: 0.114
[50,     1] loss: 0.113
[51,     1] loss: 0.113
[52,     1] loss: 0.112
[53,     1] loss: 0.112
[54,     1] loss: 0.112
[55,     1] loss: 0.111
[56,     1] loss: 0.110
[57,     1] loss: 0.109
[58,     1] loss: 0.110
[59,     1] loss: 0.109
[60,     1] loss: 0.107
[61,     1] loss: 0.106
[62,     1] loss: 0.104
[63,     1] loss: 0.106
[64,     1] loss: 0.126
[65,     1] loss: 0.271
[66,     1] loss: 0.502
[67,     1] loss: 0.460
[68,     1] loss: 0.387
[69,     1] loss: 0.395
[70,     1] loss: 0.334
[71,     1] loss: 0.331
[72,     1] loss: 0.302
[73,     1] loss: 0.300
[74,     1] loss: 0.291
[75,     1] loss: 0.277
[76,     1] loss: 0.271
[77,     1] loss: 0.254
[78,     1] loss: 0.243
[79,     1] loss: 0.236
[80,     1] loss: 0.224
[81,     1] loss: 0.217
[82,     1] loss: 0.205
[83,     1] loss: 0.197
[84,     1] loss: 0.184
[85,     1] loss: 0.175
[86,     1] loss: 0.169
[87,     1] loss: 0.157
[88,     1] loss: 0.151
[89,     1] loss: 0.143
[90,     1] loss: 0.137
[91,     1] loss: 0.128
[92,     1] loss: 0.123
[93,     1] loss: 0.116
[94,     1] loss: 0.112
[95,     1] loss: 0.108
[96,     1] loss: 0.107
[97,     1] loss: 0.102
[98,     1] loss: 0.101
[99,     1] loss: 0.098
[100,     1] loss: 0.099
[101,     1] loss: 0.093
[102,     1] loss: 0.097
[103,     1] loss: 0.097
[104,     1] loss: 0.096
[105,     1] loss: 0.096
[106,     1] loss: 0.096
[107,     1] loss: 0.098
[108,     1] loss: 0.097
[109,     1] loss: 0.098
[110,     1] loss: 0.095
[111,     1] loss: 0.097
[112,     1] loss: 0.097
[113,     1] loss: 0.098
[114,     1] loss: 0.098
[115,     1] loss: 0.099
[116,     1] loss: 0.097
[117,     1] loss: 0.098
[118,     1] loss: 0.098
[119,     1] loss: 0.099
[120,     1] loss: 0.100
[121,     1] loss: 0.099
[122,     1] loss: 0.099
[123,     1] loss: 0.100
[124,     1] loss: 0.100
[125,     1] loss: 0.103
[126,     1] loss: 0.207
[127,     1] loss: 0.379
[128,     1] loss: 0.401
[129,     1] loss: 0.474
[130,     1] loss: 0.433
[131,     1] loss: 0.428
[132,     1] loss: 0.337
[133,     1] loss: 0.350
[134,     1] loss: 0.351
Early stopping applied (best metric=0.34976881742477417)
Finished Training
Total time taken: 456.88998103141785
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.676
[3,     1] loss: 0.655
[4,     1] loss: 0.640
[5,     1] loss: 0.624
[6,     1] loss: 0.608
[7,     1] loss: 0.592
[8,     1] loss: 0.574
[9,     1] loss: 0.552
[10,     1] loss: 0.533
[11,     1] loss: 0.512
[12,     1] loss: 0.493
[13,     1] loss: 0.473
[14,     1] loss: 0.454
[15,     1] loss: 0.438
[16,     1] loss: 0.422
[17,     1] loss: 0.406
[18,     1] loss: 0.390
[19,     1] loss: 0.375
[20,     1] loss: 0.364
[21,     1] loss: 0.351
[22,     1] loss: 0.338
[23,     1] loss: 0.325
[24,     1] loss: 0.312
[25,     1] loss: 0.300
[26,     1] loss: 0.287
[27,     1] loss: 0.276
[28,     1] loss: 0.265
[29,     1] loss: 0.256
[30,     1] loss: 0.241
[31,     1] loss: 0.229
[32,     1] loss: 0.221
[33,     1] loss: 0.214
[34,     1] loss: 0.205
[35,     1] loss: 0.197
[36,     1] loss: 0.190
[37,     1] loss: 0.181
[38,     1] loss: 0.175
[39,     1] loss: 0.170
[40,     1] loss: 0.165
[41,     1] loss: 0.159
[42,     1] loss: 0.154
[43,     1] loss: 0.150
[44,     1] loss: 0.147
[45,     1] loss: 0.143
[46,     1] loss: 0.140
[47,     1] loss: 0.139
[48,     1] loss: 0.136
[49,     1] loss: 0.133
[50,     1] loss: 0.134
[51,     1] loss: 0.132
[52,     1] loss: 0.131
[53,     1] loss: 0.129
[54,     1] loss: 0.127
[55,     1] loss: 0.128
[56,     1] loss: 0.126
[57,     1] loss: 0.126
[58,     1] loss: 0.127
[59,     1] loss: 0.125
[60,     1] loss: 0.124
[61,     1] loss: 0.125
[62,     1] loss: 0.121
[63,     1] loss: 0.120
[64,     1] loss: 0.167
[65,     1] loss: 0.264
[66,     1] loss: 0.593
[67,     1] loss: 0.412
[68,     1] loss: 0.367
[69,     1] loss: 0.365
[70,     1] loss: 0.326
[71,     1] loss: 0.323
[72,     1] loss: 0.327
[73,     1] loss: 0.318
[74,     1] loss: 0.306
[75,     1] loss: 0.293
[76,     1] loss: 0.276
[77,     1] loss: 0.265
[78,     1] loss: 0.254
[79,     1] loss: 0.239
[80,     1] loss: 0.228
[81,     1] loss: 0.220
[82,     1] loss: 0.207
[83,     1] loss: 0.199
[84,     1] loss: 0.189
[85,     1] loss: 0.180
[86,     1] loss: 0.175
[87,     1] loss: 0.168
[88,     1] loss: 0.162
[89,     1] loss: 0.153
[90,     1] loss: 0.149
[91,     1] loss: 0.143
[92,     1] loss: 0.140
[93,     1] loss: 0.136
[94,     1] loss: 0.134
[95,     1] loss: 0.133
[96,     1] loss: 0.130
[97,     1] loss: 0.128
[98,     1] loss: 0.125
[99,     1] loss: 0.126
[100,     1] loss: 0.123
[101,     1] loss: 0.122
[102,     1] loss: 0.121
[103,     1] loss: 0.120
[104,     1] loss: 0.120
[105,     1] loss: 0.117
[106,     1] loss: 0.118
[107,     1] loss: 0.136
[108,     1] loss: 0.179
[109,     1] loss: 0.178
[110,     1] loss: 0.180
[111,     1] loss: 0.180
[112,     1] loss: 0.178
[113,     1] loss: 0.172
[114,     1] loss: 0.170
[115,     1] loss: 0.170
[116,     1] loss: 0.165
[117,     1] loss: 0.161
[118,     1] loss: 0.158
[119,     1] loss: 0.153
[120,     1] loss: 0.151
[121,     1] loss: 0.148
[122,     1] loss: 0.142
[123,     1] loss: 0.139
[124,     1] loss: 0.136
[125,     1] loss: 0.131
[126,     1] loss: 0.127
[127,     1] loss: 0.122
[128,     1] loss: 0.120
[129,     1] loss: 0.116
[130,     1] loss: 0.113
[131,     1] loss: 0.112
Early stopping applied (best metric=0.3871043920516968)
Finished Training
Total time taken: 445.6927764415741
{'Hydroxylation-P Validation Accuracy': 0.7893496979848739, 'Hydroxylation-P Validation Sensitivity': 0.7679365079365079, 'Hydroxylation-P Validation Specificity': 0.793909920694299, 'Hydroxylation-P Validation Precision': 0.45061589178654066, 'Hydroxylation-P AUC ROC': 0.8471031639150747, 'Hydroxylation-P AUC PR': 0.5916119074851953, 'Hydroxylation-P MCC': 0.4688027531109268, 'Hydroxylation-P F1': 0.5658687977541308, 'Validation Loss (Hydroxylation-P)': 0.3817025303840637, 'Validation Loss (total)': 0.3817025303840637}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009522169755869373,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1730589885,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.997130392799793}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.656
[4,     1] loss: 0.629
[5,     1] loss: 0.602
[6,     1] loss: 0.577
[7,     1] loss: 0.554
[8,     1] loss: 0.532
[9,     1] loss: 0.507
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007406784809976071,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1371228001,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.148448885909743}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.636
[4,     1] loss: 0.584
[5,     1] loss: 0.544
[6,     1] loss: 0.503
[7,     1] loss: 0.468
[8,     1] loss: 0.410
[9,     1] loss: 0.352
[10,     1] loss: 0.293
[11,     1] loss: 0.231
[12,     1] loss: 0.188
[13,     1] loss: 0.149
[14,     1] loss: 0.118
[15,     1] loss: 0.122
[16,     1] loss: 0.079
[17,     1] loss: 0.074
[18,     1] loss: 0.076
[19,     1] loss: 0.073
[20,     1] loss: 0.063
[21,     1] loss: 0.061
[22,     1] loss: 0.053
[23,     1] loss: 0.052
[24,     1] loss: 0.053
[25,     1] loss: 0.047
[26,     1] loss: 0.041
[27,     1] loss: 0.038
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002590076774891545,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3878415367,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.217541649587703}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.677
[4,     1] loss: 0.668
[5,     1] loss: 0.661
[6,     1] loss: 0.654
[7,     1] loss: 0.648
[8,     1] loss: 0.641
[9,     1] loss: 0.636
[10,     1] loss: 0.631
[11,     1] loss: 0.626
[12,     1] loss: 0.620
[13,     1] loss: 0.615
[14,     1] loss: 0.609
[15,     1] loss: 0.606
[16,     1] loss: 0.600
[17,     1] loss: 0.596
[18,     1] loss: 0.591
[19,     1] loss: 0.586
[20,     1] loss: 0.581
[21,     1] loss: 0.576
[22,     1] loss: 0.572
[23,     1] loss: 0.569
[24,     1] loss: 0.562
[25,     1] loss: 0.556
[26,     1] loss: 0.552
[27,     1] loss: 0.549
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017405481346595354,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1226774516,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.77727441218173}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005967262777163168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 55429955,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.8386945333849}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.682
[3,     1] loss: 0.639
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005425266623229151,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2580060219,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.1025692130835845}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.691
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025622890584152255,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 20146699,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.299491378369023}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.681
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002306135642118372,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2072443846,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.677949553991311}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.659
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002214008241175435,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 917925092,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.455889527383892}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.688
[3,     1] loss: 0.674
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005432305522763733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3035945431,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.651993968069586}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008012232002025717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2776458514,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.838138312182053}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.633
[4,     1] loss: 0.582
[5,     1] loss: 0.532
[6,     1] loss: 0.475
[7,     1] loss: 0.419
[8,     1] loss: 0.377
[9,     1] loss: 0.307
[10,     1] loss: 0.248
[11,     1] loss: 0.194
[12,     1] loss: 0.142
[13,     1] loss: 0.109
[14,     1] loss: 0.095
[15,     1] loss: 0.067
[16,     1] loss: 0.045
[17,     1] loss: 0.053
[18,     1] loss: 0.023
[19,     1] loss: 0.015
[20,     1] loss: 0.011
[21,     1] loss: 0.043
[22,     1] loss: 0.017
[23,     1] loss: 0.028
[24,     1] loss: 0.003
[25,     1] loss: 0.008
[26,     1] loss: 0.002
[27,     1] loss: 0.015
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008621815786765599,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2929358355,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.318440456492237}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.637
[4,     1] loss: 0.570
[5,     1] loss: 0.499
[6,     1] loss: 0.434
[7,     1] loss: 0.359
[8,     1] loss: 0.288
[9,     1] loss: 0.226
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007971074061904676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 428398902,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.039913719336664766}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.666
[3,     1] loss: 0.591
[4,     1] loss: 0.511
[5,     1] loss: 0.437
[6,     1] loss: 0.343
[7,     1] loss: 0.258
[8,     1] loss: 0.180
[9,     1] loss: 0.130
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002804779481568616,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2010359855,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.209486922037218}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005424550753551445,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3616486374,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.964416235358993}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.690
[3,     1] loss: 0.666
[4,     1] loss: 0.644
[5,     1] loss: 0.618
[6,     1] loss: 0.585
[7,     1] loss: 0.555
[8,     1] loss: 0.522
[9,     1] loss: 0.486
[10,     1] loss: 0.460
[11,     1] loss: 0.451
[12,     1] loss: 0.410
[13,     1] loss: 0.401
[14,     1] loss: 0.410
[15,     1] loss: 0.388
[16,     1] loss: 0.378
[17,     1] loss: 0.366
[18,     1] loss: 0.360
[19,     1] loss: 0.356
[20,     1] loss: 0.349
[21,     1] loss: 0.352
[22,     1] loss: 0.342
[23,     1] loss: 0.335
[24,     1] loss: 0.338
[25,     1] loss: 0.329
[26,     1] loss: 0.329
[27,     1] loss: 0.326
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006829918923326995,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1025674018,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5408614352344125}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.682
[3,     1] loss: 0.638
[4,     1] loss: 0.582
[5,     1] loss: 0.504
[6,     1] loss: 0.423
[7,     1] loss: 0.346
[8,     1] loss: 0.264
[9,     1] loss: 0.208
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008674852350980799,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2095737944,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.375505543708721}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.674
[3,     1] loss: 0.617
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003177610615932051,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1463536803,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.344794451551582}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008103107085506676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4153351314,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.257953123263782}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.676
[3,     1] loss: 0.622
[4,     1] loss: 0.567
[5,     1] loss: 0.497
[6,     1] loss: 0.427
[7,     1] loss: 0.351
[8,     1] loss: 0.295
[9,     1] loss: 0.267
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006580569788540718,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 125259958,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.014401304616581}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.674
[3,     1] loss: 0.620
[4,     1] loss: 0.565
[5,     1] loss: 0.507
[6,     1] loss: 0.440
[7,     1] loss: 0.361
[8,     1] loss: 0.290
[9,     1] loss: 0.235
[10,     1] loss: 0.191
[11,     1] loss: 0.130
[12,     1] loss: 0.102
[13,     1] loss: 0.069
[14,     1] loss: 0.047
[15,     1] loss: 0.051
[16,     1] loss: 0.025
[17,     1] loss: 0.028
[18,     1] loss: 0.020
[19,     1] loss: 0.014
[20,     1] loss: 0.010
[21,     1] loss: 0.010
[22,     1] loss: 0.006
[23,     1] loss: 0.005
[24,     1] loss: 0.005
[25,     1] loss: 0.004
[26,     1] loss: 0.003
[27,     1] loss: 0.003
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006141605813704986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 20401392,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.9402059310215867}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.686
[3,     1] loss: 0.647
[4,     1] loss: 0.603
[5,     1] loss: 0.544
[6,     1] loss: 0.478
[7,     1] loss: 0.402
[8,     1] loss: 0.331
[9,     1] loss: 0.267
[10,     1] loss: 0.209
[11,     1] loss: 0.154
[12,     1] loss: 0.132
[13,     1] loss: 0.111
[14,     1] loss: 0.086
[15,     1] loss: 0.054
[16,     1] loss: 0.047
[17,     1] loss: 0.071
[18,     1] loss: 0.028
[19,     1] loss: 0.046
[20,     1] loss: 0.019
[21,     1] loss: 0.021
[22,     1] loss: 0.019
[23,     1] loss: 0.010
[24,     1] loss: 0.009
[25,     1] loss: 0.007
[26,     1] loss: 0.005
[27,     1] loss: 0.004
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006511205833126085,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 69397884,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.4830773148650875}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.676
[3,     1] loss: 0.636
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007423654960590771,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 956504899,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.934814297078168}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.638
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009999389536558441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 921186590,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.830279283292906}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.615
[4,     1] loss: 0.557
[5,     1] loss: 0.483
[6,     1] loss: 0.423
[7,     1] loss: 0.368
[8,     1] loss: 0.316
[9,     1] loss: 0.297
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006621642915555168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4260406431,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.4964704755418676}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008227985858397513,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2284337163,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.727052069762584}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.638
[4,     1] loss: 0.600
[5,     1] loss: 0.557
[6,     1] loss: 0.512
[7,     1] loss: 0.469
[8,     1] loss: 0.418
[9,     1] loss: 0.351
[10,     1] loss: 0.289
[11,     1] loss: 0.261
[12,     1] loss: 0.242
[13,     1] loss: 0.180
[14,     1] loss: 0.172
[15,     1] loss: 0.159
[16,     1] loss: 0.189
[17,     1] loss: 0.121
[18,     1] loss: 0.197
[19,     1] loss: 0.120
[20,     1] loss: 0.153
[21,     1] loss: 0.118
[22,     1] loss: 0.125
[23,     1] loss: 0.110
[24,     1] loss: 0.100
[25,     1] loss: 0.089
[26,     1] loss: 0.077
[27,     1] loss: 0.068
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033587345589045866,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1178334301,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.330625334818087}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.669
[4,     1] loss: 0.652
[5,     1] loss: 0.636
[6,     1] loss: 0.616
[7,     1] loss: 0.597
[8,     1] loss: 0.575
[9,     1] loss: 0.553
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0076595583833153,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1073877602,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.889479713155398}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.673
[3,     1] loss: 0.624
[4,     1] loss: 0.579
[5,     1] loss: 0.537
[6,     1] loss: 0.492
[7,     1] loss: 0.447
[8,     1] loss: 0.412
[9,     1] loss: 0.378
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008355306439137905,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2868837795,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.285558397119889}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.650
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022690512414184722,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1958260543,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.368637462517142}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.652
[4,     1] loss: 0.634
[5,     1] loss: 0.620
[6,     1] loss: 0.602
[7,     1] loss: 0.583
[8,     1] loss: 0.565
[9,     1] loss: 0.546
[10,     1] loss: 0.525
[11,     1] loss: 0.508
[12,     1] loss: 0.487
[13,     1] loss: 0.468
[14,     1] loss: 0.443
[15,     1] loss: 0.423
[16,     1] loss: 0.402
[17,     1] loss: 0.377
[18,     1] loss: 0.357
[19,     1] loss: 0.337
[20,     1] loss: 0.317
[21,     1] loss: 0.296
[22,     1] loss: 0.278
[23,     1] loss: 0.266
[24,     1] loss: 0.247
[25,     1] loss: 0.236
[26,     1] loss: 0.220
[27,     1] loss: 0.211
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007033982633274028,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 517310852,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.615005475152985}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.688
[3,     1] loss: 0.667
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008415565730988452,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3283930245,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.654181714574184}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.685
[3,     1] loss: 0.653
[4,     1] loss: 0.615
[5,     1] loss: 0.569
[6,     1] loss: 0.513
[7,     1] loss: 0.448
[8,     1] loss: 0.380
[9,     1] loss: 0.316
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007738880371485199,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1097583358,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.157011389731912}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.672
[3,     1] loss: 0.606
[4,     1] loss: 0.537
[5,     1] loss: 0.487
[6,     1] loss: 0.441
[7,     1] loss: 0.408
[8,     1] loss: 0.371
[9,     1] loss: 0.328
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009834559835216404,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1649824967,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.2031220815936199}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.627
[4,     1] loss: 0.560
[5,     1] loss: 0.490
[6,     1] loss: 0.412
[7,     1] loss: 0.320
[8,     1] loss: 0.259
[9,     1] loss: 0.220
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-P'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003574727037033843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 428690216,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.599271481910993}
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.688
[3,     1] loss: 0.671
[4,     1] loss: 0.653
[5,     1] loss: 0.635
[6,     1] loss: 0.615
[7,     1] loss: 0.596
[8,     1] loss: 0.577
[9,     1] loss: 0.553
[10,     1] loss: 0.526
[11,     1] loss: 0.502
[12,     1] loss: 0.480
[13,     1] loss: 0.456
[14,     1] loss: 0.435
[15,     1] loss: 0.417
[16,     1] loss: 0.403
[17,     1] loss: 0.387
[18,     1] loss: 0.377
[19,     1] loss: 0.370
[20,     1] loss: 0.359
[21,     1] loss: 0.354
[22,     1] loss: 0.348
[23,     1] loss: 0.340
[24,     1] loss: 0.334
[25,     1] loss: 0.328
[26,     1] loss: 0.321
[27,     1] loss: 0.315
[28,     1] loss: 0.310
[29,     1] loss: 0.301
[30,     1] loss: 0.299
[31,     1] loss: 0.290
[32,     1] loss: 0.290
[33,     1] loss: 0.281
[34,     1] loss: 0.286
[35,     1] loss: 0.283
[36,     1] loss: 0.281
[37,     1] loss: 0.280
[38,     1] loss: 0.270
[39,     1] loss: 0.262
[40,     1] loss: 0.263
[41,     1] loss: 0.249
[42,     1] loss: 0.244
[43,     1] loss: 0.240
[44,     1] loss: 0.233
[45,     1] loss: 0.226
[46,     1] loss: 0.220
[47,     1] loss: 0.217
[48,     1] loss: 0.212
[49,     1] loss: 0.205
[50,     1] loss: 0.199
[51,     1] loss: 0.205
[52,     1] loss: 0.207
[53,     1] loss: 0.201
[54,     1] loss: 0.200
[55,     1] loss: 0.192
[56,     1] loss: 0.183
[57,     1] loss: 0.178
[58,     1] loss: 0.172
[59,     1] loss: 0.165
[60,     1] loss: 0.161
[61,     1] loss: 0.156
[62,     1] loss: 0.151
[63,     1] loss: 0.147
[64,     1] loss: 0.144
[65,     1] loss: 0.140
[66,     1] loss: 0.136
[67,     1] loss: 0.136
[68,     1] loss: 0.152
[69,     1] loss: 0.206
[70,     1] loss: 0.492
[71,     1] loss: 0.644
[72,     1] loss: 0.481
[73,     1] loss: 0.483
[74,     1] loss: 0.477
[75,     1] loss: 0.462
[76,     1] loss: 0.481
[77,     1] loss: 0.492
[78,     1] loss: 0.498
[79,     1] loss: 0.499
[80,     1] loss: 0.499
[81,     1] loss: 0.494
[82,     1] loss: 0.491
[83,     1] loss: 0.483
[84,     1] loss: 0.475
[85,     1] loss: 0.464
[86,     1] loss: 0.452
[87,     1] loss: 0.441
[88,     1] loss: 0.425
[89,     1] loss: 0.410
[90,     1] loss: 0.393
[91,     1] loss: 0.375
[92,     1] loss: 0.358
[93,     1] loss: 0.342
[94,     1] loss: 0.326
[95,     1] loss: 0.328
[96,     1] loss: 0.311
[97,     1] loss: 0.288
[98,     1] loss: 0.292
[99,     1] loss: 0.291
[100,     1] loss: 0.289
[101,     1] loss: 0.281
[102,     1] loss: 0.272
[103,     1] loss: 0.265
[104,     1] loss: 0.255
[105,     1] loss: 0.248
[106,     1] loss: 0.240
[107,     1] loss: 0.234
[108,     1] loss: 0.221
[109,     1] loss: 0.225
[110,     1] loss: 0.240
[111,     1] loss: 0.240
[112,     1] loss: 0.240
[113,     1] loss: 0.239
[114,     1] loss: 0.223
[115,     1] loss: 0.212
[116,     1] loss: 0.208
[117,     1] loss: 0.201
[118,     1] loss: 0.197
[119,     1] loss: 0.191
[120,     1] loss: 0.184
[121,     1] loss: 0.180
[122,     1] loss: 0.178
[123,     1] loss: 0.173
[124,     1] loss: 0.168
[125,     1] loss: 0.164
[126,     1] loss: 0.162
[127,     1] loss: 0.161
[128,     1] loss: 0.160
[129,     1] loss: 0.159
[130,     1] loss: 0.160
[131,     1] loss: 0.161
[132,     1] loss: 0.161
[133,     1] loss: 0.159
[134,     1] loss: 0.157
[135,     1] loss: 0.159
[136,     1] loss: 0.160
[137,     1] loss: 0.159
[138,     1] loss: 0.160
[139,     1] loss: 0.164
[140,     1] loss: 0.259
[141,     1] loss: 0.465
[142,     1] loss: 0.403
[143,     1] loss: 0.559
[144,     1] loss: 0.845
[145,     1] loss: 0.700
[146,     1] loss: 0.555
[147,     1] loss: 0.568
[148,     1] loss: 0.576
[149,     1] loss: 0.560
[150,     1] loss: 0.563
[151,     1] loss: 0.581
[152,     1] loss: 0.591
[153,     1] loss: 0.600
[154,     1] loss: 0.607
[155,     1] loss: 0.610
[156,     1] loss: 0.612
[157,     1] loss: 0.614
[158,     1] loss: 0.614
[159,     1] loss: 0.616
[160,     1] loss: 0.617
[161,     1] loss: 0.619
[162,     1] loss: 0.618
[163,     1] loss: 0.620
[164,     1] loss: 0.622
[165,     1] loss: 0.623
[166,     1] loss: 0.624
[167,     1] loss: 0.626
[168,     1] loss: 0.629
[169,     1] loss: 0.630
[170,     1] loss: 0.632
[171,     1] loss: 0.634
[172,     1] loss: 0.636
[173,     1] loss: 0.638
[174,     1] loss: 0.641
[175,     1] loss: 0.642
[176,     1] loss: 0.644
[177,     1] loss: 0.646
[178,     1] loss: 0.648
[179,     1] loss: 0.651
[180,     1] loss: 0.654
[181,     1] loss: 0.655
Early stopping applied (best metric=0.4020361304283142)
Finished Training
Total time taken: 672.6229026317596
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.680
[4,     1] loss: 0.668
[5,     1] loss: 0.655
[6,     1] loss: 0.641
[7,     1] loss: 0.626
[8,     1] loss: 0.609
[9,     1] loss: 0.590
[10,     1] loss: 0.570
[11,     1] loss: 0.550
[12,     1] loss: 0.525
[13,     1] loss: 0.505
[14,     1] loss: 0.482
[15,     1] loss: 0.465
[16,     1] loss: 0.448
[17,     1] loss: 0.436
[18,     1] loss: 0.420
[19,     1] loss: 0.409
[20,     1] loss: 0.398
[21,     1] loss: 0.385
[22,     1] loss: 0.373
[23,     1] loss: 0.361
[24,     1] loss: 0.349
[25,     1] loss: 0.339
[26,     1] loss: 0.325
[27,     1] loss: 0.314
[28,     1] loss: 0.302
[29,     1] loss: 0.291
[30,     1] loss: 0.279
[31,     1] loss: 0.268
[32,     1] loss: 0.259
[33,     1] loss: 0.246
[34,     1] loss: 0.236
[35,     1] loss: 0.224
[36,     1] loss: 0.215
[37,     1] loss: 0.204
[38,     1] loss: 0.196
[39,     1] loss: 0.189
[40,     1] loss: 0.182
[41,     1] loss: 0.172
[42,     1] loss: 0.168
[43,     1] loss: 0.160
[44,     1] loss: 0.143
[45,     1] loss: 0.290
[46,     1] loss: 0.292
[47,     1] loss: 0.674
[48,     1] loss: 0.470
[49,     1] loss: 0.428
[50,     1] loss: 0.410
[51,     1] loss: 0.412
[52,     1] loss: 0.412
[53,     1] loss: 0.402
[54,     1] loss: 0.401
[55,     1] loss: 0.394
[56,     1] loss: 0.383
[57,     1] loss: 0.371
[58,     1] loss: 0.359
[59,     1] loss: 0.349
[60,     1] loss: 0.337
[61,     1] loss: 0.325
[62,     1] loss: 0.314
[63,     1] loss: 0.298
[64,     1] loss: 0.287
[65,     1] loss: 0.270
[66,     1] loss: 0.259
[67,     1] loss: 0.255
[68,     1] loss: 0.241
[69,     1] loss: 0.251
[70,     1] loss: 0.245
[71,     1] loss: 0.219
[72,     1] loss: 0.229
[73,     1] loss: 0.217
[74,     1] loss: 0.203
[75,     1] loss: 0.197
[76,     1] loss: 0.189
[77,     1] loss: 0.182
[78,     1] loss: 0.174
[79,     1] loss: 0.171
[80,     1] loss: 0.163
[81,     1] loss: 0.160
[82,     1] loss: 0.157
[83,     1] loss: 0.153
[84,     1] loss: 0.148
[85,     1] loss: 0.145
[86,     1] loss: 0.143
[87,     1] loss: 0.144
[88,     1] loss: 0.142
[89,     1] loss: 0.143
[90,     1] loss: 0.140
[91,     1] loss: 0.141
[92,     1] loss: 0.141
[93,     1] loss: 0.141
[94,     1] loss: 0.140
[95,     1] loss: 0.142
[96,     1] loss: 0.142
[97,     1] loss: 0.140
[98,     1] loss: 0.141
[99,     1] loss: 0.144
[100,     1] loss: 0.171
[101,     1] loss: 0.259
[102,     1] loss: 0.732
[103,     1] loss: 0.531
[104,     1] loss: 0.496
[105,     1] loss: 0.466
[106,     1] loss: 0.453
[107,     1] loss: 0.469
[108,     1] loss: 0.458
[109,     1] loss: 0.449
[110,     1] loss: 0.442
[111,     1] loss: 0.431
[112,     1] loss: 0.423
[113,     1] loss: 0.418
[114,     1] loss: 0.406
[115,     1] loss: 0.401
[116,     1] loss: 0.396
[117,     1] loss: 0.387
[118,     1] loss: 0.381
[119,     1] loss: 0.367
[120,     1] loss: 0.363
[121,     1] loss: 0.353
[122,     1] loss: 0.344
[123,     1] loss: 0.333
[124,     1] loss: 0.324
[125,     1] loss: 0.311
[126,     1] loss: 0.300
[127,     1] loss: 0.298
[128,     1] loss: 0.283
[129,     1] loss: 0.272
[130,     1] loss: 0.286
[131,     1] loss: 0.270
[132,     1] loss: 0.263
[133,     1] loss: 0.261
[134,     1] loss: 0.250
[135,     1] loss: 0.242
[136,     1] loss: 0.231
[137,     1] loss: 0.219
[138,     1] loss: 0.209
[139,     1] loss: 0.201
[140,     1] loss: 0.193
[141,     1] loss: 0.187
[142,     1] loss: 0.179
[143,     1] loss: 0.174
[144,     1] loss: 0.189
[145,     1] loss: 0.264
[146,     1] loss: 0.606
[147,     1] loss: 0.481
[148,     1] loss: 0.421
[149,     1] loss: 0.357
[150,     1] loss: 0.356
[151,     1] loss: 0.367
[152,     1] loss: 0.341
[153,     1] loss: 0.318
Early stopping applied (best metric=0.3916783034801483)
Finished Training
Total time taken: 570.608827829361
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.667
[4,     1] loss: 0.652
[5,     1] loss: 0.638
[6,     1] loss: 0.622
[7,     1] loss: 0.603
[8,     1] loss: 0.581
[9,     1] loss: 0.556
[10,     1] loss: 0.530
[11,     1] loss: 0.505
[12,     1] loss: 0.480
[13,     1] loss: 0.457
[14,     1] loss: 0.444
[15,     1] loss: 0.425
[16,     1] loss: 0.417
[17,     1] loss: 0.401
[18,     1] loss: 0.393
[19,     1] loss: 0.386
[20,     1] loss: 0.381
[21,     1] loss: 0.377
[22,     1] loss: 0.374
[23,     1] loss: 0.372
[24,     1] loss: 0.369
[25,     1] loss: 0.368
[26,     1] loss: 0.366
[27,     1] loss: 0.365
[28,     1] loss: 0.363
[29,     1] loss: 0.363
[30,     1] loss: 0.362
[31,     1] loss: 0.361
[32,     1] loss: 0.360
[33,     1] loss: 0.359
[34,     1] loss: 0.356
[35,     1] loss: 0.355
[36,     1] loss: 0.352
[37,     1] loss: 0.349
[38,     1] loss: 0.346
[39,     1] loss: 0.342
[40,     1] loss: 0.336
[41,     1] loss: 0.333
[42,     1] loss: 0.327
[43,     1] loss: 0.320
[44,     1] loss: 0.316
[45,     1] loss: 0.308
[46,     1] loss: 0.301
[47,     1] loss: 0.293
[48,     1] loss: 0.286
[49,     1] loss: 0.280
[50,     1] loss: 0.281
[51,     1] loss: 0.297
[52,     1] loss: 0.603
[53,     1] loss: 0.562
[54,     1] loss: 0.543
[55,     1] loss: 0.494
[56,     1] loss: 0.451
[57,     1] loss: 0.436
[58,     1] loss: 0.420
[59,     1] loss: 0.398
[60,     1] loss: 0.378
[61,     1] loss: 0.359
[62,     1] loss: 0.338
[63,     1] loss: 0.325
[64,     1] loss: 0.308
[65,     1] loss: 0.294
[66,     1] loss: 0.284
[67,     1] loss: 0.270
[68,     1] loss: 0.262
[69,     1] loss: 0.376
[70,     1] loss: 0.272
[71,     1] loss: 0.282
[72,     1] loss: 0.281
[73,     1] loss: 0.271
[74,     1] loss: 0.266
[75,     1] loss: 0.263
[76,     1] loss: 0.259
[77,     1] loss: 0.253
[78,     1] loss: 0.249
[79,     1] loss: 0.243
[80,     1] loss: 0.241
[81,     1] loss: 0.233
[82,     1] loss: 0.227
[83,     1] loss: 0.222
[84,     1] loss: 0.216
[85,     1] loss: 0.209
[86,     1] loss: 0.202
[87,     1] loss: 0.195
[88,     1] loss: 0.193
[89,     1] loss: 0.194
[90,     1] loss: 0.188
[91,     1] loss: 0.181
[92,     1] loss: 0.185
[93,     1] loss: 0.190
[94,     1] loss: 0.181
[95,     1] loss: 0.191
[96,     1] loss: 0.206
[97,     1] loss: 0.191
[98,     1] loss: 0.211
[99,     1] loss: 0.311
[100,     1] loss: 0.444
[101,     1] loss: 0.247
[102,     1] loss: 0.308
[103,     1] loss: 0.305
[104,     1] loss: 0.258
[105,     1] loss: 0.269
Early stopping applied (best metric=0.4253779351711273)
Finished Training
Total time taken: 392.8706052303314
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.691
[3,     1] loss: 0.678
[4,     1] loss: 0.668
[5,     1] loss: 0.657
[6,     1] loss: 0.647
[7,     1] loss: 0.638
[8,     1] loss: 0.626
[9,     1] loss: 0.614
[10,     1] loss: 0.602
[11,     1] loss: 0.587
[12,     1] loss: 0.574
[13,     1] loss: 0.559
[14,     1] loss: 0.546
[15,     1] loss: 0.529
[16,     1] loss: 0.514
[17,     1] loss: 0.497
[18,     1] loss: 0.481
[19,     1] loss: 0.466
[20,     1] loss: 0.447
[21,     1] loss: 0.434
[22,     1] loss: 0.417
[23,     1] loss: 0.403
[24,     1] loss: 0.386
[25,     1] loss: 0.372
[26,     1] loss: 0.356
[27,     1] loss: 0.340
[28,     1] loss: 0.328
[29,     1] loss: 0.314
[30,     1] loss: 0.300
[31,     1] loss: 0.288
[32,     1] loss: 0.278
[33,     1] loss: 0.267
[34,     1] loss: 0.254
[35,     1] loss: 0.248
[36,     1] loss: 0.240
[37,     1] loss: 0.232
[38,     1] loss: 0.223
[39,     1] loss: 0.220
[40,     1] loss: 0.214
[41,     1] loss: 0.207
[42,     1] loss: 0.198
[43,     1] loss: 0.230
[44,     1] loss: 0.376
[45,     1] loss: 0.472
[46,     1] loss: 0.426
[47,     1] loss: 0.363
[48,     1] loss: 0.370
[49,     1] loss: 0.375
[50,     1] loss: 0.367
[51,     1] loss: 0.358
[52,     1] loss: 0.353
[53,     1] loss: 0.346
[54,     1] loss: 0.338
[55,     1] loss: 0.331
[56,     1] loss: 0.318
[57,     1] loss: 0.312
[58,     1] loss: 0.300
[59,     1] loss: 0.290
[60,     1] loss: 0.282
[61,     1] loss: 0.275
[62,     1] loss: 0.267
[63,     1] loss: 0.258
[64,     1] loss: 0.254
[65,     1] loss: 0.247
[66,     1] loss: 0.239
[67,     1] loss: 0.235
[68,     1] loss: 0.231
[69,     1] loss: 0.226
[70,     1] loss: 0.221
[71,     1] loss: 0.217
[72,     1] loss: 0.213
[73,     1] loss: 0.211
[74,     1] loss: 0.206
[75,     1] loss: 0.203
[76,     1] loss: 0.203
[77,     1] loss: 0.199
[78,     1] loss: 0.199
[79,     1] loss: 0.211
[80,     1] loss: 0.355
[81,     1] loss: 0.343
[82,     1] loss: 0.340
[83,     1] loss: 0.358
[84,     1] loss: 0.481
[85,     1] loss: 0.488
[86,     1] loss: 0.465
[87,     1] loss: 0.441
[88,     1] loss: 0.436
[89,     1] loss: 0.434
[90,     1] loss: 0.432
[91,     1] loss: 0.431
[92,     1] loss: 0.425
[93,     1] loss: 0.419
[94,     1] loss: 0.411
[95,     1] loss: 0.405
[96,     1] loss: 0.401
[97,     1] loss: 0.392
[98,     1] loss: 0.385
[99,     1] loss: 0.380
[100,     1] loss: 0.371
[101,     1] loss: 0.365
[102,     1] loss: 0.356
[103,     1] loss: 0.350
[104,     1] loss: 0.351
[105,     1] loss: 0.334
[106,     1] loss: 0.335
[107,     1] loss: 0.331
[108,     1] loss: 0.318
[109,     1] loss: 0.313
[110,     1] loss: 0.302
[111,     1] loss: 0.293
[112,     1] loss: 0.295
[113,     1] loss: 0.303
[114,     1] loss: 0.289
[115,     1] loss: 0.286
[116,     1] loss: 0.273
[117,     1] loss: 0.265
[118,     1] loss: 0.254
[119,     1] loss: 0.248
[120,     1] loss: 0.247
[121,     1] loss: 0.245
[122,     1] loss: 0.266
[123,     1] loss: 0.238
[124,     1] loss: 0.257
[125,     1] loss: 0.294
[126,     1] loss: 0.341
[127,     1] loss: 0.336
[128,     1] loss: 0.288
[129,     1] loss: 0.287
[130,     1] loss: 0.272
[131,     1] loss: 0.266
[132,     1] loss: 0.267
[133,     1] loss: 0.274
[134,     1] loss: 0.259
[135,     1] loss: 0.268
[136,     1] loss: 0.258
[137,     1] loss: 0.255
[138,     1] loss: 0.250
[139,     1] loss: 0.242
[140,     1] loss: 0.241
[141,     1] loss: 0.240
[142,     1] loss: 0.243
[143,     1] loss: 0.237
[144,     1] loss: 0.238
[145,     1] loss: 0.236
[146,     1] loss: 0.237
[147,     1] loss: 0.248
[148,     1] loss: 0.248
[149,     1] loss: 0.242
[150,     1] loss: 0.243
[151,     1] loss: 0.238
[152,     1] loss: 0.242
[153,     1] loss: 0.240
[154,     1] loss: 0.244
[155,     1] loss: 0.259
[156,     1] loss: 0.279
[157,     1] loss: 0.263
[158,     1] loss: 0.276
[159,     1] loss: 0.279
[160,     1] loss: 0.266
[161,     1] loss: 0.266
[162,     1] loss: 0.282
[163,     1] loss: 0.265
[164,     1] loss: 0.267
[165,     1] loss: 0.261
[166,     1] loss: 0.265
[167,     1] loss: 0.254
[168,     1] loss: 0.252
[169,     1] loss: 0.253
[170,     1] loss: 0.250
[171,     1] loss: 0.251
[172,     1] loss: 0.259
[173,     1] loss: 0.252
[174,     1] loss: 0.258
[175,     1] loss: 0.267
[176,     1] loss: 0.264
[177,     1] loss: 0.273
[178,     1] loss: 0.265
[179,     1] loss: 0.261
[180,     1] loss: 0.260
[181,     1] loss: 0.262
[182,     1] loss: 0.264
[183,     1] loss: 0.258
[184,     1] loss: 0.262
[185,     1] loss: 0.276
[186,     1] loss: 0.273
[187,     1] loss: 0.273
[188,     1] loss: 0.268
[189,     1] loss: 0.269
[190,     1] loss: 0.264
[191,     1] loss: 0.264
[192,     1] loss: 0.264
[193,     1] loss: 0.277
[194,     1] loss: 0.264
[195,     1] loss: 0.266
[196,     1] loss: 0.281
[197,     1] loss: 0.275
[198,     1] loss: 0.279
[199,     1] loss: 0.277
Early stopping applied (best metric=0.38488876819610596)
Finished Training
Total time taken: 742.6260867118835
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.661
[4,     1] loss: 0.642
[5,     1] loss: 0.624
[6,     1] loss: 0.605
[7,     1] loss: 0.583
[8,     1] loss: 0.560
[9,     1] loss: 0.535
[10,     1] loss: 0.508
[11,     1] loss: 0.483
[12,     1] loss: 0.454
[13,     1] loss: 0.426
[14,     1] loss: 0.401
[15,     1] loss: 0.379
[16,     1] loss: 0.356
[17,     1] loss: 0.335
[18,     1] loss: 0.317
[19,     1] loss: 0.296
[20,     1] loss: 0.280
[21,     1] loss: 0.265
[22,     1] loss: 0.254
[23,     1] loss: 0.240
[24,     1] loss: 0.231
[25,     1] loss: 0.220
[26,     1] loss: 0.213
[27,     1] loss: 0.207
[28,     1] loss: 0.199
[29,     1] loss: 0.194
[30,     1] loss: 0.189
[31,     1] loss: 0.184
[32,     1] loss: 0.181
[33,     1] loss: 0.178
[34,     1] loss: 0.174
[35,     1] loss: 0.174
[36,     1] loss: 0.172
[37,     1] loss: 0.171
[38,     1] loss: 0.169
[39,     1] loss: 0.168
[40,     1] loss: 0.169
[41,     1] loss: 0.166
[42,     1] loss: 0.168
[43,     1] loss: 0.251
[44,     1] loss: 0.721
[45,     1] loss: 0.664
[46,     1] loss: 0.557
[47,     1] loss: 0.486
[48,     1] loss: 0.479
[49,     1] loss: 0.485
[50,     1] loss: 0.483
[51,     1] loss: 0.484
[52,     1] loss: 0.478
[53,     1] loss: 0.472
[54,     1] loss: 0.464
[55,     1] loss: 0.455
[56,     1] loss: 0.446
[57,     1] loss: 0.438
[58,     1] loss: 0.432
[59,     1] loss: 0.420
[60,     1] loss: 0.415
[61,     1] loss: 0.411
[62,     1] loss: 0.403
[63,     1] loss: 0.401
[64,     1] loss: 0.395
[65,     1] loss: 0.391
[66,     1] loss: 0.390
[67,     1] loss: 0.384
[68,     1] loss: 0.381
[69,     1] loss: 0.379
[70,     1] loss: 0.375
[71,     1] loss: 0.374
[72,     1] loss: 0.372
[73,     1] loss: 0.372
Early stopping applied (best metric=0.36665818095207214)
Finished Training
Total time taken: 275.14717531204224
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.654
[4,     1] loss: 0.635
[5,     1] loss: 0.617
[6,     1] loss: 0.599
[7,     1] loss: 0.578
[8,     1] loss: 0.552
[9,     1] loss: 0.528
[10,     1] loss: 0.501
[11,     1] loss: 0.479
[12,     1] loss: 0.454
[13,     1] loss: 0.434
[14,     1] loss: 0.415
[15,     1] loss: 0.404
[16,     1] loss: 0.392
[17,     1] loss: 0.383
[18,     1] loss: 0.379
[19,     1] loss: 0.380
[20,     1] loss: 0.376
[21,     1] loss: 0.379
[22,     1] loss: 0.370
[23,     1] loss: 0.374
[24,     1] loss: 0.368
[25,     1] loss: 0.373
[26,     1] loss: 0.370
[27,     1] loss: 0.368
[28,     1] loss: 0.368
[29,     1] loss: 0.367
[30,     1] loss: 0.368
[31,     1] loss: 0.367
[32,     1] loss: 0.368
[33,     1] loss: 0.367
[34,     1] loss: 0.367
[35,     1] loss: 0.367
[36,     1] loss: 0.365
[37,     1] loss: 0.364
[38,     1] loss: 0.364
[39,     1] loss: 0.366
[40,     1] loss: 0.363
[41,     1] loss: 0.366
[42,     1] loss: 0.365
[43,     1] loss: 0.364
[44,     1] loss: 0.360
[45,     1] loss: 0.361
[46,     1] loss: 0.360
[47,     1] loss: 0.356
[48,     1] loss: 0.355
[49,     1] loss: 0.352
[50,     1] loss: 0.348
[51,     1] loss: 0.345
[52,     1] loss: 0.341
[53,     1] loss: 0.336
[54,     1] loss: 0.334
[55,     1] loss: 0.327
[56,     1] loss: 0.320
[57,     1] loss: 0.352
[58,     1] loss: 0.369
[59,     1] loss: 0.440
[60,     1] loss: 0.498
[61,     1] loss: 0.414
[62,     1] loss: 0.394
[63,     1] loss: 0.397
[64,     1] loss: 0.391
[65,     1] loss: 0.367
[66,     1] loss: 0.350
[67,     1] loss: 0.341
[68,     1] loss: 0.319
[69,     1] loss: 0.309
[70,     1] loss: 0.297
[71,     1] loss: 0.291
[72,     1] loss: 0.280
[73,     1] loss: 0.271
[74,     1] loss: 0.260
[75,     1] loss: 0.249
[76,     1] loss: 0.240
[77,     1] loss: 0.228
[78,     1] loss: 0.219
[79,     1] loss: 0.208
[80,     1] loss: 0.199
[81,     1] loss: 0.191
[82,     1] loss: 0.180
[83,     1] loss: 0.172
[84,     1] loss: 0.163
[85,     1] loss: 0.157
[86,     1] loss: 0.148
[87,     1] loss: 0.146
[88,     1] loss: 0.134
[89,     1] loss: 0.129
[90,     1] loss: 0.125
[91,     1] loss: 0.118
[92,     1] loss: 0.120
[93,     1] loss: 0.146
[94,     1] loss: 0.145
[95,     1] loss: 0.155
[96,     1] loss: 0.144
[97,     1] loss: 0.140
[98,     1] loss: 0.154
[99,     1] loss: 0.523
[100,     1] loss: 0.300
[101,     1] loss: 0.255
[102,     1] loss: 0.364
[103,     1] loss: 0.341
[104,     1] loss: 0.328
[105,     1] loss: 0.293
[106,     1] loss: 0.284
[107,     1] loss: 0.265
[108,     1] loss: 0.267
[109,     1] loss: 0.240
[110,     1] loss: 0.228
[111,     1] loss: 0.215
[112,     1] loss: 0.205
[113,     1] loss: 0.194
[114,     1] loss: 0.185
[115,     1] loss: 0.175
[116,     1] loss: 0.166
[117,     1] loss: 0.159
[118,     1] loss: 0.156
[119,     1] loss: 0.154
[120,     1] loss: 0.154
[121,     1] loss: 0.153
[122,     1] loss: 0.152
[123,     1] loss: 0.153
[124,     1] loss: 0.152
[125,     1] loss: 0.151
[126,     1] loss: 0.156
[127,     1] loss: 0.158
[128,     1] loss: 0.158
[129,     1] loss: 0.159
[130,     1] loss: 0.160
[131,     1] loss: 0.161
[132,     1] loss: 0.161
[133,     1] loss: 0.161
[134,     1] loss: 0.160
[135,     1] loss: 0.163
[136,     1] loss: 0.161
[137,     1] loss: 0.161
[138,     1] loss: 0.163
[139,     1] loss: 0.162
[140,     1] loss: 0.162
[141,     1] loss: 0.162
[142,     1] loss: 0.162
[143,     1] loss: 0.163
[144,     1] loss: 0.162
[145,     1] loss: 0.168
[146,     1] loss: 0.211
[147,     1] loss: 0.219
[148,     1] loss: 0.356
[149,     1] loss: 0.754
[150,     1] loss: 0.672
[151,     1] loss: 0.650
[152,     1] loss: 0.624
[153,     1] loss: 0.611
[154,     1] loss: 0.605
[155,     1] loss: 0.608
[156,     1] loss: 0.613
[157,     1] loss: 0.616
[158,     1] loss: 0.618
[159,     1] loss: 0.619
[160,     1] loss: 0.620
[161,     1] loss: 0.621
[162,     1] loss: 0.623
[163,     1] loss: 0.625
[164,     1] loss: 0.627
[165,     1] loss: 0.627
[166,     1] loss: 0.629
[167,     1] loss: 0.631
[168,     1] loss: 0.633
[169,     1] loss: 0.634
[170,     1] loss: 0.637
[171,     1] loss: 0.637
[172,     1] loss: 0.639
[173,     1] loss: 0.641
[174,     1] loss: 0.642
[175,     1] loss: 0.643
[176,     1] loss: 0.644
[177,     1] loss: 0.645
Early stopping applied (best metric=0.30470117926597595)
Finished Training
Total time taken: 663.6103088855743
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.661
[4,     1] loss: 0.643
[5,     1] loss: 0.626
[6,     1] loss: 0.609
[7,     1] loss: 0.592
[8,     1] loss: 0.577
[9,     1] loss: 0.559
[10,     1] loss: 0.540
[11,     1] loss: 0.523
[12,     1] loss: 0.502
[13,     1] loss: 0.482
[14,     1] loss: 0.462
[15,     1] loss: 0.445
[16,     1] loss: 0.426
[17,     1] loss: 0.407
[18,     1] loss: 0.392
[19,     1] loss: 0.374
[20,     1] loss: 0.361
[21,     1] loss: 0.345
[22,     1] loss: 0.330
[23,     1] loss: 0.311
[24,     1] loss: 0.297
[25,     1] loss: 0.290
[26,     1] loss: 0.280
[27,     1] loss: 0.267
[28,     1] loss: 0.257
[29,     1] loss: 0.245
[30,     1] loss: 0.235
[31,     1] loss: 0.229
[32,     1] loss: 0.221
[33,     1] loss: 0.213
[34,     1] loss: 0.207
[35,     1] loss: 0.202
[36,     1] loss: 0.197
[37,     1] loss: 0.193
[38,     1] loss: 0.187
[39,     1] loss: 0.180
[40,     1] loss: 0.172
[41,     1] loss: 0.169
[42,     1] loss: 0.205
[43,     1] loss: 0.183
[44,     1] loss: 0.210
[45,     1] loss: 0.359
[46,     1] loss: 0.393
[47,     1] loss: 0.366
[48,     1] loss: 0.355
[49,     1] loss: 0.371
[50,     1] loss: 0.350
[51,     1] loss: 0.343
[52,     1] loss: 0.337
[53,     1] loss: 0.333
[54,     1] loss: 0.329
[55,     1] loss: 0.320
[56,     1] loss: 0.313
[57,     1] loss: 0.300
[58,     1] loss: 0.292
[59,     1] loss: 0.284
[60,     1] loss: 0.272
[61,     1] loss: 0.263
[62,     1] loss: 0.255
[63,     1] loss: 0.246
[64,     1] loss: 0.237
[65,     1] loss: 0.226
[66,     1] loss: 0.221
[67,     1] loss: 0.215
[68,     1] loss: 0.212
[69,     1] loss: 0.206
[70,     1] loss: 0.201
[71,     1] loss: 0.197
[72,     1] loss: 0.196
[73,     1] loss: 0.199
[74,     1] loss: 0.249
[75,     1] loss: 0.242
[76,     1] loss: 0.245
[77,     1] loss: 0.247
[78,     1] loss: 0.242
[79,     1] loss: 0.237
[80,     1] loss: 0.232
[81,     1] loss: 0.227
[82,     1] loss: 0.223
[83,     1] loss: 0.218
[84,     1] loss: 0.213
[85,     1] loss: 0.210
[86,     1] loss: 0.204
[87,     1] loss: 0.203
[88,     1] loss: 0.199
[89,     1] loss: 0.198
Early stopping applied (best metric=0.33646640181541443)
Finished Training
Total time taken: 335.3518633842468
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.666
[5,     1] loss: 0.657
[6,     1] loss: 0.647
[7,     1] loss: 0.637
[8,     1] loss: 0.624
[9,     1] loss: 0.614
[10,     1] loss: 0.600
[11,     1] loss: 0.586
[12,     1] loss: 0.571
[13,     1] loss: 0.553
[14,     1] loss: 0.536
[15,     1] loss: 0.518
[16,     1] loss: 0.502
[17,     1] loss: 0.485
[18,     1] loss: 0.468
[19,     1] loss: 0.444
[20,     1] loss: 0.426
[21,     1] loss: 0.407
[22,     1] loss: 0.388
[23,     1] loss: 0.368
[24,     1] loss: 0.352
[25,     1] loss: 0.334
[26,     1] loss: 0.317
[27,     1] loss: 0.302
[28,     1] loss: 0.286
[29,     1] loss: 0.274
[30,     1] loss: 0.259
[31,     1] loss: 0.247
[32,     1] loss: 0.237
[33,     1] loss: 0.230
[34,     1] loss: 0.217
[35,     1] loss: 0.210
[36,     1] loss: 0.202
[37,     1] loss: 0.195
[38,     1] loss: 0.190
[39,     1] loss: 0.186
[40,     1] loss: 0.198
[41,     1] loss: 0.199
[42,     1] loss: 0.185
[43,     1] loss: 0.174
[44,     1] loss: 0.186
[45,     1] loss: 0.295
[46,     1] loss: 0.339
[47,     1] loss: 0.382
[48,     1] loss: 0.328
[49,     1] loss: 0.330
[50,     1] loss: 0.321
[51,     1] loss: 0.302
[52,     1] loss: 0.300
[53,     1] loss: 0.293
[54,     1] loss: 0.295
[55,     1] loss: 0.287
[56,     1] loss: 0.274
[57,     1] loss: 0.267
[58,     1] loss: 0.258
[59,     1] loss: 0.249
[60,     1] loss: 0.238
[61,     1] loss: 0.230
[62,     1] loss: 0.222
[63,     1] loss: 0.212
[64,     1] loss: 0.209
[65,     1] loss: 0.205
[66,     1] loss: 0.198
[67,     1] loss: 0.188
[68,     1] loss: 0.183
[69,     1] loss: 0.179
[70,     1] loss: 0.184
[71,     1] loss: 0.206
[72,     1] loss: 0.185
[73,     1] loss: 0.186
[74,     1] loss: 0.178
[75,     1] loss: 0.185
[76,     1] loss: 0.179
[77,     1] loss: 0.181
[78,     1] loss: 0.176
[79,     1] loss: 0.175
[80,     1] loss: 0.173
[81,     1] loss: 0.169
[82,     1] loss: 0.170
[83,     1] loss: 0.168
[84,     1] loss: 0.168
[85,     1] loss: 0.165
[86,     1] loss: 0.163
[87,     1] loss: 0.165
[88,     1] loss: 0.164
[89,     1] loss: 0.162
[90,     1] loss: 0.165
[91,     1] loss: 0.163
[92,     1] loss: 0.165
[93,     1] loss: 0.166
[94,     1] loss: 0.166
[95,     1] loss: 0.164
[96,     1] loss: 0.168
[97,     1] loss: 0.167
[98,     1] loss: 0.167
[99,     1] loss: 0.167
[100,     1] loss: 0.173
[101,     1] loss: 0.175
[102,     1] loss: 0.323
[103,     1] loss: 0.558
[104,     1] loss: 0.802
[105,     1] loss: 0.638
[106,     1] loss: 0.556
[107,     1] loss: 0.554
[108,     1] loss: 0.582
[109,     1] loss: 0.590
[110,     1] loss: 0.589
[111,     1] loss: 0.590
[112,     1] loss: 0.597
[113,     1] loss: 0.604
Early stopping applied (best metric=0.355439692735672)
Finished Training
Total time taken: 425.6510639190674
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.682
[3,     1] loss: 0.655
[4,     1] loss: 0.630
[5,     1] loss: 0.608
[6,     1] loss: 0.587
[7,     1] loss: 0.568
[8,     1] loss: 0.547
[9,     1] loss: 0.526
[10,     1] loss: 0.500
[11,     1] loss: 0.477
[12,     1] loss: 0.455
[13,     1] loss: 0.434
[14,     1] loss: 0.415
[15,     1] loss: 0.403
[16,     1] loss: 0.393
[17,     1] loss: 0.388
[18,     1] loss: 0.382
[19,     1] loss: 0.377
[20,     1] loss: 0.374
[21,     1] loss: 0.370
[22,     1] loss: 0.367
[23,     1] loss: 0.366
[24,     1] loss: 0.365
[25,     1] loss: 0.363
[26,     1] loss: 0.362
[27,     1] loss: 0.362
[28,     1] loss: 0.361
[29,     1] loss: 0.360
[30,     1] loss: 0.359
[31,     1] loss: 0.357
[32,     1] loss: 0.356
[33,     1] loss: 0.354
[34,     1] loss: 0.352
[35,     1] loss: 0.350
[36,     1] loss: 0.348
[37,     1] loss: 0.344
[38,     1] loss: 0.342
[39,     1] loss: 0.343
[40,     1] loss: 0.336
[41,     1] loss: 0.333
[42,     1] loss: 0.325
[43,     1] loss: 0.320
[44,     1] loss: 0.315
[45,     1] loss: 0.309
[46,     1] loss: 0.303
[47,     1] loss: 0.297
[48,     1] loss: 0.289
[49,     1] loss: 0.283
[50,     1] loss: 0.276
[51,     1] loss: 0.287
[52,     1] loss: 0.277
[53,     1] loss: 0.266
[54,     1] loss: 0.303
[55,     1] loss: 0.492
[56,     1] loss: 0.454
[57,     1] loss: 0.424
[58,     1] loss: 0.390
[59,     1] loss: 0.405
[60,     1] loss: 0.394
[61,     1] loss: 0.366
[62,     1] loss: 0.354
[63,     1] loss: 0.340
[64,     1] loss: 0.330
[65,     1] loss: 0.314
[66,     1] loss: 0.301
[67,     1] loss: 0.289
[68,     1] loss: 0.281
[69,     1] loss: 0.268
[70,     1] loss: 0.260
[71,     1] loss: 0.250
[72,     1] loss: 0.238
[73,     1] loss: 0.231
[74,     1] loss: 0.221
[75,     1] loss: 0.217
[76,     1] loss: 0.212
[77,     1] loss: 0.203
[78,     1] loss: 0.202
[79,     1] loss: 0.196
[80,     1] loss: 0.191
[81,     1] loss: 0.191
[82,     1] loss: 0.189
[83,     1] loss: 0.183
[84,     1] loss: 0.182
[85,     1] loss: 0.179
[86,     1] loss: 0.178
[87,     1] loss: 0.177
[88,     1] loss: 0.176
[89,     1] loss: 0.175
[90,     1] loss: 0.173
[91,     1] loss: 0.169
[92,     1] loss: 0.167
[93,     1] loss: 0.167
[94,     1] loss: 0.169
[95,     1] loss: 0.168
[96,     1] loss: 0.166
[97,     1] loss: 0.165
[98,     1] loss: 0.166
[99,     1] loss: 0.164
[100,     1] loss: 0.166
[101,     1] loss: 0.168
[102,     1] loss: 0.166
[103,     1] loss: 0.168
[104,     1] loss: 0.172
[105,     1] loss: 0.215
[106,     1] loss: 0.230
[107,     1] loss: 0.348
[108,     1] loss: 0.490
[109,     1] loss: 0.704
[110,     1] loss: 0.539
[111,     1] loss: 0.551
[112,     1] loss: 0.547
[113,     1] loss: 0.513
[114,     1] loss: 0.534
[115,     1] loss: 0.545
[116,     1] loss: 0.550
[117,     1] loss: 0.550
[118,     1] loss: 0.551
[119,     1] loss: 0.552
[120,     1] loss: 0.551
[121,     1] loss: 0.556
[122,     1] loss: 0.555
[123,     1] loss: 0.551
[124,     1] loss: 0.551
[125,     1] loss: 0.546
[126,     1] loss: 0.542
[127,     1] loss: 0.535
[128,     1] loss: 0.530
[129,     1] loss: 0.523
[130,     1] loss: 0.515
[131,     1] loss: 0.506
[132,     1] loss: 0.495
[133,     1] loss: 0.484
[134,     1] loss: 0.474
[135,     1] loss: 0.460
Early stopping applied (best metric=0.38799381256103516)
Finished Training
Total time taken: 510.51216888427734
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.659
[4,     1] loss: 0.641
[5,     1] loss: 0.622
[6,     1] loss: 0.606
[7,     1] loss: 0.586
[8,     1] loss: 0.566
[9,     1] loss: 0.545
[10,     1] loss: 0.524
[11,     1] loss: 0.500
[12,     1] loss: 0.474
[13,     1] loss: 0.452
[14,     1] loss: 0.426
[15,     1] loss: 0.405
[16,     1] loss: 0.381
[17,     1] loss: 0.360
[18,     1] loss: 0.340
[19,     1] loss: 0.319
[20,     1] loss: 0.304
[21,     1] loss: 0.313
[22,     1] loss: 0.287
[23,     1] loss: 0.323
[24,     1] loss: 0.290
[25,     1] loss: 0.317
[26,     1] loss: 0.266
[27,     1] loss: 0.283
[28,     1] loss: 0.269
[29,     1] loss: 0.270
[30,     1] loss: 0.261
[31,     1] loss: 0.256
[32,     1] loss: 0.248
[33,     1] loss: 0.248
[34,     1] loss: 0.240
[35,     1] loss: 0.233
[36,     1] loss: 0.228
[37,     1] loss: 0.220
[38,     1] loss: 0.219
[39,     1] loss: 0.213
[40,     1] loss: 0.208
[41,     1] loss: 0.205
[42,     1] loss: 0.198
[43,     1] loss: 0.197
[44,     1] loss: 0.192
[45,     1] loss: 0.192
[46,     1] loss: 0.199
[47,     1] loss: 0.209
[48,     1] loss: 0.210
[49,     1] loss: 0.206
[50,     1] loss: 0.204
[51,     1] loss: 0.200
[52,     1] loss: 0.195
[53,     1] loss: 0.191
[54,     1] loss: 0.185
[55,     1] loss: 0.180
[56,     1] loss: 0.173
[57,     1] loss: 0.170
[58,     1] loss: 0.244
[59,     1] loss: 0.396
[60,     1] loss: 0.727
[61,     1] loss: 0.398
[62,     1] loss: 0.471
[63,     1] loss: 0.499
[64,     1] loss: 0.502
[65,     1] loss: 0.507
[66,     1] loss: 0.517
[67,     1] loss: 0.523
[68,     1] loss: 0.530
[69,     1] loss: 0.535
[70,     1] loss: 0.539
[71,     1] loss: 0.541
[72,     1] loss: 0.544
[73,     1] loss: 0.543
[74,     1] loss: 0.546
[75,     1] loss: 0.546
[76,     1] loss: 0.546
[77,     1] loss: 0.548
[78,     1] loss: 0.547
[79,     1] loss: 0.545
[80,     1] loss: 0.546
[81,     1] loss: 0.541
[82,     1] loss: 0.540
[83,     1] loss: 0.536
[84,     1] loss: 0.533
[85,     1] loss: 0.528
[86,     1] loss: 0.522
[87,     1] loss: 0.518
[88,     1] loss: 0.512
[89,     1] loss: 0.508
[90,     1] loss: 0.501
[91,     1] loss: 0.495
[92,     1] loss: 0.489
[93,     1] loss: 0.485
[94,     1] loss: 0.484
[95,     1] loss: 0.473
[96,     1] loss: 0.465
[97,     1] loss: 0.457
[98,     1] loss: 0.452
[99,     1] loss: 0.451
[100,     1] loss: 0.476
[101,     1] loss: 0.462
[102,     1] loss: 0.450
[103,     1] loss: 0.442
[104,     1] loss: 0.433
[105,     1] loss: 0.430
Early stopping applied (best metric=0.3508804440498352)
Finished Training
Total time taken: 397.10213255882263
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.678
[3,     1] loss: 0.648
[4,     1] loss: 0.626
[5,     1] loss: 0.605
[6,     1] loss: 0.592
[7,     1] loss: 0.575
[8,     1] loss: 0.555
[9,     1] loss: 0.538
[10,     1] loss: 0.519
[11,     1] loss: 0.502
[12,     1] loss: 0.483
[13,     1] loss: 0.466
[14,     1] loss: 0.443
[15,     1] loss: 0.426
[16,     1] loss: 0.406
[17,     1] loss: 0.392
[18,     1] loss: 0.381
[19,     1] loss: 0.367
[20,     1] loss: 0.355
[21,     1] loss: 0.346
[22,     1] loss: 0.338
[23,     1] loss: 0.331
[24,     1] loss: 0.323
[25,     1] loss: 0.317
[26,     1] loss: 0.317
[27,     1] loss: 0.315
[28,     1] loss: 0.324
[29,     1] loss: 0.304
[30,     1] loss: 0.305
[31,     1] loss: 0.299
[32,     1] loss: 0.298
[33,     1] loss: 0.288
[34,     1] loss: 0.285
[35,     1] loss: 0.278
[36,     1] loss: 0.272
[37,     1] loss: 0.264
[38,     1] loss: 0.259
[39,     1] loss: 0.253
[40,     1] loss: 0.247
[41,     1] loss: 0.246
[42,     1] loss: 0.238
[43,     1] loss: 0.233
[44,     1] loss: 0.230
[45,     1] loss: 0.223
[46,     1] loss: 0.220
[47,     1] loss: 0.217
[48,     1] loss: 0.209
[49,     1] loss: 0.206
[50,     1] loss: 0.208
[51,     1] loss: 0.462
[52,     1] loss: 0.311
[53,     1] loss: 0.341
[54,     1] loss: 0.377
[55,     1] loss: 0.375
[56,     1] loss: 0.379
[57,     1] loss: 0.352
[58,     1] loss: 0.352
[59,     1] loss: 0.333
[60,     1] loss: 0.330
[61,     1] loss: 0.315
[62,     1] loss: 0.301
[63,     1] loss: 0.292
[64,     1] loss: 0.283
Early stopping applied (best metric=0.43179258704185486)
Finished Training
Total time taken: 243.67741417884827
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.699
[3,     1] loss: 0.690
[4,     1] loss: 0.681
[5,     1] loss: 0.673
[6,     1] loss: 0.662
[7,     1] loss: 0.651
[8,     1] loss: 0.637
[9,     1] loss: 0.624
[10,     1] loss: 0.607
[11,     1] loss: 0.589
[12,     1] loss: 0.575
[13,     1] loss: 0.565
[14,     1] loss: 0.542
[15,     1] loss: 0.535
[16,     1] loss: 0.514
[17,     1] loss: 0.500
[18,     1] loss: 0.483
[19,     1] loss: 0.469
[20,     1] loss: 0.457
[21,     1] loss: 0.446
[22,     1] loss: 0.436
[23,     1] loss: 0.423
[24,     1] loss: 0.412
[25,     1] loss: 0.407
[26,     1] loss: 0.400
[27,     1] loss: 0.390
[28,     1] loss: 0.385
[29,     1] loss: 0.377
[30,     1] loss: 0.373
[31,     1] loss: 0.368
[32,     1] loss: 0.362
[33,     1] loss: 0.358
[34,     1] loss: 0.355
[35,     1] loss: 0.350
[36,     1] loss: 0.343
[37,     1] loss: 0.340
[38,     1] loss: 0.336
[39,     1] loss: 0.328
[40,     1] loss: 0.324
[41,     1] loss: 0.323
[42,     1] loss: 0.358
[43,     1] loss: 0.336
[44,     1] loss: 0.323
[45,     1] loss: 0.306
[46,     1] loss: 0.297
[47,     1] loss: 0.283
[48,     1] loss: 0.266
[49,     1] loss: 0.253
[50,     1] loss: 0.271
[51,     1] loss: 0.359
[52,     1] loss: 0.483
[53,     1] loss: 0.313
[54,     1] loss: 0.329
[55,     1] loss: 0.360
[56,     1] loss: 0.422
[57,     1] loss: 0.389
[58,     1] loss: 0.363
[59,     1] loss: 0.353
[60,     1] loss: 0.331
[61,     1] loss: 0.315
[62,     1] loss: 0.298
[63,     1] loss: 0.284
[64,     1] loss: 0.272
[65,     1] loss: 0.257
[66,     1] loss: 0.246
[67,     1] loss: 0.231
[68,     1] loss: 0.217
[69,     1] loss: 0.205
[70,     1] loss: 0.196
[71,     1] loss: 0.186
[72,     1] loss: 0.177
[73,     1] loss: 0.172
[74,     1] loss: 0.165
[75,     1] loss: 0.160
[76,     1] loss: 0.155
[77,     1] loss: 0.150
[78,     1] loss: 0.150
[79,     1] loss: 0.146
[80,     1] loss: 0.146
[81,     1] loss: 0.143
[82,     1] loss: 0.142
[83,     1] loss: 0.142
[84,     1] loss: 0.139
[85,     1] loss: 0.139
[86,     1] loss: 0.137
[87,     1] loss: 0.139
[88,     1] loss: 0.187
[89,     1] loss: 0.342
[90,     1] loss: 0.484
[91,     1] loss: 0.706
[92,     1] loss: 0.443
[93,     1] loss: 0.494
[94,     1] loss: 0.523
[95,     1] loss: 0.535
[96,     1] loss: 0.537
[97,     1] loss: 0.534
[98,     1] loss: 0.535
[99,     1] loss: 0.532
[100,     1] loss: 0.533
[101,     1] loss: 0.534
[102,     1] loss: 0.534
[103,     1] loss: 0.532
[104,     1] loss: 0.531
[105,     1] loss: 0.525
[106,     1] loss: 0.520
[107,     1] loss: 0.516
[108,     1] loss: 0.509
[109,     1] loss: 0.502
[110,     1] loss: 0.494
[111,     1] loss: 0.487
[112,     1] loss: 0.478
[113,     1] loss: 0.473
[114,     1] loss: 0.459
[115,     1] loss: 0.449
[116,     1] loss: 0.437
[117,     1] loss: 0.424
Early stopping applied (best metric=0.35823947191238403)
Finished Training
Total time taken: 443.26064801216125
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.659
[5,     1] loss: 0.649
[6,     1] loss: 0.636
[7,     1] loss: 0.622
[8,     1] loss: 0.609
[9,     1] loss: 0.596
[10,     1] loss: 0.580
[11,     1] loss: 0.566
[12,     1] loss: 0.548
[13,     1] loss: 0.534
[14,     1] loss: 0.521
[15,     1] loss: 0.505
[16,     1] loss: 0.490
[17,     1] loss: 0.474
[18,     1] loss: 0.471
[19,     1] loss: 0.477
[20,     1] loss: 0.464
[21,     1] loss: 0.438
[22,     1] loss: 0.445
[23,     1] loss: 0.425
[24,     1] loss: 0.427
[25,     1] loss: 0.408
[26,     1] loss: 0.402
[27,     1] loss: 0.391
[28,     1] loss: 0.382
[29,     1] loss: 0.376
[30,     1] loss: 0.370
[31,     1] loss: 0.361
[32,     1] loss: 0.356
[33,     1] loss: 0.351
[34,     1] loss: 0.343
[35,     1] loss: 0.337
[36,     1] loss: 0.331
[37,     1] loss: 0.324
[38,     1] loss: 0.319
[39,     1] loss: 0.311
[40,     1] loss: 0.338
[41,     1] loss: 0.382
[42,     1] loss: 0.369
[43,     1] loss: 0.369
[44,     1] loss: 0.355
[45,     1] loss: 0.349
[46,     1] loss: 0.340
[47,     1] loss: 0.330
[48,     1] loss: 0.319
[49,     1] loss: 0.308
[50,     1] loss: 0.294
[51,     1] loss: 0.280
[52,     1] loss: 0.264
[53,     1] loss: 0.252
[54,     1] loss: 0.243
[55,     1] loss: 0.235
[56,     1] loss: 0.235
[57,     1] loss: 0.246
[58,     1] loss: 0.229
[59,     1] loss: 0.205
[60,     1] loss: 0.198
[61,     1] loss: 0.190
[62,     1] loss: 0.181
[63,     1] loss: 0.174
[64,     1] loss: 0.167
[65,     1] loss: 0.160
[66,     1] loss: 0.153
[67,     1] loss: 0.148
[68,     1] loss: 0.145
[69,     1] loss: 0.138
[70,     1] loss: 0.136
[71,     1] loss: 0.134
[72,     1] loss: 0.131
[73,     1] loss: 0.131
[74,     1] loss: 0.129
[75,     1] loss: 0.129
[76,     1] loss: 0.130
[77,     1] loss: 0.128
[78,     1] loss: 0.131
[79,     1] loss: 0.129
[80,     1] loss: 0.130
[81,     1] loss: 0.131
[82,     1] loss: 0.131
[83,     1] loss: 0.134
[84,     1] loss: 0.134
[85,     1] loss: 0.134
[86,     1] loss: 0.136
[87,     1] loss: 0.156
[88,     1] loss: 0.342
[89,     1] loss: 0.641
[90,     1] loss: 0.476
[91,     1] loss: 0.484
[92,     1] loss: 0.523
[93,     1] loss: 0.546
[94,     1] loss: 0.539
[95,     1] loss: 0.538
[96,     1] loss: 0.545
[97,     1] loss: 0.552
[98,     1] loss: 0.555
[99,     1] loss: 0.558
[100,     1] loss: 0.554
[101,     1] loss: 0.552
Early stopping applied (best metric=0.3741337060928345)
Finished Training
Total time taken: 383.66905450820923
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.679
[4,     1] loss: 0.670
[5,     1] loss: 0.660
[6,     1] loss: 0.650
[7,     1] loss: 0.639
[8,     1] loss: 0.624
[9,     1] loss: 0.611
[10,     1] loss: 0.594
[11,     1] loss: 0.577
[12,     1] loss: 0.556
[13,     1] loss: 0.535
[14,     1] loss: 0.515
[15,     1] loss: 0.494
[16,     1] loss: 0.468
[17,     1] loss: 0.447
[18,     1] loss: 0.423
[19,     1] loss: 0.402
[20,     1] loss: 0.378
[21,     1] loss: 0.359
[22,     1] loss: 0.334
[23,     1] loss: 0.319
[24,     1] loss: 0.299
[25,     1] loss: 0.284
[26,     1] loss: 0.267
[27,     1] loss: 0.254
[28,     1] loss: 0.240
[29,     1] loss: 0.228
[30,     1] loss: 0.220
[31,     1] loss: 0.208
[32,     1] loss: 0.202
[33,     1] loss: 0.193
[34,     1] loss: 0.187
[35,     1] loss: 0.182
[36,     1] loss: 0.178
[37,     1] loss: 0.173
[38,     1] loss: 0.169
[39,     1] loss: 0.165
[40,     1] loss: 0.161
[41,     1] loss: 0.165
[42,     1] loss: 0.282
[43,     1] loss: 0.443
[44,     1] loss: 0.389
[45,     1] loss: 0.363
[46,     1] loss: 0.327
[47,     1] loss: 0.323
[48,     1] loss: 0.320
[49,     1] loss: 0.306
[50,     1] loss: 0.304
[51,     1] loss: 0.296
[52,     1] loss: 0.288
[53,     1] loss: 0.282
[54,     1] loss: 0.272
[55,     1] loss: 0.264
[56,     1] loss: 0.254
[57,     1] loss: 0.249
[58,     1] loss: 0.240
[59,     1] loss: 0.235
[60,     1] loss: 0.223
[61,     1] loss: 0.216
[62,     1] loss: 0.207
[63,     1] loss: 0.199
[64,     1] loss: 0.191
[65,     1] loss: 0.182
[66,     1] loss: 0.177
[67,     1] loss: 0.167
[68,     1] loss: 0.168
[69,     1] loss: 0.160
[70,     1] loss: 0.194
[71,     1] loss: 0.184
[72,     1] loss: 0.173
[73,     1] loss: 0.182
[74,     1] loss: 0.186
[75,     1] loss: 0.178
[76,     1] loss: 0.164
[77,     1] loss: 0.211
[78,     1] loss: 0.195
[79,     1] loss: 0.195
[80,     1] loss: 0.188
[81,     1] loss: 0.185
[82,     1] loss: 0.179
[83,     1] loss: 0.179
[84,     1] loss: 0.169
[85,     1] loss: 0.162
[86,     1] loss: 0.158
[87,     1] loss: 0.159
[88,     1] loss: 0.151
[89,     1] loss: 0.150
[90,     1] loss: 0.145
[91,     1] loss: 0.143
[92,     1] loss: 0.140
[93,     1] loss: 0.138
[94,     1] loss: 0.137
[95,     1] loss: 0.137
[96,     1] loss: 0.135
[97,     1] loss: 0.135
[98,     1] loss: 0.137
[99,     1] loss: 0.134
[100,     1] loss: 0.137
[101,     1] loss: 0.135
[102,     1] loss: 0.141
[103,     1] loss: 0.136
[104,     1] loss: 0.138
[105,     1] loss: 0.136
[106,     1] loss: 0.136
[107,     1] loss: 0.138
[108,     1] loss: 0.137
[109,     1] loss: 0.137
[110,     1] loss: 0.138
[111,     1] loss: 0.138
[112,     1] loss: 0.135
[113,     1] loss: 0.136
[114,     1] loss: 0.137
[115,     1] loss: 0.163
[116,     1] loss: 0.328
[117,     1] loss: 0.527
[118,     1] loss: 0.571
[119,     1] loss: 0.591
[120,     1] loss: 0.551
[121,     1] loss: 0.488
[122,     1] loss: 0.489
[123,     1] loss: 0.440
[124,     1] loss: 0.459
[125,     1] loss: 0.450
[126,     1] loss: 0.442
[127,     1] loss: 0.432
[128,     1] loss: 0.427
[129,     1] loss: 0.417
[130,     1] loss: 0.408
[131,     1] loss: 0.395
[132,     1] loss: 0.384
[133,     1] loss: 0.374
[134,     1] loss: 0.361
[135,     1] loss: 0.350
[136,     1] loss: 0.336
[137,     1] loss: 0.321
[138,     1] loss: 0.306
[139,     1] loss: 0.293
[140,     1] loss: 0.299
Early stopping applied (best metric=0.3506077826023102)
Finished Training
Total time taken: 530.77947306633
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.657
[4,     1] loss: 0.642
[5,     1] loss: 0.625
[6,     1] loss: 0.609
[7,     1] loss: 0.590
[8,     1] loss: 0.570
[9,     1] loss: 0.547
[10,     1] loss: 0.530
[11,     1] loss: 0.504
[12,     1] loss: 0.486
[13,     1] loss: 0.466
[14,     1] loss: 0.447
[15,     1] loss: 0.433
[16,     1] loss: 0.415
[17,     1] loss: 0.404
[18,     1] loss: 0.395
[19,     1] loss: 0.386
[20,     1] loss: 0.380
[21,     1] loss: 0.372
[22,     1] loss: 0.366
[23,     1] loss: 0.361
[24,     1] loss: 0.356
[25,     1] loss: 0.351
[26,     1] loss: 0.345
[27,     1] loss: 0.342
[28,     1] loss: 0.338
[29,     1] loss: 0.334
[30,     1] loss: 0.331
[31,     1] loss: 0.327
[32,     1] loss: 0.322
[33,     1] loss: 0.317
[34,     1] loss: 0.313
[35,     1] loss: 0.309
[36,     1] loss: 0.305
[37,     1] loss: 0.300
[38,     1] loss: 0.297
[39,     1] loss: 0.291
[40,     1] loss: 0.287
[41,     1] loss: 0.284
[42,     1] loss: 0.279
[43,     1] loss: 0.277
[44,     1] loss: 0.270
[45,     1] loss: 0.266
[46,     1] loss: 0.261
[47,     1] loss: 0.258
[48,     1] loss: 0.252
[49,     1] loss: 0.249
[50,     1] loss: 0.338
[51,     1] loss: 0.446
[52,     1] loss: 0.560
[53,     1] loss: 0.456
[54,     1] loss: 0.463
[55,     1] loss: 0.465
[56,     1] loss: 0.447
[57,     1] loss: 0.449
[58,     1] loss: 0.453
[59,     1] loss: 0.445
[60,     1] loss: 0.438
[61,     1] loss: 0.424
[62,     1] loss: 0.418
[63,     1] loss: 0.402
[64,     1] loss: 0.391
[65,     1] loss: 0.380
[66,     1] loss: 0.369
[67,     1] loss: 0.360
[68,     1] loss: 0.347
[69,     1] loss: 0.341
[70,     1] loss: 0.330
[71,     1] loss: 0.323
[72,     1] loss: 0.314
[73,     1] loss: 0.307
[74,     1] loss: 0.302
[75,     1] loss: 0.295
[76,     1] loss: 0.294
[77,     1] loss: 0.289
[78,     1] loss: 0.291
[79,     1] loss: 0.291
[80,     1] loss: 0.287
[81,     1] loss: 0.284
[82,     1] loss: 0.281
[83,     1] loss: 0.278
[84,     1] loss: 0.277
[85,     1] loss: 0.271
[86,     1] loss: 0.271
[87,     1] loss: 0.269
[88,     1] loss: 0.264
[89,     1] loss: 0.263
[90,     1] loss: 0.259
[91,     1] loss: 0.259
[92,     1] loss: 0.272
[93,     1] loss: 0.278
[94,     1] loss: 0.278
[95,     1] loss: 0.280
[96,     1] loss: 0.277
[97,     1] loss: 0.308
[98,     1] loss: 0.378
[99,     1] loss: 0.374
[100,     1] loss: 0.337
[101,     1] loss: 0.365
[102,     1] loss: 0.315
[103,     1] loss: 0.331
[104,     1] loss: 0.341
[105,     1] loss: 0.330
[106,     1] loss: 0.322
[107,     1] loss: 0.322
[108,     1] loss: 0.321
[109,     1] loss: 0.316
[110,     1] loss: 0.321
[111,     1] loss: 0.320
[112,     1] loss: 0.322
[113,     1] loss: 0.328
[114,     1] loss: 0.321
[115,     1] loss: 0.319
[116,     1] loss: 0.318
[117,     1] loss: 0.322
[118,     1] loss: 0.319
[119,     1] loss: 0.317
[120,     1] loss: 0.318
[121,     1] loss: 0.317
[122,     1] loss: 0.317
[123,     1] loss: 0.316
[124,     1] loss: 0.315
[125,     1] loss: 0.314
[126,     1] loss: 0.314
[127,     1] loss: 0.310
[128,     1] loss: 0.307
[129,     1] loss: 0.314
[130,     1] loss: 0.322
[131,     1] loss: 0.322
[132,     1] loss: 0.326
[133,     1] loss: 0.316
[134,     1] loss: 0.314
[135,     1] loss: 0.310
[136,     1] loss: 0.310
[137,     1] loss: 0.308
[138,     1] loss: 0.304
[139,     1] loss: 0.301
[140,     1] loss: 0.303
Early stopping applied (best metric=0.45227739214897156)
Finished Training
Total time taken: 532.6756179332733
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.687
[3,     1] loss: 0.670
[4,     1] loss: 0.655
[5,     1] loss: 0.642
[6,     1] loss: 0.630
[7,     1] loss: 0.617
[8,     1] loss: 0.603
[9,     1] loss: 0.590
[10,     1] loss: 0.575
[11,     1] loss: 0.559
[12,     1] loss: 0.543
[13,     1] loss: 0.522
[14,     1] loss: 0.503
[15,     1] loss: 0.481
[16,     1] loss: 0.460
[17,     1] loss: 0.441
[18,     1] loss: 0.417
[19,     1] loss: 0.398
[20,     1] loss: 0.377
[21,     1] loss: 0.356
[22,     1] loss: 0.339
[23,     1] loss: 0.320
[24,     1] loss: 0.305
[25,     1] loss: 0.288
[26,     1] loss: 0.269
[27,     1] loss: 0.261
[28,     1] loss: 0.241
[29,     1] loss: 0.233
[30,     1] loss: 0.223
[31,     1] loss: 0.215
[32,     1] loss: 0.207
[33,     1] loss: 0.194
[34,     1] loss: 0.189
[35,     1] loss: 0.182
[36,     1] loss: 0.174
[37,     1] loss: 0.171
[38,     1] loss: 0.165
[39,     1] loss: 0.158
[40,     1] loss: 0.154
[41,     1] loss: 0.148
[42,     1] loss: 0.145
[43,     1] loss: 0.217
[44,     1] loss: 0.345
[45,     1] loss: 0.431
[46,     1] loss: 0.319
[47,     1] loss: 0.353
[48,     1] loss: 0.334
[49,     1] loss: 0.331
[50,     1] loss: 0.330
[51,     1] loss: 0.327
[52,     1] loss: 0.321
[53,     1] loss: 0.312
[54,     1] loss: 0.301
[55,     1] loss: 0.293
[56,     1] loss: 0.288
[57,     1] loss: 0.274
[58,     1] loss: 0.263
[59,     1] loss: 0.255
[60,     1] loss: 0.245
[61,     1] loss: 0.235
[62,     1] loss: 0.224
[63,     1] loss: 0.218
[64,     1] loss: 0.208
[65,     1] loss: 0.204
[66,     1] loss: 0.197
[67,     1] loss: 0.191
[68,     1] loss: 0.187
[69,     1] loss: 0.182
[70,     1] loss: 0.179
[71,     1] loss: 0.177
[72,     1] loss: 0.174
[73,     1] loss: 0.174
[74,     1] loss: 0.174
[75,     1] loss: 0.173
[76,     1] loss: 0.173
[77,     1] loss: 0.170
[78,     1] loss: 0.170
[79,     1] loss: 0.171
[80,     1] loss: 0.175
[81,     1] loss: 0.176
[82,     1] loss: 0.189
[83,     1] loss: 0.222
[84,     1] loss: 0.409
[85,     1] loss: 0.391
[86,     1] loss: 0.315
[87,     1] loss: 0.340
[88,     1] loss: 0.304
[89,     1] loss: 0.308
[90,     1] loss: 0.304
[91,     1] loss: 0.296
[92,     1] loss: 0.289
[93,     1] loss: 0.286
[94,     1] loss: 0.278
[95,     1] loss: 0.272
[96,     1] loss: 0.266
[97,     1] loss: 0.258
[98,     1] loss: 0.255
[99,     1] loss: 0.249
[100,     1] loss: 0.244
[101,     1] loss: 0.240
[102,     1] loss: 0.233
[103,     1] loss: 0.228
[104,     1] loss: 0.225
[105,     1] loss: 0.220
[106,     1] loss: 0.231
[107,     1] loss: 0.257
[108,     1] loss: 0.331
[109,     1] loss: 0.280
[110,     1] loss: 0.347
[111,     1] loss: 0.286
[112,     1] loss: 0.341
Early stopping applied (best metric=0.3450503349304199)
Finished Training
Total time taken: 428.09403896331787
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.661
[4,     1] loss: 0.645
[5,     1] loss: 0.629
[6,     1] loss: 0.612
[7,     1] loss: 0.595
[8,     1] loss: 0.578
[9,     1] loss: 0.557
[10,     1] loss: 0.537
[11,     1] loss: 0.516
[12,     1] loss: 0.491
[13,     1] loss: 0.467
[14,     1] loss: 0.445
[15,     1] loss: 0.428
[16,     1] loss: 0.403
[17,     1] loss: 0.385
[18,     1] loss: 0.366
[19,     1] loss: 0.353
[20,     1] loss: 0.337
[21,     1] loss: 0.321
[22,     1] loss: 0.308
[23,     1] loss: 0.294
[24,     1] loss: 0.284
[25,     1] loss: 0.273
[26,     1] loss: 0.264
[27,     1] loss: 0.254
[28,     1] loss: 0.246
[29,     1] loss: 0.238
[30,     1] loss: 0.233
[31,     1] loss: 0.225
[32,     1] loss: 0.219
[33,     1] loss: 0.213
[34,     1] loss: 0.210
[35,     1] loss: 0.208
[36,     1] loss: 0.202
[37,     1] loss: 0.198
[38,     1] loss: 0.193
[39,     1] loss: 0.192
[40,     1] loss: 0.194
[41,     1] loss: 0.195
[42,     1] loss: 0.192
[43,     1] loss: 0.186
[44,     1] loss: 0.177
[45,     1] loss: 0.175
[46,     1] loss: 0.396
[47,     1] loss: 0.463
[48,     1] loss: 0.395
[49,     1] loss: 0.415
[50,     1] loss: 0.365
[51,     1] loss: 0.349
[52,     1] loss: 0.350
[53,     1] loss: 0.353
[54,     1] loss: 0.340
[55,     1] loss: 0.336
[56,     1] loss: 0.327
[57,     1] loss: 0.319
[58,     1] loss: 0.312
[59,     1] loss: 0.301
[60,     1] loss: 0.292
[61,     1] loss: 0.280
[62,     1] loss: 0.270
[63,     1] loss: 0.259
[64,     1] loss: 0.247
[65,     1] loss: 0.238
[66,     1] loss: 0.223
[67,     1] loss: 0.217
[68,     1] loss: 0.246
[69,     1] loss: 0.245
[70,     1] loss: 0.239
[71,     1] loss: 0.233
[72,     1] loss: 0.217
[73,     1] loss: 0.238
[74,     1] loss: 0.217
[75,     1] loss: 0.216
[76,     1] loss: 0.206
[77,     1] loss: 0.197
[78,     1] loss: 0.194
[79,     1] loss: 0.196
[80,     1] loss: 0.181
[81,     1] loss: 0.253
[82,     1] loss: 0.222
[83,     1] loss: 0.217
[84,     1] loss: 0.199
[85,     1] loss: 0.201
[86,     1] loss: 0.198
[87,     1] loss: 0.189
[88,     1] loss: 0.191
[89,     1] loss: 0.178
[90,     1] loss: 0.176
[91,     1] loss: 0.173
[92,     1] loss: 0.170
[93,     1] loss: 0.166
[94,     1] loss: 0.159
[95,     1] loss: 0.159
[96,     1] loss: 0.154
[97,     1] loss: 0.151
[98,     1] loss: 0.148
[99,     1] loss: 0.149
[100,     1] loss: 0.148
[101,     1] loss: 0.147
[102,     1] loss: 0.147
[103,     1] loss: 0.147
[104,     1] loss: 0.143
[105,     1] loss: 0.147
[106,     1] loss: 0.147
[107,     1] loss: 0.148
[108,     1] loss: 0.147
[109,     1] loss: 0.149
[110,     1] loss: 0.150
[111,     1] loss: 0.149
[112,     1] loss: 0.150
[113,     1] loss: 0.157
[114,     1] loss: 0.375
[115,     1] loss: 0.317
[116,     1] loss: 0.935
[117,     1] loss: 0.892
[118,     1] loss: 0.801
[119,     1] loss: 0.722
[120,     1] loss: 0.688
[121,     1] loss: 0.680
[122,     1] loss: 0.682
[123,     1] loss: 0.684
[124,     1] loss: 0.686
[125,     1] loss: 0.687
[126,     1] loss: 0.688
[127,     1] loss: 0.689
[128,     1] loss: 0.690
[129,     1] loss: 0.690
[130,     1] loss: 0.691
[131,     1] loss: 0.691
[132,     1] loss: 0.691
[133,     1] loss: 0.691
[134,     1] loss: 0.691
[135,     1] loss: 0.691
[136,     1] loss: 0.692
[137,     1] loss: 0.692
[138,     1] loss: 0.692
[139,     1] loss: 0.692
[140,     1] loss: 0.693
[141,     1] loss: 0.693
[142,     1] loss: 0.693
[143,     1] loss: 0.693
[144,     1] loss: 0.693
[145,     1] loss: 0.693
[146,     1] loss: 0.693
[147,     1] loss: 0.693
[148,     1] loss: 0.693
[149,     1] loss: 0.693
[150,     1] loss: 0.693
[151,     1] loss: 0.693
[152,     1] loss: 0.693
[153,     1] loss: 0.693
[154,     1] loss: 0.693
[155,     1] loss: 0.693
[156,     1] loss: 0.693
[157,     1] loss: 0.693
[158,     1] loss: 0.693
[159,     1] loss: 0.693
[160,     1] loss: 0.693
[161,     1] loss: 0.693
Early stopping applied (best metric=0.37387198209762573)
Finished Training
Total time taken: 610.8461797237396
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.683
[3,     1] loss: 0.660
[4,     1] loss: 0.639
[5,     1] loss: 0.618
[6,     1] loss: 0.602
[7,     1] loss: 0.581
[8,     1] loss: 0.558
[9,     1] loss: 0.538
[10,     1] loss: 0.510
[11,     1] loss: 0.487
[12,     1] loss: 0.470
[13,     1] loss: 0.467
[14,     1] loss: 0.445
[15,     1] loss: 0.449
[16,     1] loss: 0.417
[17,     1] loss: 0.417
[18,     1] loss: 0.406
[19,     1] loss: 0.396
[20,     1] loss: 0.389
[21,     1] loss: 0.382
[22,     1] loss: 0.375
[23,     1] loss: 0.371
[24,     1] loss: 0.371
[25,     1] loss: 0.370
[26,     1] loss: 0.372
[27,     1] loss: 0.362
[28,     1] loss: 0.373
[29,     1] loss: 0.385
[30,     1] loss: 0.365
[31,     1] loss: 0.378
[32,     1] loss: 0.371
[33,     1] loss: 0.375
[34,     1] loss: 0.367
[35,     1] loss: 0.381
[36,     1] loss: 0.368
[37,     1] loss: 0.370
[38,     1] loss: 0.365
[39,     1] loss: 0.370
[40,     1] loss: 0.383
[41,     1] loss: 0.379
[42,     1] loss: 0.371
[43,     1] loss: 0.367
[44,     1] loss: 0.364
[45,     1] loss: 0.367
[46,     1] loss: 0.365
[47,     1] loss: 0.361
[48,     1] loss: 0.361
[49,     1] loss: 0.362
[50,     1] loss: 0.360
[51,     1] loss: 0.359
[52,     1] loss: 0.361
[53,     1] loss: 0.404
[54,     1] loss: 0.509
[55,     1] loss: 0.434
[56,     1] loss: 0.446
[57,     1] loss: 0.436
[58,     1] loss: 0.425
[59,     1] loss: 0.401
[60,     1] loss: 0.397
[61,     1] loss: 0.396
[62,     1] loss: 0.387
[63,     1] loss: 0.384
[64,     1] loss: 0.376
[65,     1] loss: 0.372
[66,     1] loss: 0.366
[67,     1] loss: 0.362
Early stopping applied (best metric=0.39847323298454285)
Finished Training
Total time taken: 256.2919797897339
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.687
[3,     1] loss: 0.670
[4,     1] loss: 0.655
[5,     1] loss: 0.640
[6,     1] loss: 0.621
[7,     1] loss: 0.603
[8,     1] loss: 0.582
[9,     1] loss: 0.562
[10,     1] loss: 0.540
[11,     1] loss: 0.517
[12,     1] loss: 0.495
[13,     1] loss: 0.474
[14,     1] loss: 0.454
[15,     1] loss: 0.430
[16,     1] loss: 0.411
[17,     1] loss: 0.392
[18,     1] loss: 0.373
[19,     1] loss: 0.355
[20,     1] loss: 0.338
[21,     1] loss: 0.319
[22,     1] loss: 0.303
[23,     1] loss: 0.285
[24,     1] loss: 0.272
[25,     1] loss: 0.258
[26,     1] loss: 0.243
[27,     1] loss: 0.230
[28,     1] loss: 0.219
[29,     1] loss: 0.211
[30,     1] loss: 0.202
[31,     1] loss: 0.193
[32,     1] loss: 0.188
[33,     1] loss: 0.179
[34,     1] loss: 0.177
[35,     1] loss: 0.173
[36,     1] loss: 0.170
[37,     1] loss: 0.169
[38,     1] loss: 0.165
[39,     1] loss: 0.164
[40,     1] loss: 0.161
[41,     1] loss: 0.159
[42,     1] loss: 0.158
[43,     1] loss: 0.157
[44,     1] loss: 0.178
[45,     1] loss: 0.257
[46,     1] loss: 0.554
[47,     1] loss: 0.410
[48,     1] loss: 0.432
[49,     1] loss: 0.419
[50,     1] loss: 0.421
[51,     1] loss: 0.428
[52,     1] loss: 0.427
[53,     1] loss: 0.424
[54,     1] loss: 0.416
[55,     1] loss: 0.407
[56,     1] loss: 0.392
[57,     1] loss: 0.380
[58,     1] loss: 0.364
[59,     1] loss: 0.347
[60,     1] loss: 0.329
[61,     1] loss: 0.309
[62,     1] loss: 0.294
[63,     1] loss: 0.281
[64,     1] loss: 0.265
[65,     1] loss: 0.251
[66,     1] loss: 0.235
[67,     1] loss: 0.222
[68,     1] loss: 0.218
[69,     1] loss: 0.205
[70,     1] loss: 0.192
[71,     1] loss: 0.183
[72,     1] loss: 0.178
[73,     1] loss: 0.171
[74,     1] loss: 0.162
[75,     1] loss: 0.159
[76,     1] loss: 0.154
[77,     1] loss: 0.151
[78,     1] loss: 0.148
[79,     1] loss: 0.146
[80,     1] loss: 0.145
[81,     1] loss: 0.144
[82,     1] loss: 0.145
[83,     1] loss: 0.143
[84,     1] loss: 0.143
[85,     1] loss: 0.145
[86,     1] loss: 0.145
[87,     1] loss: 0.143
[88,     1] loss: 0.142
[89,     1] loss: 0.145
[90,     1] loss: 0.145
[91,     1] loss: 0.144
[92,     1] loss: 0.147
[93,     1] loss: 0.148
[94,     1] loss: 0.148
[95,     1] loss: 0.162
[96,     1] loss: 0.310
[97,     1] loss: 0.448
[98,     1] loss: 0.813
[99,     1] loss: 0.502
[100,     1] loss: 0.494
[101,     1] loss: 0.535
[102,     1] loss: 0.546
[103,     1] loss: 0.530
[104,     1] loss: 0.524
[105,     1] loss: 0.525
[106,     1] loss: 0.533
[107,     1] loss: 0.539
[108,     1] loss: 0.542
[109,     1] loss: 0.545
[110,     1] loss: 0.548
[111,     1] loss: 0.548
[112,     1] loss: 0.549
[113,     1] loss: 0.549
[114,     1] loss: 0.547
[115,     1] loss: 0.545
[116,     1] loss: 0.542
[117,     1] loss: 0.536
[118,     1] loss: 0.535
[119,     1] loss: 0.527
[120,     1] loss: 0.523
[121,     1] loss: 0.514
[122,     1] loss: 0.508
[123,     1] loss: 0.501
[124,     1] loss: 0.493
[125,     1] loss: 0.486
[126,     1] loss: 0.482
[127,     1] loss: 0.486
Early stopping applied (best metric=0.385483056306839)
Finished Training
Total time taken: 483.7085630893707
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.676
[3,     1] loss: 0.653
[4,     1] loss: 0.637
[5,     1] loss: 0.617
[6,     1] loss: 0.599
[7,     1] loss: 0.579
[8,     1] loss: 0.559
[9,     1] loss: 0.539
[10,     1] loss: 0.515
[11,     1] loss: 0.492
[12,     1] loss: 0.468
[13,     1] loss: 0.443
[14,     1] loss: 0.412
[15,     1] loss: 0.385
[16,     1] loss: 0.356
[17,     1] loss: 0.333
[18,     1] loss: 0.314
[19,     1] loss: 0.292
[20,     1] loss: 0.275
[21,     1] loss: 0.255
[22,     1] loss: 0.239
[23,     1] loss: 0.225
[24,     1] loss: 0.216
[25,     1] loss: 0.208
[26,     1] loss: 0.206
[27,     1] loss: 0.196
[28,     1] loss: 0.187
[29,     1] loss: 0.185
[30,     1] loss: 0.180
[31,     1] loss: 0.174
[32,     1] loss: 0.172
[33,     1] loss: 0.170
[34,     1] loss: 0.167
[35,     1] loss: 0.166
[36,     1] loss: 0.165
[37,     1] loss: 0.160
[38,     1] loss: 0.160
[39,     1] loss: 0.158
[40,     1] loss: 0.157
[41,     1] loss: 0.155
[42,     1] loss: 0.155
[43,     1] loss: 0.205
[44,     1] loss: 0.358
[45,     1] loss: 0.367
[46,     1] loss: 0.347
[47,     1] loss: 0.352
[48,     1] loss: 0.338
[49,     1] loss: 0.337
[50,     1] loss: 0.331
[51,     1] loss: 0.322
[52,     1] loss: 0.313
[53,     1] loss: 0.296
[54,     1] loss: 0.286
[55,     1] loss: 0.274
[56,     1] loss: 0.265
[57,     1] loss: 0.254
[58,     1] loss: 0.247
[59,     1] loss: 0.239
[60,     1] loss: 0.233
[61,     1] loss: 0.224
[62,     1] loss: 0.218
[63,     1] loss: 0.215
[64,     1] loss: 0.208
[65,     1] loss: 0.205
[66,     1] loss: 0.201
[67,     1] loss: 0.199
[68,     1] loss: 0.196
[69,     1] loss: 0.193
[70,     1] loss: 0.192
[71,     1] loss: 0.190
[72,     1] loss: 0.188
[73,     1] loss: 0.188
[74,     1] loss: 0.202
[75,     1] loss: 0.342
[76,     1] loss: 0.274
[77,     1] loss: 0.274
[78,     1] loss: 0.416
[79,     1] loss: 0.540
Early stopping applied (best metric=0.3342840075492859)
Finished Training
Total time taken: 302.8178446292877
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.678
[4,     1] loss: 0.668
[5,     1] loss: 0.658
[6,     1] loss: 0.645
[7,     1] loss: 0.633
[8,     1] loss: 0.618
[9,     1] loss: 0.603
[10,     1] loss: 0.586
[11,     1] loss: 0.571
[12,     1] loss: 0.554
[13,     1] loss: 0.537
[14,     1] loss: 0.517
[15,     1] loss: 0.499
[16,     1] loss: 0.480
[17,     1] loss: 0.459
[18,     1] loss: 0.443
[19,     1] loss: 0.423
[20,     1] loss: 0.410
[21,     1] loss: 0.389
[22,     1] loss: 0.372
[23,     1] loss: 0.363
[24,     1] loss: 0.357
[25,     1] loss: 0.332
[26,     1] loss: 0.329
[27,     1] loss: 0.307
[28,     1] loss: 0.294
[29,     1] loss: 0.285
[30,     1] loss: 0.270
[31,     1] loss: 0.262
[32,     1] loss: 0.251
[33,     1] loss: 0.241
[34,     1] loss: 0.233
[35,     1] loss: 0.225
[36,     1] loss: 0.218
[37,     1] loss: 0.211
[38,     1] loss: 0.201
[39,     1] loss: 0.197
[40,     1] loss: 0.191
[41,     1] loss: 0.183
[42,     1] loss: 0.179
[43,     1] loss: 0.169
[44,     1] loss: 0.161
[45,     1] loss: 0.159
[46,     1] loss: 0.250
[47,     1] loss: 0.466
[48,     1] loss: 0.370
[49,     1] loss: 0.706
[50,     1] loss: 0.549
[51,     1] loss: 0.459
[52,     1] loss: 0.478
[53,     1] loss: 0.500
[54,     1] loss: 0.515
[55,     1] loss: 0.511
[56,     1] loss: 0.522
[57,     1] loss: 0.523
[58,     1] loss: 0.521
[59,     1] loss: 0.519
[60,     1] loss: 0.514
[61,     1] loss: 0.510
[62,     1] loss: 0.503
[63,     1] loss: 0.496
[64,     1] loss: 0.489
[65,     1] loss: 0.482
[66,     1] loss: 0.473
[67,     1] loss: 0.467
Early stopping applied (best metric=0.4203824996948242)
Finished Training
Total time taken: 257.64084100723267
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.682
[3,     1] loss: 0.663
[4,     1] loss: 0.646
[5,     1] loss: 0.632
[6,     1] loss: 0.616
[7,     1] loss: 0.599
[8,     1] loss: 0.586
[9,     1] loss: 0.570
[10,     1] loss: 0.553
[11,     1] loss: 0.534
[12,     1] loss: 0.516
[13,     1] loss: 0.494
[14,     1] loss: 0.475
[15,     1] loss: 0.450
[16,     1] loss: 0.428
[17,     1] loss: 0.402
[18,     1] loss: 0.377
[19,     1] loss: 0.363
[20,     1] loss: 0.340
[21,     1] loss: 0.319
[22,     1] loss: 0.304
[23,     1] loss: 0.290
[24,     1] loss: 0.271
[25,     1] loss: 0.262
[26,     1] loss: 0.253
[27,     1] loss: 0.248
[28,     1] loss: 0.235
[29,     1] loss: 0.225
[30,     1] loss: 0.221
[31,     1] loss: 0.214
[32,     1] loss: 0.208
[33,     1] loss: 0.203
[34,     1] loss: 0.200
[35,     1] loss: 0.194
[36,     1] loss: 0.189
[37,     1] loss: 0.187
[38,     1] loss: 0.185
[39,     1] loss: 0.181
[40,     1] loss: 0.183
[41,     1] loss: 0.188
[42,     1] loss: 0.183
[43,     1] loss: 0.181
[44,     1] loss: 0.172
[45,     1] loss: 0.168
[46,     1] loss: 0.210
[47,     1] loss: 0.420
[48,     1] loss: 0.324
[49,     1] loss: 0.420
[50,     1] loss: 0.407
[51,     1] loss: 0.460
[52,     1] loss: 0.419
[53,     1] loss: 0.432
[54,     1] loss: 0.433
[55,     1] loss: 0.436
[56,     1] loss: 0.440
[57,     1] loss: 0.438
[58,     1] loss: 0.435
[59,     1] loss: 0.429
[60,     1] loss: 0.417
[61,     1] loss: 0.404
[62,     1] loss: 0.390
[63,     1] loss: 0.371
[64,     1] loss: 0.355
[65,     1] loss: 0.340
[66,     1] loss: 0.325
[67,     1] loss: 0.312
[68,     1] loss: 0.300
[69,     1] loss: 0.290
[70,     1] loss: 0.278
[71,     1] loss: 0.266
[72,     1] loss: 0.259
[73,     1] loss: 0.250
[74,     1] loss: 0.244
[75,     1] loss: 0.238
[76,     1] loss: 0.234
[77,     1] loss: 0.228
[78,     1] loss: 0.226
[79,     1] loss: 0.221
[80,     1] loss: 0.221
[81,     1] loss: 0.228
[82,     1] loss: 0.268
[83,     1] loss: 0.252
[84,     1] loss: 0.241
[85,     1] loss: 0.249
[86,     1] loss: 0.235
[87,     1] loss: 0.231
[88,     1] loss: 0.223
[89,     1] loss: 0.220
[90,     1] loss: 0.210
[91,     1] loss: 0.202
[92,     1] loss: 0.198
[93,     1] loss: 0.199
[94,     1] loss: 0.227
[95,     1] loss: 0.200
[96,     1] loss: 0.209
[97,     1] loss: 0.215
[98,     1] loss: 0.193
[99,     1] loss: 0.191
[100,     1] loss: 0.183
[101,     1] loss: 0.179
[102,     1] loss: 0.174
[103,     1] loss: 0.170
[104,     1] loss: 0.167
[105,     1] loss: 0.162
[106,     1] loss: 0.162
[107,     1] loss: 0.162
[108,     1] loss: 0.157
[109,     1] loss: 0.161
[110,     1] loss: 0.170
[111,     1] loss: 0.220
[112,     1] loss: 0.194
[113,     1] loss: 0.202
[114,     1] loss: 0.203
[115,     1] loss: 0.261
[116,     1] loss: 0.303
[117,     1] loss: 0.474
[118,     1] loss: 0.282
[119,     1] loss: 0.341
[120,     1] loss: 0.286
[121,     1] loss: 0.288
[122,     1] loss: 0.279
[123,     1] loss: 0.290
[124,     1] loss: 0.294
[125,     1] loss: 0.288
[126,     1] loss: 0.281
[127,     1] loss: 0.271
[128,     1] loss: 0.268
[129,     1] loss: 0.254
[130,     1] loss: 0.245
[131,     1] loss: 0.239
[132,     1] loss: 0.229
[133,     1] loss: 0.222
[134,     1] loss: 0.215
[135,     1] loss: 0.207
[136,     1] loss: 0.204
[137,     1] loss: 0.199
[138,     1] loss: 0.196
[139,     1] loss: 0.195
[140,     1] loss: 0.188
[141,     1] loss: 0.187
[142,     1] loss: 0.182
[143,     1] loss: 0.182
[144,     1] loss: 0.178
[145,     1] loss: 0.180
[146,     1] loss: 0.177
[147,     1] loss: 0.175
[148,     1] loss: 0.175
[149,     1] loss: 0.180
[150,     1] loss: 0.281
[151,     1] loss: 0.331
[152,     1] loss: 0.584
[153,     1] loss: 0.558
[154,     1] loss: 0.512
[155,     1] loss: 0.471
[156,     1] loss: 0.498
[157,     1] loss: 0.489
[158,     1] loss: 0.474
[159,     1] loss: 0.469
[160,     1] loss: 0.465
[161,     1] loss: 0.461
[162,     1] loss: 0.453
[163,     1] loss: 0.442
[164,     1] loss: 0.438
[165,     1] loss: 0.432
[166,     1] loss: 0.421
[167,     1] loss: 0.413
[168,     1] loss: 0.414
[169,     1] loss: 0.397
[170,     1] loss: 0.400
[171,     1] loss: 0.396
[172,     1] loss: 0.391
[173,     1] loss: 0.380
[174,     1] loss: 0.374
[175,     1] loss: 0.359
[176,     1] loss: 0.347
[177,     1] loss: 0.336
[178,     1] loss: 0.323
Early stopping applied (best metric=0.37525877356529236)
Finished Training
Total time taken: 679.4940440654755
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.670
[4,     1] loss: 0.659
[5,     1] loss: 0.648
[6,     1] loss: 0.637
[7,     1] loss: 0.625
[8,     1] loss: 0.612
[9,     1] loss: 0.600
[10,     1] loss: 0.586
[11,     1] loss: 0.571
[12,     1] loss: 0.552
[13,     1] loss: 0.535
[14,     1] loss: 0.516
[15,     1] loss: 0.497
[16,     1] loss: 0.477
[17,     1] loss: 0.463
[18,     1] loss: 0.456
[19,     1] loss: 0.443
[20,     1] loss: 0.423
[21,     1] loss: 0.415
[22,     1] loss: 0.398
[23,     1] loss: 0.391
[24,     1] loss: 0.381
[25,     1] loss: 0.370
[26,     1] loss: 0.363
[27,     1] loss: 0.356
[28,     1] loss: 0.353
[29,     1] loss: 0.352
[30,     1] loss: 0.352
[31,     1] loss: 0.340
[32,     1] loss: 0.338
[33,     1] loss: 0.334
[34,     1] loss: 0.332
[35,     1] loss: 0.329
[36,     1] loss: 0.326
[37,     1] loss: 0.323
[38,     1] loss: 0.321
[39,     1] loss: 0.318
[40,     1] loss: 0.317
[41,     1] loss: 0.316
[42,     1] loss: 0.311
[43,     1] loss: 0.307
[44,     1] loss: 0.305
[45,     1] loss: 0.301
[46,     1] loss: 0.297
[47,     1] loss: 0.293
[48,     1] loss: 0.291
[49,     1] loss: 0.312
[50,     1] loss: 0.380
[51,     1] loss: 0.353
[52,     1] loss: 0.397
[53,     1] loss: 0.410
[54,     1] loss: 0.385
[55,     1] loss: 0.370
[56,     1] loss: 0.369
[57,     1] loss: 0.361
[58,     1] loss: 0.354
[59,     1] loss: 0.348
[60,     1] loss: 0.339
[61,     1] loss: 0.331
[62,     1] loss: 0.323
[63,     1] loss: 0.316
[64,     1] loss: 0.308
[65,     1] loss: 0.304
[66,     1] loss: 0.310
[67,     1] loss: 0.347
[68,     1] loss: 0.338
[69,     1] loss: 0.311
[70,     1] loss: 0.324
[71,     1] loss: 0.306
[72,     1] loss: 0.308
[73,     1] loss: 0.300
[74,     1] loss: 0.302
[75,     1] loss: 0.297
[76,     1] loss: 0.294
[77,     1] loss: 0.290
[78,     1] loss: 0.288
[79,     1] loss: 0.285
[80,     1] loss: 0.279
[81,     1] loss: 0.276
[82,     1] loss: 0.274
[83,     1] loss: 0.272
[84,     1] loss: 0.271
[85,     1] loss: 0.267
[86,     1] loss: 0.266
[87,     1] loss: 0.298
[88,     1] loss: 0.334
[89,     1] loss: 0.435
[90,     1] loss: 0.366
[91,     1] loss: 0.352
[92,     1] loss: 0.345
[93,     1] loss: 0.329
[94,     1] loss: 0.335
[95,     1] loss: 0.320
[96,     1] loss: 0.319
[97,     1] loss: 0.304
[98,     1] loss: 0.295
[99,     1] loss: 0.289
[100,     1] loss: 0.282
[101,     1] loss: 0.281
[102,     1] loss: 0.279
[103,     1] loss: 0.270
[104,     1] loss: 0.267
[105,     1] loss: 0.265
[106,     1] loss: 0.261
[107,     1] loss: 0.260
[108,     1] loss: 0.260
[109,     1] loss: 0.255
[110,     1] loss: 0.253
[111,     1] loss: 0.251
[112,     1] loss: 0.249
[113,     1] loss: 0.249
[114,     1] loss: 0.253
[115,     1] loss: 0.279
[116,     1] loss: 0.382
[117,     1] loss: 0.461
[118,     1] loss: 0.483
[119,     1] loss: 0.478
[120,     1] loss: 0.457
[121,     1] loss: 0.453
[122,     1] loss: 0.441
[123,     1] loss: 0.427
[124,     1] loss: 0.416
[125,     1] loss: 0.408
[126,     1] loss: 0.397
[127,     1] loss: 0.390
[128,     1] loss: 0.376
Early stopping applied (best metric=0.40922775864601135)
Finished Training
Total time taken: 490.27560567855835
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.671
[3,     1] loss: 0.650
[4,     1] loss: 0.632
[5,     1] loss: 0.612
[6,     1] loss: 0.596
[7,     1] loss: 0.581
[8,     1] loss: 0.562
[9,     1] loss: 0.542
[10,     1] loss: 0.525
[11,     1] loss: 0.504
[12,     1] loss: 0.482
[13,     1] loss: 0.461
[14,     1] loss: 0.440
[15,     1] loss: 0.416
[16,     1] loss: 0.396
[17,     1] loss: 0.373
[18,     1] loss: 0.356
[19,     1] loss: 0.365
[20,     1] loss: 0.323
[21,     1] loss: 0.335
[22,     1] loss: 0.306
[23,     1] loss: 0.309
[24,     1] loss: 0.283
[25,     1] loss: 0.280
[26,     1] loss: 0.270
[27,     1] loss: 0.261
[28,     1] loss: 0.252
[29,     1] loss: 0.244
[30,     1] loss: 0.238
[31,     1] loss: 0.231
[32,     1] loss: 0.223
[33,     1] loss: 0.217
[34,     1] loss: 0.213
[35,     1] loss: 0.207
[36,     1] loss: 0.197
[37,     1] loss: 0.194
[38,     1] loss: 0.186
[39,     1] loss: 0.181
[40,     1] loss: 0.178
[41,     1] loss: 0.171
[42,     1] loss: 0.167
[43,     1] loss: 0.161
[44,     1] loss: 0.158
[45,     1] loss: 0.213
[46,     1] loss: 0.173
[47,     1] loss: 0.195
[48,     1] loss: 0.171
[49,     1] loss: 0.202
[50,     1] loss: 0.429
[51,     1] loss: 0.335
[52,     1] loss: 0.307
[53,     1] loss: 0.297
[54,     1] loss: 0.281
[55,     1] loss: 0.286
[56,     1] loss: 0.294
[57,     1] loss: 0.292
[58,     1] loss: 0.279
[59,     1] loss: 0.268
[60,     1] loss: 0.258
[61,     1] loss: 0.247
[62,     1] loss: 0.237
[63,     1] loss: 0.228
[64,     1] loss: 0.218
[65,     1] loss: 0.209
[66,     1] loss: 0.200
[67,     1] loss: 0.194
[68,     1] loss: 0.186
[69,     1] loss: 0.185
[70,     1] loss: 0.185
[71,     1] loss: 0.193
[72,     1] loss: 0.179
[73,     1] loss: 0.180
[74,     1] loss: 0.185
[75,     1] loss: 0.175
[76,     1] loss: 0.173
[77,     1] loss: 0.169
[78,     1] loss: 0.167
[79,     1] loss: 0.169
[80,     1] loss: 0.163
[81,     1] loss: 0.164
[82,     1] loss: 0.159
[83,     1] loss: 0.159
[84,     1] loss: 0.161
[85,     1] loss: 0.168
[86,     1] loss: 0.253
[87,     1] loss: 0.417
[88,     1] loss: 0.298
[89,     1] loss: 0.337
[90,     1] loss: 0.302
[91,     1] loss: 0.284
[92,     1] loss: 0.285
[93,     1] loss: 0.276
[94,     1] loss: 0.263
[95,     1] loss: 0.254
[96,     1] loss: 0.246
[97,     1] loss: 0.236
[98,     1] loss: 0.231
[99,     1] loss: 0.224
[100,     1] loss: 0.222
[101,     1] loss: 0.211
[102,     1] loss: 0.207
Early stopping applied (best metric=0.4299866259098053)
Finished Training
Total time taken: 391.93752789497375
(176, 33, 1024)
(819, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/embeddings (995 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.698
[3,     1] loss: 0.680
[4,     1] loss: 0.664
[5,     1] loss: 0.649
[6,     1] loss: 0.633
[7,     1] loss: 0.620
[8,     1] loss: 0.599
[9,     1] loss: 0.584
[10,     1] loss: 0.564
[11,     1] loss: 0.548
[12,     1] loss: 0.529
[13,     1] loss: 0.509
[14,     1] loss: 0.491
[15,     1] loss: 0.472
[16,     1] loss: 0.458
[17,     1] loss: 0.435
[18,     1] loss: 0.423
[19,     1] loss: 0.412
[20,     1] loss: 0.396
[21,     1] loss: 0.386
[22,     1] loss: 0.379
[23,     1] loss: 0.366
[24,     1] loss: 0.361
[25,     1] loss: 0.359
[26,     1] loss: 0.339
[27,     1] loss: 0.335
[28,     1] loss: 0.328
[29,     1] loss: 0.320
[30,     1] loss: 0.308
[31,     1] loss: 0.299
[32,     1] loss: 0.290
[33,     1] loss: 0.277
[34,     1] loss: 0.266
[35,     1] loss: 0.258
[36,     1] loss: 0.248
[37,     1] loss: 0.239
[38,     1] loss: 0.232
[39,     1] loss: 0.224
[40,     1] loss: 0.218
[41,     1] loss: 0.207
[42,     1] loss: 0.202
[43,     1] loss: 0.196
[44,     1] loss: 0.190
[45,     1] loss: 0.180
[46,     1] loss: 0.177
[47,     1] loss: 0.170
[48,     1] loss: 0.170
[49,     1] loss: 0.234
[50,     1] loss: 0.592
[51,     1] loss: 0.442
[52,     1] loss: 0.543
[53,     1] loss: 0.454
[54,     1] loss: 0.480
[55,     1] loss: 0.507
[56,     1] loss: 0.513
[57,     1] loss: 0.516
[58,     1] loss: 0.521
[59,     1] loss: 0.540
[60,     1] loss: 0.545
[61,     1] loss: 0.545
[62,     1] loss: 0.548
[63,     1] loss: 0.549
[64,     1] loss: 0.551
[65,     1] loss: 0.547
[66,     1] loss: 0.544
[67,     1] loss: 0.543
[68,     1] loss: 0.538
[69,     1] loss: 0.534
[70,     1] loss: 0.529
[71,     1] loss: 0.522
[72,     1] loss: 0.516
[73,     1] loss: 0.510
[74,     1] loss: 0.507
[75,     1] loss: 0.499
[76,     1] loss: 0.493
[77,     1] loss: 0.488
[78,     1] loss: 0.480
[79,     1] loss: 0.473
Early stopping applied (best metric=0.3854579031467438)
Finished Training
Total time taken: 304.46949195861816
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009793573098663728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 158485449,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.249970018618864}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.670
[4,     1] loss: 0.660
[5,     1] loss: 0.649
[6,     1] loss: 0.639
[7,     1] loss: 0.626
[8,     1] loss: 0.615
[9,     1] loss: 0.603
[10,     1] loss: 0.590
[11,     1] loss: 0.577
[12,     1] loss: 0.565
[13,     1] loss: 0.550
[14,     1] loss: 0.537
[15,     1] loss: 0.528
[16,     1] loss: 0.518
[17,     1] loss: 0.527
[18,     1] loss: 0.500
[19,     1] loss: 0.508
[20,     1] loss: 0.510
[21,     1] loss: 0.504
[22,     1] loss: 0.485
[23,     1] loss: 0.472
[24,     1] loss: 0.472
[25,     1] loss: 0.451
[26,     1] loss: 0.460
[27,     1] loss: 0.538
[28,     1] loss: 0.499
[29,     1] loss: 0.504
[30,     1] loss: 0.488
[31,     1] loss: 0.480
[32,     1] loss: 0.470
[33,     1] loss: 0.460
[34,     1] loss: 0.450
[35,     1] loss: 0.442
[36,     1] loss: 0.436
[37,     1] loss: 0.432
[38,     1] loss: 0.427
[39,     1] loss: 0.425
[40,     1] loss: 0.431
[41,     1] loss: 0.427
[42,     1] loss: 0.432
[43,     1] loss: 0.439
[44,     1] loss: 0.521
[45,     1] loss: 0.558
[46,     1] loss: 0.561
[47,     1] loss: 0.560
[48,     1] loss: 0.555
[49,     1] loss: 0.548
[50,     1] loss: 0.540
[51,     1] loss: 0.534
[52,     1] loss: 0.524
[53,     1] loss: 0.517
[54,     1] loss: 0.509
[55,     1] loss: 0.502
[56,     1] loss: 0.497
[57,     1] loss: 0.488
[58,     1] loss: 0.484
[59,     1] loss: 0.482
[60,     1] loss: 0.485
[61,     1] loss: 0.544
[62,     1] loss: 0.519
[63,     1] loss: 0.510
[64,     1] loss: 0.499
[65,     1] loss: 0.493
[66,     1] loss: 0.484
[67,     1] loss: 0.476
[68,     1] loss: 0.470
[69,     1] loss: 0.462
[70,     1] loss: 0.458
[71,     1] loss: 0.451
[72,     1] loss: 0.448
[73,     1] loss: 0.449
[74,     1] loss: 0.489
[75,     1] loss: 0.575
[76,     1] loss: 0.540
[77,     1] loss: 0.566
[78,     1] loss: 0.565
[79,     1] loss: 0.557
[80,     1] loss: 0.548
[81,     1] loss: 0.540
[82,     1] loss: 0.534
[83,     1] loss: 0.529
Early stopping applied (best metric=0.37141257524490356)
Finished Training
Total time taken: 299.90587043762207
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.690
[3,     1] loss: 0.660
[4,     1] loss: 0.641
[5,     1] loss: 0.622
[6,     1] loss: 0.605
[7,     1] loss: 0.590
[8,     1] loss: 0.576
[9,     1] loss: 0.564
[10,     1] loss: 0.551
[11,     1] loss: 0.540
[12,     1] loss: 0.527
[13,     1] loss: 0.516
[14,     1] loss: 0.503
[15,     1] loss: 0.489
[16,     1] loss: 0.485
[17,     1] loss: 0.524
[18,     1] loss: 0.488
[19,     1] loss: 0.491
[20,     1] loss: 0.472
[21,     1] loss: 0.459
[22,     1] loss: 0.443
[23,     1] loss: 0.441
[24,     1] loss: 0.470
[25,     1] loss: 0.445
[26,     1] loss: 0.446
[27,     1] loss: 0.439
[28,     1] loss: 0.426
[29,     1] loss: 0.407
[30,     1] loss: 0.406
[31,     1] loss: 0.393
[32,     1] loss: 0.386
[33,     1] loss: 0.375
[34,     1] loss: 0.368
[35,     1] loss: 0.368
[36,     1] loss: 0.361
[37,     1] loss: 0.348
[38,     1] loss: 0.342
[39,     1] loss: 0.343
[40,     1] loss: 0.455
[41,     1] loss: 0.464
[42,     1] loss: 0.513
[43,     1] loss: 0.491
[44,     1] loss: 0.472
[45,     1] loss: 0.466
[46,     1] loss: 0.451
[47,     1] loss: 0.440
[48,     1] loss: 0.429
[49,     1] loss: 0.418
[50,     1] loss: 0.404
[51,     1] loss: 0.385
[52,     1] loss: 0.366
[53,     1] loss: 0.347
[54,     1] loss: 0.329
[55,     1] loss: 0.312
[56,     1] loss: 0.334
[57,     1] loss: 0.321
[58,     1] loss: 0.334
[59,     1] loss: 0.402
[60,     1] loss: 0.412
[61,     1] loss: 0.423
[62,     1] loss: 0.403
[63,     1] loss: 0.388
[64,     1] loss: 0.374
[65,     1] loss: 0.362
[66,     1] loss: 0.347
[67,     1] loss: 0.335
[68,     1] loss: 0.322
[69,     1] loss: 0.308
[70,     1] loss: 0.297
[71,     1] loss: 0.284
[72,     1] loss: 0.275
[73,     1] loss: 0.272
[74,     1] loss: 0.275
[75,     1] loss: 0.271
[76,     1] loss: 0.272
[77,     1] loss: 0.259
[78,     1] loss: 0.251
[79,     1] loss: 0.248
[80,     1] loss: 0.254
[81,     1] loss: 0.244
[82,     1] loss: 0.237
[83,     1] loss: 0.232
[84,     1] loss: 0.227
[85,     1] loss: 0.219
[86,     1] loss: 0.220
[87,     1] loss: 0.220
[88,     1] loss: 0.219
[89,     1] loss: 0.231
[90,     1] loss: 0.389
[91,     1] loss: 0.499
[92,     1] loss: 0.450
[93,     1] loss: 0.473
[94,     1] loss: 0.464
[95,     1] loss: 0.450
[96,     1] loss: 0.435
[97,     1] loss: 0.426
[98,     1] loss: 0.414
[99,     1] loss: 0.403
[100,     1] loss: 0.396
[101,     1] loss: 0.384
[102,     1] loss: 0.375
[103,     1] loss: 0.366
[104,     1] loss: 0.357
[105,     1] loss: 0.347
[106,     1] loss: 0.335
[107,     1] loss: 0.327
[108,     1] loss: 0.317
[109,     1] loss: 0.307
[110,     1] loss: 0.301
[111,     1] loss: 0.293
[112,     1] loss: 0.285
[113,     1] loss: 0.280
[114,     1] loss: 0.291
[115,     1] loss: 0.327
[116,     1] loss: 0.587
[117,     1] loss: 0.480
[118,     1] loss: 0.436
[119,     1] loss: 0.426
[120,     1] loss: 0.414
[121,     1] loss: 0.408
[122,     1] loss: 0.403
[123,     1] loss: 0.398
[124,     1] loss: 0.394
[125,     1] loss: 0.387
[126,     1] loss: 0.381
[127,     1] loss: 0.374
[128,     1] loss: 0.365
[129,     1] loss: 0.358
[130,     1] loss: 0.350
[131,     1] loss: 0.352
[132,     1] loss: 0.339
[133,     1] loss: 0.338
[134,     1] loss: 0.365
[135,     1] loss: 0.576
[136,     1] loss: 0.523
[137,     1] loss: 0.514
Early stopping applied (best metric=0.3032676875591278)
Finished Training
Total time taken: 493.57026720046997
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.667
[3,     1] loss: 0.629
[4,     1] loss: 0.607
[5,     1] loss: 0.587
[6,     1] loss: 0.570
[7,     1] loss: 0.558
[8,     1] loss: 0.548
[9,     1] loss: 0.541
[10,     1] loss: 0.531
[11,     1] loss: 0.526
[12,     1] loss: 0.518
[13,     1] loss: 0.508
[14,     1] loss: 0.498
[15,     1] loss: 0.486
[16,     1] loss: 0.472
[17,     1] loss: 0.460
[18,     1] loss: 0.461
[19,     1] loss: 0.533
[20,     1] loss: 0.521
[21,     1] loss: 0.503
[22,     1] loss: 0.491
[23,     1] loss: 0.477
[24,     1] loss: 0.462
[25,     1] loss: 0.447
[26,     1] loss: 0.436
[27,     1] loss: 0.429
[28,     1] loss: 0.413
[29,     1] loss: 0.406
[30,     1] loss: 0.392
[31,     1] loss: 0.381
[32,     1] loss: 0.371
[33,     1] loss: 0.362
[34,     1] loss: 0.368
[35,     1] loss: 0.364
[36,     1] loss: 0.440
[37,     1] loss: 0.441
[38,     1] loss: 0.512
[39,     1] loss: 0.509
[40,     1] loss: 0.498
[41,     1] loss: 0.488
[42,     1] loss: 0.477
[43,     1] loss: 0.463
[44,     1] loss: 0.449
[45,     1] loss: 0.435
[46,     1] loss: 0.419
[47,     1] loss: 0.403
[48,     1] loss: 0.384
[49,     1] loss: 0.365
[50,     1] loss: 0.358
[51,     1] loss: 0.332
[52,     1] loss: 0.334
[53,     1] loss: 0.318
[54,     1] loss: 0.318
[55,     1] loss: 0.299
[56,     1] loss: 0.331
[57,     1] loss: 0.438
[58,     1] loss: 0.413
[59,     1] loss: 0.436
[60,     1] loss: 0.439
[61,     1] loss: 0.443
[62,     1] loss: 0.463
[63,     1] loss: 0.458
[64,     1] loss: 0.451
[65,     1] loss: 0.442
[66,     1] loss: 0.424
[67,     1] loss: 0.411
[68,     1] loss: 0.391
[69,     1] loss: 0.380
[70,     1] loss: 0.365
[71,     1] loss: 0.349
[72,     1] loss: 0.334
[73,     1] loss: 0.325
[74,     1] loss: 0.312
[75,     1] loss: 0.299
[76,     1] loss: 0.291
[77,     1] loss: 0.282
[78,     1] loss: 0.277
[79,     1] loss: 0.287
[80,     1] loss: 0.371
[81,     1] loss: 0.623
[82,     1] loss: 0.581
[83,     1] loss: 0.537
[84,     1] loss: 0.548
[85,     1] loss: 0.551
[86,     1] loss: 0.548
[87,     1] loss: 0.546
[88,     1] loss: 0.544
[89,     1] loss: 0.542
[90,     1] loss: 0.540
[91,     1] loss: 0.541
[92,     1] loss: 0.538
[93,     1] loss: 0.536
[94,     1] loss: 0.532
[95,     1] loss: 0.528
[96,     1] loss: 0.525
[97,     1] loss: 0.519
[98,     1] loss: 0.524
[99,     1] loss: 0.544
Early stopping applied (best metric=0.3411658704280853)
Finished Training
Total time taken: 358.23147535324097
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.695
[3,     1] loss: 0.684
[4,     1] loss: 0.676
[5,     1] loss: 0.669
[6,     1] loss: 0.660
[7,     1] loss: 0.650
[8,     1] loss: 0.640
[9,     1] loss: 0.626
[10,     1] loss: 0.610
[11,     1] loss: 0.594
[12,     1] loss: 0.576
[13,     1] loss: 0.558
[14,     1] loss: 0.545
[15,     1] loss: 0.549
[16,     1] loss: 0.515
[17,     1] loss: 0.526
[18,     1] loss: 0.533
[19,     1] loss: 0.529
[20,     1] loss: 0.502
[21,     1] loss: 0.492
[22,     1] loss: 0.473
[23,     1] loss: 0.456
[24,     1] loss: 0.443
[25,     1] loss: 0.439
[26,     1] loss: 0.430
[27,     1] loss: 0.460
[28,     1] loss: 0.446
[29,     1] loss: 0.461
[30,     1] loss: 0.460
[31,     1] loss: 0.449
[32,     1] loss: 0.434
[33,     1] loss: 0.421
[34,     1] loss: 0.415
[35,     1] loss: 0.409
[36,     1] loss: 0.404
[37,     1] loss: 0.404
[38,     1] loss: 0.402
[39,     1] loss: 0.410
[40,     1] loss: 0.425
[41,     1] loss: 0.453
[42,     1] loss: 0.488
[43,     1] loss: 0.488
[44,     1] loss: 0.474
[45,     1] loss: 0.462
[46,     1] loss: 0.451
[47,     1] loss: 0.439
[48,     1] loss: 0.431
[49,     1] loss: 0.426
[50,     1] loss: 0.419
[51,     1] loss: 0.413
[52,     1] loss: 0.409
[53,     1] loss: 0.407
[54,     1] loss: 0.406
[55,     1] loss: 0.407
[56,     1] loss: 0.416
[57,     1] loss: 0.436
[58,     1] loss: 0.567
[59,     1] loss: 0.519
[60,     1] loss: 0.542
[61,     1] loss: 0.541
[62,     1] loss: 0.535
[63,     1] loss: 0.530
[64,     1] loss: 0.521
[65,     1] loss: 0.514
[66,     1] loss: 0.505
[67,     1] loss: 0.497
[68,     1] loss: 0.487
[69,     1] loss: 0.478
[70,     1] loss: 0.468
[71,     1] loss: 0.458
[72,     1] loss: 0.447
[73,     1] loss: 0.440
[74,     1] loss: 0.434
[75,     1] loss: 0.428
[76,     1] loss: 0.424
[77,     1] loss: 0.420
[78,     1] loss: 0.415
[79,     1] loss: 0.415
[80,     1] loss: 0.411
[81,     1] loss: 0.421
[82,     1] loss: 0.693
[83,     1] loss: 0.532
[84,     1] loss: 0.558
[85,     1] loss: 0.563
[86,     1] loss: 0.563
[87,     1] loss: 0.561
[88,     1] loss: 0.560
[89,     1] loss: 0.556
[90,     1] loss: 0.552
[91,     1] loss: 0.548
[92,     1] loss: 0.539
[93,     1] loss: 0.531
[94,     1] loss: 0.520
[95,     1] loss: 0.506
[96,     1] loss: 0.493
[97,     1] loss: 0.482
[98,     1] loss: 0.470
[99,     1] loss: 0.461
Early stopping applied (best metric=0.35905420780181885)
Finished Training
Total time taken: 359.4908137321472
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.664
[4,     1] loss: 0.648
[5,     1] loss: 0.633
[6,     1] loss: 0.621
[7,     1] loss: 0.609
[8,     1] loss: 0.600
[9,     1] loss: 0.591
[10,     1] loss: 0.582
[11,     1] loss: 0.573
[12,     1] loss: 0.564
[13,     1] loss: 0.557
[14,     1] loss: 0.545
[15,     1] loss: 0.535
[16,     1] loss: 0.533
[17,     1] loss: 0.522
[18,     1] loss: 0.523
[19,     1] loss: 0.505
[20,     1] loss: 0.498
[21,     1] loss: 0.503
[22,     1] loss: 0.484
[23,     1] loss: 0.480
[24,     1] loss: 0.466
[25,     1] loss: 0.458
[26,     1] loss: 0.446
[27,     1] loss: 0.437
[28,     1] loss: 0.429
[29,     1] loss: 0.454
[30,     1] loss: 0.497
[31,     1] loss: 0.534
[32,     1] loss: 0.541
[33,     1] loss: 0.536
[34,     1] loss: 0.533
[35,     1] loss: 0.522
[36,     1] loss: 0.512
[37,     1] loss: 0.503
[38,     1] loss: 0.494
[39,     1] loss: 0.482
[40,     1] loss: 0.470
[41,     1] loss: 0.457
[42,     1] loss: 0.439
[43,     1] loss: 0.424
[44,     1] loss: 0.411
[45,     1] loss: 0.395
[46,     1] loss: 0.376
[47,     1] loss: 0.359
[48,     1] loss: 0.351
[49,     1] loss: 0.349
[50,     1] loss: 0.440
[51,     1] loss: 0.462
[52,     1] loss: 0.481
[53,     1] loss: 0.449
[54,     1] loss: 0.435
[55,     1] loss: 0.434
[56,     1] loss: 0.426
[57,     1] loss: 0.413
[58,     1] loss: 0.397
[59,     1] loss: 0.381
[60,     1] loss: 0.366
[61,     1] loss: 0.350
[62,     1] loss: 0.338
[63,     1] loss: 0.327
[64,     1] loss: 0.321
[65,     1] loss: 0.316
[66,     1] loss: 0.302
[67,     1] loss: 0.298
[68,     1] loss: 0.292
[69,     1] loss: 0.297
[70,     1] loss: 0.292
[71,     1] loss: 0.293
[72,     1] loss: 0.329
[73,     1] loss: 0.525
[74,     1] loss: 0.542
[75,     1] loss: 0.539
[76,     1] loss: 0.519
[77,     1] loss: 0.517
[78,     1] loss: 0.520
[79,     1] loss: 0.517
[80,     1] loss: 0.508
[81,     1] loss: 0.498
[82,     1] loss: 0.492
[83,     1] loss: 0.484
[84,     1] loss: 0.478
[85,     1] loss: 0.469
[86,     1] loss: 0.463
[87,     1] loss: 0.455
[88,     1] loss: 0.450
[89,     1] loss: 0.446
[90,     1] loss: 0.441
[91,     1] loss: 0.457
[92,     1] loss: 0.498
[93,     1] loss: 0.503
[94,     1] loss: 0.518
[95,     1] loss: 0.491
[96,     1] loss: 0.507
[97,     1] loss: 0.495
[98,     1] loss: 0.486
[99,     1] loss: 0.480
[100,     1] loss: 0.481
[101,     1] loss: 0.477
[102,     1] loss: 0.475
[103,     1] loss: 0.472
[104,     1] loss: 0.472
[105,     1] loss: 0.471
[106,     1] loss: 0.472
[107,     1] loss: 0.505
[108,     1] loss: 0.515
[109,     1] loss: 0.520
Early stopping applied (best metric=0.3479677140712738)
Finished Training
Total time taken: 396.39373898506165
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.7696774193548387, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9041666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.7450381679389313, 'Pyrrolidone carboxylic acid Validation Precision': 0.4215573339299649, 'Pyrrolidone carboxylic acid AUC ROC': 0.9315839694656488, 'Pyrrolidone carboxylic acid AUC PR': 0.7839452154375539, 'Pyrrolidone carboxylic acid MCC': 0.5083034449203658, 'Pyrrolidone carboxylic acid F1': 0.5650556460864427, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.3445736110210419, 'Validation Loss (total)': 0.3445736110210419}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008432883983618512,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1078204474,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.95150932133077}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.644
[4,     1] loss: 0.607
[5,     1] loss: 0.563
[6,     1] loss: 0.514
[7,     1] loss: 0.463
[8,     1] loss: 0.411
[9,     1] loss: 0.353
[10,     1] loss: 0.288
[11,     1] loss: 0.231
[12,     1] loss: 0.176
[13,     1] loss: 0.123
[14,     1] loss: 0.092
[15,     1] loss: 0.067
[16,     1] loss: 0.052
[17,     1] loss: 0.055
[18,     1] loss: 0.035
[19,     1] loss: 0.058
[20,     1] loss: 0.038
[21,     1] loss: 0.035
[22,     1] loss: 0.041
[23,     1] loss: 0.039
[24,     1] loss: 0.039
[25,     1] loss: 0.034
[26,     1] loss: 0.032
[27,     1] loss: 0.028
[28,     1] loss: 0.026
[29,     1] loss: 0.025
[30,     1] loss: 0.023
[31,     1] loss: 0.021
[32,     1] loss: 0.021
[33,     1] loss: 0.021
[34,     1] loss: 0.021
[35,     1] loss: 0.021
[36,     1] loss: 0.021
[37,     1] loss: 0.022
[38,     1] loss: 0.022
[39,     1] loss: 0.022
[40,     1] loss: 0.022
[41,     1] loss: 0.022
[42,     1] loss: 0.022
[43,     1] loss: 0.021
[44,     1] loss: 0.021
[45,     1] loss: 0.021
[46,     1] loss: 0.020
[47,     1] loss: 0.020
[48,     1] loss: 0.020
[49,     1] loss: 0.019
[50,     1] loss: 0.020
[51,     1] loss: 0.018
[52,     1] loss: 0.020
[53,     1] loss: 0.020
[54,     1] loss: 0.019
[55,     1] loss: 0.019
[56,     1] loss: 0.018
[57,     1] loss: 0.019
[58,     1] loss: 0.019
[59,     1] loss: 0.065
[60,     1] loss: 0.281
[61,     1] loss: 0.726
Early stopping applied (best metric=0.25776439905166626)
Finished Training
Total time taken: 223.28426027297974
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.628
[4,     1] loss: 0.579
[5,     1] loss: 0.519
[6,     1] loss: 0.458
[7,     1] loss: 0.395
[8,     1] loss: 0.333
[9,     1] loss: 0.274
[10,     1] loss: 0.221
[11,     1] loss: 0.172
[12,     1] loss: 0.137
[13,     1] loss: 0.096
[14,     1] loss: 0.071
[15,     1] loss: 0.053
[16,     1] loss: 0.041
[17,     1] loss: 0.032
[18,     1] loss: 0.027
[19,     1] loss: 0.022
[20,     1] loss: 0.021
[21,     1] loss: 0.019
[22,     1] loss: 0.019
[23,     1] loss: 0.020
[24,     1] loss: 0.020
[25,     1] loss: 0.020
[26,     1] loss: 0.021
[27,     1] loss: 0.021
[28,     1] loss: 0.022
[29,     1] loss: 0.022
[30,     1] loss: 0.023
[31,     1] loss: 0.022
[32,     1] loss: 0.022
[33,     1] loss: 0.021
[34,     1] loss: 0.020
[35,     1] loss: 0.020
[36,     1] loss: 0.020
[37,     1] loss: 0.018
[38,     1] loss: 0.018
[39,     1] loss: 0.017
[40,     1] loss: 0.017
[41,     1] loss: 0.016
[42,     1] loss: 0.017
[43,     1] loss: 0.016
[44,     1] loss: 0.016
[45,     1] loss: 0.017
[46,     1] loss: 0.016
[47,     1] loss: 0.016
[48,     1] loss: 0.016
[49,     1] loss: 0.015
[50,     1] loss: 0.016
[51,     1] loss: 0.016
[52,     1] loss: 0.015
[53,     1] loss: 0.015
[54,     1] loss: 0.016
[55,     1] loss: 0.015
[56,     1] loss: 0.014
[57,     1] loss: 0.015
[58,     1] loss: 0.015
[59,     1] loss: 0.015
[60,     1] loss: 0.016
[61,     1] loss: 0.015
[62,     1] loss: 0.021
[63,     1] loss: 0.158
[64,     1] loss: 1.170
[65,     1] loss: 0.620
[66,     1] loss: 0.568
[67,     1] loss: 0.579
[68,     1] loss: 0.576
[69,     1] loss: 0.572
[70,     1] loss: 0.552
[71,     1] loss: 0.525
[72,     1] loss: 0.496
[73,     1] loss: 0.471
[74,     1] loss: 0.481
[75,     1] loss: 0.480
[76,     1] loss: 0.409
[77,     1] loss: 0.363
[78,     1] loss: 0.379
[79,     1] loss: 0.306
[80,     1] loss: 0.294
[81,     1] loss: 0.250
[82,     1] loss: 0.211
[83,     1] loss: 0.220
[84,     1] loss: 0.349
[85,     1] loss: 0.233
[86,     1] loss: 0.288
[87,     1] loss: 0.292
[88,     1] loss: 0.253
[89,     1] loss: 0.239
[90,     1] loss: 0.194
[91,     1] loss: 0.145
[92,     1] loss: 0.130
[93,     1] loss: 0.113
[94,     1] loss: 0.110
[95,     1] loss: 0.082
[96,     1] loss: 0.077
[97,     1] loss: 0.072
[98,     1] loss: 0.074
[99,     1] loss: 0.060
[100,     1] loss: 0.078
[101,     1] loss: 0.064
[102,     1] loss: 0.050
[103,     1] loss: 0.051
[104,     1] loss: 0.045
[105,     1] loss: 0.045
[106,     1] loss: 0.044
[107,     1] loss: 0.041
[108,     1] loss: 0.041
[109,     1] loss: 0.037
[110,     1] loss: 0.045
Early stopping applied (best metric=0.23601152002811432)
Finished Training
Total time taken: 401.19350814819336
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.676
[3,     1] loss: 0.621
[4,     1] loss: 0.566
[5,     1] loss: 0.514
[6,     1] loss: 0.462
[7,     1] loss: 0.415
[8,     1] loss: 0.363
[9,     1] loss: 0.315
[10,     1] loss: 0.268
[11,     1] loss: 0.225
[12,     1] loss: 0.194
[13,     1] loss: 0.155
[14,     1] loss: 0.119
[15,     1] loss: 0.087
[16,     1] loss: 0.065
[17,     1] loss: 0.045
[18,     1] loss: 0.035
[19,     1] loss: 0.029
[20,     1] loss: 0.026
[21,     1] loss: 0.021
[22,     1] loss: 0.021
[23,     1] loss: 0.019
[24,     1] loss: 0.018
[25,     1] loss: 0.017
[26,     1] loss: 0.016
[27,     1] loss: 0.016
[28,     1] loss: 0.017
[29,     1] loss: 0.016
[30,     1] loss: 0.017
[31,     1] loss: 0.017
[32,     1] loss: 0.017
[33,     1] loss: 0.017
[34,     1] loss: 0.017
[35,     1] loss: 0.016
[36,     1] loss: 0.016
[37,     1] loss: 0.017
[38,     1] loss: 0.016
[39,     1] loss: 0.016
[40,     1] loss: 0.015
[41,     1] loss: 0.015
[42,     1] loss: 0.014
[43,     1] loss: 0.014
[44,     1] loss: 0.014
[45,     1] loss: 0.014
[46,     1] loss: 0.015
[47,     1] loss: 0.014
[48,     1] loss: 0.014
[49,     1] loss: 0.014
[50,     1] loss: 0.014
[51,     1] loss: 0.014
[52,     1] loss: 0.014
[53,     1] loss: 0.013
[54,     1] loss: 0.013
[55,     1] loss: 0.014
[56,     1] loss: 0.013
[57,     1] loss: 0.014
[58,     1] loss: 0.013
[59,     1] loss: 0.014
[60,     1] loss: 0.015
[61,     1] loss: 0.152
[62,     1] loss: 0.752
[63,     1] loss: 0.873
[64,     1] loss: 0.629
[65,     1] loss: 0.598
[66,     1] loss: 0.607
[67,     1] loss: 0.606
[68,     1] loss: 0.600
[69,     1] loss: 0.609
[70,     1] loss: 0.617
[71,     1] loss: 0.614
[72,     1] loss: 0.607
[73,     1] loss: 0.604
[74,     1] loss: 0.601
[75,     1] loss: 0.595
[76,     1] loss: 0.580
[77,     1] loss: 0.566
[78,     1] loss: 0.544
[79,     1] loss: 0.516
[80,     1] loss: 0.487
[81,     1] loss: 0.448
[82,     1] loss: 0.404
[83,     1] loss: 0.352
[84,     1] loss: 0.285
[85,     1] loss: 0.223
[86,     1] loss: 0.172
[87,     1] loss: 0.133
[88,     1] loss: 0.101
[89,     1] loss: 0.087
[90,     1] loss: 0.065
[91,     1] loss: 0.062
[92,     1] loss: 0.055
[93,     1] loss: 0.031
[94,     1] loss: 0.807
[95,     1] loss: 0.158
[96,     1] loss: 0.452
[97,     1] loss: 0.423
[98,     1] loss: 0.491
[99,     1] loss: 0.511
[100,     1] loss: 0.529
[101,     1] loss: 0.537
[102,     1] loss: 0.545
[103,     1] loss: 0.549
[104,     1] loss: 0.547
[105,     1] loss: 0.546
[106,     1] loss: 0.543
[107,     1] loss: 0.541
[108,     1] loss: 0.533
[109,     1] loss: 0.526
[110,     1] loss: 0.515
[111,     1] loss: 0.499
[112,     1] loss: 0.480
[113,     1] loss: 0.458
[114,     1] loss: 0.434
[115,     1] loss: 0.406
[116,     1] loss: 0.378
[117,     1] loss: 0.355
[118,     1] loss: 0.332
[119,     1] loss: 0.314
[120,     1] loss: 0.297
[121,     1] loss: 0.281
[122,     1] loss: 0.273
[123,     1] loss: 0.267
[124,     1] loss: 0.294
[125,     1] loss: 0.331
[126,     1] loss: 0.376
[127,     1] loss: 0.348
[128,     1] loss: 0.317
[129,     1] loss: 0.281
[130,     1] loss: 0.259
[131,     1] loss: 0.243
[132,     1] loss: 0.217
[133,     1] loss: 0.191
[134,     1] loss: 0.164
Early stopping applied (best metric=0.28373467922210693)
Finished Training
Total time taken: 488.5449049472809
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.627
[4,     1] loss: 0.581
[5,     1] loss: 0.527
[6,     1] loss: 0.470
[7,     1] loss: 0.409
[8,     1] loss: 0.350
[9,     1] loss: 0.282
[10,     1] loss: 0.223
[11,     1] loss: 0.167
[12,     1] loss: 0.124
[13,     1] loss: 0.091
[14,     1] loss: 0.073
[15,     1] loss: 0.053
[16,     1] loss: 0.042
[17,     1] loss: 0.036
[18,     1] loss: 0.032
[19,     1] loss: 0.026
[20,     1] loss: 0.026
[21,     1] loss: 0.024
[22,     1] loss: 0.023
[23,     1] loss: 0.023
[24,     1] loss: 0.021
[25,     1] loss: 0.022
[26,     1] loss: 0.022
[27,     1] loss: 0.021
[28,     1] loss: 0.021
[29,     1] loss: 0.021
[30,     1] loss: 0.021
[31,     1] loss: 0.020
[32,     1] loss: 0.020
[33,     1] loss: 0.019
[34,     1] loss: 0.018
[35,     1] loss: 0.018
[36,     1] loss: 0.018
[37,     1] loss: 0.017
[38,     1] loss: 0.017
[39,     1] loss: 0.016
[40,     1] loss: 0.016
[41,     1] loss: 0.016
[42,     1] loss: 0.016
[43,     1] loss: 0.016
[44,     1] loss: 0.016
[45,     1] loss: 0.016
[46,     1] loss: 0.015
[47,     1] loss: 0.015
[48,     1] loss: 0.015
[49,     1] loss: 0.015
[50,     1] loss: 0.015
[51,     1] loss: 0.015
[52,     1] loss: 0.014
[53,     1] loss: 0.014
[54,     1] loss: 0.015
[55,     1] loss: 0.048
[56,     1] loss: 0.127
[57,     1] loss: 1.293
[58,     1] loss: 0.704
[59,     1] loss: 0.659
[60,     1] loss: 0.585
[61,     1] loss: 0.608
[62,     1] loss: 0.629
Early stopping applied (best metric=0.2587612569332123)
Finished Training
Total time taken: 228.51237034797668
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.667
[3,     1] loss: 0.603
[4,     1] loss: 0.530
[5,     1] loss: 0.453
[6,     1] loss: 0.378
[7,     1] loss: 0.304
[8,     1] loss: 0.233
[9,     1] loss: 0.174
[10,     1] loss: 0.136
[11,     1] loss: 0.085
[12,     1] loss: 0.068
[13,     1] loss: 0.047
[14,     1] loss: 0.049
[15,     1] loss: 0.036
[16,     1] loss: 0.033
[17,     1] loss: 0.029
[18,     1] loss: 0.027
[19,     1] loss: 0.025
[20,     1] loss: 0.022
[21,     1] loss: 0.021
[22,     1] loss: 0.022
[23,     1] loss: 0.022
[24,     1] loss: 0.022
[25,     1] loss: 0.022
[26,     1] loss: 0.022
[27,     1] loss: 0.021
[28,     1] loss: 0.021
[29,     1] loss: 0.020
[30,     1] loss: 0.020
[31,     1] loss: 0.020
[32,     1] loss: 0.019
[33,     1] loss: 0.019
[34,     1] loss: 0.018
[35,     1] loss: 0.018
[36,     1] loss: 0.018
[37,     1] loss: 0.017
[38,     1] loss: 0.017
[39,     1] loss: 0.017
[40,     1] loss: 0.017
[41,     1] loss: 0.016
[42,     1] loss: 0.017
[43,     1] loss: 0.016
[44,     1] loss: 0.016
[45,     1] loss: 0.016
[46,     1] loss: 0.016
[47,     1] loss: 0.016
[48,     1] loss: 0.015
[49,     1] loss: 0.016
[50,     1] loss: 0.015
[51,     1] loss: 0.016
[52,     1] loss: 0.015
[53,     1] loss: 0.016
[54,     1] loss: 0.014
[55,     1] loss: 0.020
[56,     1] loss: 0.178
[57,     1] loss: 0.981
[58,     1] loss: 0.625
[59,     1] loss: 0.470
Early stopping applied (best metric=0.25182482600212097)
Finished Training
Total time taken: 217.6352710723877
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8690322580645161, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8875000000000001, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8656488549618321, 'Pyrrolidone carboxylic acid Validation Precision': 0.5553254542501854, 'Pyrrolidone carboxylic acid AUC ROC': 0.9377862595419847, 'Pyrrolidone carboxylic acid AUC PR': 0.7603630247119291, 'Pyrrolidone carboxylic acid MCC': 0.6324598711061952, 'Pyrrolidone carboxylic acid F1': 0.6804821895091974, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.25761933624744415, 'Validation Loss (total)': 0.25761933624744415}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007938293688577515,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4283883258,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.405677900624}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.647
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007267689359107045,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 10487957,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.36845310179869}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.684
[3,     1] loss: 0.623
[4,     1] loss: 0.554
[5,     1] loss: 0.491
[6,     1] loss: 0.446
[7,     1] loss: 0.413
[8,     1] loss: 0.367
[9,     1] loss: 0.319
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008742005095983489,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3196851644,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.2293627101903377}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.644
[4,     1] loss: 0.576
[5,     1] loss: 0.489
[6,     1] loss: 0.413
[7,     1] loss: 0.354
[8,     1] loss: 0.294
[9,     1] loss: 0.231
[10,     1] loss: 0.183
[11,     1] loss: 0.153
[12,     1] loss: 0.122
[13,     1] loss: 0.168
[14,     1] loss: 0.136
[15,     1] loss: 0.137
[16,     1] loss: 0.068
[17,     1] loss: 0.030
[18,     1] loss: 0.105
[19,     1] loss: 0.016
[20,     1] loss: 0.019
[21,     1] loss: 0.009
[22,     1] loss: 0.008
[23,     1] loss: 0.008
[24,     1] loss: 0.007
[25,     1] loss: 0.007
[26,     1] loss: 0.006
[27,     1] loss: 0.004
[28,     1] loss: 0.003
[29,     1] loss: 0.003
[30,     1] loss: 0.002
[31,     1] loss: 0.002
[32,     1] loss: 0.002
[33,     1] loss: 0.002
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.001
[39,     1] loss: 0.001
[40,     1] loss: 0.001
[41,     1] loss: 0.001
[42,     1] loss: 0.001
[43,     1] loss: 0.001
[44,     1] loss: 0.001
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
[59,     1] loss: 0.001
[60,     1] loss: 0.001
[61,     1] loss: 0.001
[62,     1] loss: 0.001
[63,     1] loss: 0.001
Early stopping applied (best metric=0.3691200315952301)
Finished Training
Total time taken: 232.47440552711487
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.637
[4,     1] loss: 0.572
[5,     1] loss: 0.476
[6,     1] loss: 0.361
[7,     1] loss: 0.260
[8,     1] loss: 0.199
[9,     1] loss: 0.155
[10,     1] loss: 0.121
[11,     1] loss: 0.082
[12,     1] loss: 0.050
[13,     1] loss: 0.052
[14,     1] loss: 0.056
[15,     1] loss: 0.035
[16,     1] loss: 0.029
[17,     1] loss: 0.019
[18,     1] loss: 0.017
[19,     1] loss: 0.011
[20,     1] loss: 0.006
[21,     1] loss: 0.004
[22,     1] loss: 0.003
[23,     1] loss: 0.001
[24,     1] loss: 0.001
[25,     1] loss: 0.001
[26,     1] loss: 0.000
[27,     1] loss: 0.000
[28,     1] loss: 0.000
[29,     1] loss: 0.000
[30,     1] loss: 0.000
[31,     1] loss: 0.000
[32,     1] loss: 0.000
[33,     1] loss: 0.000
[34,     1] loss: 0.000
[35,     1] loss: 0.000
[36,     1] loss: 0.000
[37,     1] loss: 0.000
[38,     1] loss: 0.000
[39,     1] loss: 0.000
[40,     1] loss: 0.000
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
Early stopping applied (best metric=0.3058222830295563)
Finished Training
Total time taken: 207.41349053382874
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.686
[3,     1] loss: 0.632
[4,     1] loss: 0.557
[5,     1] loss: 0.468
[6,     1] loss: 0.388
[7,     1] loss: 0.295
[8,     1] loss: 0.211
[9,     1] loss: 0.145
[10,     1] loss: 0.099
[11,     1] loss: 0.073
[12,     1] loss: 0.058
[13,     1] loss: 0.047
[14,     1] loss: 0.056
[15,     1] loss: 0.069
[16,     1] loss: 0.064
[17,     1] loss: 0.059
[18,     1] loss: 0.037
[19,     1] loss: 0.027
[20,     1] loss: 0.033
[21,     1] loss: 0.024
[22,     1] loss: 0.018
[23,     1] loss: 0.017
[24,     1] loss: 0.012
[25,     1] loss: 0.009
[26,     1] loss: 0.006
[27,     1] loss: 0.004
[28,     1] loss: 0.003
[29,     1] loss: 0.002
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.000
[33,     1] loss: 0.000
[34,     1] loss: 0.000
[35,     1] loss: 0.000
[36,     1] loss: 0.000
[37,     1] loss: 0.000
[38,     1] loss: 0.000
[39,     1] loss: 0.000
[40,     1] loss: 0.000
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.000
[51,     1] loss: 0.000
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
[57,     1] loss: 0.001
[58,     1] loss: 0.001
Early stopping applied (best metric=0.29619428515434265)
Finished Training
Total time taken: 214.88850784301758
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.599
[4,     1] loss: 0.498
[5,     1] loss: 0.401
[6,     1] loss: 0.298
[7,     1] loss: 0.206
[8,     1] loss: 0.139
[9,     1] loss: 0.096
[10,     1] loss: 0.073
[11,     1] loss: 0.060
[12,     1] loss: 0.073
[13,     1] loss: 0.096
[14,     1] loss: 0.048
[15,     1] loss: 0.046
[16,     1] loss: 0.027
[17,     1] loss: 0.026
[18,     1] loss: 0.022
[19,     1] loss: 0.020
[20,     1] loss: 0.016
[21,     1] loss: 0.014
[22,     1] loss: 0.010
[23,     1] loss: 0.007
[24,     1] loss: 0.004
[25,     1] loss: 0.004
[26,     1] loss: 0.003
[27,     1] loss: 0.003
[28,     1] loss: 0.002
[29,     1] loss: 0.001
[30,     1] loss: 0.001
[31,     1] loss: 0.001
[32,     1] loss: 0.001
[33,     1] loss: 0.001
[34,     1] loss: 0.001
[35,     1] loss: 0.001
[36,     1] loss: 0.001
[37,     1] loss: 0.001
[38,     1] loss: 0.000
[39,     1] loss: 0.000
[40,     1] loss: 0.000
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.001
[46,     1] loss: 0.001
[47,     1] loss: 0.001
[48,     1] loss: 0.001
[49,     1] loss: 0.001
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
Early stopping applied (best metric=0.3002677261829376)
Finished Training
Total time taken: 207.7638282775879
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.602
[4,     1] loss: 0.503
[5,     1] loss: 0.382
[6,     1] loss: 0.277
[7,     1] loss: 0.199
[8,     1] loss: 0.147
[9,     1] loss: 0.110
[10,     1] loss: 0.089
[11,     1] loss: 0.046
[12,     1] loss: 0.044
[13,     1] loss: 0.032
[14,     1] loss: 0.025
[15,     1] loss: 0.025
[16,     1] loss: 0.017
[17,     1] loss: 0.009
[18,     1] loss: 0.006
[19,     1] loss: 0.003
[20,     1] loss: 0.001
[21,     1] loss: 0.001
[22,     1] loss: 0.000
[23,     1] loss: 0.000
[24,     1] loss: 0.000
[25,     1] loss: 0.000
[26,     1] loss: 0.000
[27,     1] loss: 0.000
[28,     1] loss: 0.000
[29,     1] loss: 0.000
[30,     1] loss: 0.000
[31,     1] loss: 0.000
[32,     1] loss: 0.000
[33,     1] loss: 0.000
[34,     1] loss: 0.000
[35,     1] loss: 0.000
[36,     1] loss: 0.000
[37,     1] loss: 0.000
[38,     1] loss: 0.000
[39,     1] loss: 0.000
[40,     1] loss: 0.000
[41,     1] loss: 0.000
[42,     1] loss: 0.000
[43,     1] loss: 0.000
[44,     1] loss: 0.000
[45,     1] loss: 0.000
[46,     1] loss: 0.000
[47,     1] loss: 0.000
[48,     1] loss: 0.000
[49,     1] loss: 0.000
[50,     1] loss: 0.001
[51,     1] loss: 0.001
[52,     1] loss: 0.001
[53,     1] loss: 0.001
[54,     1] loss: 0.001
[55,     1] loss: 0.001
[56,     1] loss: 0.001
Early stopping applied (best metric=0.30814144015312195)
Finished Training
Total time taken: 207.81504273414612
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8012903225806451, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.85, 'Pyrrolidone carboxylic acid Validation Specificity': 0.7923664122137405, 'Pyrrolidone carboxylic acid Validation Precision': 0.4345454130281352, 'Pyrrolidone carboxylic acid AUC ROC': 0.909748727735369, 'Pyrrolidone carboxylic acid AUC PR': 0.6979027962432497, 'Pyrrolidone carboxylic acid MCC': 0.5072078741977758, 'Pyrrolidone carboxylic acid F1': 0.5731046665696614, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.3159091532230377, 'Validation Loss (total)': 0.3159091532230377}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007350311302083881,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 877021942,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.545097125907102}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.692
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016505917001100976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1502342545,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.981502192068248}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004173450797047024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1285105903,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.052604776129987}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.647
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00476130888183061,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3988412716,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.275712664547862}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.666
[4,     1] loss: 0.647
[5,     1] loss: 0.626
[6,     1] loss: 0.602
[7,     1] loss: 0.577
[8,     1] loss: 0.550
[9,     1] loss: 0.521
[10,     1] loss: 0.490
[11,     1] loss: 0.454
[12,     1] loss: 0.421
[13,     1] loss: 0.386
[14,     1] loss: 0.352
[15,     1] loss: 0.317
[16,     1] loss: 0.284
[17,     1] loss: 0.256
[18,     1] loss: 0.232
[19,     1] loss: 0.211
[20,     1] loss: 0.189
[21,     1] loss: 0.171
[22,     1] loss: 0.158
[23,     1] loss: 0.144
[24,     1] loss: 0.131
[25,     1] loss: 0.122
[26,     1] loss: 0.111
[27,     1] loss: 0.103
[28,     1] loss: 0.098
[29,     1] loss: 0.091
[30,     1] loss: 0.085
[31,     1] loss: 0.082
[32,     1] loss: 0.081
[33,     1] loss: 0.077
[34,     1] loss: 0.074
[35,     1] loss: 0.072
[36,     1] loss: 0.072
[37,     1] loss: 0.071
[38,     1] loss: 0.071
[39,     1] loss: 0.071
[40,     1] loss: 0.069
[41,     1] loss: 0.070
[42,     1] loss: 0.068
[43,     1] loss: 0.069
[44,     1] loss: 0.068
[45,     1] loss: 0.068
[46,     1] loss: 0.068
[47,     1] loss: 0.068
[48,     1] loss: 0.066
[49,     1] loss: 0.066
[50,     1] loss: 0.067
[51,     1] loss: 0.067
[52,     1] loss: 0.067
[53,     1] loss: 0.066
[54,     1] loss: 0.066
[55,     1] loss: 0.065
[56,     1] loss: 0.065
[57,     1] loss: 0.065
[58,     1] loss: 0.064
[59,     1] loss: 0.065
[60,     1] loss: 0.065
[61,     1] loss: 0.065
[62,     1] loss: 0.089
[63,     1] loss: 0.166
[64,     1] loss: 0.465
[65,     1] loss: 0.350
[66,     1] loss: 0.296
[67,     1] loss: 0.496
[68,     1] loss: 0.328
[69,     1] loss: 0.392
[70,     1] loss: 0.365
[71,     1] loss: 0.354
[72,     1] loss: 0.360
[73,     1] loss: 0.368
[74,     1] loss: 0.361
[75,     1] loss: 0.346
[76,     1] loss: 0.330
[77,     1] loss: 0.315
[78,     1] loss: 0.299
[79,     1] loss: 0.276
[80,     1] loss: 0.258
[81,     1] loss: 0.239
[82,     1] loss: 0.219
[83,     1] loss: 0.199
[84,     1] loss: 0.182
[85,     1] loss: 0.174
[86,     1] loss: 0.154
[87,     1] loss: 0.141
[88,     1] loss: 0.136
[89,     1] loss: 0.119
[90,     1] loss: 0.113
[91,     1] loss: 0.108
[92,     1] loss: 0.102
[93,     1] loss: 0.101
Early stopping applied (best metric=0.24073278903961182)
Finished Training
Total time taken: 343.8860924243927
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.654
[4,     1] loss: 0.628
[5,     1] loss: 0.602
[6,     1] loss: 0.574
[7,     1] loss: 0.544
[8,     1] loss: 0.513
[9,     1] loss: 0.484
[10,     1] loss: 0.450
[11,     1] loss: 0.416
[12,     1] loss: 0.382
[13,     1] loss: 0.353
[14,     1] loss: 0.321
[15,     1] loss: 0.292
[16,     1] loss: 0.259
[17,     1] loss: 0.233
[18,     1] loss: 0.208
[19,     1] loss: 0.184
[20,     1] loss: 0.164
[21,     1] loss: 0.146
[22,     1] loss: 0.131
[23,     1] loss: 0.117
[24,     1] loss: 0.103
[25,     1] loss: 0.092
[26,     1] loss: 0.081
[27,     1] loss: 0.073
[28,     1] loss: 0.067
[29,     1] loss: 0.061
[30,     1] loss: 0.058
[31,     1] loss: 0.054
[32,     1] loss: 0.053
[33,     1] loss: 0.051
[34,     1] loss: 0.049
[35,     1] loss: 0.048
[36,     1] loss: 0.049
[37,     1] loss: 0.048
[38,     1] loss: 0.049
[39,     1] loss: 0.049
[40,     1] loss: 0.048
[41,     1] loss: 0.049
[42,     1] loss: 0.049
[43,     1] loss: 0.050
[44,     1] loss: 0.050
[45,     1] loss: 0.051
[46,     1] loss: 0.049
[47,     1] loss: 0.050
[48,     1] loss: 0.050
[49,     1] loss: 0.048
[50,     1] loss: 0.048
[51,     1] loss: 0.048
[52,     1] loss: 0.047
[53,     1] loss: 0.047
[54,     1] loss: 0.046
[55,     1] loss: 0.045
[56,     1] loss: 0.045
[57,     1] loss: 0.044
[58,     1] loss: 0.045
[59,     1] loss: 0.044
[60,     1] loss: 0.084
[61,     1] loss: 0.194
[62,     1] loss: 0.610
[63,     1] loss: 0.396
[64,     1] loss: 0.350
[65,     1] loss: 0.352
[66,     1] loss: 0.344
[67,     1] loss: 0.327
[68,     1] loss: 0.318
[69,     1] loss: 0.312
[70,     1] loss: 0.292
[71,     1] loss: 0.271
[72,     1] loss: 0.248
[73,     1] loss: 0.221
[74,     1] loss: 0.192
[75,     1] loss: 0.168
[76,     1] loss: 0.147
[77,     1] loss: 0.126
[78,     1] loss: 0.113
[79,     1] loss: 0.099
[80,     1] loss: 0.090
Early stopping applied (best metric=0.24125146865844727)
Finished Training
Total time taken: 296.7002911567688
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.674
[3,     1] loss: 0.638
[4,     1] loss: 0.603
[5,     1] loss: 0.559
[6,     1] loss: 0.520
[7,     1] loss: 0.476
[8,     1] loss: 0.434
[9,     1] loss: 0.396
[10,     1] loss: 0.361
[11,     1] loss: 0.336
[12,     1] loss: 0.319
[13,     1] loss: 0.307
[14,     1] loss: 0.299
[15,     1] loss: 0.291
[16,     1] loss: 0.283
[17,     1] loss: 0.275
[18,     1] loss: 0.267
[19,     1] loss: 0.257
[20,     1] loss: 0.247
[21,     1] loss: 0.237
[22,     1] loss: 0.227
[23,     1] loss: 0.216
[24,     1] loss: 0.206
[25,     1] loss: 0.195
[26,     1] loss: 0.181
[27,     1] loss: 0.168
[28,     1] loss: 0.156
[29,     1] loss: 0.145
[30,     1] loss: 0.132
[31,     1] loss: 0.121
[32,     1] loss: 0.111
[33,     1] loss: 0.102
[34,     1] loss: 0.090
[35,     1] loss: 0.082
[36,     1] loss: 0.075
[37,     1] loss: 0.069
[38,     1] loss: 0.062
[39,     1] loss: 0.060
[40,     1] loss: 0.056
[41,     1] loss: 0.054
[42,     1] loss: 0.051
[43,     1] loss: 0.051
[44,     1] loss: 0.050
[45,     1] loss: 0.049
[46,     1] loss: 0.049
[47,     1] loss: 0.050
[48,     1] loss: 0.050
[49,     1] loss: 0.050
[50,     1] loss: 0.052
[51,     1] loss: 0.052
[52,     1] loss: 0.052
[53,     1] loss: 0.055
[54,     1] loss: 0.053
[55,     1] loss: 0.052
[56,     1] loss: 0.055
[57,     1] loss: 0.055
[58,     1] loss: 0.053
[59,     1] loss: 0.055
[60,     1] loss: 0.137
[61,     1] loss: 0.483
[62,     1] loss: 0.525
[63,     1] loss: 0.416
[64,     1] loss: 0.421
[65,     1] loss: 0.444
[66,     1] loss: 0.446
[67,     1] loss: 0.446
[68,     1] loss: 0.447
[69,     1] loss: 0.440
[70,     1] loss: 0.431
[71,     1] loss: 0.419
[72,     1] loss: 0.401
[73,     1] loss: 0.380
[74,     1] loss: 0.359
[75,     1] loss: 0.331
[76,     1] loss: 0.307
[77,     1] loss: 0.277
[78,     1] loss: 0.244
[79,     1] loss: 0.222
[80,     1] loss: 0.203
[81,     1] loss: 0.210
[82,     1] loss: 0.189
[83,     1] loss: 0.193
[84,     1] loss: 0.165
[85,     1] loss: 0.153
[86,     1] loss: 0.161
[87,     1] loss: 0.135
[88,     1] loss: 0.145
[89,     1] loss: 0.119
[90,     1] loss: 0.130
[91,     1] loss: 0.118
[92,     1] loss: 0.115
[93,     1] loss: 0.105
[94,     1] loss: 0.103
[95,     1] loss: 0.098
[96,     1] loss: 0.093
[97,     1] loss: 0.088
[98,     1] loss: 0.089
[99,     1] loss: 0.080
[100,     1] loss: 0.077
[101,     1] loss: 0.079
[102,     1] loss: 0.078
[103,     1] loss: 0.077
[104,     1] loss: 0.075
[105,     1] loss: 0.073
[106,     1] loss: 0.075
[107,     1] loss: 0.071
[108,     1] loss: 0.075
[109,     1] loss: 0.075
[110,     1] loss: 0.072
[111,     1] loss: 0.075
[112,     1] loss: 0.074
[113,     1] loss: 0.074
[114,     1] loss: 0.074
[115,     1] loss: 0.074
[116,     1] loss: 0.075
[117,     1] loss: 0.074
[118,     1] loss: 0.075
[119,     1] loss: 0.076
[120,     1] loss: 0.074
[121,     1] loss: 0.076
[122,     1] loss: 0.076
[123,     1] loss: 0.076
[124,     1] loss: 0.076
[125,     1] loss: 0.076
[126,     1] loss: 0.078
[127,     1] loss: 0.076
[128,     1] loss: 0.076
[129,     1] loss: 0.076
Early stopping applied (best metric=0.2628031075000763)
Finished Training
Total time taken: 477.09457445144653
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.681
[3,     1] loss: 0.641
[4,     1] loss: 0.606
[5,     1] loss: 0.567
[6,     1] loss: 0.526
[7,     1] loss: 0.492
[8,     1] loss: 0.452
[9,     1] loss: 0.415
[10,     1] loss: 0.380
[11,     1] loss: 0.344
[12,     1] loss: 0.313
[13,     1] loss: 0.291
[14,     1] loss: 0.269
[15,     1] loss: 0.254
[16,     1] loss: 0.239
[17,     1] loss: 0.222
[18,     1] loss: 0.209
[19,     1] loss: 0.194
[20,     1] loss: 0.184
[21,     1] loss: 0.171
[22,     1] loss: 0.157
[23,     1] loss: 0.149
[24,     1] loss: 0.134
[25,     1] loss: 0.123
[26,     1] loss: 0.115
[27,     1] loss: 0.104
[28,     1] loss: 0.099
[29,     1] loss: 0.091
[30,     1] loss: 0.084
[31,     1] loss: 0.079
[32,     1] loss: 0.073
[33,     1] loss: 0.071
[34,     1] loss: 0.065
[35,     1] loss: 0.062
[36,     1] loss: 0.059
[37,     1] loss: 0.056
[38,     1] loss: 0.054
[39,     1] loss: 0.052
[40,     1] loss: 0.050
[41,     1] loss: 0.050
[42,     1] loss: 0.050
[43,     1] loss: 0.048
[44,     1] loss: 0.049
[45,     1] loss: 0.047
[46,     1] loss: 0.047
[47,     1] loss: 0.048
[48,     1] loss: 0.047
[49,     1] loss: 0.048
[50,     1] loss: 0.046
[51,     1] loss: 0.047
[52,     1] loss: 0.046
[53,     1] loss: 0.048
[54,     1] loss: 0.047
[55,     1] loss: 0.047
[56,     1] loss: 0.048
[57,     1] loss: 0.047
[58,     1] loss: 0.047
[59,     1] loss: 0.046
[60,     1] loss: 0.048
[61,     1] loss: 0.047
[62,     1] loss: 0.045
[63,     1] loss: 0.046
[64,     1] loss: 0.046
[65,     1] loss: 0.045
[66,     1] loss: 0.049
[67,     1] loss: 0.188
[68,     1] loss: 0.640
[69,     1] loss: 0.573
[70,     1] loss: 0.543
[71,     1] loss: 0.480
[72,     1] loss: 0.462
[73,     1] loss: 0.487
[74,     1] loss: 0.475
[75,     1] loss: 0.463
[76,     1] loss: 0.466
[77,     1] loss: 0.466
[78,     1] loss: 0.456
[79,     1] loss: 0.438
[80,     1] loss: 0.423
[81,     1] loss: 0.408
[82,     1] loss: 0.384
[83,     1] loss: 0.362
[84,     1] loss: 0.337
[85,     1] loss: 0.307
[86,     1] loss: 0.278
[87,     1] loss: 0.251
[88,     1] loss: 0.225
[89,     1] loss: 0.193
[90,     1] loss: 0.164
[91,     1] loss: 0.141
[92,     1] loss: 0.126
[93,     1] loss: 0.114
[94,     1] loss: 0.102
[95,     1] loss: 0.093
[96,     1] loss: 0.086
[97,     1] loss: 0.080
[98,     1] loss: 0.077
[99,     1] loss: 0.075
[100,     1] loss: 0.075
[101,     1] loss: 0.074
[102,     1] loss: 0.073
[103,     1] loss: 0.073
[104,     1] loss: 0.073
[105,     1] loss: 0.077
[106,     1] loss: 0.077
[107,     1] loss: 0.078
[108,     1] loss: 0.078
[109,     1] loss: 0.080
[110,     1] loss: 0.080
[111,     1] loss: 0.079
[112,     1] loss: 0.079
[113,     1] loss: 0.079
[114,     1] loss: 0.077
[115,     1] loss: 0.076
[116,     1] loss: 0.077
[117,     1] loss: 0.078
[118,     1] loss: 0.076
[119,     1] loss: 0.075
[120,     1] loss: 0.076
[121,     1] loss: 0.075
[122,     1] loss: 0.076
[123,     1] loss: 0.075
[124,     1] loss: 0.076
[125,     1] loss: 0.077
[126,     1] loss: 0.077
[127,     1] loss: 0.077
[128,     1] loss: 0.117
[129,     1] loss: 0.293
[130,     1] loss: 1.005
[131,     1] loss: 0.739
[132,     1] loss: 0.590
[133,     1] loss: 0.634
[134,     1] loss: 0.625
[135,     1] loss: 0.600
[136,     1] loss: 0.587
[137,     1] loss: 0.600
[138,     1] loss: 0.602
[139,     1] loss: 0.595
[140,     1] loss: 0.588
[141,     1] loss: 0.580
[142,     1] loss: 0.574
[143,     1] loss: 0.568
[144,     1] loss: 0.561
Early stopping applied (best metric=0.23684854805469513)
Finished Training
Total time taken: 533.8955230712891
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.661
[5,     1] loss: 0.640
[6,     1] loss: 0.614
[7,     1] loss: 0.583
[8,     1] loss: 0.551
[9,     1] loss: 0.516
[10,     1] loss: 0.474
[11,     1] loss: 0.439
[12,     1] loss: 0.411
[13,     1] loss: 0.388
[14,     1] loss: 0.375
[15,     1] loss: 0.363
[16,     1] loss: 0.356
[17,     1] loss: 0.350
[18,     1] loss: 0.347
[19,     1] loss: 0.342
[20,     1] loss: 0.338
[21,     1] loss: 0.337
[22,     1] loss: 0.332
[23,     1] loss: 0.333
[24,     1] loss: 0.325
[25,     1] loss: 0.324
[26,     1] loss: 0.319
[27,     1] loss: 0.311
[28,     1] loss: 0.312
[29,     1] loss: 0.309
[30,     1] loss: 0.305
[31,     1] loss: 0.302
[32,     1] loss: 0.273
[33,     1] loss: 0.271
[34,     1] loss: 0.258
[35,     1] loss: 0.238
[36,     1] loss: 0.221
[37,     1] loss: 0.202
[38,     1] loss: 0.184
[39,     1] loss: 0.169
[40,     1] loss: 0.153
[41,     1] loss: 0.139
[42,     1] loss: 0.124
[43,     1] loss: 0.112
[44,     1] loss: 0.105
[45,     1] loss: 0.094
[46,     1] loss: 0.090
[47,     1] loss: 0.083
[48,     1] loss: 0.078
[49,     1] loss: 0.075
[50,     1] loss: 0.075
[51,     1] loss: 0.067
[52,     1] loss: 0.067
[53,     1] loss: 0.065
[54,     1] loss: 0.063
[55,     1] loss: 0.064
[56,     1] loss: 0.064
[57,     1] loss: 0.066
[58,     1] loss: 0.066
[59,     1] loss: 0.065
[60,     1] loss: 0.066
[61,     1] loss: 0.066
[62,     1] loss: 0.066
[63,     1] loss: 0.065
[64,     1] loss: 0.065
[65,     1] loss: 0.064
[66,     1] loss: 0.066
[67,     1] loss: 0.067
[68,     1] loss: 0.068
[69,     1] loss: 0.067
[70,     1] loss: 0.068
[71,     1] loss: 0.069
[72,     1] loss: 0.069
[73,     1] loss: 0.067
[74,     1] loss: 0.069
[75,     1] loss: 0.068
[76,     1] loss: 0.068
[77,     1] loss: 0.070
[78,     1] loss: 0.070
[79,     1] loss: 0.071
[80,     1] loss: 0.076
[81,     1] loss: 0.073
[82,     1] loss: 0.074
[83,     1] loss: 0.074
[84,     1] loss: 0.089
[85,     1] loss: 0.131
[86,     1] loss: 0.472
[87,     1] loss: 0.365
[88,     1] loss: 0.547
[89,     1] loss: 0.400
[90,     1] loss: 0.435
[91,     1] loss: 0.432
[92,     1] loss: 0.408
[93,     1] loss: 0.385
[94,     1] loss: 0.373
[95,     1] loss: 0.358
[96,     1] loss: 0.348
[97,     1] loss: 0.324
[98,     1] loss: 0.301
[99,     1] loss: 0.272
[100,     1] loss: 0.248
[101,     1] loss: 0.229
[102,     1] loss: 0.214
[103,     1] loss: 0.219
Early stopping applied (best metric=0.26522237062454224)
Finished Training
Total time taken: 383.3082094192505
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8812903225806451, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8666666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8839694656488549, 'Pyrrolidone carboxylic acid Validation Precision': 0.5812790056462996, 'Pyrrolidone carboxylic acid AUC ROC': 0.9353530534351145, 'Pyrrolidone carboxylic acid AUC PR': 0.8189523276376561, 'Pyrrolidone carboxylic acid MCC': 0.644856580005042, 'Pyrrolidone carboxylic acid F1': 0.6945877758309764, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.24937165677547454, 'Validation Loss (total)': 0.24937165677547454}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009532951185670194,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3149014932,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.69077126290869}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.682
[3,     1] loss: 0.657
[4,     1] loss: 0.635
[5,     1] loss: 0.612
[6,     1] loss: 0.590
[7,     1] loss: 0.568
[8,     1] loss: 0.546
[9,     1] loss: 0.526
[10,     1] loss: 0.509
[11,     1] loss: 0.489
[12,     1] loss: 0.472
[13,     1] loss: 0.458
[14,     1] loss: 0.443
[15,     1] loss: 0.432
[16,     1] loss: 0.418
[17,     1] loss: 0.406
[18,     1] loss: 0.395
[19,     1] loss: 0.384
[20,     1] loss: 0.375
[21,     1] loss: 0.366
[22,     1] loss: 0.353
[23,     1] loss: 0.343
[24,     1] loss: 0.340
[25,     1] loss: 0.468
[26,     1] loss: 0.431
[27,     1] loss: 0.475
[28,     1] loss: 0.459
[29,     1] loss: 0.457
[30,     1] loss: 0.444
[31,     1] loss: 0.430
[32,     1] loss: 0.413
[33,     1] loss: 0.398
[34,     1] loss: 0.384
[35,     1] loss: 0.366
[36,     1] loss: 0.349
[37,     1] loss: 0.333
[38,     1] loss: 0.318
[39,     1] loss: 0.303
[40,     1] loss: 0.290
[41,     1] loss: 0.276
[42,     1] loss: 0.262
[43,     1] loss: 0.248
[44,     1] loss: 0.240
[45,     1] loss: 0.226
[46,     1] loss: 0.220
[47,     1] loss: 0.219
[48,     1] loss: 0.376
[49,     1] loss: 0.336
[50,     1] loss: 0.465
[51,     1] loss: 0.388
[52,     1] loss: 0.399
[53,     1] loss: 0.374
[54,     1] loss: 0.370
[55,     1] loss: 0.360
[56,     1] loss: 0.343
[57,     1] loss: 0.328
[58,     1] loss: 0.311
[59,     1] loss: 0.297
[60,     1] loss: 0.283
[61,     1] loss: 0.265
[62,     1] loss: 0.253
[63,     1] loss: 0.240
[64,     1] loss: 0.229
[65,     1] loss: 0.219
[66,     1] loss: 0.210
[67,     1] loss: 0.203
[68,     1] loss: 0.197
[69,     1] loss: 0.192
[70,     1] loss: 0.192
[71,     1] loss: 0.189
[72,     1] loss: 0.187
[73,     1] loss: 0.188
[74,     1] loss: 0.188
[75,     1] loss: 0.199
[76,     1] loss: 0.307
[77,     1] loss: 0.676
[78,     1] loss: 0.521
[79,     1] loss: 0.530
[80,     1] loss: 0.511
[81,     1] loss: 0.502
[82,     1] loss: 0.505
[83,     1] loss: 0.499
[84,     1] loss: 0.490
[85,     1] loss: 0.478
[86,     1] loss: 0.469
[87,     1] loss: 0.458
[88,     1] loss: 0.447
[89,     1] loss: 0.436
[90,     1] loss: 0.423
[91,     1] loss: 0.416
[92,     1] loss: 0.403
[93,     1] loss: 0.389
[94,     1] loss: 0.375
[95,     1] loss: 0.366
[96,     1] loss: 0.352
[97,     1] loss: 0.344
[98,     1] loss: 0.347
[99,     1] loss: 0.342
[100,     1] loss: 0.328
[101,     1] loss: 0.318
[102,     1] loss: 0.382
[103,     1] loss: 0.635
[104,     1] loss: 0.473
[105,     1] loss: 0.450
[106,     1] loss: 0.451
[107,     1] loss: 0.435
[108,     1] loss: 0.431
Early stopping applied (best metric=0.3037804961204529)
Finished Training
Total time taken: 402.7303023338318
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.681
[3,     1] loss: 0.636
[4,     1] loss: 0.599
[5,     1] loss: 0.565
[6,     1] loss: 0.536
[7,     1] loss: 0.512
[8,     1] loss: 0.492
[9,     1] loss: 0.472
[10,     1] loss: 0.456
[11,     1] loss: 0.443
[12,     1] loss: 0.433
[13,     1] loss: 0.424
[14,     1] loss: 0.415
[15,     1] loss: 0.411
[16,     1] loss: 0.400
[17,     1] loss: 0.394
[18,     1] loss: 0.399
[19,     1] loss: 0.398
[20,     1] loss: 0.410
[21,     1] loss: 0.409
[22,     1] loss: 0.392
[23,     1] loss: 0.381
[24,     1] loss: 0.367
[25,     1] loss: 0.356
[26,     1] loss: 0.345
[27,     1] loss: 0.333
[28,     1] loss: 0.323
[29,     1] loss: 0.311
[30,     1] loss: 0.298
[31,     1] loss: 0.288
[32,     1] loss: 0.276
[33,     1] loss: 0.270
[34,     1] loss: 0.314
[35,     1] loss: 0.489
[36,     1] loss: 0.436
[37,     1] loss: 0.445
[38,     1] loss: 0.427
[39,     1] loss: 0.411
[40,     1] loss: 0.394
[41,     1] loss: 0.379
[42,     1] loss: 0.356
[43,     1] loss: 0.335
[44,     1] loss: 0.315
[45,     1] loss: 0.292
[46,     1] loss: 0.271
[47,     1] loss: 0.250
[48,     1] loss: 0.232
[49,     1] loss: 0.218
[50,     1] loss: 0.202
[51,     1] loss: 0.188
[52,     1] loss: 0.181
[53,     1] loss: 0.174
[54,     1] loss: 0.166
[55,     1] loss: 0.163
[56,     1] loss: 0.159
[57,     1] loss: 0.157
[58,     1] loss: 0.159
[59,     1] loss: 0.158
[60,     1] loss: 0.162
[61,     1] loss: 0.185
[62,     1] loss: 0.437
[63,     1] loss: 0.595
[64,     1] loss: 0.567
[65,     1] loss: 0.566
[66,     1] loss: 0.567
[67,     1] loss: 0.563
[68,     1] loss: 0.559
[69,     1] loss: 0.550
[70,     1] loss: 0.537
[71,     1] loss: 0.532
[72,     1] loss: 0.519
[73,     1] loss: 0.505
[74,     1] loss: 0.490
[75,     1] loss: 0.476
[76,     1] loss: 0.457
[77,     1] loss: 0.442
[78,     1] loss: 0.424
[79,     1] loss: 0.410
[80,     1] loss: 0.401
[81,     1] loss: 0.440
[82,     1] loss: 0.390
[83,     1] loss: 0.376
[84,     1] loss: 0.359
[85,     1] loss: 0.344
[86,     1] loss: 0.330
[87,     1] loss: 0.315
[88,     1] loss: 0.302
[89,     1] loss: 0.286
[90,     1] loss: 0.269
[91,     1] loss: 0.258
[92,     1] loss: 0.244
[93,     1] loss: 0.235
[94,     1] loss: 0.224
[95,     1] loss: 0.221
[96,     1] loss: 0.280
[97,     1] loss: 0.452
[98,     1] loss: 0.474
[99,     1] loss: 0.449
[100,     1] loss: 0.434
[101,     1] loss: 0.418
[102,     1] loss: 0.411
[103,     1] loss: 0.407
[104,     1] loss: 0.401
[105,     1] loss: 0.395
[106,     1] loss: 0.392
[107,     1] loss: 0.386
[108,     1] loss: 0.381
[109,     1] loss: 0.373
Early stopping applied (best metric=0.28920015692710876)
Finished Training
Total time taken: 406.67937874794006
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.694
[3,     1] loss: 0.681
[4,     1] loss: 0.672
[5,     1] loss: 0.660
[6,     1] loss: 0.646
[7,     1] loss: 0.631
[8,     1] loss: 0.616
[9,     1] loss: 0.600
[10,     1] loss: 0.583
[11,     1] loss: 0.565
[12,     1] loss: 0.549
[13,     1] loss: 0.530
[14,     1] loss: 0.511
[15,     1] loss: 0.494
[16,     1] loss: 0.472
[17,     1] loss: 0.455
[18,     1] loss: 0.436
[19,     1] loss: 0.415
[20,     1] loss: 0.410
[21,     1] loss: 0.478
[22,     1] loss: 0.425
[23,     1] loss: 0.426
[24,     1] loss: 0.417
[25,     1] loss: 0.400
[26,     1] loss: 0.379
[27,     1] loss: 0.356
[28,     1] loss: 0.337
[29,     1] loss: 0.312
[30,     1] loss: 0.292
[31,     1] loss: 0.272
[32,     1] loss: 0.254
[33,     1] loss: 0.242
[34,     1] loss: 0.229
[35,     1] loss: 0.216
[36,     1] loss: 0.208
[37,     1] loss: 0.197
[38,     1] loss: 0.193
[39,     1] loss: 0.188
[40,     1] loss: 0.188
[41,     1] loss: 0.186
[42,     1] loss: 0.219
[43,     1] loss: 0.385
[44,     1] loss: 0.627
[45,     1] loss: 0.457
[46,     1] loss: 0.516
[47,     1] loss: 0.546
[48,     1] loss: 0.556
[49,     1] loss: 0.559
[50,     1] loss: 0.563
[51,     1] loss: 0.566
[52,     1] loss: 0.568
[53,     1] loss: 0.570
[54,     1] loss: 0.570
[55,     1] loss: 0.570
[56,     1] loss: 0.571
[57,     1] loss: 0.571
[58,     1] loss: 0.568
[59,     1] loss: 0.567
[60,     1] loss: 0.562
[61,     1] loss: 0.558
[62,     1] loss: 0.552
[63,     1] loss: 0.545
[64,     1] loss: 0.535
[65,     1] loss: 0.525
[66,     1] loss: 0.517
[67,     1] loss: 0.504
[68,     1] loss: 0.492
[69,     1] loss: 0.480
[70,     1] loss: 0.467
[71,     1] loss: 0.454
[72,     1] loss: 0.461
[73,     1] loss: 0.599
[74,     1] loss: 0.529
[75,     1] loss: 0.523
[76,     1] loss: 0.505
[77,     1] loss: 0.497
[78,     1] loss: 0.490
[79,     1] loss: 0.485
[80,     1] loss: 0.476
[81,     1] loss: 0.469
[82,     1] loss: 0.460
[83,     1] loss: 0.450
[84,     1] loss: 0.441
Early stopping applied (best metric=0.3042677938938141)
Finished Training
Total time taken: 314.4826397895813
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.693
[3,     1] loss: 0.672
[4,     1] loss: 0.653
[5,     1] loss: 0.635
[6,     1] loss: 0.614
[7,     1] loss: 0.594
[8,     1] loss: 0.575
[9,     1] loss: 0.558
[10,     1] loss: 0.539
[11,     1] loss: 0.522
[12,     1] loss: 0.504
[13,     1] loss: 0.486
[14,     1] loss: 0.466
[15,     1] loss: 0.446
[16,     1] loss: 0.425
[17,     1] loss: 0.403
[18,     1] loss: 0.382
[19,     1] loss: 0.363
[20,     1] loss: 0.352
[21,     1] loss: 0.405
[22,     1] loss: 0.430
[23,     1] loss: 0.360
[24,     1] loss: 0.389
[25,     1] loss: 0.373
[26,     1] loss: 0.358
[27,     1] loss: 0.340
[28,     1] loss: 0.322
[29,     1] loss: 0.307
[30,     1] loss: 0.290
[31,     1] loss: 0.275
[32,     1] loss: 0.261
[33,     1] loss: 0.251
[34,     1] loss: 0.241
[35,     1] loss: 0.233
[36,     1] loss: 0.228
[37,     1] loss: 0.220
[38,     1] loss: 0.220
[39,     1] loss: 0.240
[40,     1] loss: 0.253
[41,     1] loss: 0.346
[42,     1] loss: 0.546
[43,     1] loss: 0.511
[44,     1] loss: 0.470
[45,     1] loss: 0.469
[46,     1] loss: 0.469
[47,     1] loss: 0.470
[48,     1] loss: 0.472
[49,     1] loss: 0.467
[50,     1] loss: 0.465
[51,     1] loss: 0.459
[52,     1] loss: 0.450
[53,     1] loss: 0.443
[54,     1] loss: 0.434
[55,     1] loss: 0.425
[56,     1] loss: 0.417
[57,     1] loss: 0.409
[58,     1] loss: 0.400
[59,     1] loss: 0.394
[60,     1] loss: 0.389
[61,     1] loss: 0.383
[62,     1] loss: 0.375
[63,     1] loss: 0.373
[64,     1] loss: 0.370
[65,     1] loss: 0.371
[66,     1] loss: 0.370
[67,     1] loss: 0.370
[68,     1] loss: 0.386
[69,     1] loss: 0.473
[70,     1] loss: 0.582
[71,     1] loss: 0.597
[72,     1] loss: 0.597
[73,     1] loss: 0.592
[74,     1] loss: 0.589
[75,     1] loss: 0.590
[76,     1] loss: 0.590
[77,     1] loss: 0.588
[78,     1] loss: 0.587
[79,     1] loss: 0.587
[80,     1] loss: 0.584
[81,     1] loss: 0.580
Early stopping applied (best metric=0.32390767335891724)
Finished Training
Total time taken: 305.04525423049927
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.666
[5,     1] loss: 0.650
[6,     1] loss: 0.633
[7,     1] loss: 0.615
[8,     1] loss: 0.595
[9,     1] loss: 0.575
[10,     1] loss: 0.553
[11,     1] loss: 0.533
[12,     1] loss: 0.514
[13,     1] loss: 0.497
[14,     1] loss: 0.482
[15,     1] loss: 0.468
[16,     1] loss: 0.455
[17,     1] loss: 0.460
[18,     1] loss: 0.435
[19,     1] loss: 0.422
[20,     1] loss: 0.405
[21,     1] loss: 0.388
[22,     1] loss: 0.370
[23,     1] loss: 0.353
[24,     1] loss: 0.338
[25,     1] loss: 0.320
[26,     1] loss: 0.300
[27,     1] loss: 0.283
[28,     1] loss: 0.267
[29,     1] loss: 0.260
[30,     1] loss: 0.275
[31,     1] loss: 0.515
[32,     1] loss: 0.430
[33,     1] loss: 0.429
[34,     1] loss: 0.413
[35,     1] loss: 0.424
[36,     1] loss: 0.419
[37,     1] loss: 0.407
[38,     1] loss: 0.393
[39,     1] loss: 0.378
[40,     1] loss: 0.361
[41,     1] loss: 0.345
[42,     1] loss: 0.329
[43,     1] loss: 0.311
[44,     1] loss: 0.299
[45,     1] loss: 0.282
[46,     1] loss: 0.269
[47,     1] loss: 0.253
[48,     1] loss: 0.245
[49,     1] loss: 0.237
[50,     1] loss: 0.227
[51,     1] loss: 0.219
[52,     1] loss: 0.212
[53,     1] loss: 0.210
[54,     1] loss: 0.210
[55,     1] loss: 0.317
[56,     1] loss: 0.420
[57,     1] loss: 0.467
[58,     1] loss: 0.413
[59,     1] loss: 0.431
[60,     1] loss: 0.426
[61,     1] loss: 0.407
[62,     1] loss: 0.390
[63,     1] loss: 0.377
[64,     1] loss: 0.366
[65,     1] loss: 0.349
[66,     1] loss: 0.341
[67,     1] loss: 0.331
[68,     1] loss: 0.322
[69,     1] loss: 0.314
[70,     1] loss: 0.306
[71,     1] loss: 0.298
[72,     1] loss: 0.292
[73,     1] loss: 0.287
[74,     1] loss: 0.279
[75,     1] loss: 0.274
[76,     1] loss: 0.268
[77,     1] loss: 0.264
[78,     1] loss: 0.264
[79,     1] loss: 0.257
[80,     1] loss: 0.255
[81,     1] loss: 0.257
[82,     1] loss: 0.272
[83,     1] loss: 0.593
[84,     1] loss: 0.671
[85,     1] loss: 0.610
[86,     1] loss: 0.580
[87,     1] loss: 0.571
[88,     1] loss: 0.569
[89,     1] loss: 0.565
[90,     1] loss: 0.571
[91,     1] loss: 0.576
[92,     1] loss: 0.575
[93,     1] loss: 0.575
[94,     1] loss: 0.574
[95,     1] loss: 0.571
[96,     1] loss: 0.570
[97,     1] loss: 0.565
[98,     1] loss: 0.560
[99,     1] loss: 0.554
[100,     1] loss: 0.547
[101,     1] loss: 0.539
[102,     1] loss: 0.533
[103,     1] loss: 0.551
[104,     1] loss: 0.593
[105,     1] loss: 0.555
[106,     1] loss: 0.561
[107,     1] loss: 0.556
[108,     1] loss: 0.546
[109,     1] loss: 0.539
[110,     1] loss: 0.531
[111,     1] loss: 0.521
[112,     1] loss: 0.515
[113,     1] loss: 0.505
[114,     1] loss: 0.498
Early stopping applied (best metric=0.31782054901123047)
Finished Training
Total time taken: 426.95975160598755
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8716129032258065, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8208333333333333, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8809160305343511, 'Pyrrolidone carboxylic acid Validation Precision': 0.5742411385961438, 'Pyrrolidone carboxylic acid AUC ROC': 0.9373409669211196, 'Pyrrolidone carboxylic acid AUC PR': 0.8320506419942032, 'Pyrrolidone carboxylic acid MCC': 0.6132870137120676, 'Pyrrolidone carboxylic acid F1': 0.667999836420889, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.3077953338623047, 'Validation Loss (total)': 0.3077953338623047}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007819508993961912,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 449739105,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.9632845201886462}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.650
[4,     1] loss: 0.591
[5,     1] loss: 0.511
[6,     1] loss: 0.420
[7,     1] loss: 0.325
[8,     1] loss: 0.229
[9,     1] loss: 0.149
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004989041647083908,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3182234869,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.685664796950913}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008599850129145598,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2339343791,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.4509078763323755}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.609
[4,     1] loss: 0.527
[5,     1] loss: 0.470
[6,     1] loss: 0.414
[7,     1] loss: 0.371
[8,     1] loss: 0.334
[9,     1] loss: 0.296
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008368286222820655,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 212240665,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.7754210278341658}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.635
[4,     1] loss: 0.564
[5,     1] loss: 0.483
[6,     1] loss: 0.394
[7,     1] loss: 0.297
[8,     1] loss: 0.195
[9,     1] loss: 0.109
[10,     1] loss: 0.061
[11,     1] loss: 0.035
[12,     1] loss: 0.027
[13,     1] loss: 0.018
[14,     1] loss: 0.015
[15,     1] loss: 0.009
[16,     1] loss: 0.016
[17,     1] loss: 0.117
[18,     1] loss: 0.095
[19,     1] loss: 0.033
[20,     1] loss: 0.018
[21,     1] loss: 0.006
[22,     1] loss: 0.017
[23,     1] loss: 0.013
[24,     1] loss: 0.010
[25,     1] loss: 0.007
[26,     1] loss: 0.008
[27,     1] loss: 0.009
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007434124048210968,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3487738439,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.948630738867607}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.635
[4,     1] loss: 0.603
[5,     1] loss: 0.572
[6,     1] loss: 0.544
[7,     1] loss: 0.518
[8,     1] loss: 0.496
[9,     1] loss: 0.478
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007280939611209034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3019284145,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.348851897804842}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.684
[3,     1] loss: 0.654
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004773583231536544,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4269177737,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.738969131774022}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.681
[3,     1] loss: 0.643
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007796748336233334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 983166930,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.554107852190091}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.688
[3,     1] loss: 0.640
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00983683017002053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2421956812,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.627046826571174}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.685
[3,     1] loss: 0.644
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011647346809500205,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3409844838,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8605970786086914}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.673
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009551480648717785,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2507340381,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.951741431956462}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.668
[3,     1] loss: 0.641
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00048030716909161004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 957610816,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.613997064024137}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.685
[3,     1] loss: 0.666
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0057962016099820666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3494169576,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.81885320611847}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.667
[4,     1] loss: 0.651
[5,     1] loss: 0.635
[6,     1] loss: 0.619
[7,     1] loss: 0.602
[8,     1] loss: 0.584
[9,     1] loss: 0.567
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005312828360587083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1658649965,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.660740134734109}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.679
[3,     1] loss: 0.639
[4,     1] loss: 0.591
[5,     1] loss: 0.543
[6,     1] loss: 0.489
[7,     1] loss: 0.428
[8,     1] loss: 0.372
[9,     1] loss: 0.313
[10,     1] loss: 0.262
[11,     1] loss: 0.217
[12,     1] loss: 0.169
[13,     1] loss: 0.128
[14,     1] loss: 0.090
[15,     1] loss: 0.065
[16,     1] loss: 0.048
[17,     1] loss: 0.037
[18,     1] loss: 0.027
[19,     1] loss: 0.020
[20,     1] loss: 0.015
[21,     1] loss: 0.011
[22,     1] loss: 0.009
[23,     1] loss: 0.008
[24,     1] loss: 0.008
[25,     1] loss: 0.007
[26,     1] loss: 0.007
[27,     1] loss: 0.007
[28,     1] loss: 0.008
[29,     1] loss: 0.008
[30,     1] loss: 0.009
[31,     1] loss: 0.009
[32,     1] loss: 0.010
[33,     1] loss: 0.011
[34,     1] loss: 0.012
[35,     1] loss: 0.012
[36,     1] loss: 0.013
[37,     1] loss: 0.013
[38,     1] loss: 0.013
[39,     1] loss: 0.013
[40,     1] loss: 0.012
[41,     1] loss: 0.012
[42,     1] loss: 0.012
[43,     1] loss: 0.011
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.009
[48,     1] loss: 0.008
[49,     1] loss: 0.008
[50,     1] loss: 0.008
[51,     1] loss: 0.007
[52,     1] loss: 0.007
[53,     1] loss: 0.007
[54,     1] loss: 0.007
[55,     1] loss: 0.007
[56,     1] loss: 0.007
[57,     1] loss: 0.007
[58,     1] loss: 0.008
[59,     1] loss: 0.008
[60,     1] loss: 0.007
[61,     1] loss: 0.008
[62,     1] loss: 0.008
Early stopping applied (best metric=0.28272029757499695)
Finished Training
Total time taken: 235.6808421611786
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.666
[3,     1] loss: 0.602
[4,     1] loss: 0.536
[5,     1] loss: 0.473
[6,     1] loss: 0.414
[7,     1] loss: 0.362
[8,     1] loss: 0.317
[9,     1] loss: 0.284
[10,     1] loss: 0.267
[11,     1] loss: 0.250
[12,     1] loss: 0.235
[13,     1] loss: 0.220
[14,     1] loss: 0.201
[15,     1] loss: 0.182
[16,     1] loss: 0.163
[17,     1] loss: 0.139
[18,     1] loss: 0.117
[19,     1] loss: 0.097
[20,     1] loss: 0.081
[21,     1] loss: 0.062
[22,     1] loss: 0.050
[23,     1] loss: 0.039
[24,     1] loss: 0.029
[25,     1] loss: 0.022
[26,     1] loss: 0.019
[27,     1] loss: 0.015
[28,     1] loss: 0.014
[29,     1] loss: 0.012
[30,     1] loss: 0.010
[31,     1] loss: 0.010
[32,     1] loss: 0.009
[33,     1] loss: 0.009
[34,     1] loss: 0.009
[35,     1] loss: 0.008
[36,     1] loss: 0.008
[37,     1] loss: 0.008
[38,     1] loss: 0.009
[39,     1] loss: 0.009
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.012
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.013
[48,     1] loss: 0.012
[49,     1] loss: 0.012
[50,     1] loss: 0.012
[51,     1] loss: 0.012
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.011
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.009
Early stopping applied (best metric=0.35661566257476807)
Finished Training
Total time taken: 220.80760836601257
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.641
[4,     1] loss: 0.596
[5,     1] loss: 0.546
[6,     1] loss: 0.491
[7,     1] loss: 0.433
[8,     1] loss: 0.371
[9,     1] loss: 0.311
[10,     1] loss: 0.261
[11,     1] loss: 0.205
[12,     1] loss: 0.160
[13,     1] loss: 0.116
[14,     1] loss: 0.084
[15,     1] loss: 0.058
[16,     1] loss: 0.039
[17,     1] loss: 0.025
[18,     1] loss: 0.019
[19,     1] loss: 0.015
[20,     1] loss: 0.012
[21,     1] loss: 0.010
[22,     1] loss: 0.009
[23,     1] loss: 0.008
[24,     1] loss: 0.007
[25,     1] loss: 0.008
[26,     1] loss: 0.008
[27,     1] loss: 0.008
[28,     1] loss: 0.009
[29,     1] loss: 0.009
[30,     1] loss: 0.010
[31,     1] loss: 0.010
[32,     1] loss: 0.012
[33,     1] loss: 0.013
[34,     1] loss: 0.014
[35,     1] loss: 0.014
[36,     1] loss: 0.015
[37,     1] loss: 0.015
[38,     1] loss: 0.016
[39,     1] loss: 0.015
[40,     1] loss: 0.015
[41,     1] loss: 0.015
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.012
[45,     1] loss: 0.012
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.008
[53,     1] loss: 0.008
[54,     1] loss: 0.008
[55,     1] loss: 0.009
[56,     1] loss: 0.008
[57,     1] loss: 0.009
[58,     1] loss: 0.008
[59,     1] loss: 0.009
[60,     1] loss: 0.008
Early stopping applied (best metric=0.31303322315216064)
Finished Training
Total time taken: 228.93819904327393
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.645
[4,     1] loss: 0.607
[5,     1] loss: 0.559
[6,     1] loss: 0.510
[7,     1] loss: 0.459
[8,     1] loss: 0.408
[9,     1] loss: 0.356
[10,     1] loss: 0.304
[11,     1] loss: 0.267
[12,     1] loss: 0.232
[13,     1] loss: 0.201
[14,     1] loss: 0.172
[15,     1] loss: 0.145
[16,     1] loss: 0.121
[17,     1] loss: 0.095
[18,     1] loss: 0.074
[19,     1] loss: 0.057
[20,     1] loss: 0.042
[21,     1] loss: 0.031
[22,     1] loss: 0.022
[23,     1] loss: 0.017
[24,     1] loss: 0.013
[25,     1] loss: 0.011
[26,     1] loss: 0.010
[27,     1] loss: 0.008
[28,     1] loss: 0.008
[29,     1] loss: 0.008
[30,     1] loss: 0.008
[31,     1] loss: 0.008
[32,     1] loss: 0.008
[33,     1] loss: 0.008
[34,     1] loss: 0.008
[35,     1] loss: 0.008
[36,     1] loss: 0.009
[37,     1] loss: 0.009
[38,     1] loss: 0.009
[39,     1] loss: 0.010
[40,     1] loss: 0.010
[41,     1] loss: 0.010
[42,     1] loss: 0.011
[43,     1] loss: 0.011
[44,     1] loss: 0.011
[45,     1] loss: 0.011
[46,     1] loss: 0.011
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.008
[55,     1] loss: 0.008
[56,     1] loss: 0.008
[57,     1] loss: 0.008
[58,     1] loss: 0.008
[59,     1] loss: 0.007
[60,     1] loss: 0.008
[61,     1] loss: 0.007
[62,     1] loss: 0.008
[63,     1] loss: 0.008
[64,     1] loss: 0.008
[65,     1] loss: 0.007
[66,     1] loss: 0.007
[67,     1] loss: 0.008
[68,     1] loss: 0.007
[69,     1] loss: 0.008
[70,     1] loss: 0.007
[71,     1] loss: 0.007
[72,     1] loss: 0.007
[73,     1] loss: 0.007
[74,     1] loss: 0.007
[75,     1] loss: 0.007
[76,     1] loss: 0.007
[77,     1] loss: 0.007
[78,     1] loss: 0.008
[79,     1] loss: 0.007
[80,     1] loss: 0.007
[81,     1] loss: 0.007
[82,     1] loss: 0.007
[83,     1] loss: 0.007
[84,     1] loss: 0.007
[85,     1] loss: 0.007
[86,     1] loss: 0.007
[87,     1] loss: 0.006
[88,     1] loss: 0.007
[89,     1] loss: 0.007
[90,     1] loss: 0.007
[91,     1] loss: 0.007
[92,     1] loss: 0.007
[93,     1] loss: 0.006
[94,     1] loss: 0.007
[95,     1] loss: 0.007
[96,     1] loss: 0.007
[97,     1] loss: 0.006
[98,     1] loss: 0.007
[99,     1] loss: 0.007
[100,     1] loss: 0.007
[101,     1] loss: 0.007
[102,     1] loss: 0.007
[103,     1] loss: 0.007
[104,     1] loss: 0.011
[105,     1] loss: 0.480
[106,     1] loss: 2.254
[107,     1] loss: 0.911
[108,     1] loss: 0.510
[109,     1] loss: 0.590
[110,     1] loss: 0.601
[111,     1] loss: 0.560
[112,     1] loss: 0.557
[113,     1] loss: 0.561
[114,     1] loss: 0.558
[115,     1] loss: 0.554
[116,     1] loss: 0.546
[117,     1] loss: 0.536
[118,     1] loss: 0.532
[119,     1] loss: 0.526
[120,     1] loss: 0.518
[121,     1] loss: 0.509
[122,     1] loss: 0.504
[123,     1] loss: 0.498
[124,     1] loss: 0.493
[125,     1] loss: 0.489
[126,     1] loss: 0.482
[127,     1] loss: 0.481
[128,     1] loss: 0.482
[129,     1] loss: 0.481
[130,     1] loss: 0.477
Early stopping applied (best metric=0.33807283639907837)
Finished Training
Total time taken: 492.16202998161316
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.674
[3,     1] loss: 0.632
[4,     1] loss: 0.587
[5,     1] loss: 0.533
[6,     1] loss: 0.472
[7,     1] loss: 0.409
[8,     1] loss: 0.345
[9,     1] loss: 0.279
[10,     1] loss: 0.217
[11,     1] loss: 0.160
[12,     1] loss: 0.125
[13,     1] loss: 0.095
[14,     1] loss: 0.071
[15,     1] loss: 0.057
[16,     1] loss: 0.046
[17,     1] loss: 0.035
[18,     1] loss: 0.025
[19,     1] loss: 0.020
[20,     1] loss: 0.016
[21,     1] loss: 0.013
[22,     1] loss: 0.012
[23,     1] loss: 0.011
[24,     1] loss: 0.011
[25,     1] loss: 0.011
[26,     1] loss: 0.011
[27,     1] loss: 0.011
[28,     1] loss: 0.012
[29,     1] loss: 0.012
[30,     1] loss: 0.013
[31,     1] loss: 0.014
[32,     1] loss: 0.014
[33,     1] loss: 0.015
[34,     1] loss: 0.016
[35,     1] loss: 0.016
[36,     1] loss: 0.016
[37,     1] loss: 0.016
[38,     1] loss: 0.016
[39,     1] loss: 0.016
[40,     1] loss: 0.015
[41,     1] loss: 0.014
[42,     1] loss: 0.014
[43,     1] loss: 0.013
[44,     1] loss: 0.013
[45,     1] loss: 0.012
[46,     1] loss: 0.012
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.010
[51,     1] loss: 0.009
[52,     1] loss: 0.010
[53,     1] loss: 0.009
[54,     1] loss: 0.010
[55,     1] loss: 0.009
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.009
[59,     1] loss: 0.009
[60,     1] loss: 0.009
Early stopping applied (best metric=0.27642104029655457)
Finished Training
Total time taken: 229.40482091903687
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.7825806451612903, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9, 'Pyrrolidone carboxylic acid Validation Specificity': 0.7610687022900763, 'Pyrrolidone carboxylic acid Validation Precision': 0.41772299272493096, 'Pyrrolidone carboxylic acid AUC ROC': 0.9181615776081425, 'Pyrrolidone carboxylic acid AUC PR': 0.6831264756172518, 'Pyrrolidone carboxylic acid MCC': 0.5103465788929032, 'Pyrrolidone carboxylic acid F1': 0.5664762377495092, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.3133726119995117, 'Validation Loss (total)': 0.3133726119995117}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007204709770975675,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2086568247,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.67053627197753}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006358149354634796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2065104514,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.19910769823721}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.676
[3,     1] loss: 0.638
[4,     1] loss: 0.595
[5,     1] loss: 0.546
[6,     1] loss: 0.487
[7,     1] loss: 0.429
[8,     1] loss: 0.368
[9,     1] loss: 0.315
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009871528921895878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3038422798,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.696795015311512}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005134237829029625,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 657485917,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.219357670645982}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.694
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0001094764096095267,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2960371897,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.63107129114273}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.682
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006456450100038631,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1985150984,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.718401908098231}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.673
[3,     1] loss: 0.620
[4,     1] loss: 0.567
[5,     1] loss: 0.509
[6,     1] loss: 0.447
[7,     1] loss: 0.389
[8,     1] loss: 0.333
[9,     1] loss: 0.284
[10,     1] loss: 0.242
[11,     1] loss: 0.196
[12,     1] loss: 0.158
[13,     1] loss: 0.125
[14,     1] loss: 0.093
[15,     1] loss: 0.078
[16,     1] loss: 0.066
[17,     1] loss: 0.058
[18,     1] loss: 0.053
[19,     1] loss: 0.045
[20,     1] loss: 0.041
[21,     1] loss: 0.036
[22,     1] loss: 0.032
[23,     1] loss: 0.031
[24,     1] loss: 0.029
[25,     1] loss: 0.028
[26,     1] loss: 0.027
[27,     1] loss: 0.026
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009282413346580067,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4061575921,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.924777244803112}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.627
[4,     1] loss: 0.565
[5,     1] loss: 0.482
[6,     1] loss: 0.394
[7,     1] loss: 0.311
[8,     1] loss: 0.233
[9,     1] loss: 0.177
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024044302415948803,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3912444130,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.411041863782582}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007859417772818938,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2793362409,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.918147446274869}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.682
[3,     1] loss: 0.639
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004344876389824375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4014218503,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.186705768194596}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.687
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008962500802378055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2060028167,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.400970995272377}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.669
[3,     1] loss: 0.614
[4,     1] loss: 0.556
[5,     1] loss: 0.493
[6,     1] loss: 0.429
[7,     1] loss: 0.365
[8,     1] loss: 0.302
[9,     1] loss: 0.244
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009840799538861397,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2964259171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.324522803942946}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.627
[4,     1] loss: 0.557
[5,     1] loss: 0.485
[6,     1] loss: 0.433
[7,     1] loss: 0.390
[8,     1] loss: 0.355
[9,     1] loss: 0.320
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008436129730408146,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3204437290,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.188875404501502}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.690
[3,     1] loss: 0.664
[4,     1] loss: 0.637
[5,     1] loss: 0.608
[6,     1] loss: 0.573
[7,     1] loss: 0.540
[8,     1] loss: 0.503
[9,     1] loss: 0.464
[10,     1] loss: 0.426
[11,     1] loss: 0.389
[12,     1] loss: 0.350
[13,     1] loss: 0.317
[14,     1] loss: 0.286
[15,     1] loss: 0.256
[16,     1] loss: 0.233
[17,     1] loss: 0.206
[18,     1] loss: 0.187
[19,     1] loss: 0.165
[20,     1] loss: 0.148
[21,     1] loss: 0.134
[22,     1] loss: 0.120
[23,     1] loss: 0.109
[24,     1] loss: 0.103
[25,     1] loss: 0.096
[26,     1] loss: 0.093
[27,     1] loss: 0.085
[28,     1] loss: 0.082
[29,     1] loss: 0.081
[30,     1] loss: 0.080
[31,     1] loss: 0.080
[32,     1] loss: 0.077
[33,     1] loss: 0.076
[34,     1] loss: 0.076
[35,     1] loss: 0.077
[36,     1] loss: 0.075
[37,     1] loss: 0.076
[38,     1] loss: 0.077
[39,     1] loss: 0.076
[40,     1] loss: 0.078
[41,     1] loss: 0.107
[42,     1] loss: 0.226
[43,     1] loss: 0.726
[44,     1] loss: 0.632
[45,     1] loss: 0.465
[46,     1] loss: 0.519
[47,     1] loss: 0.559
[48,     1] loss: 0.584
[49,     1] loss: 0.595
[50,     1] loss: 0.602
[51,     1] loss: 0.604
[52,     1] loss: 0.610
[53,     1] loss: 0.612
[54,     1] loss: 0.614
[55,     1] loss: 0.613
[56,     1] loss: 0.613
[57,     1] loss: 0.612
[58,     1] loss: 0.611
[59,     1] loss: 0.610
[60,     1] loss: 0.606
[61,     1] loss: 0.603
[62,     1] loss: 0.600
[63,     1] loss: 0.595
[64,     1] loss: 0.587
[65,     1] loss: 0.577
[66,     1] loss: 0.569
[67,     1] loss: 0.556
[68,     1] loss: 0.541
[69,     1] loss: 0.521
[70,     1] loss: 0.500
[71,     1] loss: 0.476
[72,     1] loss: 0.449
[73,     1] loss: 0.422
[74,     1] loss: 0.398
[75,     1] loss: 0.376
[76,     1] loss: 0.355
[77,     1] loss: 0.339
[78,     1] loss: 0.325
[79,     1] loss: 0.313
[80,     1] loss: 0.303
[81,     1] loss: 0.293
Early stopping applied (best metric=0.24911028146743774)
Finished Training
Total time taken: 310.6069986820221
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.673
[4,     1] loss: 0.655
[5,     1] loss: 0.636
[6,     1] loss: 0.616
[7,     1] loss: 0.594
[8,     1] loss: 0.568
[9,     1] loss: 0.543
[10,     1] loss: 0.513
[11,     1] loss: 0.484
[12,     1] loss: 0.453
[13,     1] loss: 0.423
[14,     1] loss: 0.392
[15,     1] loss: 0.363
[16,     1] loss: 0.332
[17,     1] loss: 0.303
[18,     1] loss: 0.275
[19,     1] loss: 0.248
[20,     1] loss: 0.221
[21,     1] loss: 0.198
[22,     1] loss: 0.176
[23,     1] loss: 0.159
[24,     1] loss: 0.144
[25,     1] loss: 0.130
[26,     1] loss: 0.118
[27,     1] loss: 0.113
[28,     1] loss: 0.103
[29,     1] loss: 0.103
[30,     1] loss: 0.101
[31,     1] loss: 0.097
[32,     1] loss: 0.097
[33,     1] loss: 0.096
[34,     1] loss: 0.099
[35,     1] loss: 0.096
[36,     1] loss: 0.098
[37,     1] loss: 0.100
[38,     1] loss: 0.099
[39,     1] loss: 0.099
[40,     1] loss: 0.120
[41,     1] loss: 0.198
[42,     1] loss: 0.548
[43,     1] loss: 0.509
[44,     1] loss: 0.527
[45,     1] loss: 0.513
[46,     1] loss: 0.537
[47,     1] loss: 0.512
[48,     1] loss: 0.516
[49,     1] loss: 0.523
[50,     1] loss: 0.527
[51,     1] loss: 0.523
[52,     1] loss: 0.516
[53,     1] loss: 0.508
[54,     1] loss: 0.498
[55,     1] loss: 0.487
[56,     1] loss: 0.474
[57,     1] loss: 0.462
[58,     1] loss: 0.450
[59,     1] loss: 0.440
[60,     1] loss: 0.425
[61,     1] loss: 0.412
[62,     1] loss: 0.399
[63,     1] loss: 0.387
[64,     1] loss: 0.376
[65,     1] loss: 0.365
[66,     1] loss: 0.370
[67,     1] loss: 0.416
[68,     1] loss: 0.364
[69,     1] loss: 0.385
[70,     1] loss: 0.352
[71,     1] loss: 0.351
[72,     1] loss: 0.332
[73,     1] loss: 0.306
[74,     1] loss: 0.284
[75,     1] loss: 0.258
Early stopping applied (best metric=0.228875994682312)
Finished Training
Total time taken: 288.2069721221924
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.683
[3,     1] loss: 0.646
[4,     1] loss: 0.612
[5,     1] loss: 0.576
[6,     1] loss: 0.541
[7,     1] loss: 0.507
[8,     1] loss: 0.474
[9,     1] loss: 0.446
[10,     1] loss: 0.419
[11,     1] loss: 0.396
[12,     1] loss: 0.375
[13,     1] loss: 0.355
[14,     1] loss: 0.335
[15,     1] loss: 0.319
[16,     1] loss: 0.299
[17,     1] loss: 0.283
[18,     1] loss: 0.268
[19,     1] loss: 0.262
[20,     1] loss: 0.234
[21,     1] loss: 0.222
[22,     1] loss: 0.197
[23,     1] loss: 0.190
[24,     1] loss: 0.165
[25,     1] loss: 0.149
[26,     1] loss: 0.132
[27,     1] loss: 0.119
[28,     1] loss: 0.108
[29,     1] loss: 0.096
[30,     1] loss: 0.084
[31,     1] loss: 0.078
[32,     1] loss: 0.074
[33,     1] loss: 0.071
[34,     1] loss: 0.067
[35,     1] loss: 0.070
[36,     1] loss: 0.069
[37,     1] loss: 0.070
[38,     1] loss: 0.080
[39,     1] loss: 0.106
[40,     1] loss: 0.141
[41,     1] loss: 0.109
[42,     1] loss: 0.215
[43,     1] loss: 0.417
[44,     1] loss: 0.337
[45,     1] loss: 0.360
[46,     1] loss: 0.327
[47,     1] loss: 0.321
[48,     1] loss: 0.297
[49,     1] loss: 0.281
[50,     1] loss: 0.248
[51,     1] loss: 0.227
[52,     1] loss: 0.200
[53,     1] loss: 0.192
[54,     1] loss: 0.176
[55,     1] loss: 0.152
[56,     1] loss: 0.141
[57,     1] loss: 0.166
[58,     1] loss: 0.154
[59,     1] loss: 0.260
[60,     1] loss: 0.195
[61,     1] loss: 0.151
[62,     1] loss: 0.163
[63,     1] loss: 0.157
[64,     1] loss: 0.149
[65,     1] loss: 0.133
[66,     1] loss: 0.122
[67,     1] loss: 0.111
[68,     1] loss: 0.102
[69,     1] loss: 0.093
[70,     1] loss: 0.092
[71,     1] loss: 0.088
[72,     1] loss: 0.088
[73,     1] loss: 0.089
[74,     1] loss: 0.089
[75,     1] loss: 0.090
[76,     1] loss: 0.092
[77,     1] loss: 0.096
[78,     1] loss: 0.098
[79,     1] loss: 0.099
[80,     1] loss: 0.099
[81,     1] loss: 0.100
[82,     1] loss: 0.098
[83,     1] loss: 0.101
[84,     1] loss: 0.102
[85,     1] loss: 0.153
[86,     1] loss: 0.531
[87,     1] loss: 0.460
[88,     1] loss: 0.411
[89,     1] loss: 0.372
[90,     1] loss: 0.345
[91,     1] loss: 0.351
[92,     1] loss: 0.338
[93,     1] loss: 0.313
[94,     1] loss: 0.290
[95,     1] loss: 0.268
[96,     1] loss: 0.249
[97,     1] loss: 0.219
[98,     1] loss: 0.200
[99,     1] loss: 0.180
[100,     1] loss: 0.159
[101,     1] loss: 0.146
[102,     1] loss: 0.135
[103,     1] loss: 0.126
[104,     1] loss: 0.118
[105,     1] loss: 0.112
[106,     1] loss: 0.108
[107,     1] loss: 0.105
Early stopping applied (best metric=0.2294326275587082)
Finished Training
Total time taken: 410.5841417312622
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.647
[4,     1] loss: 0.615
[5,     1] loss: 0.579
[6,     1] loss: 0.541
[7,     1] loss: 0.501
[8,     1] loss: 0.470
[9,     1] loss: 0.445
[10,     1] loss: 0.423
[11,     1] loss: 0.407
[12,     1] loss: 0.399
[13,     1] loss: 0.395
[14,     1] loss: 0.383
[15,     1] loss: 0.385
[16,     1] loss: 0.384
[17,     1] loss: 0.369
[18,     1] loss: 0.364
[19,     1] loss: 0.353
[20,     1] loss: 0.348
[21,     1] loss: 0.344
[22,     1] loss: 0.339
[23,     1] loss: 0.335
[24,     1] loss: 0.331
[25,     1] loss: 0.328
[26,     1] loss: 0.325
[27,     1] loss: 0.320
[28,     1] loss: 0.325
[29,     1] loss: 0.328
[30,     1] loss: 0.332
[31,     1] loss: 0.314
[32,     1] loss: 0.307
[33,     1] loss: 0.310
[34,     1] loss: 0.317
[35,     1] loss: 0.296
[36,     1] loss: 0.289
[37,     1] loss: 0.277
[38,     1] loss: 0.259
[39,     1] loss: 0.246
[40,     1] loss: 0.233
[41,     1] loss: 0.216
[42,     1] loss: 0.197
[43,     1] loss: 0.180
[44,     1] loss: 0.163
[45,     1] loss: 0.148
[46,     1] loss: 0.135
[47,     1] loss: 0.122
[48,     1] loss: 0.113
[49,     1] loss: 0.100
[50,     1] loss: 0.091
[51,     1] loss: 0.085
[52,     1] loss: 0.081
[53,     1] loss: 0.076
[54,     1] loss: 0.075
[55,     1] loss: 0.075
[56,     1] loss: 0.074
[57,     1] loss: 0.076
[58,     1] loss: 0.076
[59,     1] loss: 0.077
[60,     1] loss: 0.136
[61,     1] loss: 0.135
[62,     1] loss: 0.126
[63,     1] loss: 0.632
[64,     1] loss: 0.528
[65,     1] loss: 0.575
[66,     1] loss: 0.513
[67,     1] loss: 0.491
[68,     1] loss: 0.471
[69,     1] loss: 0.461
[70,     1] loss: 0.454
[71,     1] loss: 0.434
[72,     1] loss: 0.405
[73,     1] loss: 0.378
[74,     1] loss: 0.348
[75,     1] loss: 0.317
[76,     1] loss: 0.286
[77,     1] loss: 0.259
[78,     1] loss: 0.236
[79,     1] loss: 0.218
[80,     1] loss: 0.201
[81,     1] loss: 0.188
[82,     1] loss: 0.178
[83,     1] loss: 0.167
[84,     1] loss: 0.158
[85,     1] loss: 0.150
[86,     1] loss: 0.141
[87,     1] loss: 0.135
[88,     1] loss: 0.130
[89,     1] loss: 0.124
[90,     1] loss: 0.120
[91,     1] loss: 0.118
[92,     1] loss: 0.113
[93,     1] loss: 0.110
[94,     1] loss: 0.108
[95,     1] loss: 0.107
[96,     1] loss: 0.106
[97,     1] loss: 0.106
[98,     1] loss: 0.106
[99,     1] loss: 0.146
[100,     1] loss: 0.216
[101,     1] loss: 0.592
[102,     1] loss: 0.717
[103,     1] loss: 0.737
[104,     1] loss: 0.722
[105,     1] loss: 0.704
[106,     1] loss: 0.693
[107,     1] loss: 0.688
[108,     1] loss: 0.686
[109,     1] loss: 0.686
[110,     1] loss: 0.687
[111,     1] loss: 0.689
[112,     1] loss: 0.690
[113,     1] loss: 0.691
[114,     1] loss: 0.692
[115,     1] loss: 0.692
[116,     1] loss: 0.692
[117,     1] loss: 0.693
[118,     1] loss: 0.693
[119,     1] loss: 0.693
[120,     1] loss: 0.693
[121,     1] loss: 0.693
[122,     1] loss: 0.693
[123,     1] loss: 0.693
[124,     1] loss: 0.693
[125,     1] loss: 0.693
[126,     1] loss: 0.693
[127,     1] loss: 0.693
[128,     1] loss: 0.693
[129,     1] loss: 0.693
Early stopping applied (best metric=0.24808381497859955)
Finished Training
Total time taken: 495.30299186706543
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.662
[4,     1] loss: 0.631
[5,     1] loss: 0.595
[6,     1] loss: 0.556
[7,     1] loss: 0.516
[8,     1] loss: 0.473
[9,     1] loss: 0.431
[10,     1] loss: 0.398
[11,     1] loss: 0.372
[12,     1] loss: 0.356
[13,     1] loss: 0.342
[14,     1] loss: 0.330
[15,     1] loss: 0.321
[16,     1] loss: 0.311
[17,     1] loss: 0.299
[18,     1] loss: 0.287
[19,     1] loss: 0.274
[20,     1] loss: 0.270
[21,     1] loss: 0.266
[22,     1] loss: 0.257
[23,     1] loss: 0.242
[24,     1] loss: 0.270
[25,     1] loss: 0.336
[26,     1] loss: 0.313
[27,     1] loss: 0.276
[28,     1] loss: 0.252
[29,     1] loss: 0.231
[30,     1] loss: 0.211
[31,     1] loss: 0.185
[32,     1] loss: 0.168
[33,     1] loss: 0.154
[34,     1] loss: 0.141
[35,     1] loss: 0.128
[36,     1] loss: 0.118
[37,     1] loss: 0.109
[38,     1] loss: 0.105
[39,     1] loss: 0.101
[40,     1] loss: 0.097
[41,     1] loss: 0.094
[42,     1] loss: 0.092
[43,     1] loss: 0.091
[44,     1] loss: 0.091
[45,     1] loss: 0.091
[46,     1] loss: 0.091
[47,     1] loss: 0.091
[48,     1] loss: 0.090
[49,     1] loss: 0.089
[50,     1] loss: 0.090
[51,     1] loss: 0.090
[52,     1] loss: 0.089
[53,     1] loss: 0.087
[54,     1] loss: 0.088
[55,     1] loss: 0.089
[56,     1] loss: 0.098
[57,     1] loss: 0.164
[58,     1] loss: 0.487
[59,     1] loss: 0.930
[60,     1] loss: 0.566
[61,     1] loss: 0.538
[62,     1] loss: 0.574
[63,     1] loss: 0.601
[64,     1] loss: 0.607
[65,     1] loss: 0.610
[66,     1] loss: 0.616
[67,     1] loss: 0.620
[68,     1] loss: 0.621
[69,     1] loss: 0.621
[70,     1] loss: 0.620
[71,     1] loss: 0.617
[72,     1] loss: 0.614
[73,     1] loss: 0.610
[74,     1] loss: 0.606
[75,     1] loss: 0.601
[76,     1] loss: 0.597
[77,     1] loss: 0.589
[78,     1] loss: 0.582
[79,     1] loss: 0.573
[80,     1] loss: 0.565
[81,     1] loss: 0.558
[82,     1] loss: 0.545
[83,     1] loss: 0.531
[84,     1] loss: 0.514
[85,     1] loss: 0.499
[86,     1] loss: 0.482
[87,     1] loss: 0.470
[88,     1] loss: 0.465
[89,     1] loss: 0.521
Early stopping applied (best metric=0.22037510573863983)
Finished Training
Total time taken: 343.39537382125854
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8593548387096774, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8916666666666666, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8534351145038168, 'Pyrrolidone carboxylic acid Validation Precision': 0.5451857824210765, 'Pyrrolidone carboxylic acid AUC ROC': 0.9559478371501272, 'Pyrrolidone carboxylic acid AUC PR': 0.8644069931771174, 'Pyrrolidone carboxylic acid MCC': 0.6229799110543417, 'Pyrrolidone carboxylic acid F1': 0.6702209302856412, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.23517556488513947, 'Validation Loss (total)': 0.23517556488513947}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007044140392025356,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2796712292,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.787408860526753}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007362678812740097,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3968231927,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.077605541418277}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006381220485495498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 356447307,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.653146067435548}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.650
[4,     1] loss: 0.610
[5,     1] loss: 0.560
[6,     1] loss: 0.502
[7,     1] loss: 0.441
[8,     1] loss: 0.381
[9,     1] loss: 0.318
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005243146462112191,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 868099100,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.1526287076181685}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.673
[3,     1] loss: 0.603
[4,     1] loss: 0.526
[5,     1] loss: 0.436
[6,     1] loss: 0.335
[7,     1] loss: 0.242
[8,     1] loss: 0.170
[9,     1] loss: 0.114
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011055846400840293,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2701032792,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.945142413478872}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.669
[3,     1] loss: 0.634
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009561075631694083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3295395140,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.305032406218956}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.681
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005693825119886478,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1585803168,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.159888755746735}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.686
[3,     1] loss: 0.659
[4,     1] loss: 0.630
[5,     1] loss: 0.598
[6,     1] loss: 0.560
[7,     1] loss: 0.522
[8,     1] loss: 0.485
[9,     1] loss: 0.444
[10,     1] loss: 0.407
[11,     1] loss: 0.374
[12,     1] loss: 0.334
[13,     1] loss: 0.300
[14,     1] loss: 0.265
[15,     1] loss: 0.235
[16,     1] loss: 0.207
[17,     1] loss: 0.182
[18,     1] loss: 0.159
[19,     1] loss: 0.138
[20,     1] loss: 0.125
[21,     1] loss: 0.109
[22,     1] loss: 0.097
[23,     1] loss: 0.086
[24,     1] loss: 0.076
[25,     1] loss: 0.069
[26,     1] loss: 0.064
[27,     1] loss: 0.061
[28,     1] loss: 0.056
[29,     1] loss: 0.054
[30,     1] loss: 0.052
[31,     1] loss: 0.051
[32,     1] loss: 0.051
[33,     1] loss: 0.050
[34,     1] loss: 0.050
[35,     1] loss: 0.051
[36,     1] loss: 0.051
[37,     1] loss: 0.050
[38,     1] loss: 0.051
[39,     1] loss: 0.052
[40,     1] loss: 0.052
[41,     1] loss: 0.052
[42,     1] loss: 0.052
[43,     1] loss: 0.051
[44,     1] loss: 0.052
[45,     1] loss: 0.052
[46,     1] loss: 0.051
[47,     1] loss: 0.050
[48,     1] loss: 0.050
[49,     1] loss: 0.049
[50,     1] loss: 0.048
[51,     1] loss: 0.048
[52,     1] loss: 0.049
[53,     1] loss: 0.048
[54,     1] loss: 0.048
[55,     1] loss: 0.048
[56,     1] loss: 0.056
[57,     1] loss: 0.108
[58,     1] loss: 0.396
[59,     1] loss: 0.686
[60,     1] loss: 0.447
[61,     1] loss: 0.455
[62,     1] loss: 0.466
[63,     1] loss: 0.452
[64,     1] loss: 0.466
[65,     1] loss: 0.477
[66,     1] loss: 0.457
Early stopping applied (best metric=0.2903638482093811)
Finished Training
Total time taken: 256.32010316848755
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.672
[4,     1] loss: 0.654
[5,     1] loss: 0.631
[6,     1] loss: 0.605
[7,     1] loss: 0.576
[8,     1] loss: 0.545
[9,     1] loss: 0.514
[10,     1] loss: 0.480
[11,     1] loss: 0.441
[12,     1] loss: 0.407
[13,     1] loss: 0.373
[14,     1] loss: 0.338
[15,     1] loss: 0.308
[16,     1] loss: 0.279
[17,     1] loss: 0.253
[18,     1] loss: 0.229
[19,     1] loss: 0.205
[20,     1] loss: 0.183
[21,     1] loss: 0.164
[22,     1] loss: 0.145
[23,     1] loss: 0.130
[24,     1] loss: 0.117
[25,     1] loss: 0.105
[26,     1] loss: 0.094
[27,     1] loss: 0.087
[28,     1] loss: 0.079
[29,     1] loss: 0.075
[30,     1] loss: 0.071
[31,     1] loss: 0.067
[32,     1] loss: 0.064
[33,     1] loss: 0.063
[34,     1] loss: 0.062
[35,     1] loss: 0.061
[36,     1] loss: 0.061
[37,     1] loss: 0.061
[38,     1] loss: 0.062
[39,     1] loss: 0.062
[40,     1] loss: 0.062
[41,     1] loss: 0.062
[42,     1] loss: 0.062
[43,     1] loss: 0.063
[44,     1] loss: 0.063
[45,     1] loss: 0.064
[46,     1] loss: 0.063
[47,     1] loss: 0.064
[48,     1] loss: 0.064
[49,     1] loss: 0.062
[50,     1] loss: 0.065
[51,     1] loss: 0.062
[52,     1] loss: 0.062
[53,     1] loss: 0.062
[54,     1] loss: 0.061
[55,     1] loss: 0.062
[56,     1] loss: 0.063
[57,     1] loss: 0.156
[58,     1] loss: 0.421
[59,     1] loss: 0.384
[60,     1] loss: 0.455
[61,     1] loss: 0.389
[62,     1] loss: 0.478
[63,     1] loss: 0.394
[64,     1] loss: 0.432
[65,     1] loss: 0.454
[66,     1] loss: 0.442
[67,     1] loss: 0.433
[68,     1] loss: 0.421
[69,     1] loss: 0.418
[70,     1] loss: 0.417
[71,     1] loss: 0.410
[72,     1] loss: 0.387
[73,     1] loss: 0.374
[74,     1] loss: 0.360
[75,     1] loss: 0.333
[76,     1] loss: 0.315
[77,     1] loss: 0.300
[78,     1] loss: 0.286
[79,     1] loss: 0.266
[80,     1] loss: 0.258
[81,     1] loss: 0.243
[82,     1] loss: 0.232
[83,     1] loss: 0.224
[84,     1] loss: 0.216
[85,     1] loss: 0.208
[86,     1] loss: 0.193
[87,     1] loss: 0.184
[88,     1] loss: 0.165
[89,     1] loss: 0.175
[90,     1] loss: 0.504
[91,     1] loss: 0.287
[92,     1] loss: 0.296
[93,     1] loss: 0.523
[94,     1] loss: 0.331
[95,     1] loss: 0.400
[96,     1] loss: 0.406
[97,     1] loss: 0.410
[98,     1] loss: 0.397
[99,     1] loss: 0.381
[100,     1] loss: 0.362
[101,     1] loss: 0.350
[102,     1] loss: 0.331
[103,     1] loss: 0.307
[104,     1] loss: 0.284
[105,     1] loss: 0.264
[106,     1] loss: 0.248
[107,     1] loss: 0.231
[108,     1] loss: 0.209
[109,     1] loss: 0.195
[110,     1] loss: 0.179
[111,     1] loss: 0.164
[112,     1] loss: 0.154
[113,     1] loss: 0.139
[114,     1] loss: 0.130
[115,     1] loss: 0.119
[116,     1] loss: 0.110
[117,     1] loss: 0.108
[118,     1] loss: 0.103
[119,     1] loss: 0.099
[120,     1] loss: 0.096
[121,     1] loss: 0.097
[122,     1] loss: 0.097
[123,     1] loss: 0.094
[124,     1] loss: 0.095
[125,     1] loss: 0.093
[126,     1] loss: 0.092
[127,     1] loss: 0.091
[128,     1] loss: 0.090
[129,     1] loss: 0.090
[130,     1] loss: 0.090
[131,     1] loss: 0.091
[132,     1] loss: 0.091
[133,     1] loss: 0.090
[134,     1] loss: 0.087
[135,     1] loss: 0.088
[136,     1] loss: 0.090
[137,     1] loss: 0.088
[138,     1] loss: 0.089
[139,     1] loss: 0.088
[140,     1] loss: 0.089
[141,     1] loss: 0.089
Early stopping applied (best metric=0.28990063071250916)
Finished Training
Total time taken: 544.8828480243683
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.667
[3,     1] loss: 0.622
[4,     1] loss: 0.580
[5,     1] loss: 0.538
[6,     1] loss: 0.489
[7,     1] loss: 0.446
[8,     1] loss: 0.401
[9,     1] loss: 0.356
[10,     1] loss: 0.315
[11,     1] loss: 0.276
[12,     1] loss: 0.238
[13,     1] loss: 0.200
[14,     1] loss: 0.171
[15,     1] loss: 0.150
[16,     1] loss: 0.131
[17,     1] loss: 0.117
[18,     1] loss: 0.102
[19,     1] loss: 0.092
[20,     1] loss: 0.085
[21,     1] loss: 0.077
[22,     1] loss: 0.069
[23,     1] loss: 0.067
[24,     1] loss: 0.065
[25,     1] loss: 0.061
[26,     1] loss: 0.059
[27,     1] loss: 0.059
[28,     1] loss: 0.057
[29,     1] loss: 0.057
[30,     1] loss: 0.057
[31,     1] loss: 0.056
[32,     1] loss: 0.056
[33,     1] loss: 0.055
[34,     1] loss: 0.055
[35,     1] loss: 0.054
[36,     1] loss: 0.055
[37,     1] loss: 0.054
[38,     1] loss: 0.053
[39,     1] loss: 0.053
[40,     1] loss: 0.052
[41,     1] loss: 0.052
[42,     1] loss: 0.052
[43,     1] loss: 0.051
[44,     1] loss: 0.050
[45,     1] loss: 0.050
[46,     1] loss: 0.050
[47,     1] loss: 0.050
[48,     1] loss: 0.050
[49,     1] loss: 0.050
[50,     1] loss: 0.049
[51,     1] loss: 0.049
[52,     1] loss: 0.049
[53,     1] loss: 0.048
[54,     1] loss: 0.050
[55,     1] loss: 0.048
[56,     1] loss: 0.048
[57,     1] loss: 0.048
[58,     1] loss: 0.052
[59,     1] loss: 0.114
[60,     1] loss: 0.402
[61,     1] loss: 0.339
Early stopping applied (best metric=0.2915508449077606)
Finished Training
Total time taken: 238.2645230293274
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.646
[4,     1] loss: 0.609
[5,     1] loss: 0.569
[6,     1] loss: 0.525
[7,     1] loss: 0.479
[8,     1] loss: 0.435
[9,     1] loss: 0.390
[10,     1] loss: 0.347
[11,     1] loss: 0.302
[12,     1] loss: 0.261
[13,     1] loss: 0.223
[14,     1] loss: 0.189
[15,     1] loss: 0.167
[16,     1] loss: 0.142
[17,     1] loss: 0.126
[18,     1] loss: 0.110
[19,     1] loss: 0.097
[20,     1] loss: 0.087
[21,     1] loss: 0.078
[22,     1] loss: 0.072
[23,     1] loss: 0.066
[24,     1] loss: 0.063
[25,     1] loss: 0.060
[26,     1] loss: 0.058
[27,     1] loss: 0.057
[28,     1] loss: 0.056
[29,     1] loss: 0.055
[30,     1] loss: 0.056
[31,     1] loss: 0.055
[32,     1] loss: 0.055
[33,     1] loss: 0.053
[34,     1] loss: 0.054
[35,     1] loss: 0.054
[36,     1] loss: 0.054
[37,     1] loss: 0.054
[38,     1] loss: 0.053
[39,     1] loss: 0.053
[40,     1] loss: 0.050
[41,     1] loss: 0.051
[42,     1] loss: 0.050
[43,     1] loss: 0.049
[44,     1] loss: 0.049
[45,     1] loss: 0.050
[46,     1] loss: 0.048
[47,     1] loss: 0.046
[48,     1] loss: 0.046
[49,     1] loss: 0.046
[50,     1] loss: 0.045
[51,     1] loss: 0.045
[52,     1] loss: 0.044
[53,     1] loss: 0.045
[54,     1] loss: 0.045
[55,     1] loss: 0.075
[56,     1] loss: 0.295
[57,     1] loss: 0.389
[58,     1] loss: 0.422
[59,     1] loss: 0.390
[60,     1] loss: 0.434
[61,     1] loss: 0.421
[62,     1] loss: 0.442
[63,     1] loss: 0.422
Early stopping applied (best metric=0.27640071511268616)
Finished Training
Total time taken: 246.35717582702637
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.683
[3,     1] loss: 0.653
[4,     1] loss: 0.626
[5,     1] loss: 0.596
[6,     1] loss: 0.564
[7,     1] loss: 0.530
[8,     1] loss: 0.494
[9,     1] loss: 0.461
[10,     1] loss: 0.424
[11,     1] loss: 0.388
[12,     1] loss: 0.354
[13,     1] loss: 0.325
[14,     1] loss: 0.299
[15,     1] loss: 0.277
[16,     1] loss: 0.254
[17,     1] loss: 0.234
[18,     1] loss: 0.212
[19,     1] loss: 0.193
[20,     1] loss: 0.176
[21,     1] loss: 0.156
[22,     1] loss: 0.139
[23,     1] loss: 0.124
[24,     1] loss: 0.109
[25,     1] loss: 0.094
[26,     1] loss: 0.083
[27,     1] loss: 0.074
[28,     1] loss: 0.066
[29,     1] loss: 0.062
[30,     1] loss: 0.057
[31,     1] loss: 0.053
[32,     1] loss: 0.052
[33,     1] loss: 0.049
[34,     1] loss: 0.047
[35,     1] loss: 0.047
[36,     1] loss: 0.046
[37,     1] loss: 0.044
[38,     1] loss: 0.046
[39,     1] loss: 0.047
[40,     1] loss: 0.046
[41,     1] loss: 0.046
[42,     1] loss: 0.046
[43,     1] loss: 0.046
[44,     1] loss: 0.046
[45,     1] loss: 0.047
[46,     1] loss: 0.047
[47,     1] loss: 0.046
[48,     1] loss: 0.047
[49,     1] loss: 0.048
[50,     1] loss: 0.047
[51,     1] loss: 0.046
[52,     1] loss: 0.046
[53,     1] loss: 0.046
[54,     1] loss: 0.045
[55,     1] loss: 0.045
[56,     1] loss: 0.044
[57,     1] loss: 0.044
[58,     1] loss: 0.044
[59,     1] loss: 0.045
[60,     1] loss: 0.044
[61,     1] loss: 0.044
[62,     1] loss: 0.044
[63,     1] loss: 0.047
[64,     1] loss: 0.117
[65,     1] loss: 0.952
[66,     1] loss: 0.473
[67,     1] loss: 0.483
[68,     1] loss: 0.470
[69,     1] loss: 0.515
[70,     1] loss: 0.485
[71,     1] loss: 0.506
[72,     1] loss: 0.478
Early stopping applied (best metric=0.3316088616847992)
Finished Training
Total time taken: 281.23654222488403
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8406451612903226, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8333333333333334, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8419847328244275, 'Pyrrolidone carboxylic acid Validation Precision': 0.4979248529487214, 'Pyrrolidone carboxylic acid AUC ROC': 0.917159669211196, 'Pyrrolidone carboxylic acid AUC PR': 0.6803710607271922, 'Pyrrolidone carboxylic acid MCC': 0.5587013467522675, 'Pyrrolidone carboxylic acid F1': 0.6202560590513302, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.29596498012542727, 'Validation Loss (total)': 0.29596498012542727}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009450142892538638,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3932092782,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.739218034497439}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.624
[4,     1] loss: 0.561
[5,     1] loss: 0.487
[6,     1] loss: 0.406
[7,     1] loss: 0.332
[8,     1] loss: 0.254
[9,     1] loss: 0.192
[10,     1] loss: 0.142
[11,     1] loss: 0.091
[12,     1] loss: 0.073
[13,     1] loss: 0.054
[14,     1] loss: 0.040
[15,     1] loss: 0.030
[16,     1] loss: 0.024
[17,     1] loss: 0.021
[18,     1] loss: 0.019
[19,     1] loss: 0.017
[20,     1] loss: 0.015
[21,     1] loss: 0.014
[22,     1] loss: 0.012
[23,     1] loss: 0.012
[24,     1] loss: 0.012
[25,     1] loss: 0.012
[26,     1] loss: 0.012
[27,     1] loss: 0.012
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005870748079724314,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1906602504,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.697380681140794}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.674
[3,     1] loss: 0.634
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009861176841087262,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 683702883,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.750553841653538}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.683
[3,     1] loss: 0.641
[4,     1] loss: 0.602
[5,     1] loss: 0.561
[6,     1] loss: 0.520
[7,     1] loss: 0.487
[8,     1] loss: 0.456
[9,     1] loss: 0.431
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005035283148111724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 872939830,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.423138215016401}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.669
[3,     1] loss: 0.608
[4,     1] loss: 0.550
[5,     1] loss: 0.490
[6,     1] loss: 0.434
[7,     1] loss: 0.393
[8,     1] loss: 0.349
[9,     1] loss: 0.318
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008514839014610734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 159738600,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.922373298096094}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007545669625732741,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2807674874,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.691798555570546}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.635
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00047398342967804936,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1220288512,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.671489598831467}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.673
[3,     1] loss: 0.652
[4,     1] loss: 0.640
[5,     1] loss: 0.627
[6,     1] loss: 0.615
[7,     1] loss: 0.606
[8,     1] loss: 0.597
[9,     1] loss: 0.588
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00971964846473949,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 593334950,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.473882594682287}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.674
[3,     1] loss: 0.611
[4,     1] loss: 0.548
[5,     1] loss: 0.487
[6,     1] loss: 0.429
[7,     1] loss: 0.378
[8,     1] loss: 0.332
[9,     1] loss: 0.284
[10,     1] loss: 0.236
[11,     1] loss: 0.197
[12,     1] loss: 0.177
[13,     1] loss: 0.133
[14,     1] loss: 0.117
[15,     1] loss: 0.091
[16,     1] loss: 0.081
[17,     1] loss: 0.064
[18,     1] loss: 0.054
[19,     1] loss: 0.048
[20,     1] loss: 0.042
[21,     1] loss: 0.036
[22,     1] loss: 0.032
[23,     1] loss: 0.029
[24,     1] loss: 0.028
[25,     1] loss: 0.028
[26,     1] loss: 0.027
[27,     1] loss: 0.026
[28,     1] loss: 0.026
[29,     1] loss: 0.026
[30,     1] loss: 0.026
[31,     1] loss: 0.026
[32,     1] loss: 0.025
[33,     1] loss: 0.026
[34,     1] loss: 0.026
[35,     1] loss: 0.026
[36,     1] loss: 0.026
[37,     1] loss: 0.025
[38,     1] loss: 0.024
[39,     1] loss: 0.024
[40,     1] loss: 0.025
[41,     1] loss: 0.024
[42,     1] loss: 0.023
[43,     1] loss: 0.023
[44,     1] loss: 0.023
[45,     1] loss: 0.024
[46,     1] loss: 0.024
[47,     1] loss: 0.056
[48,     1] loss: 0.255
[49,     1] loss: 1.048
[50,     1] loss: 0.443
[51,     1] loss: 0.487
[52,     1] loss: 0.526
[53,     1] loss: 0.543
[54,     1] loss: 0.553
[55,     1] loss: 0.552
[56,     1] loss: 0.543
[57,     1] loss: 0.530
[58,     1] loss: 0.516
[59,     1] loss: 0.504
[60,     1] loss: 0.488
[61,     1] loss: 0.469
[62,     1] loss: 0.452
[63,     1] loss: 0.432
[64,     1] loss: 0.410
[65,     1] loss: 0.389
Early stopping applied (best metric=0.24927063286304474)
Finished Training
Total time taken: 254.96462965011597
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.633
[4,     1] loss: 0.582
[5,     1] loss: 0.524
[6,     1] loss: 0.462
[7,     1] loss: 0.403
[8,     1] loss: 0.351
[9,     1] loss: 0.307
[10,     1] loss: 0.276
[11,     1] loss: 0.247
[12,     1] loss: 0.221
[13,     1] loss: 0.192
[14,     1] loss: 0.166
[15,     1] loss: 0.139
[16,     1] loss: 0.117
[17,     1] loss: 0.099
[18,     1] loss: 0.082
[19,     1] loss: 0.071
[20,     1] loss: 0.062
[21,     1] loss: 0.058
[22,     1] loss: 0.049
[23,     1] loss: 0.044
[24,     1] loss: 0.039
[25,     1] loss: 0.054
[26,     1] loss: 0.086
[27,     1] loss: 0.608
[28,     1] loss: 0.349
[29,     1] loss: 0.349
[30,     1] loss: 0.404
[31,     1] loss: 0.448
[32,     1] loss: 0.449
[33,     1] loss: 0.441
[34,     1] loss: 0.419
[35,     1] loss: 0.388
[36,     1] loss: 0.350
[37,     1] loss: 0.308
[38,     1] loss: 0.269
[39,     1] loss: 0.238
[40,     1] loss: 0.214
[41,     1] loss: 0.194
[42,     1] loss: 0.175
[43,     1] loss: 0.159
[44,     1] loss: 0.143
[45,     1] loss: 0.129
[46,     1] loss: 0.116
[47,     1] loss: 0.108
[48,     1] loss: 0.099
[49,     1] loss: 0.091
[50,     1] loss: 0.086
[51,     1] loss: 0.111
[52,     1] loss: 0.135
[53,     1] loss: 0.145
[54,     1] loss: 0.316
[55,     1] loss: 0.452
[56,     1] loss: 0.358
[57,     1] loss: 0.396
[58,     1] loss: 0.386
[59,     1] loss: 0.343
[60,     1] loss: 0.318
[61,     1] loss: 0.295
[62,     1] loss: 0.259
[63,     1] loss: 0.222
[64,     1] loss: 0.194
[65,     1] loss: 0.172
[66,     1] loss: 0.148
[67,     1] loss: 0.131
[68,     1] loss: 0.122
[69,     1] loss: 0.112
[70,     1] loss: 0.103
[71,     1] loss: 0.093
[72,     1] loss: 0.091
[73,     1] loss: 0.086
[74,     1] loss: 0.081
[75,     1] loss: 0.077
[76,     1] loss: 0.079
[77,     1] loss: 0.076
[78,     1] loss: 0.077
[79,     1] loss: 0.075
[80,     1] loss: 0.072
[81,     1] loss: 0.072
[82,     1] loss: 0.071
[83,     1] loss: 0.069
[84,     1] loss: 0.071
[85,     1] loss: 0.069
[86,     1] loss: 0.068
[87,     1] loss: 0.068
[88,     1] loss: 0.068
[89,     1] loss: 0.069
[90,     1] loss: 0.068
[91,     1] loss: 0.070
[92,     1] loss: 0.069
[93,     1] loss: 0.070
[94,     1] loss: 0.070
[95,     1] loss: 0.069
[96,     1] loss: 0.070
[97,     1] loss: 0.071
[98,     1] loss: 0.069
[99,     1] loss: 0.067
[100,     1] loss: 0.068
[101,     1] loss: 0.067
[102,     1] loss: 0.068
[103,     1] loss: 0.077
[104,     1] loss: 1.082
[105,     1] loss: 0.672
[106,     1] loss: 0.534
[107,     1] loss: 0.570
[108,     1] loss: 0.588
[109,     1] loss: 0.583
[110,     1] loss: 0.576
[111,     1] loss: 0.564
[112,     1] loss: 0.552
[113,     1] loss: 0.534
Early stopping applied (best metric=0.2849206328392029)
Finished Training
Total time taken: 441.7975506782532
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.660
[4,     1] loss: 0.623
[5,     1] loss: 0.580
[6,     1] loss: 0.529
[7,     1] loss: 0.474
[8,     1] loss: 0.419
[9,     1] loss: 0.364
[10,     1] loss: 0.311
[11,     1] loss: 0.258
[12,     1] loss: 0.215
[13,     1] loss: 0.165
[14,     1] loss: 0.135
[15,     1] loss: 0.112
[16,     1] loss: 0.099
[17,     1] loss: 0.100
[18,     1] loss: 0.097
[19,     1] loss: 0.066
[20,     1] loss: 0.099
[21,     1] loss: 0.077
[22,     1] loss: 0.076
[23,     1] loss: 0.086
[24,     1] loss: 0.081
[25,     1] loss: 0.063
[26,     1] loss: 0.068
[27,     1] loss: 0.055
[28,     1] loss: 0.061
[29,     1] loss: 0.049
[30,     1] loss: 0.064
[31,     1] loss: 0.091
[32,     1] loss: 0.066
[33,     1] loss: 0.064
[34,     1] loss: 0.060
[35,     1] loss: 0.066
[36,     1] loss: 0.058
[37,     1] loss: 0.058
[38,     1] loss: 0.053
[39,     1] loss: 0.050
[40,     1] loss: 0.046
[41,     1] loss: 0.044
[42,     1] loss: 0.042
[43,     1] loss: 0.042
[44,     1] loss: 0.041
[45,     1] loss: 0.041
[46,     1] loss: 0.042
[47,     1] loss: 0.042
[48,     1] loss: 0.042
[49,     1] loss: 0.042
[50,     1] loss: 0.043
[51,     1] loss: 0.043
[52,     1] loss: 0.045
[53,     1] loss: 0.044
[54,     1] loss: 0.043
[55,     1] loss: 0.043
[56,     1] loss: 0.042
[57,     1] loss: 0.042
[58,     1] loss: 0.041
[59,     1] loss: 0.040
[60,     1] loss: 0.041
[61,     1] loss: 0.050
[62,     1] loss: 0.194
[63,     1] loss: 0.710
[64,     1] loss: 0.516
[65,     1] loss: 0.537
[66,     1] loss: 0.518
[67,     1] loss: 0.507
[68,     1] loss: 0.484
[69,     1] loss: 0.461
[70,     1] loss: 0.455
[71,     1] loss: 0.423
[72,     1] loss: 0.397
[73,     1] loss: 0.363
[74,     1] loss: 0.333
[75,     1] loss: 0.304
[76,     1] loss: 0.303
[77,     1] loss: 0.282
[78,     1] loss: 0.261
[79,     1] loss: 0.222
[80,     1] loss: 0.200
[81,     1] loss: 0.183
[82,     1] loss: 0.173
[83,     1] loss: 0.141
[84,     1] loss: 0.113
[85,     1] loss: 0.096
[86,     1] loss: 0.081
[87,     1] loss: 0.076
[88,     1] loss: 0.071
[89,     1] loss: 0.068
[90,     1] loss: 0.065
[91,     1] loss: 0.055
[92,     1] loss: 0.059
[93,     1] loss: 0.058
[94,     1] loss: 0.057
[95,     1] loss: 0.057
[96,     1] loss: 0.058
[97,     1] loss: 0.058
[98,     1] loss: 0.058
[99,     1] loss: 0.058
[100,     1] loss: 0.057
[101,     1] loss: 0.057
[102,     1] loss: 0.057
[103,     1] loss: 0.056
[104,     1] loss: 0.057
[105,     1] loss: 0.058
[106,     1] loss: 0.058
[107,     1] loss: 0.057
[108,     1] loss: 0.057
[109,     1] loss: 0.055
[110,     1] loss: 0.055
[111,     1] loss: 0.055
[112,     1] loss: 0.059
[113,     1] loss: 0.582
[114,     1] loss: 1.386
[115,     1] loss: 0.917
[116,     1] loss: 0.699
[117,     1] loss: 0.683
[118,     1] loss: 0.687
[119,     1] loss: 0.689
[120,     1] loss: 0.690
[121,     1] loss: 0.691
[122,     1] loss: 0.691
[123,     1] loss: 0.691
[124,     1] loss: 0.691
[125,     1] loss: 0.691
[126,     1] loss: 0.691
[127,     1] loss: 0.692
[128,     1] loss: 0.692
[129,     1] loss: 0.692
[130,     1] loss: 0.692
[131,     1] loss: 0.692
[132,     1] loss: 0.692
[133,     1] loss: 0.693
[134,     1] loss: 0.693
Early stopping applied (best metric=0.242510586977005)
Finished Training
Total time taken: 523.5682616233826
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.690
[3,     1] loss: 0.632
[4,     1] loss: 0.570
[5,     1] loss: 0.503
[6,     1] loss: 0.441
[7,     1] loss: 0.387
[8,     1] loss: 0.339
[9,     1] loss: 0.297
[10,     1] loss: 0.248
[11,     1] loss: 0.209
[12,     1] loss: 0.171
[13,     1] loss: 0.139
[14,     1] loss: 0.103
[15,     1] loss: 0.083
[16,     1] loss: 0.071
[17,     1] loss: 0.058
[18,     1] loss: 0.045
[19,     1] loss: 0.040
[20,     1] loss: 0.037
[21,     1] loss: 0.032
[22,     1] loss: 0.030
[23,     1] loss: 0.029
[24,     1] loss: 0.028
[25,     1] loss: 0.027
[26,     1] loss: 0.028
[27,     1] loss: 0.028
[28,     1] loss: 0.028
[29,     1] loss: 0.029
[30,     1] loss: 0.030
[31,     1] loss: 0.029
[32,     1] loss: 0.030
[33,     1] loss: 0.029
[34,     1] loss: 0.028
[35,     1] loss: 0.027
[36,     1] loss: 0.028
[37,     1] loss: 0.027
[38,     1] loss: 0.027
[39,     1] loss: 0.027
[40,     1] loss: 0.027
[41,     1] loss: 0.027
[42,     1] loss: 0.026
[43,     1] loss: 0.025
[44,     1] loss: 0.026
[45,     1] loss: 0.031
[46,     1] loss: 0.074
[47,     1] loss: 0.463
[48,     1] loss: 0.773
[49,     1] loss: 0.605
[50,     1] loss: 0.516
[51,     1] loss: 0.551
[52,     1] loss: 0.587
[53,     1] loss: 0.605
[54,     1] loss: 0.613
[55,     1] loss: 0.617
[56,     1] loss: 0.618
[57,     1] loss: 0.617
[58,     1] loss: 0.615
[59,     1] loss: 0.608
[60,     1] loss: 0.599
[61,     1] loss: 0.590
[62,     1] loss: 0.577
[63,     1] loss: 0.562
[64,     1] loss: 0.545
Early stopping applied (best metric=0.2571975290775299)
Finished Training
Total time taken: 252.33777403831482
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.696
[3,     1] loss: 0.660
[4,     1] loss: 0.619
[5,     1] loss: 0.573
[6,     1] loss: 0.523
[7,     1] loss: 0.472
[8,     1] loss: 0.414
[9,     1] loss: 0.361
[10,     1] loss: 0.313
[11,     1] loss: 0.277
[12,     1] loss: 0.247
[13,     1] loss: 0.220
[14,     1] loss: 0.202
[15,     1] loss: 0.183
[16,     1] loss: 0.165
[17,     1] loss: 0.151
[18,     1] loss: 0.135
[19,     1] loss: 0.128
[20,     1] loss: 0.120
[21,     1] loss: 0.109
[22,     1] loss: 0.098
[23,     1] loss: 0.091
[24,     1] loss: 0.084
[25,     1] loss: 0.078
[26,     1] loss: 0.070
[27,     1] loss: 0.062
[28,     1] loss: 0.058
[29,     1] loss: 0.051
[30,     1] loss: 0.050
[31,     1] loss: 0.045
[32,     1] loss: 0.047
[33,     1] loss: 0.043
[34,     1] loss: 0.045
[35,     1] loss: 0.044
[36,     1] loss: 0.049
[37,     1] loss: 0.054
[38,     1] loss: 0.087
[39,     1] loss: 0.336
[40,     1] loss: 0.732
[41,     1] loss: 0.654
[42,     1] loss: 0.606
[43,     1] loss: 0.606
[44,     1] loss: 0.608
[45,     1] loss: 0.610
[46,     1] loss: 0.612
[47,     1] loss: 0.611
[48,     1] loss: 0.611
[49,     1] loss: 0.609
[50,     1] loss: 0.608
[51,     1] loss: 0.603
[52,     1] loss: 0.597
[53,     1] loss: 0.589
[54,     1] loss: 0.579
[55,     1] loss: 0.567
[56,     1] loss: 0.553
[57,     1] loss: 0.535
[58,     1] loss: 0.513
[59,     1] loss: 0.489
[60,     1] loss: 0.460
[61,     1] loss: 0.425
[62,     1] loss: 0.387
[63,     1] loss: 0.343
[64,     1] loss: 0.292
[65,     1] loss: 0.240
[66,     1] loss: 0.192
[67,     1] loss: 0.153
[68,     1] loss: 0.122
[69,     1] loss: 0.116
Early stopping applied (best metric=0.24229829013347626)
Finished Training
Total time taken: 272.2976658344269
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8251612903225807, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9125, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8091603053435115, 'Pyrrolidone carboxylic acid Validation Precision': 0.4675509986008985, 'Pyrrolidone carboxylic acid AUC ROC': 0.9330470737913485, 'Pyrrolidone carboxylic acid AUC PR': 0.7598007417109423, 'Pyrrolidone carboxylic acid MCC': 0.5686799784887568, 'Pyrrolidone carboxylic acid F1': 0.617948689193067, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.25523953437805175, 'Validation Loss (total)': 0.25523953437805175}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009623987240639049,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 345574029,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.040026904371294}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.649
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000621000246261571,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2195211168,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.379404820836266}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.673
[3,     1] loss: 0.654
[4,     1] loss: 0.636
[5,     1] loss: 0.619
[6,     1] loss: 0.601
[7,     1] loss: 0.588
[8,     1] loss: 0.572
[9,     1] loss: 0.557
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009966314469229517,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 731552203,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.242629231820546}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.678
[3,     1] loss: 0.628
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00835893595788257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1013980123,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.904307625232885}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.676
[3,     1] loss: 0.630
[4,     1] loss: 0.586
[5,     1] loss: 0.531
[6,     1] loss: 0.469
[7,     1] loss: 0.409
[8,     1] loss: 0.347
[9,     1] loss: 0.283
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012974273454115812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 402289104,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.219735399691814}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.682
[3,     1] loss: 0.665
[4,     1] loss: 0.650
[5,     1] loss: 0.640
[6,     1] loss: 0.629
[7,     1] loss: 0.619
[8,     1] loss: 0.610
[9,     1] loss: 0.600
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00551532985413184,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 157607950,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.17484136129831818}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.647
[4,     1] loss: 0.603
[5,     1] loss: 0.540
[6,     1] loss: 0.461
[7,     1] loss: 0.369
[8,     1] loss: 0.280
[9,     1] loss: 0.201
[10,     1] loss: 0.132
[11,     1] loss: 0.068
[12,     1] loss: 0.033
[13,     1] loss: 0.014
[14,     1] loss: 0.008
[15,     1] loss: 0.005
[16,     1] loss: 0.002
[17,     1] loss: 0.001
[18,     1] loss: 0.001
[19,     1] loss: 0.000
[20,     1] loss: 0.000
[21,     1] loss: 0.000
[22,     1] loss: 0.000
[23,     1] loss: 0.000
[24,     1] loss: 0.000
[25,     1] loss: 0.000
[26,     1] loss: 0.000
[27,     1] loss: 0.000
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009066244553997578,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 549462284,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.498635556248797}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.672
[3,     1] loss: 0.607
[4,     1] loss: 0.540
[5,     1] loss: 0.470
[6,     1] loss: 0.407
[7,     1] loss: 0.342
[8,     1] loss: 0.273
[9,     1] loss: 0.202
[10,     1] loss: 0.153
[11,     1] loss: 0.111
[12,     1] loss: 0.081
[13,     1] loss: 0.066
[14,     1] loss: 0.048
[15,     1] loss: 0.042
[16,     1] loss: 0.033
[17,     1] loss: 0.023
[18,     1] loss: 0.019
[19,     1] loss: 0.018
[20,     1] loss: 0.017
[21,     1] loss: 0.016
[22,     1] loss: 0.016
[23,     1] loss: 0.016
[24,     1] loss: 0.016
[25,     1] loss: 0.016
[26,     1] loss: 0.017
[27,     1] loss: 0.016
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00771133467807698,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1037492367,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.309639670524012}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.668
[3,     1] loss: 0.591
[4,     1] loss: 0.518
[5,     1] loss: 0.455
[6,     1] loss: 0.408
[7,     1] loss: 0.371
[8,     1] loss: 0.336
[9,     1] loss: 0.302
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00998338884237176,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3499418398,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.206199816683458}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.691
[3,     1] loss: 0.646
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008295240309641704,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3925520455,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.094696241433795}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.668
[4,     1] loss: 0.657
[5,     1] loss: 0.646
[6,     1] loss: 0.637
[7,     1] loss: 0.629
[8,     1] loss: 0.622
[9,     1] loss: 0.614
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007198835683377158,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3197872027,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.859504874095267}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.627
[4,     1] loss: 0.568
[5,     1] loss: 0.496
[6,     1] loss: 0.420
[7,     1] loss: 0.344
[8,     1] loss: 0.270
[9,     1] loss: 0.214
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037688526726805133,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 764523900,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.10176035933612}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.672
[3,     1] loss: 0.630
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009459725222682774,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4098826209,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.8906458249222151}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.663
[3,     1] loss: 0.574
[4,     1] loss: 0.468
[5,     1] loss: 0.371
[6,     1] loss: 0.274
[7,     1] loss: 0.187
[8,     1] loss: 0.111
[9,     1] loss: 0.069
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00016939075582297555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2470678814,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.543419461238312}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066272779030678635,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1188871896,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.06160767867324}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.689
[3,     1] loss: 0.651
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005229931506861681,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1910563847,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.298186301690134}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009514900790352485,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3443387013,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.012113484943687}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.682
[3,     1] loss: 0.618
[4,     1] loss: 0.543
[5,     1] loss: 0.463
[6,     1] loss: 0.395
[7,     1] loss: 0.339
[8,     1] loss: 0.291
[9,     1] loss: 0.245
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003611949307549653,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 599970861,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.237716334403867}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.681
[3,     1] loss: 0.661
[4,     1] loss: 0.645
[5,     1] loss: 0.632
[6,     1] loss: 0.621
[7,     1] loss: 0.611
[8,     1] loss: 0.601
[9,     1] loss: 0.589
[10,     1] loss: 0.583
[11,     1] loss: 0.571
[12,     1] loss: 0.563
[13,     1] loss: 0.555
[14,     1] loss: 0.551
[15,     1] loss: 0.538
[16,     1] loss: 0.532
[17,     1] loss: 0.522
[18,     1] loss: 0.514
[19,     1] loss: 0.506
[20,     1] loss: 0.497
[21,     1] loss: 0.490
[22,     1] loss: 0.482
[23,     1] loss: 0.471
[24,     1] loss: 0.465
[25,     1] loss: 0.454
[26,     1] loss: 0.446
[27,     1] loss: 0.439
[28,     1] loss: 0.429
[29,     1] loss: 0.420
[30,     1] loss: 0.414
[31,     1] loss: 0.402
[32,     1] loss: 0.393
[33,     1] loss: 0.382
[34,     1] loss: 0.372
[35,     1] loss: 0.364
[36,     1] loss: 0.359
[37,     1] loss: 0.346
[38,     1] loss: 0.339
[39,     1] loss: 0.329
[40,     1] loss: 0.320
[41,     1] loss: 0.315
[42,     1] loss: 0.303
[43,     1] loss: 0.295
[44,     1] loss: 0.285
[45,     1] loss: 0.280
[46,     1] loss: 0.273
[47,     1] loss: 0.259
[48,     1] loss: 0.254
[49,     1] loss: 0.248
[50,     1] loss: 0.242
[51,     1] loss: 0.235
[52,     1] loss: 0.225
[53,     1] loss: 0.221
[54,     1] loss: 0.212
[55,     1] loss: 0.205
[56,     1] loss: 0.202
[57,     1] loss: 0.195
[58,     1] loss: 0.187
[59,     1] loss: 0.181
[60,     1] loss: 0.178
[61,     1] loss: 0.173
[62,     1] loss: 0.166
[63,     1] loss: 0.161
[64,     1] loss: 0.156
[65,     1] loss: 0.154
[66,     1] loss: 0.149
[67,     1] loss: 0.144
[68,     1] loss: 0.141
[69,     1] loss: 0.136
[70,     1] loss: 0.132
[71,     1] loss: 0.129
[72,     1] loss: 0.127
[73,     1] loss: 0.123
[74,     1] loss: 0.122
[75,     1] loss: 0.117
[76,     1] loss: 0.114
[77,     1] loss: 0.110
[78,     1] loss: 0.109
[79,     1] loss: 0.105
[80,     1] loss: 0.104
[81,     1] loss: 0.102
[82,     1] loss: 0.100
[83,     1] loss: 0.096
[84,     1] loss: 0.095
[85,     1] loss: 0.094
[86,     1] loss: 0.092
[87,     1] loss: 0.089
[88,     1] loss: 0.087
[89,     1] loss: 0.088
[90,     1] loss: 0.083
[91,     1] loss: 0.083
[92,     1] loss: 0.081
[93,     1] loss: 0.080
[94,     1] loss: 0.081
[95,     1] loss: 0.078
[96,     1] loss: 0.077
[97,     1] loss: 0.077
[98,     1] loss: 0.074
[99,     1] loss: 0.074
[100,     1] loss: 0.072
[101,     1] loss: 0.071
[102,     1] loss: 0.070
[103,     1] loss: 0.071
[104,     1] loss: 0.067
[105,     1] loss: 0.069
[106,     1] loss: 0.068
[107,     1] loss: 0.066
[108,     1] loss: 0.065
[109,     1] loss: 0.063
[110,     1] loss: 0.063
Early stopping applied (best metric=0.2883502244949341)
Finished Training
Total time taken: 437.40876269340515
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.681
[3,     1] loss: 0.669
[4,     1] loss: 0.661
[5,     1] loss: 0.653
[6,     1] loss: 0.649
[7,     1] loss: 0.643
[8,     1] loss: 0.636
[9,     1] loss: 0.631
[10,     1] loss: 0.627
[11,     1] loss: 0.622
[12,     1] loss: 0.618
[13,     1] loss: 0.613
[14,     1] loss: 0.608
[15,     1] loss: 0.604
[16,     1] loss: 0.600
[17,     1] loss: 0.594
[18,     1] loss: 0.589
[19,     1] loss: 0.584
[20,     1] loss: 0.579
[21,     1] loss: 0.574
[22,     1] loss: 0.567
[23,     1] loss: 0.562
[24,     1] loss: 0.557
[25,     1] loss: 0.549
[26,     1] loss: 0.541
[27,     1] loss: 0.533
[28,     1] loss: 0.527
[29,     1] loss: 0.519
[30,     1] loss: 0.510
[31,     1] loss: 0.503
[32,     1] loss: 0.497
[33,     1] loss: 0.486
[34,     1] loss: 0.481
[35,     1] loss: 0.470
[36,     1] loss: 0.463
[37,     1] loss: 0.454
[38,     1] loss: 0.443
[39,     1] loss: 0.435
[40,     1] loss: 0.427
[41,     1] loss: 0.418
[42,     1] loss: 0.412
[43,     1] loss: 0.402
[44,     1] loss: 0.393
[45,     1] loss: 0.383
[46,     1] loss: 0.374
[47,     1] loss: 0.369
[48,     1] loss: 0.360
[49,     1] loss: 0.351
[50,     1] loss: 0.339
[51,     1] loss: 0.331
[52,     1] loss: 0.327
[53,     1] loss: 0.318
[54,     1] loss: 0.309
[55,     1] loss: 0.301
[56,     1] loss: 0.294
[57,     1] loss: 0.287
[58,     1] loss: 0.277
[59,     1] loss: 0.274
[60,     1] loss: 0.262
[61,     1] loss: 0.256
[62,     1] loss: 0.250
[63,     1] loss: 0.241
[64,     1] loss: 0.236
[65,     1] loss: 0.230
[66,     1] loss: 0.222
[67,     1] loss: 0.214
[68,     1] loss: 0.209
[69,     1] loss: 0.204
[70,     1] loss: 0.198
[71,     1] loss: 0.189
[72,     1] loss: 0.185
[73,     1] loss: 0.181
[74,     1] loss: 0.177
[75,     1] loss: 0.172
[76,     1] loss: 0.165
[77,     1] loss: 0.163
[78,     1] loss: 0.158
[79,     1] loss: 0.153
[80,     1] loss: 0.148
[81,     1] loss: 0.148
[82,     1] loss: 0.140
[83,     1] loss: 0.137
[84,     1] loss: 0.132
[85,     1] loss: 0.131
[86,     1] loss: 0.125
[87,     1] loss: 0.124
[88,     1] loss: 0.122
[89,     1] loss: 0.117
[90,     1] loss: 0.113
[91,     1] loss: 0.112
[92,     1] loss: 0.110
[93,     1] loss: 0.105
[94,     1] loss: 0.104
[95,     1] loss: 0.101
[96,     1] loss: 0.099
[97,     1] loss: 0.096
[98,     1] loss: 0.095
[99,     1] loss: 0.093
[100,     1] loss: 0.089
[101,     1] loss: 0.088
[102,     1] loss: 0.085
[103,     1] loss: 0.085
[104,     1] loss: 0.081
[105,     1] loss: 0.081
[106,     1] loss: 0.078
[107,     1] loss: 0.077
[108,     1] loss: 0.075
[109,     1] loss: 0.074
[110,     1] loss: 0.072
[111,     1] loss: 0.072
[112,     1] loss: 0.069
[113,     1] loss: 0.069
[114,     1] loss: 0.069
[115,     1] loss: 0.066
[116,     1] loss: 0.066
[117,     1] loss: 0.065
[118,     1] loss: 0.064
[119,     1] loss: 0.062
[120,     1] loss: 0.061
[121,     1] loss: 0.061
[122,     1] loss: 0.061
[123,     1] loss: 0.058
[124,     1] loss: 0.059
[125,     1] loss: 0.057
[126,     1] loss: 0.055
[127,     1] loss: 0.056
[128,     1] loss: 0.056
[129,     1] loss: 0.056
[130,     1] loss: 0.054
[131,     1] loss: 0.053
[132,     1] loss: 0.054
[133,     1] loss: 0.053
[134,     1] loss: 0.053
[135,     1] loss: 0.051
[136,     1] loss: 0.052
[137,     1] loss: 0.050
Early stopping applied (best metric=0.29383584856987)
Finished Training
Total time taken: 544.9316473007202
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.668
[4,     1] loss: 0.653
[5,     1] loss: 0.639
[6,     1] loss: 0.627
[7,     1] loss: 0.620
[8,     1] loss: 0.610
[9,     1] loss: 0.601
[10,     1] loss: 0.589
[11,     1] loss: 0.583
[12,     1] loss: 0.573
[13,     1] loss: 0.567
[14,     1] loss: 0.556
[15,     1] loss: 0.550
[16,     1] loss: 0.538
[17,     1] loss: 0.531
[18,     1] loss: 0.521
[19,     1] loss: 0.511
[20,     1] loss: 0.503
[21,     1] loss: 0.496
[22,     1] loss: 0.487
[23,     1] loss: 0.479
[24,     1] loss: 0.468
[25,     1] loss: 0.460
[26,     1] loss: 0.453
[27,     1] loss: 0.441
[28,     1] loss: 0.431
[29,     1] loss: 0.422
[30,     1] loss: 0.414
[31,     1] loss: 0.406
[32,     1] loss: 0.398
[33,     1] loss: 0.389
[34,     1] loss: 0.380
[35,     1] loss: 0.374
[36,     1] loss: 0.363
[37,     1] loss: 0.353
[38,     1] loss: 0.344
[39,     1] loss: 0.334
[40,     1] loss: 0.326
[41,     1] loss: 0.320
[42,     1] loss: 0.310
[43,     1] loss: 0.302
[44,     1] loss: 0.293
[45,     1] loss: 0.285
[46,     1] loss: 0.278
[47,     1] loss: 0.268
[48,     1] loss: 0.262
[49,     1] loss: 0.254
[50,     1] loss: 0.244
[51,     1] loss: 0.235
[52,     1] loss: 0.232
[53,     1] loss: 0.225
[54,     1] loss: 0.216
[55,     1] loss: 0.211
[56,     1] loss: 0.205
[57,     1] loss: 0.198
[58,     1] loss: 0.192
[59,     1] loss: 0.185
[60,     1] loss: 0.180
[61,     1] loss: 0.177
[62,     1] loss: 0.172
[63,     1] loss: 0.166
[64,     1] loss: 0.163
[65,     1] loss: 0.155
[66,     1] loss: 0.151
[67,     1] loss: 0.145
[68,     1] loss: 0.142
[69,     1] loss: 0.137
[70,     1] loss: 0.136
[71,     1] loss: 0.129
[72,     1] loss: 0.125
[73,     1] loss: 0.123
[74,     1] loss: 0.119
[75,     1] loss: 0.118
[76,     1] loss: 0.112
[77,     1] loss: 0.109
[78,     1] loss: 0.104
[79,     1] loss: 0.104
[80,     1] loss: 0.101
[81,     1] loss: 0.096
[82,     1] loss: 0.094
[83,     1] loss: 0.092
[84,     1] loss: 0.092
[85,     1] loss: 0.088
[86,     1] loss: 0.086
[87,     1] loss: 0.084
[88,     1] loss: 0.083
[89,     1] loss: 0.081
[90,     1] loss: 0.078
[91,     1] loss: 0.077
[92,     1] loss: 0.075
[93,     1] loss: 0.075
[94,     1] loss: 0.071
[95,     1] loss: 0.071
[96,     1] loss: 0.069
[97,     1] loss: 0.067
[98,     1] loss: 0.067
[99,     1] loss: 0.066
[100,     1] loss: 0.065
[101,     1] loss: 0.064
[102,     1] loss: 0.062
[103,     1] loss: 0.062
[104,     1] loss: 0.060
[105,     1] loss: 0.058
[106,     1] loss: 0.058
[107,     1] loss: 0.059
[108,     1] loss: 0.057
[109,     1] loss: 0.056
[110,     1] loss: 0.056
[111,     1] loss: 0.055
[112,     1] loss: 0.054
[113,     1] loss: 0.052
Early stopping applied (best metric=0.2944013476371765)
Finished Training
Total time taken: 453.01962995529175
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.661
[4,     1] loss: 0.648
[5,     1] loss: 0.634
[6,     1] loss: 0.624
[7,     1] loss: 0.614
[8,     1] loss: 0.607
[9,     1] loss: 0.598
[10,     1] loss: 0.589
[11,     1] loss: 0.582
[12,     1] loss: 0.574
[13,     1] loss: 0.568
[14,     1] loss: 0.561
[15,     1] loss: 0.554
[16,     1] loss: 0.545
[17,     1] loss: 0.537
[18,     1] loss: 0.532
[19,     1] loss: 0.526
[20,     1] loss: 0.518
[21,     1] loss: 0.511
[22,     1] loss: 0.505
[23,     1] loss: 0.498
[24,     1] loss: 0.489
[25,     1] loss: 0.482
[26,     1] loss: 0.476
[27,     1] loss: 0.469
[28,     1] loss: 0.459
[29,     1] loss: 0.450
[30,     1] loss: 0.445
[31,     1] loss: 0.436
[32,     1] loss: 0.433
[33,     1] loss: 0.423
[34,     1] loss: 0.417
[35,     1] loss: 0.409
[36,     1] loss: 0.404
[37,     1] loss: 0.394
[38,     1] loss: 0.385
[39,     1] loss: 0.377
[40,     1] loss: 0.370
[41,     1] loss: 0.363
[42,     1] loss: 0.356
[43,     1] loss: 0.348
[44,     1] loss: 0.341
[45,     1] loss: 0.336
[46,     1] loss: 0.327
[47,     1] loss: 0.317
[48,     1] loss: 0.313
[49,     1] loss: 0.305
[50,     1] loss: 0.296
[51,     1] loss: 0.292
[52,     1] loss: 0.286
[53,     1] loss: 0.278
[54,     1] loss: 0.267
[55,     1] loss: 0.264
[56,     1] loss: 0.257
[57,     1] loss: 0.253
[58,     1] loss: 0.244
[59,     1] loss: 0.237
[60,     1] loss: 0.231
[61,     1] loss: 0.225
[62,     1] loss: 0.222
[63,     1] loss: 0.216
[64,     1] loss: 0.208
[65,     1] loss: 0.205
[66,     1] loss: 0.199
[67,     1] loss: 0.193
[68,     1] loss: 0.188
[69,     1] loss: 0.181
[70,     1] loss: 0.181
[71,     1] loss: 0.175
[72,     1] loss: 0.171
[73,     1] loss: 0.167
[74,     1] loss: 0.164
[75,     1] loss: 0.158
[76,     1] loss: 0.154
[77,     1] loss: 0.151
[78,     1] loss: 0.145
[79,     1] loss: 0.141
[80,     1] loss: 0.139
[81,     1] loss: 0.136
[82,     1] loss: 0.132
[83,     1] loss: 0.129
[84,     1] loss: 0.127
[85,     1] loss: 0.124
[86,     1] loss: 0.122
[87,     1] loss: 0.118
[88,     1] loss: 0.115
[89,     1] loss: 0.113
[90,     1] loss: 0.112
[91,     1] loss: 0.110
[92,     1] loss: 0.109
[93,     1] loss: 0.106
[94,     1] loss: 0.102
[95,     1] loss: 0.101
[96,     1] loss: 0.101
[97,     1] loss: 0.097
[98,     1] loss: 0.094
[99,     1] loss: 0.093
[100,     1] loss: 0.091
[101,     1] loss: 0.089
[102,     1] loss: 0.088
[103,     1] loss: 0.087
[104,     1] loss: 0.085
[105,     1] loss: 0.085
[106,     1] loss: 0.082
[107,     1] loss: 0.083
[108,     1] loss: 0.080
[109,     1] loss: 0.080
[110,     1] loss: 0.079
[111,     1] loss: 0.077
[112,     1] loss: 0.077
[113,     1] loss: 0.074
[114,     1] loss: 0.074
[115,     1] loss: 0.072
[116,     1] loss: 0.072
[117,     1] loss: 0.070
[118,     1] loss: 0.071
[119,     1] loss: 0.069
[120,     1] loss: 0.068
[121,     1] loss: 0.066
[122,     1] loss: 0.067
[123,     1] loss: 0.067
[124,     1] loss: 0.065
[125,     1] loss: 0.065
[126,     1] loss: 0.064
[127,     1] loss: 0.064
[128,     1] loss: 0.061
[129,     1] loss: 0.062
[130,     1] loss: 0.061
[131,     1] loss: 0.061
[132,     1] loss: 0.060
[133,     1] loss: 0.060
[134,     1] loss: 0.058
Early stopping applied (best metric=0.28758037090301514)
Finished Training
Total time taken: 558.327968120575
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.657
[4,     1] loss: 0.642
[5,     1] loss: 0.627
[6,     1] loss: 0.618
[7,     1] loss: 0.607
[8,     1] loss: 0.595
[9,     1] loss: 0.587
[10,     1] loss: 0.576
[11,     1] loss: 0.566
[12,     1] loss: 0.557
[13,     1] loss: 0.551
[14,     1] loss: 0.541
[15,     1] loss: 0.531
[16,     1] loss: 0.523
[17,     1] loss: 0.513
[18,     1] loss: 0.508
[19,     1] loss: 0.496
[20,     1] loss: 0.491
[21,     1] loss: 0.480
[22,     1] loss: 0.471
[23,     1] loss: 0.462
[24,     1] loss: 0.455
[25,     1] loss: 0.444
[26,     1] loss: 0.438
[27,     1] loss: 0.428
[28,     1] loss: 0.420
[29,     1] loss: 0.412
[30,     1] loss: 0.399
[31,     1] loss: 0.390
[32,     1] loss: 0.382
[33,     1] loss: 0.370
[34,     1] loss: 0.362
[35,     1] loss: 0.351
[36,     1] loss: 0.344
[37,     1] loss: 0.336
[38,     1] loss: 0.325
[39,     1] loss: 0.316
[40,     1] loss: 0.304
[41,     1] loss: 0.296
[42,     1] loss: 0.286
[43,     1] loss: 0.276
[44,     1] loss: 0.272
[45,     1] loss: 0.259
[46,     1] loss: 0.250
[47,     1] loss: 0.246
[48,     1] loss: 0.235
[49,     1] loss: 0.227
[50,     1] loss: 0.215
[51,     1] loss: 0.213
[52,     1] loss: 0.204
[53,     1] loss: 0.198
[54,     1] loss: 0.188
[55,     1] loss: 0.183
[56,     1] loss: 0.174
[57,     1] loss: 0.172
[58,     1] loss: 0.164
[59,     1] loss: 0.155
[60,     1] loss: 0.153
[61,     1] loss: 0.145
[62,     1] loss: 0.143
[63,     1] loss: 0.135
[64,     1] loss: 0.132
[65,     1] loss: 0.128
[66,     1] loss: 0.123
[67,     1] loss: 0.119
[68,     1] loss: 0.113
[69,     1] loss: 0.109
[70,     1] loss: 0.107
[71,     1] loss: 0.102
[72,     1] loss: 0.099
[73,     1] loss: 0.097
[74,     1] loss: 0.094
[75,     1] loss: 0.091
[76,     1] loss: 0.088
[77,     1] loss: 0.086
[78,     1] loss: 0.083
[79,     1] loss: 0.079
[80,     1] loss: 0.078
[81,     1] loss: 0.076
[82,     1] loss: 0.074
[83,     1] loss: 0.071
[84,     1] loss: 0.069
[85,     1] loss: 0.069
[86,     1] loss: 0.066
[87,     1] loss: 0.064
[88,     1] loss: 0.063
[89,     1] loss: 0.063
[90,     1] loss: 0.061
[91,     1] loss: 0.060
[92,     1] loss: 0.060
[93,     1] loss: 0.057
[94,     1] loss: 0.056
[95,     1] loss: 0.055
[96,     1] loss: 0.054
[97,     1] loss: 0.053
[98,     1] loss: 0.052
[99,     1] loss: 0.051
[100,     1] loss: 0.051
[101,     1] loss: 0.049
[102,     1] loss: 0.049
[103,     1] loss: 0.049
[104,     1] loss: 0.047
[105,     1] loss: 0.046
[106,     1] loss: 0.046
[107,     1] loss: 0.045
Early stopping applied (best metric=0.28808724880218506)
Finished Training
Total time taken: 440.3374717235565
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8425806451612903, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8666666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8381679389312977, 'Pyrrolidone carboxylic acid Validation Precision': 0.49889777670574775, 'Pyrrolidone carboxylic acid AUC ROC': 0.8899968193384225, 'Pyrrolidone carboxylic acid AUC PR': 0.5384461145531901, 'Pyrrolidone carboxylic acid MCC': 0.5757311786025395, 'Pyrrolidone carboxylic acid F1': 0.6317809112309303, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.29045100808143615, 'Validation Loss (total)': 0.29045100808143615}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009947118414003014,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2353853723,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.295905145399523}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.683
[3,     1] loss: 0.631
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00808641071845773,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1723461683,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.389905841374967}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.674
[3,     1] loss: 0.616
[4,     1] loss: 0.538
[5,     1] loss: 0.463
[6,     1] loss: 0.394
[7,     1] loss: 0.318
[8,     1] loss: 0.234
[9,     1] loss: 0.169
[10,     1] loss: 0.118
[11,     1] loss: 0.089
[12,     1] loss: 0.066
[13,     1] loss: 0.051
[14,     1] loss: 0.041
[15,     1] loss: 0.041
[16,     1] loss: 0.023
[17,     1] loss: 0.015
[18,     1] loss: 0.009
[19,     1] loss: 0.009
[20,     1] loss: 0.012
[21,     1] loss: 0.004
[22,     1] loss: 0.004
[23,     1] loss: 0.004
[24,     1] loss: 0.015
[25,     1] loss: 0.078
[26,     1] loss: 0.004
[27,     1] loss: 0.062
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005589032548226013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2449436928,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.403104826350047}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.674
[4,     1] loss: 0.663
[5,     1] loss: 0.652
[6,     1] loss: 0.643
[7,     1] loss: 0.635
[8,     1] loss: 0.626
[9,     1] loss: 0.618
[10,     1] loss: 0.611
[11,     1] loss: 0.603
[12,     1] loss: 0.595
[13,     1] loss: 0.586
[14,     1] loss: 0.579
[15,     1] loss: 0.569
[16,     1] loss: 0.561
[17,     1] loss: 0.552
[18,     1] loss: 0.543
[19,     1] loss: 0.531
[20,     1] loss: 0.523
[21,     1] loss: 0.513
[22,     1] loss: 0.502
[23,     1] loss: 0.490
[24,     1] loss: 0.478
[25,     1] loss: 0.469
[26,     1] loss: 0.455
[27,     1] loss: 0.443
[28,     1] loss: 0.432
[29,     1] loss: 0.419
[30,     1] loss: 0.407
[31,     1] loss: 0.394
[32,     1] loss: 0.381
[33,     1] loss: 0.373
[34,     1] loss: 0.358
[35,     1] loss: 0.345
[36,     1] loss: 0.333
[37,     1] loss: 0.321
[38,     1] loss: 0.311
[39,     1] loss: 0.299
[40,     1] loss: 0.289
[41,     1] loss: 0.279
[42,     1] loss: 0.267
[43,     1] loss: 0.254
[44,     1] loss: 0.245
[45,     1] loss: 0.235
[46,     1] loss: 0.227
[47,     1] loss: 0.216
[48,     1] loss: 0.207
[49,     1] loss: 0.201
[50,     1] loss: 0.193
[51,     1] loss: 0.185
[52,     1] loss: 0.178
[53,     1] loss: 0.170
[54,     1] loss: 0.163
[55,     1] loss: 0.154
[56,     1] loss: 0.150
[57,     1] loss: 0.144
[58,     1] loss: 0.137
[59,     1] loss: 0.134
[60,     1] loss: 0.128
[61,     1] loss: 0.125
[62,     1] loss: 0.118
[63,     1] loss: 0.114
[64,     1] loss: 0.110
[65,     1] loss: 0.106
[66,     1] loss: 0.100
[67,     1] loss: 0.098
[68,     1] loss: 0.095
[69,     1] loss: 0.092
[70,     1] loss: 0.089
[71,     1] loss: 0.086
[72,     1] loss: 0.082
[73,     1] loss: 0.079
[74,     1] loss: 0.076
[75,     1] loss: 0.074
[76,     1] loss: 0.072
[77,     1] loss: 0.069
[78,     1] loss: 0.067
[79,     1] loss: 0.066
[80,     1] loss: 0.063
[81,     1] loss: 0.062
[82,     1] loss: 0.059
[83,     1] loss: 0.058
[84,     1] loss: 0.057
[85,     1] loss: 0.056
[86,     1] loss: 0.055
[87,     1] loss: 0.053
[88,     1] loss: 0.052
[89,     1] loss: 0.053
[90,     1] loss: 0.050
[91,     1] loss: 0.049
[92,     1] loss: 0.048
[93,     1] loss: 0.047
[94,     1] loss: 0.045
[95,     1] loss: 0.046
[96,     1] loss: 0.044
[97,     1] loss: 0.044
[98,     1] loss: 0.044
[99,     1] loss: 0.043
[100,     1] loss: 0.042
[101,     1] loss: 0.042
[102,     1] loss: 0.041
[103,     1] loss: 0.040
[104,     1] loss: 0.040
[105,     1] loss: 0.040
[106,     1] loss: 0.041
[107,     1] loss: 0.040
[108,     1] loss: 0.040
[109,     1] loss: 0.038
[110,     1] loss: 0.038
[111,     1] loss: 0.038
[112,     1] loss: 0.039
[113,     1] loss: 0.039
[114,     1] loss: 0.038
[115,     1] loss: 0.037
[116,     1] loss: 0.038
[117,     1] loss: 0.037
[118,     1] loss: 0.037
[119,     1] loss: 0.038
[120,     1] loss: 0.038
[121,     1] loss: 0.038
[122,     1] loss: 0.037
[123,     1] loss: 0.039
[124,     1] loss: 0.039
[125,     1] loss: 0.037
Early stopping applied (best metric=0.23424655199050903)
Finished Training
Total time taken: 513.0883636474609
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.694
[3,     1] loss: 0.677
[4,     1] loss: 0.662
[5,     1] loss: 0.648
[6,     1] loss: 0.635
[7,     1] loss: 0.624
[8,     1] loss: 0.614
[9,     1] loss: 0.602
[10,     1] loss: 0.592
[11,     1] loss: 0.581
[12,     1] loss: 0.571
[13,     1] loss: 0.559
[14,     1] loss: 0.548
[15,     1] loss: 0.541
[16,     1] loss: 0.528
[17,     1] loss: 0.518
[18,     1] loss: 0.510
[19,     1] loss: 0.497
[20,     1] loss: 0.486
[21,     1] loss: 0.475
[22,     1] loss: 0.465
[23,     1] loss: 0.454
[24,     1] loss: 0.443
[25,     1] loss: 0.433
[26,     1] loss: 0.420
[27,     1] loss: 0.412
[28,     1] loss: 0.398
[29,     1] loss: 0.388
[30,     1] loss: 0.381
[31,     1] loss: 0.368
[32,     1] loss: 0.357
[33,     1] loss: 0.347
[34,     1] loss: 0.336
[35,     1] loss: 0.328
[36,     1] loss: 0.316
[37,     1] loss: 0.309
[38,     1] loss: 0.295
[39,     1] loss: 0.284
[40,     1] loss: 0.277
[41,     1] loss: 0.268
[42,     1] loss: 0.259
[43,     1] loss: 0.246
[44,     1] loss: 0.241
[45,     1] loss: 0.230
[46,     1] loss: 0.223
[47,     1] loss: 0.209
[48,     1] loss: 0.201
[49,     1] loss: 0.196
[50,     1] loss: 0.187
[51,     1] loss: 0.177
[52,     1] loss: 0.170
[53,     1] loss: 0.165
[54,     1] loss: 0.158
[55,     1] loss: 0.153
[56,     1] loss: 0.144
[57,     1] loss: 0.139
[58,     1] loss: 0.134
[59,     1] loss: 0.126
[60,     1] loss: 0.122
[61,     1] loss: 0.118
[62,     1] loss: 0.113
[63,     1] loss: 0.110
[64,     1] loss: 0.103
[65,     1] loss: 0.102
[66,     1] loss: 0.099
[67,     1] loss: 0.095
[68,     1] loss: 0.091
[69,     1] loss: 0.087
[70,     1] loss: 0.083
[71,     1] loss: 0.081
[72,     1] loss: 0.077
[73,     1] loss: 0.075
[74,     1] loss: 0.074
[75,     1] loss: 0.071
[76,     1] loss: 0.069
[77,     1] loss: 0.068
[78,     1] loss: 0.064
[79,     1] loss: 0.063
[80,     1] loss: 0.062
[81,     1] loss: 0.058
[82,     1] loss: 0.058
[83,     1] loss: 0.056
[84,     1] loss: 0.057
[85,     1] loss: 0.054
[86,     1] loss: 0.052
[87,     1] loss: 0.053
[88,     1] loss: 0.052
[89,     1] loss: 0.051
[90,     1] loss: 0.050
[91,     1] loss: 0.049
[92,     1] loss: 0.048
[93,     1] loss: 0.047
[94,     1] loss: 0.047
[95,     1] loss: 0.047
[96,     1] loss: 0.045
[97,     1] loss: 0.045
[98,     1] loss: 0.044
[99,     1] loss: 0.044
[100,     1] loss: 0.043
[101,     1] loss: 0.043
[102,     1] loss: 0.042
[103,     1] loss: 0.042
[104,     1] loss: 0.042
[105,     1] loss: 0.041
[106,     1] loss: 0.040
[107,     1] loss: 0.040
[108,     1] loss: 0.040
[109,     1] loss: 0.040
[110,     1] loss: 0.039
[111,     1] loss: 0.039
[112,     1] loss: 0.039
[113,     1] loss: 0.039
[114,     1] loss: 0.038
[115,     1] loss: 0.039
[116,     1] loss: 0.039
[117,     1] loss: 0.038
[118,     1] loss: 0.039
[119,     1] loss: 0.038
[120,     1] loss: 0.038
[121,     1] loss: 0.037
[122,     1] loss: 0.037
[123,     1] loss: 0.037
[124,     1] loss: 0.038
[125,     1] loss: 0.037
[126,     1] loss: 0.037
[127,     1] loss: 0.036
[128,     1] loss: 0.036
[129,     1] loss: 0.037
[130,     1] loss: 0.036
[131,     1] loss: 0.036
[132,     1] loss: 0.036
[133,     1] loss: 0.036
[134,     1] loss: 0.036
[135,     1] loss: 0.036
[136,     1] loss: 0.036
[137,     1] loss: 0.035
[138,     1] loss: 0.035
[139,     1] loss: 0.035
[140,     1] loss: 0.036
[141,     1] loss: 0.036
[142,     1] loss: 0.036
[143,     1] loss: 0.035
[144,     1] loss: 0.036
[145,     1] loss: 0.035
Early stopping applied (best metric=0.2526717483997345)
Finished Training
Total time taken: 611.270745754242
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.716
[2,     1] loss: 0.686
[3,     1] loss: 0.664
[4,     1] loss: 0.647
[5,     1] loss: 0.632
[6,     1] loss: 0.618
[7,     1] loss: 0.605
[8,     1] loss: 0.594
[9,     1] loss: 0.580
[10,     1] loss: 0.568
[11,     1] loss: 0.557
[12,     1] loss: 0.545
[13,     1] loss: 0.535
[14,     1] loss: 0.522
[15,     1] loss: 0.513
[16,     1] loss: 0.502
[17,     1] loss: 0.491
[18,     1] loss: 0.481
[19,     1] loss: 0.471
[20,     1] loss: 0.462
[21,     1] loss: 0.453
[22,     1] loss: 0.442
[23,     1] loss: 0.435
[24,     1] loss: 0.426
[25,     1] loss: 0.419
[26,     1] loss: 0.410
[27,     1] loss: 0.404
[28,     1] loss: 0.396
[29,     1] loss: 0.390
[30,     1] loss: 0.382
[31,     1] loss: 0.378
[32,     1] loss: 0.372
[33,     1] loss: 0.365
[34,     1] loss: 0.361
[35,     1] loss: 0.356
[36,     1] loss: 0.350
[37,     1] loss: 0.344
[38,     1] loss: 0.338
[39,     1] loss: 0.334
[40,     1] loss: 0.328
[41,     1] loss: 0.323
[42,     1] loss: 0.319
[43,     1] loss: 0.314
[44,     1] loss: 0.309
[45,     1] loss: 0.303
[46,     1] loss: 0.299
[47,     1] loss: 0.294
[48,     1] loss: 0.287
[49,     1] loss: 0.283
[50,     1] loss: 0.279
[51,     1] loss: 0.273
[52,     1] loss: 0.267
[53,     1] loss: 0.262
[54,     1] loss: 0.257
[55,     1] loss: 0.251
[56,     1] loss: 0.246
[57,     1] loss: 0.241
[58,     1] loss: 0.232
[59,     1] loss: 0.230
[60,     1] loss: 0.225
[61,     1] loss: 0.220
[62,     1] loss: 0.213
[63,     1] loss: 0.207
[64,     1] loss: 0.204
[65,     1] loss: 0.197
[66,     1] loss: 0.192
[67,     1] loss: 0.186
[68,     1] loss: 0.180
[69,     1] loss: 0.175
[70,     1] loss: 0.170
[71,     1] loss: 0.165
[72,     1] loss: 0.159
[73,     1] loss: 0.154
[74,     1] loss: 0.149
[75,     1] loss: 0.144
[76,     1] loss: 0.139
[77,     1] loss: 0.134
[78,     1] loss: 0.132
[79,     1] loss: 0.124
[80,     1] loss: 0.123
[81,     1] loss: 0.116
[82,     1] loss: 0.113
[83,     1] loss: 0.110
[84,     1] loss: 0.106
[85,     1] loss: 0.103
[86,     1] loss: 0.099
[87,     1] loss: 0.095
[88,     1] loss: 0.094
[89,     1] loss: 0.091
[90,     1] loss: 0.088
[91,     1] loss: 0.086
[92,     1] loss: 0.084
[93,     1] loss: 0.081
[94,     1] loss: 0.081
[95,     1] loss: 0.079
[96,     1] loss: 0.077
[97,     1] loss: 0.074
[98,     1] loss: 0.073
[99,     1] loss: 0.073
[100,     1] loss: 0.070
[101,     1] loss: 0.069
[102,     1] loss: 0.068
[103,     1] loss: 0.067
[104,     1] loss: 0.065
[105,     1] loss: 0.064
[106,     1] loss: 0.063
[107,     1] loss: 0.064
[108,     1] loss: 0.062
[109,     1] loss: 0.062
[110,     1] loss: 0.062
[111,     1] loss: 0.059
[112,     1] loss: 0.060
[113,     1] loss: 0.060
[114,     1] loss: 0.059
[115,     1] loss: 0.057
[116,     1] loss: 0.058
[117,     1] loss: 0.058
[118,     1] loss: 0.057
[119,     1] loss: 0.058
[120,     1] loss: 0.056
[121,     1] loss: 0.056
[122,     1] loss: 0.056
[123,     1] loss: 0.056
[124,     1] loss: 0.056
[125,     1] loss: 0.055
[126,     1] loss: 0.055
[127,     1] loss: 0.056
[128,     1] loss: 0.054
[129,     1] loss: 0.055
[130,     1] loss: 0.055
[131,     1] loss: 0.055
[132,     1] loss: 0.055
[133,     1] loss: 0.056
[134,     1] loss: 0.054
[135,     1] loss: 0.054
[136,     1] loss: 0.054
[137,     1] loss: 0.055
[138,     1] loss: 0.054
[139,     1] loss: 0.055
[140,     1] loss: 0.054
[141,     1] loss: 0.055
[142,     1] loss: 0.054
[143,     1] loss: 0.055
[144,     1] loss: 0.055
[145,     1] loss: 0.054
[146,     1] loss: 0.055
[147,     1] loss: 0.053
[148,     1] loss: 0.056
[149,     1] loss: 0.055
[150,     1] loss: 0.054
[151,     1] loss: 0.054
[152,     1] loss: 0.053
[153,     1] loss: 0.054
[154,     1] loss: 0.055
[155,     1] loss: 0.053
[156,     1] loss: 0.055
[157,     1] loss: 0.056
[158,     1] loss: 0.055
[159,     1] loss: 0.054
[160,     1] loss: 0.056
[161,     1] loss: 0.057
[162,     1] loss: 0.056
[163,     1] loss: 0.056
[164,     1] loss: 0.055
[165,     1] loss: 0.056
[166,     1] loss: 0.054
[167,     1] loss: 0.054
[168,     1] loss: 0.056
[169,     1] loss: 0.055
[170,     1] loss: 0.055
[171,     1] loss: 0.055
[172,     1] loss: 0.054
[173,     1] loss: 0.056
[174,     1] loss: 0.056
[175,     1] loss: 0.056
[176,     1] loss: 0.056
[177,     1] loss: 0.055
[178,     1] loss: 0.055
[179,     1] loss: 0.057
[180,     1] loss: 0.054
[181,     1] loss: 0.055
[182,     1] loss: 0.057
[183,     1] loss: 0.055
Early stopping applied (best metric=0.2638179659843445)
Finished Training
Total time taken: 756.1673359870911
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.699
[3,     1] loss: 0.688
[4,     1] loss: 0.680
[5,     1] loss: 0.673
[6,     1] loss: 0.666
[7,     1] loss: 0.658
[8,     1] loss: 0.653
[9,     1] loss: 0.644
[10,     1] loss: 0.638
[11,     1] loss: 0.632
[12,     1] loss: 0.624
[13,     1] loss: 0.617
[14,     1] loss: 0.609
[15,     1] loss: 0.602
[16,     1] loss: 0.594
[17,     1] loss: 0.589
[18,     1] loss: 0.580
[19,     1] loss: 0.573
[20,     1] loss: 0.566
[21,     1] loss: 0.557
[22,     1] loss: 0.551
[23,     1] loss: 0.542
[24,     1] loss: 0.536
[25,     1] loss: 0.528
[26,     1] loss: 0.519
[27,     1] loss: 0.513
[28,     1] loss: 0.504
[29,     1] loss: 0.496
[30,     1] loss: 0.490
[31,     1] loss: 0.484
[32,     1] loss: 0.477
[33,     1] loss: 0.470
[34,     1] loss: 0.464
[35,     1] loss: 0.456
[36,     1] loss: 0.452
[37,     1] loss: 0.444
[38,     1] loss: 0.439
[39,     1] loss: 0.433
[40,     1] loss: 0.427
[41,     1] loss: 0.423
[42,     1] loss: 0.418
[43,     1] loss: 0.412
[44,     1] loss: 0.408
[45,     1] loss: 0.402
[46,     1] loss: 0.398
[47,     1] loss: 0.393
[48,     1] loss: 0.388
[49,     1] loss: 0.384
[50,     1] loss: 0.378
[51,     1] loss: 0.372
[52,     1] loss: 0.368
[53,     1] loss: 0.362
[54,     1] loss: 0.358
[55,     1] loss: 0.354
[56,     1] loss: 0.346
[57,     1] loss: 0.343
[58,     1] loss: 0.339
[59,     1] loss: 0.331
[60,     1] loss: 0.328
[61,     1] loss: 0.322
[62,     1] loss: 0.317
[63,     1] loss: 0.312
[64,     1] loss: 0.305
[65,     1] loss: 0.298
[66,     1] loss: 0.293
[67,     1] loss: 0.287
[68,     1] loss: 0.280
[69,     1] loss: 0.275
[70,     1] loss: 0.268
[71,     1] loss: 0.261
[72,     1] loss: 0.255
[73,     1] loss: 0.250
[74,     1] loss: 0.243
[75,     1] loss: 0.235
[76,     1] loss: 0.229
[77,     1] loss: 0.219
[78,     1] loss: 0.216
[79,     1] loss: 0.206
[80,     1] loss: 0.197
[81,     1] loss: 0.188
[82,     1] loss: 0.183
[83,     1] loss: 0.172
[84,     1] loss: 0.166
[85,     1] loss: 0.158
[86,     1] loss: 0.153
[87,     1] loss: 0.144
[88,     1] loss: 0.140
[89,     1] loss: 0.131
[90,     1] loss: 0.125
[91,     1] loss: 0.122
[92,     1] loss: 0.119
[93,     1] loss: 0.110
[94,     1] loss: 0.107
[95,     1] loss: 0.101
[96,     1] loss: 0.096
[97,     1] loss: 0.093
[98,     1] loss: 0.089
[99,     1] loss: 0.085
[100,     1] loss: 0.084
[101,     1] loss: 0.079
[102,     1] loss: 0.077
[103,     1] loss: 0.076
[104,     1] loss: 0.072
[105,     1] loss: 0.069
[106,     1] loss: 0.068
[107,     1] loss: 0.065
[108,     1] loss: 0.066
[109,     1] loss: 0.062
[110,     1] loss: 0.061
[111,     1] loss: 0.060
[112,     1] loss: 0.059
[113,     1] loss: 0.058
[114,     1] loss: 0.056
[115,     1] loss: 0.054
[116,     1] loss: 0.055
[117,     1] loss: 0.053
[118,     1] loss: 0.053
[119,     1] loss: 0.052
[120,     1] loss: 0.052
[121,     1] loss: 0.052
[122,     1] loss: 0.050
[123,     1] loss: 0.050
[124,     1] loss: 0.049
[125,     1] loss: 0.048
[126,     1] loss: 0.049
[127,     1] loss: 0.047
[128,     1] loss: 0.048
[129,     1] loss: 0.047
[130,     1] loss: 0.046
[131,     1] loss: 0.046
[132,     1] loss: 0.046
[133,     1] loss: 0.047
[134,     1] loss: 0.047
[135,     1] loss: 0.047
[136,     1] loss: 0.046
[137,     1] loss: 0.045
[138,     1] loss: 0.046
[139,     1] loss: 0.046
[140,     1] loss: 0.046
[141,     1] loss: 0.045
[142,     1] loss: 0.045
[143,     1] loss: 0.045
[144,     1] loss: 0.044
[145,     1] loss: 0.044
[146,     1] loss: 0.044
[147,     1] loss: 0.044
[148,     1] loss: 0.045
[149,     1] loss: 0.043
[150,     1] loss: 0.043
[151,     1] loss: 0.044
[152,     1] loss: 0.044
[153,     1] loss: 0.045
[154,     1] loss: 0.043
[155,     1] loss: 0.043
[156,     1] loss: 0.043
[157,     1] loss: 0.044
[158,     1] loss: 0.044
[159,     1] loss: 0.044
[160,     1] loss: 0.043
[161,     1] loss: 0.044
[162,     1] loss: 0.043
[163,     1] loss: 0.043
[164,     1] loss: 0.043
[165,     1] loss: 0.044
[166,     1] loss: 0.042
[167,     1] loss: 0.043
[168,     1] loss: 0.043
[169,     1] loss: 0.043
[170,     1] loss: 0.042
[171,     1] loss: 0.043
[172,     1] loss: 0.042
[173,     1] loss: 0.042
[174,     1] loss: 0.042
[175,     1] loss: 0.042
[176,     1] loss: 0.042
[177,     1] loss: 0.042
[178,     1] loss: 0.042
[179,     1] loss: 0.042
[180,     1] loss: 0.041
[181,     1] loss: 0.043
[182,     1] loss: 0.043
[183,     1] loss: 0.042
[184,     1] loss: 0.041
[185,     1] loss: 0.042
[186,     1] loss: 0.041
[187,     1] loss: 0.041
[188,     1] loss: 0.040
[189,     1] loss: 0.040
[190,     1] loss: 0.042
[191,     1] loss: 0.041
[192,     1] loss: 0.041
[193,     1] loss: 0.040
[194,     1] loss: 0.041
[195,     1] loss: 0.042
[196,     1] loss: 0.040
[197,     1] loss: 0.041
[198,     1] loss: 0.040
[199,     1] loss: 0.040
[200,     1] loss: 0.040
Finished Training
Total time taken: 828.7099096775055
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.693
[3,     1] loss: 0.680
[4,     1] loss: 0.669
[5,     1] loss: 0.659
[6,     1] loss: 0.650
[7,     1] loss: 0.640
[8,     1] loss: 0.631
[9,     1] loss: 0.622
[10,     1] loss: 0.616
[11,     1] loss: 0.606
[12,     1] loss: 0.596
[13,     1] loss: 0.587
[14,     1] loss: 0.580
[15,     1] loss: 0.569
[16,     1] loss: 0.562
[17,     1] loss: 0.551
[18,     1] loss: 0.541
[19,     1] loss: 0.531
[20,     1] loss: 0.522
[21,     1] loss: 0.512
[22,     1] loss: 0.502
[23,     1] loss: 0.493
[24,     1] loss: 0.481
[25,     1] loss: 0.471
[26,     1] loss: 0.461
[27,     1] loss: 0.447
[28,     1] loss: 0.435
[29,     1] loss: 0.425
[30,     1] loss: 0.416
[31,     1] loss: 0.403
[32,     1] loss: 0.391
[33,     1] loss: 0.384
[34,     1] loss: 0.373
[35,     1] loss: 0.360
[36,     1] loss: 0.350
[37,     1] loss: 0.340
[38,     1] loss: 0.330
[39,     1] loss: 0.316
[40,     1] loss: 0.305
[41,     1] loss: 0.298
[42,     1] loss: 0.285
[43,     1] loss: 0.276
[44,     1] loss: 0.269
[45,     1] loss: 0.258
[46,     1] loss: 0.250
[47,     1] loss: 0.241
[48,     1] loss: 0.235
[49,     1] loss: 0.223
[50,     1] loss: 0.218
[51,     1] loss: 0.209
[52,     1] loss: 0.202
[53,     1] loss: 0.194
[54,     1] loss: 0.185
[55,     1] loss: 0.181
[56,     1] loss: 0.171
[57,     1] loss: 0.168
[58,     1] loss: 0.162
[59,     1] loss: 0.156
[60,     1] loss: 0.149
[61,     1] loss: 0.143
[62,     1] loss: 0.139
[63,     1] loss: 0.131
[64,     1] loss: 0.128
[65,     1] loss: 0.124
[66,     1] loss: 0.120
[67,     1] loss: 0.113
[68,     1] loss: 0.111
[69,     1] loss: 0.108
[70,     1] loss: 0.102
[71,     1] loss: 0.098
[72,     1] loss: 0.097
[73,     1] loss: 0.092
[74,     1] loss: 0.090
[75,     1] loss: 0.087
[76,     1] loss: 0.084
[77,     1] loss: 0.082
[78,     1] loss: 0.077
[79,     1] loss: 0.076
[80,     1] loss: 0.075
[81,     1] loss: 0.074
[82,     1] loss: 0.071
[83,     1] loss: 0.068
[84,     1] loss: 0.067
[85,     1] loss: 0.065
[86,     1] loss: 0.063
[87,     1] loss: 0.062
[88,     1] loss: 0.062
[89,     1] loss: 0.060
[90,     1] loss: 0.058
[91,     1] loss: 0.057
[92,     1] loss: 0.057
[93,     1] loss: 0.056
[94,     1] loss: 0.055
[95,     1] loss: 0.054
[96,     1] loss: 0.053
[97,     1] loss: 0.052
[98,     1] loss: 0.051
[99,     1] loss: 0.052
[100,     1] loss: 0.051
[101,     1] loss: 0.050
[102,     1] loss: 0.048
[103,     1] loss: 0.048
[104,     1] loss: 0.048
[105,     1] loss: 0.048
[106,     1] loss: 0.046
[107,     1] loss: 0.047
[108,     1] loss: 0.046
[109,     1] loss: 0.046
[110,     1] loss: 0.045
[111,     1] loss: 0.045
[112,     1] loss: 0.045
[113,     1] loss: 0.044
[114,     1] loss: 0.044
[115,     1] loss: 0.043
[116,     1] loss: 0.044
[117,     1] loss: 0.043
[118,     1] loss: 0.045
[119,     1] loss: 0.043
[120,     1] loss: 0.042
[121,     1] loss: 0.042
[122,     1] loss: 0.041
[123,     1] loss: 0.041
[124,     1] loss: 0.041
[125,     1] loss: 0.041
[126,     1] loss: 0.039
[127,     1] loss: 0.040
[128,     1] loss: 0.040
[129,     1] loss: 0.039
[130,     1] loss: 0.039
[131,     1] loss: 0.040
[132,     1] loss: 0.040
[133,     1] loss: 0.039
[134,     1] loss: 0.040
[135,     1] loss: 0.039
[136,     1] loss: 0.039
[137,     1] loss: 0.039
[138,     1] loss: 0.039
[139,     1] loss: 0.039
[140,     1] loss: 0.039
[141,     1] loss: 0.039
[142,     1] loss: 0.038
[143,     1] loss: 0.037
[144,     1] loss: 0.039
[145,     1] loss: 0.038
[146,     1] loss: 0.038
[147,     1] loss: 0.038
[148,     1] loss: 0.037
[149,     1] loss: 0.038
[150,     1] loss: 0.038
[151,     1] loss: 0.038
[152,     1] loss: 0.037
[153,     1] loss: 0.037
[154,     1] loss: 0.037
[155,     1] loss: 0.037
[156,     1] loss: 0.037
[157,     1] loss: 0.037
[158,     1] loss: 0.036
[159,     1] loss: 0.036
[160,     1] loss: 0.036
[161,     1] loss: 0.036
[162,     1] loss: 0.036
[163,     1] loss: 0.036
[164,     1] loss: 0.035
[165,     1] loss: 0.036
[166,     1] loss: 0.037
[167,     1] loss: 0.037
[168,     1] loss: 0.036
[169,     1] loss: 0.036
[170,     1] loss: 0.036
[171,     1] loss: 0.035
[172,     1] loss: 0.035
[173,     1] loss: 0.036
[174,     1] loss: 0.035
[175,     1] loss: 0.035
[176,     1] loss: 0.035
[177,     1] loss: 0.034
[178,     1] loss: 0.034
[179,     1] loss: 0.035
[180,     1] loss: 0.035
[181,     1] loss: 0.035
[182,     1] loss: 0.035
[183,     1] loss: 0.035
[184,     1] loss: 0.035
[185,     1] loss: 0.035
[186,     1] loss: 0.035
[187,     1] loss: 0.035
[188,     1] loss: 0.034
[189,     1] loss: 0.035
[190,     1] loss: 0.033
[191,     1] loss: 0.034
[192,     1] loss: 0.035
[193,     1] loss: 0.035
[194,     1] loss: 0.034
[195,     1] loss: 0.034
[196,     1] loss: 0.034
[197,     1] loss: 0.034
[198,     1] loss: 0.034
[199,     1] loss: 0.034
[200,     1] loss: 0.034
Finished Training
Total time taken: 820.9193966388702
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8606451612903225, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9041666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8526717557251908, 'Pyrrolidone carboxylic acid Validation Precision': 0.5300492440005549, 'Pyrrolidone carboxylic acid AUC ROC': 0.9305979643765904, 'Pyrrolidone carboxylic acid AUC PR': 0.6851269338674956, 'Pyrrolidone carboxylic acid MCC': 0.6211599971470316, 'Pyrrolidone carboxylic acid F1': 0.6680504866180048, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.25097766518592834, 'Validation Loss (total)': 0.25097766518592834}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006598051318353779,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 921554553,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.680294693347335}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.693
[3,     1] loss: 0.681
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066913566685251275,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3338301435,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.187474407120758}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.644
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006438097317180446,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1825615495,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.234682581438792}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.685
[3,     1] loss: 0.658
[4,     1] loss: 0.632
[5,     1] loss: 0.600
[6,     1] loss: 0.565
[7,     1] loss: 0.531
[8,     1] loss: 0.496
[9,     1] loss: 0.461
[10,     1] loss: 0.430
[11,     1] loss: 0.397
[12,     1] loss: 0.376
[13,     1] loss: 0.361
[14,     1] loss: 0.350
[15,     1] loss: 0.339
[16,     1] loss: 0.332
[17,     1] loss: 0.324
[18,     1] loss: 0.317
[19,     1] loss: 0.310
[20,     1] loss: 0.301
[21,     1] loss: 0.290
[22,     1] loss: 0.279
[23,     1] loss: 0.266
[24,     1] loss: 0.250
[25,     1] loss: 0.231
[26,     1] loss: 0.211
[27,     1] loss: 0.187
[28,     1] loss: 0.164
[29,     1] loss: 0.141
[30,     1] loss: 0.122
[31,     1] loss: 0.108
[32,     1] loss: 0.124
[33,     1] loss: 0.185
[34,     1] loss: 0.169
[35,     1] loss: 0.368
[36,     1] loss: 0.213
[37,     1] loss: 0.281
[38,     1] loss: 0.221
[39,     1] loss: 0.244
[40,     1] loss: 0.229
[41,     1] loss: 0.220
[42,     1] loss: 0.198
[43,     1] loss: 0.178
[44,     1] loss: 0.160
[45,     1] loss: 0.139
[46,     1] loss: 0.122
[47,     1] loss: 0.114
[48,     1] loss: 0.102
[49,     1] loss: 0.096
[50,     1] loss: 0.089
[51,     1] loss: 0.084
[52,     1] loss: 0.080
[53,     1] loss: 0.081
[54,     1] loss: 0.081
[55,     1] loss: 0.082
[56,     1] loss: 0.083
[57,     1] loss: 0.081
[58,     1] loss: 0.083
[59,     1] loss: 0.084
[60,     1] loss: 0.087
[61,     1] loss: 0.087
[62,     1] loss: 0.088
[63,     1] loss: 0.087
[64,     1] loss: 0.087
[65,     1] loss: 0.088
[66,     1] loss: 0.084
[67,     1] loss: 0.086
[68,     1] loss: 0.085
[69,     1] loss: 0.084
[70,     1] loss: 0.083
[71,     1] loss: 0.083
[72,     1] loss: 0.087
[73,     1] loss: 0.117
[74,     1] loss: 0.151
[75,     1] loss: 0.401
[76,     1] loss: 0.583
[77,     1] loss: 0.441
[78,     1] loss: 0.401
[79,     1] loss: 0.396
[80,     1] loss: 0.385
[81,     1] loss: 0.383
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007576031038239126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2901989755,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.6193552494482057}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.658
[3,     1] loss: 0.565
[4,     1] loss: 0.472
[5,     1] loss: 0.389
[6,     1] loss: 0.309
[7,     1] loss: 0.225
[8,     1] loss: 0.148
[9,     1] loss: 0.105
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00796031116920241,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3545186908,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.26871399915932}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005360754354983058,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 917091805,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.32230765065555}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.632
[4,     1] loss: 0.590
[5,     1] loss: 0.544
[6,     1] loss: 0.498
[7,     1] loss: 0.456
[8,     1] loss: 0.417
[9,     1] loss: 0.387
[10,     1] loss: 0.357
[11,     1] loss: 0.332
[12,     1] loss: 0.306
[13,     1] loss: 0.281
[14,     1] loss: 0.252
[15,     1] loss: 0.224
[16,     1] loss: 0.193
[17,     1] loss: 0.170
[18,     1] loss: 0.138
[19,     1] loss: 0.116
[20,     1] loss: 0.099
[21,     1] loss: 0.078
[22,     1] loss: 0.065
[23,     1] loss: 0.052
[24,     1] loss: 0.041
[25,     1] loss: 0.034
[26,     1] loss: 0.028
[27,     1] loss: 0.024
[28,     1] loss: 0.023
[29,     1] loss: 0.020
[30,     1] loss: 0.019
[31,     1] loss: 0.019
[32,     1] loss: 0.019
[33,     1] loss: 0.019
[34,     1] loss: 0.019
[35,     1] loss: 0.020
[36,     1] loss: 0.021
[37,     1] loss: 0.022
[38,     1] loss: 0.023
[39,     1] loss: 0.024
[40,     1] loss: 0.024
[41,     1] loss: 0.024
[42,     1] loss: 0.024
[43,     1] loss: 0.024
[44,     1] loss: 0.026
[45,     1] loss: 0.024
[46,     1] loss: 0.025
[47,     1] loss: 0.024
[48,     1] loss: 0.023
[49,     1] loss: 0.023
[50,     1] loss: 0.022
[51,     1] loss: 0.022
[52,     1] loss: 0.022
[53,     1] loss: 0.022
[54,     1] loss: 0.021
[55,     1] loss: 0.021
[56,     1] loss: 0.021
[57,     1] loss: 0.020
[58,     1] loss: 0.020
[59,     1] loss: 0.020
[60,     1] loss: 0.020
[61,     1] loss: 0.020
[62,     1] loss: 0.019
[63,     1] loss: 0.020
[64,     1] loss: 0.020
[65,     1] loss: 0.020
[66,     1] loss: 0.020
[67,     1] loss: 0.020
[68,     1] loss: 0.021
[69,     1] loss: 0.063
[70,     1] loss: 0.475
[71,     1] loss: 1.472
[72,     1] loss: 0.818
[73,     1] loss: 0.606
[74,     1] loss: 0.613
[75,     1] loss: 0.637
[76,     1] loss: 0.653
Early stopping applied (best metric=0.24138985574245453)
Finished Training
Total time taken: 318.52190685272217
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.685
[3,     1] loss: 0.647
[4,     1] loss: 0.608
[5,     1] loss: 0.562
[6,     1] loss: 0.515
[7,     1] loss: 0.470
[8,     1] loss: 0.425
[9,     1] loss: 0.375
[10,     1] loss: 0.330
[11,     1] loss: 0.298
[12,     1] loss: 0.268
[13,     1] loss: 0.247
[14,     1] loss: 0.227
[15,     1] loss: 0.203
[16,     1] loss: 0.182
[17,     1] loss: 0.159
[18,     1] loss: 0.138
[19,     1] loss: 0.118
[20,     1] loss: 0.101
[21,     1] loss: 0.086
[22,     1] loss: 0.073
[23,     1] loss: 0.065
[24,     1] loss: 0.056
[25,     1] loss: 0.049
[26,     1] loss: 0.043
[27,     1] loss: 0.038
[28,     1] loss: 0.030
[29,     1] loss: 0.027
[30,     1] loss: 0.026
[31,     1] loss: 0.025
[32,     1] loss: 0.023
[33,     1] loss: 0.022
[34,     1] loss: 0.023
[35,     1] loss: 0.021
[36,     1] loss: 0.022
[37,     1] loss: 0.021
[38,     1] loss: 0.023
[39,     1] loss: 0.022
[40,     1] loss: 0.023
[41,     1] loss: 0.022
[42,     1] loss: 0.023
[43,     1] loss: 0.023
[44,     1] loss: 0.023
[45,     1] loss: 0.024
[46,     1] loss: 0.024
[47,     1] loss: 0.024
[48,     1] loss: 0.023
[49,     1] loss: 0.024
[50,     1] loss: 0.023
[51,     1] loss: 0.023
[52,     1] loss: 0.022
[53,     1] loss: 0.022
[54,     1] loss: 0.021
[55,     1] loss: 0.022
[56,     1] loss: 0.021
[57,     1] loss: 0.023
[58,     1] loss: 0.022
[59,     1] loss: 0.021
[60,     1] loss: 0.021
[61,     1] loss: 0.021
[62,     1] loss: 0.020
[63,     1] loss: 0.021
[64,     1] loss: 0.021
[65,     1] loss: 0.021
[66,     1] loss: 0.021
[67,     1] loss: 0.021
[68,     1] loss: 0.022
[69,     1] loss: 0.021
[70,     1] loss: 0.021
[71,     1] loss: 0.021
[72,     1] loss: 0.021
[73,     1] loss: 0.030
[74,     1] loss: 0.121
[75,     1] loss: 0.424
[76,     1] loss: 0.684
[77,     1] loss: 0.566
[78,     1] loss: 0.477
[79,     1] loss: 0.481
[80,     1] loss: 0.471
[81,     1] loss: 0.455
[82,     1] loss: 0.452
[83,     1] loss: 0.445
[84,     1] loss: 0.432
[85,     1] loss: 0.406
Early stopping applied (best metric=0.23301731050014496)
Finished Training
Total time taken: 354.170179605484
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.677
[3,     1] loss: 0.634
[4,     1] loss: 0.594
[5,     1] loss: 0.550
[6,     1] loss: 0.502
[7,     1] loss: 0.452
[8,     1] loss: 0.406
[9,     1] loss: 0.361
[10,     1] loss: 0.317
[11,     1] loss: 0.272
[12,     1] loss: 0.226
[13,     1] loss: 0.175
[14,     1] loss: 0.138
[15,     1] loss: 0.108
[16,     1] loss: 0.085
[17,     1] loss: 0.069
[18,     1] loss: 0.057
[19,     1] loss: 0.048
[20,     1] loss: 0.037
[21,     1] loss: 0.032
[22,     1] loss: 0.029
[23,     1] loss: 0.027
[24,     1] loss: 0.024
[25,     1] loss: 0.023
[26,     1] loss: 0.023
[27,     1] loss: 0.024
[28,     1] loss: 0.024
[29,     1] loss: 0.024
[30,     1] loss: 0.025
[31,     1] loss: 0.026
[32,     1] loss: 0.027
[33,     1] loss: 0.028
[34,     1] loss: 0.028
[35,     1] loss: 0.028
[36,     1] loss: 0.029
[37,     1] loss: 0.029
[38,     1] loss: 0.029
[39,     1] loss: 0.027
[40,     1] loss: 0.026
[41,     1] loss: 0.025
[42,     1] loss: 0.025
[43,     1] loss: 0.024
[44,     1] loss: 0.023
[45,     1] loss: 0.023
[46,     1] loss: 0.022
[47,     1] loss: 0.022
[48,     1] loss: 0.022
[49,     1] loss: 0.022
[50,     1] loss: 0.021
[51,     1] loss: 0.021
[52,     1] loss: 0.021
[53,     1] loss: 0.020
[54,     1] loss: 0.020
[55,     1] loss: 0.020
[56,     1] loss: 0.021
[57,     1] loss: 0.020
[58,     1] loss: 0.021
[59,     1] loss: 0.021
[60,     1] loss: 0.020
[61,     1] loss: 0.021
[62,     1] loss: 0.021
[63,     1] loss: 0.021
[64,     1] loss: 0.021
Early stopping applied (best metric=0.23051871359348297)
Finished Training
Total time taken: 264.6247835159302
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.671
[4,     1] loss: 0.647
[5,     1] loss: 0.615
[6,     1] loss: 0.578
[7,     1] loss: 0.533
[8,     1] loss: 0.485
[9,     1] loss: 0.434
[10,     1] loss: 0.387
[11,     1] loss: 0.344
[12,     1] loss: 0.309
[13,     1] loss: 0.277
[14,     1] loss: 0.249
[15,     1] loss: 0.219
[16,     1] loss: 0.193
[17,     1] loss: 0.165
[18,     1] loss: 0.140
[19,     1] loss: 0.117
[20,     1] loss: 0.103
[21,     1] loss: 0.096
[22,     1] loss: 0.071
[23,     1] loss: 0.070
[24,     1] loss: 0.065
[25,     1] loss: 0.055
[26,     1] loss: 0.056
[27,     1] loss: 0.050
[28,     1] loss: 0.048
[29,     1] loss: 0.046
[30,     1] loss: 0.043
[31,     1] loss: 0.040
[32,     1] loss: 0.035
[33,     1] loss: 0.033
[34,     1] loss: 0.032
[35,     1] loss: 0.031
[36,     1] loss: 0.029
[37,     1] loss: 0.028
[38,     1] loss: 0.028
[39,     1] loss: 0.027
[40,     1] loss: 0.027
[41,     1] loss: 0.027
[42,     1] loss: 0.027
[43,     1] loss: 0.028
[44,     1] loss: 0.028
[45,     1] loss: 0.028
[46,     1] loss: 0.029
[47,     1] loss: 0.028
[48,     1] loss: 0.029
[49,     1] loss: 0.029
[50,     1] loss: 0.029
[51,     1] loss: 0.029
[52,     1] loss: 0.029
[53,     1] loss: 0.029
[54,     1] loss: 0.029
[55,     1] loss: 0.027
[56,     1] loss: 0.027
[57,     1] loss: 0.027
[58,     1] loss: 0.027
[59,     1] loss: 0.028
[60,     1] loss: 0.027
[61,     1] loss: 0.027
[62,     1] loss: 0.027
[63,     1] loss: 0.026
[64,     1] loss: 0.027
[65,     1] loss: 0.027
[66,     1] loss: 0.027
[67,     1] loss: 0.026
[68,     1] loss: 0.026
[69,     1] loss: 0.026
[70,     1] loss: 0.026
[71,     1] loss: 0.026
[72,     1] loss: 0.026
[73,     1] loss: 0.026
[74,     1] loss: 0.027
[75,     1] loss: 0.027
[76,     1] loss: 0.026
[77,     1] loss: 0.026
[78,     1] loss: 0.028
[79,     1] loss: 0.082
[80,     1] loss: 0.983
[81,     1] loss: 0.892
[82,     1] loss: 0.597
[83,     1] loss: 0.495
[84,     1] loss: 0.522
[85,     1] loss: 0.557
[86,     1] loss: 0.580
[87,     1] loss: 0.591
[88,     1] loss: 0.595
[89,     1] loss: 0.599
[90,     1] loss: 0.599
Early stopping applied (best metric=0.20318512618541718)
Finished Training
Total time taken: 371.5535581111908
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.687
[3,     1] loss: 0.661
[4,     1] loss: 0.636
[5,     1] loss: 0.605
[6,     1] loss: 0.570
[7,     1] loss: 0.532
[8,     1] loss: 0.491
[9,     1] loss: 0.448
[10,     1] loss: 0.404
[11,     1] loss: 0.364
[12,     1] loss: 0.324
[13,     1] loss: 0.286
[14,     1] loss: 0.244
[15,     1] loss: 0.217
[16,     1] loss: 0.170
[17,     1] loss: 0.154
[18,     1] loss: 0.114
[19,     1] loss: 0.114
[20,     1] loss: 0.078
[21,     1] loss: 0.080
[22,     1] loss: 0.059
[23,     1] loss: 0.050
[24,     1] loss: 0.046
[25,     1] loss: 0.039
[26,     1] loss: 0.036
[27,     1] loss: 0.034
[28,     1] loss: 0.031
[29,     1] loss: 0.031
[30,     1] loss: 0.029
[31,     1] loss: 0.028
[32,     1] loss: 0.028
[33,     1] loss: 0.028
[34,     1] loss: 0.027
[35,     1] loss: 0.028
[36,     1] loss: 0.029
[37,     1] loss: 0.030
[38,     1] loss: 0.029
[39,     1] loss: 0.029
[40,     1] loss: 0.029
[41,     1] loss: 0.029
[42,     1] loss: 0.029
[43,     1] loss: 0.028
[44,     1] loss: 0.029
[45,     1] loss: 0.028
[46,     1] loss: 0.027
[47,     1] loss: 0.027
[48,     1] loss: 0.027
[49,     1] loss: 0.026
[50,     1] loss: 0.025
[51,     1] loss: 0.026
[52,     1] loss: 0.025
[53,     1] loss: 0.024
[54,     1] loss: 0.025
[55,     1] loss: 0.024
[56,     1] loss: 0.024
[57,     1] loss: 0.024
[58,     1] loss: 0.023
[59,     1] loss: 0.023
[60,     1] loss: 0.023
[61,     1] loss: 0.023
[62,     1] loss: 0.022
[63,     1] loss: 0.023
[64,     1] loss: 0.023
[65,     1] loss: 0.023
[66,     1] loss: 0.023
[67,     1] loss: 0.022
[68,     1] loss: 0.023
[69,     1] loss: 0.022
[70,     1] loss: 0.022
[71,     1] loss: 0.023
[72,     1] loss: 0.023
[73,     1] loss: 0.030
[74,     1] loss: 0.079
[75,     1] loss: 0.514
[76,     1] loss: 0.707
[77,     1] loss: 0.372
[78,     1] loss: 0.429
[79,     1] loss: 0.439
[80,     1] loss: 0.455
[81,     1] loss: 0.461
[82,     1] loss: 0.460
[83,     1] loss: 0.452
[84,     1] loss: 0.432
[85,     1] loss: 0.408
Early stopping applied (best metric=0.22179673612117767)
Finished Training
Total time taken: 353.0212368965149
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8651612903225807, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9125, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8564885496183207, 'Pyrrolidone carboxylic acid Validation Precision': 0.5409347442680776, 'Pyrrolidone carboxylic acid AUC ROC': 0.9476304071246819, 'Pyrrolidone carboxylic acid AUC PR': 0.7513305546705484, 'Pyrrolidone carboxylic acid MCC': 0.6337381768844481, 'Pyrrolidone carboxylic acid F1': 0.6779681194511703, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.22598154842853546, 'Validation Loss (total)': 0.22598154842853546}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003959071188242473,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4039431055,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.00945401863219}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.677
[3,     1] loss: 0.626
[4,     1] loss: 0.576
[5,     1] loss: 0.520
[6,     1] loss: 0.466
[7,     1] loss: 0.425
[8,     1] loss: 0.394
[9,     1] loss: 0.377
[10,     1] loss: 0.362
[11,     1] loss: 0.354
[12,     1] loss: 0.344
[13,     1] loss: 0.333
[14,     1] loss: 0.320
[15,     1] loss: 0.308
[16,     1] loss: 0.301
[17,     1] loss: 0.294
[18,     1] loss: 0.288
[19,     1] loss: 0.283
[20,     1] loss: 0.277
[21,     1] loss: 0.266
[22,     1] loss: 0.255
[23,     1] loss: 0.250
[24,     1] loss: 0.238
[25,     1] loss: 0.226
[26,     1] loss: 0.213
[27,     1] loss: 0.198
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007973034508058242,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1874441928,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.770030267994361}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.680
[3,     1] loss: 0.642
[4,     1] loss: 0.598
[5,     1] loss: 0.553
[6,     1] loss: 0.504
[7,     1] loss: 0.458
[8,     1] loss: 0.410
[9,     1] loss: 0.362
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008100139424899734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 87696947,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.573744293150756}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.701
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008314631918021355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2757757958,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.528385204060612}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.651
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00915366758886931,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3987980402,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.70554481120432}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.654
[4,     1] loss: 0.637
[5,     1] loss: 0.623
[6,     1] loss: 0.608
[7,     1] loss: 0.595
[8,     1] loss: 0.584
[9,     1] loss: 0.573
[10,     1] loss: 0.564
[11,     1] loss: 0.555
[12,     1] loss: 0.546
[13,     1] loss: 0.536
[14,     1] loss: 0.526
[15,     1] loss: 0.517
[16,     1] loss: 0.503
[17,     1] loss: 0.491
[18,     1] loss: 0.476
[19,     1] loss: 0.460
[20,     1] loss: 0.446
[21,     1] loss: 0.440
[22,     1] loss: 0.490
[23,     1] loss: 0.454
[24,     1] loss: 0.439
[25,     1] loss: 0.435
[26,     1] loss: 0.420
[27,     1] loss: 0.405
[28,     1] loss: 0.392
[29,     1] loss: 0.375
[30,     1] loss: 0.359
[31,     1] loss: 0.348
[32,     1] loss: 0.335
[33,     1] loss: 0.324
[34,     1] loss: 0.313
[35,     1] loss: 0.305
[36,     1] loss: 0.294
[37,     1] loss: 0.292
[38,     1] loss: 0.290
[39,     1] loss: 0.296
[40,     1] loss: 0.344
[41,     1] loss: 0.425
[42,     1] loss: 0.409
[43,     1] loss: 0.395
[44,     1] loss: 0.372
[45,     1] loss: 0.374
[46,     1] loss: 0.374
[47,     1] loss: 0.364
[48,     1] loss: 0.360
[49,     1] loss: 0.351
[50,     1] loss: 0.343
[51,     1] loss: 0.337
[52,     1] loss: 0.333
[53,     1] loss: 0.325
[54,     1] loss: 0.322
[55,     1] loss: 0.318
[56,     1] loss: 0.312
[57,     1] loss: 0.308
[58,     1] loss: 0.306
[59,     1] loss: 0.303
[60,     1] loss: 0.301
[61,     1] loss: 0.298
[62,     1] loss: 0.294
[63,     1] loss: 0.294
[64,     1] loss: 0.295
[65,     1] loss: 0.312
[66,     1] loss: 0.339
[67,     1] loss: 0.459
[68,     1] loss: 0.432
[69,     1] loss: 0.430
[70,     1] loss: 0.423
[71,     1] loss: 0.419
[72,     1] loss: 0.421
[73,     1] loss: 0.418
[74,     1] loss: 0.416
[75,     1] loss: 0.413
[76,     1] loss: 0.413
[77,     1] loss: 0.408
[78,     1] loss: 0.404
[79,     1] loss: 0.402
[80,     1] loss: 0.399
[81,     1] loss: 0.395
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007680974379829709,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1818313633,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.389825892044984}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.682
[3,     1] loss: 0.620
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004351988844694537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2397419427,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.307242271843712}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.646
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008555690312909182,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 63204506,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.483609445349905}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.674
[3,     1] loss: 0.584
[4,     1] loss: 0.497
[5,     1] loss: 0.435
[6,     1] loss: 0.393
[7,     1] loss: 0.351
[8,     1] loss: 0.314
[9,     1] loss: 0.280
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006585017370052734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1824759872,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.902076399672463}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.673
[3,     1] loss: 0.626
[4,     1] loss: 0.583
[5,     1] loss: 0.535
[6,     1] loss: 0.487
[7,     1] loss: 0.439
[8,     1] loss: 0.393
[9,     1] loss: 0.344
[10,     1] loss: 0.304
[11,     1] loss: 0.271
[12,     1] loss: 0.242
[13,     1] loss: 0.216
[14,     1] loss: 0.192
[15,     1] loss: 0.172
[16,     1] loss: 0.153
[17,     1] loss: 0.136
[18,     1] loss: 0.119
[19,     1] loss: 0.107
[20,     1] loss: 0.094
[21,     1] loss: 0.082
[22,     1] loss: 0.075
[23,     1] loss: 0.069
[24,     1] loss: 0.065
[25,     1] loss: 0.061
[26,     1] loss: 0.057
[27,     1] loss: 0.053
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003204980360799417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4168299511,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.525198759811108}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009985951077045332,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1390526128,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.9241542204323805}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009631765731873082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 35551080,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.929644071678371}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.694
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00838204109979245,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 417765030,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.839474640117191}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.696
[3,     1] loss: 0.675
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036559912718053004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2177449970,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.537624994516621}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.667
[3,     1] loss: 0.634
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00929059523889387,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3175189522,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.86326922418571}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.695
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008351127674530515,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2563255940,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.9533158717408066}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.662
[3,     1] loss: 0.577
[4,     1] loss: 0.484
[5,     1] loss: 0.385
[6,     1] loss: 0.280
[7,     1] loss: 0.180
[8,     1] loss: 0.113
[9,     1] loss: 0.068
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009945278426486792,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1340033410,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.86739132130695}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006423473777887629,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1382250460,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.270185532552981}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.676
[3,     1] loss: 0.629
[4,     1] loss: 0.588
[5,     1] loss: 0.546
[6,     1] loss: 0.503
[7,     1] loss: 0.469
[8,     1] loss: 0.439
[9,     1] loss: 0.411
[10,     1] loss: 0.386
[11,     1] loss: 0.364
[12,     1] loss: 0.345
[13,     1] loss: 0.326
[14,     1] loss: 0.310
[15,     1] loss: 0.289
[16,     1] loss: 0.269
[17,     1] loss: 0.248
[18,     1] loss: 0.230
[19,     1] loss: 0.208
[20,     1] loss: 0.187
[21,     1] loss: 0.168
[22,     1] loss: 0.146
[23,     1] loss: 0.129
[24,     1] loss: 0.114
[25,     1] loss: 0.099
[26,     1] loss: 0.088
[27,     1] loss: 0.078
[28,     1] loss: 0.071
[29,     1] loss: 0.063
[30,     1] loss: 0.059
[31,     1] loss: 0.057
[32,     1] loss: 0.053
[33,     1] loss: 0.053
[34,     1] loss: 0.048
[35,     1] loss: 0.051
[36,     1] loss: 0.049
[37,     1] loss: 0.051
[38,     1] loss: 0.050
[39,     1] loss: 0.051
[40,     1] loss: 0.051
[41,     1] loss: 0.051
[42,     1] loss: 0.052
[43,     1] loss: 0.053
[44,     1] loss: 0.053
[45,     1] loss: 0.053
[46,     1] loss: 0.052
[47,     1] loss: 0.050
[48,     1] loss: 0.052
[49,     1] loss: 0.051
[50,     1] loss: 0.051
[51,     1] loss: 0.049
[52,     1] loss: 0.049
[53,     1] loss: 0.048
[54,     1] loss: 0.066
[55,     1] loss: 0.107
[56,     1] loss: 0.461
[57,     1] loss: 0.494
[58,     1] loss: 0.507
[59,     1] loss: 0.375
[60,     1] loss: 0.409
[61,     1] loss: 0.393
[62,     1] loss: 0.386
[63,     1] loss: 0.394
[64,     1] loss: 0.384
[65,     1] loss: 0.364
[66,     1] loss: 0.350
[67,     1] loss: 0.330
[68,     1] loss: 0.310
[69,     1] loss: 0.297
Early stopping applied (best metric=0.26885372400283813)
Finished Training
Total time taken: 291.4563751220703
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.683
[3,     1] loss: 0.654
[4,     1] loss: 0.624
[5,     1] loss: 0.589
[6,     1] loss: 0.551
[7,     1] loss: 0.511
[8,     1] loss: 0.480
[9,     1] loss: 0.448
[10,     1] loss: 0.422
[11,     1] loss: 0.402
[12,     1] loss: 0.380
[13,     1] loss: 0.363
[14,     1] loss: 0.347
[15,     1] loss: 0.329
[16,     1] loss: 0.312
[17,     1] loss: 0.294
[18,     1] loss: 0.276
[19,     1] loss: 0.262
[20,     1] loss: 0.246
[21,     1] loss: 0.231
[22,     1] loss: 0.208
[23,     1] loss: 0.193
[24,     1] loss: 0.181
[25,     1] loss: 0.159
[26,     1] loss: 0.141
[27,     1] loss: 0.139
[28,     1] loss: 0.108
[29,     1] loss: 0.104
[30,     1] loss: 0.092
[31,     1] loss: 0.077
[32,     1] loss: 0.068
[33,     1] loss: 0.061
[34,     1] loss: 0.055
[35,     1] loss: 0.050
[36,     1] loss: 0.049
[37,     1] loss: 0.045
[38,     1] loss: 0.046
[39,     1] loss: 0.045
[40,     1] loss: 0.045
[41,     1] loss: 0.045
[42,     1] loss: 0.046
[43,     1] loss: 0.048
[44,     1] loss: 0.048
[45,     1] loss: 0.049
[46,     1] loss: 0.050
[47,     1] loss: 0.049
[48,     1] loss: 0.051
[49,     1] loss: 0.051
[50,     1] loss: 0.051
[51,     1] loss: 0.051
[52,     1] loss: 0.052
[53,     1] loss: 0.051
[54,     1] loss: 0.051
[55,     1] loss: 0.050
[56,     1] loss: 0.049
[57,     1] loss: 0.048
[58,     1] loss: 0.048
[59,     1] loss: 0.049
[60,     1] loss: 0.048
[61,     1] loss: 0.046
[62,     1] loss: 0.047
[63,     1] loss: 0.049
[64,     1] loss: 0.155
[65,     1] loss: 0.684
[66,     1] loss: 0.587
[67,     1] loss: 0.614
[68,     1] loss: 0.559
[69,     1] loss: 0.565
[70,     1] loss: 0.571
[71,     1] loss: 0.561
[72,     1] loss: 0.555
[73,     1] loss: 0.553
Early stopping applied (best metric=0.2557052969932556)
Finished Training
Total time taken: 306.9664378166199
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.691
[3,     1] loss: 0.659
[4,     1] loss: 0.628
[5,     1] loss: 0.590
[6,     1] loss: 0.553
[7,     1] loss: 0.514
[8,     1] loss: 0.479
[9,     1] loss: 0.448
[10,     1] loss: 0.417
[11,     1] loss: 0.390
[12,     1] loss: 0.368
[13,     1] loss: 0.347
[14,     1] loss: 0.326
[15,     1] loss: 0.306
[16,     1] loss: 0.281
[17,     1] loss: 0.257
[18,     1] loss: 0.235
[19,     1] loss: 0.213
[20,     1] loss: 0.187
[21,     1] loss: 0.203
[22,     1] loss: 0.206
[23,     1] loss: 0.197
[24,     1] loss: 0.172
[25,     1] loss: 0.162
[26,     1] loss: 0.149
[27,     1] loss: 0.137
[28,     1] loss: 0.119
[29,     1] loss: 0.105
[30,     1] loss: 0.093
[31,     1] loss: 0.081
[32,     1] loss: 0.076
[33,     1] loss: 0.068
[34,     1] loss: 0.064
[35,     1] loss: 0.060
[36,     1] loss: 0.058
[37,     1] loss: 0.057
[38,     1] loss: 0.057
[39,     1] loss: 0.056
[40,     1] loss: 0.058
[41,     1] loss: 0.059
[42,     1] loss: 0.058
[43,     1] loss: 0.060
[44,     1] loss: 0.060
[45,     1] loss: 0.062
[46,     1] loss: 0.062
[47,     1] loss: 0.061
[48,     1] loss: 0.063
[49,     1] loss: 0.062
[50,     1] loss: 0.061
[51,     1] loss: 0.061
[52,     1] loss: 0.060
[53,     1] loss: 0.060
[54,     1] loss: 0.058
[55,     1] loss: 0.059
[56,     1] loss: 0.059
[57,     1] loss: 0.058
[58,     1] loss: 0.057
[59,     1] loss: 0.057
[60,     1] loss: 0.058
[61,     1] loss: 0.076
[62,     1] loss: 0.109
[63,     1] loss: 0.359
[64,     1] loss: 0.584
[65,     1] loss: 0.511
[66,     1] loss: 0.464
[67,     1] loss: 0.456
[68,     1] loss: 0.430
[69,     1] loss: 0.415
[70,     1] loss: 0.404
[71,     1] loss: 0.386
[72,     1] loss: 0.362
[73,     1] loss: 0.341
[74,     1] loss: 0.311
[75,     1] loss: 0.278
[76,     1] loss: 0.246
[77,     1] loss: 0.213
[78,     1] loss: 0.180
[79,     1] loss: 0.156
[80,     1] loss: 0.130
[81,     1] loss: 0.111
[82,     1] loss: 0.096
[83,     1] loss: 0.083
[84,     1] loss: 0.076
[85,     1] loss: 0.070
[86,     1] loss: 0.070
[87,     1] loss: 0.065
[88,     1] loss: 0.066
[89,     1] loss: 0.068
[90,     1] loss: 0.067
[91,     1] loss: 0.070
[92,     1] loss: 0.072
[93,     1] loss: 0.074
[94,     1] loss: 0.073
[95,     1] loss: 0.075
[96,     1] loss: 0.076
[97,     1] loss: 0.076
[98,     1] loss: 0.076
[99,     1] loss: 0.074
[100,     1] loss: 0.075
[101,     1] loss: 0.074
[102,     1] loss: 0.072
[103,     1] loss: 0.072
[104,     1] loss: 0.071
[105,     1] loss: 0.072
[106,     1] loss: 0.071
[107,     1] loss: 0.071
[108,     1] loss: 0.072
[109,     1] loss: 0.072
[110,     1] loss: 0.072
[111,     1] loss: 0.071
[112,     1] loss: 0.072
[113,     1] loss: 0.074
[114,     1] loss: 0.093
[115,     1] loss: 0.393
[116,     1] loss: 0.693
[117,     1] loss: 0.579
[118,     1] loss: 0.620
[119,     1] loss: 0.575
[120,     1] loss: 0.540
[121,     1] loss: 0.532
[122,     1] loss: 0.535
[123,     1] loss: 0.532
[124,     1] loss: 0.521
[125,     1] loss: 0.504
[126,     1] loss: 0.487
[127,     1] loss: 0.470
Early stopping applied (best metric=0.26067471504211426)
Finished Training
Total time taken: 533.7901694774628
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.647
[5,     1] loss: 0.624
[6,     1] loss: 0.598
[7,     1] loss: 0.567
[8,     1] loss: 0.536
[9,     1] loss: 0.501
[10,     1] loss: 0.462
[11,     1] loss: 0.424
[12,     1] loss: 0.381
[13,     1] loss: 0.340
[14,     1] loss: 0.304
[15,     1] loss: 0.263
[16,     1] loss: 0.231
[17,     1] loss: 0.202
[18,     1] loss: 0.173
[19,     1] loss: 0.150
[20,     1] loss: 0.132
[21,     1] loss: 0.116
[22,     1] loss: 0.102
[23,     1] loss: 0.091
[24,     1] loss: 0.080
[25,     1] loss: 0.074
[26,     1] loss: 0.067
[27,     1] loss: 0.063
[28,     1] loss: 0.061
[29,     1] loss: 0.058
[30,     1] loss: 0.057
[31,     1] loss: 0.055
[32,     1] loss: 0.055
[33,     1] loss: 0.056
[34,     1] loss: 0.056
[35,     1] loss: 0.057
[36,     1] loss: 0.059
[37,     1] loss: 0.057
[38,     1] loss: 0.057
[39,     1] loss: 0.060
[40,     1] loss: 0.060
[41,     1] loss: 0.059
[42,     1] loss: 0.060
[43,     1] loss: 0.060
[44,     1] loss: 0.058
[45,     1] loss: 0.058
[46,     1] loss: 0.057
[47,     1] loss: 0.059
[48,     1] loss: 0.125
[49,     1] loss: 0.326
[50,     1] loss: 0.632
[51,     1] loss: 0.588
[52,     1] loss: 0.539
[53,     1] loss: 0.540
[54,     1] loss: 0.543
[55,     1] loss: 0.540
[56,     1] loss: 0.539
[57,     1] loss: 0.538
[58,     1] loss: 0.535
[59,     1] loss: 0.527
[60,     1] loss: 0.511
[61,     1] loss: 0.494
[62,     1] loss: 0.475
[63,     1] loss: 0.448
[64,     1] loss: 0.417
[65,     1] loss: 0.383
[66,     1] loss: 0.346
[67,     1] loss: 0.308
[68,     1] loss: 0.274
[69,     1] loss: 0.236
[70,     1] loss: 0.207
[71,     1] loss: 0.177
[72,     1] loss: 0.151
[73,     1] loss: 0.130
[74,     1] loss: 0.112
[75,     1] loss: 0.097
[76,     1] loss: 0.086
[77,     1] loss: 0.079
[78,     1] loss: 0.075
[79,     1] loss: 0.072
[80,     1] loss: 0.070
[81,     1] loss: 0.070
[82,     1] loss: 0.071
[83,     1] loss: 0.072
[84,     1] loss: 0.075
[85,     1] loss: 0.078
[86,     1] loss: 0.079
[87,     1] loss: 0.080
[88,     1] loss: 0.082
[89,     1] loss: 0.082
[90,     1] loss: 0.082
[91,     1] loss: 0.082
[92,     1] loss: 0.082
[93,     1] loss: 0.081
[94,     1] loss: 0.082
[95,     1] loss: 0.080
[96,     1] loss: 0.080
[97,     1] loss: 0.080
[98,     1] loss: 0.081
[99,     1] loss: 0.081
[100,     1] loss: 0.080
[101,     1] loss: 0.081
[102,     1] loss: 0.080
[103,     1] loss: 0.081
[104,     1] loss: 0.081
[105,     1] loss: 0.090
[106,     1] loss: 0.180
[107,     1] loss: 0.333
[108,     1] loss: 0.957
[109,     1] loss: 0.647
[110,     1] loss: 0.506
[111,     1] loss: 0.567
[112,     1] loss: 0.507
[113,     1] loss: 0.518
[114,     1] loss: 0.519
[115,     1] loss: 0.518
[116,     1] loss: 0.517
[117,     1] loss: 0.509
[118,     1] loss: 0.502
[119,     1] loss: 0.493
[120,     1] loss: 0.486
Early stopping applied (best metric=0.28096380829811096)
Finished Training
Total time taken: 503.55512952804565
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.677
[3,     1] loss: 0.633
[4,     1] loss: 0.592
[5,     1] loss: 0.550
[6,     1] loss: 0.507
[7,     1] loss: 0.463
[8,     1] loss: 0.418
[9,     1] loss: 0.379
[10,     1] loss: 0.343
[11,     1] loss: 0.307
[12,     1] loss: 0.271
[13,     1] loss: 0.240
[14,     1] loss: 0.210
[15,     1] loss: 0.185
[16,     1] loss: 0.160
[17,     1] loss: 0.140
[18,     1] loss: 0.121
[19,     1] loss: 0.112
[20,     1] loss: 0.097
[21,     1] loss: 0.091
[22,     1] loss: 0.080
[23,     1] loss: 0.074
[24,     1] loss: 0.069
[25,     1] loss: 0.066
[26,     1] loss: 0.061
[27,     1] loss: 0.061
[28,     1] loss: 0.059
[29,     1] loss: 0.057
[30,     1] loss: 0.057
[31,     1] loss: 0.055
[32,     1] loss: 0.057
[33,     1] loss: 0.054
[34,     1] loss: 0.054
[35,     1] loss: 0.053
[36,     1] loss: 0.055
[37,     1] loss: 0.054
[38,     1] loss: 0.054
[39,     1] loss: 0.053
[40,     1] loss: 0.054
[41,     1] loss: 0.054
[42,     1] loss: 0.053
[43,     1] loss: 0.052
[44,     1] loss: 0.051
[45,     1] loss: 0.051
[46,     1] loss: 0.051
[47,     1] loss: 0.051
[48,     1] loss: 0.051
[49,     1] loss: 0.050
[50,     1] loss: 0.050
[51,     1] loss: 0.050
[52,     1] loss: 0.050
[53,     1] loss: 0.049
[54,     1] loss: 0.057
[55,     1] loss: 0.193
[56,     1] loss: 0.435
[57,     1] loss: 0.605
[58,     1] loss: 0.408
[59,     1] loss: 0.457
[60,     1] loss: 0.504
[61,     1] loss: 0.520
[62,     1] loss: 0.528
[63,     1] loss: 0.533
[64,     1] loss: 0.531
[65,     1] loss: 0.526
Early stopping applied (best metric=0.26158568263053894)
Finished Training
Total time taken: 287.9625446796417
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8503225806451613, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8666666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8473282442748091, 'Pyrrolidone carboxylic acid Validation Precision': 0.5149388570071777, 'Pyrrolidone carboxylic acid AUC ROC': 0.9336832061068703, 'Pyrrolidone carboxylic acid AUC PR': 0.7578408346514807, 'Pyrrolidone carboxylic acid MCC': 0.5892993405333636, 'Pyrrolidone carboxylic acid F1': 0.6445051307777443, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.2655566453933716, 'Validation Loss (total)': 0.2655566453933716}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002240006113714817,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2390588615,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.54055076556496}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.692
[3,     1] loss: 0.672
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009554719084253722,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3859119608,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.084535474112888}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.614
[4,     1] loss: 0.540
[5,     1] loss: 0.455
[6,     1] loss: 0.365
[7,     1] loss: 0.284
[8,     1] loss: 0.205
[9,     1] loss: 0.132
[10,     1] loss: 0.085
[11,     1] loss: 0.091
[12,     1] loss: 0.059
[13,     1] loss: 0.043
[14,     1] loss: 0.038
[15,     1] loss: 0.034
[16,     1] loss: 0.027
[17,     1] loss: 0.022
[18,     1] loss: 0.023
[19,     1] loss: 0.022
[20,     1] loss: 0.022
[21,     1] loss: 0.020
[22,     1] loss: 0.018
[23,     1] loss: 0.016
[24,     1] loss: 0.015
[25,     1] loss: 0.013
[26,     1] loss: 0.012
[27,     1] loss: 0.011
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004516666295820073,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1611760503,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.528214171040506}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00835329582378276,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 863699133,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.198054686649838}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.653
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007709654449794976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2718457057,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.858392560735393}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.688
[3,     1] loss: 0.655
[4,     1] loss: 0.616
[5,     1] loss: 0.567
[6,     1] loss: 0.514
[7,     1] loss: 0.454
[8,     1] loss: 0.392
[9,     1] loss: 0.331
[10,     1] loss: 0.269
[11,     1] loss: 0.216
[12,     1] loss: 0.161
[13,     1] loss: 0.115
[14,     1] loss: 0.089
[15,     1] loss: 0.067
[16,     1] loss: 0.043
[17,     1] loss: 0.051
[18,     1] loss: 0.037
[19,     1] loss: 0.027
[20,     1] loss: 0.074
[21,     1] loss: 0.034
[22,     1] loss: 0.061
[23,     1] loss: 0.054
[24,     1] loss: 0.032
[25,     1] loss: 0.034
[26,     1] loss: 0.039
[27,     1] loss: 0.036
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00980772955691885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3446591331,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.645454571004198}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.686
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005981643510007519,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1297275876,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.242743651071013}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.666
[3,     1] loss: 0.626
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005350047145061165,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2174361182,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.648548928781129}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.674
[3,     1] loss: 0.609
[4,     1] loss: 0.541
[5,     1] loss: 0.471
[6,     1] loss: 0.410
[7,     1] loss: 0.354
[8,     1] loss: 0.312
[9,     1] loss: 0.282
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006261996028634413,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1290172527,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.9410351242945305}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.698
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007504937740750896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 6897416,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.413145278042089}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.679
[3,     1] loss: 0.617
[4,     1] loss: 0.541
[5,     1] loss: 0.456
[6,     1] loss: 0.373
[7,     1] loss: 0.294
[8,     1] loss: 0.228
[9,     1] loss: 0.159
[10,     1] loss: 0.107
[11,     1] loss: 0.077
[12,     1] loss: 0.061
[13,     1] loss: 0.058
[14,     1] loss: 0.032
[15,     1] loss: 0.039
[16,     1] loss: 0.015
[17,     1] loss: 0.019
[18,     1] loss: 0.015
[19,     1] loss: 0.008
[20,     1] loss: 0.007
[21,     1] loss: 0.008
[22,     1] loss: 0.006
[23,     1] loss: 0.006
[24,     1] loss: 0.006
[25,     1] loss: 0.006
[26,     1] loss: 0.006
[27,     1] loss: 0.007
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008377511507023743,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3434977559,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.701467818043858}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.641
[4,     1] loss: 0.580
[5,     1] loss: 0.509
[6,     1] loss: 0.443
[7,     1] loss: 0.388
[8,     1] loss: 0.344
[9,     1] loss: 0.314
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009887753472302498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1792491971,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.847431493828238}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.627
[4,     1] loss: 0.543
[5,     1] loss: 0.451
[6,     1] loss: 0.387
[7,     1] loss: 0.329
[8,     1] loss: 0.285
[9,     1] loss: 0.246
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006946314686325502,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 468607268,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.890153154849349}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.677
[3,     1] loss: 0.643
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008239854488297418,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 636882743,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.894890448353745}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009830000952137449,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1292621672,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.689622082767194}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009216501626141555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1860636623,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.982572613182954}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.670
[3,     1] loss: 0.608
[4,     1] loss: 0.552
[5,     1] loss: 0.504
[6,     1] loss: 0.459
[7,     1] loss: 0.425
[8,     1] loss: 0.393
[9,     1] loss: 0.369
[10,     1] loss: 0.350
[11,     1] loss: 0.331
[12,     1] loss: 0.311
[13,     1] loss: 0.290
[14,     1] loss: 0.268
[15,     1] loss: 0.240
[16,     1] loss: 0.206
[17,     1] loss: 0.179
[18,     1] loss: 0.166
[19,     1] loss: 0.133
[20,     1] loss: 0.123
[21,     1] loss: 0.127
[22,     1] loss: 0.102
[23,     1] loss: 0.087
[24,     1] loss: 0.078
[25,     1] loss: 0.064
[26,     1] loss: 0.055
[27,     1] loss: 0.049
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008476155164323537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3128499163,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.363800318552486}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.619
[4,     1] loss: 0.538
[5,     1] loss: 0.441
[6,     1] loss: 0.339
[7,     1] loss: 0.260
[8,     1] loss: 0.214
[9,     1] loss: 0.173
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00860232293807357,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1628306728,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.377361161896729}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.618
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006681613247345786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3181474767,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.936905099814056}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.675
[3,     1] loss: 0.624
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00916994495741421,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2083371797,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.911477077344816}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.691
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008071794997274251,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3528845972,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5073848275489414}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.684
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006637293544277223,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1950872426,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.065587063395958}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.693
[3,     1] loss: 0.663
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0076499369621920365,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1308282366,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.128605153200236}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.674
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003245277690877371,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3227516625,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.104273623194022}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.649
[4,     1] loss: 0.632
[5,     1] loss: 0.616
[6,     1] loss: 0.597
[7,     1] loss: 0.578
[8,     1] loss: 0.556
[9,     1] loss: 0.534
[10,     1] loss: 0.515
[11,     1] loss: 0.494
[12,     1] loss: 0.478
[13,     1] loss: 0.463
[14,     1] loss: 0.449
[15,     1] loss: 0.438
[16,     1] loss: 0.430
[17,     1] loss: 0.421
[18,     1] loss: 0.414
[19,     1] loss: 0.410
[20,     1] loss: 0.404
[21,     1] loss: 0.400
[22,     1] loss: 0.399
[23,     1] loss: 0.396
[24,     1] loss: 0.395
[25,     1] loss: 0.393
[26,     1] loss: 0.392
[27,     1] loss: 0.391
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004100861401457933,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2219397641,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.771707963011599}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.686
[3,     1] loss: 0.659
[4,     1] loss: 0.632
[5,     1] loss: 0.605
[6,     1] loss: 0.577
[7,     1] loss: 0.547
[8,     1] loss: 0.516
[9,     1] loss: 0.484
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003475001224657309,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1700667209,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.625125277289503}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.676
[3,     1] loss: 0.638
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007161144353323016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3337381274,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.087071942813353}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.664
[3,     1] loss: 0.596
[4,     1] loss: 0.533
[5,     1] loss: 0.470
[6,     1] loss: 0.404
[7,     1] loss: 0.346
[8,     1] loss: 0.285
[9,     1] loss: 0.232
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005181134376216351,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2278973501,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.090358020846537}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.671
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006935934077512098,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1443683082,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.242699356828763}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.621
[4,     1] loss: 0.553
[5,     1] loss: 0.467
[6,     1] loss: 0.381
[7,     1] loss: 0.298
[8,     1] loss: 0.223
[9,     1] loss: 0.175
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006276484985713676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1324503556,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.241638978196721}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.676
[3,     1] loss: 0.620
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006139660170229073,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1121656077,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.470272189927744}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.687
[3,     1] loss: 0.657
[4,     1] loss: 0.624
[5,     1] loss: 0.586
[6,     1] loss: 0.545
[7,     1] loss: 0.497
[8,     1] loss: 0.450
[9,     1] loss: 0.398
[10,     1] loss: 0.346
[11,     1] loss: 0.293
[12,     1] loss: 0.244
[13,     1] loss: 0.198
[14,     1] loss: 0.168
[15,     1] loss: 0.138
[16,     1] loss: 0.112
[17,     1] loss: 0.091
[18,     1] loss: 0.077
[19,     1] loss: 0.069
[20,     1] loss: 0.061
[21,     1] loss: 0.048
[22,     1] loss: 0.047
[23,     1] loss: 0.046
[24,     1] loss: 0.041
[25,     1] loss: 0.043
[26,     1] loss: 0.038
[27,     1] loss: 0.036
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00975447420257559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3615746940,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.793963523779396}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.682
[3,     1] loss: 0.635
[4,     1] loss: 0.571
[5,     1] loss: 0.503
[6,     1] loss: 0.437
[7,     1] loss: 0.388
[8,     1] loss: 0.336
[9,     1] loss: 0.296
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007048448547911485,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2108362543,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.812056522201926}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.676
[3,     1] loss: 0.632
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00953842967692955,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2421532709,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.255492305578436}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.679
[3,     1] loss: 0.641
[4,     1] loss: 0.602
[5,     1] loss: 0.555
[6,     1] loss: 0.499
[7,     1] loss: 0.439
[8,     1] loss: 0.383
[9,     1] loss: 0.323
[10,     1] loss: 0.260
[11,     1] loss: 0.219
[12,     1] loss: 0.170
[13,     1] loss: 0.142
[14,     1] loss: 0.118
[15,     1] loss: 0.086
[16,     1] loss: 0.065
[17,     1] loss: 0.057
[18,     1] loss: 0.047
[19,     1] loss: 0.042
[20,     1] loss: 0.039
[21,     1] loss: 0.034
[22,     1] loss: 0.032
[23,     1] loss: 0.033
[24,     1] loss: 0.031
[25,     1] loss: 0.030
[26,     1] loss: 0.032
[27,     1] loss: 0.032
[28,     1] loss: 0.031
[29,     1] loss: 0.031
[30,     1] loss: 0.031
[31,     1] loss: 0.032
[32,     1] loss: 0.032
[33,     1] loss: 0.031
[34,     1] loss: 0.032
[35,     1] loss: 0.031
[36,     1] loss: 0.030
[37,     1] loss: 0.030
[38,     1] loss: 0.031
[39,     1] loss: 0.030
[40,     1] loss: 0.030
[41,     1] loss: 0.029
[42,     1] loss: 0.029
[43,     1] loss: 0.030
[44,     1] loss: 0.029
[45,     1] loss: 0.029
[46,     1] loss: 0.033
[47,     1] loss: 0.094
[48,     1] loss: 0.564
[49,     1] loss: 0.819
[50,     1] loss: 0.531
[51,     1] loss: 0.560
[52,     1] loss: 0.594
[53,     1] loss: 0.613
[54,     1] loss: 0.620
[55,     1] loss: 0.623
[56,     1] loss: 0.622
[57,     1] loss: 0.618
[58,     1] loss: 0.614
[59,     1] loss: 0.607
[60,     1] loss: 0.596
[61,     1] loss: 0.585
[62,     1] loss: 0.569
[63,     1] loss: 0.549
[64,     1] loss: 0.527
[65,     1] loss: 0.502
[66,     1] loss: 0.474
Early stopping applied (best metric=0.23291578888893127)
Finished Training
Total time taken: 300.43417525291443
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.649
[4,     1] loss: 0.605
[5,     1] loss: 0.558
[6,     1] loss: 0.503
[7,     1] loss: 0.449
[8,     1] loss: 0.407
[9,     1] loss: 0.378
[10,     1] loss: 0.344
[11,     1] loss: 0.321
[12,     1] loss: 0.303
[13,     1] loss: 0.278
[14,     1] loss: 0.261
[15,     1] loss: 0.234
[16,     1] loss: 0.208
[17,     1] loss: 0.182
[18,     1] loss: 0.150
[19,     1] loss: 0.122
[20,     1] loss: 0.093
[21,     1] loss: 0.073
[22,     1] loss: 0.067
[23,     1] loss: 0.046
[24,     1] loss: 0.041
[25,     1] loss: 0.031
[26,     1] loss: 0.029
[27,     1] loss: 0.027
[28,     1] loss: 0.041
[29,     1] loss: 0.096
[30,     1] loss: 0.346
[31,     1] loss: 0.304
[32,     1] loss: 0.345
[33,     1] loss: 0.328
[34,     1] loss: 0.333
[35,     1] loss: 0.329
[36,     1] loss: 0.309
[37,     1] loss: 0.279
[38,     1] loss: 0.242
[39,     1] loss: 0.205
[40,     1] loss: 0.171
[41,     1] loss: 0.137
[42,     1] loss: 0.110
[43,     1] loss: 0.088
[44,     1] loss: 0.075
[45,     1] loss: 0.062
[46,     1] loss: 0.052
[47,     1] loss: 0.049
[48,     1] loss: 0.046
[49,     1] loss: 0.046
[50,     1] loss: 0.046
[51,     1] loss: 0.046
[52,     1] loss: 0.046
[53,     1] loss: 0.048
[54,     1] loss: 0.048
[55,     1] loss: 0.050
[56,     1] loss: 0.049
[57,     1] loss: 0.049
[58,     1] loss: 0.049
[59,     1] loss: 0.050
[60,     1] loss: 0.050
[61,     1] loss: 0.048
[62,     1] loss: 0.049
[63,     1] loss: 0.047
[64,     1] loss: 0.046
[65,     1] loss: 0.046
[66,     1] loss: 0.050
[67,     1] loss: 0.186
[68,     1] loss: 0.591
[69,     1] loss: 0.539
[70,     1] loss: 0.506
[71,     1] loss: 0.472
[72,     1] loss: 0.456
[73,     1] loss: 0.435
[74,     1] loss: 0.412
[75,     1] loss: 0.389
[76,     1] loss: 0.357
[77,     1] loss: 0.317
[78,     1] loss: 0.273
[79,     1] loss: 0.220
[80,     1] loss: 0.185
[81,     1] loss: 0.257
[82,     1] loss: 0.142
[83,     1] loss: 0.114
[84,     1] loss: 0.101
[85,     1] loss: 0.085
[86,     1] loss: 0.078
[87,     1] loss: 0.062
[88,     1] loss: 0.059
[89,     1] loss: 0.068
[90,     1] loss: 0.097
[91,     1] loss: 0.068
Early stopping applied (best metric=0.21786144375801086)
Finished Training
Total time taken: 398.7858667373657
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.685
[3,     1] loss: 0.639
[4,     1] loss: 0.591
[5,     1] loss: 0.534
[6,     1] loss: 0.476
[7,     1] loss: 0.421
[8,     1] loss: 0.364
[9,     1] loss: 0.302
[10,     1] loss: 0.251
[11,     1] loss: 0.196
[12,     1] loss: 0.162
[13,     1] loss: 0.135
[14,     1] loss: 0.115
[15,     1] loss: 0.095
[16,     1] loss: 0.092
[17,     1] loss: 0.075
[18,     1] loss: 0.070
[19,     1] loss: 0.063
[20,     1] loss: 0.058
[21,     1] loss: 0.053
[22,     1] loss: 0.049
[23,     1] loss: 0.044
[24,     1] loss: 0.041
[25,     1] loss: 0.038
[26,     1] loss: 0.036
[27,     1] loss: 0.034
[28,     1] loss: 0.033
[29,     1] loss: 0.034
[30,     1] loss: 0.034
[31,     1] loss: 0.034
[32,     1] loss: 0.034
[33,     1] loss: 0.035
[34,     1] loss: 0.035
[35,     1] loss: 0.035
[36,     1] loss: 0.035
[37,     1] loss: 0.036
[38,     1] loss: 0.034
[39,     1] loss: 0.034
[40,     1] loss: 0.034
[41,     1] loss: 0.033
[42,     1] loss: 0.033
[43,     1] loss: 0.033
[44,     1] loss: 0.032
[45,     1] loss: 0.042
[46,     1] loss: 0.145
[47,     1] loss: 0.693
[48,     1] loss: 0.662
[49,     1] loss: 0.535
[50,     1] loss: 0.509
[51,     1] loss: 0.494
[52,     1] loss: 0.466
[53,     1] loss: 0.450
[54,     1] loss: 0.429
[55,     1] loss: 0.396
[56,     1] loss: 0.365
[57,     1] loss: 0.322
[58,     1] loss: 0.282
[59,     1] loss: 0.282
[60,     1] loss: 0.206
[61,     1] loss: 0.346
[62,     1] loss: 0.328
[63,     1] loss: 0.311
[64,     1] loss: 0.223
[65,     1] loss: 0.293
[66,     1] loss: 0.216
[67,     1] loss: 0.222
[68,     1] loss: 0.185
[69,     1] loss: 0.142
[70,     1] loss: 0.125
[71,     1] loss: 0.085
[72,     1] loss: 0.073
[73,     1] loss: 0.064
[74,     1] loss: 0.058
[75,     1] loss: 0.051
[76,     1] loss: 0.047
[77,     1] loss: 0.048
[78,     1] loss: 0.049
[79,     1] loss: 0.053
[80,     1] loss: 0.055
[81,     1] loss: 0.057
[82,     1] loss: 0.059
[83,     1] loss: 0.059
[84,     1] loss: 0.061
[85,     1] loss: 0.060
[86,     1] loss: 0.059
[87,     1] loss: 0.058
[88,     1] loss: 0.057
[89,     1] loss: 0.055
[90,     1] loss: 0.054
[91,     1] loss: 0.054
[92,     1] loss: 0.053
[93,     1] loss: 0.053
[94,     1] loss: 0.053
[95,     1] loss: 0.052
[96,     1] loss: 0.053
[97,     1] loss: 0.054
[98,     1] loss: 0.068
[99,     1] loss: 0.899
[100,     1] loss: 1.487
[101,     1] loss: 0.767
[102,     1] loss: 0.688
[103,     1] loss: 0.690
[104,     1] loss: 0.692
[105,     1] loss: 0.693
[106,     1] loss: 0.693
[107,     1] loss: 0.693
[108,     1] loss: 0.693
[109,     1] loss: 0.693
[110,     1] loss: 0.693
[111,     1] loss: 0.693
[112,     1] loss: 0.693
[113,     1] loss: 0.693
[114,     1] loss: 0.693
[115,     1] loss: 0.693
[116,     1] loss: 0.693
[117,     1] loss: 0.693
[118,     1] loss: 0.693
[119,     1] loss: 0.693
Early stopping applied (best metric=0.21454952657222748)
Finished Training
Total time taken: 552.69868683815
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.634
[4,     1] loss: 0.581
[5,     1] loss: 0.524
[6,     1] loss: 0.471
[7,     1] loss: 0.418
[8,     1] loss: 0.367
[9,     1] loss: 0.321
[10,     1] loss: 0.277
[11,     1] loss: 0.253
[12,     1] loss: 0.227
[13,     1] loss: 0.200
[14,     1] loss: 0.179
[15,     1] loss: 0.156
[16,     1] loss: 0.135
[17,     1] loss: 0.123
[18,     1] loss: 0.108
[19,     1] loss: 0.099
[20,     1] loss: 0.109
[21,     1] loss: 0.121
[22,     1] loss: 0.079
[23,     1] loss: 0.084
[24,     1] loss: 0.084
[25,     1] loss: 0.072
[26,     1] loss: 0.069
[27,     1] loss: 0.059
[28,     1] loss: 0.053
[29,     1] loss: 0.048
[30,     1] loss: 0.044
[31,     1] loss: 0.042
[32,     1] loss: 0.041
[33,     1] loss: 0.040
[34,     1] loss: 0.041
[35,     1] loss: 0.041
[36,     1] loss: 0.043
[37,     1] loss: 0.044
[38,     1] loss: 0.045
[39,     1] loss: 0.046
[40,     1] loss: 0.048
[41,     1] loss: 0.048
[42,     1] loss: 0.046
[43,     1] loss: 0.047
[44,     1] loss: 0.045
[45,     1] loss: 0.045
[46,     1] loss: 0.045
[47,     1] loss: 0.043
[48,     1] loss: 0.043
[49,     1] loss: 0.042
[50,     1] loss: 0.050
[51,     1] loss: 0.510
[52,     1] loss: 0.592
[53,     1] loss: 0.539
[54,     1] loss: 0.503
[55,     1] loss: 0.486
[56,     1] loss: 0.464
[57,     1] loss: 0.453
[58,     1] loss: 0.433
[59,     1] loss: 0.390
[60,     1] loss: 0.365
[61,     1] loss: 0.316
[62,     1] loss: 0.276
[63,     1] loss: 0.231
[64,     1] loss: 0.192
[65,     1] loss: 0.166
[66,     1] loss: 0.136
[67,     1] loss: 0.129
[68,     1] loss: 0.208
[69,     1] loss: 0.426
[70,     1] loss: 0.372
[71,     1] loss: 0.420
[72,     1] loss: 0.291
[73,     1] loss: 0.310
[74,     1] loss: 0.360
[75,     1] loss: 0.346
[76,     1] loss: 0.316
[77,     1] loss: 0.303
Early stopping applied (best metric=0.2093781977891922)
Finished Training
Total time taken: 352.34011483192444
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.658
[3,     1] loss: 0.560
[4,     1] loss: 0.494
[5,     1] loss: 0.438
[6,     1] loss: 0.408
[7,     1] loss: 0.371
[8,     1] loss: 0.348
[9,     1] loss: 0.341
[10,     1] loss: 0.336
[11,     1] loss: 0.334
[12,     1] loss: 0.331
[13,     1] loss: 0.329
[14,     1] loss: 0.326
[15,     1] loss: 0.325
[16,     1] loss: 0.324
[17,     1] loss: 0.323
[18,     1] loss: 0.319
[19,     1] loss: 0.315
[20,     1] loss: 0.310
[21,     1] loss: 0.304
[22,     1] loss: 0.294
[23,     1] loss: 0.284
[24,     1] loss: 0.275
[25,     1] loss: 0.263
[26,     1] loss: 0.247
[27,     1] loss: 0.233
[28,     1] loss: 0.252
[29,     1] loss: 0.308
[30,     1] loss: 0.448
[31,     1] loss: 0.391
[32,     1] loss: 0.389
[33,     1] loss: 0.343
[34,     1] loss: 0.341
[35,     1] loss: 0.320
[36,     1] loss: 0.276
[37,     1] loss: 0.235
[38,     1] loss: 0.198
[39,     1] loss: 0.169
[40,     1] loss: 0.144
[41,     1] loss: 0.125
[42,     1] loss: 0.106
[43,     1] loss: 0.089
[44,     1] loss: 0.080
[45,     1] loss: 0.072
[46,     1] loss: 0.068
[47,     1] loss: 0.065
[48,     1] loss: 0.065
[49,     1] loss: 0.064
[50,     1] loss: 0.065
[51,     1] loss: 0.063
[52,     1] loss: 0.062
[53,     1] loss: 0.062
[54,     1] loss: 0.061
[55,     1] loss: 0.062
[56,     1] loss: 0.062
[57,     1] loss: 0.062
[58,     1] loss: 0.063
[59,     1] loss: 0.062
[60,     1] loss: 0.062
[61,     1] loss: 0.061
[62,     1] loss: 0.062
[63,     1] loss: 0.063
[64,     1] loss: 0.062
[65,     1] loss: 0.063
[66,     1] loss: 0.080
[67,     1] loss: 0.093
[68,     1] loss: 0.182
[69,     1] loss: 0.468
[70,     1] loss: 0.405
[71,     1] loss: 0.711
[72,     1] loss: 0.421
[73,     1] loss: 0.453
[74,     1] loss: 0.490
[75,     1] loss: 0.510
[76,     1] loss: 0.517
[77,     1] loss: 0.521
[78,     1] loss: 0.521
[79,     1] loss: 0.519
[80,     1] loss: 0.517
[81,     1] loss: 0.512
[82,     1] loss: 0.505
[83,     1] loss: 0.497
[84,     1] loss: 0.488
[85,     1] loss: 0.477
[86,     1] loss: 0.464
[87,     1] loss: 0.448
[88,     1] loss: 0.428
[89,     1] loss: 0.406
[90,     1] loss: 0.379
[91,     1] loss: 0.345
[92,     1] loss: 0.303
[93,     1] loss: 0.258
[94,     1] loss: 0.214
[95,     1] loss: 0.165
[96,     1] loss: 0.132
[97,     1] loss: 0.105
[98,     1] loss: 0.086
[99,     1] loss: 0.077
[100,     1] loss: 0.069
[101,     1] loss: 0.066
[102,     1] loss: 0.063
[103,     1] loss: 0.064
[104,     1] loss: 0.067
[105,     1] loss: 0.070
[106,     1] loss: 0.074
[107,     1] loss: 0.075
[108,     1] loss: 0.080
[109,     1] loss: 0.085
[110,     1] loss: 0.086
[111,     1] loss: 0.088
[112,     1] loss: 0.110
[113,     1] loss: 0.605
[114,     1] loss: 0.397
[115,     1] loss: 0.423
[116,     1] loss: 0.393
[117,     1] loss: 0.385
[118,     1] loss: 0.384
[119,     1] loss: 0.367
[120,     1] loss: 0.354
[121,     1] loss: 0.332
[122,     1] loss: 0.313
[123,     1] loss: 0.296
[124,     1] loss: 0.280
[125,     1] loss: 0.266
[126,     1] loss: 0.253
[127,     1] loss: 0.240
[128,     1] loss: 0.229
[129,     1] loss: 0.213
[130,     1] loss: 0.198
[131,     1] loss: 0.185
[132,     1] loss: 0.172
[133,     1] loss: 0.162
[134,     1] loss: 0.147
[135,     1] loss: 0.134
[136,     1] loss: 0.124
[137,     1] loss: 0.114
[138,     1] loss: 0.106
[139,     1] loss: 0.101
[140,     1] loss: 0.095
[141,     1] loss: 0.092
[142,     1] loss: 0.091
[143,     1] loss: 0.089
[144,     1] loss: 0.089
[145,     1] loss: 0.087
[146,     1] loss: 0.088
[147,     1] loss: 0.088
Early stopping applied (best metric=0.24432815611362457)
Finished Training
Total time taken: 657.8335318565369
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8606451612903225, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9083333333333333, 'Pyrrolidone carboxylic acid Validation Specificity': 0.851908396946565, 'Pyrrolidone carboxylic acid Validation Precision': 0.5478765592070197, 'Pyrrolidone carboxylic acid AUC ROC': 0.9551208651399491, 'Pyrrolidone carboxylic acid AUC PR': 0.8451405368103334, 'Pyrrolidone carboxylic acid MCC': 0.6328312068862847, 'Pyrrolidone carboxylic acid F1': 0.6774607138759383, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.22380662262439727, 'Validation Loss (total)': 0.22380662262439727}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008455424899632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1264591761,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.288975084397622}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.687
[3,     1] loss: 0.653
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00951178235307613,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4136594677,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.89720346375936}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.692
[3,     1] loss: 0.647
[4,     1] loss: 0.583
[5,     1] loss: 0.519
[6,     1] loss: 0.455
[7,     1] loss: 0.403
[8,     1] loss: 0.338
[9,     1] loss: 0.318
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007757566307235638,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1461347175,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.023356659697437}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.653
[3,     1] loss: 0.569
[4,     1] loss: 0.483
[5,     1] loss: 0.399
[6,     1] loss: 0.324
[7,     1] loss: 0.261
[8,     1] loss: 0.192
[9,     1] loss: 0.135
[10,     1] loss: 0.096
[11,     1] loss: 0.063
[12,     1] loss: 0.040
[13,     1] loss: 0.031
[14,     1] loss: 0.024
[15,     1] loss: 0.017
[16,     1] loss: 0.016
[17,     1] loss: 0.010
[18,     1] loss: 0.009
[19,     1] loss: 0.005
[20,     1] loss: 0.006
[21,     1] loss: 0.004
[22,     1] loss: 0.005
[23,     1] loss: 0.004
[24,     1] loss: 0.005
[25,     1] loss: 0.006
[26,     1] loss: 0.006
[27,     1] loss: 0.007
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009671948306488301,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 500002259,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.436680962184987}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.635
[4,     1] loss: 0.549
[5,     1] loss: 0.449
[6,     1] loss: 0.354
[7,     1] loss: 0.280
[8,     1] loss: 0.230
[9,     1] loss: 0.198
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008467120120380336,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 325805962,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.24416981834014}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.652
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009714382763102223,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3580917807,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.27321610884042}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.636
[4,     1] loss: 0.572
[5,     1] loss: 0.497
[6,     1] loss: 0.417
[7,     1] loss: 0.339
[8,     1] loss: 0.266
[9,     1] loss: 0.193
[10,     1] loss: 0.145
[11,     1] loss: 0.149
[12,     1] loss: 0.096
[13,     1] loss: 0.060
[14,     1] loss: 0.087
[15,     1] loss: 0.062
[16,     1] loss: 0.070
[17,     1] loss: 0.038
[18,     1] loss: 0.043
[19,     1] loss: 0.045
[20,     1] loss: 0.032
[21,     1] loss: 0.028
[22,     1] loss: 0.026
[23,     1] loss: 0.024
[24,     1] loss: 0.021
[25,     1] loss: 0.019
[26,     1] loss: 0.018
[27,     1] loss: 0.017
[28,     1] loss: 0.016
[29,     1] loss: 0.015
[30,     1] loss: 0.015
[31,     1] loss: 0.014
[32,     1] loss: 0.015
[33,     1] loss: 0.015
[34,     1] loss: 0.015
[35,     1] loss: 0.015
[36,     1] loss: 0.015
[37,     1] loss: 0.015
[38,     1] loss: 0.014
[39,     1] loss: 0.014
[40,     1] loss: 0.014
[41,     1] loss: 0.014
[42,     1] loss: 0.013
[43,     1] loss: 0.013
[44,     1] loss: 0.012
[45,     1] loss: 0.013
[46,     1] loss: 0.012
[47,     1] loss: 0.012
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.011
[53,     1] loss: 0.011
[54,     1] loss: 0.011
[55,     1] loss: 0.012
[56,     1] loss: 0.011
[57,     1] loss: 0.012
[58,     1] loss: 0.011
[59,     1] loss: 0.011
[60,     1] loss: 0.011
[61,     1] loss: 0.011
[62,     1] loss: 0.010
[63,     1] loss: 0.011
[64,     1] loss: 0.012
[65,     1] loss: 0.163
[66,     1] loss: 1.607
[67,     1] loss: 0.716
[68,     1] loss: 0.597
[69,     1] loss: 0.625
[70,     1] loss: 0.629
[71,     1] loss: 0.625
[72,     1] loss: 0.622
[73,     1] loss: 0.614
[74,     1] loss: 0.606
[75,     1] loss: 0.596
[76,     1] loss: 0.584
[77,     1] loss: 0.570
[78,     1] loss: 0.554
[79,     1] loss: 0.535
[80,     1] loss: 0.512
[81,     1] loss: 0.481
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007016504783118625,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2858973309,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.422374689858867}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.681
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00797821450451391,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3208418028,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.399642907897406}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.695
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009955238733829114,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3813706892,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.031651180898008}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.633
[4,     1] loss: 0.583
[5,     1] loss: 0.521
[6,     1] loss: 0.457
[7,     1] loss: 0.387
[8,     1] loss: 0.315
[9,     1] loss: 0.243
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007087077488446399,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2268364349,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.3779376488427495}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.678
[3,     1] loss: 0.618
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0077537536792603974,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 526535649,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.208456327249888}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.698
[3,     1] loss: 0.678
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008182201585684171,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3999433919,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.17840566139243}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.712
[2,     1] loss: 0.694
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00784553056053141,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3439909048,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.3282765339041793}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.700
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006939706446352229,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1718404036,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.51475673021421}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.668
[3,     1] loss: 0.601
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009947803155688117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2239041771,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.842718913184306}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.687
[3,     1] loss: 0.668
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007729289671988926,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2070912152,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.082009177977403}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.647
[4,     1] loss: 0.614
[5,     1] loss: 0.581
[6,     1] loss: 0.547
[7,     1] loss: 0.513
[8,     1] loss: 0.477
[9,     1] loss: 0.439
[10,     1] loss: 0.403
[11,     1] loss: 0.370
[12,     1] loss: 0.340
[13,     1] loss: 0.310
[14,     1] loss: 0.287
[15,     1] loss: 0.262
[16,     1] loss: 0.240
[17,     1] loss: 0.216
[18,     1] loss: 0.194
[19,     1] loss: 0.171
[20,     1] loss: 0.151
[21,     1] loss: 0.132
[22,     1] loss: 0.119
[23,     1] loss: 0.102
[24,     1] loss: 0.091
[25,     1] loss: 0.082
[26,     1] loss: 0.075
[27,     1] loss: 0.070
[28,     1] loss: 0.065
[29,     1] loss: 0.065
[30,     1] loss: 0.064
[31,     1] loss: 0.062
[32,     1] loss: 0.062
[33,     1] loss: 0.063
[34,     1] loss: 0.064
[35,     1] loss: 0.063
[36,     1] loss: 0.064
[37,     1] loss: 0.065
[38,     1] loss: 0.067
[39,     1] loss: 0.066
[40,     1] loss: 0.066
[41,     1] loss: 0.080
[42,     1] loss: 0.086
[43,     1] loss: 0.317
[44,     1] loss: 0.272
[45,     1] loss: 0.490
[46,     1] loss: 0.416
[47,     1] loss: 0.497
[48,     1] loss: 0.439
[49,     1] loss: 0.422
[50,     1] loss: 0.427
[51,     1] loss: 0.444
[52,     1] loss: 0.432
[53,     1] loss: 0.416
[54,     1] loss: 0.402
[55,     1] loss: 0.391
[56,     1] loss: 0.370
[57,     1] loss: 0.344
[58,     1] loss: 0.319
[59,     1] loss: 0.291
[60,     1] loss: 0.264
[61,     1] loss: 0.233
[62,     1] loss: 0.212
[63,     1] loss: 0.186
[64,     1] loss: 0.167
[65,     1] loss: 0.142
[66,     1] loss: 0.131
[67,     1] loss: 0.118
[68,     1] loss: 0.108
[69,     1] loss: 0.098
[70,     1] loss: 0.092
[71,     1] loss: 0.090
[72,     1] loss: 0.087
[73,     1] loss: 0.086
[74,     1] loss: 0.085
[75,     1] loss: 0.085
[76,     1] loss: 0.085
[77,     1] loss: 0.086
[78,     1] loss: 0.085
[79,     1] loss: 0.087
[80,     1] loss: 0.087
[81,     1] loss: 0.089
[82,     1] loss: 0.090
[83,     1] loss: 0.090
[84,     1] loss: 0.091
[85,     1] loss: 0.100
[86,     1] loss: 0.120
[87,     1] loss: 0.248
[88,     1] loss: 0.221
[89,     1] loss: 0.314
Early stopping applied (best metric=0.266343891620636)
Finished Training
Total time taken: 401.78038144111633
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.696
[3,     1] loss: 0.669
[4,     1] loss: 0.638
[5,     1] loss: 0.604
[6,     1] loss: 0.566
[7,     1] loss: 0.527
[8,     1] loss: 0.489
[9,     1] loss: 0.454
[10,     1] loss: 0.425
[11,     1] loss: 0.404
[12,     1] loss: 0.382
[13,     1] loss: 0.369
[14,     1] loss: 0.361
[15,     1] loss: 0.355
[16,     1] loss: 0.351
[17,     1] loss: 0.347
[18,     1] loss: 0.345
[19,     1] loss: 0.341
[20,     1] loss: 0.336
[21,     1] loss: 0.331
[22,     1] loss: 0.326
[23,     1] loss: 0.320
[24,     1] loss: 0.313
[25,     1] loss: 0.302
[26,     1] loss: 0.290
[27,     1] loss: 0.277
[28,     1] loss: 0.258
[29,     1] loss: 0.239
[30,     1] loss: 0.219
[31,     1] loss: 0.200
[32,     1] loss: 0.176
[33,     1] loss: 0.154
[34,     1] loss: 0.137
[35,     1] loss: 0.120
[36,     1] loss: 0.124
[37,     1] loss: 0.302
[38,     1] loss: 0.490
[39,     1] loss: 0.499
[40,     1] loss: 0.480
[41,     1] loss: 0.472
[42,     1] loss: 0.474
[43,     1] loss: 0.466
[44,     1] loss: 0.461
[45,     1] loss: 0.454
[46,     1] loss: 0.439
[47,     1] loss: 0.425
[48,     1] loss: 0.401
[49,     1] loss: 0.373
[50,     1] loss: 0.340
[51,     1] loss: 0.298
[52,     1] loss: 0.255
[53,     1] loss: 0.213
[54,     1] loss: 0.177
[55,     1] loss: 0.149
[56,     1] loss: 0.128
[57,     1] loss: 0.108
[58,     1] loss: 0.095
[59,     1] loss: 0.087
[60,     1] loss: 0.077
[61,     1] loss: 0.071
[62,     1] loss: 0.069
[63,     1] loss: 0.069
[64,     1] loss: 0.070
[65,     1] loss: 0.073
[66,     1] loss: 0.074
[67,     1] loss: 0.076
[68,     1] loss: 0.078
[69,     1] loss: 0.081
[70,     1] loss: 0.082
[71,     1] loss: 0.083
[72,     1] loss: 0.085
[73,     1] loss: 0.085
[74,     1] loss: 0.085
[75,     1] loss: 0.084
[76,     1] loss: 0.083
[77,     1] loss: 0.081
[78,     1] loss: 0.083
[79,     1] loss: 0.082
[80,     1] loss: 0.081
[81,     1] loss: 0.082
[82,     1] loss: 0.082
[83,     1] loss: 0.083
[84,     1] loss: 0.084
[85,     1] loss: 0.106
[86,     1] loss: 0.302
[87,     1] loss: 0.585
[88,     1] loss: 1.038
[89,     1] loss: 0.577
[90,     1] loss: 0.538
[91,     1] loss: 0.607
[92,     1] loss: 0.625
[93,     1] loss: 0.622
[94,     1] loss: 0.623
[95,     1] loss: 0.629
[96,     1] loss: 0.633
[97,     1] loss: 0.637
[98,     1] loss: 0.639
[99,     1] loss: 0.639
[100,     1] loss: 0.638
[101,     1] loss: 0.637
[102,     1] loss: 0.634
[103,     1] loss: 0.632
[104,     1] loss: 0.629
[105,     1] loss: 0.624
[106,     1] loss: 0.620
[107,     1] loss: 0.614
[108,     1] loss: 0.610
[109,     1] loss: 0.604
[110,     1] loss: 0.596
Early stopping applied (best metric=0.2152252197265625)
Finished Training
Total time taken: 503.76333928108215
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.628
[5,     1] loss: 0.594
[6,     1] loss: 0.560
[7,     1] loss: 0.524
[8,     1] loss: 0.489
[9,     1] loss: 0.451
[10,     1] loss: 0.413
[11,     1] loss: 0.374
[12,     1] loss: 0.344
[13,     1] loss: 0.311
[14,     1] loss: 0.285
[15,     1] loss: 0.261
[16,     1] loss: 0.238
[17,     1] loss: 0.219
[18,     1] loss: 0.203
[19,     1] loss: 0.186
[20,     1] loss: 0.174
[21,     1] loss: 0.160
[22,     1] loss: 0.150
[23,     1] loss: 0.141
[24,     1] loss: 0.131
[25,     1] loss: 0.124
[26,     1] loss: 0.118
[27,     1] loss: 0.114
[28,     1] loss: 0.110
[29,     1] loss: 0.105
[30,     1] loss: 0.103
[31,     1] loss: 0.100
[32,     1] loss: 0.099
[33,     1] loss: 0.098
[34,     1] loss: 0.095
[35,     1] loss: 0.094
[36,     1] loss: 0.094
[37,     1] loss: 0.095
[38,     1] loss: 0.093
[39,     1] loss: 0.093
[40,     1] loss: 0.093
[41,     1] loss: 0.095
[42,     1] loss: 0.105
[43,     1] loss: 0.171
[44,     1] loss: 0.374
[45,     1] loss: 0.339
[46,     1] loss: 0.725
[47,     1] loss: 0.405
[48,     1] loss: 0.533
[49,     1] loss: 0.510
[50,     1] loss: 0.482
[51,     1] loss: 0.502
[52,     1] loss: 0.514
[53,     1] loss: 0.514
[54,     1] loss: 0.508
[55,     1] loss: 0.508
[56,     1] loss: 0.503
[57,     1] loss: 0.493
[58,     1] loss: 0.484
[59,     1] loss: 0.469
[60,     1] loss: 0.457
[61,     1] loss: 0.444
[62,     1] loss: 0.432
[63,     1] loss: 0.414
[64,     1] loss: 0.401
[65,     1] loss: 0.386
[66,     1] loss: 0.380
Early stopping applied (best metric=0.2852376103401184)
Finished Training
Total time taken: 295.3899447917938
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.679
[3,     1] loss: 0.631
[4,     1] loss: 0.583
[5,     1] loss: 0.537
[6,     1] loss: 0.493
[7,     1] loss: 0.450
[8,     1] loss: 0.410
[9,     1] loss: 0.382
[10,     1] loss: 0.357
[11,     1] loss: 0.340
[12,     1] loss: 0.333
[13,     1] loss: 0.323
[14,     1] loss: 0.316
[15,     1] loss: 0.311
[16,     1] loss: 0.303
[17,     1] loss: 0.296
[18,     1] loss: 0.288
[19,     1] loss: 0.276
[20,     1] loss: 0.264
[21,     1] loss: 0.251
[22,     1] loss: 0.234
[23,     1] loss: 0.218
[24,     1] loss: 0.198
[25,     1] loss: 0.179
[26,     1] loss: 0.160
[27,     1] loss: 0.189
[28,     1] loss: 0.234
[29,     1] loss: 0.397
[30,     1] loss: 0.282
[31,     1] loss: 0.257
[32,     1] loss: 0.226
[33,     1] loss: 0.202
[34,     1] loss: 0.177
[35,     1] loss: 0.156
[36,     1] loss: 0.133
[37,     1] loss: 0.111
[38,     1] loss: 0.098
[39,     1] loss: 0.089
[40,     1] loss: 0.077
[41,     1] loss: 0.073
[42,     1] loss: 0.069
[43,     1] loss: 0.070
[44,     1] loss: 0.070
[45,     1] loss: 0.070
[46,     1] loss: 0.072
[47,     1] loss: 0.075
[48,     1] loss: 0.075
[49,     1] loss: 0.077
[50,     1] loss: 0.079
[51,     1] loss: 0.080
[52,     1] loss: 0.082
[53,     1] loss: 0.082
[54,     1] loss: 0.081
[55,     1] loss: 0.081
[56,     1] loss: 0.080
[57,     1] loss: 0.078
[58,     1] loss: 0.079
[59,     1] loss: 0.077
[60,     1] loss: 0.076
[61,     1] loss: 0.077
[62,     1] loss: 0.079
[63,     1] loss: 0.092
[64,     1] loss: 0.226
[65,     1] loss: 0.911
[66,     1] loss: 0.612
[67,     1] loss: 0.513
[68,     1] loss: 0.527
[69,     1] loss: 0.561
[70,     1] loss: 0.545
[71,     1] loss: 0.549
[72,     1] loss: 0.552
[73,     1] loss: 0.550
[74,     1] loss: 0.545
[75,     1] loss: 0.541
[76,     1] loss: 0.536
[77,     1] loss: 0.533
[78,     1] loss: 0.526
[79,     1] loss: 0.519
[80,     1] loss: 0.511
[81,     1] loss: 0.504
[82,     1] loss: 0.494
[83,     1] loss: 0.483
[84,     1] loss: 0.473
[85,     1] loss: 0.461
[86,     1] loss: 0.450
[87,     1] loss: 0.442
[88,     1] loss: 0.430
[89,     1] loss: 0.421
[90,     1] loss: 0.416
[91,     1] loss: 0.408
[92,     1] loss: 0.402
[93,     1] loss: 0.396
[94,     1] loss: 0.393
[95,     1] loss: 0.390
Early stopping applied (best metric=0.2696942090988159)
Finished Training
Total time taken: 418.24894428253174
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.683
[3,     1] loss: 0.640
[4,     1] loss: 0.594
[5,     1] loss: 0.545
[6,     1] loss: 0.504
[7,     1] loss: 0.471
[8,     1] loss: 0.442
[9,     1] loss: 0.427
[10,     1] loss: 0.411
[11,     1] loss: 0.394
[12,     1] loss: 0.379
[13,     1] loss: 0.375
[14,     1] loss: 0.363
[15,     1] loss: 0.358
[16,     1] loss: 0.358
[17,     1] loss: 0.350
[18,     1] loss: 0.349
[19,     1] loss: 0.345
[20,     1] loss: 0.344
[21,     1] loss: 0.340
[22,     1] loss: 0.338
[23,     1] loss: 0.335
[24,     1] loss: 0.329
[25,     1] loss: 0.320
[26,     1] loss: 0.315
[27,     1] loss: 0.323
[28,     1] loss: 0.327
[29,     1] loss: 0.306
[30,     1] loss: 0.294
[31,     1] loss: 0.284
[32,     1] loss: 0.258
[33,     1] loss: 0.239
[34,     1] loss: 0.215
[35,     1] loss: 0.196
[36,     1] loss: 0.173
[37,     1] loss: 0.149
[38,     1] loss: 0.128
[39,     1] loss: 0.109
[40,     1] loss: 0.095
[41,     1] loss: 0.081
[42,     1] loss: 0.072
[43,     1] loss: 0.064
[44,     1] loss: 0.058
[45,     1] loss: 0.056
[46,     1] loss: 0.052
[47,     1] loss: 0.052
[48,     1] loss: 0.051
[49,     1] loss: 0.051
[50,     1] loss: 0.053
[51,     1] loss: 0.055
[52,     1] loss: 0.055
[53,     1] loss: 0.056
[54,     1] loss: 0.057
[55,     1] loss: 0.059
[56,     1] loss: 0.060
[57,     1] loss: 0.061
[58,     1] loss: 0.060
[59,     1] loss: 0.060
[60,     1] loss: 0.059
[61,     1] loss: 0.061
[62,     1] loss: 0.106
[63,     1] loss: 0.366
[64,     1] loss: 0.922
[65,     1] loss: 0.465
[66,     1] loss: 0.553
[67,     1] loss: 0.597
[68,     1] loss: 0.616
[69,     1] loss: 0.622
[70,     1] loss: 0.622
[71,     1] loss: 0.624
[72,     1] loss: 0.627
[73,     1] loss: 0.630
[74,     1] loss: 0.629
[75,     1] loss: 0.630
[76,     1] loss: 0.629
[77,     1] loss: 0.627
[78,     1] loss: 0.623
[79,     1] loss: 0.618
[80,     1] loss: 0.614
[81,     1] loss: 0.610
[82,     1] loss: 0.603
[83,     1] loss: 0.595
[84,     1] loss: 0.588
[85,     1] loss: 0.580
Early stopping applied (best metric=0.25375401973724365)
Finished Training
Total time taken: 386.3064298629761
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8883870967741936, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8250000000000001, 'Pyrrolidone carboxylic acid Validation Specificity': 0.9, 'Pyrrolidone carboxylic acid Validation Precision': 0.6114871874675796, 'Pyrrolidone carboxylic acid AUC ROC': 0.9492048346055979, 'Pyrrolidone carboxylic acid AUC PR': 0.8482789361394555, 'Pyrrolidone carboxylic acid MCC': 0.6459919219370367, 'Pyrrolidone carboxylic acid F1': 0.6985073515086923, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.2580509901046753, 'Validation Loss (total)': 0.2580509901046753}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008141849096848636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 875460178,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.156792847566905}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.670
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009775746145444046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2233272281,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.543332877781847}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.656
[4,     1] loss: 0.601
[5,     1] loss: 0.524
[6,     1] loss: 0.454
[7,     1] loss: 0.396
[8,     1] loss: 0.368
[9,     1] loss: 0.358
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008104286541527728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 936506293,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.194340135311253}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.690
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004796090674901296,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2055106192,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.062979455059523}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.693
[3,     1] loss: 0.665
[4,     1] loss: 0.638
[5,     1] loss: 0.608
[6,     1] loss: 0.577
[7,     1] loss: 0.542
[8,     1] loss: 0.506
[9,     1] loss: 0.469
[10,     1] loss: 0.432
[11,     1] loss: 0.397
[12,     1] loss: 0.365
[13,     1] loss: 0.327
[14,     1] loss: 0.292
[15,     1] loss: 0.268
[16,     1] loss: 0.241
[17,     1] loss: 0.223
[18,     1] loss: 0.207
[19,     1] loss: 0.185
[20,     1] loss: 0.171
[21,     1] loss: 0.161
[22,     1] loss: 0.151
[23,     1] loss: 0.142
[24,     1] loss: 0.133
[25,     1] loss: 0.126
[26,     1] loss: 0.121
[27,     1] loss: 0.117
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021911620380825948,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3570766094,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.951919024747564}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.669
[3,     1] loss: 0.638
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005348190329902515,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1014392309,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.4627407524624}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.673
[3,     1] loss: 0.634
[4,     1] loss: 0.585
[5,     1] loss: 0.524
[6,     1] loss: 0.456
[7,     1] loss: 0.388
[8,     1] loss: 0.319
[9,     1] loss: 0.255
[10,     1] loss: 0.203
[11,     1] loss: 0.149
[12,     1] loss: 0.094
[13,     1] loss: 0.055
[14,     1] loss: 0.033
[15,     1] loss: 0.021
[16,     1] loss: 0.013
[17,     1] loss: 0.008
[18,     1] loss: 0.006
[19,     1] loss: 0.004
[20,     1] loss: 0.003
[21,     1] loss: 0.002
[22,     1] loss: 0.002
[23,     1] loss: 0.001
[24,     1] loss: 0.001
[25,     1] loss: 0.001
[26,     1] loss: 0.001
[27,     1] loss: 0.001
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008357361723832757,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3658891379,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.698786171732829}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.687
[3,     1] loss: 0.640
[4,     1] loss: 0.583
[5,     1] loss: 0.511
[6,     1] loss: 0.437
[7,     1] loss: 0.373
[8,     1] loss: 0.319
[9,     1] loss: 0.269
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008109445763402258,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1362932148,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.132874940561972}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.689
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008758575974862216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1336129072,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.617981084741292}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.687
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008657401171112156,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 453669436,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.329904776496186}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.636
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008697068982247617,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1671330406,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.877274730540961}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.649
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017143728574963608,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4064035497,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.909939211120584}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.675
[3,     1] loss: 0.654
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005883536007796157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 558200808,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.243498869457808}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.656
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009982672790973604,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2772296674,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.4594363198397415}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009514277172033703,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4283628751,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.399062314323015}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.662
[4,     1] loss: 0.629
[5,     1] loss: 0.582
[6,     1] loss: 0.526
[7,     1] loss: 0.468
[8,     1] loss: 0.415
[9,     1] loss: 0.376
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009951455401015726,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3570944630,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.86908519566502}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.672
[3,     1] loss: 0.600
[4,     1] loss: 0.533
[5,     1] loss: 0.473
[6,     1] loss: 0.426
[7,     1] loss: 0.392
[8,     1] loss: 0.362
[9,     1] loss: 0.331
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005321180072970113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 729977803,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.949456544841977}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.667
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006791401737300326,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1590966926,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.738060861231114}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.684
[3,     1] loss: 0.647
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004795578450640916,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789471019,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.61873289816907}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.649
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005999191054364102,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2928703202,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.396465382587078}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.675
[3,     1] loss: 0.624
[4,     1] loss: 0.570
[5,     1] loss: 0.504
[6,     1] loss: 0.430
[7,     1] loss: 0.360
[8,     1] loss: 0.290
[9,     1] loss: 0.235
[10,     1] loss: 0.176
[11,     1] loss: 0.130
[12,     1] loss: 0.095
[13,     1] loss: 0.073
[14,     1] loss: 0.060
[15,     1] loss: 0.049
[16,     1] loss: 0.040
[17,     1] loss: 0.033
[18,     1] loss: 0.032
[19,     1] loss: 0.026
[20,     1] loss: 0.022
[21,     1] loss: 0.018
[22,     1] loss: 0.016
[23,     1] loss: 0.014
[24,     1] loss: 0.013
[25,     1] loss: 0.014
[26,     1] loss: 0.015
[27,     1] loss: 0.015
[28,     1] loss: 0.015
[29,     1] loss: 0.015
[30,     1] loss: 0.016
[31,     1] loss: 0.015
[32,     1] loss: 0.015
[33,     1] loss: 0.015
[34,     1] loss: 0.014
[35,     1] loss: 0.014
[36,     1] loss: 0.013
[37,     1] loss: 0.012
[38,     1] loss: 0.012
[39,     1] loss: 0.011
[40,     1] loss: 0.011
[41,     1] loss: 0.010
[42,     1] loss: 0.010
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.009
[46,     1] loss: 0.009
[47,     1] loss: 0.009
[48,     1] loss: 0.009
[49,     1] loss: 0.009
[50,     1] loss: 0.009
[51,     1] loss: 0.009
[52,     1] loss: 0.009
[53,     1] loss: 0.009
[54,     1] loss: 0.009
[55,     1] loss: 0.009
[56,     1] loss: 0.009
[57,     1] loss: 0.009
[58,     1] loss: 0.009
[59,     1] loss: 0.008
Early stopping applied (best metric=0.2665961682796478)
Finished Training
Total time taken: 271.7083020210266
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.679
[3,     1] loss: 0.639
[4,     1] loss: 0.594
[5,     1] loss: 0.538
[6,     1] loss: 0.478
[7,     1] loss: 0.415
[8,     1] loss: 0.359
[9,     1] loss: 0.306
[10,     1] loss: 0.265
[11,     1] loss: 0.227
[12,     1] loss: 0.187
[13,     1] loss: 0.161
[14,     1] loss: 0.135
[15,     1] loss: 0.112
[16,     1] loss: 0.087
[17,     1] loss: 0.075
[18,     1] loss: 0.064
[19,     1] loss: 0.050
[20,     1] loss: 0.041
[21,     1] loss: 0.034
[22,     1] loss: 0.028
[23,     1] loss: 0.021
[24,     1] loss: 0.019
[25,     1] loss: 0.019
[26,     1] loss: 0.018
[27,     1] loss: 0.017
[28,     1] loss: 0.017
[29,     1] loss: 0.016
[30,     1] loss: 0.016
[31,     1] loss: 0.016
[32,     1] loss: 0.017
[33,     1] loss: 0.017
[34,     1] loss: 0.017
[35,     1] loss: 0.017
[36,     1] loss: 0.017
[37,     1] loss: 0.017
[38,     1] loss: 0.017
[39,     1] loss: 0.017
[40,     1] loss: 0.017
[41,     1] loss: 0.016
[42,     1] loss: 0.016
[43,     1] loss: 0.016
[44,     1] loss: 0.016
[45,     1] loss: 0.015
[46,     1] loss: 0.014
[47,     1] loss: 0.014
[48,     1] loss: 0.014
[49,     1] loss: 0.013
[50,     1] loss: 0.013
[51,     1] loss: 0.013
[52,     1] loss: 0.013
[53,     1] loss: 0.014
[54,     1] loss: 0.013
[55,     1] loss: 0.013
[56,     1] loss: 0.013
[57,     1] loss: 0.013
[58,     1] loss: 0.012
[59,     1] loss: 0.012
[60,     1] loss: 0.013
[61,     1] loss: 0.013
[62,     1] loss: 0.012
[63,     1] loss: 0.012
[64,     1] loss: 0.012
[65,     1] loss: 0.013
[66,     1] loss: 0.012
Early stopping applied (best metric=0.30381855368614197)
Finished Training
Total time taken: 318.0286786556244
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.642
[4,     1] loss: 0.597
[5,     1] loss: 0.548
[6,     1] loss: 0.490
[7,     1] loss: 0.428
[8,     1] loss: 0.367
[9,     1] loss: 0.311
[10,     1] loss: 0.252
[11,     1] loss: 0.201
[12,     1] loss: 0.153
[13,     1] loss: 0.115
[14,     1] loss: 0.090
[15,     1] loss: 0.072
[16,     1] loss: 0.060
[17,     1] loss: 0.050
[18,     1] loss: 0.041
[19,     1] loss: 0.034
[20,     1] loss: 0.027
[21,     1] loss: 0.018
[22,     1] loss: 0.015
[23,     1] loss: 0.013
[24,     1] loss: 0.013
[25,     1] loss: 0.013
[26,     1] loss: 0.017
[27,     1] loss: 0.030
[28,     1] loss: 0.023
[29,     1] loss: 0.017
[30,     1] loss: 0.021
[31,     1] loss: 0.021
[32,     1] loss: 0.033
[33,     1] loss: 0.062
[34,     1] loss: 0.029
[35,     1] loss: 0.073
[36,     1] loss: 0.033
[37,     1] loss: 0.038
[38,     1] loss: 0.032
[39,     1] loss: 0.027
[40,     1] loss: 0.023
[41,     1] loss: 0.021
[42,     1] loss: 0.017
[43,     1] loss: 0.015
[44,     1] loss: 0.014
[45,     1] loss: 0.013
[46,     1] loss: 0.012
[47,     1] loss: 0.011
[48,     1] loss: 0.010
[49,     1] loss: 0.010
[50,     1] loss: 0.011
[51,     1] loss: 0.011
[52,     1] loss: 0.012
[53,     1] loss: 0.012
[54,     1] loss: 0.012
[55,     1] loss: 0.014
[56,     1] loss: 0.014
[57,     1] loss: 0.014
[58,     1] loss: 0.014
[59,     1] loss: 0.014
[60,     1] loss: 0.014
[61,     1] loss: 0.013
[62,     1] loss: 0.014
[63,     1] loss: 0.013
Early stopping applied (best metric=0.26285940408706665)
Finished Training
Total time taken: 296.9396948814392
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.685
[3,     1] loss: 0.643
[4,     1] loss: 0.595
[5,     1] loss: 0.543
[6,     1] loss: 0.487
[7,     1] loss: 0.430
[8,     1] loss: 0.382
[9,     1] loss: 0.329
[10,     1] loss: 0.281
[11,     1] loss: 0.239
[12,     1] loss: 0.206
[13,     1] loss: 0.179
[14,     1] loss: 0.156
[15,     1] loss: 0.134
[16,     1] loss: 0.114
[17,     1] loss: 0.096
[18,     1] loss: 0.083
[19,     1] loss: 0.072
[20,     1] loss: 0.076
[21,     1] loss: 0.053
[22,     1] loss: 0.060
[23,     1] loss: 0.052
[24,     1] loss: 0.060
[25,     1] loss: 0.057
[26,     1] loss: 0.040
[27,     1] loss: 0.037
[28,     1] loss: 0.040
[29,     1] loss: 0.036
[30,     1] loss: 0.034
[31,     1] loss: 0.030
[32,     1] loss: 0.024
[33,     1] loss: 0.022
[34,     1] loss: 0.021
[35,     1] loss: 0.019
[36,     1] loss: 0.016
[37,     1] loss: 0.015
[38,     1] loss: 0.013
[39,     1] loss: 0.013
[40,     1] loss: 0.011
[41,     1] loss: 0.011
[42,     1] loss: 0.011
[43,     1] loss: 0.010
[44,     1] loss: 0.010
[45,     1] loss: 0.010
[46,     1] loss: 0.010
[47,     1] loss: 0.010
[48,     1] loss: 0.011
[49,     1] loss: 0.011
[50,     1] loss: 0.011
[51,     1] loss: 0.010
[52,     1] loss: 0.010
[53,     1] loss: 0.010
[54,     1] loss: 0.011
[55,     1] loss: 0.010
[56,     1] loss: 0.010
[57,     1] loss: 0.010
[58,     1] loss: 0.010
[59,     1] loss: 0.010
[60,     1] loss: 0.009
[61,     1] loss: 0.009
Early stopping applied (best metric=0.2846600413322449)
Finished Training
Total time taken: 285.96527457237244
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.679
[3,     1] loss: 0.631
[4,     1] loss: 0.582
[5,     1] loss: 0.529
[6,     1] loss: 0.478
[7,     1] loss: 0.432
[8,     1] loss: 0.397
[9,     1] loss: 0.361
[10,     1] loss: 0.335
[11,     1] loss: 0.317
[12,     1] loss: 0.307
[13,     1] loss: 0.301
[14,     1] loss: 0.294
[15,     1] loss: 0.287
[16,     1] loss: 0.278
[17,     1] loss: 0.267
[18,     1] loss: 0.254
[19,     1] loss: 0.239
[20,     1] loss: 0.222
[21,     1] loss: 0.201
[22,     1] loss: 0.176
[23,     1] loss: 0.160
[24,     1] loss: 0.133
[25,     1] loss: 0.127
[26,     1] loss: 0.104
[27,     1] loss: 0.094
[28,     1] loss: 0.084
[29,     1] loss: 0.070
[30,     1] loss: 0.066
[31,     1] loss: 0.081
[32,     1] loss: 0.051
[33,     1] loss: 0.088
[34,     1] loss: 0.059
[35,     1] loss: 0.079
[36,     1] loss: 0.049
[37,     1] loss: 0.063
[38,     1] loss: 0.037
[39,     1] loss: 0.036
[40,     1] loss: 0.038
[41,     1] loss: 0.030
[42,     1] loss: 0.026
[43,     1] loss: 0.025
[44,     1] loss: 0.021
[45,     1] loss: 0.018
[46,     1] loss: 0.017
[47,     1] loss: 0.016
[48,     1] loss: 0.015
[49,     1] loss: 0.014
[50,     1] loss: 0.015
[51,     1] loss: 0.015
[52,     1] loss: 0.014
[53,     1] loss: 0.015
[54,     1] loss: 0.016
[55,     1] loss: 0.017
[56,     1] loss: 0.016
[57,     1] loss: 0.017
[58,     1] loss: 0.016
[59,     1] loss: 0.017
[60,     1] loss: 0.017
[61,     1] loss: 0.018
[62,     1] loss: 0.017
[63,     1] loss: 0.017
[64,     1] loss: 0.017
[65,     1] loss: 0.016
[66,     1] loss: 0.016
[67,     1] loss: 0.016
[68,     1] loss: 0.015
[69,     1] loss: 0.015
[70,     1] loss: 0.015
[71,     1] loss: 0.014
[72,     1] loss: 0.015
[73,     1] loss: 0.015
[74,     1] loss: 0.015
[75,     1] loss: 0.014
[76,     1] loss: 0.015
[77,     1] loss: 0.014
Early stopping applied (best metric=0.27403777837753296)
Finished Training
Total time taken: 356.5561673641205
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.925, 'Pyrrolidone carboxylic acid Validation Specificity': 0.7770992366412214, 'Pyrrolidone carboxylic acid Validation Precision': 0.4341033057401961, 'Pyrrolidone carboxylic acid AUC ROC': 0.9166348600508906, 'Pyrrolidone carboxylic acid AUC PR': 0.6125875292827156, 'Pyrrolidone carboxylic acid MCC': 0.5408696099362367, 'Pyrrolidone carboxylic acid F1': 0.5900434424907912, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.27839438915252684, 'Validation Loss (total)': 0.27839438915252684}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007077182301076448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4034863081,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.800886092646935}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.684
[3,     1] loss: 0.621
[4,     1] loss: 0.559
[5,     1] loss: 0.486
[6,     1] loss: 0.411
[7,     1] loss: 0.347
[8,     1] loss: 0.283
[9,     1] loss: 0.221
[10,     1] loss: 0.158
[11,     1] loss: 0.111
[12,     1] loss: 0.078
[13,     1] loss: 0.060
[14,     1] loss: 0.052
[15,     1] loss: 0.045
[16,     1] loss: 0.040
[17,     1] loss: 0.031
[18,     1] loss: 0.026
[19,     1] loss: 0.028
[20,     1] loss: 0.017
[21,     1] loss: 0.016
[22,     1] loss: 0.012
[23,     1] loss: 0.038
[24,     1] loss: 0.023
[25,     1] loss: 0.028
[26,     1] loss: 0.018
[27,     1] loss: 0.029
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005610517317704793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3684546932,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.623511658497222}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.669
[3,     1] loss: 0.623
[4,     1] loss: 0.573
[5,     1] loss: 0.513
[6,     1] loss: 0.452
[7,     1] loss: 0.387
[8,     1] loss: 0.324
[9,     1] loss: 0.265
[10,     1] loss: 0.207
[11,     1] loss: 0.154
[12,     1] loss: 0.104
[13,     1] loss: 0.073
[14,     1] loss: 0.045
[15,     1] loss: 0.030
[16,     1] loss: 0.025
[17,     1] loss: 0.021
[18,     1] loss: 0.019
[19,     1] loss: 0.016
[20,     1] loss: 0.013
[21,     1] loss: 0.011
[22,     1] loss: 0.011
[23,     1] loss: 0.010
[24,     1] loss: 0.009
[25,     1] loss: 0.009
[26,     1] loss: 0.009
[27,     1] loss: 0.009
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008392968623121685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1483379196,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.913071252138776}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.676
[3,     1] loss: 0.608
[4,     1] loss: 0.542
[5,     1] loss: 0.477
[6,     1] loss: 0.430
[7,     1] loss: 0.395
[8,     1] loss: 0.362
[9,     1] loss: 0.332
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003698890665429961,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1583889431,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.730195494122895}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.677
[3,     1] loss: 0.640
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005412678739858222,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1608677369,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.109914857884102}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.668
[3,     1] loss: 0.629
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005793063161376925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1408842701,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.935048752376405}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.663
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019631617372816573,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 613592532,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5102716026301088}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.681
[3,     1] loss: 0.650
[4,     1] loss: 0.617
[5,     1] loss: 0.581
[6,     1] loss: 0.545
[7,     1] loss: 0.505
[8,     1] loss: 0.463
[9,     1] loss: 0.419
[10,     1] loss: 0.373
[11,     1] loss: 0.323
[12,     1] loss: 0.279
[13,     1] loss: 0.233
[14,     1] loss: 0.190
[15,     1] loss: 0.153
[16,     1] loss: 0.118
[17,     1] loss: 0.093
[18,     1] loss: 0.069
[19,     1] loss: 0.054
[20,     1] loss: 0.039
[21,     1] loss: 0.030
[22,     1] loss: 0.024
[23,     1] loss: 0.017
[24,     1] loss: 0.013
[25,     1] loss: 0.009
[26,     1] loss: 0.006
[27,     1] loss: 0.004
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005575697044088184,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 264590502,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.73923616645102}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.662
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006444276686873739,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3523141976,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.176930914013072}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.683
[3,     1] loss: 0.649
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0069287121584535516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1863868865,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.962357734729355}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.704
[3,     1] loss: 0.688
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003429493417828055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1725146651,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.352478772070997}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.674
[3,     1] loss: 0.627
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009644189504481092,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2738629285,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.599990148859455}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.639
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009869517092541476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2401175063,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.700950678573971}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.676
[3,     1] loss: 0.606
[4,     1] loss: 0.535
[5,     1] loss: 0.469
[6,     1] loss: 0.406
[7,     1] loss: 0.352
[8,     1] loss: 0.299
[9,     1] loss: 0.253
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009014229027824906,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2796059362,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.156598011363478}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.665
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004905972374783727,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2549593049,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.063160287433178}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.692
[3,     1] loss: 0.676
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00946037581071878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 435446382,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.2822811055891}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.680
[3,     1] loss: 0.633
[4,     1] loss: 0.571
[5,     1] loss: 0.514
[6,     1] loss: 0.467
[7,     1] loss: 0.423
[8,     1] loss: 0.380
[9,     1] loss: 0.339
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020272174656151623,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3961936483,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.257383092020905}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.678
[4,     1] loss: 0.667
[5,     1] loss: 0.657
[6,     1] loss: 0.647
[7,     1] loss: 0.638
[8,     1] loss: 0.629
[9,     1] loss: 0.619
[10,     1] loss: 0.608
[11,     1] loss: 0.598
[12,     1] loss: 0.587
[13,     1] loss: 0.575
[14,     1] loss: 0.562
[15,     1] loss: 0.548
[16,     1] loss: 0.533
[17,     1] loss: 0.516
[18,     1] loss: 0.500
[19,     1] loss: 0.482
[20,     1] loss: 0.462
[21,     1] loss: 0.444
[22,     1] loss: 0.429
[23,     1] loss: 0.409
[24,     1] loss: 0.394
[25,     1] loss: 0.374
[26,     1] loss: 0.359
[27,     1] loss: 0.343
[28,     1] loss: 0.327
[29,     1] loss: 0.314
[30,     1] loss: 0.295
[31,     1] loss: 0.282
[32,     1] loss: 0.268
[33,     1] loss: 0.255
[34,     1] loss: 0.243
[35,     1] loss: 0.233
[36,     1] loss: 0.222
[37,     1] loss: 0.210
[38,     1] loss: 0.200
[39,     1] loss: 0.191
[40,     1] loss: 0.186
[41,     1] loss: 0.177
[42,     1] loss: 0.167
[43,     1] loss: 0.166
[44,     1] loss: 0.158
[45,     1] loss: 0.152
[46,     1] loss: 0.150
[47,     1] loss: 0.144
[48,     1] loss: 0.141
[49,     1] loss: 0.136
[50,     1] loss: 0.133
[51,     1] loss: 0.132
[52,     1] loss: 0.129
[53,     1] loss: 0.125
[54,     1] loss: 0.123
[55,     1] loss: 0.123
[56,     1] loss: 0.121
[57,     1] loss: 0.120
[58,     1] loss: 0.117
[59,     1] loss: 0.119
[60,     1] loss: 0.118
[61,     1] loss: 0.117
[62,     1] loss: 0.119
[63,     1] loss: 0.117
[64,     1] loss: 0.114
[65,     1] loss: 0.117
[66,     1] loss: 0.116
[67,     1] loss: 0.115
[68,     1] loss: 0.117
[69,     1] loss: 0.116
[70,     1] loss: 0.115
[71,     1] loss: 0.116
[72,     1] loss: 0.116
[73,     1] loss: 0.119
[74,     1] loss: 0.120
[75,     1] loss: 0.178
[76,     1] loss: 0.693
[77,     1] loss: 0.350
[78,     1] loss: 0.334
[79,     1] loss: 0.366
[80,     1] loss: 0.352
[81,     1] loss: 0.333
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008151763810362471,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 653114060,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.921284596952384}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.664
[3,     1] loss: 0.588
[4,     1] loss: 0.523
[5,     1] loss: 0.471
[6,     1] loss: 0.435
[7,     1] loss: 0.398
[8,     1] loss: 0.360
[9,     1] loss: 0.335
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007035434269892448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4219987339,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.839360849314614}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.630
[4,     1] loss: 0.572
[5,     1] loss: 0.497
[6,     1] loss: 0.420
[7,     1] loss: 0.337
[8,     1] loss: 0.260
[9,     1] loss: 0.183
[10,     1] loss: 0.118
[11,     1] loss: 0.090
[12,     1] loss: 0.067
[13,     1] loss: 0.046
[14,     1] loss: 0.057
[15,     1] loss: 0.050
[16,     1] loss: 0.026
[17,     1] loss: 0.024
[18,     1] loss: 0.026
[19,     1] loss: 0.021
[20,     1] loss: 0.020
[21,     1] loss: 0.019
[22,     1] loss: 0.017
[23,     1] loss: 0.015
[24,     1] loss: 0.014
[25,     1] loss: 0.014
[26,     1] loss: 0.013
[27,     1] loss: 0.013
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00680913387255161,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1819947199,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.851423864372544}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.687
[3,     1] loss: 0.638
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00990880768014306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4131385383,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.1726461264393695}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.693
[3,     1] loss: 0.639
[4,     1] loss: 0.575
[5,     1] loss: 0.499
[6,     1] loss: 0.427
[7,     1] loss: 0.353
[8,     1] loss: 0.291
[9,     1] loss: 0.225
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009646397746114739,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2212022702,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.938103404867821}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.668
[4,     1] loss: 0.647
[5,     1] loss: 0.622
[6,     1] loss: 0.596
[7,     1] loss: 0.568
[8,     1] loss: 0.539
[9,     1] loss: 0.508
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075993025449970384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 589399304,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.159142406660262}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.674
[3,     1] loss: 0.621
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008823753556437882,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2591227053,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.574468367973278}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.672
[3,     1] loss: 0.654
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0057806494041493725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3850270082,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.43799733939283}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.680
[3,     1] loss: 0.634
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074767029199940965,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3523118921,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.126823685169528}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.689
[3,     1] loss: 0.648
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067688033560523685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3603218035,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.6626812519110397}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.672
[3,     1] loss: 0.597
[4,     1] loss: 0.507
[5,     1] loss: 0.421
[6,     1] loss: 0.345
[7,     1] loss: 0.271
[8,     1] loss: 0.202
[9,     1] loss: 0.140
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008177698710061057,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3647930028,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.626189243975718}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.663
[4,     1] loss: 0.638
[5,     1] loss: 0.608
[6,     1] loss: 0.578
[7,     1] loss: 0.550
[8,     1] loss: 0.517
[9,     1] loss: 0.485
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00822932004719296,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1548750529,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.884476595147726}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.675
[3,     1] loss: 0.616
[4,     1] loss: 0.552
[5,     1] loss: 0.485
[6,     1] loss: 0.412
[7,     1] loss: 0.337
[8,     1] loss: 0.269
[9,     1] loss: 0.202
[10,     1] loss: 0.144
[11,     1] loss: 0.093
[12,     1] loss: 0.059
[13,     1] loss: 0.043
[14,     1] loss: 0.029
[15,     1] loss: 0.021
[16,     1] loss: 0.022
[17,     1] loss: 0.021
[18,     1] loss: 0.013
[19,     1] loss: 0.012
[20,     1] loss: 0.012
[21,     1] loss: 0.013
[22,     1] loss: 0.014
[23,     1] loss: 0.016
[24,     1] loss: 0.016
[25,     1] loss: 0.017
[26,     1] loss: 0.017
[27,     1] loss: 0.016
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007196491639360479,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2043584759,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.252054393202814}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.654
[4,     1] loss: 0.623
[5,     1] loss: 0.589
[6,     1] loss: 0.550
[7,     1] loss: 0.507
[8,     1] loss: 0.462
[9,     1] loss: 0.414
[10,     1] loss: 0.369
[11,     1] loss: 0.325
[12,     1] loss: 0.286
[13,     1] loss: 0.251
[14,     1] loss: 0.223
[15,     1] loss: 0.195
[16,     1] loss: 0.173
[17,     1] loss: 0.156
[18,     1] loss: 0.141
[19,     1] loss: 0.128
[20,     1] loss: 0.119
[21,     1] loss: 0.111
[22,     1] loss: 0.102
[23,     1] loss: 0.098
[24,     1] loss: 0.093
[25,     1] loss: 0.089
[26,     1] loss: 0.085
[27,     1] loss: 0.083
[28,     1] loss: 0.081
[29,     1] loss: 0.081
[30,     1] loss: 0.078
[31,     1] loss: 0.077
[32,     1] loss: 0.078
[33,     1] loss: 0.075
[34,     1] loss: 0.075
[35,     1] loss: 0.075
[36,     1] loss: 0.074
[37,     1] loss: 0.073
[38,     1] loss: 0.074
[39,     1] loss: 0.073
[40,     1] loss: 0.083
[41,     1] loss: 0.087
[42,     1] loss: 0.381
[43,     1] loss: 0.517
[44,     1] loss: 0.467
[45,     1] loss: 0.418
[46,     1] loss: 0.455
[47,     1] loss: 0.481
[48,     1] loss: 0.485
[49,     1] loss: 0.483
[50,     1] loss: 0.476
[51,     1] loss: 0.464
[52,     1] loss: 0.449
[53,     1] loss: 0.430
[54,     1] loss: 0.410
[55,     1] loss: 0.385
[56,     1] loss: 0.362
[57,     1] loss: 0.339
[58,     1] loss: 0.316
[59,     1] loss: 0.298
[60,     1] loss: 0.277
[61,     1] loss: 0.254
[62,     1] loss: 0.237
[63,     1] loss: 0.218
[64,     1] loss: 0.202
[65,     1] loss: 0.185
[66,     1] loss: 0.170
[67,     1] loss: 0.159
[68,     1] loss: 0.146
[69,     1] loss: 0.133
[70,     1] loss: 0.125
[71,     1] loss: 0.119
[72,     1] loss: 0.114
[73,     1] loss: 0.113
[74,     1] loss: 0.106
[75,     1] loss: 0.105
[76,     1] loss: 0.102
[77,     1] loss: 0.100
[78,     1] loss: 0.102
[79,     1] loss: 0.101
[80,     1] loss: 0.101
[81,     1] loss: 0.101
[82,     1] loss: 0.104
[83,     1] loss: 0.104
[84,     1] loss: 0.105
[85,     1] loss: 0.107
[86,     1] loss: 0.145
[87,     1] loss: 0.284
[88,     1] loss: 0.608
[89,     1] loss: 0.442
[90,     1] loss: 0.457
[91,     1] loss: 0.400
[92,     1] loss: 0.443
[93,     1] loss: 0.388
[94,     1] loss: 0.397
[95,     1] loss: 0.388
[96,     1] loss: 0.369
[97,     1] loss: 0.352
[98,     1] loss: 0.324
[99,     1] loss: 0.297
[100,     1] loss: 0.267
[101,     1] loss: 0.242
[102,     1] loss: 0.216
[103,     1] loss: 0.193
[104,     1] loss: 0.174
[105,     1] loss: 0.156
[106,     1] loss: 0.142
[107,     1] loss: 0.129
[108,     1] loss: 0.121
[109,     1] loss: 0.115
[110,     1] loss: 0.111
[111,     1] loss: 0.107
[112,     1] loss: 0.107
[113,     1] loss: 0.108
[114,     1] loss: 0.110
[115,     1] loss: 0.113
[116,     1] loss: 0.112
[117,     1] loss: 0.114
[118,     1] loss: 0.117
[119,     1] loss: 0.116
[120,     1] loss: 0.117
[121,     1] loss: 0.117
[122,     1] loss: 0.119
[123,     1] loss: 0.127
[124,     1] loss: 0.239
Early stopping applied (best metric=0.22710487246513367)
Finished Training
Total time taken: 591.812935590744
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.669
[3,     1] loss: 0.624
[4,     1] loss: 0.582
[5,     1] loss: 0.538
[6,     1] loss: 0.491
[7,     1] loss: 0.452
[8,     1] loss: 0.413
[9,     1] loss: 0.384
[10,     1] loss: 0.364
[11,     1] loss: 0.352
[12,     1] loss: 0.342
[13,     1] loss: 0.332
[14,     1] loss: 0.323
[15,     1] loss: 0.313
[16,     1] loss: 0.303
[17,     1] loss: 0.291
[18,     1] loss: 0.277
[19,     1] loss: 0.263
[20,     1] loss: 0.245
[21,     1] loss: 0.225
[22,     1] loss: 0.204
[23,     1] loss: 0.183
[24,     1] loss: 0.163
[25,     1] loss: 0.144
[26,     1] loss: 0.153
[27,     1] loss: 0.243
[28,     1] loss: 0.244
[29,     1] loss: 0.328
[30,     1] loss: 0.302
[31,     1] loss: 0.323
[32,     1] loss: 0.292
[33,     1] loss: 0.298
[34,     1] loss: 0.293
[35,     1] loss: 0.264
[36,     1] loss: 0.234
[37,     1] loss: 0.198
[38,     1] loss: 0.163
[39,     1] loss: 0.137
[40,     1] loss: 0.119
[41,     1] loss: 0.105
[42,     1] loss: 0.095
[43,     1] loss: 0.087
[44,     1] loss: 0.082
[45,     1] loss: 0.080
[46,     1] loss: 0.079
[47,     1] loss: 0.078
[48,     1] loss: 0.081
[49,     1] loss: 0.083
[50,     1] loss: 0.085
[51,     1] loss: 0.087
[52,     1] loss: 0.089
[53,     1] loss: 0.091
[54,     1] loss: 0.092
[55,     1] loss: 0.092
[56,     1] loss: 0.092
[57,     1] loss: 0.092
[58,     1] loss: 0.092
[59,     1] loss: 0.091
[60,     1] loss: 0.088
[61,     1] loss: 0.088
[62,     1] loss: 0.086
[63,     1] loss: 0.088
[64,     1] loss: 0.107
[65,     1] loss: 0.170
[66,     1] loss: 0.818
[67,     1] loss: 0.648
[68,     1] loss: 0.509
[69,     1] loss: 0.484
[70,     1] loss: 0.500
[71,     1] loss: 0.496
[72,     1] loss: 0.493
[73,     1] loss: 0.495
[74,     1] loss: 0.497
[75,     1] loss: 0.497
[76,     1] loss: 0.491
[77,     1] loss: 0.487
[78,     1] loss: 0.480
[79,     1] loss: 0.475
[80,     1] loss: 0.466
[81,     1] loss: 0.457
[82,     1] loss: 0.448
[83,     1] loss: 0.437
[84,     1] loss: 0.429
[85,     1] loss: 0.417
[86,     1] loss: 0.406
[87,     1] loss: 0.397
[88,     1] loss: 0.387
[89,     1] loss: 0.380
[90,     1] loss: 0.376
[91,     1] loss: 0.371
[92,     1] loss: 0.368
[93,     1] loss: 0.363
[94,     1] loss: 0.365
[95,     1] loss: 0.372
[96,     1] loss: 0.366
[97,     1] loss: 0.375
[98,     1] loss: 0.389
[99,     1] loss: 0.372
[100,     1] loss: 0.371
[101,     1] loss: 0.368
[102,     1] loss: 0.367
[103,     1] loss: 0.366
[104,     1] loss: 0.363
[105,     1] loss: 0.363
[106,     1] loss: 0.361
[107,     1] loss: 0.362
[108,     1] loss: 0.362
[109,     1] loss: 0.364
[110,     1] loss: 0.363
[111,     1] loss: 0.364
[112,     1] loss: 0.365
Early stopping applied (best metric=0.22781603038311005)
Finished Training
Total time taken: 528.3711965084076
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.683
[3,     1] loss: 0.652
[4,     1] loss: 0.618
[5,     1] loss: 0.578
[6,     1] loss: 0.538
[7,     1] loss: 0.499
[8,     1] loss: 0.459
[9,     1] loss: 0.424
[10,     1] loss: 0.392
[11,     1] loss: 0.360
[12,     1] loss: 0.325
[13,     1] loss: 0.297
[14,     1] loss: 0.277
[15,     1] loss: 0.257
[16,     1] loss: 0.239
[17,     1] loss: 0.226
[18,     1] loss: 0.209
[19,     1] loss: 0.197
[20,     1] loss: 0.183
[21,     1] loss: 0.170
[22,     1] loss: 0.158
[23,     1] loss: 0.146
[24,     1] loss: 0.134
[25,     1] loss: 0.124
[26,     1] loss: 0.115
[27,     1] loss: 0.107
[28,     1] loss: 0.100
[29,     1] loss: 0.094
[30,     1] loss: 0.089
[31,     1] loss: 0.084
[32,     1] loss: 0.082
[33,     1] loss: 0.079
[34,     1] loss: 0.077
[35,     1] loss: 0.074
[36,     1] loss: 0.073
[37,     1] loss: 0.073
[38,     1] loss: 0.072
[39,     1] loss: 0.072
[40,     1] loss: 0.072
[41,     1] loss: 0.071
[42,     1] loss: 0.071
[43,     1] loss: 0.071
[44,     1] loss: 0.070
[45,     1] loss: 0.072
[46,     1] loss: 0.077
[47,     1] loss: 0.176
[48,     1] loss: 0.734
[49,     1] loss: 0.531
[50,     1] loss: 0.654
[51,     1] loss: 0.525
[52,     1] loss: 0.554
[53,     1] loss: 0.568
[54,     1] loss: 0.573
[55,     1] loss: 0.576
[56,     1] loss: 0.578
[57,     1] loss: 0.577
[58,     1] loss: 0.574
[59,     1] loss: 0.569
[60,     1] loss: 0.561
[61,     1] loss: 0.552
[62,     1] loss: 0.540
[63,     1] loss: 0.525
[64,     1] loss: 0.509
[65,     1] loss: 0.489
[66,     1] loss: 0.470
[67,     1] loss: 0.452
[68,     1] loss: 0.432
[69,     1] loss: 0.417
[70,     1] loss: 0.403
[71,     1] loss: 0.395
[72,     1] loss: 0.387
[73,     1] loss: 0.378
[74,     1] loss: 0.371
[75,     1] loss: 0.362
[76,     1] loss: 0.359
[77,     1] loss: 0.356
[78,     1] loss: 0.355
Early stopping applied (best metric=0.22669681906700134)
Finished Training
Total time taken: 380.4167878627777
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.681
[3,     1] loss: 0.638
[4,     1] loss: 0.600
[5,     1] loss: 0.555
[6,     1] loss: 0.514
[7,     1] loss: 0.483
[8,     1] loss: 0.452
[9,     1] loss: 0.428
[10,     1] loss: 0.407
[11,     1] loss: 0.391
[12,     1] loss: 0.380
[13,     1] loss: 0.370
[14,     1] loss: 0.359
[15,     1] loss: 0.359
[16,     1] loss: 0.366
[17,     1] loss: 0.363
[18,     1] loss: 0.356
[19,     1] loss: 0.351
[20,     1] loss: 0.350
[21,     1] loss: 0.348
[22,     1] loss: 0.348
[23,     1] loss: 0.344
[24,     1] loss: 0.343
[25,     1] loss: 0.343
[26,     1] loss: 0.344
[27,     1] loss: 0.344
[28,     1] loss: 0.343
[29,     1] loss: 0.343
[30,     1] loss: 0.343
[31,     1] loss: 0.343
[32,     1] loss: 0.342
[33,     1] loss: 0.341
[34,     1] loss: 0.343
[35,     1] loss: 0.349
[36,     1] loss: 0.356
[37,     1] loss: 0.430
[38,     1] loss: 0.457
[39,     1] loss: 0.411
[40,     1] loss: 0.400
[41,     1] loss: 0.394
[42,     1] loss: 0.398
[43,     1] loss: 0.377
[44,     1] loss: 0.357
[45,     1] loss: 0.357
[46,     1] loss: 0.348
[47,     1] loss: 0.350
[48,     1] loss: 0.347
[49,     1] loss: 0.343
[50,     1] loss: 0.341
[51,     1] loss: 0.341
[52,     1] loss: 0.341
[53,     1] loss: 0.341
[54,     1] loss: 0.343
[55,     1] loss: 0.344
[56,     1] loss: 0.345
[57,     1] loss: 0.347
[58,     1] loss: 0.346
[59,     1] loss: 0.347
[60,     1] loss: 0.347
[61,     1] loss: 0.346
[62,     1] loss: 0.346
[63,     1] loss: 0.345
[64,     1] loss: 0.344
[65,     1] loss: 0.345
[66,     1] loss: 0.344
[67,     1] loss: 0.346
[68,     1] loss: 0.347
[69,     1] loss: 0.409
[70,     1] loss: 0.948
[71,     1] loss: 0.570
[72,     1] loss: 0.628
[73,     1] loss: 0.643
[74,     1] loss: 0.646
[75,     1] loss: 0.646
[76,     1] loss: 0.645
[77,     1] loss: 0.642
[78,     1] loss: 0.639
[79,     1] loss: 0.634
[80,     1] loss: 0.629
[81,     1] loss: 0.621
[82,     1] loss: 0.609
[83,     1] loss: 0.596
[84,     1] loss: 0.578
[85,     1] loss: 0.559
[86,     1] loss: 0.534
[87,     1] loss: 0.507
[88,     1] loss: 0.477
[89,     1] loss: 0.450
[90,     1] loss: 0.425
[91,     1] loss: 0.400
[92,     1] loss: 0.400
[93,     1] loss: 0.388
[94,     1] loss: 0.411
[95,     1] loss: 0.376
[96,     1] loss: 0.374
Early stopping applied (best metric=0.3056156635284424)
Finished Training
Total time taken: 432.951119184494
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.693
[3,     1] loss: 0.663
[4,     1] loss: 0.636
[5,     1] loss: 0.608
[6,     1] loss: 0.579
[7,     1] loss: 0.547
[8,     1] loss: 0.514
[9,     1] loss: 0.483
[10,     1] loss: 0.450
[11,     1] loss: 0.415
[12,     1] loss: 0.381
[13,     1] loss: 0.345
[14,     1] loss: 0.315
[15,     1] loss: 0.282
[16,     1] loss: 0.256
[17,     1] loss: 0.230
[18,     1] loss: 0.203
[19,     1] loss: 0.180
[20,     1] loss: 0.157
[21,     1] loss: 0.136
[22,     1] loss: 0.118
[23,     1] loss: 0.101
[24,     1] loss: 0.089
[25,     1] loss: 0.077
[26,     1] loss: 0.067
[27,     1] loss: 0.061
[28,     1] loss: 0.057
[29,     1] loss: 0.055
[30,     1] loss: 0.053
[31,     1] loss: 0.052
[32,     1] loss: 0.052
[33,     1] loss: 0.052
[34,     1] loss: 0.053
[35,     1] loss: 0.054
[36,     1] loss: 0.054
[37,     1] loss: 0.056
[38,     1] loss: 0.056
[39,     1] loss: 0.057
[40,     1] loss: 0.057
[41,     1] loss: 0.056
[42,     1] loss: 0.057
[43,     1] loss: 0.058
[44,     1] loss: 0.079
[45,     1] loss: 0.109
[46,     1] loss: 0.491
[47,     1] loss: 0.466
[48,     1] loss: 0.452
[49,     1] loss: 0.394
[50,     1] loss: 0.442
[51,     1] loss: 0.407
[52,     1] loss: 0.419
[53,     1] loss: 0.391
[54,     1] loss: 0.369
[55,     1] loss: 0.356
[56,     1] loss: 0.364
[57,     1] loss: 0.305
[58,     1] loss: 0.294
[59,     1] loss: 0.244
[60,     1] loss: 0.206
[61,     1] loss: 0.187
[62,     1] loss: 0.145
[63,     1] loss: 0.181
[64,     1] loss: 0.113
[65,     1] loss: 0.138
[66,     1] loss: 0.098
[67,     1] loss: 0.101
[68,     1] loss: 0.082
[69,     1] loss: 0.085
[70,     1] loss: 0.078
[71,     1] loss: 0.077
[72,     1] loss: 0.077
[73,     1] loss: 0.080
[74,     1] loss: 0.079
[75,     1] loss: 0.081
[76,     1] loss: 0.083
[77,     1] loss: 0.079
[78,     1] loss: 0.081
[79,     1] loss: 0.079
[80,     1] loss: 0.079
[81,     1] loss: 0.078
[82,     1] loss: 0.077
[83,     1] loss: 0.076
[84,     1] loss: 0.072
[85,     1] loss: 0.072
[86,     1] loss: 0.072
[87,     1] loss: 0.071
[88,     1] loss: 0.070
[89,     1] loss: 0.069
[90,     1] loss: 0.069
[91,     1] loss: 0.069
[92,     1] loss: 0.068
[93,     1] loss: 0.069
[94,     1] loss: 0.070
[95,     1] loss: 0.070
[96,     1] loss: 0.069
[97,     1] loss: 0.070
[98,     1] loss: 0.068
[99,     1] loss: 0.072
[100,     1] loss: 0.078
[101,     1] loss: 0.091
[102,     1] loss: 0.180
[103,     1] loss: 1.078
[104,     1] loss: 0.721
[105,     1] loss: 0.584
[106,     1] loss: 0.587
[107,     1] loss: 0.602
[108,     1] loss: 0.610
[109,     1] loss: 0.616
[110,     1] loss: 0.619
[111,     1] loss: 0.619
[112,     1] loss: 0.620
[113,     1] loss: 0.621
[114,     1] loss: 0.621
[115,     1] loss: 0.620
[116,     1] loss: 0.621
[117,     1] loss: 0.622
[118,     1] loss: 0.620
[119,     1] loss: 0.620
[120,     1] loss: 0.619
[121,     1] loss: 0.617
[122,     1] loss: 0.615
[123,     1] loss: 0.612
[124,     1] loss: 0.607
[125,     1] loss: 0.602
[126,     1] loss: 0.598
[127,     1] loss: 0.592
[128,     1] loss: 0.588
[129,     1] loss: 0.580
[130,     1] loss: 0.573
[131,     1] loss: 0.566
[132,     1] loss: 0.556
[133,     1] loss: 0.545
[134,     1] loss: 0.533
[135,     1] loss: 0.521
[136,     1] loss: 0.505
[137,     1] loss: 0.485
[138,     1] loss: 0.465
[139,     1] loss: 0.442
[140,     1] loss: 0.422
[141,     1] loss: 0.402
[142,     1] loss: 0.386
[143,     1] loss: 0.372
[144,     1] loss: 0.374
[145,     1] loss: 0.398
[146,     1] loss: 0.378
[147,     1] loss: 0.378
[148,     1] loss: 0.371
Early stopping applied (best metric=0.212180033326149)
Finished Training
Total time taken: 695.9316682815552
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8935483870967742, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8708333333333333, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8977099236641222, 'Pyrrolidone carboxylic acid Validation Precision': 0.623592903828198, 'Pyrrolidone carboxylic acid AUC ROC': 0.9451017811704835, 'Pyrrolidone carboxylic acid AUC PR': 0.853033160366555, 'Pyrrolidone carboxylic acid MCC': 0.676636159777637, 'Pyrrolidone carboxylic acid F1': 0.7215176919186841, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.2398826837539673, 'Validation Loss (total)': 0.2398826837539673}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008275204132571476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3315142135,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.442490378301976}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.688
[3,     1] loss: 0.653
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004636330977548125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4018263948,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.204905022533291}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.681
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009856429837039517,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2875488223,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.841186278518045}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.689
[3,     1] loss: 0.664
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007194702784980269,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1840329277,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.7917988674882155}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008264484461686976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1717160675,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.304955667519466}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.689
[3,     1] loss: 0.645
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007127055582631162,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3752348904,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.528768533730254}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.646
[4,     1] loss: 0.602
[5,     1] loss: 0.550
[6,     1] loss: 0.491
[7,     1] loss: 0.431
[8,     1] loss: 0.375
[9,     1] loss: 0.321
[10,     1] loss: 0.264
[11,     1] loss: 0.199
[12,     1] loss: 0.153
[13,     1] loss: 0.116
[14,     1] loss: 0.088
[15,     1] loss: 0.082
[16,     1] loss: 0.071
[17,     1] loss: 0.064
[18,     1] loss: 0.049
[19,     1] loss: 0.044
[20,     1] loss: 0.044
[21,     1] loss: 0.049
[22,     1] loss: 0.046
[23,     1] loss: 0.040
[24,     1] loss: 0.058
[25,     1] loss: 0.033
[26,     1] loss: 0.035
[27,     1] loss: 0.033
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008512977786232282,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3180079011,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.1656875642576825}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.669
[3,     1] loss: 0.600
[4,     1] loss: 0.524
[5,     1] loss: 0.456
[6,     1] loss: 0.388
[7,     1] loss: 0.317
[8,     1] loss: 0.254
[9,     1] loss: 0.195
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008155026431978442,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 481652882,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.6155085186556954}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.672
[3,     1] loss: 0.613
[4,     1] loss: 0.553
[5,     1] loss: 0.485
[6,     1] loss: 0.418
[7,     1] loss: 0.356
[8,     1] loss: 0.306
[9,     1] loss: 0.259
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00861450691733892,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 532228968,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.316981437467992}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.670
[3,     1] loss: 0.597
[4,     1] loss: 0.522
[5,     1] loss: 0.446
[6,     1] loss: 0.368
[7,     1] loss: 0.297
[8,     1] loss: 0.232
[9,     1] loss: 0.168
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004533182123430627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3417770,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.135549434000136}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.682
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008738887260743858,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 341427648,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.468960226999162}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.691
[3,     1] loss: 0.644
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006042162655748395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2707101836,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.289532713811871}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.682
[3,     1] loss: 0.639
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008606660446522092,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1604443790,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.903391801498316}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.679
[3,     1] loss: 0.661
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007270741568144431,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 743970693,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.962424607094487}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.647
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006565328582713884,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2078523975,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.440735115704008}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.680
[3,     1] loss: 0.628
[4,     1] loss: 0.578
[5,     1] loss: 0.522
[6,     1] loss: 0.473
[7,     1] loss: 0.429
[8,     1] loss: 0.387
[9,     1] loss: 0.353
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0056236454613540566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 37623592,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.285891858381675}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.649
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00996749233986551,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2287398326,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.13850291316548358}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.685
[3,     1] loss: 0.603
[4,     1] loss: 0.522
[5,     1] loss: 0.453
[6,     1] loss: 0.380
[7,     1] loss: 0.294
[8,     1] loss: 0.219
[9,     1] loss: 0.155
[10,     1] loss: 0.122
[11,     1] loss: 0.100
[12,     1] loss: 0.103
[13,     1] loss: 0.043
[14,     1] loss: 0.027
[15,     1] loss: 0.011
[16,     1] loss: 0.003
[17,     1] loss: 0.005
[18,     1] loss: 0.001
[19,     1] loss: 0.001
[20,     1] loss: 0.001
[21,     1] loss: 0.000
[22,     1] loss: 0.000
[23,     1] loss: 0.000
[24,     1] loss: 0.000
[25,     1] loss: 0.000
[26,     1] loss: 0.000
[27,     1] loss: 0.000
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009817990535240545,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 766053931,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.7102344713465687}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.685
[3,     1] loss: 0.621
[4,     1] loss: 0.532
[5,     1] loss: 0.424
[6,     1] loss: 0.321
[7,     1] loss: 0.222
[8,     1] loss: 0.137
[9,     1] loss: 0.084
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009156396622320036,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 962028277,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.6545877895907226}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.678
[3,     1] loss: 0.599
[4,     1] loss: 0.514
[5,     1] loss: 0.453
[6,     1] loss: 0.392
[7,     1] loss: 0.330
[8,     1] loss: 0.272
[9,     1] loss: 0.213
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008144559203924493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1106202497,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.62665186627029}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.688
[3,     1] loss: 0.645
[4,     1] loss: 0.594
[5,     1] loss: 0.534
[6,     1] loss: 0.481
[7,     1] loss: 0.439
[8,     1] loss: 0.413
[9,     1] loss: 0.385
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009167922314976758,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3113039668,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.0861333275256002}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.676
[3,     1] loss: 0.591
[4,     1] loss: 0.480
[5,     1] loss: 0.390
[6,     1] loss: 0.331
[7,     1] loss: 0.251
[8,     1] loss: 0.186
[9,     1] loss: 0.130
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009877447546345735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2323939366,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9646985200238095}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.679
[3,     1] loss: 0.614
[4,     1] loss: 0.523
[5,     1] loss: 0.412
[6,     1] loss: 0.300
[7,     1] loss: 0.221
[8,     1] loss: 0.157
[9,     1] loss: 0.119
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009011634505935031,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2085711878,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.3318176819381785}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.658
[3,     1] loss: 0.557
[4,     1] loss: 0.452
[5,     1] loss: 0.369
[6,     1] loss: 0.279
[7,     1] loss: 0.190
[8,     1] loss: 0.113
[9,     1] loss: 0.074
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009954540235251115,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1982264278,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.9394901730438905}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.670
[3,     1] loss: 0.579
[4,     1] loss: 0.483
[5,     1] loss: 0.402
[6,     1] loss: 0.331
[7,     1] loss: 0.256
[8,     1] loss: 0.200
[9,     1] loss: 0.153
[10,     1] loss: 0.117
[11,     1] loss: 0.095
[12,     1] loss: 0.077
[13,     1] loss: 0.064
[14,     1] loss: 0.055
[15,     1] loss: 0.041
[16,     1] loss: 0.023
[17,     1] loss: 0.012
[18,     1] loss: 0.010
[19,     1] loss: 0.005
[20,     1] loss: 0.004
[21,     1] loss: 0.003
[22,     1] loss: 0.003
[23,     1] loss: 0.003
[24,     1] loss: 0.003
[25,     1] loss: 0.003
[26,     1] loss: 0.003
[27,     1] loss: 0.004
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009178950013074305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1264841786,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.484625830200907}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.675
[3,     1] loss: 0.614
[4,     1] loss: 0.548
[5,     1] loss: 0.485
[6,     1] loss: 0.417
[7,     1] loss: 0.357
[8,     1] loss: 0.300
[9,     1] loss: 0.248
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008459303533984126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 279088169,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.21057700129559}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.671
[4,     1] loss: 0.650
[5,     1] loss: 0.626
[6,     1] loss: 0.596
[7,     1] loss: 0.565
[8,     1] loss: 0.531
[9,     1] loss: 0.501
[10,     1] loss: 0.472
[11,     1] loss: 0.448
[12,     1] loss: 0.427
[13,     1] loss: 0.409
[14,     1] loss: 0.413
[15,     1] loss: 0.389
[16,     1] loss: 0.377
[17,     1] loss: 0.367
[18,     1] loss: 0.363
[19,     1] loss: 0.355
[20,     1] loss: 0.351
[21,     1] loss: 0.348
[22,     1] loss: 0.345
[23,     1] loss: 0.335
[24,     1] loss: 0.341
[25,     1] loss: 0.355
[26,     1] loss: 0.338
[27,     1] loss: 0.346
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017801670724368088,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2786502867,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.1819340778907979}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.681
[3,     1] loss: 0.659
[4,     1] loss: 0.639
[5,     1] loss: 0.615
[6,     1] loss: 0.592
[7,     1] loss: 0.560
[8,     1] loss: 0.529
[9,     1] loss: 0.493
[10,     1] loss: 0.459
[11,     1] loss: 0.418
[12,     1] loss: 0.380
[13,     1] loss: 0.341
[14,     1] loss: 0.308
[15,     1] loss: 0.277
[16,     1] loss: 0.245
[17,     1] loss: 0.216
[18,     1] loss: 0.192
[19,     1] loss: 0.171
[20,     1] loss: 0.150
[21,     1] loss: 0.133
[22,     1] loss: 0.116
[23,     1] loss: 0.103
[24,     1] loss: 0.088
[25,     1] loss: 0.077
[26,     1] loss: 0.069
[27,     1] loss: 0.061
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008690624006303747,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4233369970,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.373217823902852}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.690
[3,     1] loss: 0.640
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007640388383208199,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2032910303,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.757625848149408}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.665
[3,     1] loss: 0.609
[4,     1] loss: 0.550
[5,     1] loss: 0.496
[6,     1] loss: 0.445
[7,     1] loss: 0.403
[8,     1] loss: 0.366
[9,     1] loss: 0.332
[10,     1] loss: 0.306
[11,     1] loss: 0.277
[12,     1] loss: 0.256
[13,     1] loss: 0.234
[14,     1] loss: 0.213
[15,     1] loss: 0.187
[16,     1] loss: 0.163
[17,     1] loss: 0.147
[18,     1] loss: 0.115
[19,     1] loss: 0.097
[20,     1] loss: 0.082
[21,     1] loss: 0.071
[22,     1] loss: 0.062
[23,     1] loss: 0.054
[24,     1] loss: 0.047
[25,     1] loss: 0.043
[26,     1] loss: 0.040
[27,     1] loss: 0.037
[28,     1] loss: 0.035
[29,     1] loss: 0.033
[30,     1] loss: 0.033
[31,     1] loss: 0.032
[32,     1] loss: 0.034
[33,     1] loss: 0.033
[34,     1] loss: 0.033
[35,     1] loss: 0.033
[36,     1] loss: 0.032
[37,     1] loss: 0.033
[38,     1] loss: 0.032
[39,     1] loss: 0.033
[40,     1] loss: 0.032
[41,     1] loss: 0.031
[42,     1] loss: 0.031
[43,     1] loss: 0.031
[44,     1] loss: 0.030
[45,     1] loss: 0.029
[46,     1] loss: 0.029
[47,     1] loss: 0.029
[48,     1] loss: 0.028
[49,     1] loss: 0.029
[50,     1] loss: 0.028
[51,     1] loss: 0.029
[52,     1] loss: 0.028
[53,     1] loss: 0.028
[54,     1] loss: 0.028
[55,     1] loss: 0.028
[56,     1] loss: 0.028
[57,     1] loss: 0.027
[58,     1] loss: 0.028
[59,     1] loss: 0.027
[60,     1] loss: 0.027
[61,     1] loss: 0.028
[62,     1] loss: 0.065
[63,     1] loss: 0.622
[64,     1] loss: 0.966
[65,     1] loss: 0.506
[66,     1] loss: 0.640
[67,     1] loss: 0.639
Early stopping applied (best metric=0.23916740715503693)
Finished Training
Total time taken: 308.44450092315674
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.654
[4,     1] loss: 0.609
[5,     1] loss: 0.553
[6,     1] loss: 0.502
[7,     1] loss: 0.461
[8,     1] loss: 0.426
[9,     1] loss: 0.408
[10,     1] loss: 0.395
[11,     1] loss: 0.385
[12,     1] loss: 0.377
[13,     1] loss: 0.369
[14,     1] loss: 0.364
[15,     1] loss: 0.356
[16,     1] loss: 0.353
[17,     1] loss: 0.346
[18,     1] loss: 0.339
[19,     1] loss: 0.329
[20,     1] loss: 0.318
[21,     1] loss: 0.304
[22,     1] loss: 0.291
[23,     1] loss: 0.269
[24,     1] loss: 0.245
[25,     1] loss: 0.218
[26,     1] loss: 0.201
[27,     1] loss: 0.200
[28,     1] loss: 0.171
[29,     1] loss: 0.135
[30,     1] loss: 0.113
[31,     1] loss: 0.100
[32,     1] loss: 0.117
[33,     1] loss: 0.107
[34,     1] loss: 0.127
[35,     1] loss: 0.084
[36,     1] loss: 0.072
[37,     1] loss: 0.055
[38,     1] loss: 0.052
[39,     1] loss: 0.045
[40,     1] loss: 0.041
[41,     1] loss: 0.035
[42,     1] loss: 0.037
[43,     1] loss: 0.044
[44,     1] loss: 0.033
[45,     1] loss: 0.033
[46,     1] loss: 0.048
[47,     1] loss: 0.034
[48,     1] loss: 0.038
[49,     1] loss: 0.042
[50,     1] loss: 0.039
[51,     1] loss: 0.040
[52,     1] loss: 0.037
[53,     1] loss: 0.036
[54,     1] loss: 0.036
[55,     1] loss: 0.035
[56,     1] loss: 0.035
[57,     1] loss: 0.034
[58,     1] loss: 0.033
[59,     1] loss: 0.033
[60,     1] loss: 0.032
[61,     1] loss: 0.032
[62,     1] loss: 0.032
[63,     1] loss: 0.032
[64,     1] loss: 0.032
[65,     1] loss: 0.033
[66,     1] loss: 0.033
[67,     1] loss: 0.034
[68,     1] loss: 0.034
[69,     1] loss: 0.035
[70,     1] loss: 0.035
[71,     1] loss: 0.034
[72,     1] loss: 0.033
[73,     1] loss: 0.033
[74,     1] loss: 0.033
[75,     1] loss: 0.033
[76,     1] loss: 0.033
[77,     1] loss: 0.032
[78,     1] loss: 0.031
Early stopping applied (best metric=0.2527194023132324)
Finished Training
Total time taken: 359.01873445510864
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.673
[3,     1] loss: 0.617
[4,     1] loss: 0.560
[5,     1] loss: 0.505
[6,     1] loss: 0.461
[7,     1] loss: 0.420
[8,     1] loss: 0.381
[9,     1] loss: 0.355
[10,     1] loss: 0.339
[11,     1] loss: 0.332
[12,     1] loss: 0.328
[13,     1] loss: 0.322
[14,     1] loss: 0.320
[15,     1] loss: 0.316
[16,     1] loss: 0.312
[17,     1] loss: 0.307
[18,     1] loss: 0.302
[19,     1] loss: 0.296
[20,     1] loss: 0.286
[21,     1] loss: 0.276
[22,     1] loss: 0.266
[23,     1] loss: 0.250
[24,     1] loss: 0.233
[25,     1] loss: 0.215
[26,     1] loss: 0.195
[27,     1] loss: 0.173
[28,     1] loss: 0.151
[29,     1] loss: 0.128
[30,     1] loss: 0.111
[31,     1] loss: 0.093
[32,     1] loss: 0.076
[33,     1] loss: 0.067
[34,     1] loss: 0.057
[35,     1] loss: 0.049
[36,     1] loss: 0.148
[37,     1] loss: 0.588
[38,     1] loss: 0.181
[39,     1] loss: 0.416
[40,     1] loss: 0.319
[41,     1] loss: 0.344
[42,     1] loss: 0.379
[43,     1] loss: 0.390
[44,     1] loss: 0.379
[45,     1] loss: 0.358
[46,     1] loss: 0.332
[47,     1] loss: 0.302
[48,     1] loss: 0.269
[49,     1] loss: 0.234
[50,     1] loss: 0.199
[51,     1] loss: 0.168
[52,     1] loss: 0.145
[53,     1] loss: 0.125
[54,     1] loss: 0.109
[55,     1] loss: 0.095
[56,     1] loss: 0.088
[57,     1] loss: 0.082
[58,     1] loss: 0.076
[59,     1] loss: 0.075
[60,     1] loss: 0.071
[61,     1] loss: 0.071
[62,     1] loss: 0.071
[63,     1] loss: 0.073
[64,     1] loss: 0.073
[65,     1] loss: 0.075
[66,     1] loss: 0.075
[67,     1] loss: 0.078
[68,     1] loss: 0.078
[69,     1] loss: 0.077
[70,     1] loss: 0.080
[71,     1] loss: 0.078
[72,     1] loss: 0.077
[73,     1] loss: 0.075
[74,     1] loss: 0.074
[75,     1] loss: 0.075
[76,     1] loss: 0.074
[77,     1] loss: 0.072
[78,     1] loss: 0.071
[79,     1] loss: 0.072
[80,     1] loss: 0.073
[81,     1] loss: 0.074
[82,     1] loss: 0.073
[83,     1] loss: 0.073
[84,     1] loss: 0.073
[85,     1] loss: 0.073
[86,     1] loss: 0.073
[87,     1] loss: 0.072
[88,     1] loss: 0.074
[89,     1] loss: 0.079
[90,     1] loss: 0.211
[91,     1] loss: 0.498
[92,     1] loss: 0.626
[93,     1] loss: 0.406
[94,     1] loss: 0.519
[95,     1] loss: 0.445
[96,     1] loss: 0.445
[97,     1] loss: 0.464
[98,     1] loss: 0.471
[99,     1] loss: 0.472
[100,     1] loss: 0.463
[101,     1] loss: 0.446
[102,     1] loss: 0.426
[103,     1] loss: 0.406
[104,     1] loss: 0.381
[105,     1] loss: 0.348
[106,     1] loss: 0.318
[107,     1] loss: 0.292
[108,     1] loss: 0.262
[109,     1] loss: 0.236
[110,     1] loss: 0.215
[111,     1] loss: 0.188
[112,     1] loss: 0.187
[113,     1] loss: 0.169
[114,     1] loss: 0.175
Early stopping applied (best metric=0.24722324311733246)
Finished Training
Total time taken: 525.5750916004181
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.681
[3,     1] loss: 0.647
[4,     1] loss: 0.610
[5,     1] loss: 0.569
[6,     1] loss: 0.526
[7,     1] loss: 0.478
[8,     1] loss: 0.430
[9,     1] loss: 0.378
[10,     1] loss: 0.327
[11,     1] loss: 0.271
[12,     1] loss: 0.226
[13,     1] loss: 0.181
[14,     1] loss: 0.147
[15,     1] loss: 0.120
[16,     1] loss: 0.099
[17,     1] loss: 0.084
[18,     1] loss: 0.064
[19,     1] loss: 0.052
[20,     1] loss: 0.047
[21,     1] loss: 0.042
[22,     1] loss: 0.038
[23,     1] loss: 0.036
[24,     1] loss: 0.035
[25,     1] loss: 0.034
[26,     1] loss: 0.035
[27,     1] loss: 0.035
[28,     1] loss: 0.035
[29,     1] loss: 0.036
[30,     1] loss: 0.037
[31,     1] loss: 0.038
[32,     1] loss: 0.038
[33,     1] loss: 0.037
[34,     1] loss: 0.037
[35,     1] loss: 0.037
[36,     1] loss: 0.036
[37,     1] loss: 0.036
[38,     1] loss: 0.035
[39,     1] loss: 0.035
[40,     1] loss: 0.034
[41,     1] loss: 0.033
[42,     1] loss: 0.032
[43,     1] loss: 0.033
[44,     1] loss: 0.032
[45,     1] loss: 0.032
[46,     1] loss: 0.031
[47,     1] loss: 0.032
[48,     1] loss: 0.031
[49,     1] loss: 0.032
[50,     1] loss: 0.032
[51,     1] loss: 0.032
[52,     1] loss: 0.032
[53,     1] loss: 0.032
[54,     1] loss: 0.032
[55,     1] loss: 0.032
[56,     1] loss: 0.032
[57,     1] loss: 0.033
[58,     1] loss: 0.035
[59,     1] loss: 0.202
[60,     1] loss: 1.114
[61,     1] loss: 0.529
[62,     1] loss: 0.641
[63,     1] loss: 0.526
[64,     1] loss: 0.577
[65,     1] loss: 0.591
[66,     1] loss: 0.592
[67,     1] loss: 0.588
[68,     1] loss: 0.581
[69,     1] loss: 0.574
[70,     1] loss: 0.561
[71,     1] loss: 0.548
[72,     1] loss: 0.531
[73,     1] loss: 0.515
[74,     1] loss: 0.506
[75,     1] loss: 0.483
[76,     1] loss: 0.469
[77,     1] loss: 0.461
[78,     1] loss: 0.448
[79,     1] loss: 0.437
[80,     1] loss: 0.427
[81,     1] loss: 0.418
[82,     1] loss: 0.410
[83,     1] loss: 0.400
[84,     1] loss: 0.395
[85,     1] loss: 0.382
[86,     1] loss: 0.374
[87,     1] loss: 0.369
[88,     1] loss: 0.364
[89,     1] loss: 0.360
[90,     1] loss: 0.356
[91,     1] loss: 0.353
[92,     1] loss: 0.349
[93,     1] loss: 0.346
[94,     1] loss: 0.343
[95,     1] loss: 0.342
[96,     1] loss: 0.341
[97,     1] loss: 0.342
[98,     1] loss: 0.340
[99,     1] loss: 0.342
[100,     1] loss: 0.340
[101,     1] loss: 0.341
[102,     1] loss: 0.340
[103,     1] loss: 0.341
[104,     1] loss: 0.340
[105,     1] loss: 0.340
[106,     1] loss: 0.341
Early stopping applied (best metric=0.24286356568336487)
Finished Training
Total time taken: 514.2196445465088
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.673
[3,     1] loss: 0.615
[4,     1] loss: 0.549
[5,     1] loss: 0.487
[6,     1] loss: 0.446
[7,     1] loss: 0.420
[8,     1] loss: 0.401
[9,     1] loss: 0.385
[10,     1] loss: 0.376
[11,     1] loss: 0.368
[12,     1] loss: 0.363
[13,     1] loss: 0.356
[14,     1] loss: 0.351
[15,     1] loss: 0.346
[16,     1] loss: 0.337
[17,     1] loss: 0.330
[18,     1] loss: 0.321
[19,     1] loss: 0.311
[20,     1] loss: 0.299
[21,     1] loss: 0.287
[22,     1] loss: 0.269
[23,     1] loss: 0.252
[24,     1] loss: 0.230
[25,     1] loss: 0.212
[26,     1] loss: 0.191
[27,     1] loss: 0.170
[28,     1] loss: 0.153
[29,     1] loss: 0.136
[30,     1] loss: 0.192
[31,     1] loss: 0.269
[32,     1] loss: 0.303
[33,     1] loss: 0.255
[34,     1] loss: 0.264
[35,     1] loss: 0.256
[36,     1] loss: 0.237
[37,     1] loss: 0.213
[38,     1] loss: 0.180
[39,     1] loss: 0.152
[40,     1] loss: 0.125
[41,     1] loss: 0.101
[42,     1] loss: 0.086
[43,     1] loss: 0.072
[44,     1] loss: 0.063
[45,     1] loss: 0.058
[46,     1] loss: 0.056
[47,     1] loss: 0.055
[48,     1] loss: 0.053
[49,     1] loss: 0.053
[50,     1] loss: 0.052
[51,     1] loss: 0.054
[52,     1] loss: 0.055
[53,     1] loss: 0.055
[54,     1] loss: 0.058
[55,     1] loss: 0.058
[56,     1] loss: 0.058
[57,     1] loss: 0.059
[58,     1] loss: 0.059
[59,     1] loss: 0.058
[60,     1] loss: 0.057
[61,     1] loss: 0.057
[62,     1] loss: 0.058
[63,     1] loss: 0.056
[64,     1] loss: 0.056
[65,     1] loss: 0.055
[66,     1] loss: 0.055
[67,     1] loss: 0.055
[68,     1] loss: 0.055
[69,     1] loss: 0.055
[70,     1] loss: 0.055
[71,     1] loss: 0.055
[72,     1] loss: 0.055
[73,     1] loss: 0.055
[74,     1] loss: 0.056
[75,     1] loss: 0.056
[76,     1] loss: 0.062
[77,     1] loss: 0.099
[78,     1] loss: 0.364
[79,     1] loss: 0.632
[80,     1] loss: 0.452
[81,     1] loss: 0.568
[82,     1] loss: 0.425
[83,     1] loss: 0.453
[84,     1] loss: 0.459
[85,     1] loss: 0.462
[86,     1] loss: 0.467
[87,     1] loss: 0.461
[88,     1] loss: 0.441
[89,     1] loss: 0.410
[90,     1] loss: 0.378
Early stopping applied (best metric=0.27445146441459656)
Finished Training
Total time taken: 448.40014600753784
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8464516129032258, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8791666666666667, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8404580152671756, 'Pyrrolidone carboxylic acid Validation Precision': 0.5106245149170852, 'Pyrrolidone carboxylic acid AUC ROC': 0.9357188295165395, 'Pyrrolidone carboxylic acid AUC PR': 0.7831781292089521, 'Pyrrolidone carboxylic acid MCC': 0.5902086666652008, 'Pyrrolidone carboxylic acid F1': 0.6421466155810983, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.25128501653671265, 'Validation Loss (total)': 0.25128501653671265}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009690940154720395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4268676753,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.202527939989478}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.652
[4,     1] loss: 0.617
[5,     1] loss: 0.575
[6,     1] loss: 0.529
[7,     1] loss: 0.479
[8,     1] loss: 0.422
[9,     1] loss: 0.373
[10,     1] loss: 0.325
[11,     1] loss: 0.276
[12,     1] loss: 0.230
[13,     1] loss: 0.193
[14,     1] loss: 0.164
[15,     1] loss: 0.136
[16,     1] loss: 0.118
[17,     1] loss: 0.104
[18,     1] loss: 0.088
[19,     1] loss: 0.081
[20,     1] loss: 0.072
[21,     1] loss: 0.074
[22,     1] loss: 0.066
[23,     1] loss: 0.070
[24,     1] loss: 0.063
[25,     1] loss: 0.069
[26,     1] loss: 0.063
[27,     1] loss: 0.063
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006552329968054665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3355521741,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.574009836518584}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.660
[4,     1] loss: 0.636
[5,     1] loss: 0.610
[6,     1] loss: 0.578
[7,     1] loss: 0.547
[8,     1] loss: 0.516
[9,     1] loss: 0.483
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006162929389150787,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 288886347,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.525685252042742}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.680
[3,     1] loss: 0.640
[4,     1] loss: 0.603
[5,     1] loss: 0.564
[6,     1] loss: 0.528
[7,     1] loss: 0.495
[8,     1] loss: 0.464
[9,     1] loss: 0.439
[10,     1] loss: 0.417
[11,     1] loss: 0.397
[12,     1] loss: 0.381
[13,     1] loss: 0.368
[14,     1] loss: 0.359
[15,     1] loss: 0.350
[16,     1] loss: 0.342
[17,     1] loss: 0.333
[18,     1] loss: 0.322
[19,     1] loss: 0.313
[20,     1] loss: 0.301
[21,     1] loss: 0.290
[22,     1] loss: 0.280
[23,     1] loss: 0.264
[24,     1] loss: 0.276
[25,     1] loss: 0.295
[26,     1] loss: 0.289
[27,     1] loss: 0.269
[28,     1] loss: 0.261
[29,     1] loss: 0.240
[30,     1] loss: 0.225
[31,     1] loss: 0.205
[32,     1] loss: 0.186
[33,     1] loss: 0.168
[34,     1] loss: 0.154
[35,     1] loss: 0.137
[36,     1] loss: 0.123
[37,     1] loss: 0.114
[38,     1] loss: 0.103
[39,     1] loss: 0.095
[40,     1] loss: 0.087
[41,     1] loss: 0.082
[42,     1] loss: 0.076
[43,     1] loss: 0.072
[44,     1] loss: 0.071
[45,     1] loss: 0.070
[46,     1] loss: 0.067
[47,     1] loss: 0.068
[48,     1] loss: 0.066
[49,     1] loss: 0.068
[50,     1] loss: 0.069
[51,     1] loss: 0.068
[52,     1] loss: 0.068
[53,     1] loss: 0.070
[54,     1] loss: 0.070
[55,     1] loss: 0.070
[56,     1] loss: 0.071
[57,     1] loss: 0.070
[58,     1] loss: 0.072
[59,     1] loss: 0.071
[60,     1] loss: 0.074
[61,     1] loss: 0.085
[62,     1] loss: 0.084
[63,     1] loss: 0.087
[64,     1] loss: 0.225
[65,     1] loss: 0.564
[66,     1] loss: 0.351
[67,     1] loss: 0.318
[68,     1] loss: 0.349
[69,     1] loss: 0.305
[70,     1] loss: 0.297
[71,     1] loss: 0.287
[72,     1] loss: 0.268
[73,     1] loss: 0.236
[74,     1] loss: 0.212
[75,     1] loss: 0.190
[76,     1] loss: 0.170
[77,     1] loss: 0.151
[78,     1] loss: 0.138
[79,     1] loss: 0.125
[80,     1] loss: 0.116
[81,     1] loss: 0.108
[82,     1] loss: 0.104
[83,     1] loss: 0.102
[84,     1] loss: 0.098
[85,     1] loss: 0.097
[86,     1] loss: 0.098
[87,     1] loss: 0.099
[88,     1] loss: 0.100
[89,     1] loss: 0.102
[90,     1] loss: 0.103
[91,     1] loss: 0.105
[92,     1] loss: 0.107
[93,     1] loss: 0.105
[94,     1] loss: 0.110
[95,     1] loss: 0.104
[96,     1] loss: 0.105
[97,     1] loss: 0.103
[98,     1] loss: 0.103
[99,     1] loss: 0.103
[100,     1] loss: 0.105
[101,     1] loss: 0.102
[102,     1] loss: 0.104
[103,     1] loss: 0.102
[104,     1] loss: 0.106
[105,     1] loss: 0.133
[106,     1] loss: 0.387
[107,     1] loss: 0.715
[108,     1] loss: 0.669
[109,     1] loss: 0.589
Early stopping applied (best metric=0.24485589563846588)
Finished Training
Total time taken: 533.7764520645142
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.713
[2,     1] loss: 0.686
[3,     1] loss: 0.658
[4,     1] loss: 0.633
[5,     1] loss: 0.605
[6,     1] loss: 0.575
[7,     1] loss: 0.546
[8,     1] loss: 0.521
[9,     1] loss: 0.492
[10,     1] loss: 0.468
[11,     1] loss: 0.448
[12,     1] loss: 0.430
[13,     1] loss: 0.415
[14,     1] loss: 0.402
[15,     1] loss: 0.388
[16,     1] loss: 0.380
[17,     1] loss: 0.370
[18,     1] loss: 0.363
[19,     1] loss: 0.353
[20,     1] loss: 0.343
[21,     1] loss: 0.336
[22,     1] loss: 0.331
[23,     1] loss: 0.329
[24,     1] loss: 0.320
[25,     1] loss: 0.328
[26,     1] loss: 0.328
[27,     1] loss: 0.319
[28,     1] loss: 0.305
[29,     1] loss: 0.302
[30,     1] loss: 0.285
[31,     1] loss: 0.285
[32,     1] loss: 0.273
[33,     1] loss: 0.263
[34,     1] loss: 0.253
[35,     1] loss: 0.245
[36,     1] loss: 0.234
[37,     1] loss: 0.227
[38,     1] loss: 0.217
[39,     1] loss: 0.230
[40,     1] loss: 0.262
[41,     1] loss: 0.218
[42,     1] loss: 0.221
[43,     1] loss: 0.209
[44,     1] loss: 0.205
[45,     1] loss: 0.180
[46,     1] loss: 0.172
[47,     1] loss: 0.164
[48,     1] loss: 0.149
[49,     1] loss: 0.139
[50,     1] loss: 0.130
[51,     1] loss: 0.123
[52,     1] loss: 0.117
[53,     1] loss: 0.110
[54,     1] loss: 0.106
[55,     1] loss: 0.103
[56,     1] loss: 0.099
[57,     1] loss: 0.098
[58,     1] loss: 0.096
[59,     1] loss: 0.095
[60,     1] loss: 0.096
[61,     1] loss: 0.095
[62,     1] loss: 0.095
[63,     1] loss: 0.094
[64,     1] loss: 0.092
[65,     1] loss: 0.093
[66,     1] loss: 0.095
[67,     1] loss: 0.095
[68,     1] loss: 0.093
[69,     1] loss: 0.093
[70,     1] loss: 0.095
[71,     1] loss: 0.094
[72,     1] loss: 0.094
[73,     1] loss: 0.095
[74,     1] loss: 0.095
[75,     1] loss: 0.095
[76,     1] loss: 0.095
[77,     1] loss: 0.105
[78,     1] loss: 0.175
[79,     1] loss: 1.160
[80,     1] loss: 0.511
[81,     1] loss: 0.615
[82,     1] loss: 0.597
[83,     1] loss: 0.596
[84,     1] loss: 0.600
[85,     1] loss: 0.607
[86,     1] loss: 0.616
[87,     1] loss: 0.620
[88,     1] loss: 0.621
[89,     1] loss: 0.620
[90,     1] loss: 0.618
[91,     1] loss: 0.616
[92,     1] loss: 0.614
[93,     1] loss: 0.614
[94,     1] loss: 0.612
[95,     1] loss: 0.611
[96,     1] loss: 0.610
[97,     1] loss: 0.608
[98,     1] loss: 0.607
[99,     1] loss: 0.605
[100,     1] loss: 0.602
[101,     1] loss: 0.599
[102,     1] loss: 0.596
[103,     1] loss: 0.593
[104,     1] loss: 0.590
[105,     1] loss: 0.587
[106,     1] loss: 0.582
[107,     1] loss: 0.574
Early stopping applied (best metric=0.2248762547969818)
Finished Training
Total time taken: 521.2771315574646
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.694
[3,     1] loss: 0.658
[4,     1] loss: 0.628
[5,     1] loss: 0.596
[6,     1] loss: 0.562
[7,     1] loss: 0.526
[8,     1] loss: 0.488
[9,     1] loss: 0.449
[10,     1] loss: 0.411
[11,     1] loss: 0.376
[12,     1] loss: 0.332
[13,     1] loss: 0.304
[14,     1] loss: 0.272
[15,     1] loss: 0.242
[16,     1] loss: 0.214
[17,     1] loss: 0.192
[18,     1] loss: 0.175
[19,     1] loss: 0.159
[20,     1] loss: 0.147
[21,     1] loss: 0.134
[22,     1] loss: 0.122
[23,     1] loss: 0.117
[24,     1] loss: 0.107
[25,     1] loss: 0.100
[26,     1] loss: 0.096
[27,     1] loss: 0.092
[28,     1] loss: 0.087
[29,     1] loss: 0.084
[30,     1] loss: 0.081
[31,     1] loss: 0.076
[32,     1] loss: 0.076
[33,     1] loss: 0.075
[34,     1] loss: 0.072
[35,     1] loss: 0.081
[36,     1] loss: 0.102
[37,     1] loss: 0.252
[38,     1] loss: 0.475
[39,     1] loss: 0.334
[40,     1] loss: 0.337
[41,     1] loss: 0.321
[42,     1] loss: 0.322
[43,     1] loss: 0.313
[44,     1] loss: 0.302
[45,     1] loss: 0.281
[46,     1] loss: 0.260
[47,     1] loss: 0.237
[48,     1] loss: 0.219
[49,     1] loss: 0.200
[50,     1] loss: 0.183
[51,     1] loss: 0.165
[52,     1] loss: 0.151
[53,     1] loss: 0.135
[54,     1] loss: 0.127
[55,     1] loss: 0.120
[56,     1] loss: 0.112
[57,     1] loss: 0.108
[58,     1] loss: 0.104
[59,     1] loss: 0.103
[60,     1] loss: 0.101
[61,     1] loss: 0.101
[62,     1] loss: 0.101
[63,     1] loss: 0.103
[64,     1] loss: 0.102
[65,     1] loss: 0.108
[66,     1] loss: 0.107
[67,     1] loss: 0.106
[68,     1] loss: 0.105
[69,     1] loss: 0.106
[70,     1] loss: 0.106
[71,     1] loss: 0.106
[72,     1] loss: 0.109
[73,     1] loss: 0.115
[74,     1] loss: 0.122
[75,     1] loss: 0.164
[76,     1] loss: 0.330
[77,     1] loss: 0.545
[78,     1] loss: 0.367
[79,     1] loss: 0.352
[80,     1] loss: 0.340
[81,     1] loss: 0.333
[82,     1] loss: 0.322
[83,     1] loss: 0.315
[84,     1] loss: 0.291
[85,     1] loss: 0.274
[86,     1] loss: 0.258
[87,     1] loss: 0.243
[88,     1] loss: 0.227
[89,     1] loss: 0.213
[90,     1] loss: 0.200
[91,     1] loss: 0.186
[92,     1] loss: 0.175
[93,     1] loss: 0.163
[94,     1] loss: 0.155
[95,     1] loss: 0.149
[96,     1] loss: 0.143
[97,     1] loss: 0.137
[98,     1] loss: 0.138
[99,     1] loss: 0.133
[100,     1] loss: 0.134
[101,     1] loss: 0.130
[102,     1] loss: 0.127
[103,     1] loss: 0.128
[104,     1] loss: 0.127
[105,     1] loss: 0.127
[106,     1] loss: 0.126
[107,     1] loss: 0.126
[108,     1] loss: 0.123
[109,     1] loss: 0.126
[110,     1] loss: 0.124
[111,     1] loss: 0.128
[112,     1] loss: 0.240
[113,     1] loss: 0.210
[114,     1] loss: 0.818
[115,     1] loss: 0.593
[116,     1] loss: 0.512
[117,     1] loss: 0.518
[118,     1] loss: 0.497
[119,     1] loss: 0.460
[120,     1] loss: 0.445
[121,     1] loss: 0.435
[122,     1] loss: 0.426
[123,     1] loss: 0.415
[124,     1] loss: 0.396
[125,     1] loss: 0.381
[126,     1] loss: 0.368
[127,     1] loss: 0.351
[128,     1] loss: 0.340
[129,     1] loss: 0.324
[130,     1] loss: 0.309
[131,     1] loss: 0.295
[132,     1] loss: 0.282
[133,     1] loss: 0.269
[134,     1] loss: 0.259
[135,     1] loss: 0.246
[136,     1] loss: 0.238
[137,     1] loss: 0.228
[138,     1] loss: 0.218
[139,     1] loss: 0.211
[140,     1] loss: 0.200
[141,     1] loss: 0.196
[142,     1] loss: 0.186
[143,     1] loss: 0.178
[144,     1] loss: 0.170
[145,     1] loss: 0.164
[146,     1] loss: 0.162
[147,     1] loss: 0.205
Early stopping applied (best metric=0.22606666386127472)
Finished Training
Total time taken: 700.010924577713
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.709
[2,     1] loss: 0.698
[3,     1] loss: 0.684
[4,     1] loss: 0.670
[5,     1] loss: 0.652
[6,     1] loss: 0.633
[7,     1] loss: 0.610
[8,     1] loss: 0.583
[9,     1] loss: 0.552
[10,     1] loss: 0.523
[11,     1] loss: 0.494
[12,     1] loss: 0.467
[13,     1] loss: 0.438
[14,     1] loss: 0.416
[15,     1] loss: 0.393
[16,     1] loss: 0.370
[17,     1] loss: 0.344
[18,     1] loss: 0.317
[19,     1] loss: 0.293
[20,     1] loss: 0.264
[21,     1] loss: 0.242
[22,     1] loss: 0.215
[23,     1] loss: 0.190
[24,     1] loss: 0.170
[25,     1] loss: 0.151
[26,     1] loss: 0.130
[27,     1] loss: 0.116
[28,     1] loss: 0.101
[29,     1] loss: 0.091
[30,     1] loss: 0.081
[31,     1] loss: 0.073
[32,     1] loss: 0.068
[33,     1] loss: 0.063
[34,     1] loss: 0.060
[35,     1] loss: 0.057
[36,     1] loss: 0.056
[37,     1] loss: 0.054
[38,     1] loss: 0.054
[39,     1] loss: 0.054
[40,     1] loss: 0.054
[41,     1] loss: 0.054
[42,     1] loss: 0.056
[43,     1] loss: 0.055
[44,     1] loss: 0.056
[45,     1] loss: 0.056
[46,     1] loss: 0.056
[47,     1] loss: 0.057
[48,     1] loss: 0.056
[49,     1] loss: 0.055
[50,     1] loss: 0.055
[51,     1] loss: 0.054
[52,     1] loss: 0.061
[53,     1] loss: 0.121
[54,     1] loss: 0.676
[55,     1] loss: 0.515
[56,     1] loss: 0.558
[57,     1] loss: 0.557
[58,     1] loss: 0.558
[59,     1] loss: 0.551
[60,     1] loss: 0.536
[61,     1] loss: 0.537
[62,     1] loss: 0.529
[63,     1] loss: 0.517
[64,     1] loss: 0.513
[65,     1] loss: 0.500
[66,     1] loss: 0.482
[67,     1] loss: 0.465
[68,     1] loss: 0.445
[69,     1] loss: 0.421
[70,     1] loss: 0.396
[71,     1] loss: 0.372
[72,     1] loss: 0.344
[73,     1] loss: 0.317
[74,     1] loss: 0.289
[75,     1] loss: 0.259
[76,     1] loss: 0.235
[77,     1] loss: 0.209
[78,     1] loss: 0.187
[79,     1] loss: 0.164
[80,     1] loss: 0.146
Early stopping applied (best metric=0.2310379296541214)
Finished Training
Total time taken: 391.73719811439514
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.671
[3,     1] loss: 0.621
[4,     1] loss: 0.575
[5,     1] loss: 0.526
[6,     1] loss: 0.481
[7,     1] loss: 0.443
[8,     1] loss: 0.408
[9,     1] loss: 0.377
[10,     1] loss: 0.352
[11,     1] loss: 0.336
[12,     1] loss: 0.324
[13,     1] loss: 0.326
[14,     1] loss: 0.322
[15,     1] loss: 0.320
[16,     1] loss: 0.315
[17,     1] loss: 0.314
[18,     1] loss: 0.311
[19,     1] loss: 0.310
[20,     1] loss: 0.309
[21,     1] loss: 0.309
[22,     1] loss: 0.307
[23,     1] loss: 0.304
[24,     1] loss: 0.302
[25,     1] loss: 0.298
[26,     1] loss: 0.294
[27,     1] loss: 0.289
[28,     1] loss: 0.283
[29,     1] loss: 0.278
[30,     1] loss: 0.271
[31,     1] loss: 0.265
[32,     1] loss: 0.257
[33,     1] loss: 0.248
[34,     1] loss: 0.241
[35,     1] loss: 0.229
[36,     1] loss: 0.221
[37,     1] loss: 0.210
[38,     1] loss: 0.200
[39,     1] loss: 0.189
[40,     1] loss: 0.179
[41,     1] loss: 0.169
[42,     1] loss: 0.161
[43,     1] loss: 0.154
[44,     1] loss: 0.143
[45,     1] loss: 0.139
[46,     1] loss: 0.131
[47,     1] loss: 0.132
[48,     1] loss: 0.190
[49,     1] loss: 0.550
[50,     1] loss: 0.499
[51,     1] loss: 0.396
[52,     1] loss: 0.369
[53,     1] loss: 0.396
[54,     1] loss: 0.423
[55,     1] loss: 0.398
[56,     1] loss: 0.397
[57,     1] loss: 0.375
[58,     1] loss: 0.350
[59,     1] loss: 0.332
[60,     1] loss: 0.298
[61,     1] loss: 0.276
[62,     1] loss: 0.264
[63,     1] loss: 0.250
[64,     1] loss: 0.242
[65,     1] loss: 0.235
[66,     1] loss: 0.231
[67,     1] loss: 0.225
[68,     1] loss: 0.222
[69,     1] loss: 0.219
[70,     1] loss: 0.217
[71,     1] loss: 0.214
[72,     1] loss: 0.213
[73,     1] loss: 0.210
[74,     1] loss: 0.207
[75,     1] loss: 0.205
[76,     1] loss: 0.203
[77,     1] loss: 0.199
[78,     1] loss: 0.195
[79,     1] loss: 0.188
[80,     1] loss: 0.185
[81,     1] loss: 0.176
[82,     1] loss: 0.173
[83,     1] loss: 0.171
[84,     1] loss: 0.166
[85,     1] loss: 0.162
[86,     1] loss: 0.161
[87,     1] loss: 0.157
[88,     1] loss: 0.155
[89,     1] loss: 0.151
[90,     1] loss: 0.150
[91,     1] loss: 0.150
[92,     1] loss: 0.145
[93,     1] loss: 0.145
[94,     1] loss: 0.145
[95,     1] loss: 0.144
Early stopping applied (best metric=0.2497408241033554)
Finished Training
Total time taken: 480.03238677978516
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8877419354838709, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8708333333333333, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8908396946564886, 'Pyrrolidone carboxylic acid Validation Precision': 0.622603292911447, 'Pyrrolidone carboxylic acid AUC ROC': 0.9524650127226464, 'Pyrrolidone carboxylic acid AUC PR': 0.867321476964726, 'Pyrrolidone carboxylic acid MCC': 0.6717335430698522, 'Pyrrolidone carboxylic acid F1': 0.715090273514441, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.23531551361083985, 'Validation Loss (total)': 0.23531551361083985}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007988552511427747,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1495263012,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.735204138552211}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.673
[3,     1] loss: 0.626
[4,     1] loss: 0.575
[5,     1] loss: 0.526
[6,     1] loss: 0.480
[7,     1] loss: 0.442
[8,     1] loss: 0.408
[9,     1] loss: 0.380
[10,     1] loss: 0.352
[11,     1] loss: 0.324
[12,     1] loss: 0.299
[13,     1] loss: 0.293
[14,     1] loss: 0.263
[15,     1] loss: 0.250
[16,     1] loss: 0.226
[17,     1] loss: 0.202
[18,     1] loss: 0.178
[19,     1] loss: 0.157
[20,     1] loss: 0.138
[21,     1] loss: 0.121
[22,     1] loss: 0.101
[23,     1] loss: 0.089
[24,     1] loss: 0.080
[25,     1] loss: 0.070
[26,     1] loss: 0.063
[27,     1] loss: 0.057
[28,     1] loss: 0.053
[29,     1] loss: 0.048
[30,     1] loss: 0.047
[31,     1] loss: 0.045
[32,     1] loss: 0.045
[33,     1] loss: 0.045
[34,     1] loss: 0.043
[35,     1] loss: 0.043
[36,     1] loss: 0.044
[37,     1] loss: 0.043
[38,     1] loss: 0.044
[39,     1] loss: 0.044
[40,     1] loss: 0.044
[41,     1] loss: 0.045
[42,     1] loss: 0.044
[43,     1] loss: 0.044
[44,     1] loss: 0.045
[45,     1] loss: 0.043
[46,     1] loss: 0.043
[47,     1] loss: 0.043
[48,     1] loss: 0.043
[49,     1] loss: 0.043
[50,     1] loss: 0.042
[51,     1] loss: 0.041
[52,     1] loss: 0.042
[53,     1] loss: 0.040
[54,     1] loss: 0.041
[55,     1] loss: 0.041
[56,     1] loss: 0.047
[57,     1] loss: 0.636
[58,     1] loss: 0.775
[59,     1] loss: 0.634
[60,     1] loss: 0.653
[61,     1] loss: 0.625
[62,     1] loss: 0.611
[63,     1] loss: 0.606
[64,     1] loss: 0.604
[65,     1] loss: 0.597
[66,     1] loss: 0.590
[67,     1] loss: 0.583
[68,     1] loss: 0.573
[69,     1] loss: 0.562
[70,     1] loss: 0.548
Early stopping applied (best metric=0.23971468210220337)
Finished Training
Total time taken: 351.82513523101807
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.651
[4,     1] loss: 0.613
[5,     1] loss: 0.570
[6,     1] loss: 0.525
[7,     1] loss: 0.482
[8,     1] loss: 0.444
[9,     1] loss: 0.407
[10,     1] loss: 0.374
[11,     1] loss: 0.341
[12,     1] loss: 0.312
[13,     1] loss: 0.281
[14,     1] loss: 0.251
[15,     1] loss: 0.212
[16,     1] loss: 0.173
[17,     1] loss: 0.143
[18,     1] loss: 0.116
[19,     1] loss: 0.097
[20,     1] loss: 0.080
[21,     1] loss: 0.067
[22,     1] loss: 0.059
[23,     1] loss: 0.048
[24,     1] loss: 0.045
[25,     1] loss: 0.042
[26,     1] loss: 0.060
[27,     1] loss: 0.084
[28,     1] loss: 0.128
[29,     1] loss: 0.111
[30,     1] loss: 0.088
[31,     1] loss: 0.103
[32,     1] loss: 0.111
[33,     1] loss: 0.100
[34,     1] loss: 0.087
[35,     1] loss: 0.072
[36,     1] loss: 0.064
[37,     1] loss: 0.053
[38,     1] loss: 0.048
[39,     1] loss: 0.045
[40,     1] loss: 0.042
[41,     1] loss: 0.040
[42,     1] loss: 0.042
[43,     1] loss: 0.041
[44,     1] loss: 0.043
[45,     1] loss: 0.044
[46,     1] loss: 0.045
[47,     1] loss: 0.047
[48,     1] loss: 0.047
[49,     1] loss: 0.048
[50,     1] loss: 0.049
[51,     1] loss: 0.049
[52,     1] loss: 0.048
[53,     1] loss: 0.048
[54,     1] loss: 0.048
[55,     1] loss: 0.048
[56,     1] loss: 0.046
[57,     1] loss: 0.046
[58,     1] loss: 0.045
[59,     1] loss: 0.045
[60,     1] loss: 0.044
[61,     1] loss: 0.045
[62,     1] loss: 0.045
[63,     1] loss: 0.051
[64,     1] loss: 0.077
[65,     1] loss: 0.887
[66,     1] loss: 0.495
[67,     1] loss: 0.444
[68,     1] loss: 0.436
[69,     1] loss: 0.442
[70,     1] loss: 0.449
[71,     1] loss: 0.441
[72,     1] loss: 0.430
[73,     1] loss: 0.426
[74,     1] loss: 0.402
[75,     1] loss: 0.390
[76,     1] loss: 0.378
[77,     1] loss: 0.365
[78,     1] loss: 0.352
[79,     1] loss: 0.335
[80,     1] loss: 0.321
[81,     1] loss: 0.308
[82,     1] loss: 0.293
[83,     1] loss: 0.280
[84,     1] loss: 0.263
[85,     1] loss: 0.248
[86,     1] loss: 0.230
Early stopping applied (best metric=0.20350493490695953)
Finished Training
Total time taken: 436.17811942100525
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.656
[4,     1] loss: 0.620
[5,     1] loss: 0.577
[6,     1] loss: 0.532
[7,     1] loss: 0.485
[8,     1] loss: 0.429
[9,     1] loss: 0.374
[10,     1] loss: 0.331
[11,     1] loss: 0.294
[12,     1] loss: 0.253
[13,     1] loss: 0.231
[14,     1] loss: 0.207
[15,     1] loss: 0.181
[16,     1] loss: 0.156
[17,     1] loss: 0.142
[18,     1] loss: 0.128
[19,     1] loss: 0.107
[20,     1] loss: 0.109
[21,     1] loss: 0.087
[22,     1] loss: 0.094
[23,     1] loss: 0.077
[24,     1] loss: 0.065
[25,     1] loss: 0.063
[26,     1] loss: 0.057
[27,     1] loss: 0.050
[28,     1] loss: 0.046
[29,     1] loss: 0.043
[30,     1] loss: 0.042
[31,     1] loss: 0.040
[32,     1] loss: 0.040
[33,     1] loss: 0.039
[34,     1] loss: 0.039
[35,     1] loss: 0.040
[36,     1] loss: 0.041
[37,     1] loss: 0.042
[38,     1] loss: 0.043
[39,     1] loss: 0.042
[40,     1] loss: 0.044
[41,     1] loss: 0.042
[42,     1] loss: 0.043
[43,     1] loss: 0.044
[44,     1] loss: 0.043
[45,     1] loss: 0.043
[46,     1] loss: 0.041
[47,     1] loss: 0.042
[48,     1] loss: 0.040
[49,     1] loss: 0.040
[50,     1] loss: 0.040
[51,     1] loss: 0.039
[52,     1] loss: 0.038
[53,     1] loss: 0.039
[54,     1] loss: 0.044
[55,     1] loss: 0.060
[56,     1] loss: 0.435
[57,     1] loss: 1.093
[58,     1] loss: 0.747
[59,     1] loss: 0.624
[60,     1] loss: 0.634
[61,     1] loss: 0.648
[62,     1] loss: 0.655
[63,     1] loss: 0.657
[64,     1] loss: 0.657
[65,     1] loss: 0.657
[66,     1] loss: 0.655
[67,     1] loss: 0.652
[68,     1] loss: 0.649
[69,     1] loss: 0.644
[70,     1] loss: 0.640
[71,     1] loss: 0.634
[72,     1] loss: 0.626
Early stopping applied (best metric=0.2177673578262329)
Finished Training
Total time taken: 360.2715275287628
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.677
[3,     1] loss: 0.634
[4,     1] loss: 0.584
[5,     1] loss: 0.535
[6,     1] loss: 0.495
[7,     1] loss: 0.456
[8,     1] loss: 0.423
[9,     1] loss: 0.403
[10,     1] loss: 0.372
[11,     1] loss: 0.364
[12,     1] loss: 0.357
[13,     1] loss: 0.350
[14,     1] loss: 0.348
[15,     1] loss: 0.345
[16,     1] loss: 0.342
[17,     1] loss: 0.340
[18,     1] loss: 0.337
[19,     1] loss: 0.335
[20,     1] loss: 0.332
[21,     1] loss: 0.330
[22,     1] loss: 0.327
[23,     1] loss: 0.325
[24,     1] loss: 0.322
[25,     1] loss: 0.318
[26,     1] loss: 0.314
[27,     1] loss: 0.308
[28,     1] loss: 0.302
[29,     1] loss: 0.295
[30,     1] loss: 0.284
[31,     1] loss: 0.274
[32,     1] loss: 0.260
[33,     1] loss: 0.244
[34,     1] loss: 0.227
[35,     1] loss: 0.208
[36,     1] loss: 0.188
[37,     1] loss: 0.165
[38,     1] loss: 0.142
[39,     1] loss: 0.121
[40,     1] loss: 0.169
[41,     1] loss: 0.429
[42,     1] loss: 0.271
[43,     1] loss: 0.682
[44,     1] loss: 0.403
[45,     1] loss: 0.478
[46,     1] loss: 0.489
[47,     1] loss: 0.508
[48,     1] loss: 0.530
[49,     1] loss: 0.538
[50,     1] loss: 0.537
[51,     1] loss: 0.528
[52,     1] loss: 0.519
[53,     1] loss: 0.509
[54,     1] loss: 0.500
[55,     1] loss: 0.489
[56,     1] loss: 0.482
[57,     1] loss: 0.470
[58,     1] loss: 0.460
[59,     1] loss: 0.446
[60,     1] loss: 0.432
[61,     1] loss: 0.415
[62,     1] loss: 0.396
[63,     1] loss: 0.374
[64,     1] loss: 0.343
[65,     1] loss: 0.306
[66,     1] loss: 0.262
[67,     1] loss: 0.218
[68,     1] loss: 0.175
[69,     1] loss: 0.142
[70,     1] loss: 0.115
[71,     1] loss: 0.097
[72,     1] loss: 0.086
[73,     1] loss: 0.081
[74,     1] loss: 0.073
[75,     1] loss: 0.071
[76,     1] loss: 0.070
[77,     1] loss: 0.070
[78,     1] loss: 0.078
[79,     1] loss: 0.072
[80,     1] loss: 0.072
[81,     1] loss: 0.082
[82,     1] loss: 0.082
[83,     1] loss: 0.079
[84,     1] loss: 0.086
[85,     1] loss: 0.085
[86,     1] loss: 0.083
[87,     1] loss: 0.083
[88,     1] loss: 0.082
[89,     1] loss: 0.079
[90,     1] loss: 0.079
[91,     1] loss: 0.076
[92,     1] loss: 0.075
[93,     1] loss: 0.072
[94,     1] loss: 0.072
[95,     1] loss: 0.073
[96,     1] loss: 0.072
[97,     1] loss: 0.074
[98,     1] loss: 0.073
[99,     1] loss: 0.073
[100,     1] loss: 0.073
[101,     1] loss: 0.074
[102,     1] loss: 0.074
[103,     1] loss: 0.074
[104,     1] loss: 0.076
[105,     1] loss: 0.076
[106,     1] loss: 0.076
[107,     1] loss: 0.077
[108,     1] loss: 0.077
[109,     1] loss: 0.076
[110,     1] loss: 0.078
[111,     1] loss: 0.079
[112,     1] loss: 0.103
[113,     1] loss: 0.472
[114,     1] loss: 0.780
[115,     1] loss: 0.679
[116,     1] loss: 0.579
[117,     1] loss: 0.567
[118,     1] loss: 0.554
[119,     1] loss: 0.565
[120,     1] loss: 0.575
[121,     1] loss: 0.572
[122,     1] loss: 0.562
[123,     1] loss: 0.552
[124,     1] loss: 0.544
[125,     1] loss: 0.530
[126,     1] loss: 0.516
[127,     1] loss: 0.498
[128,     1] loss: 0.477
[129,     1] loss: 0.452
[130,     1] loss: 0.430
[131,     1] loss: 0.404
[132,     1] loss: 0.388
[133,     1] loss: 0.377
[134,     1] loss: 0.364
[135,     1] loss: 0.376
[136,     1] loss: 0.370
[137,     1] loss: 0.371
[138,     1] loss: 0.360
[139,     1] loss: 0.365
[140,     1] loss: 0.356
[141,     1] loss: 0.358
[142,     1] loss: 0.356
[143,     1] loss: 0.354
[144,     1] loss: 0.354
Early stopping applied (best metric=0.22153054177761078)
Finished Training
Total time taken: 702.2693386077881
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.682
[3,     1] loss: 0.646
[4,     1] loss: 0.610
[5,     1] loss: 0.570
[6,     1] loss: 0.525
[7,     1] loss: 0.475
[8,     1] loss: 0.426
[9,     1] loss: 0.379
[10,     1] loss: 0.330
[11,     1] loss: 0.284
[12,     1] loss: 0.233
[13,     1] loss: 0.208
[14,     1] loss: 0.183
[15,     1] loss: 0.149
[16,     1] loss: 0.130
[17,     1] loss: 0.104
[18,     1] loss: 0.113
[19,     1] loss: 0.078
[20,     1] loss: 0.077
[21,     1] loss: 0.065
[22,     1] loss: 0.058
[23,     1] loss: 0.055
[24,     1] loss: 0.050
[25,     1] loss: 0.047
[26,     1] loss: 0.045
[27,     1] loss: 0.042
[28,     1] loss: 0.041
[29,     1] loss: 0.040
[30,     1] loss: 0.041
[31,     1] loss: 0.041
[32,     1] loss: 0.042
[33,     1] loss: 0.041
[34,     1] loss: 0.042
[35,     1] loss: 0.042
[36,     1] loss: 0.041
[37,     1] loss: 0.042
[38,     1] loss: 0.041
[39,     1] loss: 0.041
[40,     1] loss: 0.041
[41,     1] loss: 0.040
[42,     1] loss: 0.040
[43,     1] loss: 0.039
[44,     1] loss: 0.038
[45,     1] loss: 0.038
[46,     1] loss: 0.038
[47,     1] loss: 0.037
[48,     1] loss: 0.038
[49,     1] loss: 0.037
[50,     1] loss: 0.038
[51,     1] loss: 0.039
[52,     1] loss: 0.082
[53,     1] loss: 0.363
[54,     1] loss: 0.747
[55,     1] loss: 0.430
[56,     1] loss: 0.468
[57,     1] loss: 0.452
[58,     1] loss: 0.462
[59,     1] loss: 0.476
[60,     1] loss: 0.479
[61,     1] loss: 0.472
[62,     1] loss: 0.456
[63,     1] loss: 0.441
Early stopping applied (best metric=0.22933027148246765)
Finished Training
Total time taken: 307.6735665798187
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8767741935483871, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.9, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8725190839694656, 'Pyrrolidone carboxylic acid Validation Precision': 0.5678240917600883, 'Pyrrolidone carboxylic acid AUC ROC': 0.9489503816793893, 'Pyrrolidone carboxylic acid AUC PR': 0.7650789692297587, 'Pyrrolidone carboxylic acid MCC': 0.6499600610090309, 'Pyrrolidone carboxylic acid F1': 0.6950220423989955, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.22236955761909485, 'Validation Loss (total)': 0.22236955761909485}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006643716449457791,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1434557739,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.992555215503266}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.683
[3,     1] loss: 0.658
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005222559630955959,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2016527634,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.390526090278303}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.686
[3,     1] loss: 0.655
[4,     1] loss: 0.621
[5,     1] loss: 0.582
[6,     1] loss: 0.545
[7,     1] loss: 0.501
[8,     1] loss: 0.457
[9,     1] loss: 0.407
[10,     1] loss: 0.359
[11,     1] loss: 0.311
[12,     1] loss: 0.265
[13,     1] loss: 0.217
[14,     1] loss: 0.179
[15,     1] loss: 0.140
[16,     1] loss: 0.114
[17,     1] loss: 0.092
[18,     1] loss: 0.072
[19,     1] loss: 0.062
[20,     1] loss: 0.053
[21,     1] loss: 0.047
[22,     1] loss: 0.041
[23,     1] loss: 0.037
[24,     1] loss: 0.034
[25,     1] loss: 0.032
[26,     1] loss: 0.030
[27,     1] loss: 0.028
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007615478649705032,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3002880359,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.270381187052674}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.674
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009504901811856039,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1754375000,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.484467386839086}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.679
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007339388831018132,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2703891725,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.211400942823788}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.660
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009490427873670364,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2585749754,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.31142084381939}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.682
[3,     1] loss: 0.632
[4,     1] loss: 0.574
[5,     1] loss: 0.514
[6,     1] loss: 0.450
[7,     1] loss: 0.387
[8,     1] loss: 0.327
[9,     1] loss: 0.271
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009771380612452273,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1330029440,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.2007927528808438}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.687
[3,     1] loss: 0.590
[4,     1] loss: 0.501
[5,     1] loss: 0.440
[6,     1] loss: 0.389
[7,     1] loss: 0.311
[8,     1] loss: 0.249
[9,     1] loss: 0.205
[10,     1] loss: 0.161
[11,     1] loss: 0.123
[12,     1] loss: 0.084
[13,     1] loss: 0.056
[14,     1] loss: 0.036
[15,     1] loss: 0.023
[16,     1] loss: 0.019
[17,     1] loss: 0.020
[18,     1] loss: 0.014
[19,     1] loss: 0.017
[20,     1] loss: 0.011
[21,     1] loss: 0.008
[22,     1] loss: 0.003
[23,     1] loss: 0.001
[24,     1] loss: 0.001
[25,     1] loss: 0.001
[26,     1] loss: 0.001
[27,     1] loss: 0.001
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007085434638136724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1107938222,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.039957821945064}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.695
[3,     1] loss: 0.667
[4,     1] loss: 0.636
[5,     1] loss: 0.598
[6,     1] loss: 0.551
[7,     1] loss: 0.501
[8,     1] loss: 0.445
[9,     1] loss: 0.383
[10,     1] loss: 0.316
[11,     1] loss: 0.252
[12,     1] loss: 0.199
[13,     1] loss: 0.154
[14,     1] loss: 0.119
[15,     1] loss: 0.092
[16,     1] loss: 0.070
[17,     1] loss: 0.044
[18,     1] loss: 0.031
[19,     1] loss: 0.023
[20,     1] loss: 0.018
[21,     1] loss: 0.015
[22,     1] loss: 0.012
[23,     1] loss: 0.011
[24,     1] loss: 0.011
[25,     1] loss: 0.011
[26,     1] loss: 0.012
[27,     1] loss: 0.012
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00027071282869482375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4180560720,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.56818078016602}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.670
[4,     1] loss: 0.657
[5,     1] loss: 0.646
[6,     1] loss: 0.634
[7,     1] loss: 0.625
[8,     1] loss: 0.617
[9,     1] loss: 0.609
[10,     1] loss: 0.597
[11,     1] loss: 0.593
[12,     1] loss: 0.582
[13,     1] loss: 0.576
[14,     1] loss: 0.567
[15,     1] loss: 0.561
[16,     1] loss: 0.553
[17,     1] loss: 0.546
[18,     1] loss: 0.537
[19,     1] loss: 0.530
[20,     1] loss: 0.522
[21,     1] loss: 0.517
[22,     1] loss: 0.506
[23,     1] loss: 0.502
[24,     1] loss: 0.492
[25,     1] loss: 0.485
[26,     1] loss: 0.480
[27,     1] loss: 0.473
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008164219780569037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 496346770,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.636377322705485}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.694
[3,     1] loss: 0.657
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00837848353680476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1363835975,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.522958271648834}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.661
[3,     1] loss: 0.598
[4,     1] loss: 0.540
[5,     1] loss: 0.483
[6,     1] loss: 0.429
[7,     1] loss: 0.375
[8,     1] loss: 0.321
[9,     1] loss: 0.270
[10,     1] loss: 0.223
[11,     1] loss: 0.191
[12,     1] loss: 0.162
[13,     1] loss: 0.136
[14,     1] loss: 0.115
[15,     1] loss: 0.099
[16,     1] loss: 0.087
[17,     1] loss: 0.076
[18,     1] loss: 0.067
[19,     1] loss: 0.063
[20,     1] loss: 0.057
[21,     1] loss: 0.051
[22,     1] loss: 0.050
[23,     1] loss: 0.044
[24,     1] loss: 0.058
[25,     1] loss: 0.143
[26,     1] loss: 0.237
[27,     1] loss: 0.194
[28,     1] loss: 0.203
[29,     1] loss: 0.200
[30,     1] loss: 0.193
[31,     1] loss: 0.166
[32,     1] loss: 0.134
[33,     1] loss: 0.107
[34,     1] loss: 0.083
[35,     1] loss: 0.068
[36,     1] loss: 0.058
[37,     1] loss: 0.051
[38,     1] loss: 0.046
[39,     1] loss: 0.043
[40,     1] loss: 0.041
[41,     1] loss: 0.043
[42,     1] loss: 0.044
[43,     1] loss: 0.046
[44,     1] loss: 0.046
[45,     1] loss: 0.050
[46,     1] loss: 0.051
[47,     1] loss: 0.052
[48,     1] loss: 0.055
[49,     1] loss: 0.055
[50,     1] loss: 0.053
[51,     1] loss: 0.052
[52,     1] loss: 0.053
[53,     1] loss: 0.049
[54,     1] loss: 0.049
[55,     1] loss: 0.138
[56,     1] loss: 0.104
[57,     1] loss: 0.224
[58,     1] loss: 0.873
[59,     1] loss: 0.442
[60,     1] loss: 0.420
Early stopping applied (best metric=0.2879597842693329)
Finished Training
Total time taken: 281.8541028499603
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.683
[3,     1] loss: 0.642
[4,     1] loss: 0.595
[5,     1] loss: 0.548
[6,     1] loss: 0.503
[7,     1] loss: 0.457
[8,     1] loss: 0.408
[9,     1] loss: 0.361
[10,     1] loss: 0.321
[11,     1] loss: 0.287
[12,     1] loss: 0.255
[13,     1] loss: 0.229
[14,     1] loss: 0.203
[15,     1] loss: 0.182
[16,     1] loss: 0.160
[17,     1] loss: 0.140
[18,     1] loss: 0.120
[19,     1] loss: 0.108
[20,     1] loss: 0.090
[21,     1] loss: 0.083
[22,     1] loss: 0.075
[23,     1] loss: 0.066
[24,     1] loss: 0.063
[25,     1] loss: 0.081
[26,     1] loss: 0.069
[27,     1] loss: 0.165
[28,     1] loss: 0.319
[29,     1] loss: 0.184
[30,     1] loss: 0.244
[31,     1] loss: 0.189
[32,     1] loss: 0.187
[33,     1] loss: 0.185
[34,     1] loss: 0.159
[35,     1] loss: 0.140
[36,     1] loss: 0.124
[37,     1] loss: 0.108
[38,     1] loss: 0.093
[39,     1] loss: 0.082
[40,     1] loss: 0.073
[41,     1] loss: 0.068
[42,     1] loss: 0.065
[43,     1] loss: 0.063
[44,     1] loss: 0.061
[45,     1] loss: 0.060
[46,     1] loss: 0.063
[47,     1] loss: 0.062
[48,     1] loss: 0.065
[49,     1] loss: 0.066
[50,     1] loss: 0.066
[51,     1] loss: 0.068
[52,     1] loss: 0.069
[53,     1] loss: 0.067
[54,     1] loss: 0.067
[55,     1] loss: 0.065
[56,     1] loss: 0.065
[57,     1] loss: 0.064
[58,     1] loss: 0.062
[59,     1] loss: 0.062
[60,     1] loss: 0.066
[61,     1] loss: 0.124
[62,     1] loss: 0.277
[63,     1] loss: 0.814
[64,     1] loss: 0.432
[65,     1] loss: 0.552
[66,     1] loss: 0.520
[67,     1] loss: 0.498
[68,     1] loss: 0.512
[69,     1] loss: 0.523
[70,     1] loss: 0.520
[71,     1] loss: 0.509
[72,     1] loss: 0.492
Early stopping applied (best metric=0.25905534625053406)
Finished Training
Total time taken: 351.24042296409607
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.687
[3,     1] loss: 0.657
[4,     1] loss: 0.624
[5,     1] loss: 0.583
[6,     1] loss: 0.537
[7,     1] loss: 0.485
[8,     1] loss: 0.434
[9,     1] loss: 0.381
[10,     1] loss: 0.321
[11,     1] loss: 0.267
[12,     1] loss: 0.217
[13,     1] loss: 0.179
[14,     1] loss: 0.143
[15,     1] loss: 0.115
[16,     1] loss: 0.096
[17,     1] loss: 0.078
[18,     1] loss: 0.066
[19,     1] loss: 0.058
[20,     1] loss: 0.048
[21,     1] loss: 0.042
[22,     1] loss: 0.040
[23,     1] loss: 0.034
[24,     1] loss: 0.074
[25,     1] loss: 0.085
[26,     1] loss: 0.204
[27,     1] loss: 0.163
[28,     1] loss: 0.108
[29,     1] loss: 0.148
[30,     1] loss: 0.157
[31,     1] loss: 0.146
[32,     1] loss: 0.131
[33,     1] loss: 0.109
[34,     1] loss: 0.089
[35,     1] loss: 0.071
[36,     1] loss: 0.062
[37,     1] loss: 0.052
[38,     1] loss: 0.048
[39,     1] loss: 0.044
[40,     1] loss: 0.042
[41,     1] loss: 0.041
[42,     1] loss: 0.043
[43,     1] loss: 0.044
[44,     1] loss: 0.047
[45,     1] loss: 0.049
[46,     1] loss: 0.051
[47,     1] loss: 0.052
[48,     1] loss: 0.053
[49,     1] loss: 0.053
[50,     1] loss: 0.053
[51,     1] loss: 0.054
[52,     1] loss: 0.053
[53,     1] loss: 0.051
[54,     1] loss: 0.050
[55,     1] loss: 0.049
[56,     1] loss: 0.058
[57,     1] loss: 0.167
[58,     1] loss: 0.767
[59,     1] loss: 0.475
[60,     1] loss: 0.430
[61,     1] loss: 0.435
[62,     1] loss: 0.459
[63,     1] loss: 0.465
[64,     1] loss: 0.458
[65,     1] loss: 0.439
[66,     1] loss: 0.417
[67,     1] loss: 0.391
[68,     1] loss: 0.363
[69,     1] loss: 0.330
[70,     1] loss: 0.293
[71,     1] loss: 0.259
[72,     1] loss: 0.225
[73,     1] loss: 0.194
[74,     1] loss: 0.170
[75,     1] loss: 0.141
[76,     1] loss: 0.121
[77,     1] loss: 0.102
[78,     1] loss: 0.090
[79,     1] loss: 0.080
[80,     1] loss: 0.074
[81,     1] loss: 0.072
[82,     1] loss: 0.067
[83,     1] loss: 0.064
[84,     1] loss: 0.068
Early stopping applied (best metric=0.273497998714447)
Finished Training
Total time taken: 415.9864730834961
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.677
[3,     1] loss: 0.627
[4,     1] loss: 0.575
[5,     1] loss: 0.527
[6,     1] loss: 0.482
[7,     1] loss: 0.440
[8,     1] loss: 0.401
[9,     1] loss: 0.375
[10,     1] loss: 0.357
[11,     1] loss: 0.346
[12,     1] loss: 0.340
[13,     1] loss: 0.333
[14,     1] loss: 0.331
[15,     1] loss: 0.328
[16,     1] loss: 0.327
[17,     1] loss: 0.324
[18,     1] loss: 0.319
[19,     1] loss: 0.315
[20,     1] loss: 0.310
[21,     1] loss: 0.304
[22,     1] loss: 0.296
[23,     1] loss: 0.286
[24,     1] loss: 0.274
[25,     1] loss: 0.259
[26,     1] loss: 0.243
[27,     1] loss: 0.225
[28,     1] loss: 0.206
[29,     1] loss: 0.185
[30,     1] loss: 0.163
[31,     1] loss: 0.139
[32,     1] loss: 0.120
[33,     1] loss: 0.101
[34,     1] loss: 0.083
[35,     1] loss: 0.070
[36,     1] loss: 0.059
[37,     1] loss: 0.052
[38,     1] loss: 0.047
[39,     1] loss: 0.043
[40,     1] loss: 0.041
[41,     1] loss: 0.039
[42,     1] loss: 0.040
[43,     1] loss: 0.040
[44,     1] loss: 0.041
[45,     1] loss: 0.042
[46,     1] loss: 0.043
[47,     1] loss: 0.049
[48,     1] loss: 0.199
[49,     1] loss: 1.325
[50,     1] loss: 0.695
[51,     1] loss: 0.586
[52,     1] loss: 0.605
[53,     1] loss: 0.616
[54,     1] loss: 0.618
[55,     1] loss: 0.616
[56,     1] loss: 0.614
[57,     1] loss: 0.612
[58,     1] loss: 0.610
[59,     1] loss: 0.609
[60,     1] loss: 0.609
[61,     1] loss: 0.609
[62,     1] loss: 0.607
[63,     1] loss: 0.605
[64,     1] loss: 0.604
[65,     1] loss: 0.602
[66,     1] loss: 0.600
[67,     1] loss: 0.596
[68,     1] loss: 0.592
[69,     1] loss: 0.587
[70,     1] loss: 0.582
[71,     1] loss: 0.573
[72,     1] loss: 0.564
[73,     1] loss: 0.552
[74,     1] loss: 0.541
[75,     1] loss: 0.527
[76,     1] loss: 0.516
[77,     1] loss: 0.506
[78,     1] loss: 0.493
[79,     1] loss: 0.481
[80,     1] loss: 0.460
[81,     1] loss: 0.442
[82,     1] loss: 0.429
[83,     1] loss: 0.416
[84,     1] loss: 0.405
[85,     1] loss: 0.398
[86,     1] loss: 0.394
[87,     1] loss: 0.389
[88,     1] loss: 0.385
[89,     1] loss: 0.382
[90,     1] loss: 0.381
[91,     1] loss: 0.380
[92,     1] loss: 0.379
[93,     1] loss: 0.378
[94,     1] loss: 0.378
[95,     1] loss: 0.379
[96,     1] loss: 0.378
Early stopping applied (best metric=0.3160754144191742)
Finished Training
Total time taken: 473.05426383018494
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.654
[4,     1] loss: 0.614
[5,     1] loss: 0.565
[6,     1] loss: 0.510
[7,     1] loss: 0.449
[8,     1] loss: 0.385
[9,     1] loss: 0.320
[10,     1] loss: 0.255
[11,     1] loss: 0.202
[12,     1] loss: 0.160
[13,     1] loss: 0.125
[14,     1] loss: 0.101
[15,     1] loss: 0.078
[16,     1] loss: 0.066
[17,     1] loss: 0.056
[18,     1] loss: 0.050
[19,     1] loss: 0.043
[20,     1] loss: 0.039
[21,     1] loss: 0.044
[22,     1] loss: 0.037
[23,     1] loss: 0.039
[24,     1] loss: 0.044
[25,     1] loss: 0.046
[26,     1] loss: 0.068
[27,     1] loss: 0.209
[28,     1] loss: 0.328
[29,     1] loss: 0.279
[30,     1] loss: 0.233
[31,     1] loss: 0.268
[32,     1] loss: 0.265
[33,     1] loss: 0.225
[34,     1] loss: 0.199
[35,     1] loss: 0.163
[36,     1] loss: 0.128
[37,     1] loss: 0.102
[38,     1] loss: 0.083
[39,     1] loss: 0.068
[40,     1] loss: 0.058
[41,     1] loss: 0.049
[42,     1] loss: 0.044
[43,     1] loss: 0.041
[44,     1] loss: 0.041
[45,     1] loss: 0.042
[46,     1] loss: 0.044
[47,     1] loss: 0.046
[48,     1] loss: 0.048
[49,     1] loss: 0.049
[50,     1] loss: 0.054
[51,     1] loss: 0.054
[52,     1] loss: 0.056
[53,     1] loss: 0.055
[54,     1] loss: 0.053
[55,     1] loss: 0.053
[56,     1] loss: 0.050
[57,     1] loss: 0.050
[58,     1] loss: 0.048
[59,     1] loss: 0.047
[60,     1] loss: 0.047
[61,     1] loss: 0.047
[62,     1] loss: 0.051
[63,     1] loss: 0.098
[64,     1] loss: 0.162
[65,     1] loss: 0.705
[66,     1] loss: 0.381
[67,     1] loss: 0.334
[68,     1] loss: 0.335
[69,     1] loss: 0.337
[70,     1] loss: 0.329
[71,     1] loss: 0.301
[72,     1] loss: 0.280
[73,     1] loss: 0.259
[74,     1] loss: 0.239
[75,     1] loss: 0.218
[76,     1] loss: 0.197
[77,     1] loss: 0.177
[78,     1] loss: 0.156
[79,     1] loss: 0.139
[80,     1] loss: 0.121
[81,     1] loss: 0.109
[82,     1] loss: 0.160
[83,     1] loss: 0.406
[84,     1] loss: 0.347
Early stopping applied (best metric=0.2607939541339874)
Finished Training
Total time taken: 416.8824462890625
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.86, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.8458333333333333, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8625954198473282, 'Pyrrolidone carboxylic acid Validation Precision': 0.5343528308708368, 'Pyrrolidone carboxylic acid AUC ROC': 0.9210003180661578, 'Pyrrolidone carboxylic acid AUC PR': 0.7409346434940858, 'Pyrrolidone carboxylic acid MCC': 0.5964262463324526, 'Pyrrolidone carboxylic acid F1': 0.6533270373280639, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.2794764995574951, 'Validation Loss (total)': 0.2794764995574951}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009836832978720216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2250188239,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.050876180987585}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.697
[3,     1] loss: 0.669
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008260517198920216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2845895354,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.699483782443155}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.669
[3,     1] loss: 0.616
[4,     1] loss: 0.570
[5,     1] loss: 0.519
[6,     1] loss: 0.471
[7,     1] loss: 0.424
[8,     1] loss: 0.382
[9,     1] loss: 0.341
[10,     1] loss: 0.299
[11,     1] loss: 0.262
[12,     1] loss: 0.231
[13,     1] loss: 0.198
[14,     1] loss: 0.170
[15,     1] loss: 0.148
[16,     1] loss: 0.131
[17,     1] loss: 0.116
[18,     1] loss: 0.105
[19,     1] loss: 0.096
[20,     1] loss: 0.085
[21,     1] loss: 0.078
[22,     1] loss: 0.071
[23,     1] loss: 0.067
[24,     1] loss: 0.066
[25,     1] loss: 0.064
[26,     1] loss: 0.062
[27,     1] loss: 0.061
[28,     1] loss: 0.060
[29,     1] loss: 0.060
[30,     1] loss: 0.060
[31,     1] loss: 0.062
[32,     1] loss: 0.060
[33,     1] loss: 0.060
[34,     1] loss: 0.061
[35,     1] loss: 0.060
[36,     1] loss: 0.059
[37,     1] loss: 0.060
[38,     1] loss: 0.058
[39,     1] loss: 0.057
[40,     1] loss: 0.056
[41,     1] loss: 0.058
[42,     1] loss: 0.070
[43,     1] loss: 0.136
[44,     1] loss: 0.476
[45,     1] loss: 0.680
[46,     1] loss: 0.645
[47,     1] loss: 0.574
[48,     1] loss: 0.561
[49,     1] loss: 0.568
[50,     1] loss: 0.574
[51,     1] loss: 0.583
[52,     1] loss: 0.579
[53,     1] loss: 0.576
[54,     1] loss: 0.573
[55,     1] loss: 0.568
[56,     1] loss: 0.559
[57,     1] loss: 0.551
[58,     1] loss: 0.540
[59,     1] loss: 0.518
[60,     1] loss: 0.503
[61,     1] loss: 0.487
[62,     1] loss: 0.466
[63,     1] loss: 0.439
[64,     1] loss: 0.412
Early stopping applied (best metric=0.28026556968688965)
Finished Training
Total time taken: 318.9104220867157
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.675
[3,     1] loss: 0.625
[4,     1] loss: 0.575
[5,     1] loss: 0.527
[6,     1] loss: 0.484
[7,     1] loss: 0.445
[8,     1] loss: 0.406
[9,     1] loss: 0.375
[10,     1] loss: 0.345
[11,     1] loss: 0.335
[12,     1] loss: 0.322
[13,     1] loss: 0.310
[14,     1] loss: 0.299
[15,     1] loss: 0.288
[16,     1] loss: 0.277
[17,     1] loss: 0.263
[18,     1] loss: 0.248
[19,     1] loss: 0.233
[20,     1] loss: 0.215
[21,     1] loss: 0.196
[22,     1] loss: 0.178
[23,     1] loss: 0.155
[24,     1] loss: 0.133
[25,     1] loss: 0.114
[26,     1] loss: 0.099
[27,     1] loss: 0.083
[28,     1] loss: 0.070
[29,     1] loss: 0.060
[30,     1] loss: 0.053
[31,     1] loss: 0.049
[32,     1] loss: 0.045
[33,     1] loss: 0.045
[34,     1] loss: 0.044
[35,     1] loss: 0.044
[36,     1] loss: 0.044
[37,     1] loss: 0.045
[38,     1] loss: 0.047
[39,     1] loss: 0.048
[40,     1] loss: 0.114
[41,     1] loss: 0.323
[42,     1] loss: 0.624
[43,     1] loss: 0.484
[44,     1] loss: 0.501
[45,     1] loss: 0.501
[46,     1] loss: 0.521
[47,     1] loss: 0.548
[48,     1] loss: 0.553
[49,     1] loss: 0.547
[50,     1] loss: 0.537
[51,     1] loss: 0.524
[52,     1] loss: 0.511
[53,     1] loss: 0.503
[54,     1] loss: 0.488
[55,     1] loss: 0.474
[56,     1] loss: 0.459
[57,     1] loss: 0.443
[58,     1] loss: 0.423
[59,     1] loss: 0.406
[60,     1] loss: 0.382
[61,     1] loss: 0.359
[62,     1] loss: 0.335
[63,     1] loss: 0.308
[64,     1] loss: 0.277
[65,     1] loss: 0.251
[66,     1] loss: 0.212
[67,     1] loss: 0.179
[68,     1] loss: 0.170
[69,     1] loss: 0.180
[70,     1] loss: 0.406
[71,     1] loss: 0.291
[72,     1] loss: 0.273
[73,     1] loss: 0.282
[74,     1] loss: 0.255
[75,     1] loss: 0.231
[76,     1] loss: 0.215
[77,     1] loss: 0.192
[78,     1] loss: 0.166
[79,     1] loss: 0.142
[80,     1] loss: 0.129
[81,     1] loss: 0.111
[82,     1] loss: 0.099
[83,     1] loss: 0.090
Early stopping applied (best metric=0.2664256691932678)
Finished Training
Total time taken: 420.14135003089905
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.655
[5,     1] loss: 0.627
[6,     1] loss: 0.597
[7,     1] loss: 0.561
[8,     1] loss: 0.525
[9,     1] loss: 0.490
[10,     1] loss: 0.455
[11,     1] loss: 0.423
[12,     1] loss: 0.389
[13,     1] loss: 0.353
[14,     1] loss: 0.318
[15,     1] loss: 0.285
[16,     1] loss: 0.260
[17,     1] loss: 0.238
[18,     1] loss: 0.215
[19,     1] loss: 0.196
[20,     1] loss: 0.180
[21,     1] loss: 0.159
[22,     1] loss: 0.145
[23,     1] loss: 0.126
[24,     1] loss: 0.116
[25,     1] loss: 0.106
[26,     1] loss: 0.096
[27,     1] loss: 0.087
[28,     1] loss: 0.078
[29,     1] loss: 0.072
[30,     1] loss: 0.068
[31,     1] loss: 0.065
[32,     1] loss: 0.064
[33,     1] loss: 0.063
[34,     1] loss: 0.062
[35,     1] loss: 0.063
[36,     1] loss: 0.062
[37,     1] loss: 0.063
[38,     1] loss: 0.062
[39,     1] loss: 0.065
[40,     1] loss: 0.065
[41,     1] loss: 0.067
[42,     1] loss: 0.066
[43,     1] loss: 0.067
[44,     1] loss: 0.067
[45,     1] loss: 0.068
[46,     1] loss: 0.067
[47,     1] loss: 0.077
[48,     1] loss: 0.139
[49,     1] loss: 0.366
[50,     1] loss: 0.734
[51,     1] loss: 0.620
[52,     1] loss: 0.567
[53,     1] loss: 0.580
[54,     1] loss: 0.568
[55,     1] loss: 0.582
[56,     1] loss: 0.590
[57,     1] loss: 0.587
[58,     1] loss: 0.580
[59,     1] loss: 0.575
[60,     1] loss: 0.568
[61,     1] loss: 0.559
[62,     1] loss: 0.549
[63,     1] loss: 0.535
[64,     1] loss: 0.521
[65,     1] loss: 0.500
[66,     1] loss: 0.476
[67,     1] loss: 0.451
[68,     1] loss: 0.423
Early stopping applied (best metric=0.3140828609466553)
Finished Training
Total time taken: 368.04861068725586
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.698
[3,     1] loss: 0.679
[4,     1] loss: 0.661
[5,     1] loss: 0.639
[6,     1] loss: 0.612
[7,     1] loss: 0.584
[8,     1] loss: 0.554
[9,     1] loss: 0.524
[10,     1] loss: 0.492
[11,     1] loss: 0.458
[12,     1] loss: 0.426
[13,     1] loss: 0.399
[14,     1] loss: 0.376
[15,     1] loss: 0.353
[16,     1] loss: 0.331
[17,     1] loss: 0.311
[18,     1] loss: 0.286
[19,     1] loss: 0.265
[20,     1] loss: 0.243
[21,     1] loss: 0.220
[22,     1] loss: 0.197
[23,     1] loss: 0.177
[24,     1] loss: 0.158
[25,     1] loss: 0.137
[26,     1] loss: 0.121
[27,     1] loss: 0.108
[28,     1] loss: 0.100
[29,     1] loss: 0.154
[30,     1] loss: 0.231
[31,     1] loss: 0.277
[32,     1] loss: 0.586
[33,     1] loss: 0.568
[34,     1] loss: 0.513
[35,     1] loss: 0.455
[36,     1] loss: 0.454
[37,     1] loss: 0.459
[38,     1] loss: 0.452
[39,     1] loss: 0.440
[40,     1] loss: 0.419
[41,     1] loss: 0.397
[42,     1] loss: 0.376
[43,     1] loss: 0.352
[44,     1] loss: 0.326
[45,     1] loss: 0.298
[46,     1] loss: 0.274
[47,     1] loss: 0.250
[48,     1] loss: 0.225
[49,     1] loss: 0.207
[50,     1] loss: 0.187
[51,     1] loss: 0.171
[52,     1] loss: 0.157
[53,     1] loss: 0.142
[54,     1] loss: 0.134
[55,     1] loss: 0.123
[56,     1] loss: 0.117
[57,     1] loss: 0.112
[58,     1] loss: 0.112
[59,     1] loss: 0.108
[60,     1] loss: 0.108
[61,     1] loss: 0.104
[62,     1] loss: 0.106
[63,     1] loss: 0.107
[64,     1] loss: 0.106
[65,     1] loss: 0.106
[66,     1] loss: 0.106
[67,     1] loss: 0.106
[68,     1] loss: 0.107
[69,     1] loss: 0.108
[70,     1] loss: 0.108
[71,     1] loss: 0.110
[72,     1] loss: 0.120
[73,     1] loss: 0.399
[74,     1] loss: 0.574
[75,     1] loss: 0.485
[76,     1] loss: 0.500
[77,     1] loss: 0.431
[78,     1] loss: 0.453
Early stopping applied (best metric=0.29188695549964905)
Finished Training
Total time taken: 407.2829222679138
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.681
[3,     1] loss: 0.638
[4,     1] loss: 0.601
[5,     1] loss: 0.560
[6,     1] loss: 0.515
[7,     1] loss: 0.474
[8,     1] loss: 0.429
[9,     1] loss: 0.389
[10,     1] loss: 0.350
[11,     1] loss: 0.310
[12,     1] loss: 0.275
[13,     1] loss: 0.236
[14,     1] loss: 0.200
[15,     1] loss: 0.169
[16,     1] loss: 0.144
[17,     1] loss: 0.126
[18,     1] loss: 0.106
[19,     1] loss: 0.093
[20,     1] loss: 0.082
[21,     1] loss: 0.075
[22,     1] loss: 0.068
[23,     1] loss: 0.063
[24,     1] loss: 0.059
[25,     1] loss: 0.058
[26,     1] loss: 0.056
[27,     1] loss: 0.056
[28,     1] loss: 0.056
[29,     1] loss: 0.056
[30,     1] loss: 0.057
[31,     1] loss: 0.057
[32,     1] loss: 0.058
[33,     1] loss: 0.058
[34,     1] loss: 0.058
[35,     1] loss: 0.057
[36,     1] loss: 0.057
[37,     1] loss: 0.057
[38,     1] loss: 0.056
[39,     1] loss: 0.054
[40,     1] loss: 0.054
[41,     1] loss: 0.053
[42,     1] loss: 0.060
[43,     1] loss: 0.119
[44,     1] loss: 0.470
[45,     1] loss: 0.773
[46,     1] loss: 0.464
[47,     1] loss: 0.485
[48,     1] loss: 0.488
[49,     1] loss: 0.499
[50,     1] loss: 0.509
[51,     1] loss: 0.510
[52,     1] loss: 0.504
[53,     1] loss: 0.493
[54,     1] loss: 0.486
[55,     1] loss: 0.478
[56,     1] loss: 0.468
[57,     1] loss: 0.457
[58,     1] loss: 0.447
[59,     1] loss: 0.438
[60,     1] loss: 0.427
[61,     1] loss: 0.420
[62,     1] loss: 0.413
[63,     1] loss: 0.407
[64,     1] loss: 0.403
[65,     1] loss: 0.396
[66,     1] loss: 0.392
[67,     1] loss: 0.386
[68,     1] loss: 0.380
[69,     1] loss: 0.378
[70,     1] loss: 0.375
[71,     1] loss: 0.372
[72,     1] loss: 0.369
Early stopping applied (best metric=0.28855955600738525)
Finished Training
Total time taken: 352.4139950275421
{'Pyrrolidone carboxylic acid Validation Accuracy': 0.8154838709677419, 'Pyrrolidone carboxylic acid Validation Sensitivity': 0.875, 'Pyrrolidone carboxylic acid Validation Specificity': 0.8045801526717558, 'Pyrrolidone carboxylic acid Validation Precision': 0.4600617777602749, 'Pyrrolidone carboxylic acid AUC ROC': 0.9114503816793893, 'Pyrrolidone carboxylic acid AUC PR': 0.6568170662694607, 'Pyrrolidone carboxylic acid MCC': 0.5417691671094864, 'Pyrrolidone carboxylic acid F1': 0.5984660311920676, 'Validation Loss (Pyrrolidone carboxylic acid)': 0.28824412226676943, 'Validation Loss (total)': 0.28824412226676943}
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0076767682686957875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4024813172,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.413646846503042}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.649
[4,     1] loss: 0.621
[5,     1] loss: 0.591
[6,     1] loss: 0.558
[7,     1] loss: 0.526
[8,     1] loss: 0.494
[9,     1] loss: 0.468
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007238584562125046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3091430297,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.93356671653815}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.680
[3,     1] loss: 0.651
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006080017695224609,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2494075344,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.668004392226312}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.683
[3,     1] loss: 0.646
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009463971681291726,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 216479163,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.33854343671821}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.687
[3,     1] loss: 0.627
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005513587256296973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 489376896,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.594100964180294}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.684
[3,     1] loss: 0.639
[4,     1] loss: 0.589
[5,     1] loss: 0.530
[6,     1] loss: 0.471
[7,     1] loss: 0.427
[8,     1] loss: 0.388
[9,     1] loss: 0.367
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006779037222065146,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 230703570,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.334268673263194}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.674
[3,     1] loss: 0.617
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007826044912136407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 922088495,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.32036609864381}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.655
{'CNNType': 'Adapt',
 'CV_Repeats': 5,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Pyrrolidone carboxylic acid'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007988552511427747,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3145685007,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.735204138552211}
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.680
[3,     1] loss: 0.634
[4,     1] loss: 0.590
[5,     1] loss: 0.538
[6,     1] loss: 0.486
[7,     1] loss: 0.436
[8,     1] loss: 0.389
[9,     1] loss: 0.344
[10,     1] loss: 0.299
[11,     1] loss: 0.249
[12,     1] loss: 0.218
[13,     1] loss: 0.196
[14,     1] loss: 0.151
[15,     1] loss: 0.124
[16,     1] loss: 0.108
[17,     1] loss: 0.093
[18,     1] loss: 0.079
[19,     1] loss: 0.066
[20,     1] loss: 0.059
[21,     1] loss: 0.053
[22,     1] loss: 0.049
[23,     1] loss: 0.044
[24,     1] loss: 0.041
[25,     1] loss: 0.041
[26,     1] loss: 0.040
[27,     1] loss: 0.040
[28,     1] loss: 0.041
[29,     1] loss: 0.040
[30,     1] loss: 0.040
[31,     1] loss: 0.041
[32,     1] loss: 0.042
[33,     1] loss: 0.041
[34,     1] loss: 0.041
[35,     1] loss: 0.040
[36,     1] loss: 0.039
[37,     1] loss: 0.039
[38,     1] loss: 0.039
[39,     1] loss: 0.038
[40,     1] loss: 0.036
[41,     1] loss: 0.036
[42,     1] loss: 0.036
[43,     1] loss: 0.036
[44,     1] loss: 0.036
[45,     1] loss: 0.037
[46,     1] loss: 0.035
[47,     1] loss: 0.036
[48,     1] loss: 0.036
[49,     1] loss: 0.035
[50,     1] loss: 0.056
[51,     1] loss: 0.238
[52,     1] loss: 1.382
[53,     1] loss: 0.709
[54,     1] loss: 0.594
[55,     1] loss: 0.613
[56,     1] loss: 0.625
[57,     1] loss: 0.629
[58,     1] loss: 0.630
[59,     1] loss: 0.629
[60,     1] loss: 0.627
[61,     1] loss: 0.624
[62,     1] loss: 0.618
[63,     1] loss: 0.614
[64,     1] loss: 0.606
[65,     1] loss: 0.598
[66,     1] loss: 0.587
[67,     1] loss: 0.576
Early stopping applied (best metric=0.2488953024148941)
Finished Training
Total time taken: 338.823166847229
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.696
[3,     1] loss: 0.675
[4,     1] loss: 0.652
[5,     1] loss: 0.625
[6,     1] loss: 0.592
[7,     1] loss: 0.555
[8,     1] loss: 0.513
[9,     1] loss: 0.466
[10,     1] loss: 0.415
[11,     1] loss: 0.367
[12,     1] loss: 0.317
[13,     1] loss: 0.269
[14,     1] loss: 0.226
[15,     1] loss: 0.186
[16,     1] loss: 0.152
[17,     1] loss: 0.123
[18,     1] loss: 0.119
[19,     1] loss: 0.125
[20,     1] loss: 0.086
[21,     1] loss: 0.088
[22,     1] loss: 0.161
[23,     1] loss: 0.098
[24,     1] loss: 0.093
[25,     1] loss: 0.099
[26,     1] loss: 0.091
[27,     1] loss: 0.079
[28,     1] loss: 0.072
[29,     1] loss: 0.061
[30,     1] loss: 0.055
[31,     1] loss: 0.051
[32,     1] loss: 0.050
[33,     1] loss: 0.047
[34,     1] loss: 0.046
[35,     1] loss: 0.046
[36,     1] loss: 0.045
[37,     1] loss: 0.046
[38,     1] loss: 0.046
[39,     1] loss: 0.047
[40,     1] loss: 0.048
[41,     1] loss: 0.049
[42,     1] loss: 0.049
[43,     1] loss: 0.049
[44,     1] loss: 0.048
[45,     1] loss: 0.048
[46,     1] loss: 0.048
[47,     1] loss: 0.048
[48,     1] loss: 0.047
[49,     1] loss: 0.045
[50,     1] loss: 0.045
[51,     1] loss: 0.044
[52,     1] loss: 0.044
[53,     1] loss: 0.045
[54,     1] loss: 0.049
[55,     1] loss: 0.099
[56,     1] loss: 0.481
[57,     1] loss: 0.636
[58,     1] loss: 0.519
[59,     1] loss: 0.483
[60,     1] loss: 0.490
[61,     1] loss: 0.479
[62,     1] loss: 0.466
[63,     1] loss: 0.461
[64,     1] loss: 0.449
[65,     1] loss: 0.432
[66,     1] loss: 0.410
[67,     1] loss: 0.371
[68,     1] loss: 0.337
[69,     1] loss: 0.302
[70,     1] loss: 0.259
[71,     1] loss: 0.223
[72,     1] loss: 0.200
[73,     1] loss: 0.164
[74,     1] loss: 0.138
[75,     1] loss: 0.121
Early stopping applied (best metric=0.2470470815896988)
Finished Training
Total time taken: 378.42739486694336
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.690
[3,     1] loss: 0.661
[4,     1] loss: 0.625
[5,     1] loss: 0.585
[6,     1] loss: 0.541
[7,     1] loss: 0.497
[8,     1] loss: 0.453
[9,     1] loss: 0.403
[10,     1] loss: 0.359
[11,     1] loss: 0.316
[12,     1] loss: 0.270
[13,     1] loss: 0.229
[14,     1] loss: 0.193
[15,     1] loss: 0.160
[16,     1] loss: 0.135
[17,     1] loss: 0.113
[18,     1] loss: 0.096
[19,     1] loss: 0.082
[20,     1] loss: 0.072
[21,     1] loss: 0.062
[22,     1] loss: 0.055
[23,     1] loss: 0.051
[24,     1] loss: 0.049
[25,     1] loss: 0.048
[26,     1] loss: 0.046
[27,     1] loss: 0.046
[28,     1] loss: 0.045
[29,     1] loss: 0.044
[30,     1] loss: 0.046
[31,     1] loss: 0.045
[32,     1] loss: 0.046
[33,     1] loss: 0.048
[34,     1] loss: 0.047
[35,     1] loss: 0.047
[36,     1] loss: 0.046
[37,     1] loss: 0.047
[38,     1] loss: 0.046
[39,     1] loss: 0.044
[40,     1] loss: 0.044
[41,     1] loss: 0.044
[42,     1] loss: 0.044
[43,     1] loss: 0.043
[44,     1] loss: 0.043
[45,     1] loss: 0.042
[46,     1] loss: 0.059
[47,     1] loss: 0.131
[48,     1] loss: 1.004
[49,     1] loss: 0.452
[50,     1] loss: 0.525
[51,     1] loss: 0.595
[52,     1] loss: 0.559
[53,     1] loss: 0.580
[54,     1] loss: 0.589
[55,     1] loss: 0.587
[56,     1] loss: 0.583
[57,     1] loss: 0.582
[58,     1] loss: 0.578
[59,     1] loss: 0.570
[60,     1] loss: 0.562
[61,     1] loss: 0.549
[62,     1] loss: 0.535
[63,     1] loss: 0.517
[64,     1] loss: 0.490
[65,     1] loss: 0.467
[66,     1] loss: 0.438
[67,     1] loss: 0.416
Early stopping applied (best metric=0.25762006640434265)
Finished Training
Total time taken: 339.009685754776
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.676
[3,     1] loss: 0.619
[4,     1] loss: 0.563
[5,     1] loss: 0.504
[6,     1] loss: 0.458
[7,     1] loss: 0.422
[8,     1] loss: 0.391
[9,     1] loss: 0.362
[10,     1] loss: 0.335
[11,     1] loss: 0.314
[12,     1] loss: 0.295
[13,     1] loss: 0.274
[14,     1] loss: 0.252
[15,     1] loss: 0.232
[16,     1] loss: 0.208
[17,     1] loss: 0.185
[18,     1] loss: 0.161
[19,     1] loss: 0.139
[20,     1] loss: 0.117
[21,     1] loss: 0.097
[22,     1] loss: 0.080
[23,     1] loss: 0.066
[24,     1] loss: 0.056
[25,     1] loss: 0.049
[26,     1] loss: 0.041
[27,     1] loss: 0.036
[28,     1] loss: 0.033
[29,     1] loss: 0.032
[30,     1] loss: 0.031
[31,     1] loss: 0.030
[32,     1] loss: 0.031
[33,     1] loss: 0.032
[34,     1] loss: 0.032
[35,     1] loss: 0.035
[36,     1] loss: 0.035
[37,     1] loss: 0.037
[38,     1] loss: 0.036
[39,     1] loss: 0.037
[40,     1] loss: 0.037
[41,     1] loss: 0.038
[42,     1] loss: 0.036
[43,     1] loss: 0.037
[44,     1] loss: 0.036
[45,     1] loss: 0.036
[46,     1] loss: 0.034
[47,     1] loss: 0.033
[48,     1] loss: 0.033
[49,     1] loss: 0.033
[50,     1] loss: 0.033
[51,     1] loss: 0.033
[52,     1] loss: 0.093
[53,     1] loss: 0.448
[54,     1] loss: 1.118
[55,     1] loss: 0.673
[56,     1] loss: 0.624
[57,     1] loss: 0.655
[58,     1] loss: 0.659
[59,     1] loss: 0.653
[60,     1] loss: 0.652
[61,     1] loss: 0.651
[62,     1] loss: 0.651
[63,     1] loss: 0.650
[64,     1] loss: 0.650
[65,     1] loss: 0.648
[66,     1] loss: 0.643
[67,     1] loss: 0.639
[68,     1] loss: 0.633
Early stopping applied (best metric=0.25333160161972046)
Finished Training
Total time taken: 344.30381441116333
(237, 33, 1024)
(1306, 33, 1024)
Loaded folder code/Thesis/dataset/train/Pyrrolidone carboxylic acid/embeddings (1543 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.694
[3,     1] loss: 0.653
[4,     1] loss: 0.607
[5,     1] loss: 0.559
[6,     1] loss: 0.513
[7,     1] loss: 0.468
[8,     1] loss: 0.419
[9,     1] loss: 0.378
[10,     1] loss: 0.346
[11,     1] loss: 0.321
[12,     1] loss: 0.301
[13,     1] loss: 0.288
[14,     1] loss: 0.271
[15,     1] loss: 0.258
[16,     1] loss: 0.238
[17,     1] loss: 0.217
[18,     1] loss: 0.197
[19,     1] loss: 0.179
[20,     1] loss: 0.158
[21,     1] loss: 0.138
[22,     1] loss: 0.113
[23,     1] loss: 0.097
[24,     1] loss: 0.081
[25,     1] loss: 0.070
[26,     1] loss: 0.058
[27,     1] loss: 0.049
[28,     1] loss: 0.042
[29,     1] loss: 0.039
[30,     1] loss: 0.036
[31,     1] loss: 0.033
[32,     1] loss: 0.033
[33,     1] loss: 0.031
[34,     1] loss: 0.033
[35,     1] loss: 0.033
[36,     1] loss: 0.034
[37,     1] loss: 0.035
[38,     1] loss: 0.036
[39,     1] loss: 0.037
[40,     1] loss: 0.038
[41,     1] loss: 0.039
[42,     1] loss: 0.038
[43,     1] loss: 0.038
[44,     1] loss: 0.038
[45,     1] loss: 0.037
[46,     1] loss: 0.037
[47,     1] loss: 0.037
[48,     1] loss: 0.037
[49,     1] loss: 0.036
[50,     1] loss: 0.034
[51,     1] loss: 0.034
[52,     1] loss: 0.066
[53,     1] loss: 0.145
[54,     1] loss: 1.172
[55,     1] loss: 0.635
[56,     1] loss: 0.608
[57,     1] loss: 0.518
[58,     1] loss: 0.560
[59,     1] loss: 0.590
[60,     1] loss: 0.603
[61,     1] loss: 0.604
[62,     1] loss: 0.595
[63,     1] loss: 0.586
[64,     1] loss: 0.571
[65,     1] loss: 0.560
[66,     1] loss: 0.545
[67,     1] loss: 0.526
[68,     1] loss: 0.506
[69,     1] loss: 0.492
[70,     1] loss: 0.469
[71,     1] loss: 0.449
[72,     1] loss: 0.429
[73,     1] loss: 0.410
[74,     1] loss: 0.399
[75,     1] loss: 0.389
[76,     1] loss: 0.382
[77,     1] loss: 0.377
[78,     1] loss: 0.373
[79,     1] loss: 0.370
[80,     1] loss: 0.369
[81,     1] loss: 0.368
[82,     1] loss: 0.368
[83,     1] loss: 0.367
[84,     1] loss: 0.367
[85,     1] loss: 0.367
[86,     1] loss: 0.367
[87,     1] loss: 0.366
[88,     1] loss: 0.366
[89,     1] loss: 0.365
[90,     1] loss: 0.366
[91,     1] loss: 0.366
[92,     1] loss: 0.365
[93,     1] loss: 0.363
[94,     1] loss: 0.363
[95,     1] loss: 0.362
[96,     1] loss: 0.362
[97,     1] loss: 0.362
[98,     1] loss: 0.360
[99,     1] loss: 0.360
Early stopping applied (best metric=0.29407137632369995)
Finished Training
Total time taken: 504.56891918182373
{'CNNType': 'Adapt',
 'CV_Repeats': 1,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007428693396424604,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3562052654,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.210794054266945}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,     8] loss: 0.601
[3,     8] loss: 0.541
[4,     8] loss: 0.492
[5,     8] loss: 0.418
[6,     8] loss: 0.436
[7,     8] loss: 0.391
[8,     8] loss: 0.357
[9,     8] loss: 0.452
[10,     8] loss: 0.338
[11,     8] loss: 0.440
[12,     8] loss: 0.600
[13,     8] loss: 0.606
[14,     8] loss: 0.551
[15,     8] loss: 0.516
[16,     8] loss: 0.610
[17,     8] loss: 0.585
[18,     8] loss: 0.534
[19,     8] loss: 0.528
[20,     8] loss: 0.534
[21,     8] loss: 0.490
[22,     8] loss: 0.511
[23,     8] loss: 0.522
[24,     8] loss: 0.471
[25,     8] loss: 0.513
[26,     8] loss: 0.572
[27,     8] loss: 0.522
[28,     8] loss: 0.468
[29,     8] loss: 0.534
[30,     8] loss: 0.527
[31,     8] loss: 0.517
[32,     8] loss: 0.583
[33,     8] loss: 0.508
[34,     8] loss: 0.473
[35,     8] loss: 0.516
[36,     8] loss: 0.579
[37,     8] loss: 0.507
[38,     8] loss: 0.467
[39,     8] loss: 0.523
[40,     8] loss: 0.512
[41,     8] loss: 0.467
[42,     8] loss: 0.521
[43,     8] loss: 0.526
[44,     8] loss: 0.495
[45,     8] loss: 0.478
[46,     8] loss: 0.476
[47,     8] loss: 0.506
[48,     8] loss: 0.474
[49,     8] loss: 0.482
[50,     8] loss: 0.485
[51,     8] loss: 0.454
[52,     8] loss: 0.491
Early stopping applied (best metric=0.48658138513565063)
Finished Training
Total time taken: 300.4854221343994
{'S-palmitoylation-C Validation Accuracy': 0.6992063492063492, 'S-palmitoylation-C Validation Sensitivity': 0.6435643564356436, 'S-palmitoylation-C Validation Specificity': 0.713151364764268, 'S-palmitoylation-C Validation Precision': 0.3599114064230343, 'S-palmitoylation-C AUC ROC': 0.7380134142446504, 'S-palmitoylation-C AUC PR': 0.4048620722697306, 'S-palmitoylation-C MCC': 0.2977875791658431, 'S-palmitoylation-C F1': 0.46164772727272724, 'Validation Loss (S-palmitoylation-C)': 0.48658138513565063, 'Validation Loss (total)': 0.48658138513565063}
{'CNNType': 'Adapt',
 'CV_Repeats': 1,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006187784549230836,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1181879289,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.410837456592567}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.705
[2,     8] loss: 0.664
[3,     8] loss: 0.634
[4,     8] loss: 0.596
[5,     8] loss: 0.546
[6,     8] loss: 0.475
[7,     8] loss: 0.390
[8,     8] loss: 0.295
[9,     8] loss: 0.218
[10,     8] loss: 0.146
[11,     8] loss: 0.101
[12,     8] loss: 0.077
[13,     8] loss: 0.068
[14,     8] loss: 0.079
[15,     8] loss: 0.074
[16,     8] loss: 0.056
[17,     8] loss: 0.038
[18,     8] loss: 0.026
[19,     8] loss: 0.022
[20,     8] loss: 0.018
[21,     8] loss: 0.016
[22,     8] loss: 0.015
[23,     8] loss: 0.015
[24,     8] loss: 0.015
[25,     8] loss: 0.014
[26,     8] loss: 0.014
[27,     8] loss: 0.014
[28,     8] loss: 0.013
[29,     8] loss: 0.013
[30,     8] loss: 0.013
[31,     8] loss: 0.012
[32,     8] loss: 0.012
[33,     8] loss: 0.011
[34,     8] loss: 0.012
[35,     8] loss: 0.011
[36,     8] loss: 0.011
[37,     8] loss: 0.011
[38,     8] loss: 0.011
[39,     8] loss: 0.011
[40,     8] loss: 0.011
[41,     8] loss: 0.010
[42,     8] loss: 0.010
[43,     8] loss: 0.010
[44,     8] loss: 0.010
[45,     8] loss: 0.010
[46,     8] loss: 0.010
[47,     8] loss: 0.010
[48,     8] loss: 0.010
[49,     8] loss: 0.009
[50,     8] loss: 0.009
[51,     8] loss: 0.009
[52,     8] loss: 0.009
[53,     8] loss: 0.607
[54,     8] loss: 0.791
[55,     8] loss: 0.675
[56,     8] loss: 0.677
Early stopping applied (best metric=0.4939483106136322)
Finished Training
Total time taken: 324.47076320648193
{'S-palmitoylation-C Validation Accuracy': 0.6492063492063492, 'S-palmitoylation-C Validation Sensitivity': 0.6376237623762376, 'S-palmitoylation-C Validation Specificity': 0.6521091811414392, 'S-palmitoylation-C Validation Precision': 0.3147605083088954, 'S-palmitoylation-C AUC ROC': 0.7227015207724247, 'S-palmitoylation-C AUC PR': 0.4055676962468532, 'S-palmitoylation-C MCC': 0.236174158101726, 'S-palmitoylation-C F1': 0.42146596858638746, 'Validation Loss (S-palmitoylation-C)': 0.4939483106136322, 'Validation Loss (total)': 0.4939483106136322}
{'CNNType': 'Adapt',
 'CV_Repeats': 1,
 'Experiment Name': 'Model architecture - sampling method - local: ',
 'FCType': 'Musite',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': False,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['balanced'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004853930056954993,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2048901079,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.11752944606953919}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.704
[2,     8] loss: 0.577
[3,     8] loss: 0.467
[4,     8] loss: 0.314
[5,     8] loss: 0.145
[6,     8] loss: 0.087
[7,     8] loss: 0.071
[8,     8] loss: 0.073
[9,     8] loss: 0.067
[10,     8] loss: 0.046
[11,     8] loss: 0.030
[12,     8] loss: 0.025
[13,     8] loss: 0.014
[14,     8] loss: 0.014
[15,     8] loss: 0.012
[16,     8] loss: 0.022
[17,     8] loss: 0.014
[18,     8] loss: 0.015
[19,     8] loss: 0.015
