{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008979822218257183,
 'learning_rate_Hydroxylation-K': 0.0020593818437950715,
 'learning_rate_Hydroxylation-P': 0.005202725507722085,
 'log_base': 2.5131766511937066,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1080212687,
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4725964202194497,
 'weight_decay_Hydroxylation-K': 1.9048119302053146,
 'weight_decay_Hydroxylation-P': 2.346706990130106}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.098
[2,     1] loss: 1296.794
[3,     1] loss: 1297.347
[4,     1] loss: 1289.772
[5,     1] loss: 1290.308
[6,     1] loss: 1290.965
[7,     1] loss: 1290.044
[8,     1] loss: 1289.639
[9,     1] loss: 1286.374
[10,     1] loss: 1285.847
[11,     1] loss: 1283.228
[12,     1] loss: 1276.413
[13,     1] loss: 1268.168
[14,     1] loss: 1267.118
[15,     1] loss: 1241.692
[16,     1] loss: 1211.391
[17,     1] loss: 1185.006
[18,     1] loss: 1147.492
[19,     1] loss: 1120.903
[20,     1] loss: 1093.669
[21,     1] loss: 1079.334
[22,     1] loss: 1105.155
[23,     1] loss: 1056.211
[24,     1] loss: 1055.070
[25,     1] loss: 1035.605
[26,     1] loss: 1057.248
[27,     1] loss: 1038.123
[28,     1] loss: 1055.967
[29,     1] loss: 1052.807
[30,     1] loss: 1052.653
[31,     1] loss: 1023.970
[32,     1] loss: 966.255
[33,     1] loss: 974.771
[34,     1] loss: 987.804
[35,     1] loss: 983.915
[36,     1] loss: 970.806
[37,     1] loss: 965.820
[38,     1] loss: 998.737
[39,     1] loss: 926.188
[40,     1] loss: 994.137
[41,     1] loss: 972.100
[42,     1] loss: 1000.589
[43,     1] loss: 978.450
[44,     1] loss: 942.968
[45,     1] loss: 1000.891
[46,     1] loss: 934.000
[47,     1] loss: 903.164
[48,     1] loss: 946.250
[49,     1] loss: 902.291
[50,     1] loss: 849.331
[51,     1] loss: 888.880
[52,     1] loss: 923.283
[53,     1] loss: 883.718
[54,     1] loss: 877.687
[55,     1] loss: 892.298
[56,     1] loss: 876.025
[57,     1] loss: 873.153
[58,     1] loss: 904.737
[59,     1] loss: 917.396
[60,     1] loss: 821.829
[61,     1] loss: 889.333
[62,     1] loss: 893.274
[63,     1] loss: 810.548
[64,     1] loss: 848.856
[65,     1] loss: 840.378
[66,     1] loss: 785.763
[67,     1] loss: 779.283
[68,     1] loss: 848.243
[69,     1] loss: 801.887
[70,     1] loss: 769.259
[71,     1] loss: 768.400
[72,     1] loss: 770.535
[73,     1] loss: 744.894
[74,     1] loss: 695.811
[75,     1] loss: 878.265
[76,     1] loss: 858.720
[77,     1] loss: 688.578
[78,     1] loss: 747.682
[79,     1] loss: 698.993
[80,     1] loss: 759.533
[81,     1] loss: 699.815
[82,     1] loss: 651.467
[83,     1] loss: 644.518
[84,     1] loss: 641.153
[85,     1] loss: 731.042
[86,     1] loss: 643.143
[87,     1] loss: 607.545
[88,     1] loss: 560.933
[89,     1] loss: 551.533
[90,     1] loss: 763.522
[91,     1] loss: 1052.626
[92,     1] loss: 598.645
[93,     1] loss: 797.324
[94,     1] loss: 601.535
[95,     1] loss: 685.165
[96,     1] loss: 733.687
[97,     1] loss: 612.674
[98,     1] loss: 673.397
[99,     1] loss: 626.529
[100,     1] loss: 576.512
[101,     1] loss: 604.370
[102,     1] loss: 595.072
[103,     1] loss: 505.246
[104,     1] loss: 527.555
[105,     1] loss: 502.478
[106,     1] loss: 517.124
[107,     1] loss: 458.403
[108,     1] loss: 497.091
[109,     1] loss: 495.370
[110,     1] loss: 567.959
[111,     1] loss: 586.668
[112,     1] loss: 479.032
[113,     1] loss: 541.551
[114,     1] loss: 429.576
[115,     1] loss: 544.268
[116,     1] loss: 443.055
[117,     1] loss: 478.308
[118,     1] loss: 414.272
[119,     1] loss: 444.429
[120,     1] loss: 415.815
[121,     1] loss: 416.560
[122,     1] loss: 428.503
[123,     1] loss: 402.912
[124,     1] loss: 373.092
[125,     1] loss: 403.431
[126,     1] loss: 482.524
[127,     1] loss: 450.631
[128,     1] loss: 385.431
[129,     1] loss: 397.382
[130,     1] loss: 356.213
[131,     1] loss: 334.971
[132,     1] loss: 372.325
[133,     1] loss: 389.954
[134,     1] loss: 385.806
[135,     1] loss: 417.264
[136,     1] loss: 396.625
[137,     1] loss: 393.974
[138,     1] loss: 423.603
[139,     1] loss: 450.994
[140,     1] loss: 385.899
[141,     1] loss: 408.897
[142,     1] loss: 482.365
[143,     1] loss: 394.118
[144,     1] loss: 395.071
[145,     1] loss: 566.330
[146,     1] loss: 383.645
[147,     1] loss: 380.717
[148,     1] loss: 429.662
[149,     1] loss: 344.658
[150,     1] loss: 376.970
[151,     1] loss: 392.210
[152,     1] loss: 360.426
[153,     1] loss: 347.176
[154,     1] loss: 310.645
[155,     1] loss: 325.643
[156,     1] loss: 308.321
[157,     1] loss: 359.934
[158,     1] loss: 387.904
[159,     1] loss: 323.565
[160,     1] loss: 327.419
[161,     1] loss: 306.513
[162,     1] loss: 298.487
[163,     1] loss: 294.936
[164,     1] loss: 293.142
Early stopping applied (best metric=0.33963191509246826)
Finished Training
Total time taken: 40.63819479942322
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.997
[2,     1] loss: 1296.878
[3,     1] loss: 1292.319
[4,     1] loss: 1295.652
[5,     1] loss: 1291.813
[6,     1] loss: 1293.298
[7,     1] loss: 1289.490
[8,     1] loss: 1287.987
[9,     1] loss: 1287.982
[10,     1] loss: 1286.086
[11,     1] loss: 1285.305
[12,     1] loss: 1279.538
[13,     1] loss: 1276.457
[14,     1] loss: 1268.482
[15,     1] loss: 1252.957
[16,     1] loss: 1230.923
[17,     1] loss: 1208.288
[18,     1] loss: 1160.405
[19,     1] loss: 1163.233
[20,     1] loss: 1099.171
[21,     1] loss: 1121.785
[22,     1] loss: 1059.812
[23,     1] loss: 1106.513
[24,     1] loss: 1086.075
[25,     1] loss: 1059.752
[26,     1] loss: 1053.444
[27,     1] loss: 1053.644
[28,     1] loss: 1032.373
[29,     1] loss: 1046.747
[30,     1] loss: 1030.785
[31,     1] loss: 990.436
[32,     1] loss: 1058.123
[33,     1] loss: 1040.480
[34,     1] loss: 1056.256
[35,     1] loss: 991.400
[36,     1] loss: 993.124
[37,     1] loss: 1010.147
[38,     1] loss: 973.414
[39,     1] loss: 1010.506
[40,     1] loss: 961.852
[41,     1] loss: 966.205
[42,     1] loss: 960.530
[43,     1] loss: 1004.366
[44,     1] loss: 972.207
[45,     1] loss: 954.044
[46,     1] loss: 988.091
[47,     1] loss: 936.173
[48,     1] loss: 890.813
[49,     1] loss: 914.687
[50,     1] loss: 939.460
[51,     1] loss: 931.486
[52,     1] loss: 912.786
[53,     1] loss: 872.960
[54,     1] loss: 894.874
[55,     1] loss: 894.436
[56,     1] loss: 899.533
[57,     1] loss: 873.608
[58,     1] loss: 828.449
[59,     1] loss: 875.464
[60,     1] loss: 848.370
[61,     1] loss: 861.909
[62,     1] loss: 792.621
[63,     1] loss: 841.484
[64,     1] loss: 818.897
[65,     1] loss: 773.832
[66,     1] loss: 813.148
[67,     1] loss: 778.039
[68,     1] loss: 789.173
[69,     1] loss: 757.271
[70,     1] loss: 724.405
[71,     1] loss: 761.617
[72,     1] loss: 777.037
[73,     1] loss: 822.911
[74,     1] loss: 827.554
[75,     1] loss: 739.823
[76,     1] loss: 740.042
[77,     1] loss: 792.146
[78,     1] loss: 720.319
[79,     1] loss: 722.323
[80,     1] loss: 667.715
[81,     1] loss: 681.358
[82,     1] loss: 650.160
[83,     1] loss: 707.802
[84,     1] loss: 652.806
[85,     1] loss: 639.722
[86,     1] loss: 562.296
[87,     1] loss: 622.652
[88,     1] loss: 606.203
[89,     1] loss: 618.330
[90,     1] loss: 596.357
[91,     1] loss: 692.070
[92,     1] loss: 730.550
[93,     1] loss: 624.147
[94,     1] loss: 590.886
[95,     1] loss: 585.761
[96,     1] loss: 540.023
[97,     1] loss: 563.333
[98,     1] loss: 535.944
[99,     1] loss: 522.395
[100,     1] loss: 565.590
[101,     1] loss: 551.082
[102,     1] loss: 541.742
Early stopping applied (best metric=0.3920823633670807)
Finished Training
Total time taken: 16.701890468597412
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1291.596
[2,     1] loss: 1297.940
[3,     1] loss: 1291.533
[4,     1] loss: 1291.331
[5,     1] loss: 1296.132
[6,     1] loss: 1292.167
[7,     1] loss: 1286.602
[8,     1] loss: 1289.669
[9,     1] loss: 1290.536
[10,     1] loss: 1293.589
[11,     1] loss: 1290.168
[12,     1] loss: 1290.173
[13,     1] loss: 1289.766
[14,     1] loss: 1289.157
[15,     1] loss: 1288.854
[16,     1] loss: 1290.154
[17,     1] loss: 1289.380
[18,     1] loss: 1290.355
[19,     1] loss: 1288.972
[20,     1] loss: 1291.760
[21,     1] loss: 1290.713
[22,     1] loss: 1289.811
[23,     1] loss: 1289.242
[24,     1] loss: 1288.634
[25,     1] loss: 1289.683
[26,     1] loss: 1288.852
[27,     1] loss: 1290.159
[28,     1] loss: 1289.636
[29,     1] loss: 1290.176
[30,     1] loss: 1290.599
[31,     1] loss: 1290.245
[32,     1] loss: 1290.711
[33,     1] loss: 1288.215
[34,     1] loss: 1289.049
[35,     1] loss: 1289.812
[36,     1] loss: 1287.873
[37,     1] loss: 1287.001
[38,     1] loss: 1287.284
[39,     1] loss: 1286.663
[40,     1] loss: 1287.627
[41,     1] loss: 1284.112
[42,     1] loss: 1280.560
[43,     1] loss: 1272.384
[44,     1] loss: 1257.780
[45,     1] loss: 1248.255
[46,     1] loss: 1217.659
[47,     1] loss: 1191.639
[48,     1] loss: 1180.235
[49,     1] loss: 1135.142
[50,     1] loss: 1104.684
[51,     1] loss: 1152.567
[52,     1] loss: 1071.916
[53,     1] loss: 1050.885
[54,     1] loss: 1073.645
[55,     1] loss: 1088.915
[56,     1] loss: 1083.201
[57,     1] loss: 1055.878
[58,     1] loss: 1070.963
[59,     1] loss: 1001.350
[60,     1] loss: 1090.191
[61,     1] loss: 1023.845
[62,     1] loss: 1036.369
[63,     1] loss: 1039.302
[64,     1] loss: 1027.964
[65,     1] loss: 1028.216
[66,     1] loss: 1023.141
[67,     1] loss: 996.064
[68,     1] loss: 988.325
[69,     1] loss: 942.612
[70,     1] loss: 989.003
[71,     1] loss: 962.569
[72,     1] loss: 963.410
[73,     1] loss: 969.556
[74,     1] loss: 974.425
[75,     1] loss: 904.602
[76,     1] loss: 949.192
[77,     1] loss: 922.690
[78,     1] loss: 886.551
[79,     1] loss: 889.814
[80,     1] loss: 883.909
[81,     1] loss: 905.472
[82,     1] loss: 950.133
[83,     1] loss: 877.238
[84,     1] loss: 886.837
[85,     1] loss: 832.244
[86,     1] loss: 877.769
[87,     1] loss: 856.982
[88,     1] loss: 865.847
[89,     1] loss: 812.358
[90,     1] loss: 774.329
[91,     1] loss: 817.475
[92,     1] loss: 809.829
[93,     1] loss: 896.309
[94,     1] loss: 743.322
[95,     1] loss: 812.089
[96,     1] loss: 869.800
[97,     1] loss: 730.098
[98,     1] loss: 756.979
[99,     1] loss: 738.321
[100,     1] loss: 771.510
[101,     1] loss: 673.944
[102,     1] loss: 797.305
[103,     1] loss: 724.764
[104,     1] loss: 666.488
[105,     1] loss: 725.732
[106,     1] loss: 656.474
[107,     1] loss: 640.795
[108,     1] loss: 649.315
[109,     1] loss: 657.038
[110,     1] loss: 585.788
[111,     1] loss: 669.476
[112,     1] loss: 894.725
[113,     1] loss: 1048.839
[114,     1] loss: 593.906
[115,     1] loss: 971.819
[116,     1] loss: 697.941
[117,     1] loss: 800.896
[118,     1] loss: 814.781
[119,     1] loss: 793.112
[120,     1] loss: 704.223
[121,     1] loss: 807.797
[122,     1] loss: 697.019
[123,     1] loss: 675.927
[124,     1] loss: 669.192
[125,     1] loss: 600.885
[126,     1] loss: 602.729
[127,     1] loss: 687.851
[128,     1] loss: 538.656
[129,     1] loss: 559.798
[130,     1] loss: 559.094
[131,     1] loss: 623.433
[132,     1] loss: 534.136
Early stopping applied (best metric=0.3631404638290405)
Finished Training
Total time taken: 25.664008140563965
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.533
[2,     1] loss: 1288.892
[3,     1] loss: 1303.012
[4,     1] loss: 1292.485
[5,     1] loss: 1292.478
[6,     1] loss: 1287.248
[7,     1] loss: 1287.655
[8,     1] loss: 1291.491
[9,     1] loss: 1291.999
[10,     1] loss: 1290.714
[11,     1] loss: 1287.364
[12,     1] loss: 1287.229
[13,     1] loss: 1282.984
[14,     1] loss: 1283.874
[15,     1] loss: 1276.934
[16,     1] loss: 1274.667
[17,     1] loss: 1264.938
[18,     1] loss: 1249.830
[19,     1] loss: 1216.754
[20,     1] loss: 1179.759
[21,     1] loss: 1152.630
[22,     1] loss: 1139.137
[23,     1] loss: 1117.604
[24,     1] loss: 1073.777
[25,     1] loss: 1069.701
[26,     1] loss: 1072.543
[27,     1] loss: 1049.968
[28,     1] loss: 1077.675
[29,     1] loss: 1056.238
[30,     1] loss: 1053.296
[31,     1] loss: 1017.855
[32,     1] loss: 1023.262
[33,     1] loss: 1038.866
[34,     1] loss: 1053.546
[35,     1] loss: 1060.280
[36,     1] loss: 1024.048
[37,     1] loss: 1005.160
[38,     1] loss: 992.705
[39,     1] loss: 1000.259
[40,     1] loss: 960.074
[41,     1] loss: 968.961
[42,     1] loss: 979.515
[43,     1] loss: 982.570
[44,     1] loss: 980.974
[45,     1] loss: 995.776
[46,     1] loss: 939.478
[47,     1] loss: 954.354
[48,     1] loss: 979.946
[49,     1] loss: 915.925
[50,     1] loss: 896.588
[51,     1] loss: 880.252
[52,     1] loss: 930.190
[53,     1] loss: 870.746
[54,     1] loss: 877.769
[55,     1] loss: 819.763
[56,     1] loss: 924.678
[57,     1] loss: 936.267
[58,     1] loss: 930.217
[59,     1] loss: 879.062
[60,     1] loss: 859.653
[61,     1] loss: 844.652
[62,     1] loss: 840.730
[63,     1] loss: 807.915
[64,     1] loss: 846.922
[65,     1] loss: 838.251
[66,     1] loss: 818.798
[67,     1] loss: 773.952
[68,     1] loss: 767.841
[69,     1] loss: 819.906
[70,     1] loss: 772.178
[71,     1] loss: 783.110
[72,     1] loss: 785.988
[73,     1] loss: 792.092
[74,     1] loss: 776.469
[75,     1] loss: 783.783
[76,     1] loss: 765.959
[77,     1] loss: 806.968
Early stopping applied (best metric=0.40279752016067505)
Finished Training
Total time taken: 13.082257270812988
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1293.562
[2,     1] loss: 1300.260
[3,     1] loss: 1295.510
[4,     1] loss: 1294.770
[5,     1] loss: 1291.536
[6,     1] loss: 1290.295
[7,     1] loss: 1291.477
[8,     1] loss: 1287.597
[9,     1] loss: 1292.738
[10,     1] loss: 1294.177
[11,     1] loss: 1291.019
[12,     1] loss: 1293.371
[13,     1] loss: 1291.323
[14,     1] loss: 1287.259
[15,     1] loss: 1288.153
[16,     1] loss: 1286.481
[17,     1] loss: 1279.003
[18,     1] loss: 1275.045
[19,     1] loss: 1266.401
[20,     1] loss: 1252.954
[21,     1] loss: 1232.181
[22,     1] loss: 1202.644
[23,     1] loss: 1191.728
[24,     1] loss: 1145.406
[25,     1] loss: 1146.924
[26,     1] loss: 1065.354
[27,     1] loss: 1105.847
[28,     1] loss: 1085.237
[29,     1] loss: 1095.971
[30,     1] loss: 1039.879
[31,     1] loss: 1016.168
[32,     1] loss: 1040.015
[33,     1] loss: 1027.918
[34,     1] loss: 1058.634
[35,     1] loss: 1033.559
[36,     1] loss: 1033.859
[37,     1] loss: 1022.065
[38,     1] loss: 984.316
[39,     1] loss: 979.696
[40,     1] loss: 1000.654
[41,     1] loss: 987.151
[42,     1] loss: 978.362
[43,     1] loss: 976.636
[44,     1] loss: 943.262
[45,     1] loss: 936.872
[46,     1] loss: 988.087
[47,     1] loss: 944.229
[48,     1] loss: 953.531
[49,     1] loss: 958.033
[50,     1] loss: 924.922
[51,     1] loss: 964.063
[52,     1] loss: 919.233
[53,     1] loss: 873.318
[54,     1] loss: 895.258
[55,     1] loss: 881.755
[56,     1] loss: 863.165
[57,     1] loss: 876.463
[58,     1] loss: 966.092
[59,     1] loss: 827.839
[60,     1] loss: 897.860
[61,     1] loss: 831.200
[62,     1] loss: 889.261
[63,     1] loss: 839.773
[64,     1] loss: 819.668
[65,     1] loss: 813.394
[66,     1] loss: 822.362
[67,     1] loss: 832.302
[68,     1] loss: 822.462
[69,     1] loss: 831.190
[70,     1] loss: 893.262
[71,     1] loss: 807.481
[72,     1] loss: 768.582
[73,     1] loss: 755.446
[74,     1] loss: 806.114
[75,     1] loss: 731.633
[76,     1] loss: 786.637
[77,     1] loss: 776.558
[78,     1] loss: 765.558
[79,     1] loss: 795.891
[80,     1] loss: 727.509
[81,     1] loss: 766.465
[82,     1] loss: 813.725
[83,     1] loss: 710.345
[84,     1] loss: 786.972
[85,     1] loss: 670.773
[86,     1] loss: 750.297
[87,     1] loss: 663.959
[88,     1] loss: 692.764
[89,     1] loss: 675.896
[90,     1] loss: 674.582
[91,     1] loss: 688.981
[92,     1] loss: 660.443
[93,     1] loss: 648.348
[94,     1] loss: 661.454
[95,     1] loss: 608.892
[96,     1] loss: 559.155
[97,     1] loss: 601.675
[98,     1] loss: 615.946
[99,     1] loss: 616.284
Early stopping applied (best metric=0.35287922620773315)
Finished Training
Total time taken: 16.476754665374756
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.492
[2,     1] loss: 1292.300
[3,     1] loss: 1289.529
[4,     1] loss: 1288.390
[5,     1] loss: 1292.475
[6,     1] loss: 1287.796
[7,     1] loss: 1292.468
[8,     1] loss: 1290.155
[9,     1] loss: 1290.356
[10,     1] loss: 1287.047
[11,     1] loss: 1283.993
[12,     1] loss: 1284.664
[13,     1] loss: 1286.782
[14,     1] loss: 1279.559
[15,     1] loss: 1274.860
[16,     1] loss: 1273.916
[17,     1] loss: 1253.294
[18,     1] loss: 1243.360
[19,     1] loss: 1213.539
[20,     1] loss: 1178.665
[21,     1] loss: 1109.573
[22,     1] loss: 1110.536
[23,     1] loss: 1103.591
[24,     1] loss: 1076.549
[25,     1] loss: 1060.437
[26,     1] loss: 1031.500
[27,     1] loss: 1074.001
[28,     1] loss: 1077.091
[29,     1] loss: 1036.733
[30,     1] loss: 1018.220
[31,     1] loss: 1056.845
[32,     1] loss: 966.940
[33,     1] loss: 1043.418
[34,     1] loss: 1003.063
[35,     1] loss: 1008.682
[36,     1] loss: 998.223
[37,     1] loss: 995.966
[38,     1] loss: 953.827
[39,     1] loss: 976.777
[40,     1] loss: 978.132
[41,     1] loss: 992.995
[42,     1] loss: 930.359
[43,     1] loss: 963.796
[44,     1] loss: 968.523
[45,     1] loss: 991.692
[46,     1] loss: 927.434
[47,     1] loss: 956.094
[48,     1] loss: 921.998
[49,     1] loss: 922.097
[50,     1] loss: 918.930
[51,     1] loss: 886.263
[52,     1] loss: 911.406
[53,     1] loss: 894.619
[54,     1] loss: 885.602
[55,     1] loss: 926.643
[56,     1] loss: 879.469
[57,     1] loss: 865.408
[58,     1] loss: 894.129
[59,     1] loss: 868.722
[60,     1] loss: 891.380
[61,     1] loss: 894.879
[62,     1] loss: 862.644
[63,     1] loss: 818.968
[64,     1] loss: 853.818
[65,     1] loss: 835.497
[66,     1] loss: 827.214
[67,     1] loss: 823.178
[68,     1] loss: 796.255
[69,     1] loss: 733.952
[70,     1] loss: 877.770
[71,     1] loss: 805.491
[72,     1] loss: 785.645
[73,     1] loss: 729.720
[74,     1] loss: 773.230
[75,     1] loss: 841.361
[76,     1] loss: 840.676
[77,     1] loss: 739.641
[78,     1] loss: 772.329
[79,     1] loss: 694.823
[80,     1] loss: 752.205
[81,     1] loss: 731.097
[82,     1] loss: 714.808
[83,     1] loss: 696.606
[84,     1] loss: 676.772
[85,     1] loss: 752.407
[86,     1] loss: 772.915
[87,     1] loss: 671.155
[88,     1] loss: 642.989
[89,     1] loss: 613.661
[90,     1] loss: 654.664
[91,     1] loss: 675.745
[92,     1] loss: 603.837
[93,     1] loss: 662.540
[94,     1] loss: 664.907
[95,     1] loss: 566.788
[96,     1] loss: 628.845
[97,     1] loss: 582.233
[98,     1] loss: 608.619
[99,     1] loss: 667.995
[100,     1] loss: 559.898
[101,     1] loss: 608.731
[102,     1] loss: 574.633
[103,     1] loss: 540.325
[104,     1] loss: 574.177
[105,     1] loss: 549.289
[106,     1] loss: 476.664
[107,     1] loss: 529.729
[108,     1] loss: 495.300
[109,     1] loss: 520.563
[110,     1] loss: 526.532
[111,     1] loss: 487.860
[112,     1] loss: 432.758
[113,     1] loss: 503.893
[114,     1] loss: 439.274
[115,     1] loss: 472.198
[116,     1] loss: 421.370
[117,     1] loss: 467.608
[118,     1] loss: 552.032
[119,     1] loss: 625.540
[120,     1] loss: 514.256
[121,     1] loss: 504.349
Early stopping applied (best metric=0.3840530216693878)
Finished Training
Total time taken: 20.741944313049316
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1291.101
[2,     1] loss: 1298.337
[3,     1] loss: 1292.351
[4,     1] loss: 1290.707
[5,     1] loss: 1289.086
[6,     1] loss: 1290.141
[7,     1] loss: 1296.176
[8,     1] loss: 1287.073
[9,     1] loss: 1290.094
[10,     1] loss: 1288.271
[11,     1] loss: 1288.637
[12,     1] loss: 1290.731
[13,     1] loss: 1288.730
[14,     1] loss: 1288.580
[15,     1] loss: 1288.560
[16,     1] loss: 1291.605
[17,     1] loss: 1285.259
[18,     1] loss: 1285.676
[19,     1] loss: 1287.120
[20,     1] loss: 1291.388
[21,     1] loss: 1290.111
[22,     1] loss: 1290.376
[23,     1] loss: 1285.113
[24,     1] loss: 1284.552
[25,     1] loss: 1286.425
[26,     1] loss: 1282.448
[27,     1] loss: 1282.631
[28,     1] loss: 1278.790
[29,     1] loss: 1265.840
[30,     1] loss: 1265.788
[31,     1] loss: 1252.244
[32,     1] loss: 1236.684
[33,     1] loss: 1216.717
[34,     1] loss: 1208.655
[35,     1] loss: 1160.591
[36,     1] loss: 1155.359
[37,     1] loss: 1143.943
[38,     1] loss: 1109.876
[39,     1] loss: 1115.945
[40,     1] loss: 1116.132
[41,     1] loss: 1097.546
[42,     1] loss: 1064.167
[43,     1] loss: 1090.055
[44,     1] loss: 1081.032
[45,     1] loss: 1041.993
[46,     1] loss: 1060.080
[47,     1] loss: 1032.101
[48,     1] loss: 1023.240
[49,     1] loss: 958.030
[50,     1] loss: 1022.866
[51,     1] loss: 1029.603
[52,     1] loss: 981.223
[53,     1] loss: 1027.094
[54,     1] loss: 943.192
[55,     1] loss: 1027.074
[56,     1] loss: 958.864
[57,     1] loss: 959.264
[58,     1] loss: 990.574
[59,     1] loss: 966.687
[60,     1] loss: 1008.702
[61,     1] loss: 924.526
[62,     1] loss: 1002.488
[63,     1] loss: 888.745
[64,     1] loss: 983.516
[65,     1] loss: 906.018
[66,     1] loss: 890.266
[67,     1] loss: 926.121
[68,     1] loss: 863.170
[69,     1] loss: 922.730
[70,     1] loss: 903.183
[71,     1] loss: 911.090
[72,     1] loss: 854.835
[73,     1] loss: 858.904
[74,     1] loss: 833.773
[75,     1] loss: 809.148
[76,     1] loss: 866.719
[77,     1] loss: 853.400
[78,     1] loss: 851.051
[79,     1] loss: 874.923
[80,     1] loss: 778.479
[81,     1] loss: 788.721
[82,     1] loss: 772.806
[83,     1] loss: 815.445
[84,     1] loss: 808.851
[85,     1] loss: 818.103
[86,     1] loss: 734.105
[87,     1] loss: 858.670
[88,     1] loss: 785.253
[89,     1] loss: 777.936
[90,     1] loss: 762.071
[91,     1] loss: 762.432
[92,     1] loss: 755.886
[93,     1] loss: 722.769
[94,     1] loss: 764.958
[95,     1] loss: 706.865
[96,     1] loss: 650.291
[97,     1] loss: 659.898
[98,     1] loss: 677.654
[99,     1] loss: 645.986
[100,     1] loss: 619.672
[101,     1] loss: 637.305
[102,     1] loss: 627.702
[103,     1] loss: 541.008
[104,     1] loss: 594.722
[105,     1] loss: 640.995
[106,     1] loss: 662.650
[107,     1] loss: 561.922
[108,     1] loss: 556.632
[109,     1] loss: 571.639
[110,     1] loss: 597.008
[111,     1] loss: 693.926
[112,     1] loss: 771.770
[113,     1] loss: 604.653
[114,     1] loss: 594.971
[115,     1] loss: 660.417
[116,     1] loss: 602.602
[117,     1] loss: 647.464
[118,     1] loss: 598.612
[119,     1] loss: 607.958
[120,     1] loss: 535.663
Early stopping applied (best metric=0.3893382251262665)
Finished Training
Total time taken: 18.792510271072388
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.577
[2,     1] loss: 1337.263
[3,     1] loss: 1293.099
[4,     1] loss: 1294.028
[5,     1] loss: 1290.808
[6,     1] loss: 1290.725
[7,     1] loss: 1289.772
[8,     1] loss: 1291.406
[9,     1] loss: 1291.469
[10,     1] loss: 1290.791
[11,     1] loss: 1290.176
[12,     1] loss: 1291.417
[13,     1] loss: 1291.932
[14,     1] loss: 1290.163
[15,     1] loss: 1290.250
[16,     1] loss: 1287.029
[17,     1] loss: 1292.138
[18,     1] loss: 1290.163
[19,     1] loss: 1289.687
[20,     1] loss: 1289.827
[21,     1] loss: 1288.217
[22,     1] loss: 1288.006
[23,     1] loss: 1286.558
[24,     1] loss: 1289.962
[25,     1] loss: 1289.206
[26,     1] loss: 1289.792
[27,     1] loss: 1286.553
[28,     1] loss: 1287.933
[29,     1] loss: 1286.583
[30,     1] loss: 1286.383
[31,     1] loss: 1284.244
[32,     1] loss: 1280.649
[33,     1] loss: 1278.380
[34,     1] loss: 1272.871
[35,     1] loss: 1260.889
[36,     1] loss: 1240.534
[37,     1] loss: 1218.533
[38,     1] loss: 1199.085
[39,     1] loss: 1144.905
[40,     1] loss: 1126.461
[41,     1] loss: 1054.708
[42,     1] loss: 1123.885
[43,     1] loss: 1030.107
[44,     1] loss: 1114.026
[45,     1] loss: 1025.370
[46,     1] loss: 1092.026
[47,     1] loss: 1042.155
[48,     1] loss: 1056.952
[49,     1] loss: 1061.210
[50,     1] loss: 1041.726
[51,     1] loss: 1031.525
[52,     1] loss: 1039.305
[53,     1] loss: 1031.783
[54,     1] loss: 1016.527
[55,     1] loss: 1039.466
[56,     1] loss: 1043.626
[57,     1] loss: 997.570
[58,     1] loss: 991.560
[59,     1] loss: 986.744
[60,     1] loss: 1025.867
[61,     1] loss: 996.933
[62,     1] loss: 975.441
[63,     1] loss: 968.748
[64,     1] loss: 1000.498
[65,     1] loss: 954.052
[66,     1] loss: 967.103
[67,     1] loss: 973.215
[68,     1] loss: 1007.619
[69,     1] loss: 931.856
[70,     1] loss: 985.366
[71,     1] loss: 914.531
[72,     1] loss: 898.549
[73,     1] loss: 937.293
[74,     1] loss: 912.810
[75,     1] loss: 912.129
[76,     1] loss: 846.456
[77,     1] loss: 926.847
[78,     1] loss: 911.105
[79,     1] loss: 945.339
[80,     1] loss: 873.809
[81,     1] loss: 921.693
[82,     1] loss: 907.673
[83,     1] loss: 976.218
[84,     1] loss: 894.864
[85,     1] loss: 951.602
[86,     1] loss: 869.813
[87,     1] loss: 917.180
[88,     1] loss: 881.912
[89,     1] loss: 840.520
[90,     1] loss: 934.216
[91,     1] loss: 887.346
[92,     1] loss: 864.602
[93,     1] loss: 945.078
[94,     1] loss: 821.882
[95,     1] loss: 893.649
[96,     1] loss: 799.884
[97,     1] loss: 856.892
[98,     1] loss: 844.610
[99,     1] loss: 805.487
[100,     1] loss: 819.338
[101,     1] loss: 781.319
[102,     1] loss: 798.756
[103,     1] loss: 747.853
[104,     1] loss: 835.131
[105,     1] loss: 805.565
[106,     1] loss: 842.215
[107,     1] loss: 730.238
[108,     1] loss: 837.669
[109,     1] loss: 741.691
[110,     1] loss: 757.619
[111,     1] loss: 687.355
[112,     1] loss: 737.479
[113,     1] loss: 732.032
[114,     1] loss: 822.254
[115,     1] loss: 671.193
[116,     1] loss: 683.420
[117,     1] loss: 639.695
[118,     1] loss: 671.752
[119,     1] loss: 638.240
[120,     1] loss: 718.976
[121,     1] loss: 734.557
[122,     1] loss: 622.472
[123,     1] loss: 630.716
[124,     1] loss: 586.597
[125,     1] loss: 658.184
[126,     1] loss: 657.710
[127,     1] loss: 626.260
[128,     1] loss: 593.209
[129,     1] loss: 638.348
[130,     1] loss: 557.864
[131,     1] loss: 563.962
[132,     1] loss: 561.962
[133,     1] loss: 644.813
[134,     1] loss: 769.889
[135,     1] loss: 541.178
[136,     1] loss: 605.979
[137,     1] loss: 656.335
[138,     1] loss: 622.723
[139,     1] loss: 620.241
[140,     1] loss: 530.097
[141,     1] loss: 619.050
[142,     1] loss: 516.400
[143,     1] loss: 512.512
[144,     1] loss: 649.432
[145,     1] loss: 385.197
[146,     1] loss: 522.800
[147,     1] loss: 561.892
[148,     1] loss: 469.016
[149,     1] loss: 539.980
[150,     1] loss: 478.708
[151,     1] loss: 567.633
[152,     1] loss: 509.535
[153,     1] loss: 468.955
[154,     1] loss: 480.753
[155,     1] loss: 422.575
[156,     1] loss: 486.066
[157,     1] loss: 387.705
[158,     1] loss: 446.471
[159,     1] loss: 397.468
[160,     1] loss: 468.926
[161,     1] loss: 554.033
[162,     1] loss: 380.889
[163,     1] loss: 492.703
[164,     1] loss: 465.213
[165,     1] loss: 414.990
[166,     1] loss: 499.210
[167,     1] loss: 351.626
[168,     1] loss: 471.519
[169,     1] loss: 371.797
[170,     1] loss: 514.610
[171,     1] loss: 398.073
[172,     1] loss: 418.241
[173,     1] loss: 426.152
[174,     1] loss: 364.767
[175,     1] loss: 393.582
[176,     1] loss: 358.862
[177,     1] loss: 386.317
[178,     1] loss: 342.347
[179,     1] loss: 348.621
[180,     1] loss: 318.067
[181,     1] loss: 332.217
[182,     1] loss: 371.139
[183,     1] loss: 361.643
[184,     1] loss: 319.943
[185,     1] loss: 382.491
[186,     1] loss: 370.510
[187,     1] loss: 390.559
Early stopping applied (best metric=0.380159467458725)
Finished Training
Total time taken: 32.513662338256836
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1298.544
[2,     1] loss: 1296.550
[3,     1] loss: 1297.085
[4,     1] loss: 1288.596
[5,     1] loss: 1290.970
[6,     1] loss: 1288.397
[7,     1] loss: 1291.403
[8,     1] loss: 1287.758
[9,     1] loss: 1289.493
[10,     1] loss: 1291.015
[11,     1] loss: 1290.213
[12,     1] loss: 1289.521
[13,     1] loss: 1289.395
[14,     1] loss: 1289.949
[15,     1] loss: 1290.238
[16,     1] loss: 1288.389
[17,     1] loss: 1287.381
[18,     1] loss: 1288.770
[19,     1] loss: 1285.887
[20,     1] loss: 1288.055
[21,     1] loss: 1285.601
[22,     1] loss: 1282.886
[23,     1] loss: 1277.834
[24,     1] loss: 1273.430
[25,     1] loss: 1264.580
[26,     1] loss: 1254.597
[27,     1] loss: 1225.504
[28,     1] loss: 1195.729
[29,     1] loss: 1162.755
[30,     1] loss: 1137.951
[31,     1] loss: 1106.924
[32,     1] loss: 1076.231
[33,     1] loss: 1071.434
[34,     1] loss: 1031.142
[35,     1] loss: 1061.939
[36,     1] loss: 1023.675
[37,     1] loss: 1010.223
[38,     1] loss: 1022.347
[39,     1] loss: 1023.306
[40,     1] loss: 1000.834
[41,     1] loss: 1030.293
[42,     1] loss: 1005.870
[43,     1] loss: 1013.655
[44,     1] loss: 961.623
[45,     1] loss: 977.855
[46,     1] loss: 987.192
[47,     1] loss: 941.609
[48,     1] loss: 998.368
[49,     1] loss: 937.995
[50,     1] loss: 952.665
[51,     1] loss: 942.906
[52,     1] loss: 944.961
[53,     1] loss: 948.131
[54,     1] loss: 935.433
[55,     1] loss: 953.197
[56,     1] loss: 939.682
[57,     1] loss: 928.410
[58,     1] loss: 932.684
[59,     1] loss: 865.332
[60,     1] loss: 872.127
[61,     1] loss: 873.153
[62,     1] loss: 917.177
[63,     1] loss: 872.352
[64,     1] loss: 869.412
[65,     1] loss: 914.765
[66,     1] loss: 849.177
[67,     1] loss: 880.844
[68,     1] loss: 807.676
[69,     1] loss: 876.926
[70,     1] loss: 815.867
[71,     1] loss: 817.355
[72,     1] loss: 841.900
[73,     1] loss: 829.778
[74,     1] loss: 795.272
[75,     1] loss: 832.717
[76,     1] loss: 768.167
[77,     1] loss: 776.051
[78,     1] loss: 762.484
[79,     1] loss: 778.713
[80,     1] loss: 763.230
[81,     1] loss: 825.564
[82,     1] loss: 844.250
[83,     1] loss: 677.206
[84,     1] loss: 882.783
[85,     1] loss: 750.248
[86,     1] loss: 875.688
[87,     1] loss: 789.472
[88,     1] loss: 846.304
[89,     1] loss: 781.246
[90,     1] loss: 741.380
[91,     1] loss: 828.827
[92,     1] loss: 707.315
[93,     1] loss: 748.247
[94,     1] loss: 736.740
[95,     1] loss: 590.258
[96,     1] loss: 738.460
[97,     1] loss: 625.003
[98,     1] loss: 757.121
[99,     1] loss: 641.785
[100,     1] loss: 642.162
[101,     1] loss: 608.032
[102,     1] loss: 611.193
[103,     1] loss: 634.285
[104,     1] loss: 579.351
[105,     1] loss: 617.885
[106,     1] loss: 581.254
[107,     1] loss: 549.153
[108,     1] loss: 541.086
Early stopping applied (best metric=0.3634611964225769)
Finished Training
Total time taken: 17.849809646606445
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1305.987
[2,     1] loss: 1291.554
[3,     1] loss: 1295.640
[4,     1] loss: 1297.331
[5,     1] loss: 1297.681
[6,     1] loss: 1295.576
[7,     1] loss: 1292.678
[8,     1] loss: 1294.644
[9,     1] loss: 1291.109
[10,     1] loss: 1290.188
[11,     1] loss: 1293.145
[12,     1] loss: 1289.796
[13,     1] loss: 1293.048
[14,     1] loss: 1288.656
[15,     1] loss: 1290.728
[16,     1] loss: 1289.691
[17,     1] loss: 1290.637
[18,     1] loss: 1286.839
[19,     1] loss: 1285.548
[20,     1] loss: 1282.203
[21,     1] loss: 1276.500
[22,     1] loss: 1271.374
[23,     1] loss: 1259.553
[24,     1] loss: 1230.455
[25,     1] loss: 1215.269
[26,     1] loss: 1174.496
[27,     1] loss: 1143.857
[28,     1] loss: 1159.405
[29,     1] loss: 1122.566
[30,     1] loss: 1120.766
[31,     1] loss: 1091.953
[32,     1] loss: 1094.172
[33,     1] loss: 1086.481
[34,     1] loss: 1070.261
[35,     1] loss: 1040.954
[36,     1] loss: 1077.013
[37,     1] loss: 1034.451
[38,     1] loss: 1063.943
[39,     1] loss: 1054.341
[40,     1] loss: 1050.822
[41,     1] loss: 1021.708
[42,     1] loss: 995.711
[43,     1] loss: 988.998
[44,     1] loss: 995.444
[45,     1] loss: 1005.178
[46,     1] loss: 999.307
[47,     1] loss: 981.704
[48,     1] loss: 994.167
[49,     1] loss: 1009.984
[50,     1] loss: 927.119
[51,     1] loss: 927.557
[52,     1] loss: 967.312
[53,     1] loss: 937.156
[54,     1] loss: 921.840
[55,     1] loss: 902.033
[56,     1] loss: 916.813
[57,     1] loss: 925.151
[58,     1] loss: 927.433
[59,     1] loss: 969.893
[60,     1] loss: 905.922
[61,     1] loss: 920.634
[62,     1] loss: 867.611
[63,     1] loss: 914.181
[64,     1] loss: 884.739
[65,     1] loss: 916.931
[66,     1] loss: 922.242
[67,     1] loss: 860.563
[68,     1] loss: 874.609
[69,     1] loss: 817.021
[70,     1] loss: 850.709
[71,     1] loss: 798.891
[72,     1] loss: 903.619
[73,     1] loss: 846.782
[74,     1] loss: 856.157
[75,     1] loss: 840.613
[76,     1] loss: 786.179
[77,     1] loss: 796.205
[78,     1] loss: 764.022
[79,     1] loss: 758.652
[80,     1] loss: 758.353
[81,     1] loss: 734.035
[82,     1] loss: 768.111
[83,     1] loss: 733.355
[84,     1] loss: 761.768
[85,     1] loss: 761.890
[86,     1] loss: 663.996
[87,     1] loss: 709.394
[88,     1] loss: 675.568
[89,     1] loss: 639.350
[90,     1] loss: 652.945
[91,     1] loss: 615.938
[92,     1] loss: 723.809
[93,     1] loss: 1039.255
[94,     1] loss: 1335.018
[95,     1] loss: 860.552
[96,     1] loss: 736.558
[97,     1] loss: 980.625
[98,     1] loss: 941.253
[99,     1] loss: 874.041
[100,     1] loss: 873.073
[101,     1] loss: 866.694
[102,     1] loss: 904.389
[103,     1] loss: 836.015
[104,     1] loss: 791.939
[105,     1] loss: 770.473
[106,     1] loss: 799.286
[107,     1] loss: 673.718
[108,     1] loss: 712.334
[109,     1] loss: 656.813
[110,     1] loss: 582.678
[111,     1] loss: 633.359
[112,     1] loss: 577.293
[113,     1] loss: 604.102
Early stopping applied (best metric=0.3359900116920471)
Finished Training
Total time taken: 21.188560247421265
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.281
[2,     1] loss: 1289.525
[3,     1] loss: 1292.775
[4,     1] loss: 1292.813
[5,     1] loss: 1291.220
[6,     1] loss: 1292.814
[7,     1] loss: 1288.268
[8,     1] loss: 1286.240
[9,     1] loss: 1285.938
[10,     1] loss: 1285.821
[11,     1] loss: 1280.948
[12,     1] loss: 1278.733
[13,     1] loss: 1272.719
[14,     1] loss: 1267.001
[15,     1] loss: 1253.793
[16,     1] loss: 1232.400
[17,     1] loss: 1218.245
[18,     1] loss: 1178.844
[19,     1] loss: 1166.550
[20,     1] loss: 1122.339
[21,     1] loss: 1133.153
[22,     1] loss: 1102.240
[23,     1] loss: 1062.678
[24,     1] loss: 1074.115
[25,     1] loss: 1084.845
[26,     1] loss: 1013.326
[27,     1] loss: 986.126
[28,     1] loss: 1014.569
[29,     1] loss: 1058.503
[30,     1] loss: 1041.911
[31,     1] loss: 1050.967
[32,     1] loss: 1023.030
[33,     1] loss: 1056.687
[34,     1] loss: 976.672
[35,     1] loss: 987.695
[36,     1] loss: 1000.171
[37,     1] loss: 947.116
[38,     1] loss: 960.454
[39,     1] loss: 988.397
[40,     1] loss: 950.583
[41,     1] loss: 959.059
[42,     1] loss: 903.769
[43,     1] loss: 951.264
[44,     1] loss: 895.795
[45,     1] loss: 934.214
[46,     1] loss: 906.486
[47,     1] loss: 975.824
[48,     1] loss: 931.698
[49,     1] loss: 874.654
[50,     1] loss: 895.553
[51,     1] loss: 903.766
[52,     1] loss: 844.360
[53,     1] loss: 864.063
[54,     1] loss: 838.343
[55,     1] loss: 877.159
[56,     1] loss: 812.427
[57,     1] loss: 891.640
[58,     1] loss: 820.899
[59,     1] loss: 874.007
[60,     1] loss: 859.750
[61,     1] loss: 830.390
[62,     1] loss: 819.697
[63,     1] loss: 760.699
[64,     1] loss: 808.910
[65,     1] loss: 804.567
[66,     1] loss: 751.545
[67,     1] loss: 851.629
[68,     1] loss: 845.051
[69,     1] loss: 729.042
[70,     1] loss: 809.557
[71,     1] loss: 753.241
[72,     1] loss: 744.776
[73,     1] loss: 762.951
[74,     1] loss: 744.379
[75,     1] loss: 725.783
[76,     1] loss: 707.947
[77,     1] loss: 680.724
[78,     1] loss: 687.356
[79,     1] loss: 702.717
[80,     1] loss: 656.323
[81,     1] loss: 665.521
[82,     1] loss: 636.617
Early stopping applied (best metric=0.3716988265514374)
Finished Training
Total time taken: 13.622310876846313
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.272
[2,     1] loss: 1297.013
[3,     1] loss: 1293.188
[4,     1] loss: 1292.801
[5,     1] loss: 1288.904
[6,     1] loss: 1296.524
[7,     1] loss: 1293.565
[8,     1] loss: 1291.529
[9,     1] loss: 1288.967
[10,     1] loss: 1292.614
[11,     1] loss: 1290.154
[12,     1] loss: 1291.466
[13,     1] loss: 1291.176
[14,     1] loss: 1290.213
[15,     1] loss: 1289.525
[16,     1] loss: 1290.943
[17,     1] loss: 1291.720
[18,     1] loss: 1286.132
[19,     1] loss: 1291.610
[20,     1] loss: 1290.682
[21,     1] loss: 1292.155
[22,     1] loss: 1290.644
[23,     1] loss: 1289.930
[24,     1] loss: 1292.112
[25,     1] loss: 1290.573
[26,     1] loss: 1289.806
[27,     1] loss: 1289.801
[28,     1] loss: 1289.126
[29,     1] loss: 1290.646
[30,     1] loss: 1288.976
[31,     1] loss: 1288.843
[32,     1] loss: 1288.769
[33,     1] loss: 1288.126
[34,     1] loss: 1287.893
[35,     1] loss: 1286.741
[36,     1] loss: 1285.915
[37,     1] loss: 1284.045
[38,     1] loss: 1286.574
[39,     1] loss: 1282.683
[40,     1] loss: 1275.997
[41,     1] loss: 1268.540
[42,     1] loss: 1246.816
[43,     1] loss: 1242.221
[44,     1] loss: 1212.829
[45,     1] loss: 1201.041
[46,     1] loss: 1169.402
[47,     1] loss: 1115.216
[48,     1] loss: 1140.830
[49,     1] loss: 1114.236
[50,     1] loss: 1096.155
[51,     1] loss: 1121.583
[52,     1] loss: 1096.890
[53,     1] loss: 1083.705
[54,     1] loss: 1066.347
[55,     1] loss: 1059.782
[56,     1] loss: 1059.127
[57,     1] loss: 1031.773
[58,     1] loss: 991.843
[59,     1] loss: 969.725
[60,     1] loss: 1001.813
[61,     1] loss: 964.226
[62,     1] loss: 1018.320
[63,     1] loss: 992.389
[64,     1] loss: 960.058
[65,     1] loss: 943.728
[66,     1] loss: 965.167
[67,     1] loss: 925.319
[68,     1] loss: 941.621
[69,     1] loss: 969.633
[70,     1] loss: 920.689
[71,     1] loss: 969.780
[72,     1] loss: 953.896
[73,     1] loss: 922.917
[74,     1] loss: 884.004
[75,     1] loss: 950.608
[76,     1] loss: 893.058
[77,     1] loss: 974.598
[78,     1] loss: 902.306
[79,     1] loss: 939.697
[80,     1] loss: 881.724
[81,     1] loss: 959.196
[82,     1] loss: 842.072
[83,     1] loss: 920.696
[84,     1] loss: 846.652
[85,     1] loss: 846.070
[86,     1] loss: 802.021
[87,     1] loss: 804.559
[88,     1] loss: 836.456
[89,     1] loss: 861.307
[90,     1] loss: 781.995
[91,     1] loss: 771.875
[92,     1] loss: 780.127
[93,     1] loss: 732.773
[94,     1] loss: 756.214
[95,     1] loss: 819.524
[96,     1] loss: 703.174
[97,     1] loss: 773.488
[98,     1] loss: 757.004
[99,     1] loss: 715.351
[100,     1] loss: 756.585
[101,     1] loss: 721.891
[102,     1] loss: 725.184
[103,     1] loss: 673.547
[104,     1] loss: 723.514
[105,     1] loss: 681.504
[106,     1] loss: 693.454
[107,     1] loss: 692.298
[108,     1] loss: 619.972
[109,     1] loss: 637.469
[110,     1] loss: 609.884
[111,     1] loss: 653.818
[112,     1] loss: 736.958
[113,     1] loss: 627.118
[114,     1] loss: 656.492
[115,     1] loss: 658.878
[116,     1] loss: 547.482
[117,     1] loss: 694.838
Early stopping applied (best metric=0.3435382843017578)
Finished Training
Total time taken: 20.917072296142578
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.738
[2,     1] loss: 1312.185
[3,     1] loss: 1292.522
[4,     1] loss: 1289.615
[5,     1] loss: 1295.994
[6,     1] loss: 1289.163
[7,     1] loss: 1290.281
[8,     1] loss: 1288.471
[9,     1] loss: 1291.690
[10,     1] loss: 1289.144
[11,     1] loss: 1290.120
[12,     1] loss: 1290.116
[13,     1] loss: 1286.101
[14,     1] loss: 1288.371
[15,     1] loss: 1290.212
[16,     1] loss: 1285.743
[17,     1] loss: 1282.916
[18,     1] loss: 1282.616
[19,     1] loss: 1281.677
[20,     1] loss: 1275.193
[21,     1] loss: 1274.050
[22,     1] loss: 1262.617
[23,     1] loss: 1241.800
[24,     1] loss: 1220.117
[25,     1] loss: 1199.504
[26,     1] loss: 1192.695
[27,     1] loss: 1169.784
[28,     1] loss: 1131.599
[29,     1] loss: 1087.009
[30,     1] loss: 1105.476
[31,     1] loss: 1098.298
[32,     1] loss: 1061.963
[33,     1] loss: 1068.986
[34,     1] loss: 1046.281
[35,     1] loss: 1081.791
[36,     1] loss: 1084.463
[37,     1] loss: 1141.434
[38,     1] loss: 1010.248
[39,     1] loss: 1038.814
[40,     1] loss: 1084.714
[41,     1] loss: 1032.069
[42,     1] loss: 992.690
[43,     1] loss: 1057.481
[44,     1] loss: 1021.032
[45,     1] loss: 1006.667
[46,     1] loss: 1007.999
[47,     1] loss: 999.742
[48,     1] loss: 970.021
[49,     1] loss: 996.189
[50,     1] loss: 1039.688
[51,     1] loss: 959.300
[52,     1] loss: 938.208
[53,     1] loss: 995.755
[54,     1] loss: 947.952
[55,     1] loss: 959.830
[56,     1] loss: 1004.575
[57,     1] loss: 911.555
[58,     1] loss: 909.092
[59,     1] loss: 958.035
[60,     1] loss: 877.409
[61,     1] loss: 945.376
[62,     1] loss: 844.757
[63,     1] loss: 896.255
[64,     1] loss: 877.472
[65,     1] loss: 915.228
[66,     1] loss: 898.220
[67,     1] loss: 839.856
[68,     1] loss: 914.838
[69,     1] loss: 849.212
[70,     1] loss: 855.808
[71,     1] loss: 831.961
[72,     1] loss: 785.850
[73,     1] loss: 855.386
[74,     1] loss: 839.068
[75,     1] loss: 830.932
[76,     1] loss: 790.615
[77,     1] loss: 857.554
[78,     1] loss: 785.716
[79,     1] loss: 761.035
[80,     1] loss: 804.928
[81,     1] loss: 810.607
[82,     1] loss: 784.599
[83,     1] loss: 744.467
[84,     1] loss: 794.072
[85,     1] loss: 819.893
[86,     1] loss: 727.855
[87,     1] loss: 797.600
[88,     1] loss: 708.245
[89,     1] loss: 794.850
[90,     1] loss: 747.696
[91,     1] loss: 732.488
[92,     1] loss: 673.174
[93,     1] loss: 723.668
[94,     1] loss: 671.880
[95,     1] loss: 714.803
[96,     1] loss: 634.620
[97,     1] loss: 760.344
[98,     1] loss: 698.453
[99,     1] loss: 618.115
[100,     1] loss: 716.975
[101,     1] loss: 636.677
[102,     1] loss: 689.510
Early stopping applied (best metric=0.4074101746082306)
Finished Training
Total time taken: 19.148361444473267
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.098
[2,     1] loss: 1298.689
[3,     1] loss: 1290.776
[4,     1] loss: 1293.691
[5,     1] loss: 1286.286
[6,     1] loss: 1291.569
[7,     1] loss: 1288.088
[8,     1] loss: 1286.755
[9,     1] loss: 1294.412
[10,     1] loss: 1283.065
[11,     1] loss: 1281.443
[12,     1] loss: 1278.432
[13,     1] loss: 1274.496
[14,     1] loss: 1256.378
[15,     1] loss: 1240.439
[16,     1] loss: 1206.572
[17,     1] loss: 1169.765
[18,     1] loss: 1144.975
[19,     1] loss: 1117.317
[20,     1] loss: 1043.079
[21,     1] loss: 1114.122
[22,     1] loss: 1044.163
[23,     1] loss: 1054.969
[24,     1] loss: 979.235
[25,     1] loss: 1073.018
[26,     1] loss: 999.116
[27,     1] loss: 1019.943
[28,     1] loss: 964.008
[29,     1] loss: 965.743
[30,     1] loss: 962.448
[31,     1] loss: 998.314
[32,     1] loss: 982.553
[33,     1] loss: 1044.998
[34,     1] loss: 933.087
[35,     1] loss: 972.982
[36,     1] loss: 945.047
[37,     1] loss: 914.935
[38,     1] loss: 902.953
[39,     1] loss: 871.145
[40,     1] loss: 879.110
[41,     1] loss: 942.651
[42,     1] loss: 912.578
[43,     1] loss: 874.284
[44,     1] loss: 919.128
[45,     1] loss: 854.841
[46,     1] loss: 893.446
[47,     1] loss: 844.572
[48,     1] loss: 860.562
[49,     1] loss: 866.085
[50,     1] loss: 786.283
[51,     1] loss: 824.021
[52,     1] loss: 794.543
[53,     1] loss: 785.142
[54,     1] loss: 788.268
[55,     1] loss: 828.393
[56,     1] loss: 815.565
[57,     1] loss: 787.016
[58,     1] loss: 783.727
[59,     1] loss: 782.463
[60,     1] loss: 785.800
[61,     1] loss: 799.558
[62,     1] loss: 720.975
[63,     1] loss: 745.743
[64,     1] loss: 707.483
[65,     1] loss: 697.809
[66,     1] loss: 773.413
[67,     1] loss: 691.625
[68,     1] loss: 695.944
[69,     1] loss: 636.120
[70,     1] loss: 673.051
[71,     1] loss: 717.071
[72,     1] loss: 662.285
[73,     1] loss: 606.541
[74,     1] loss: 709.706
[75,     1] loss: 767.623
[76,     1] loss: 684.118
[77,     1] loss: 774.443
[78,     1] loss: 783.630
Early stopping applied (best metric=0.466410368680954)
Finished Training
Total time taken: 12.253801584243774
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1294.905
[2,     1] loss: 1298.572
[3,     1] loss: 1294.190
[4,     1] loss: 1295.006
[5,     1] loss: 1289.983
[6,     1] loss: 1290.752
[7,     1] loss: 1286.914
[8,     1] loss: 1287.418
[9,     1] loss: 1286.395
[10,     1] loss: 1285.745
[11,     1] loss: 1282.481
[12,     1] loss: 1275.426
[13,     1] loss: 1268.537
[14,     1] loss: 1250.166
[15,     1] loss: 1224.711
[16,     1] loss: 1194.825
[17,     1] loss: 1154.172
[18,     1] loss: 1151.566
[19,     1] loss: 1144.616
[20,     1] loss: 1101.790
[21,     1] loss: 1120.626
[22,     1] loss: 1038.383
[23,     1] loss: 1097.262
[24,     1] loss: 1078.707
[25,     1] loss: 1055.743
[26,     1] loss: 1026.888
[27,     1] loss: 1049.646
[28,     1] loss: 1064.647
[29,     1] loss: 1042.294
[30,     1] loss: 1050.144
[31,     1] loss: 1070.958
[32,     1] loss: 984.183
[33,     1] loss: 999.351
[34,     1] loss: 1012.341
[35,     1] loss: 1000.448
[36,     1] loss: 947.229
[37,     1] loss: 997.899
[38,     1] loss: 931.915
[39,     1] loss: 928.265
[40,     1] loss: 980.910
[41,     1] loss: 940.439
[42,     1] loss: 945.528
[43,     1] loss: 894.136
[44,     1] loss: 946.010
[45,     1] loss: 954.856
[46,     1] loss: 944.231
[47,     1] loss: 855.679
[48,     1] loss: 894.372
[49,     1] loss: 886.963
[50,     1] loss: 896.596
[51,     1] loss: 864.897
[52,     1] loss: 859.878
[53,     1] loss: 862.879
[54,     1] loss: 874.106
[55,     1] loss: 850.724
[56,     1] loss: 856.196
[57,     1] loss: 836.983
[58,     1] loss: 819.692
[59,     1] loss: 846.364
[60,     1] loss: 736.654
[61,     1] loss: 829.832
[62,     1] loss: 790.153
[63,     1] loss: 850.703
[64,     1] loss: 900.209
[65,     1] loss: 817.133
[66,     1] loss: 805.080
[67,     1] loss: 843.264
[68,     1] loss: 726.696
[69,     1] loss: 698.804
[70,     1] loss: 729.272
[71,     1] loss: 728.733
[72,     1] loss: 683.466
[73,     1] loss: 761.586
[74,     1] loss: 713.319
[75,     1] loss: 668.436
[76,     1] loss: 779.314
[77,     1] loss: 674.313
[78,     1] loss: 676.670
[79,     1] loss: 779.749
[80,     1] loss: 663.606
[81,     1] loss: 732.875
[82,     1] loss: 643.329
[83,     1] loss: 653.588
[84,     1] loss: 570.555
[85,     1] loss: 639.132
[86,     1] loss: 574.990
[87,     1] loss: 523.883
[88,     1] loss: 543.822
[89,     1] loss: 586.633
[90,     1] loss: 568.358
[91,     1] loss: 530.196
[92,     1] loss: 569.326
[93,     1] loss: 513.968
[94,     1] loss: 505.249
Early stopping applied (best metric=0.39813947677612305)
Finished Training
Total time taken: 14.124841451644897
{'Hydroxylation-K Validation Accuracy': 0.7549054373522459, 'Hydroxylation-K Validation Sensitivity': 0.6525925925925926, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.42727106227106226, 'Hydroxylation-K AUC ROC': 0.7684210526315789, 'Hydroxylation-K AUC PR': 0.5327257471932141, 'Hydroxylation-K MCC': 0.37685339229153186, 'Hydroxylation-K F1': 0.5133286691837416, 'Validation Loss (Hydroxylation-K)': 0.47663015921910606, 'Hydroxylation-P Validation Accuracy': 0.7812323401519381, 'Hydroxylation-P Validation Sensitivity': 0.7970370370370371, 'Hydroxylation-P Validation Specificity': 0.7777719587011821, 'Hydroxylation-P Validation Precision': 0.4366465428594343, 'Hydroxylation-P AUC ROC': 0.8414197735389566, 'Hydroxylation-P AUC PR': 0.5599939498564472, 'Hydroxylation-P MCC': 0.46962594647854033, 'Hydroxylation-P F1': 0.5634290514236645, 'Validation Loss (Hydroxylation-P)': 0.37938203612963356, 'Validation Loss (total)': 0.8560121973355611, 'TimeToTrain': 20.247731987635294}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005976550826378617,
 'learning_rate_Hydroxylation-K': 0.005526935401461419,
 'learning_rate_Hydroxylation-P': 0.0008339752994320193,
 'log_base': 1.4133819375668168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1560637546,
 'sample_weights': [1.8129086619557304, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.267397597301677,
 'weight_decay_Hydroxylation-K': 2.7024069834014277,
 'weight_decay_Hydroxylation-P': 9.518115725245577}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1935.394
[2,     1] loss: 1921.659
[3,     1] loss: 1930.421
[4,     1] loss: 1923.667
[5,     1] loss: 1928.414
[6,     1] loss: 1924.109
[7,     1] loss: 1932.088
[8,     1] loss: 1922.820
[9,     1] loss: 1926.283
[10,     1] loss: 1920.234
[11,     1] loss: 1917.774
[12,     1] loss: 1918.606
[13,     1] loss: 1915.012
[14,     1] loss: 1911.836
[15,     1] loss: 1909.248
[16,     1] loss: 1905.621
[17,     1] loss: 1891.090
[18,     1] loss: 1860.953
[19,     1] loss: 1839.966
[20,     1] loss: 1767.680
[21,     1] loss: 1704.718
[22,     1] loss: 1734.689
[23,     1] loss: 1761.006
[24,     1] loss: 1631.500
[25,     1] loss: 1689.025
[26,     1] loss: 1654.181
[27,     1] loss: 1598.938
[28,     1] loss: 1555.166
[29,     1] loss: 1573.381
[30,     1] loss: 1565.059
[31,     1] loss: 1582.042
[32,     1] loss: 1621.453
[33,     1] loss: 1594.354
[34,     1] loss: 1454.879
[35,     1] loss: 1549.369
[36,     1] loss: 1487.625
[37,     1] loss: 1637.964
[38,     1] loss: 1422.231
[39,     1] loss: 1438.106
[40,     1] loss: 1385.355
[41,     1] loss: 1431.633
[42,     1] loss: 1392.760
[43,     1] loss: 1322.002
[44,     1] loss: 1287.559
[45,     1] loss: 1313.926
[46,     1] loss: 1325.270
[47,     1] loss: 1325.689
[48,     1] loss: 1269.841
[49,     1] loss: 1276.658
[50,     1] loss: 1216.096
[51,     1] loss: 1213.377
[52,     1] loss: 1138.324
[53,     1] loss: 1262.699
[54,     1] loss: 1115.837
[55,     1] loss: 1122.895
[56,     1] loss: 1113.991
[57,     1] loss: 1219.875
[58,     1] loss: 1533.752
[59,     1] loss: 1503.448
[60,     1] loss: 1224.457
[61,     1] loss: 1251.273
[62,     1] loss: 1397.023
[63,     1] loss: 1165.007
[64,     1] loss: 1355.148
[65,     1] loss: 1284.460
[66,     1] loss: 1226.382
[67,     1] loss: 1123.644
[68,     1] loss: 1248.194
[69,     1] loss: 1117.992
[70,     1] loss: 1156.271
[71,     1] loss: 1060.583
[72,     1] loss: 1094.668
[73,     1] loss: 1074.795
[74,     1] loss: 973.554
[75,     1] loss: 1074.366
[76,     1] loss: 1000.719
[77,     1] loss: 1069.815
[78,     1] loss: 1030.771
[79,     1] loss: 1051.909
[80,     1] loss: 931.787
[81,     1] loss: 1063.175
[82,     1] loss: 994.755
[83,     1] loss: 1084.761
[84,     1] loss: 1043.281
[85,     1] loss: 954.077
[86,     1] loss: 996.869
[87,     1] loss: 953.332
[88,     1] loss: 937.856
[89,     1] loss: 894.914
[90,     1] loss: 928.613
[91,     1] loss: 830.949
[92,     1] loss: 876.240
[93,     1] loss: 806.933
[94,     1] loss: 827.923
[95,     1] loss: 1108.787
[96,     1] loss: 1054.939
[97,     1] loss: 947.356
[98,     1] loss: 1409.170
[99,     1] loss: 944.841
[100,     1] loss: 1179.334
[101,     1] loss: 995.172
[102,     1] loss: 1267.972
[103,     1] loss: 993.066
[104,     1] loss: 1032.565
[105,     1] loss: 1026.687
[106,     1] loss: 998.503
[107,     1] loss: 1183.840
[108,     1] loss: 955.158
[109,     1] loss: 973.915
[110,     1] loss: 874.623
[111,     1] loss: 955.660
[112,     1] loss: 983.748
[113,     1] loss: 824.083
[114,     1] loss: 979.603
[115,     1] loss: 827.636
[116,     1] loss: 992.001
[117,     1] loss: 909.580
[118,     1] loss: 982.800
[119,     1] loss: 768.452
[120,     1] loss: 903.984
[121,     1] loss: 868.033
[122,     1] loss: 859.274
[123,     1] loss: 836.745
[124,     1] loss: 848.185
[125,     1] loss: 879.649
[126,     1] loss: 829.630
[127,     1] loss: 818.137
[128,     1] loss: 809.997
[129,     1] loss: 784.492
[130,     1] loss: 741.984
[131,     1] loss: 746.190
[132,     1] loss: 889.281
[133,     1] loss: 819.056
[134,     1] loss: 1176.276
[135,     1] loss: 1300.057
[136,     1] loss: 1154.476
[137,     1] loss: 874.501
[138,     1] loss: 1136.276
[139,     1] loss: 934.718
Early stopping applied (best metric=0.3391960561275482)
Finished Training
Total time taken: 23.701099157333374
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1924.282
[2,     1] loss: 1921.832
[3,     1] loss: 1923.735
[4,     1] loss: 1932.942
[5,     1] loss: 1915.164
[6,     1] loss: 1945.096
[7,     1] loss: 1923.091
[8,     1] loss: 1917.449
[9,     1] loss: 1922.904
[10,     1] loss: 1926.087
[11,     1] loss: 1928.446
[12,     1] loss: 1919.548
[13,     1] loss: 1919.992
[14,     1] loss: 1920.951
[15,     1] loss: 1916.544
[16,     1] loss: 1916.634
[17,     1] loss: 1911.064
[18,     1] loss: 1902.822
[19,     1] loss: 1882.092
[20,     1] loss: 1857.281
[21,     1] loss: 1851.758
[22,     1] loss: 1783.022
[23,     1] loss: 1744.471
[24,     1] loss: 1694.072
[25,     1] loss: 1670.547
[26,     1] loss: 1599.734
[27,     1] loss: 1659.354
[28,     1] loss: 1575.861
[29,     1] loss: 1592.428
[30,     1] loss: 1649.734
[31,     1] loss: 1639.678
[32,     1] loss: 1538.999
[33,     1] loss: 1512.517
[34,     1] loss: 1575.747
[35,     1] loss: 1432.016
[36,     1] loss: 1420.042
[37,     1] loss: 1425.945
[38,     1] loss: 1400.851
[39,     1] loss: 1515.413
[40,     1] loss: 1379.709
[41,     1] loss: 1388.960
[42,     1] loss: 1463.007
[43,     1] loss: 1397.187
[44,     1] loss: 1316.231
[45,     1] loss: 1369.535
[46,     1] loss: 1400.153
[47,     1] loss: 1351.674
[48,     1] loss: 1311.261
[49,     1] loss: 1204.986
[50,     1] loss: 1252.074
[51,     1] loss: 1255.867
[52,     1] loss: 1195.243
[53,     1] loss: 1175.445
[54,     1] loss: 1296.714
[55,     1] loss: 1245.853
[56,     1] loss: 1053.687
[57,     1] loss: 1275.842
[58,     1] loss: 1147.266
[59,     1] loss: 1114.000
[60,     1] loss: 1109.511
[61,     1] loss: 1133.875
[62,     1] loss: 1193.240
[63,     1] loss: 1123.559
[64,     1] loss: 1044.506
[65,     1] loss: 1110.562
[66,     1] loss: 1186.329
[67,     1] loss: 1025.981
[68,     1] loss: 1039.067
[69,     1] loss: 1075.126
[70,     1] loss: 1011.343
[71,     1] loss: 1112.365
[72,     1] loss: 980.040
[73,     1] loss: 1005.149
[74,     1] loss: 1097.868
[75,     1] loss: 914.226
[76,     1] loss: 940.004
[77,     1] loss: 970.002
[78,     1] loss: 896.813
[79,     1] loss: 913.640
Early stopping applied (best metric=0.33625537157058716)
Finished Training
Total time taken: 13.300831079483032
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1931.502
[2,     1] loss: 1923.036
[3,     1] loss: 1925.554
[4,     1] loss: 1925.181
[5,     1] loss: 1910.821
[6,     1] loss: 1927.373
[7,     1] loss: 1915.443
[8,     1] loss: 1912.364
[9,     1] loss: 1927.381
[10,     1] loss: 1906.101
[11,     1] loss: 1908.301
[12,     1] loss: 1903.594
[13,     1] loss: 1881.058
[14,     1] loss: 1848.434
[15,     1] loss: 1842.446
[16,     1] loss: 1786.929
[17,     1] loss: 1761.630
[18,     1] loss: 1776.432
[19,     1] loss: 1738.950
[20,     1] loss: 1698.093
[21,     1] loss: 1664.958
[22,     1] loss: 1724.372
[23,     1] loss: 1630.625
[24,     1] loss: 1630.992
[25,     1] loss: 1606.927
[26,     1] loss: 1598.191
[27,     1] loss: 1608.451
[28,     1] loss: 1583.523
[29,     1] loss: 1559.246
[30,     1] loss: 1437.439
[31,     1] loss: 1484.357
[32,     1] loss: 1502.002
[33,     1] loss: 1521.598
[34,     1] loss: 1477.784
[35,     1] loss: 1415.543
[36,     1] loss: 1472.577
[37,     1] loss: 1346.472
[38,     1] loss: 1351.307
[39,     1] loss: 1326.375
[40,     1] loss: 1425.201
[41,     1] loss: 1404.880
[42,     1] loss: 1454.360
[43,     1] loss: 1310.248
[44,     1] loss: 1396.273
[45,     1] loss: 1340.684
[46,     1] loss: 1298.088
[47,     1] loss: 1293.401
[48,     1] loss: 1378.474
[49,     1] loss: 1229.171
[50,     1] loss: 1318.764
[51,     1] loss: 1167.211
[52,     1] loss: 1140.480
[53,     1] loss: 1097.769
[54,     1] loss: 1181.264
[55,     1] loss: 1261.159
[56,     1] loss: 1181.781
[57,     1] loss: 1343.287
[58,     1] loss: 1042.827
[59,     1] loss: 1252.027
[60,     1] loss: 1103.897
[61,     1] loss: 1160.609
[62,     1] loss: 1149.005
[63,     1] loss: 1153.709
[64,     1] loss: 1016.327
[65,     1] loss: 1176.949
[66,     1] loss: 1031.120
[67,     1] loss: 1027.868
[68,     1] loss: 940.293
[69,     1] loss: 932.708
[70,     1] loss: 995.752
[71,     1] loss: 1059.514
[72,     1] loss: 1078.852
[73,     1] loss: 942.874
[74,     1] loss: 890.617
[75,     1] loss: 932.322
[76,     1] loss: 1075.067
[77,     1] loss: 974.590
[78,     1] loss: 965.012
[79,     1] loss: 1025.201
[80,     1] loss: 871.903
[81,     1] loss: 890.601
[82,     1] loss: 989.164
[83,     1] loss: 856.416
[84,     1] loss: 1081.110
[85,     1] loss: 1107.359
[86,     1] loss: 826.980
[87,     1] loss: 976.111
[88,     1] loss: 904.409
[89,     1] loss: 999.563
[90,     1] loss: 892.974
[91,     1] loss: 898.626
[92,     1] loss: 838.290
[93,     1] loss: 827.262
[94,     1] loss: 896.622
[95,     1] loss: 874.475
[96,     1] loss: 820.385
[97,     1] loss: 1011.705
[98,     1] loss: 922.198
[99,     1] loss: 779.217
[100,     1] loss: 948.696
[101,     1] loss: 779.651
[102,     1] loss: 882.527
[103,     1] loss: 798.195
[104,     1] loss: 792.372
[105,     1] loss: 772.807
[106,     1] loss: 824.385
[107,     1] loss: 1136.034
[108,     1] loss: 1249.080
[109,     1] loss: 798.806
[110,     1] loss: 993.878
[111,     1] loss: 823.644
Early stopping applied (best metric=0.37357810139656067)
Finished Training
Total time taken: 19.74188494682312
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1922.885
[2,     1] loss: 1933.153
[3,     1] loss: 1923.637
[4,     1] loss: 1925.078
[5,     1] loss: 1926.011
[6,     1] loss: 1924.567
[7,     1] loss: 1916.173
[8,     1] loss: 1919.865
[9,     1] loss: 1922.814
[10,     1] loss: 1914.335
[11,     1] loss: 1923.186
[12,     1] loss: 1910.829
[13,     1] loss: 1912.364
[14,     1] loss: 1915.522
[15,     1] loss: 1898.108
[16,     1] loss: 1871.414
[17,     1] loss: 1835.857
[18,     1] loss: 1840.921
[19,     1] loss: 1772.420
[20,     1] loss: 1763.993
[21,     1] loss: 1734.141
[22,     1] loss: 1753.307
[23,     1] loss: 1553.030
[24,     1] loss: 1621.231
[25,     1] loss: 1599.413
[26,     1] loss: 1640.315
[27,     1] loss: 1592.153
[28,     1] loss: 1604.444
[29,     1] loss: 1570.994
[30,     1] loss: 1689.719
[31,     1] loss: 1559.115
[32,     1] loss: 1571.010
[33,     1] loss: 1580.190
[34,     1] loss: 1501.323
[35,     1] loss: 1530.600
[36,     1] loss: 1459.898
[37,     1] loss: 1495.354
[38,     1] loss: 1543.335
[39,     1] loss: 1405.041
[40,     1] loss: 1395.919
[41,     1] loss: 1374.428
[42,     1] loss: 1363.398
[43,     1] loss: 1395.465
[44,     1] loss: 1443.830
[45,     1] loss: 1403.287
[46,     1] loss: 1374.986
[47,     1] loss: 1358.412
[48,     1] loss: 1330.929
[49,     1] loss: 1319.924
[50,     1] loss: 1308.838
[51,     1] loss: 1284.090
[52,     1] loss: 1330.905
[53,     1] loss: 1160.434
[54,     1] loss: 1282.542
[55,     1] loss: 1203.535
[56,     1] loss: 1266.052
[57,     1] loss: 1126.080
[58,     1] loss: 1138.127
[59,     1] loss: 1248.361
[60,     1] loss: 1144.025
[61,     1] loss: 1102.156
[62,     1] loss: 1177.985
[63,     1] loss: 1105.502
[64,     1] loss: 1065.546
[65,     1] loss: 1006.855
[66,     1] loss: 990.375
[67,     1] loss: 950.203
[68,     1] loss: 1049.853
[69,     1] loss: 1009.971
[70,     1] loss: 1112.042
[71,     1] loss: 1427.589
[72,     1] loss: 1298.765
[73,     1] loss: 1100.203
[74,     1] loss: 1085.005
[75,     1] loss: 1166.611
[76,     1] loss: 1072.715
[77,     1] loss: 1203.135
[78,     1] loss: 1012.895
[79,     1] loss: 1134.950
[80,     1] loss: 976.095
[81,     1] loss: 969.119
[82,     1] loss: 1036.861
[83,     1] loss: 1118.983
[84,     1] loss: 1003.603
[85,     1] loss: 947.630
[86,     1] loss: 927.459
[87,     1] loss: 892.178
[88,     1] loss: 835.658
Early stopping applied (best metric=0.4588615298271179)
Finished Training
Total time taken: 14.55023717880249
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1927.425
[2,     1] loss: 1924.796
[3,     1] loss: 1923.607
[4,     1] loss: 1935.136
[5,     1] loss: 1924.649
[6,     1] loss: 1928.620
[7,     1] loss: 1937.499
[8,     1] loss: 1919.170
[9,     1] loss: 1917.544
[10,     1] loss: 1930.798
[11,     1] loss: 1923.677
[12,     1] loss: 1912.628
[13,     1] loss: 1920.337
[14,     1] loss: 1907.910
[15,     1] loss: 1893.400
[16,     1] loss: 1872.319
[17,     1] loss: 1838.813
[18,     1] loss: 1789.490
[19,     1] loss: 1702.014
[20,     1] loss: 1693.022
[21,     1] loss: 1635.777
[22,     1] loss: 1676.414
[23,     1] loss: 1700.685
[24,     1] loss: 1552.373
[25,     1] loss: 1631.557
[26,     1] loss: 1618.201
[27,     1] loss: 1648.293
[28,     1] loss: 1583.191
[29,     1] loss: 1483.166
[30,     1] loss: 1525.263
[31,     1] loss: 1488.975
[32,     1] loss: 1439.705
[33,     1] loss: 1538.463
[34,     1] loss: 1554.498
[35,     1] loss: 1377.312
[36,     1] loss: 1420.509
[37,     1] loss: 1340.156
[38,     1] loss: 1396.926
[39,     1] loss: 1301.125
[40,     1] loss: 1354.637
[41,     1] loss: 1229.522
[42,     1] loss: 1266.010
[43,     1] loss: 1302.501
[44,     1] loss: 1264.578
[45,     1] loss: 1194.912
[46,     1] loss: 1158.179
[47,     1] loss: 1152.753
[48,     1] loss: 1074.868
[49,     1] loss: 1019.915
[50,     1] loss: 1139.952
[51,     1] loss: 1369.980
[52,     1] loss: 1604.689
[53,     1] loss: 1055.553
[54,     1] loss: 1411.752
[55,     1] loss: 1247.693
[56,     1] loss: 1246.796
[57,     1] loss: 1376.423
[58,     1] loss: 1255.807
[59,     1] loss: 1158.419
[60,     1] loss: 1402.863
[61,     1] loss: 1077.721
[62,     1] loss: 1110.363
[63,     1] loss: 1191.159
[64,     1] loss: 1086.870
[65,     1] loss: 1146.355
[66,     1] loss: 1178.851
[67,     1] loss: 1161.007
[68,     1] loss: 1185.733
[69,     1] loss: 989.679
[70,     1] loss: 1182.268
[71,     1] loss: 1000.001
[72,     1] loss: 1058.590
[73,     1] loss: 1044.230
[74,     1] loss: 895.841
[75,     1] loss: 1013.221
[76,     1] loss: 917.568
[77,     1] loss: 933.009
[78,     1] loss: 889.619
Early stopping applied (best metric=0.4729570746421814)
Finished Training
Total time taken: 14.530424118041992
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1927.495
[2,     1] loss: 1922.248
[3,     1] loss: 1925.991
[4,     1] loss: 1936.082
[5,     1] loss: 1929.482
[6,     1] loss: 1921.794
[7,     1] loss: 1927.325
[8,     1] loss: 1920.139
[9,     1] loss: 1923.705
[10,     1] loss: 1919.605
[11,     1] loss: 1919.936
[12,     1] loss: 1934.656
[13,     1] loss: 1921.097
[14,     1] loss: 1924.327
[15,     1] loss: 1915.552
[16,     1] loss: 1913.733
[17,     1] loss: 1907.734
[18,     1] loss: 1897.774
[19,     1] loss: 1873.279
[20,     1] loss: 1846.219
[21,     1] loss: 1830.897
[22,     1] loss: 1789.448
[23,     1] loss: 1740.836
[24,     1] loss: 1708.154
[25,     1] loss: 1645.101
[26,     1] loss: 1628.397
[27,     1] loss: 1621.320
[28,     1] loss: 1620.706
[29,     1] loss: 1656.065
[30,     1] loss: 1496.529
[31,     1] loss: 1578.036
[32,     1] loss: 1498.403
[33,     1] loss: 1562.518
[34,     1] loss: 1544.890
[35,     1] loss: 1523.610
[36,     1] loss: 1525.079
[37,     1] loss: 1488.186
[38,     1] loss: 1451.180
[39,     1] loss: 1423.432
[40,     1] loss: 1535.819
[41,     1] loss: 1499.387
[42,     1] loss: 1428.421
[43,     1] loss: 1419.197
[44,     1] loss: 1331.846
[45,     1] loss: 1508.435
[46,     1] loss: 1321.996
[47,     1] loss: 1278.289
[48,     1] loss: 1286.505
[49,     1] loss: 1210.871
[50,     1] loss: 1182.396
[51,     1] loss: 1177.174
[52,     1] loss: 1172.535
[53,     1] loss: 1195.360
[54,     1] loss: 1167.822
[55,     1] loss: 1178.076
[56,     1] loss: 1173.349
[57,     1] loss: 1203.942
[58,     1] loss: 1066.470
[59,     1] loss: 1102.071
[60,     1] loss: 1054.694
[61,     1] loss: 1014.564
[62,     1] loss: 1062.128
[63,     1] loss: 1021.318
[64,     1] loss: 1004.621
[65,     1] loss: 996.823
[66,     1] loss: 938.950
[67,     1] loss: 1008.424
[68,     1] loss: 971.515
[69,     1] loss: 1029.593
[70,     1] loss: 939.042
[71,     1] loss: 892.419
[72,     1] loss: 998.858
[73,     1] loss: 891.323
[74,     1] loss: 1059.658
[75,     1] loss: 1672.105
[76,     1] loss: 905.110
[77,     1] loss: 1333.497
[78,     1] loss: 1033.613
[79,     1] loss: 1315.566
[80,     1] loss: 1163.176
[81,     1] loss: 1075.950
[82,     1] loss: 1202.836
[83,     1] loss: 1075.232
[84,     1] loss: 1022.566
[85,     1] loss: 957.750
[86,     1] loss: 1170.594
[87,     1] loss: 899.934
[88,     1] loss: 1099.639
[89,     1] loss: 875.363
[90,     1] loss: 1000.979
[91,     1] loss: 967.132
[92,     1] loss: 942.698
[93,     1] loss: 947.651
[94,     1] loss: 845.424
[95,     1] loss: 789.113
[96,     1] loss: 801.558
[97,     1] loss: 746.773
[98,     1] loss: 730.091
[99,     1] loss: 718.096
[100,     1] loss: 817.413
[101,     1] loss: 856.933
[102,     1] loss: 798.390
[103,     1] loss: 699.306
[104,     1] loss: 845.073
[105,     1] loss: 805.495
[106,     1] loss: 853.541
[107,     1] loss: 864.413
[108,     1] loss: 740.082
[109,     1] loss: 776.738
[110,     1] loss: 697.870
[111,     1] loss: 720.296
[112,     1] loss: 971.709
[113,     1] loss: 889.213
[114,     1] loss: 765.426
[115,     1] loss: 736.144
[116,     1] loss: 748.543
[117,     1] loss: 736.957
[118,     1] loss: 907.421
[119,     1] loss: 918.059
[120,     1] loss: 787.755
[121,     1] loss: 725.838
[122,     1] loss: 777.236
[123,     1] loss: 798.482
[124,     1] loss: 728.187
[125,     1] loss: 710.319
[126,     1] loss: 687.505
[127,     1] loss: 678.143
[128,     1] loss: 730.432
[129,     1] loss: 760.093
Early stopping applied (best metric=0.3816364109516144)
Finished Training
Total time taken: 22.978086709976196
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1928.631
[2,     1] loss: 1928.233
[3,     1] loss: 1923.200
[4,     1] loss: 1926.526
[5,     1] loss: 1936.571
[6,     1] loss: 1930.258
[7,     1] loss: 1927.355
[8,     1] loss: 1921.704
[9,     1] loss: 1925.056
[10,     1] loss: 1924.949
[11,     1] loss: 1918.612
[12,     1] loss: 1925.177
[13,     1] loss: 1917.251
[14,     1] loss: 1920.542
[15,     1] loss: 1919.833
[16,     1] loss: 1918.630
[17,     1] loss: 1916.695
[18,     1] loss: 1907.210
[19,     1] loss: 1897.038
[20,     1] loss: 1893.190
[21,     1] loss: 1877.866
[22,     1] loss: 1826.299
[23,     1] loss: 1775.966
[24,     1] loss: 1751.702
[25,     1] loss: 1727.205
[26,     1] loss: 1745.956
[27,     1] loss: 1682.665
[28,     1] loss: 1526.455
[29,     1] loss: 1695.575
[30,     1] loss: 1612.330
[31,     1] loss: 1580.603
[32,     1] loss: 1638.721
[33,     1] loss: 1550.186
[34,     1] loss: 1619.582
[35,     1] loss: 1589.874
[36,     1] loss: 1620.688
[37,     1] loss: 1619.790
[38,     1] loss: 1595.674
[39,     1] loss: 1509.619
[40,     1] loss: 1564.225
[41,     1] loss: 1481.499
[42,     1] loss: 1494.486
[43,     1] loss: 1507.310
[44,     1] loss: 1398.954
[45,     1] loss: 1405.164
[46,     1] loss: 1518.457
[47,     1] loss: 1314.753
[48,     1] loss: 1471.044
[49,     1] loss: 1304.333
[50,     1] loss: 1391.041
[51,     1] loss: 1257.452
[52,     1] loss: 1343.095
[53,     1] loss: 1330.595
[54,     1] loss: 1283.984
[55,     1] loss: 1292.506
[56,     1] loss: 1186.958
[57,     1] loss: 1253.006
[58,     1] loss: 1173.564
[59,     1] loss: 1141.129
[60,     1] loss: 1092.651
[61,     1] loss: 1152.581
[62,     1] loss: 1165.502
[63,     1] loss: 2112.997
[64,     1] loss: 1460.822
[65,     1] loss: 1195.577
[66,     1] loss: 1257.583
[67,     1] loss: 1302.521
[68,     1] loss: 1239.923
[69,     1] loss: 1211.814
[70,     1] loss: 1197.172
[71,     1] loss: 1148.272
[72,     1] loss: 1067.381
[73,     1] loss: 1087.487
[74,     1] loss: 998.983
[75,     1] loss: 1136.162
[76,     1] loss: 1036.805
[77,     1] loss: 960.329
[78,     1] loss: 1086.825
[79,     1] loss: 992.940
[80,     1] loss: 883.867
[81,     1] loss: 972.227
[82,     1] loss: 875.441
[83,     1] loss: 996.023
[84,     1] loss: 882.419
[85,     1] loss: 1087.019
[86,     1] loss: 1052.594
[87,     1] loss: 932.566
[88,     1] loss: 1063.610
[89,     1] loss: 900.169
[90,     1] loss: 1072.993
[91,     1] loss: 873.373
[92,     1] loss: 903.660
[93,     1] loss: 870.653
[94,     1] loss: 826.603
[95,     1] loss: 888.241
[96,     1] loss: 815.002
Early stopping applied (best metric=0.3808625042438507)
Finished Training
Total time taken: 16.110942363739014
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1934.344
[2,     1] loss: 1950.323
[3,     1] loss: 1921.722
[4,     1] loss: 1924.323
[5,     1] loss: 1939.517
[6,     1] loss: 1926.336
[7,     1] loss: 1918.057
[8,     1] loss: 1924.850
[9,     1] loss: 1924.967
[10,     1] loss: 1931.551
[11,     1] loss: 1927.845
[12,     1] loss: 1923.039
[13,     1] loss: 1927.211
[14,     1] loss: 1920.218
[15,     1] loss: 1927.995
[16,     1] loss: 1922.715
[17,     1] loss: 1924.253
[18,     1] loss: 1926.426
[19,     1] loss: 1923.832
[20,     1] loss: 1918.325
[21,     1] loss: 1924.326
[22,     1] loss: 1920.544
[23,     1] loss: 1925.552
[24,     1] loss: 1921.482
[25,     1] loss: 1921.969
[26,     1] loss: 1918.249
[27,     1] loss: 1921.026
[28,     1] loss: 1914.426
[29,     1] loss: 1907.776
[30,     1] loss: 1903.584
[31,     1] loss: 1877.583
[32,     1] loss: 1852.152
[33,     1] loss: 1838.526
[34,     1] loss: 1808.956
[35,     1] loss: 1747.747
[36,     1] loss: 1754.944
[37,     1] loss: 1685.952
[38,     1] loss: 1643.102
[39,     1] loss: 1676.951
[40,     1] loss: 1693.025
[41,     1] loss: 1642.719
[42,     1] loss: 1597.459
[43,     1] loss: 1610.683
[44,     1] loss: 1638.941
[45,     1] loss: 1576.150
[46,     1] loss: 1629.009
[47,     1] loss: 1608.801
[48,     1] loss: 1591.982
[49,     1] loss: 1568.669
[50,     1] loss: 1525.030
[51,     1] loss: 1458.186
[52,     1] loss: 1483.486
[53,     1] loss: 1440.446
[54,     1] loss: 1461.245
[55,     1] loss: 1488.462
[56,     1] loss: 1480.161
[57,     1] loss: 1447.211
[58,     1] loss: 1499.096
[59,     1] loss: 1475.686
[60,     1] loss: 1401.205
[61,     1] loss: 1385.564
[62,     1] loss: 1334.818
[63,     1] loss: 1256.145
[64,     1] loss: 1298.322
[65,     1] loss: 1524.476
[66,     1] loss: 1570.435
[67,     1] loss: 1256.274
[68,     1] loss: 1504.779
[69,     1] loss: 1272.582
[70,     1] loss: 1359.254
[71,     1] loss: 1237.513
[72,     1] loss: 1279.550
[73,     1] loss: 1244.648
[74,     1] loss: 1075.702
[75,     1] loss: 1250.902
[76,     1] loss: 1060.664
[77,     1] loss: 1248.287
[78,     1] loss: 1132.272
[79,     1] loss: 1061.547
[80,     1] loss: 1184.856
[81,     1] loss: 1161.208
[82,     1] loss: 1197.621
[83,     1] loss: 1032.391
[84,     1] loss: 1004.694
[85,     1] loss: 1123.350
[86,     1] loss: 1061.100
[87,     1] loss: 1101.787
[88,     1] loss: 1249.307
[89,     1] loss: 1018.074
[90,     1] loss: 1071.062
[91,     1] loss: 1055.236
[92,     1] loss: 1048.532
[93,     1] loss: 1186.443
[94,     1] loss: 983.629
[95,     1] loss: 1109.117
[96,     1] loss: 958.805
[97,     1] loss: 1032.919
[98,     1] loss: 913.427
[99,     1] loss: 986.308
[100,     1] loss: 895.640
[101,     1] loss: 940.062
[102,     1] loss: 917.781
[103,     1] loss: 1120.439
[104,     1] loss: 1315.459
[105,     1] loss: 939.239
[106,     1] loss: 1257.587
[107,     1] loss: 1051.271
[108,     1] loss: 1155.914
[109,     1] loss: 982.417
[110,     1] loss: 1090.537
[111,     1] loss: 1025.265
[112,     1] loss: 915.132
Early stopping applied (best metric=0.3751358389854431)
Finished Training
Total time taken: 19.52240538597107
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1929.458
[2,     1] loss: 1925.826
[3,     1] loss: 1926.302
[4,     1] loss: 1925.528
[5,     1] loss: 1918.890
[6,     1] loss: 1922.467
[7,     1] loss: 1915.816
[8,     1] loss: 1918.639
[9,     1] loss: 1911.089
[10,     1] loss: 1908.830
[11,     1] loss: 1896.270
[12,     1] loss: 1869.948
[13,     1] loss: 1825.808
[14,     1] loss: 1771.682
[15,     1] loss: 1771.265
[16,     1] loss: 1672.025
[17,     1] loss: 1619.829
[18,     1] loss: 1626.997
[19,     1] loss: 1647.000
[20,     1] loss: 1691.485
[21,     1] loss: 1626.483
[22,     1] loss: 1606.410
[23,     1] loss: 1601.262
[24,     1] loss: 1641.021
[25,     1] loss: 1628.357
[26,     1] loss: 1564.122
[27,     1] loss: 1540.441
[28,     1] loss: 1554.187
[29,     1] loss: 1525.967
[30,     1] loss: 1397.276
[31,     1] loss: 1538.060
[32,     1] loss: 1482.883
[33,     1] loss: 1358.476
[34,     1] loss: 1431.134
[35,     1] loss: 1370.155
[36,     1] loss: 1303.078
[37,     1] loss: 1346.062
[38,     1] loss: 1305.871
[39,     1] loss: 1305.745
[40,     1] loss: 1275.217
[41,     1] loss: 1327.257
[42,     1] loss: 1437.380
[43,     1] loss: 1226.432
[44,     1] loss: 1325.993
[45,     1] loss: 1431.685
[46,     1] loss: 1195.125
[47,     1] loss: 1329.635
[48,     1] loss: 1237.094
[49,     1] loss: 1279.403
[50,     1] loss: 1138.440
[51,     1] loss: 1247.789
[52,     1] loss: 1123.486
[53,     1] loss: 1130.132
[54,     1] loss: 1194.686
[55,     1] loss: 1036.081
[56,     1] loss: 1180.117
[57,     1] loss: 1003.993
[58,     1] loss: 990.810
[59,     1] loss: 978.797
[60,     1] loss: 945.109
[61,     1] loss: 997.273
[62,     1] loss: 892.469
[63,     1] loss: 1003.208
[64,     1] loss: 1015.126
[65,     1] loss: 986.609
[66,     1] loss: 948.637
[67,     1] loss: 1005.650
[68,     1] loss: 1244.846
[69,     1] loss: 1200.834
[70,     1] loss: 1123.896
[71,     1] loss: 1015.509
[72,     1] loss: 1013.275
[73,     1] loss: 1110.477
[74,     1] loss: 943.899
[75,     1] loss: 923.510
[76,     1] loss: 866.859
[77,     1] loss: 948.801
[78,     1] loss: 872.883
[79,     1] loss: 849.445
[80,     1] loss: 904.476
[81,     1] loss: 808.982
[82,     1] loss: 842.529
[83,     1] loss: 928.861
[84,     1] loss: 1302.658
Early stopping applied (best metric=0.38301584124565125)
Finished Training
Total time taken: 14.928864002227783
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1933.938
[2,     1] loss: 1931.768
[3,     1] loss: 1915.967
[4,     1] loss: 1932.631
[5,     1] loss: 1924.717
[6,     1] loss: 1942.414
[7,     1] loss: 1923.579
[8,     1] loss: 1920.477
[9,     1] loss: 1919.409
[10,     1] loss: 1908.139
[11,     1] loss: 1894.447
[12,     1] loss: 1866.504
[13,     1] loss: 1830.234
[14,     1] loss: 1773.683
[15,     1] loss: 1717.921
[16,     1] loss: 1674.398
[17,     1] loss: 1664.750
[18,     1] loss: 1668.357
[19,     1] loss: 1641.611
[20,     1] loss: 1675.671
[21,     1] loss: 1588.338
[22,     1] loss: 1537.188
[23,     1] loss: 1578.292
[24,     1] loss: 1636.391
[25,     1] loss: 1628.300
[26,     1] loss: 1573.559
[27,     1] loss: 1544.137
[28,     1] loss: 1561.867
[29,     1] loss: 1491.680
[30,     1] loss: 1597.884
[31,     1] loss: 1484.174
[32,     1] loss: 1563.432
[33,     1] loss: 1530.188
[34,     1] loss: 1550.493
[35,     1] loss: 1481.457
[36,     1] loss: 1564.328
[37,     1] loss: 1556.011
[38,     1] loss: 1496.734
[39,     1] loss: 1461.847
[40,     1] loss: 1500.989
[41,     1] loss: 1515.298
[42,     1] loss: 1505.453
[43,     1] loss: 1401.088
[44,     1] loss: 1461.280
[45,     1] loss: 1375.742
[46,     1] loss: 1388.016
[47,     1] loss: 1223.294
[48,     1] loss: 1363.531
[49,     1] loss: 1252.004
[50,     1] loss: 1350.832
[51,     1] loss: 1171.857
[52,     1] loss: 1165.212
[53,     1] loss: 1235.207
[54,     1] loss: 1118.274
[55,     1] loss: 1123.451
[56,     1] loss: 1180.097
[57,     1] loss: 1115.425
[58,     1] loss: 1286.026
[59,     1] loss: 1558.483
[60,     1] loss: 1188.474
[61,     1] loss: 1203.874
[62,     1] loss: 1188.089
[63,     1] loss: 1190.741
[64,     1] loss: 1102.044
[65,     1] loss: 1120.102
[66,     1] loss: 1064.994
[67,     1] loss: 1062.841
[68,     1] loss: 1046.360
[69,     1] loss: 1069.291
[70,     1] loss: 1045.191
[71,     1] loss: 931.418
[72,     1] loss: 1033.769
[73,     1] loss: 967.831
[74,     1] loss: 1303.110
[75,     1] loss: 1159.144
[76,     1] loss: 1002.917
[77,     1] loss: 1099.415
[78,     1] loss: 936.757
[79,     1] loss: 1177.009
[80,     1] loss: 1003.287
[81,     1] loss: 1163.317
[82,     1] loss: 907.289
[83,     1] loss: 1144.207
[84,     1] loss: 1006.170
[85,     1] loss: 976.538
[86,     1] loss: 1014.425
[87,     1] loss: 848.415
[88,     1] loss: 942.909
[89,     1] loss: 825.001
[90,     1] loss: 963.022
[91,     1] loss: 793.856
[92,     1] loss: 806.033
[93,     1] loss: 735.961
[94,     1] loss: 738.560
[95,     1] loss: 775.781
[96,     1] loss: 822.857
[97,     1] loss: 907.986
[98,     1] loss: 783.914
[99,     1] loss: 967.490
[100,     1] loss: 905.168
[101,     1] loss: 853.493
[102,     1] loss: 948.497
[103,     1] loss: 788.575
[104,     1] loss: 891.894
[105,     1] loss: 738.028
Early stopping applied (best metric=0.3440486490726471)
Finished Training
Total time taken: 15.973990678787231
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1932.042
[2,     1] loss: 1927.000
[3,     1] loss: 1953.374
[4,     1] loss: 1919.608
[5,     1] loss: 1931.689
[6,     1] loss: 1922.490
[7,     1] loss: 1904.597
[8,     1] loss: 1911.002
[9,     1] loss: 1907.198
[10,     1] loss: 1911.548
[11,     1] loss: 1899.324
[12,     1] loss: 1876.315
[13,     1] loss: 1857.139
[14,     1] loss: 1793.686
[15,     1] loss: 1770.346
[16,     1] loss: 1781.291
[17,     1] loss: 1704.755
[18,     1] loss: 1710.269
[19,     1] loss: 1673.525
[20,     1] loss: 1595.394
[21,     1] loss: 1648.348
[22,     1] loss: 1625.009
[23,     1] loss: 1544.506
[24,     1] loss: 1673.744
[25,     1] loss: 1630.917
[26,     1] loss: 1494.711
[27,     1] loss: 1587.313
[28,     1] loss: 1579.909
[29,     1] loss: 1494.502
[30,     1] loss: 1601.768
[31,     1] loss: 1483.743
[32,     1] loss: 1582.176
[33,     1] loss: 1480.878
[34,     1] loss: 1480.258
[35,     1] loss: 1428.072
[36,     1] loss: 1359.862
[37,     1] loss: 1410.056
[38,     1] loss: 1420.504
[39,     1] loss: 1319.565
[40,     1] loss: 1498.162
[41,     1] loss: 1255.686
[42,     1] loss: 1258.538
[43,     1] loss: 1314.488
[44,     1] loss: 1250.908
[45,     1] loss: 1324.626
[46,     1] loss: 1341.006
[47,     1] loss: 1127.462
[48,     1] loss: 1110.474
[49,     1] loss: 1148.748
[50,     1] loss: 1103.811
[51,     1] loss: 1184.233
[52,     1] loss: 1171.076
[53,     1] loss: 1189.524
[54,     1] loss: 1107.228
[55,     1] loss: 1090.607
[56,     1] loss: 1176.262
[57,     1] loss: 1115.037
[58,     1] loss: 1125.799
[59,     1] loss: 1159.262
[60,     1] loss: 1060.111
[61,     1] loss: 1122.717
[62,     1] loss: 1196.559
[63,     1] loss: 1062.660
[64,     1] loss: 1025.136
[65,     1] loss: 1114.592
[66,     1] loss: 1029.186
[67,     1] loss: 945.880
[68,     1] loss: 1045.746
[69,     1] loss: 923.207
[70,     1] loss: 1029.932
[71,     1] loss: 935.705
[72,     1] loss: 1101.250
[73,     1] loss: 980.021
[74,     1] loss: 923.451
[75,     1] loss: 921.593
[76,     1] loss: 942.296
[77,     1] loss: 909.806
Early stopping applied (best metric=0.4593820869922638)
Finished Training
Total time taken: 11.066218137741089
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1922.549
[2,     1] loss: 1953.590
[3,     1] loss: 1929.160
[4,     1] loss: 1928.470
[5,     1] loss: 1923.482
[6,     1] loss: 1924.268
[7,     1] loss: 1929.202
[8,     1] loss: 1921.088
[9,     1] loss: 1920.464
[10,     1] loss: 1922.967
[11,     1] loss: 1925.478
[12,     1] loss: 1921.731
[13,     1] loss: 1926.281
[14,     1] loss: 1918.459
[15,     1] loss: 1917.633
[16,     1] loss: 1919.144
[17,     1] loss: 1909.348
[18,     1] loss: 1908.196
[19,     1] loss: 1893.671
[20,     1] loss: 1871.417
[21,     1] loss: 1831.183
[22,     1] loss: 1819.833
[23,     1] loss: 1744.451
[24,     1] loss: 1755.251
[25,     1] loss: 1634.028
[26,     1] loss: 1654.002
[27,     1] loss: 1677.026
[28,     1] loss: 1611.690
[29,     1] loss: 1581.197
[30,     1] loss: 1530.354
[31,     1] loss: 1688.541
[32,     1] loss: 1615.865
[33,     1] loss: 1576.762
[34,     1] loss: 1537.078
[35,     1] loss: 1593.325
[36,     1] loss: 1539.194
[37,     1] loss: 1614.073
[38,     1] loss: 1575.576
[39,     1] loss: 1487.075
[40,     1] loss: 1489.727
[41,     1] loss: 1420.010
[42,     1] loss: 1412.894
[43,     1] loss: 1436.460
[44,     1] loss: 1358.359
[45,     1] loss: 1485.044
[46,     1] loss: 1380.794
[47,     1] loss: 1451.350
[48,     1] loss: 1426.434
[49,     1] loss: 1327.376
[50,     1] loss: 1352.317
[51,     1] loss: 1177.511
[52,     1] loss: 1335.695
[53,     1] loss: 1180.088
[54,     1] loss: 1260.255
[55,     1] loss: 1164.193
[56,     1] loss: 1242.327
[57,     1] loss: 1156.713
[58,     1] loss: 1058.145
[59,     1] loss: 1101.166
[60,     1] loss: 1384.107
[61,     1] loss: 1324.204
[62,     1] loss: 1097.557
[63,     1] loss: 1135.134
[64,     1] loss: 1075.399
[65,     1] loss: 1167.409
[66,     1] loss: 1112.768
[67,     1] loss: 1160.128
[68,     1] loss: 1011.333
[69,     1] loss: 1170.582
[70,     1] loss: 1076.229
[71,     1] loss: 1006.793
[72,     1] loss: 1061.601
[73,     1] loss: 982.688
[74,     1] loss: 991.467
[75,     1] loss: 863.547
[76,     1] loss: 888.134
[77,     1] loss: 915.602
[78,     1] loss: 896.099
[79,     1] loss: 992.049
[80,     1] loss: 988.819
[81,     1] loss: 913.196
[82,     1] loss: 891.932
[83,     1] loss: 931.600
[84,     1] loss: 871.694
[85,     1] loss: 847.971
Early stopping applied (best metric=0.4446775019168854)
Finished Training
Total time taken: 11.833747625350952
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1935.019
[2,     1] loss: 1930.501
[3,     1] loss: 1937.715
[4,     1] loss: 1921.568
[5,     1] loss: 1929.021
[6,     1] loss: 1934.669
[7,     1] loss: 1922.186
[8,     1] loss: 1926.625
[9,     1] loss: 1927.127
[10,     1] loss: 1923.671
[11,     1] loss: 1926.241
[12,     1] loss: 1919.857
[13,     1] loss: 1917.484
[14,     1] loss: 1920.403
[15,     1] loss: 1916.142
[16,     1] loss: 1920.119
[17,     1] loss: 1908.957
[18,     1] loss: 1905.658
[19,     1] loss: 1889.341
[20,     1] loss: 1867.521
[21,     1] loss: 1841.662
[22,     1] loss: 1800.907
[23,     1] loss: 1812.610
[24,     1] loss: 1745.914
[25,     1] loss: 1722.979
[26,     1] loss: 1683.322
[27,     1] loss: 1602.658
[28,     1] loss: 1709.101
[29,     1] loss: 1628.373
[30,     1] loss: 1517.071
[31,     1] loss: 1673.418
[32,     1] loss: 1557.423
[33,     1] loss: 1568.534
[34,     1] loss: 1563.334
[35,     1] loss: 1618.147
[36,     1] loss: 1538.392
[37,     1] loss: 1553.517
[38,     1] loss: 1518.196
[39,     1] loss: 1445.810
[40,     1] loss: 1501.291
[41,     1] loss: 1443.740
[42,     1] loss: 1557.122
[43,     1] loss: 1441.609
[44,     1] loss: 1491.178
[45,     1] loss: 1309.783
[46,     1] loss: 1389.338
[47,     1] loss: 1305.727
[48,     1] loss: 1363.593
[49,     1] loss: 1308.928
[50,     1] loss: 1201.053
[51,     1] loss: 1182.111
[52,     1] loss: 1231.820
[53,     1] loss: 1070.360
[54,     1] loss: 1074.083
[55,     1] loss: 1210.710
[56,     1] loss: 1094.843
[57,     1] loss: 1191.035
[58,     1] loss: 1789.495
[59,     1] loss: 1241.214
[60,     1] loss: 1288.099
[61,     1] loss: 1219.137
[62,     1] loss: 1283.221
[63,     1] loss: 1192.298
[64,     1] loss: 1231.917
[65,     1] loss: 1181.060
[66,     1] loss: 1174.139
[67,     1] loss: 1146.094
[68,     1] loss: 1079.677
[69,     1] loss: 1049.389
[70,     1] loss: 1111.957
[71,     1] loss: 1077.472
[72,     1] loss: 1059.643
[73,     1] loss: 932.928
[74,     1] loss: 1019.882
[75,     1] loss: 991.002
[76,     1] loss: 956.077
[77,     1] loss: 933.841
[78,     1] loss: 1043.892
[79,     1] loss: 855.916
[80,     1] loss: 1040.934
[81,     1] loss: 1319.506
[82,     1] loss: 1350.687
[83,     1] loss: 962.280
Early stopping applied (best metric=0.3868771493434906)
Finished Training
Total time taken: 11.447247982025146
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1930.638
[2,     1] loss: 1921.662
[3,     1] loss: 1918.474
[4,     1] loss: 1943.386
[5,     1] loss: 1927.523
[6,     1] loss: 1928.166
[7,     1] loss: 1926.014
[8,     1] loss: 1923.752
[9,     1] loss: 1923.269
[10,     1] loss: 1924.115
[11,     1] loss: 1922.465
[12,     1] loss: 1919.840
[13,     1] loss: 1921.495
[14,     1] loss: 1915.637
[15,     1] loss: 1912.281
[16,     1] loss: 1901.071
[17,     1] loss: 1890.740
[18,     1] loss: 1860.497
[19,     1] loss: 1827.229
[20,     1] loss: 1797.854
[21,     1] loss: 1791.647
[22,     1] loss: 1698.315
[23,     1] loss: 1692.743
[24,     1] loss: 1696.323
[25,     1] loss: 1631.379
[26,     1] loss: 1630.309
[27,     1] loss: 1615.446
[28,     1] loss: 1561.653
[29,     1] loss: 1553.605
[30,     1] loss: 1568.691
[31,     1] loss: 1503.391
[32,     1] loss: 1559.754
[33,     1] loss: 1533.459
[34,     1] loss: 1408.739
[35,     1] loss: 1484.074
[36,     1] loss: 1401.532
[37,     1] loss: 1446.486
[38,     1] loss: 1309.421
[39,     1] loss: 1334.258
[40,     1] loss: 1380.284
[41,     1] loss: 1245.145
[42,     1] loss: 1300.295
[43,     1] loss: 1301.201
[44,     1] loss: 1081.220
[45,     1] loss: 1091.242
[46,     1] loss: 1251.800
[47,     1] loss: 1270.876
[48,     1] loss: 1371.071
[49,     1] loss: 1371.438
[50,     1] loss: 1181.675
[51,     1] loss: 1265.585
[52,     1] loss: 1386.677
[53,     1] loss: 1287.174
[54,     1] loss: 1232.319
[55,     1] loss: 1235.625
[56,     1] loss: 1104.057
[57,     1] loss: 1130.099
[58,     1] loss: 1119.219
[59,     1] loss: 1148.229
[60,     1] loss: 998.107
[61,     1] loss: 1146.563
[62,     1] loss: 1204.985
[63,     1] loss: 1137.512
[64,     1] loss: 1173.182
[65,     1] loss: 943.775
[66,     1] loss: 944.205
[67,     1] loss: 944.309
[68,     1] loss: 937.764
[69,     1] loss: 905.336
[70,     1] loss: 862.026
[71,     1] loss: 830.819
[72,     1] loss: 878.282
[73,     1] loss: 831.077
[74,     1] loss: 842.477
[75,     1] loss: 794.728
[76,     1] loss: 881.299
[77,     1] loss: 876.344
[78,     1] loss: 807.711
[79,     1] loss: 862.304
Early stopping applied (best metric=0.40282151103019714)
Finished Training
Total time taken: 11.65700364112854
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1931.353
[2,     1] loss: 1928.110
[3,     1] loss: 1930.990
[4,     1] loss: 1929.965
[5,     1] loss: 1925.931
[6,     1] loss: 1924.233
[7,     1] loss: 1923.485
[8,     1] loss: 1916.960
[9,     1] loss: 1921.671
[10,     1] loss: 1914.811
[11,     1] loss: 1915.636
[12,     1] loss: 1907.533
[13,     1] loss: 1875.930
[14,     1] loss: 1877.703
[15,     1] loss: 1826.817
[16,     1] loss: 1799.996
[17,     1] loss: 1742.699
[18,     1] loss: 1707.680
[19,     1] loss: 1698.360
[20,     1] loss: 1704.738
[21,     1] loss: 1684.458
[22,     1] loss: 1659.239
[23,     1] loss: 1698.783
[24,     1] loss: 1684.465
[25,     1] loss: 1596.906
[26,     1] loss: 1690.889
[27,     1] loss: 1589.651
[28,     1] loss: 1614.374
[29,     1] loss: 1568.641
[30,     1] loss: 1584.362
[31,     1] loss: 1496.922
[32,     1] loss: 1504.660
[33,     1] loss: 1497.447
[34,     1] loss: 1527.860
[35,     1] loss: 1488.902
[36,     1] loss: 1574.911
[37,     1] loss: 1446.080
[38,     1] loss: 1479.307
[39,     1] loss: 1477.016
[40,     1] loss: 1346.684
[41,     1] loss: 1438.194
[42,     1] loss: 1459.817
[43,     1] loss: 1424.399
[44,     1] loss: 1355.887
[45,     1] loss: 1347.675
[46,     1] loss: 1230.222
[47,     1] loss: 1309.197
[48,     1] loss: 1209.332
[49,     1] loss: 1291.120
[50,     1] loss: 1201.516
[51,     1] loss: 1174.990
[52,     1] loss: 1290.432
[53,     1] loss: 1192.317
[54,     1] loss: 1167.871
[55,     1] loss: 1221.717
[56,     1] loss: 1117.650
[57,     1] loss: 1173.035
[58,     1] loss: 1142.142
[59,     1] loss: 1230.143
[60,     1] loss: 1129.974
[61,     1] loss: 1237.643
[62,     1] loss: 1153.864
[63,     1] loss: 1120.823
[64,     1] loss: 1071.312
[65,     1] loss: 1100.102
[66,     1] loss: 1001.410
[67,     1] loss: 1106.589
[68,     1] loss: 1227.699
[69,     1] loss: 1085.270
[70,     1] loss: 1009.457
[71,     1] loss: 1214.524
[72,     1] loss: 1194.480
[73,     1] loss: 1084.656
[74,     1] loss: 1209.620
[75,     1] loss: 1121.405
[76,     1] loss: 1114.619
[77,     1] loss: 1201.135
[78,     1] loss: 1016.743
[79,     1] loss: 1127.295
[80,     1] loss: 1077.895
[81,     1] loss: 1083.475
[82,     1] loss: 903.412
[83,     1] loss: 875.000
[84,     1] loss: 898.580
[85,     1] loss: 838.902
[86,     1] loss: 890.950
Early stopping applied (best metric=0.3920508623123169)
Finished Training
Total time taken: 11.974787712097168
{'Hydroxylation-K Validation Accuracy': 0.7281619385342789, 'Hydroxylation-K Validation Sensitivity': 0.7496296296296296, 'Hydroxylation-K Validation Specificity': 0.7228070175438597, 'Hydroxylation-K Validation Precision': 0.415518461882912, 'Hydroxylation-K AUC ROC': 0.81729044834308, 'Hydroxylation-K AUC PR': 0.5834399795365803, 'Hydroxylation-K MCC': 0.398686443667045, 'Hydroxylation-K F1': 0.5290476434461941, 'Validation Loss (Hydroxylation-K)': 0.4584801952044169, 'Hydroxylation-P Validation Accuracy': 0.7624950002537942, 'Hydroxylation-P Validation Sensitivity': 0.7631746031746032, 'Hydroxylation-P Validation Specificity': 0.7622873958800938, 'Hydroxylation-P Validation Precision': 0.41338077919902033, 'Hydroxylation-P AUC ROC': 0.8262230808329117, 'Hydroxylation-P AUC PR': 0.5792460915433103, 'Hydroxylation-P MCC': 0.42930791571214527, 'Hydroxylation-P F1': 0.534399651832027, 'Validation Loss (Hydroxylation-P)': 0.39542376597722373, 'Validation Loss (total)': 0.8539039611816406, 'TimeToTrain': 15.554518047968546}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006053377446140383,
 'learning_rate_Hydroxylation-K': 0.0035116232363019122,
 'learning_rate_Hydroxylation-P': 0.007220528903337963,
 'log_base': 2.2412946566766845,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 432902057,
 'sample_weights': [4.828763536372673, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.950132093715668,
 'weight_decay_Hydroxylation-K': 0.21846496905956414,
 'weight_decay_Hydroxylation-P': 7.402515649957371}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.055
[2,     1] loss: 1344.297
[3,     1] loss: 1342.699
[4,     1] loss: 1340.417
[5,     1] loss: 1341.546
[6,     1] loss: 1340.998
[7,     1] loss: 1342.776
[8,     1] loss: 1343.864
[9,     1] loss: 1337.313
[10,     1] loss: 1327.023
[11,     1] loss: 1311.022
[12,     1] loss: 1281.677
[13,     1] loss: 1221.526
[14,     1] loss: 1190.692
[15,     1] loss: 1155.259
[16,     1] loss: 1137.092
[17,     1] loss: 1160.154
[18,     1] loss: 1134.874
[19,     1] loss: 1131.354
[20,     1] loss: 1117.388
[21,     1] loss: 1173.151
[22,     1] loss: 1091.417
[23,     1] loss: 1126.613
[24,     1] loss: 1079.379
[25,     1] loss: 1053.669
[26,     1] loss: 1068.737
[27,     1] loss: 1096.273
[28,     1] loss: 1000.745
[29,     1] loss: 997.498
[30,     1] loss: 942.613
[31,     1] loss: 941.430
[32,     1] loss: 966.751
[33,     1] loss: 961.240
[34,     1] loss: 963.240
[35,     1] loss: 1110.945
[36,     1] loss: 1136.196
[37,     1] loss: 942.329
[38,     1] loss: 972.431
[39,     1] loss: 1013.198
[40,     1] loss: 928.241
[41,     1] loss: 965.892
[42,     1] loss: 916.027
[43,     1] loss: 925.623
[44,     1] loss: 915.688
[45,     1] loss: 861.454
[46,     1] loss: 856.372
[47,     1] loss: 874.215
[48,     1] loss: 815.752
[49,     1] loss: 844.314
[50,     1] loss: 806.427
[51,     1] loss: 804.094
[52,     1] loss: 793.854
[53,     1] loss: 823.185
[54,     1] loss: 748.030
[55,     1] loss: 765.570
[56,     1] loss: 735.561
[57,     1] loss: 754.989
[58,     1] loss: 1009.309
[59,     1] loss: 1606.265
[60,     1] loss: 1065.644
[61,     1] loss: 1103.495
[62,     1] loss: 1120.622
[63,     1] loss: 1237.169
[64,     1] loss: 1134.865
[65,     1] loss: 1110.457
[66,     1] loss: 1085.846
[67,     1] loss: 1072.927
[68,     1] loss: 978.188
[69,     1] loss: 1095.114
[70,     1] loss: 1015.212
[71,     1] loss: 1025.861
[72,     1] loss: 980.622
[73,     1] loss: 1026.685
[74,     1] loss: 903.064
[75,     1] loss: 1000.770
[76,     1] loss: 877.038
[77,     1] loss: 923.386
[78,     1] loss: 887.709
[79,     1] loss: 911.139
[80,     1] loss: 809.843
[81,     1] loss: 858.926
[82,     1] loss: 853.748
Early stopping applied (best metric=0.3859512507915497)
Finished Training
Total time taken: 14.174222946166992
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1350.418
[2,     1] loss: 1344.668
[3,     1] loss: 1349.060
[4,     1] loss: 1341.208
[5,     1] loss: 1354.205
[6,     1] loss: 1342.215
[7,     1] loss: 1343.312
[8,     1] loss: 1340.029
[9,     1] loss: 1334.470
[10,     1] loss: 1321.609
[11,     1] loss: 1307.721
[12,     1] loss: 1280.102
[13,     1] loss: 1233.116
[14,     1] loss: 1204.903
[15,     1] loss: 1174.473
[16,     1] loss: 1141.291
[17,     1] loss: 1149.684
[18,     1] loss: 1135.609
[19,     1] loss: 1097.002
[20,     1] loss: 1141.308
[21,     1] loss: 1123.365
[22,     1] loss: 1062.612
[23,     1] loss: 1085.910
[24,     1] loss: 1080.331
[25,     1] loss: 1040.772
[26,     1] loss: 1035.754
[27,     1] loss: 1039.881
[28,     1] loss: 995.977
[29,     1] loss: 1021.028
[30,     1] loss: 1099.645
[31,     1] loss: 1142.103
[32,     1] loss: 1019.033
[33,     1] loss: 1069.967
[34,     1] loss: 1057.361
[35,     1] loss: 1013.119
[36,     1] loss: 975.525
[37,     1] loss: 1007.608
[38,     1] loss: 991.027
[39,     1] loss: 1051.348
[40,     1] loss: 920.805
[41,     1] loss: 951.966
[42,     1] loss: 954.076
[43,     1] loss: 916.234
[44,     1] loss: 866.551
[45,     1] loss: 970.081
[46,     1] loss: 914.783
[47,     1] loss: 844.515
[48,     1] loss: 872.964
[49,     1] loss: 846.301
[50,     1] loss: 815.401
[51,     1] loss: 789.026
[52,     1] loss: 848.910
[53,     1] loss: 791.910
[54,     1] loss: 1329.804
[55,     1] loss: 2016.745
[56,     1] loss: 1235.599
[57,     1] loss: 1248.040
[58,     1] loss: 1305.833
[59,     1] loss: 1323.042
[60,     1] loss: 1329.202
[61,     1] loss: 1326.537
[62,     1] loss: 1326.077
[63,     1] loss: 1323.691
[64,     1] loss: 1317.110
[65,     1] loss: 1317.727
[66,     1] loss: 1306.800
[67,     1] loss: 1294.428
[68,     1] loss: 1286.247
[69,     1] loss: 1266.568
[70,     1] loss: 1247.096
[71,     1] loss: 1220.628
[72,     1] loss: 1256.192
[73,     1] loss: 1223.615
[74,     1] loss: 1213.509
[75,     1] loss: 1228.385
[76,     1] loss: 1219.906
[77,     1] loss: 1239.782
[78,     1] loss: 1217.346
[79,     1] loss: 1201.727
[80,     1] loss: 1201.371
[81,     1] loss: 1186.210
[82,     1] loss: 1196.446
[83,     1] loss: 1221.087
[84,     1] loss: 1161.569
[85,     1] loss: 1201.529
[86,     1] loss: 1150.130
[87,     1] loss: 1133.955
[88,     1] loss: 1182.974
[89,     1] loss: 1151.138
[90,     1] loss: 1147.919
[91,     1] loss: 1135.289
[92,     1] loss: 1132.762
[93,     1] loss: 1149.506
[94,     1] loss: 1159.163
[95,     1] loss: 1152.709
[96,     1] loss: 1081.473
[97,     1] loss: 1210.394
[98,     1] loss: 1167.217
Early stopping applied (best metric=0.3734147548675537)
Finished Training
Total time taken: 16.218170881271362
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1345.753
[2,     1] loss: 1340.594
[3,     1] loss: 1346.479
[4,     1] loss: 1352.115
[5,     1] loss: 1337.294
[6,     1] loss: 1340.385
[7,     1] loss: 1343.946
[8,     1] loss: 1353.292
[9,     1] loss: 1327.257
[10,     1] loss: 1317.461
[11,     1] loss: 1291.901
[12,     1] loss: 1246.379
[13,     1] loss: 1203.905
[14,     1] loss: 1138.008
[15,     1] loss: 1148.158
[16,     1] loss: 1219.496
[17,     1] loss: 1104.625
[18,     1] loss: 1196.178
[19,     1] loss: 1125.377
[20,     1] loss: 1144.791
[21,     1] loss: 1078.989
[22,     1] loss: 1092.465
[23,     1] loss: 1086.943
[24,     1] loss: 1048.342
[25,     1] loss: 1070.212
[26,     1] loss: 1017.770
[27,     1] loss: 1083.737
[28,     1] loss: 974.355
[29,     1] loss: 1058.072
[30,     1] loss: 973.600
[31,     1] loss: 989.023
[32,     1] loss: 1013.550
[33,     1] loss: 968.272
[34,     1] loss: 962.299
[35,     1] loss: 952.006
[36,     1] loss: 988.075
[37,     1] loss: 901.570
[38,     1] loss: 982.201
[39,     1] loss: 874.634
[40,     1] loss: 906.677
[41,     1] loss: 886.869
[42,     1] loss: 825.869
[43,     1] loss: 876.181
[44,     1] loss: 1228.752
[45,     1] loss: 1404.595
[46,     1] loss: 1083.175
[47,     1] loss: 1060.028
[48,     1] loss: 1185.754
[49,     1] loss: 1162.551
[50,     1] loss: 1158.490
[51,     1] loss: 1152.323
[52,     1] loss: 1087.248
[53,     1] loss: 1074.340
[54,     1] loss: 1060.208
[55,     1] loss: 1098.270
[56,     1] loss: 1038.194
[57,     1] loss: 1140.650
[58,     1] loss: 1026.277
[59,     1] loss: 1032.055
[60,     1] loss: 1018.415
[61,     1] loss: 979.013
[62,     1] loss: 988.381
[63,     1] loss: 970.384
[64,     1] loss: 954.877
[65,     1] loss: 992.128
[66,     1] loss: 915.941
[67,     1] loss: 895.077
[68,     1] loss: 908.654
[69,     1] loss: 892.432
[70,     1] loss: 850.827
[71,     1] loss: 915.961
[72,     1] loss: 853.592
[73,     1] loss: 936.764
[74,     1] loss: 988.002
[75,     1] loss: 865.000
[76,     1] loss: 904.501
[77,     1] loss: 878.063
[78,     1] loss: 827.921
[79,     1] loss: 801.564
[80,     1] loss: 860.630
[81,     1] loss: 1016.559
[82,     1] loss: 1088.230
[83,     1] loss: 944.587
[84,     1] loss: 1042.243
[85,     1] loss: 1034.410
[86,     1] loss: 938.879
[87,     1] loss: 932.909
[88,     1] loss: 888.466
[89,     1] loss: 924.898
Early stopping applied (best metric=0.3953059911727905)
Finished Training
Total time taken: 12.31735873222351
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1341.208
[2,     1] loss: 1346.098
[3,     1] loss: 1340.463
[4,     1] loss: 1349.654
[5,     1] loss: 1343.529
[6,     1] loss: 1347.250
[7,     1] loss: 1342.512
[8,     1] loss: 1337.464
[9,     1] loss: 1333.151
[10,     1] loss: 1333.208
[11,     1] loss: 1318.524
[12,     1] loss: 1294.460
[13,     1] loss: 1259.093
[14,     1] loss: 1221.363
[15,     1] loss: 1179.042
[16,     1] loss: 1148.223
[17,     1] loss: 1152.595
[18,     1] loss: 1121.265
[19,     1] loss: 1153.144
[20,     1] loss: 1123.654
[21,     1] loss: 1041.288
[22,     1] loss: 1120.688
[23,     1] loss: 1093.321
[24,     1] loss: 1066.774
[25,     1] loss: 1049.842
[26,     1] loss: 995.549
[27,     1] loss: 1018.195
[28,     1] loss: 975.734
[29,     1] loss: 912.060
[30,     1] loss: 978.220
[31,     1] loss: 937.245
[32,     1] loss: 904.242
[33,     1] loss: 975.384
[34,     1] loss: 1004.309
[35,     1] loss: 925.015
[36,     1] loss: 974.035
[37,     1] loss: 946.618
[38,     1] loss: 916.163
[39,     1] loss: 903.648
[40,     1] loss: 887.126
[41,     1] loss: 829.926
[42,     1] loss: 824.725
[43,     1] loss: 1003.353
[44,     1] loss: 1906.741
[45,     1] loss: 1062.220
[46,     1] loss: 1043.878
[47,     1] loss: 1196.236
[48,     1] loss: 1203.918
[49,     1] loss: 1196.736
[50,     1] loss: 1212.318
[51,     1] loss: 1231.184
[52,     1] loss: 1197.837
[53,     1] loss: 1161.122
[54,     1] loss: 1150.489
[55,     1] loss: 1117.896
[56,     1] loss: 1117.528
[57,     1] loss: 1112.786
[58,     1] loss: 1067.047
[59,     1] loss: 1050.691
[60,     1] loss: 1063.107
[61,     1] loss: 1009.682
[62,     1] loss: 1008.259
[63,     1] loss: 955.396
[64,     1] loss: 966.210
[65,     1] loss: 950.286
[66,     1] loss: 936.202
[67,     1] loss: 970.800
[68,     1] loss: 952.665
[69,     1] loss: 930.333
[70,     1] loss: 907.539
[71,     1] loss: 1003.151
[72,     1] loss: 957.288
[73,     1] loss: 917.070
[74,     1] loss: 980.766
[75,     1] loss: 831.771
[76,     1] loss: 901.550
[77,     1] loss: 841.711
[78,     1] loss: 787.245
[79,     1] loss: 816.520
[80,     1] loss: 802.469
[81,     1] loss: 765.832
[82,     1] loss: 737.927
[83,     1] loss: 747.688
[84,     1] loss: 1136.818
[85,     1] loss: 2218.552
[86,     1] loss: 1413.876
[87,     1] loss: 1293.095
[88,     1] loss: 1323.862
[89,     1] loss: 1320.102
[90,     1] loss: 1310.353
[91,     1] loss: 1331.140
[92,     1] loss: 1314.776
[93,     1] loss: 1310.179
[94,     1] loss: 1322.219
[95,     1] loss: 1300.693
[96,     1] loss: 1298.735
[97,     1] loss: 1295.931
[98,     1] loss: 1280.855
[99,     1] loss: 1262.940
[100,     1] loss: 1236.988
[101,     1] loss: 1243.462
[102,     1] loss: 1232.179
[103,     1] loss: 1242.144
[104,     1] loss: 1189.878
[105,     1] loss: 1196.587
[106,     1] loss: 1201.555
[107,     1] loss: 1200.381
Early stopping applied (best metric=0.37552231550216675)
Finished Training
Total time taken: 14.65829610824585
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1349.253
[2,     1] loss: 1347.359
[3,     1] loss: 1350.656
[4,     1] loss: 1349.177
[5,     1] loss: 1347.412
[6,     1] loss: 1346.183
[7,     1] loss: 1345.122
[8,     1] loss: 1343.359
[9,     1] loss: 1340.969
[10,     1] loss: 1337.915
[11,     1] loss: 1330.262
[12,     1] loss: 1316.227
[13,     1] loss: 1289.044
[14,     1] loss: 1259.950
[15,     1] loss: 1225.193
[16,     1] loss: 1157.807
[17,     1] loss: 1184.492
[18,     1] loss: 1085.042
[19,     1] loss: 1108.807
[20,     1] loss: 1129.240
[21,     1] loss: 1088.733
[22,     1] loss: 1074.243
[23,     1] loss: 1041.028
[24,     1] loss: 1055.872
[25,     1] loss: 1066.144
[26,     1] loss: 1031.658
[27,     1] loss: 989.542
[28,     1] loss: 1030.207
[29,     1] loss: 1065.687
[30,     1] loss: 1117.755
[31,     1] loss: 975.544
[32,     1] loss: 1000.396
[33,     1] loss: 947.017
[34,     1] loss: 958.027
[35,     1] loss: 911.202
[36,     1] loss: 980.663
[37,     1] loss: 951.752
[38,     1] loss: 863.010
[39,     1] loss: 913.978
[40,     1] loss: 922.637
[41,     1] loss: 985.849
[42,     1] loss: 1071.600
[43,     1] loss: 972.128
[44,     1] loss: 999.367
[45,     1] loss: 969.977
[46,     1] loss: 1022.724
[47,     1] loss: 925.209
[48,     1] loss: 983.761
[49,     1] loss: 891.897
[50,     1] loss: 963.108
[51,     1] loss: 855.566
[52,     1] loss: 853.029
[53,     1] loss: 853.376
[54,     1] loss: 814.190
[55,     1] loss: 788.212
[56,     1] loss: 790.891
[57,     1] loss: 1113.835
[58,     1] loss: 1027.021
[59,     1] loss: 855.479
[60,     1] loss: 940.760
[61,     1] loss: 855.713
[62,     1] loss: 872.217
[63,     1] loss: 885.490
[64,     1] loss: 926.856
[65,     1] loss: 751.696
[66,     1] loss: 903.978
[67,     1] loss: 855.363
[68,     1] loss: 811.975
[69,     1] loss: 812.150
[70,     1] loss: 694.078
[71,     1] loss: 676.260
[72,     1] loss: 781.562
[73,     1] loss: 1370.471
[74,     1] loss: 997.454
[75,     1] loss: 941.702
[76,     1] loss: 968.279
[77,     1] loss: 904.345
[78,     1] loss: 836.115
[79,     1] loss: 982.644
[80,     1] loss: 944.209
[81,     1] loss: 887.669
[82,     1] loss: 920.866
[83,     1] loss: 850.257
[84,     1] loss: 890.932
Early stopping applied (best metric=0.4152281880378723)
Finished Training
Total time taken: 12.832812547683716
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.391
[2,     1] loss: 1343.922
[3,     1] loss: 1348.635
[4,     1] loss: 1343.329
[5,     1] loss: 1342.687
[6,     1] loss: 1335.929
[7,     1] loss: 1328.395
[8,     1] loss: 1306.715
[9,     1] loss: 1266.641
[10,     1] loss: 1225.587
[11,     1] loss: 1168.702
[12,     1] loss: 1138.405
[13,     1] loss: 1256.916
[14,     1] loss: 1163.034
[15,     1] loss: 1125.758
[16,     1] loss: 1082.417
[17,     1] loss: 1159.185
[18,     1] loss: 1085.699
[19,     1] loss: 1094.164
[20,     1] loss: 1124.866
[21,     1] loss: 1104.900
[22,     1] loss: 1030.023
[23,     1] loss: 1056.707
[24,     1] loss: 1050.912
[25,     1] loss: 1000.121
[26,     1] loss: 971.109
[27,     1] loss: 945.434
[28,     1] loss: 993.565
[29,     1] loss: 958.740
[30,     1] loss: 934.479
[31,     1] loss: 982.109
[32,     1] loss: 927.331
[33,     1] loss: 868.721
[34,     1] loss: 894.104
[35,     1] loss: 902.543
[36,     1] loss: 822.573
[37,     1] loss: 849.696
[38,     1] loss: 910.877
[39,     1] loss: 1110.836
[40,     1] loss: 905.910
[41,     1] loss: 1073.757
[42,     1] loss: 896.077
[43,     1] loss: 908.081
[44,     1] loss: 929.411
[45,     1] loss: 792.373
[46,     1] loss: 915.910
[47,     1] loss: 753.046
[48,     1] loss: 886.185
[49,     1] loss: 733.564
[50,     1] loss: 969.425
[51,     1] loss: 812.178
[52,     1] loss: 885.359
[53,     1] loss: 813.281
[54,     1] loss: 856.953
[55,     1] loss: 784.105
[56,     1] loss: 811.430
[57,     1] loss: 771.866
[58,     1] loss: 748.911
[59,     1] loss: 798.993
[60,     1] loss: 734.836
[61,     1] loss: 713.869
[62,     1] loss: 600.256
[63,     1] loss: 747.194
[64,     1] loss: 800.548
[65,     1] loss: 669.621
[66,     1] loss: 669.075
[67,     1] loss: 663.681
[68,     1] loss: 585.121
[69,     1] loss: 697.677
[70,     1] loss: 1702.357
[71,     1] loss: 759.868
[72,     1] loss: 1125.014
[73,     1] loss: 1013.262
[74,     1] loss: 1105.616
[75,     1] loss: 1112.420
[76,     1] loss: 1006.414
[77,     1] loss: 913.065
[78,     1] loss: 1048.468
[79,     1] loss: 894.414
[80,     1] loss: 893.264
[81,     1] loss: 857.810
[82,     1] loss: 835.536
[83,     1] loss: 823.836
[84,     1] loss: 822.972
[85,     1] loss: 875.745
[86,     1] loss: 941.920
[87,     1] loss: 790.535
[88,     1] loss: 931.888
[89,     1] loss: 776.696
[90,     1] loss: 842.559
[91,     1] loss: 767.783
[92,     1] loss: 812.690
[93,     1] loss: 642.968
Early stopping applied (best metric=0.4061259329319)
Finished Training
Total time taken: 14.199273347854614
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.696
[2,     1] loss: 1346.647
[3,     1] loss: 1345.778
[4,     1] loss: 1344.151
[5,     1] loss: 1341.919
[6,     1] loss: 1343.357
[7,     1] loss: 1343.309
[8,     1] loss: 1343.804
[9,     1] loss: 1342.464
[10,     1] loss: 1328.327
[11,     1] loss: 1322.530
[12,     1] loss: 1299.364
[13,     1] loss: 1267.065
[14,     1] loss: 1228.963
[15,     1] loss: 1197.988
[16,     1] loss: 1179.298
[17,     1] loss: 1121.464
[18,     1] loss: 1116.016
[19,     1] loss: 1141.760
[20,     1] loss: 1305.911
[21,     1] loss: 1060.470
[22,     1] loss: 1121.450
[23,     1] loss: 1102.605
[24,     1] loss: 1135.890
[25,     1] loss: 1097.083
[26,     1] loss: 1096.688
[27,     1] loss: 1094.179
[28,     1] loss: 1050.474
[29,     1] loss: 1055.035
[30,     1] loss: 995.851
[31,     1] loss: 1052.693
[32,     1] loss: 1018.171
[33,     1] loss: 1006.632
[34,     1] loss: 990.574
[35,     1] loss: 963.709
[36,     1] loss: 971.508
[37,     1] loss: 1050.360
[38,     1] loss: 1031.867
[39,     1] loss: 920.512
[40,     1] loss: 1044.279
[41,     1] loss: 968.993
[42,     1] loss: 999.292
[43,     1] loss: 926.134
[44,     1] loss: 903.699
[45,     1] loss: 886.655
[46,     1] loss: 911.439
[47,     1] loss: 892.359
[48,     1] loss: 963.415
[49,     1] loss: 1308.618
[50,     1] loss: 925.777
[51,     1] loss: 1164.239
[52,     1] loss: 1079.084
[53,     1] loss: 1064.884
[54,     1] loss: 1082.004
[55,     1] loss: 1088.111
[56,     1] loss: 1014.104
[57,     1] loss: 981.743
[58,     1] loss: 962.873
[59,     1] loss: 951.403
[60,     1] loss: 926.138
[61,     1] loss: 940.539
[62,     1] loss: 955.007
[63,     1] loss: 921.860
[64,     1] loss: 870.997
[65,     1] loss: 866.714
[66,     1] loss: 796.892
[67,     1] loss: 817.662
[68,     1] loss: 780.583
[69,     1] loss: 804.606
[70,     1] loss: 1178.299
[71,     1] loss: 1194.733
[72,     1] loss: 1078.304
[73,     1] loss: 1046.225
[74,     1] loss: 1119.118
[75,     1] loss: 1162.369
[76,     1] loss: 1066.322
[77,     1] loss: 1044.863
[78,     1] loss: 1066.861
[79,     1] loss: 1020.926
[80,     1] loss: 965.865
[81,     1] loss: 1006.606
[82,     1] loss: 902.963
[83,     1] loss: 1004.912
[84,     1] loss: 976.567
[85,     1] loss: 934.112
[86,     1] loss: 1003.409
[87,     1] loss: 858.173
[88,     1] loss: 916.306
[89,     1] loss: 798.062
[90,     1] loss: 883.859
[91,     1] loss: 887.965
[92,     1] loss: 784.764
[93,     1] loss: 892.677
[94,     1] loss: 855.647
[95,     1] loss: 770.789
[96,     1] loss: 810.426
Early stopping applied (best metric=0.29549577832221985)
Finished Training
Total time taken: 16.853936433792114
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.481
[2,     1] loss: 1347.354
[3,     1] loss: 1341.927
[4,     1] loss: 1346.042
[5,     1] loss: 1341.513
[6,     1] loss: 1338.007
[7,     1] loss: 1342.373
[8,     1] loss: 1326.650
[9,     1] loss: 1306.231
[10,     1] loss: 1270.947
[11,     1] loss: 1217.714
[12,     1] loss: 1187.899
[13,     1] loss: 1186.835
[14,     1] loss: 1190.075
[15,     1] loss: 1139.003
[16,     1] loss: 1202.666
[17,     1] loss: 1128.941
[18,     1] loss: 1234.449
[19,     1] loss: 1118.895
[20,     1] loss: 1164.440
[21,     1] loss: 1111.314
[22,     1] loss: 1110.914
[23,     1] loss: 1073.393
[24,     1] loss: 1106.748
[25,     1] loss: 1020.986
[26,     1] loss: 1139.985
[27,     1] loss: 990.144
[28,     1] loss: 1042.444
[29,     1] loss: 1000.087
[30,     1] loss: 1016.638
[31,     1] loss: 983.825
[32,     1] loss: 989.765
[33,     1] loss: 931.873
[34,     1] loss: 942.720
[35,     1] loss: 932.418
[36,     1] loss: 970.423
[37,     1] loss: 902.605
[38,     1] loss: 884.879
[39,     1] loss: 911.604
[40,     1] loss: 835.661
[41,     1] loss: 990.373
[42,     1] loss: 1046.937
[43,     1] loss: 869.750
[44,     1] loss: 978.797
[45,     1] loss: 939.873
[46,     1] loss: 934.590
[47,     1] loss: 929.878
[48,     1] loss: 871.658
[49,     1] loss: 891.922
[50,     1] loss: 806.186
[51,     1] loss: 868.467
[52,     1] loss: 934.038
[53,     1] loss: 790.593
[54,     1] loss: 791.177
[55,     1] loss: 816.349
[56,     1] loss: 695.612
[57,     1] loss: 746.791
[58,     1] loss: 852.648
[59,     1] loss: 1098.235
[60,     1] loss: 1228.378
[61,     1] loss: 1107.828
[62,     1] loss: 1108.400
[63,     1] loss: 1154.234
[64,     1] loss: 1105.412
[65,     1] loss: 1052.034
[66,     1] loss: 1020.672
[67,     1] loss: 996.780
[68,     1] loss: 999.439
[69,     1] loss: 942.220
[70,     1] loss: 941.458
[71,     1] loss: 1017.830
[72,     1] loss: 926.529
[73,     1] loss: 1004.258
[74,     1] loss: 921.112
[75,     1] loss: 888.103
[76,     1] loss: 886.268
[77,     1] loss: 993.522
[78,     1] loss: 871.888
[79,     1] loss: 861.576
[80,     1] loss: 812.870
[81,     1] loss: 891.600
[82,     1] loss: 851.365
[83,     1] loss: 833.595
[84,     1] loss: 827.846
[85,     1] loss: 744.360
[86,     1] loss: 826.094
[87,     1] loss: 1281.293
[88,     1] loss: 852.234
[89,     1] loss: 1182.353
[90,     1] loss: 1052.142
[91,     1] loss: 1093.487
[92,     1] loss: 1084.740
[93,     1] loss: 972.952
[94,     1] loss: 1008.580
[95,     1] loss: 967.308
[96,     1] loss: 978.785
[97,     1] loss: 978.527
[98,     1] loss: 931.401
[99,     1] loss: 901.015
[100,     1] loss: 871.063
[101,     1] loss: 888.486
[102,     1] loss: 797.639
[103,     1] loss: 787.398
[104,     1] loss: 782.218
[105,     1] loss: 781.246
[106,     1] loss: 733.523
[107,     1] loss: 761.669
[108,     1] loss: 824.378
[109,     1] loss: 956.880
[110,     1] loss: 727.503
[111,     1] loss: 923.235
[112,     1] loss: 982.860
[113,     1] loss: 822.807
[114,     1] loss: 920.893
[115,     1] loss: 784.449
[116,     1] loss: 829.605
[117,     1] loss: 658.346
[118,     1] loss: 751.323
[119,     1] loss: 917.435
[120,     1] loss: 740.230
[121,     1] loss: 716.500
[122,     1] loss: 758.084
[123,     1] loss: 680.942
[124,     1] loss: 703.424
[125,     1] loss: 668.276
Early stopping applied (best metric=0.3749891221523285)
Finished Training
Total time taken: 22.248894691467285
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1345.606
[2,     1] loss: 1342.717
[3,     1] loss: 1345.053
[4,     1] loss: 1346.166
[5,     1] loss: 1346.401
[6,     1] loss: 1339.519
[7,     1] loss: 1333.910
[8,     1] loss: 1322.261
[9,     1] loss: 1310.832
[10,     1] loss: 1297.455
[11,     1] loss: 1268.548
[12,     1] loss: 1222.632
[13,     1] loss: 1180.827
[14,     1] loss: 1158.936
[15,     1] loss: 1192.552
[16,     1] loss: 1194.753
[17,     1] loss: 1124.774
[18,     1] loss: 1222.764
[19,     1] loss: 1105.113
[20,     1] loss: 1182.242
[21,     1] loss: 1157.019
[22,     1] loss: 1128.420
[23,     1] loss: 1135.217
[24,     1] loss: 1068.062
[25,     1] loss: 1122.682
[26,     1] loss: 1088.245
[27,     1] loss: 1107.373
[28,     1] loss: 1027.217
[29,     1] loss: 1003.338
[30,     1] loss: 1022.519
[31,     1] loss: 975.401
[32,     1] loss: 942.776
[33,     1] loss: 957.832
[34,     1] loss: 1035.485
[35,     1] loss: 967.660
[36,     1] loss: 949.992
[37,     1] loss: 956.520
[38,     1] loss: 1054.636
[39,     1] loss: 984.539
[40,     1] loss: 912.215
[41,     1] loss: 968.383
[42,     1] loss: 896.687
[43,     1] loss: 962.083
[44,     1] loss: 857.783
[45,     1] loss: 906.820
[46,     1] loss: 916.589
[47,     1] loss: 931.339
[48,     1] loss: 850.508
[49,     1] loss: 792.595
[50,     1] loss: 889.784
[51,     1] loss: 1384.944
[52,     1] loss: 1091.292
[53,     1] loss: 998.702
[54,     1] loss: 1034.451
[55,     1] loss: 1094.622
[56,     1] loss: 1092.743
[57,     1] loss: 1023.390
[58,     1] loss: 951.987
[59,     1] loss: 1024.161
[60,     1] loss: 965.527
[61,     1] loss: 1016.521
[62,     1] loss: 968.287
[63,     1] loss: 951.741
[64,     1] loss: 985.139
[65,     1] loss: 950.231
[66,     1] loss: 933.507
[67,     1] loss: 885.452
[68,     1] loss: 887.204
[69,     1] loss: 806.473
[70,     1] loss: 878.402
[71,     1] loss: 805.889
[72,     1] loss: 754.438
[73,     1] loss: 758.500
[74,     1] loss: 728.400
[75,     1] loss: 795.246
[76,     1] loss: 1314.247
[77,     1] loss: 1804.799
[78,     1] loss: 1214.509
[79,     1] loss: 1307.798
[80,     1] loss: 1336.363
[81,     1] loss: 1342.260
[82,     1] loss: 1341.767
[83,     1] loss: 1342.490
[84,     1] loss: 1337.914
[85,     1] loss: 1344.612
[86,     1] loss: 1339.729
[87,     1] loss: 1337.192
[88,     1] loss: 1339.099
[89,     1] loss: 1341.884
[90,     1] loss: 1320.858
[91,     1] loss: 1337.980
[92,     1] loss: 1328.216
[93,     1] loss: 1330.766
[94,     1] loss: 1327.453
[95,     1] loss: 1323.521
[96,     1] loss: 1313.704
[97,     1] loss: 1284.255
[98,     1] loss: 1311.635
Early stopping applied (best metric=0.2913682460784912)
Finished Training
Total time taken: 16.317347764968872
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1347.217
[2,     1] loss: 1344.829
[3,     1] loss: 1353.538
[4,     1] loss: 1349.009
[5,     1] loss: 1344.842
[6,     1] loss: 1343.213
[7,     1] loss: 1344.810
[8,     1] loss: 1344.767
[9,     1] loss: 1339.428
[10,     1] loss: 1340.563
[11,     1] loss: 1329.831
[12,     1] loss: 1297.992
[13,     1] loss: 1257.374
[14,     1] loss: 1229.637
[15,     1] loss: 1180.200
[16,     1] loss: 1140.872
[17,     1] loss: 1116.616
[18,     1] loss: 1111.902
[19,     1] loss: 1114.721
[20,     1] loss: 1067.703
[21,     1] loss: 1089.279
[22,     1] loss: 1185.247
[23,     1] loss: 1060.891
[24,     1] loss: 1049.707
[25,     1] loss: 1033.738
[26,     1] loss: 1010.951
[27,     1] loss: 1029.841
[28,     1] loss: 955.093
[29,     1] loss: 934.249
[30,     1] loss: 950.912
[31,     1] loss: 940.474
[32,     1] loss: 996.322
[33,     1] loss: 1242.522
[34,     1] loss: 927.356
[35,     1] loss: 1100.331
[36,     1] loss: 1033.764
[37,     1] loss: 1028.234
[38,     1] loss: 999.289
[39,     1] loss: 1042.089
[40,     1] loss: 940.509
[41,     1] loss: 972.625
[42,     1] loss: 960.725
[43,     1] loss: 902.631
[44,     1] loss: 849.958
[45,     1] loss: 910.182
[46,     1] loss: 869.068
[47,     1] loss: 854.420
[48,     1] loss: 870.944
[49,     1] loss: 772.047
[50,     1] loss: 732.622
[51,     1] loss: 723.836
[52,     1] loss: 848.936
[53,     1] loss: 1402.481
[54,     1] loss: 1552.025
[55,     1] loss: 982.457
[56,     1] loss: 1067.405
[57,     1] loss: 1163.962
[58,     1] loss: 1170.432
[59,     1] loss: 1139.024
[60,     1] loss: 1118.984
[61,     1] loss: 1098.481
[62,     1] loss: 1064.615
[63,     1] loss: 1117.044
[64,     1] loss: 1015.873
[65,     1] loss: 1028.772
[66,     1] loss: 999.649
[67,     1] loss: 955.768
[68,     1] loss: 951.596
[69,     1] loss: 939.107
[70,     1] loss: 932.341
[71,     1] loss: 957.049
[72,     1] loss: 946.889
[73,     1] loss: 901.098
[74,     1] loss: 900.954
[75,     1] loss: 900.617
[76,     1] loss: 921.783
[77,     1] loss: 865.715
[78,     1] loss: 917.359
Early stopping applied (best metric=0.383777379989624)
Finished Training
Total time taken: 10.557247400283813
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1354.254
[2,     1] loss: 1337.658
[3,     1] loss: 1348.794
[4,     1] loss: 1351.109
[5,     1] loss: 1345.167
[6,     1] loss: 1342.680
[7,     1] loss: 1342.312
[8,     1] loss: 1341.559
[9,     1] loss: 1338.128
[10,     1] loss: 1336.784
[11,     1] loss: 1332.721
[12,     1] loss: 1320.090
[13,     1] loss: 1286.063
[14,     1] loss: 1261.752
[15,     1] loss: 1237.962
[16,     1] loss: 1182.900
[17,     1] loss: 1181.153
[18,     1] loss: 1206.240
[19,     1] loss: 1168.703
[20,     1] loss: 1174.339
[21,     1] loss: 1166.551
[22,     1] loss: 1143.711
[23,     1] loss: 1179.232
[24,     1] loss: 1087.736
[25,     1] loss: 1121.810
[26,     1] loss: 1076.370
[27,     1] loss: 1028.614
[28,     1] loss: 1026.928
[29,     1] loss: 1000.519
[30,     1] loss: 1031.182
[31,     1] loss: 1002.974
[32,     1] loss: 1054.959
[33,     1] loss: 997.233
[34,     1] loss: 1034.280
[35,     1] loss: 983.635
[36,     1] loss: 943.349
[37,     1] loss: 1069.220
[38,     1] loss: 933.334
[39,     1] loss: 943.403
[40,     1] loss: 871.945
[41,     1] loss: 985.884
[42,     1] loss: 943.634
[43,     1] loss: 905.939
[44,     1] loss: 870.352
[45,     1] loss: 873.499
[46,     1] loss: 834.351
[47,     1] loss: 844.975
[48,     1] loss: 809.119
[49,     1] loss: 939.812
[50,     1] loss: 1247.076
[51,     1] loss: 954.260
[52,     1] loss: 1060.651
[53,     1] loss: 1045.584
[54,     1] loss: 1001.597
[55,     1] loss: 950.007
[56,     1] loss: 969.295
[57,     1] loss: 970.862
[58,     1] loss: 968.415
[59,     1] loss: 942.483
[60,     1] loss: 945.098
[61,     1] loss: 852.347
[62,     1] loss: 973.725
[63,     1] loss: 909.781
[64,     1] loss: 927.666
[65,     1] loss: 869.562
[66,     1] loss: 925.633
[67,     1] loss: 978.663
[68,     1] loss: 771.899
[69,     1] loss: 854.722
[70,     1] loss: 771.334
[71,     1] loss: 816.291
[72,     1] loss: 1086.695
[73,     1] loss: 1178.193
[74,     1] loss: 823.613
[75,     1] loss: 954.860
[76,     1] loss: 901.632
[77,     1] loss: 975.210
[78,     1] loss: 922.517
[79,     1] loss: 1033.006
[80,     1] loss: 854.354
[81,     1] loss: 964.104
[82,     1] loss: 860.555
[83,     1] loss: 842.097
[84,     1] loss: 817.964
[85,     1] loss: 856.680
[86,     1] loss: 850.041
[87,     1] loss: 737.665
[88,     1] loss: 781.615
[89,     1] loss: 730.652
[90,     1] loss: 669.627
[91,     1] loss: 702.428
[92,     1] loss: 722.578
[93,     1] loss: 885.876
[94,     1] loss: 2415.365
[95,     1] loss: 1075.528
[96,     1] loss: 1199.110
[97,     1] loss: 1223.519
[98,     1] loss: 1195.791
[99,     1] loss: 1203.429
[100,     1] loss: 1166.091
[101,     1] loss: 1151.827
[102,     1] loss: 1124.490
[103,     1] loss: 1145.003
[104,     1] loss: 1079.439
[105,     1] loss: 1124.815
[106,     1] loss: 1158.921
[107,     1] loss: 1126.982
[108,     1] loss: 1117.507
[109,     1] loss: 1096.766
[110,     1] loss: 1124.564
[111,     1] loss: 1136.088
[112,     1] loss: 1087.648
[113,     1] loss: 1138.686
[114,     1] loss: 1134.692
[115,     1] loss: 1120.314
[116,     1] loss: 1066.801
[117,     1] loss: 1067.055
[118,     1] loss: 1081.252
[119,     1] loss: 1095.373
[120,     1] loss: 1017.033
[121,     1] loss: 1073.448
[122,     1] loss: 1042.581
[123,     1] loss: 1133.489
[124,     1] loss: 1012.855
[125,     1] loss: 1197.757
[126,     1] loss: 1097.748
[127,     1] loss: 1162.870
[128,     1] loss: 1049.417
[129,     1] loss: 1056.107
[130,     1] loss: 1059.667
[131,     1] loss: 988.663
[132,     1] loss: 1017.500
[133,     1] loss: 942.559
Early stopping applied (best metric=0.34671416878700256)
Finished Training
Total time taken: 19.429555416107178
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1348.947
[2,     1] loss: 1349.737
[3,     1] loss: 1344.428
[4,     1] loss: 1342.924
[5,     1] loss: 1341.757
[6,     1] loss: 1341.184
[7,     1] loss: 1341.222
[8,     1] loss: 1338.192
[9,     1] loss: 1336.027
[10,     1] loss: 1328.989
[11,     1] loss: 1310.901
[12,     1] loss: 1282.791
[13,     1] loss: 1246.540
[14,     1] loss: 1304.866
[15,     1] loss: 1243.708
[16,     1] loss: 1211.293
[17,     1] loss: 1195.748
[18,     1] loss: 1150.547
[19,     1] loss: 1122.672
[20,     1] loss: 1104.730
[21,     1] loss: 1135.024
[22,     1] loss: 1081.259
[23,     1] loss: 1099.327
[24,     1] loss: 1082.568
[25,     1] loss: 1037.720
[26,     1] loss: 1106.967
[27,     1] loss: 1009.142
[28,     1] loss: 1075.795
[29,     1] loss: 1021.247
[30,     1] loss: 1036.470
[31,     1] loss: 980.428
[32,     1] loss: 1014.880
[33,     1] loss: 947.604
[34,     1] loss: 1062.638
[35,     1] loss: 911.233
[36,     1] loss: 996.206
[37,     1] loss: 902.941
[38,     1] loss: 952.485
[39,     1] loss: 942.555
[40,     1] loss: 902.749
[41,     1] loss: 927.415
[42,     1] loss: 931.609
[43,     1] loss: 826.186
[44,     1] loss: 860.575
[45,     1] loss: 830.821
[46,     1] loss: 1036.054
[47,     1] loss: 1380.936
[48,     1] loss: 893.043
[49,     1] loss: 1058.081
[50,     1] loss: 1106.125
[51,     1] loss: 1076.616
[52,     1] loss: 1064.952
[53,     1] loss: 1072.783
[54,     1] loss: 1014.938
[55,     1] loss: 957.192
[56,     1] loss: 979.064
[57,     1] loss: 938.024
[58,     1] loss: 866.596
[59,     1] loss: 878.436
[60,     1] loss: 890.178
[61,     1] loss: 886.703
[62,     1] loss: 827.373
[63,     1] loss: 821.055
[64,     1] loss: 857.969
[65,     1] loss: 757.878
[66,     1] loss: 751.270
[67,     1] loss: 898.765
[68,     1] loss: 928.784
[69,     1] loss: 876.329
[70,     1] loss: 826.946
[71,     1] loss: 826.844
[72,     1] loss: 758.684
[73,     1] loss: 834.784
[74,     1] loss: 941.224
[75,     1] loss: 1397.723
[76,     1] loss: 970.908
[77,     1] loss: 983.096
[78,     1] loss: 1093.812
[79,     1] loss: 1032.253
[80,     1] loss: 1067.386
[81,     1] loss: 1016.183
[82,     1] loss: 920.461
[83,     1] loss: 1022.329
[84,     1] loss: 1024.813
[85,     1] loss: 977.930
[86,     1] loss: 900.895
[87,     1] loss: 903.607
[88,     1] loss: 937.117
[89,     1] loss: 909.393
[90,     1] loss: 883.186
[91,     1] loss: 836.186
[92,     1] loss: 802.676
Early stopping applied (best metric=0.35957762598991394)
Finished Training
Total time taken: 14.435425043106079
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1357.450
[2,     1] loss: 1348.024
[3,     1] loss: 1345.301
[4,     1] loss: 1343.153
[5,     1] loss: 1346.581
[6,     1] loss: 1345.719
[7,     1] loss: 1338.145
[8,     1] loss: 1340.679
[9,     1] loss: 1337.753
[10,     1] loss: 1333.035
[11,     1] loss: 1319.195
[12,     1] loss: 1308.302
[13,     1] loss: 1271.143
[14,     1] loss: 1240.649
[15,     1] loss: 1197.014
[16,     1] loss: 1183.588
[17,     1] loss: 1181.105
[18,     1] loss: 1146.548
[19,     1] loss: 1120.634
[20,     1] loss: 1128.274
[21,     1] loss: 1101.919
[22,     1] loss: 1069.492
[23,     1] loss: 1082.109
[24,     1] loss: 1054.343
[25,     1] loss: 1025.993
[26,     1] loss: 1053.901
[27,     1] loss: 1009.863
[28,     1] loss: 971.865
[29,     1] loss: 970.456
[30,     1] loss: 977.024
[31,     1] loss: 917.359
[32,     1] loss: 922.408
[33,     1] loss: 1156.704
[34,     1] loss: 1296.406
[35,     1] loss: 1089.226
[36,     1] loss: 1051.118
[37,     1] loss: 1117.481
[38,     1] loss: 1150.996
[39,     1] loss: 1113.264
[40,     1] loss: 1112.619
[41,     1] loss: 1086.708
[42,     1] loss: 1056.123
[43,     1] loss: 1033.364
[44,     1] loss: 1026.964
[45,     1] loss: 973.809
[46,     1] loss: 919.718
[47,     1] loss: 954.065
[48,     1] loss: 901.966
[49,     1] loss: 901.167
[50,     1] loss: 859.868
[51,     1] loss: 855.692
[52,     1] loss: 889.618
[53,     1] loss: 846.625
[54,     1] loss: 821.498
[55,     1] loss: 845.533
[56,     1] loss: 792.104
[57,     1] loss: 863.120
[58,     1] loss: 1264.132
[59,     1] loss: 1025.155
[60,     1] loss: 877.627
[61,     1] loss: 1001.402
[62,     1] loss: 996.430
[63,     1] loss: 872.412
[64,     1] loss: 1032.861
[65,     1] loss: 906.240
[66,     1] loss: 921.792
[67,     1] loss: 836.953
[68,     1] loss: 949.230
[69,     1] loss: 955.849
[70,     1] loss: 850.693
[71,     1] loss: 940.106
[72,     1] loss: 832.008
[73,     1] loss: 864.194
[74,     1] loss: 820.556
[75,     1] loss: 863.189
[76,     1] loss: 788.349
[77,     1] loss: 748.574
[78,     1] loss: 893.602
[79,     1] loss: 1313.922
[80,     1] loss: 816.766
[81,     1] loss: 1108.142
[82,     1] loss: 948.280
[83,     1] loss: 1021.531
[84,     1] loss: 1001.094
[85,     1] loss: 857.708
[86,     1] loss: 1080.676
[87,     1] loss: 876.153
[88,     1] loss: 968.612
[89,     1] loss: 890.425
[90,     1] loss: 964.028
[91,     1] loss: 958.732
[92,     1] loss: 879.589
[93,     1] loss: 924.830
[94,     1] loss: 790.502
[95,     1] loss: 928.262
[96,     1] loss: 803.865
[97,     1] loss: 850.364
[98,     1] loss: 782.455
[99,     1] loss: 778.582
[100,     1] loss: 872.079
[101,     1] loss: 758.822
[102,     1] loss: 718.699
[103,     1] loss: 962.609
[104,     1] loss: 762.464
[105,     1] loss: 721.582
[106,     1] loss: 747.268
[107,     1] loss: 761.745
[108,     1] loss: 719.470
[109,     1] loss: 751.005
[110,     1] loss: 911.086
[111,     1] loss: 1458.553
[112,     1] loss: 1220.030
[113,     1] loss: 1329.278
[114,     1] loss: 1263.991
[115,     1] loss: 1294.212
[116,     1] loss: 1281.071
[117,     1] loss: 1263.075
[118,     1] loss: 1267.376
[119,     1] loss: 1258.485
[120,     1] loss: 1244.405
[121,     1] loss: 1195.667
[122,     1] loss: 1231.059
Early stopping applied (best metric=0.3252192735671997)
Finished Training
Total time taken: 19.497578859329224
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.368
[2,     1] loss: 1347.309
[3,     1] loss: 1348.760
[4,     1] loss: 1340.995
[5,     1] loss: 1342.224
[6,     1] loss: 1341.801
[7,     1] loss: 1339.023
[8,     1] loss: 1333.482
[9,     1] loss: 1322.838
[10,     1] loss: 1303.275
[11,     1] loss: 1270.248
[12,     1] loss: 1238.984
[13,     1] loss: 1205.420
[14,     1] loss: 1161.200
[15,     1] loss: 1125.070
[16,     1] loss: 1158.490
[17,     1] loss: 1140.933
[18,     1] loss: 1121.752
[19,     1] loss: 1064.067
[20,     1] loss: 1080.389
[21,     1] loss: 1073.685
[22,     1] loss: 1064.960
[23,     1] loss: 1042.285
[24,     1] loss: 1076.830
[25,     1] loss: 1050.062
[26,     1] loss: 1061.524
[27,     1] loss: 998.706
[28,     1] loss: 1014.568
[29,     1] loss: 1009.897
[30,     1] loss: 1001.706
[31,     1] loss: 994.896
[32,     1] loss: 958.433
[33,     1] loss: 960.106
[34,     1] loss: 922.913
[35,     1] loss: 913.645
[36,     1] loss: 864.842
[37,     1] loss: 988.563
[38,     1] loss: 1163.625
[39,     1] loss: 1008.629
[40,     1] loss: 1010.085
[41,     1] loss: 960.670
[42,     1] loss: 1017.393
[43,     1] loss: 977.377
[44,     1] loss: 972.151
[45,     1] loss: 1014.400
[46,     1] loss: 897.931
[47,     1] loss: 928.859
[48,     1] loss: 893.261
[49,     1] loss: 904.719
[50,     1] loss: 882.250
[51,     1] loss: 902.255
[52,     1] loss: 833.762
[53,     1] loss: 824.374
[54,     1] loss: 854.487
[55,     1] loss: 1012.239
[56,     1] loss: 1259.745
[57,     1] loss: 885.789
[58,     1] loss: 1009.438
[59,     1] loss: 992.012
[60,     1] loss: 982.096
[61,     1] loss: 1014.182
[62,     1] loss: 943.401
[63,     1] loss: 934.201
[64,     1] loss: 967.438
[65,     1] loss: 912.863
[66,     1] loss: 894.452
[67,     1] loss: 893.014
[68,     1] loss: 868.900
[69,     1] loss: 831.469
[70,     1] loss: 856.520
[71,     1] loss: 851.117
[72,     1] loss: 827.006
[73,     1] loss: 796.284
[74,     1] loss: 792.302
[75,     1] loss: 1051.076
[76,     1] loss: 1168.320
[77,     1] loss: 934.530
[78,     1] loss: 1017.506
[79,     1] loss: 1002.661
[80,     1] loss: 911.194
[81,     1] loss: 895.909
[82,     1] loss: 912.096
[83,     1] loss: 845.931
[84,     1] loss: 889.784
[85,     1] loss: 811.972
[86,     1] loss: 908.268
[87,     1] loss: 986.530
[88,     1] loss: 813.097
[89,     1] loss: 991.948
[90,     1] loss: 826.144
[91,     1] loss: 924.082
[92,     1] loss: 765.302
[93,     1] loss: 866.697
[94,     1] loss: 835.592
[95,     1] loss: 785.443
[96,     1] loss: 824.275
[97,     1] loss: 753.057
[98,     1] loss: 696.811
[99,     1] loss: 705.912
[100,     1] loss: 723.969
[101,     1] loss: 932.810
[102,     1] loss: 1809.780
[103,     1] loss: 1075.981
[104,     1] loss: 1200.397
[105,     1] loss: 1208.549
[106,     1] loss: 1314.383
[107,     1] loss: 1186.618
[108,     1] loss: 1209.738
[109,     1] loss: 1194.186
[110,     1] loss: 1173.286
[111,     1] loss: 1128.658
[112,     1] loss: 1143.944
[113,     1] loss: 1122.742
[114,     1] loss: 1084.172
[115,     1] loss: 1099.013
[116,     1] loss: 1115.662
[117,     1] loss: 1046.417
[118,     1] loss: 1072.238
[119,     1] loss: 1047.751
[120,     1] loss: 1020.965
[121,     1] loss: 1022.016
[122,     1] loss: 970.479
[123,     1] loss: 941.327
[124,     1] loss: 946.217
[125,     1] loss: 902.623
[126,     1] loss: 894.825
[127,     1] loss: 877.842
[128,     1] loss: 940.841
[129,     1] loss: 1262.210
[130,     1] loss: 1333.835
[131,     1] loss: 1105.203
[132,     1] loss: 1197.163
[133,     1] loss: 1215.712
[134,     1] loss: 1196.832
[135,     1] loss: 1147.705
[136,     1] loss: 1035.203
[137,     1] loss: 1125.466
[138,     1] loss: 1061.536
[139,     1] loss: 992.403
Early stopping applied (best metric=0.37664592266082764)
Finished Training
Total time taken: 22.89250659942627
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1346.941
[2,     1] loss: 1346.082
[3,     1] loss: 1343.093
[4,     1] loss: 1347.426
[5,     1] loss: 1343.670
[6,     1] loss: 1340.013
[7,     1] loss: 1334.167
[8,     1] loss: 1311.038
[9,     1] loss: 1280.115
[10,     1] loss: 1229.910
[11,     1] loss: 1184.484
[12,     1] loss: 1176.039
[13,     1] loss: 1129.899
[14,     1] loss: 1089.656
[15,     1] loss: 1185.102
[16,     1] loss: 1085.370
[17,     1] loss: 1075.022
[18,     1] loss: 1098.096
[19,     1] loss: 1112.151
[20,     1] loss: 1001.594
[21,     1] loss: 1054.021
[22,     1] loss: 1045.211
[23,     1] loss: 1057.726
[24,     1] loss: 1004.125
[25,     1] loss: 936.628
[26,     1] loss: 972.480
[27,     1] loss: 1044.141
[28,     1] loss: 940.266
[29,     1] loss: 933.952
[30,     1] loss: 898.233
[31,     1] loss: 952.802
[32,     1] loss: 946.880
[33,     1] loss: 908.859
[34,     1] loss: 944.383
[35,     1] loss: 881.479
[36,     1] loss: 829.006
[37,     1] loss: 939.249
[38,     1] loss: 920.060
[39,     1] loss: 855.059
[40,     1] loss: 846.945
[41,     1] loss: 951.011
[42,     1] loss: 909.174
[43,     1] loss: 831.441
[44,     1] loss: 822.439
[45,     1] loss: 740.410
[46,     1] loss: 781.896
[47,     1] loss: 887.070
[48,     1] loss: 861.916
[49,     1] loss: 1015.739
[50,     1] loss: 768.039
[51,     1] loss: 933.223
[52,     1] loss: 802.161
[53,     1] loss: 832.317
[54,     1] loss: 795.970
[55,     1] loss: 772.730
[56,     1] loss: 738.967
[57,     1] loss: 729.975
[58,     1] loss: 692.222
[59,     1] loss: 668.831
[60,     1] loss: 682.777
[61,     1] loss: 673.604
[62,     1] loss: 1000.198
[63,     1] loss: 1426.489
[64,     1] loss: 906.586
[65,     1] loss: 1136.529
[66,     1] loss: 1152.007
[67,     1] loss: 1128.387
[68,     1] loss: 1137.310
[69,     1] loss: 1101.357
[70,     1] loss: 1004.450
[71,     1] loss: 947.958
[72,     1] loss: 994.538
[73,     1] loss: 1009.795
[74,     1] loss: 927.102
[75,     1] loss: 967.216
[76,     1] loss: 893.732
[77,     1] loss: 912.786
[78,     1] loss: 945.532
[79,     1] loss: 943.752
[80,     1] loss: 873.775
[81,     1] loss: 860.011
[82,     1] loss: 918.578
[83,     1] loss: 921.060
[84,     1] loss: 802.637
[85,     1] loss: 855.975
[86,     1] loss: 842.085
[87,     1] loss: 805.148
[88,     1] loss: 838.263
[89,     1] loss: 994.238
[90,     1] loss: 1188.843
[91,     1] loss: 890.550
[92,     1] loss: 1030.626
[93,     1] loss: 998.896
Early stopping applied (best metric=0.4538435637950897)
Finished Training
Total time taken: 15.696420192718506
{'Hydroxylation-K Validation Accuracy': 0.6975472813238771, 'Hydroxylation-K Validation Sensitivity': 0.6822222222222222, 'Hydroxylation-K Validation Specificity': 0.7017543859649122, 'Hydroxylation-K Validation Precision': 0.39204520803625664, 'Hydroxylation-K AUC ROC': 0.7682456140350877, 'Hydroxylation-K AUC PR': 0.5842388880769814, 'Hydroxylation-K MCC': 0.33110404671565613, 'Hydroxylation-K F1': 0.48890276528837895, 'Validation Loss (Hydroxylation-K)': 0.49334651430447896, 'Hydroxylation-P Validation Accuracy': 0.7704966076172106, 'Hydroxylation-P Validation Sensitivity': 0.7973015873015873, 'Hydroxylation-P Validation Specificity': 0.7647214324903985, 'Hydroxylation-P Validation Precision': 0.4288914841368326, 'Hydroxylation-P AUC ROC': 0.8473814016770025, 'Hydroxylation-P AUC PR': 0.5909592088568811, 'Hydroxylation-P MCC': 0.45896496715362545, 'Hydroxylation-P F1': 0.5549356664852546, 'Validation Loss (Hydroxylation-P)': 0.370611967643102, 'Validation Loss (total)': 0.8639584859212239, 'TimeToTrain': 16.155269797643026}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006901800012799705,
 'learning_rate_Hydroxylation-K': 0.002342379839206595,
 'learning_rate_Hydroxylation-P': 0.008525181273599424,
 'log_base': 2.535368359083747,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4235816289,
 'sample_weights': [2.0700996705183883, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.252797444381422,
 'weight_decay_Hydroxylation-K': 6.631692946325973,
 'weight_decay_Hydroxylation-P': 6.27263749328406}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.438
[2,     1] loss: 1293.294
[3,     1] loss: 1287.972
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005170309287308472,
 'learning_rate_Hydroxylation-K': 0.009581986259907678,
 'learning_rate_Hydroxylation-P': 0.004159254419065382,
 'log_base': 1.1386210932370187,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2858560934,
 'sample_weights': [1.7944461778394927, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.696177137669854,
 'weight_decay_Hydroxylation-K': 1.541797700290175,
 'weight_decay_Hydroxylation-P': 0.4966391453413088}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4168.320
[2,     1] loss: 4174.290
[3,     1] loss: 4184.849
[4,     1] loss: 4178.106
[5,     1] loss: 4180.747
[6,     1] loss: 4192.854
[7,     1] loss: 4163.074
[8,     1] loss: 4155.828
[9,     1] loss: 4166.005
[10,     1] loss: 4159.309
[11,     1] loss: 4153.627
[12,     1] loss: 4172.590
[13,     1] loss: 4150.536
[14,     1] loss: 4126.109
[15,     1] loss: 4080.382
[16,     1] loss: 4045.191
[17,     1] loss: 3969.469
[18,     1] loss: 3809.575
[19,     1] loss: 3669.472
[20,     1] loss: 3475.707
[21,     1] loss: 3470.399
[22,     1] loss: 3381.619
[23,     1] loss: 3442.959
[24,     1] loss: 3168.403
[25,     1] loss: 3456.352
[26,     1] loss: 3186.283
[27,     1] loss: 3379.318
[28,     1] loss: 3340.532
[29,     1] loss: 3422.151
[30,     1] loss: 3025.256
[31,     1] loss: 3075.347
[32,     1] loss: 2762.685
[33,     1] loss: 2927.824
[34,     1] loss: 2795.353
[35,     1] loss: 2858.415
[36,     1] loss: 3195.194
[37,     1] loss: 2770.648
[38,     1] loss: 2820.332
[39,     1] loss: 2474.469
[40,     1] loss: 2910.365
[41,     1] loss: 2451.371
[42,     1] loss: 2810.695
[43,     1] loss: 2389.912
[44,     1] loss: 2775.542
[45,     1] loss: 2379.352
[46,     1] loss: 2331.216
[47,     1] loss: 2131.669
[48,     1] loss: 2417.128
[49,     1] loss: 2003.155
[50,     1] loss: 2474.871
[51,     1] loss: 3002.775
[52,     1] loss: 2163.334
[53,     1] loss: 2444.305
[54,     1] loss: 2443.441
[55,     1] loss: 2360.907
[56,     1] loss: 2478.855
[57,     1] loss: 2302.792
[58,     1] loss: 2366.945
[59,     1] loss: 2351.928
[60,     1] loss: 1703.881
[61,     1] loss: 2273.944
[62,     1] loss: 2125.140
[63,     1] loss: 1815.328
[64,     1] loss: 2292.694
[65,     1] loss: 1899.155
[66,     1] loss: 2007.760
[67,     1] loss: 1963.749
[68,     1] loss: 1836.014
[69,     1] loss: 1998.769
[70,     1] loss: 1796.364
[71,     1] loss: 1681.473
[72,     1] loss: 1827.053
[73,     1] loss: 1784.235
[74,     1] loss: 1993.847
[75,     1] loss: 3638.074
[76,     1] loss: 4028.545
[77,     1] loss: 2251.198
[78,     1] loss: 3319.940
[79,     1] loss: 3065.844
[80,     1] loss: 2795.547
[81,     1] loss: 2842.452
[82,     1] loss: 3113.026
[83,     1] loss: 2924.987
[84,     1] loss: 2529.397
[85,     1] loss: 2340.220
[86,     1] loss: 2404.002
[87,     1] loss: 2687.008
[88,     1] loss: 2107.204
[89,     1] loss: 2352.631
Early stopping applied (best metric=0.38925883173942566)
Finished Training
Total time taken: 15.036822080612183
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4167.730
[2,     1] loss: 4182.262
[3,     1] loss: 4170.960
[4,     1] loss: 4193.188
[5,     1] loss: 4157.631
[6,     1] loss: 4162.217
[7,     1] loss: 4146.183
[8,     1] loss: 4098.594
[9,     1] loss: 4091.899
[10,     1] loss: 3953.212
[11,     1] loss: 3798.240
[12,     1] loss: 3611.242
[13,     1] loss: 3653.591
[14,     1] loss: 3439.907
[15,     1] loss: 3487.775
[16,     1] loss: 3694.642
[17,     1] loss: 3385.021
[18,     1] loss: 3457.489
[19,     1] loss: 3066.762
[20,     1] loss: 3246.738
[21,     1] loss: 3223.670
[22,     1] loss: 3183.248
[23,     1] loss: 3199.004
[24,     1] loss: 3098.943
[25,     1] loss: 2976.059
[26,     1] loss: 2995.549
[27,     1] loss: 2940.676
[28,     1] loss: 2931.339
[29,     1] loss: 2951.796
[30,     1] loss: 2919.562
[31,     1] loss: 2631.635
[32,     1] loss: 2729.249
[33,     1] loss: 2757.686
[34,     1] loss: 2358.671
[35,     1] loss: 4219.069
[36,     1] loss: 2651.672
[37,     1] loss: 3575.636
[38,     1] loss: 2754.293
[39,     1] loss: 3199.622
[40,     1] loss: 3328.070
[41,     1] loss: 2884.286
[42,     1] loss: 2689.702
[43,     1] loss: 2811.134
[44,     1] loss: 2739.178
[45,     1] loss: 2553.378
[46,     1] loss: 2587.802
[47,     1] loss: 2840.927
[48,     1] loss: 2702.922
[49,     1] loss: 2680.536
[50,     1] loss: 2280.453
[51,     1] loss: 2438.832
[52,     1] loss: 2181.386
[53,     1] loss: 2272.854
[54,     1] loss: 2127.066
[55,     1] loss: 2184.646
[56,     1] loss: 1944.348
[57,     1] loss: 2206.242
[58,     1] loss: 1824.485
[59,     1] loss: 2300.354
[60,     1] loss: 2090.687
[61,     1] loss: 1856.982
[62,     1] loss: 1895.524
[63,     1] loss: 1666.196
[64,     1] loss: 1897.355
[65,     1] loss: 2303.387
[66,     1] loss: 5751.383
[67,     1] loss: 1944.431
[68,     1] loss: 3433.926
[69,     1] loss: 2355.891
[70,     1] loss: 2867.132
[71,     1] loss: 2935.403
[72,     1] loss: 3074.689
[73,     1] loss: 2724.428
[74,     1] loss: 2495.384
[75,     1] loss: 2615.007
[76,     1] loss: 2456.417
[77,     1] loss: 2442.222
[78,     1] loss: 2637.323
[79,     1] loss: 2330.495
[80,     1] loss: 2147.096
[81,     1] loss: 2183.396
[82,     1] loss: 2017.414
[83,     1] loss: 1951.892
[84,     1] loss: 1903.212
[85,     1] loss: 1837.220
[86,     1] loss: 1861.824
[87,     1] loss: 1737.841
[88,     1] loss: 1723.159
Early stopping applied (best metric=0.43524011969566345)
Finished Training
Total time taken: 14.484880685806274
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4168.762
[2,     1] loss: 4185.331
[3,     1] loss: 4165.397
[4,     1] loss: 4157.323
[5,     1] loss: 4161.136
[6,     1] loss: 4154.049
[7,     1] loss: 4161.708
[8,     1] loss: 4141.537
[9,     1] loss: 4126.432
[10,     1] loss: 4060.201
[11,     1] loss: 3981.854
[12,     1] loss: 3897.085
[13,     1] loss: 3813.637
[14,     1] loss: 3594.859
[15,     1] loss: 3490.103
[16,     1] loss: 3404.121
[17,     1] loss: 3721.783
[18,     1] loss: 3568.107
[19,     1] loss: 3613.412
[20,     1] loss: 3393.395
[21,     1] loss: 3337.280
[22,     1] loss: 3260.501
[23,     1] loss: 3260.494
[24,     1] loss: 3229.239
[25,     1] loss: 3455.307
[26,     1] loss: 3153.595
[27,     1] loss: 3270.647
[28,     1] loss: 3030.394
[29,     1] loss: 2970.668
[30,     1] loss: 3048.710
[31,     1] loss: 2893.911
[32,     1] loss: 2999.445
[33,     1] loss: 3002.753
[34,     1] loss: 2790.549
[35,     1] loss: 2642.885
[36,     1] loss: 2650.548
[37,     1] loss: 2689.845
[38,     1] loss: 2611.308
[39,     1] loss: 2683.841
[40,     1] loss: 2673.271
[41,     1] loss: 2599.231
[42,     1] loss: 2700.501
[43,     1] loss: 2446.072
[44,     1] loss: 2231.990
[45,     1] loss: 2797.873
[46,     1] loss: 2395.018
[47,     1] loss: 2129.988
[48,     1] loss: 2242.196
[49,     1] loss: 2153.215
[50,     1] loss: 1850.517
[51,     1] loss: 2136.868
[52,     1] loss: 1889.190
[53,     1] loss: 3324.240
[54,     1] loss: 7313.357
[55,     1] loss: 2402.405
[56,     1] loss: 3462.214
[57,     1] loss: 3681.513
[58,     1] loss: 3641.413
[59,     1] loss: 3520.998
[60,     1] loss: 3648.289
[61,     1] loss: 3680.196
[62,     1] loss: 3638.021
[63,     1] loss: 3613.948
[64,     1] loss: 3551.254
[65,     1] loss: 3505.996
[66,     1] loss: 3202.314
[67,     1] loss: 3234.307
[68,     1] loss: 3094.704
[69,     1] loss: 3014.010
[70,     1] loss: 2856.564
[71,     1] loss: 2828.823
[72,     1] loss: 2861.897
[73,     1] loss: 2662.011
[74,     1] loss: 2720.815
[75,     1] loss: 2715.841
[76,     1] loss: 2949.013
[77,     1] loss: 2838.969
Early stopping applied (best metric=0.4311539828777313)
Finished Training
Total time taken: 12.923737287521362
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4170.645
[2,     1] loss: 4172.374
[3,     1] loss: 4168.247
[4,     1] loss: 4155.435
[5,     1] loss: 4150.915
[6,     1] loss: 4159.619
[7,     1] loss: 4183.803
[8,     1] loss: 4144.534
[9,     1] loss: 4138.955
[10,     1] loss: 4107.064
[11,     1] loss: 4078.611
[12,     1] loss: 4034.577
[13,     1] loss: 3893.471
[14,     1] loss: 3868.697
[15,     1] loss: 3687.371
[16,     1] loss: 3547.132
[17,     1] loss: 3351.011
[18,     1] loss: 3820.271
[19,     1] loss: 3489.271
[20,     1] loss: 3318.431
[21,     1] loss: 3280.150
[22,     1] loss: 3360.910
[23,     1] loss: 3559.604
[24,     1] loss: 3495.007
[25,     1] loss: 3137.542
[26,     1] loss: 3303.333
[27,     1] loss: 3157.590
[28,     1] loss: 2955.942
[29,     1] loss: 3570.707
[30,     1] loss: 3168.992
[31,     1] loss: 3258.335
[32,     1] loss: 3015.394
[33,     1] loss: 3237.087
[34,     1] loss: 3143.597
[35,     1] loss: 2845.473
[36,     1] loss: 3071.944
[37,     1] loss: 2620.806
[38,     1] loss: 2884.675
[39,     1] loss: 2898.382
[40,     1] loss: 2554.679
[41,     1] loss: 3011.659
[42,     1] loss: 2627.606
[43,     1] loss: 3079.418
[44,     1] loss: 2475.871
[45,     1] loss: 3023.205
[46,     1] loss: 2460.272
[47,     1] loss: 2841.643
[48,     1] loss: 2701.874
[49,     1] loss: 2553.273
[50,     1] loss: 2699.683
[51,     1] loss: 2378.489
[52,     1] loss: 2594.020
[53,     1] loss: 2188.338
[54,     1] loss: 2523.488
[55,     1] loss: 2661.971
[56,     1] loss: 2167.128
[57,     1] loss: 2221.250
[58,     1] loss: 2141.519
[59,     1] loss: 2335.338
[60,     1] loss: 2140.865
[61,     1] loss: 2077.119
[62,     1] loss: 1757.778
[63,     1] loss: 1852.715
[64,     1] loss: 2103.148
[65,     1] loss: 1659.965
[66,     1] loss: 1768.922
[67,     1] loss: 2415.193
[68,     1] loss: 4947.514
[69,     1] loss: 3303.566
[70,     1] loss: 3571.562
[71,     1] loss: 3055.328
[72,     1] loss: 3284.628
[73,     1] loss: 3370.829
[74,     1] loss: 3427.197
[75,     1] loss: 3489.650
[76,     1] loss: 3719.408
[77,     1] loss: 3280.774
Early stopping applied (best metric=0.42922401428222656)
Finished Training
Total time taken: 12.117178678512573
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4184.765
[2,     1] loss: 4176.665
[3,     1] loss: 4190.856
[4,     1] loss: 4155.003
[5,     1] loss: 4168.748
[6,     1] loss: 4158.537
[7,     1] loss: 4155.858
[8,     1] loss: 4166.811
[9,     1] loss: 4094.832
[10,     1] loss: 4030.776
[11,     1] loss: 3971.460
[12,     1] loss: 3841.201
[13,     1] loss: 3673.759
[14,     1] loss: 3765.938
[15,     1] loss: 3528.779
[16,     1] loss: 3627.123
[17,     1] loss: 3224.682
[18,     1] loss: 3508.646
[19,     1] loss: 3216.413
[20,     1] loss: 3760.890
[21,     1] loss: 3444.722
[22,     1] loss: 3271.920
[23,     1] loss: 3319.019
[24,     1] loss: 3021.276
[25,     1] loss: 3162.864
[26,     1] loss: 3162.261
[27,     1] loss: 2918.556
[28,     1] loss: 3184.618
[29,     1] loss: 3129.650
[30,     1] loss: 2921.378
[31,     1] loss: 2842.597
[32,     1] loss: 3078.842
[33,     1] loss: 2872.927
[34,     1] loss: 2944.460
[35,     1] loss: 2845.687
[36,     1] loss: 2911.368
[37,     1] loss: 2502.504
[38,     1] loss: 2613.596
[39,     1] loss: 2434.131
[40,     1] loss: 2492.132
[41,     1] loss: 2593.667
[42,     1] loss: 2349.671
[43,     1] loss: 3121.158
[44,     1] loss: 2809.612
[45,     1] loss: 2235.487
[46,     1] loss: 2963.222
[47,     1] loss: 2545.579
[48,     1] loss: 2699.729
[49,     1] loss: 2292.877
[50,     1] loss: 2449.983
[51,     1] loss: 2427.038
[52,     1] loss: 2752.026
[53,     1] loss: 2252.572
[54,     1] loss: 2442.182
[55,     1] loss: 2206.966
[56,     1] loss: 2472.718
[57,     1] loss: 2188.640
[58,     1] loss: 2706.725
[59,     1] loss: 2052.469
[60,     1] loss: 2245.183
[61,     1] loss: 2011.922
[62,     1] loss: 2131.088
[63,     1] loss: 1808.944
[64,     1] loss: 1871.992
[65,     1] loss: 1704.954
[66,     1] loss: 2480.963
[67,     1] loss: 3345.605
[68,     1] loss: 1719.302
[69,     1] loss: 2549.059
[70,     1] loss: 2365.284
[71,     1] loss: 2250.104
[72,     1] loss: 2192.922
[73,     1] loss: 2236.017
[74,     1] loss: 1887.136
[75,     1] loss: 2108.165
[76,     1] loss: 1779.475
[77,     1] loss: 2193.663
[78,     1] loss: 1583.720
[79,     1] loss: 2204.391
[80,     1] loss: 1736.897
[81,     1] loss: 1973.017
[82,     1] loss: 1748.156
[83,     1] loss: 2475.784
Early stopping applied (best metric=0.4356776475906372)
Finished Training
Total time taken: 12.726312398910522
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4168.389
[2,     1] loss: 4233.381
[3,     1] loss: 4169.068
[4,     1] loss: 4188.845
[5,     1] loss: 4172.490
[6,     1] loss: 4170.215
[7,     1] loss: 4185.724
[8,     1] loss: 4180.569
[9,     1] loss: 4176.559
[10,     1] loss: 4177.271
[11,     1] loss: 4160.231
[12,     1] loss: 4165.423
[13,     1] loss: 4154.214
[14,     1] loss: 4132.495
[15,     1] loss: 4177.856
[16,     1] loss: 4235.378
[17,     1] loss: 4177.799
[18,     1] loss: 4133.427
[19,     1] loss: 4111.505
[20,     1] loss: 4074.122
[21,     1] loss: 4048.148
[22,     1] loss: 3977.198
[23,     1] loss: 3886.081
[24,     1] loss: 3849.506
[25,     1] loss: 3778.232
[26,     1] loss: 3681.025
[27,     1] loss: 3541.474
[28,     1] loss: 3679.228
[29,     1] loss: 3467.228
[30,     1] loss: 3540.773
[31,     1] loss: 3375.810
[32,     1] loss: 3334.363
[33,     1] loss: 3571.832
[34,     1] loss: 3175.556
[35,     1] loss: 3091.899
[36,     1] loss: 3098.379
[37,     1] loss: 3015.535
[38,     1] loss: 3238.677
[39,     1] loss: 3101.481
[40,     1] loss: 3044.313
[41,     1] loss: 3129.667
[42,     1] loss: 2889.062
[43,     1] loss: 3136.394
[44,     1] loss: 3096.544
[45,     1] loss: 2878.474
[46,     1] loss: 2601.591
[47,     1] loss: 2830.268
[48,     1] loss: 2540.366
[49,     1] loss: 2460.751
[50,     1] loss: 2951.145
[51,     1] loss: 2761.275
[52,     1] loss: 2472.073
[53,     1] loss: 2606.816
[54,     1] loss: 2653.173
[55,     1] loss: 2358.249
[56,     1] loss: 2573.378
[57,     1] loss: 2243.574
[58,     1] loss: 2535.014
[59,     1] loss: 2331.312
[60,     1] loss: 2307.179
[61,     1] loss: 2252.510
[62,     1] loss: 3144.442
[63,     1] loss: 2732.380
[64,     1] loss: 2620.671
[65,     1] loss: 2683.167
[66,     1] loss: 3165.703
[67,     1] loss: 2889.103
[68,     1] loss: 2722.028
[69,     1] loss: 2989.323
[70,     1] loss: 2650.871
[71,     1] loss: 2346.821
[72,     1] loss: 2182.245
[73,     1] loss: 2194.959
[74,     1] loss: 1916.793
[75,     1] loss: 2076.961
[76,     1] loss: 3296.323
[77,     1] loss: 2031.594
[78,     1] loss: 2419.453
[79,     1] loss: 2075.560
[80,     1] loss: 2505.593
[81,     1] loss: 2279.947
Early stopping applied (best metric=0.41547802090644836)
Finished Training
Total time taken: 12.630232810974121
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4185.771
[2,     1] loss: 4165.092
[3,     1] loss: 4154.365
[4,     1] loss: 4167.955
[5,     1] loss: 4139.985
[6,     1] loss: 4128.407
[7,     1] loss: 4008.216
[8,     1] loss: 3947.246
[9,     1] loss: 3852.802
[10,     1] loss: 3688.522
[11,     1] loss: 3491.638
[12,     1] loss: 3338.291
[13,     1] loss: 3304.110
[14,     1] loss: 3413.597
[15,     1] loss: 3225.332
[16,     1] loss: 3334.771
[17,     1] loss: 3428.106
[18,     1] loss: 3208.768
[19,     1] loss: 3380.438
[20,     1] loss: 3309.938
[21,     1] loss: 3235.849
[22,     1] loss: 3272.509
[23,     1] loss: 2934.753
[24,     1] loss: 3127.477
[25,     1] loss: 3012.074
[26,     1] loss: 2870.974
[27,     1] loss: 2947.618
[28,     1] loss: 2632.395
[29,     1] loss: 2888.378
[30,     1] loss: 2781.684
[31,     1] loss: 2775.016
[32,     1] loss: 2386.954
[33,     1] loss: 2884.680
[34,     1] loss: 2669.660
[35,     1] loss: 2258.782
[36,     1] loss: 2306.368
[37,     1] loss: 2114.500
[38,     1] loss: 2043.559
[39,     1] loss: 2234.073
[40,     1] loss: 2450.932
[41,     1] loss: 2237.251
[42,     1] loss: 2262.780
[43,     1] loss: 2472.987
[44,     1] loss: 2158.298
[45,     1] loss: 2363.585
[46,     1] loss: 2211.997
[47,     1] loss: 2041.253
[48,     1] loss: 2119.758
[49,     1] loss: 1858.893
[50,     1] loss: 2071.692
[51,     1] loss: 1933.858
[52,     1] loss: 1842.527
[53,     1] loss: 1835.523
[54,     1] loss: 1963.465
[55,     1] loss: 1715.573
[56,     1] loss: 1876.745
[57,     1] loss: 1717.439
Early stopping applied (best metric=0.5087595582008362)
Finished Training
Total time taken: 9.546752214431763
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4232.663
[2,     1] loss: 4174.223
[3,     1] loss: 4168.856
[4,     1] loss: 4166.442
[5,     1] loss: 4168.283
[6,     1] loss: 4180.234
[7,     1] loss: 4163.357
[8,     1] loss: 4152.045
[9,     1] loss: 4148.558
[10,     1] loss: 4149.922
[11,     1] loss: 4141.220
[12,     1] loss: 4121.539
[13,     1] loss: 4104.961
[14,     1] loss: 4115.359
[15,     1] loss: 3966.636
[16,     1] loss: 3944.587
[17,     1] loss: 3914.768
[18,     1] loss: 3805.154
[19,     1] loss: 3724.685
[20,     1] loss: 3664.452
[21,     1] loss: 3623.340
[22,     1] loss: 3448.983
[23,     1] loss: 3540.644
[24,     1] loss: 3708.317
[25,     1] loss: 3372.521
[26,     1] loss: 3216.339
[27,     1] loss: 3481.829
[28,     1] loss: 3004.399
[29,     1] loss: 3078.379
[30,     1] loss: 2925.911
[31,     1] loss: 2877.591
[32,     1] loss: 2789.405
[33,     1] loss: 2782.801
[34,     1] loss: 3234.617
[35,     1] loss: 3304.499
[36,     1] loss: 2834.623
[37,     1] loss: 2902.120
[38,     1] loss: 2874.428
[39,     1] loss: 3081.447
[40,     1] loss: 2796.576
[41,     1] loss: 2941.489
[42,     1] loss: 2709.119
[43,     1] loss: 2875.412
[44,     1] loss: 2502.169
[45,     1] loss: 2454.286
[46,     1] loss: 2994.056
[47,     1] loss: 2513.829
[48,     1] loss: 2206.074
[49,     1] loss: 2549.566
[50,     1] loss: 2337.637
[51,     1] loss: 2167.973
[52,     1] loss: 2359.141
[53,     1] loss: 2045.086
[54,     1] loss: 1943.711
[55,     1] loss: 2080.291
[56,     1] loss: 1944.856
[57,     1] loss: 1824.986
[58,     1] loss: 1923.540
[59,     1] loss: 3886.981
[60,     1] loss: 5939.717
[61,     1] loss: 2932.009
[62,     1] loss: 2635.711
[63,     1] loss: 3147.042
[64,     1] loss: 3425.569
[65,     1] loss: 3404.309
[66,     1] loss: 3310.937
[67,     1] loss: 3229.887
[68,     1] loss: 3215.860
[69,     1] loss: 3273.467
Early stopping applied (best metric=0.4401646852493286)
Finished Training
Total time taken: 9.31218934059143
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4184.494
[2,     1] loss: 4212.013
[3,     1] loss: 4161.618
[4,     1] loss: 4178.917
[5,     1] loss: 4173.512
[6,     1] loss: 4156.622
[7,     1] loss: 4170.337
[8,     1] loss: 4144.460
[9,     1] loss: 4137.282
[10,     1] loss: 4108.341
[11,     1] loss: 4068.408
[12,     1] loss: 3938.877
[13,     1] loss: 3871.551
[14,     1] loss: 3632.554
[15,     1] loss: 3557.238
[16,     1] loss: 3428.309
[17,     1] loss: 3539.668
[18,     1] loss: 3467.783
[19,     1] loss: 3545.871
[20,     1] loss: 3312.771
[21,     1] loss: 3238.063
[22,     1] loss: 3218.919
[23,     1] loss: 3049.073
[24,     1] loss: 3060.874
[25,     1] loss: 3279.260
[26,     1] loss: 3333.471
[27,     1] loss: 3078.403
[28,     1] loss: 3099.922
[29,     1] loss: 2895.707
[30,     1] loss: 2772.333
[31,     1] loss: 2942.784
[32,     1] loss: 2506.599
[33,     1] loss: 3168.690
[34,     1] loss: 2620.638
[35,     1] loss: 2677.203
[36,     1] loss: 2486.201
[37,     1] loss: 2729.808
[38,     1] loss: 2573.985
[39,     1] loss: 2728.788
[40,     1] loss: 2484.680
[41,     1] loss: 2682.902
[42,     1] loss: 2516.051
[43,     1] loss: 2275.396
[44,     1] loss: 2291.741
[45,     1] loss: 2061.269
[46,     1] loss: 2264.780
[47,     1] loss: 2260.424
[48,     1] loss: 2004.625
[49,     1] loss: 1971.116
[50,     1] loss: 2420.450
[51,     1] loss: 2802.383
[52,     1] loss: 2902.092
[53,     1] loss: 2494.222
[54,     1] loss: 2399.421
[55,     1] loss: 2806.325
[56,     1] loss: 2417.207
[57,     1] loss: 2471.802
[58,     1] loss: 2683.948
[59,     1] loss: 2289.661
[60,     1] loss: 2220.841
[61,     1] loss: 2328.739
[62,     1] loss: 2187.899
[63,     1] loss: 2182.337
[64,     1] loss: 2466.556
[65,     1] loss: 2248.649
[66,     1] loss: 2082.473
[67,     1] loss: 1890.940
[68,     1] loss: 2562.481
[69,     1] loss: 1904.487
[70,     1] loss: 1915.647
[71,     1] loss: 2235.574
[72,     1] loss: 1943.031
Early stopping applied (best metric=0.45748719573020935)
Finished Training
Total time taken: 10.591065168380737
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4179.761
[2,     1] loss: 4179.869
[3,     1] loss: 4179.638
[4,     1] loss: 4179.197
[5,     1] loss: 4177.140
[6,     1] loss: 4161.209
[7,     1] loss: 4177.895
[8,     1] loss: 4213.812
[9,     1] loss: 4169.387
[10,     1] loss: 4154.098
[11,     1] loss: 4152.953
[12,     1] loss: 4158.652
[13,     1] loss: 4140.919
[14,     1] loss: 4112.582
[15,     1] loss: 4085.133
[16,     1] loss: 4015.233
[17,     1] loss: 4004.237
[18,     1] loss: 3767.849
[19,     1] loss: 3753.770
[20,     1] loss: 3673.883
[21,     1] loss: 3535.751
[22,     1] loss: 3616.665
[23,     1] loss: 3409.819
[24,     1] loss: 3420.464
[25,     1] loss: 3307.713
[26,     1] loss: 3357.400
[27,     1] loss: 3321.724
[28,     1] loss: 3149.401
[29,     1] loss: 3185.071
[30,     1] loss: 3104.063
[31,     1] loss: 3027.873
[32,     1] loss: 2867.646
[33,     1] loss: 2872.374
[34,     1] loss: 2874.857
[35,     1] loss: 3077.217
[36,     1] loss: 2928.380
[37,     1] loss: 2654.962
[38,     1] loss: 3328.324
[39,     1] loss: 3083.309
[40,     1] loss: 3061.696
[41,     1] loss: 2799.415
[42,     1] loss: 2843.345
[43,     1] loss: 2499.373
[44,     1] loss: 2828.424
[45,     1] loss: 2600.571
[46,     1] loss: 2600.745
[47,     1] loss: 2222.449
[48,     1] loss: 2455.751
[49,     1] loss: 2221.473
[50,     1] loss: 2275.180
[51,     1] loss: 2641.158
[52,     1] loss: 2085.677
[53,     1] loss: 2043.516
[54,     1] loss: 2282.637
[55,     1] loss: 2295.363
[56,     1] loss: 2143.804
[57,     1] loss: 1937.206
[58,     1] loss: 2082.735
[59,     1] loss: 1955.702
[60,     1] loss: 1913.292
[61,     1] loss: 2091.420
[62,     1] loss: 5558.443
[63,     1] loss: 4727.454
[64,     1] loss: 3520.916
[65,     1] loss: 3207.590
[66,     1] loss: 3394.447
[67,     1] loss: 3569.149
[68,     1] loss: 3631.287
[69,     1] loss: 3772.746
[70,     1] loss: 3678.845
[71,     1] loss: 3573.456
[72,     1] loss: 3544.458
[73,     1] loss: 3560.610
[74,     1] loss: 3385.315
[75,     1] loss: 3274.526
Early stopping applied (best metric=0.3606337606906891)
Finished Training
Total time taken: 12.316283702850342
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4174.095
[2,     1] loss: 4174.182
[3,     1] loss: 4186.371
[4,     1] loss: 4184.574
[5,     1] loss: 4208.703
[6,     1] loss: 4183.657
[7,     1] loss: 4157.336
[8,     1] loss: 4170.864
[9,     1] loss: 4176.672
[10,     1] loss: 4161.016
[11,     1] loss: 4174.682
[12,     1] loss: 4152.146
[13,     1] loss: 4152.683
[14,     1] loss: 4155.954
[15,     1] loss: 4156.226
[16,     1] loss: 4110.859
[17,     1] loss: 4071.204
[18,     1] loss: 4026.080
[19,     1] loss: 3951.002
[20,     1] loss: 3905.327
[21,     1] loss: 3875.443
[22,     1] loss: 3805.335
[23,     1] loss: 3725.064
[24,     1] loss: 3493.528
[25,     1] loss: 3476.656
[26,     1] loss: 3736.293
[27,     1] loss: 3350.717
[28,     1] loss: 3593.372
[29,     1] loss: 3309.646
[30,     1] loss: 3178.025
[31,     1] loss: 3180.423
[32,     1] loss: 3295.861
[33,     1] loss: 2966.416
[34,     1] loss: 3228.167
[35,     1] loss: 3028.268
[36,     1] loss: 2858.598
[37,     1] loss: 3295.423
[38,     1] loss: 3064.946
[39,     1] loss: 2976.083
[40,     1] loss: 2728.251
[41,     1] loss: 3016.673
[42,     1] loss: 2781.621
[43,     1] loss: 2841.137
[44,     1] loss: 2680.258
[45,     1] loss: 2562.020
[46,     1] loss: 2848.820
[47,     1] loss: 2275.779
[48,     1] loss: 2720.921
[49,     1] loss: 2315.440
[50,     1] loss: 2775.397
[51,     1] loss: 2398.330
[52,     1] loss: 2757.263
[53,     1] loss: 2267.812
[54,     1] loss: 2680.431
[55,     1] loss: 2118.168
[56,     1] loss: 2730.241
[57,     1] loss: 2166.979
[58,     1] loss: 2114.607
[59,     1] loss: 2169.815
[60,     1] loss: 1927.309
[61,     1] loss: 2014.664
[62,     1] loss: 1749.380
[63,     1] loss: 1887.984
[64,     1] loss: 1846.695
[65,     1] loss: 1801.448
[66,     1] loss: 1628.543
[67,     1] loss: 2202.549
[68,     1] loss: 5574.041
[69,     1] loss: 5100.337
[70,     1] loss: 3952.025
[71,     1] loss: 3680.586
[72,     1] loss: 3783.786
[73,     1] loss: 3909.662
[74,     1] loss: 3969.645
[75,     1] loss: 4013.222
[76,     1] loss: 4029.467
[77,     1] loss: 3977.118
Early stopping applied (best metric=0.40652182698249817)
Finished Training
Total time taken: 10.37680459022522
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4165.741
[2,     1] loss: 4161.688
[3,     1] loss: 4165.570
[4,     1] loss: 4181.771
[5,     1] loss: 4161.557
[6,     1] loss: 4183.093
[7,     1] loss: 4149.762
[8,     1] loss: 4155.783
[9,     1] loss: 4131.585
[10,     1] loss: 4107.890
[11,     1] loss: 4074.660
[12,     1] loss: 4005.351
[13,     1] loss: 3988.190
[14,     1] loss: 3892.493
[15,     1] loss: 3873.821
[16,     1] loss: 3586.135
[17,     1] loss: 3606.543
[18,     1] loss: 3490.668
[19,     1] loss: 3540.128
[20,     1] loss: 3511.485
[21,     1] loss: 3595.521
[22,     1] loss: 3647.732
[23,     1] loss: 3253.351
[24,     1] loss: 3235.549
[25,     1] loss: 3331.514
[26,     1] loss: 3144.879
[27,     1] loss: 3266.865
[28,     1] loss: 3119.479
[29,     1] loss: 3183.119
[30,     1] loss: 2927.195
[31,     1] loss: 3179.339
[32,     1] loss: 2695.178
[33,     1] loss: 2961.197
[34,     1] loss: 2705.609
[35,     1] loss: 2893.491
[36,     1] loss: 2585.933
[37,     1] loss: 2587.579
[38,     1] loss: 2536.102
[39,     1] loss: 2717.649
[40,     1] loss: 2350.417
[41,     1] loss: 2438.059
[42,     1] loss: 2517.151
[43,     1] loss: 3189.797
[44,     1] loss: 2904.440
[45,     1] loss: 2372.008
[46,     1] loss: 2616.580
[47,     1] loss: 2894.493
[48,     1] loss: 2618.461
[49,     1] loss: 2351.537
[50,     1] loss: 2605.628
[51,     1] loss: 2174.985
[52,     1] loss: 2281.299
[53,     1] loss: 2088.191
[54,     1] loss: 1894.448
[55,     1] loss: 2459.646
[56,     1] loss: 2209.958
[57,     1] loss: 1750.911
[58,     1] loss: 2299.499
[59,     1] loss: 2105.258
[60,     1] loss: 2178.240
[61,     1] loss: 2267.922
[62,     1] loss: 1731.361
[63,     1] loss: 1980.889
[64,     1] loss: 1757.859
[65,     1] loss: 2363.847
[66,     1] loss: 3272.204
[67,     1] loss: 2056.656
[68,     1] loss: 2605.890
[69,     1] loss: 2075.472
[70,     1] loss: 2519.876
[71,     1] loss: 2702.291
[72,     1] loss: 2048.732
[73,     1] loss: 2601.938
[74,     1] loss: 1774.654
[75,     1] loss: 1781.853
[76,     1] loss: 1988.333
[77,     1] loss: 1577.309
[78,     1] loss: 1968.741
[79,     1] loss: 1422.917
[80,     1] loss: 1798.171
[81,     1] loss: 1524.328
[82,     1] loss: 1586.164
Early stopping applied (best metric=0.3896864354610443)
Finished Training
Total time taken: 10.992754697799683
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4182.304
[2,     1] loss: 4174.713
[3,     1] loss: 4161.231
[4,     1] loss: 4177.370
[5,     1] loss: 4206.329
[6,     1] loss: 4172.260
[7,     1] loss: 4188.610
[8,     1] loss: 4157.087
[9,     1] loss: 4157.052
[10,     1] loss: 4172.693
[11,     1] loss: 4191.383
[12,     1] loss: 4163.225
[13,     1] loss: 4135.455
[14,     1] loss: 4156.331
[15,     1] loss: 4148.671
[16,     1] loss: 4123.053
[17,     1] loss: 4094.743
[18,     1] loss: 4018.102
[19,     1] loss: 3972.253
[20,     1] loss: 3890.996
[21,     1] loss: 3828.296
[22,     1] loss: 3775.963
[23,     1] loss: 3728.289
[24,     1] loss: 3643.073
[25,     1] loss: 3613.364
[26,     1] loss: 3691.490
[27,     1] loss: 3516.125
[28,     1] loss: 3658.916
[29,     1] loss: 3482.941
[30,     1] loss: 3580.750
[31,     1] loss: 3501.191
[32,     1] loss: 3441.967
[33,     1] loss: 3276.509
[34,     1] loss: 3177.323
[35,     1] loss: 3254.065
[36,     1] loss: 3052.481
[37,     1] loss: 2931.169
[38,     1] loss: 2799.041
[39,     1] loss: 2946.065
[40,     1] loss: 2663.495
[41,     1] loss: 2768.166
[42,     1] loss: 2720.953
[43,     1] loss: 3143.716
[44,     1] loss: 2735.414
[45,     1] loss: 3139.000
[46,     1] loss: 2809.264
[47,     1] loss: 2962.442
[48,     1] loss: 2464.390
[49,     1] loss: 2694.631
[50,     1] loss: 2850.211
[51,     1] loss: 2346.094
[52,     1] loss: 2370.807
[53,     1] loss: 2781.845
[54,     1] loss: 2530.844
[55,     1] loss: 2389.638
[56,     1] loss: 2106.085
[57,     1] loss: 2132.087
[58,     1] loss: 2319.875
[59,     1] loss: 2638.516
[60,     1] loss: 2846.574
[61,     1] loss: 1956.411
[62,     1] loss: 2245.000
[63,     1] loss: 2145.383
[64,     1] loss: 1857.527
[65,     1] loss: 2095.169
[66,     1] loss: 2205.793
[67,     1] loss: 3513.012
[68,     1] loss: 3692.357
[69,     1] loss: 2692.819
[70,     1] loss: 2404.359
[71,     1] loss: 3280.129
[72,     1] loss: 3096.329
[73,     1] loss: 2940.958
[74,     1] loss: 2876.179
[75,     1] loss: 2938.835
[76,     1] loss: 2867.208
[77,     1] loss: 2646.334
[78,     1] loss: 2477.314
[79,     1] loss: 2853.098
[80,     1] loss: 2547.161
Early stopping applied (best metric=0.44502371549606323)
Finished Training
Total time taken: 10.945696353912354
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4174.289
[2,     1] loss: 4170.468
[3,     1] loss: 4185.932
[4,     1] loss: 4158.519
[5,     1] loss: 4172.964
[6,     1] loss: 4174.975
[7,     1] loss: 4160.732
[8,     1] loss: 4168.634
[9,     1] loss: 4185.751
[10,     1] loss: 4148.934
[11,     1] loss: 4119.617
[12,     1] loss: 4115.857
[13,     1] loss: 4055.019
[14,     1] loss: 3985.183
[15,     1] loss: 3826.264
[16,     1] loss: 3661.757
[17,     1] loss: 3638.314
[18,     1] loss: 3865.465
[19,     1] loss: 3322.410
[20,     1] loss: 3460.880
[21,     1] loss: 3409.891
[22,     1] loss: 3804.292
[23,     1] loss: 3543.529
[24,     1] loss: 3533.582
[25,     1] loss: 3183.829
[26,     1] loss: 3243.737
[27,     1] loss: 3096.032
[28,     1] loss: 3191.206
[29,     1] loss: 3261.146
[30,     1] loss: 3026.894
[31,     1] loss: 2785.634
[32,     1] loss: 3083.550
[33,     1] loss: 2623.894
[34,     1] loss: 2831.476
[35,     1] loss: 2826.264
[36,     1] loss: 3836.643
[37,     1] loss: 2722.822
[38,     1] loss: 3454.869
[39,     1] loss: 2875.482
[40,     1] loss: 3172.754
[41,     1] loss: 3174.558
[42,     1] loss: 2783.963
[43,     1] loss: 2786.517
[44,     1] loss: 2804.414
[45,     1] loss: 2710.415
[46,     1] loss: 2846.563
[47,     1] loss: 2888.528
[48,     1] loss: 2681.614
[49,     1] loss: 2616.329
[50,     1] loss: 2490.251
[51,     1] loss: 2296.373
[52,     1] loss: 2373.657
[53,     1] loss: 1965.177
[54,     1] loss: 2407.083
[55,     1] loss: 2435.767
[56,     1] loss: 2704.988
[57,     1] loss: 2457.265
[58,     1] loss: 2525.672
[59,     1] loss: 2080.018
[60,     1] loss: 2734.368
[61,     1] loss: 2233.813
[62,     1] loss: 2459.993
[63,     1] loss: 2363.269
[64,     1] loss: 2010.056
[65,     1] loss: 2041.983
[66,     1] loss: 1620.928
[67,     1] loss: 2015.680
[68,     1] loss: 2533.868
[69,     1] loss: 2271.326
[70,     1] loss: 1890.932
[71,     1] loss: 2115.014
[72,     1] loss: 2070.795
[73,     1] loss: 1765.347
[74,     1] loss: 1855.186
[75,     1] loss: 1513.135
[76,     1] loss: 2277.357
[77,     1] loss: 3438.138
[78,     1] loss: 2069.596
[79,     1] loss: 1845.156
[80,     1] loss: 1859.131
[81,     1] loss: 2147.568
[82,     1] loss: 1884.751
[83,     1] loss: 2242.002
[84,     1] loss: 1678.698
[85,     1] loss: 1787.045
[86,     1] loss: 1745.135
[87,     1] loss: 1932.121
[88,     1] loss: 1535.035
Early stopping applied (best metric=0.42036256194114685)
Finished Training
Total time taken: 12.630799770355225
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4175.632
[2,     1] loss: 4170.733
[3,     1] loss: 4176.795
[4,     1] loss: 4169.264
[5,     1] loss: 4176.555
[6,     1] loss: 4162.609
[7,     1] loss: 4145.554
[8,     1] loss: 4124.692
[9,     1] loss: 4097.215
[10,     1] loss: 4040.158
[11,     1] loss: 3976.877
[12,     1] loss: 3882.251
[13,     1] loss: 3857.108
[14,     1] loss: 3887.117
[15,     1] loss: 3640.691
[16,     1] loss: 3813.388
[17,     1] loss: 3755.806
[18,     1] loss: 3790.100
[19,     1] loss: 3601.701
[20,     1] loss: 3509.295
[21,     1] loss: 3649.870
[22,     1] loss: 3341.555
[23,     1] loss: 3288.952
[24,     1] loss: 3253.341
[25,     1] loss: 3449.963
[26,     1] loss: 3177.776
[27,     1] loss: 3451.647
[28,     1] loss: 3279.266
[29,     1] loss: 2957.340
[30,     1] loss: 2939.837
[31,     1] loss: 3035.033
[32,     1] loss: 3132.297
[33,     1] loss: 2792.545
[34,     1] loss: 2732.921
[35,     1] loss: 2773.608
[36,     1] loss: 2807.081
[37,     1] loss: 2596.998
[38,     1] loss: 2672.700
[39,     1] loss: 2563.001
[40,     1] loss: 2588.980
[41,     1] loss: 2689.636
[42,     1] loss: 3512.844
[43,     1] loss: 2411.895
[44,     1] loss: 2822.180
[45,     1] loss: 2520.049
[46,     1] loss: 3112.431
[47,     1] loss: 2957.933
[48,     1] loss: 2630.093
[49,     1] loss: 2873.748
[50,     1] loss: 2764.478
[51,     1] loss: 2473.996
[52,     1] loss: 2544.721
[53,     1] loss: 2785.530
[54,     1] loss: 2130.566
[55,     1] loss: 2693.430
[56,     1] loss: 2355.231
[57,     1] loss: 2342.994
[58,     1] loss: 2340.414
[59,     1] loss: 2204.719
[60,     1] loss: 2246.767
[61,     1] loss: 2140.731
[62,     1] loss: 1991.015
[63,     1] loss: 2138.887
[64,     1] loss: 2673.866
[65,     1] loss: 2398.117
[66,     1] loss: 2102.478
[67,     1] loss: 1878.735
[68,     1] loss: 2046.764
[69,     1] loss: 1936.789
[70,     1] loss: 2095.655
[71,     1] loss: 1779.782
[72,     1] loss: 2054.568
[73,     1] loss: 1756.800
[74,     1] loss: 1763.226
[75,     1] loss: 2143.590
[76,     1] loss: 1905.958
[77,     1] loss: 1623.093
[78,     1] loss: 1777.608
[79,     1] loss: 1460.243
Early stopping applied (best metric=0.4206180274486542)
Finished Training
Total time taken: 10.57082486152649
{'Hydroxylation-K Validation Accuracy': 0.6924645390070922, 'Hydroxylation-K Validation Sensitivity': 0.6985185185185185, 'Hydroxylation-K Validation Specificity': 0.6894736842105263, 'Hydroxylation-K Validation Precision': 0.3761734281044626, 'Hydroxylation-K AUC ROC': 0.7906432748538011, 'Hydroxylation-K AUC PR': 0.5838300966013815, 'Hydroxylation-K MCC': 0.3288629534984866, 'Hydroxylation-K F1': 0.4813970133604001, 'Validation Loss (Hydroxylation-K)': 0.4776732822259267, 'Hydroxylation-P Validation Accuracy': 0.7199456203576806, 'Hydroxylation-P Validation Sensitivity': 0.7419576719576719, 'Hydroxylation-P Validation Specificity': 0.715152875455135, 'Hydroxylation-P Validation Precision': 0.36610704265960864, 'Hydroxylation-P AUC ROC': 0.7913274065840279, 'Hydroxylation-P AUC PR': 0.5129355153912513, 'Hydroxylation-P MCC': 0.3669745174069929, 'Hydroxylation-P F1': 0.4867619557956427, 'Validation Loss (Hydroxylation-P)': 0.42568602561950686, 'Validation Loss (total)': 0.903359301884969, 'TimeToTrain': 11.813488976160686}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009802953707870966,
 'learning_rate_Hydroxylation-K': 0.005519437175876368,
 'learning_rate_Hydroxylation-P': 0.004515571999418938,
 'log_base': 1.7941326877835417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4265066814,
 'sample_weights': [12.86941729530702, 1.6053346149924896],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.767701606510297,
 'weight_decay_Hydroxylation-K': 7.092960592567048,
 'weight_decay_Hydroxylation-P': 0.7081588681133288}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1512.413
[2,     1] loss: 1516.219
[3,     1] loss: 1510.613
[4,     1] loss: 1518.975
[5,     1] loss: 1505.952
[6,     1] loss: 1513.532
[7,     1] loss: 1506.806
[8,     1] loss: 1507.862
[9,     1] loss: 1502.532
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005245684385926154,
 'learning_rate_Hydroxylation-K': 0.0047815110279302245,
 'learning_rate_Hydroxylation-P': 0.0032293620659609693,
 'log_base': 2.746803959344983,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1025148742,
 'sample_weights': [2.856084012083201, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8312493848835563,
 'weight_decay_Hydroxylation-K': 8.266074497379854,
 'weight_decay_Hydroxylation-P': 2.6736924442669285}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.477
[2,     1] loss: 1262.598
[3,     1] loss: 1253.938
[4,     1] loss: 1260.207
[5,     1] loss: 1262.651
[6,     1] loss: 1251.280
[7,     1] loss: 1251.377
[8,     1] loss: 1248.622
[9,     1] loss: 1236.446
[10,     1] loss: 1228.550
[11,     1] loss: 1204.403
[12,     1] loss: 1190.932
[13,     1] loss: 1151.829
[14,     1] loss: 1142.958
[15,     1] loss: 1104.060
[16,     1] loss: 1096.209
[17,     1] loss: 1033.923
[18,     1] loss: 1042.614
[19,     1] loss: 1061.713
[20,     1] loss: 1032.980
[21,     1] loss: 997.940
[22,     1] loss: 1000.236
[23,     1] loss: 1026.788
[24,     1] loss: 994.385
[25,     1] loss: 1003.690
[26,     1] loss: 1030.814
[27,     1] loss: 990.467
[28,     1] loss: 986.459
[29,     1] loss: 956.421
[30,     1] loss: 994.589
[31,     1] loss: 961.964
[32,     1] loss: 1036.770
[33,     1] loss: 954.031
[34,     1] loss: 1028.236
[35,     1] loss: 904.288
[36,     1] loss: 948.221
[37,     1] loss: 891.091
[38,     1] loss: 896.012
[39,     1] loss: 920.409
[40,     1] loss: 845.132
[41,     1] loss: 923.115
[42,     1] loss: 966.490
[43,     1] loss: 902.795
[44,     1] loss: 837.373
[45,     1] loss: 909.408
[46,     1] loss: 850.031
[47,     1] loss: 868.049
[48,     1] loss: 855.997
[49,     1] loss: 855.750
[50,     1] loss: 821.956
[51,     1] loss: 782.032
[52,     1] loss: 837.721
[53,     1] loss: 820.208
[54,     1] loss: 726.554
[55,     1] loss: 855.788
[56,     1] loss: 730.479
[57,     1] loss: 840.858
[58,     1] loss: 757.466
[59,     1] loss: 774.079
[60,     1] loss: 720.073
[61,     1] loss: 684.218
[62,     1] loss: 766.291
[63,     1] loss: 731.263
[64,     1] loss: 775.344
[65,     1] loss: 677.780
[66,     1] loss: 724.721
[67,     1] loss: 675.563
[68,     1] loss: 753.332
[69,     1] loss: 711.455
[70,     1] loss: 683.618
[71,     1] loss: 707.239
[72,     1] loss: 680.850
[73,     1] loss: 647.387
[74,     1] loss: 650.110
[75,     1] loss: 589.592
[76,     1] loss: 611.004
[77,     1] loss: 598.750
[78,     1] loss: 609.689
[79,     1] loss: 574.838
[80,     1] loss: 676.985
[81,     1] loss: 630.125
[82,     1] loss: 596.656
[83,     1] loss: 566.803
[84,     1] loss: 599.703
[85,     1] loss: 641.098
[86,     1] loss: 551.311
[87,     1] loss: 585.007
[88,     1] loss: 692.150
[89,     1] loss: 521.471
[90,     1] loss: 539.696
[91,     1] loss: 523.966
[92,     1] loss: 471.119
[93,     1] loss: 510.715
[94,     1] loss: 469.820
[95,     1] loss: 494.650
[96,     1] loss: 456.395
[97,     1] loss: 489.033
[98,     1] loss: 520.730
[99,     1] loss: 451.009
[100,     1] loss: 494.888
[101,     1] loss: 439.114
[102,     1] loss: 457.464
[103,     1] loss: 486.372
[104,     1] loss: 405.021
[105,     1] loss: 424.237
[106,     1] loss: 502.951
[107,     1] loss: 497.682
[108,     1] loss: 431.758
[109,     1] loss: 525.234
[110,     1] loss: 426.694
[111,     1] loss: 389.711
[112,     1] loss: 394.383
[113,     1] loss: 403.388
[114,     1] loss: 413.686
Early stopping applied (best metric=0.31967905163764954)
Finished Training
Total time taken: 15.246766328811646
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.053
[2,     1] loss: 1261.574
[3,     1] loss: 1257.886
[4,     1] loss: 1259.356
[5,     1] loss: 1256.746
[6,     1] loss: 1256.776
[7,     1] loss: 1256.656
[8,     1] loss: 1253.201
[9,     1] loss: 1251.477
[10,     1] loss: 1246.840
[11,     1] loss: 1242.910
[12,     1] loss: 1234.241
[13,     1] loss: 1221.889
[14,     1] loss: 1180.264
[15,     1] loss: 1186.496
[16,     1] loss: 1152.949
[17,     1] loss: 1126.231
[18,     1] loss: 1123.432
[19,     1] loss: 1077.874
[20,     1] loss: 1068.423
[21,     1] loss: 1063.296
[22,     1] loss: 1007.593
[23,     1] loss: 1031.132
[24,     1] loss: 995.104
[25,     1] loss: 1061.714
[26,     1] loss: 1022.246
[27,     1] loss: 975.961
[28,     1] loss: 971.098
[29,     1] loss: 1006.329
[30,     1] loss: 945.169
[31,     1] loss: 958.347
[32,     1] loss: 912.777
[33,     1] loss: 923.041
[34,     1] loss: 929.650
[35,     1] loss: 918.627
[36,     1] loss: 902.562
[37,     1] loss: 923.985
[38,     1] loss: 913.258
[39,     1] loss: 880.481
[40,     1] loss: 904.706
[41,     1] loss: 906.703
[42,     1] loss: 881.036
[43,     1] loss: 838.272
[44,     1] loss: 854.239
[45,     1] loss: 913.376
[46,     1] loss: 843.385
[47,     1] loss: 879.978
[48,     1] loss: 829.909
[49,     1] loss: 848.289
[50,     1] loss: 853.552
[51,     1] loss: 828.175
[52,     1] loss: 796.299
[53,     1] loss: 789.436
[54,     1] loss: 739.769
[55,     1] loss: 784.880
[56,     1] loss: 905.789
[57,     1] loss: 708.624
[58,     1] loss: 833.560
[59,     1] loss: 773.752
[60,     1] loss: 813.168
[61,     1] loss: 731.442
[62,     1] loss: 808.775
[63,     1] loss: 719.526
[64,     1] loss: 725.981
[65,     1] loss: 714.309
[66,     1] loss: 695.656
[67,     1] loss: 693.617
[68,     1] loss: 697.460
[69,     1] loss: 648.758
[70,     1] loss: 786.282
[71,     1] loss: 636.407
[72,     1] loss: 650.627
[73,     1] loss: 653.822
[74,     1] loss: 631.302
[75,     1] loss: 643.919
[76,     1] loss: 642.020
[77,     1] loss: 593.657
[78,     1] loss: 644.565
[79,     1] loss: 597.758
[80,     1] loss: 671.248
[81,     1] loss: 628.114
[82,     1] loss: 567.974
[83,     1] loss: 647.490
[84,     1] loss: 538.931
[85,     1] loss: 569.377
[86,     1] loss: 538.210
[87,     1] loss: 562.039
[88,     1] loss: 615.679
Early stopping applied (best metric=0.37025055289268494)
Finished Training
Total time taken: 11.807739019393921
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.214
[2,     1] loss: 1255.295
[3,     1] loss: 1254.218
[4,     1] loss: 1251.532
[5,     1] loss: 1252.622
[6,     1] loss: 1246.724
[7,     1] loss: 1233.336
[8,     1] loss: 1209.748
[9,     1] loss: 1178.351
[10,     1] loss: 1150.092
[11,     1] loss: 1094.217
[12,     1] loss: 1085.774
[13,     1] loss: 1059.678
[14,     1] loss: 1038.362
[15,     1] loss: 1029.145
[16,     1] loss: 983.812
[17,     1] loss: 1013.065
[18,     1] loss: 994.145
[19,     1] loss: 998.461
[20,     1] loss: 1006.444
[21,     1] loss: 1006.013
[22,     1] loss: 951.641
[23,     1] loss: 1002.995
[24,     1] loss: 960.075
[25,     1] loss: 982.958
[26,     1] loss: 964.435
[27,     1] loss: 962.009
[28,     1] loss: 885.860
[29,     1] loss: 919.510
[30,     1] loss: 906.703
[31,     1] loss: 905.318
[32,     1] loss: 903.212
[33,     1] loss: 890.555
[34,     1] loss: 917.703
[35,     1] loss: 911.275
[36,     1] loss: 902.251
[37,     1] loss: 913.357
[38,     1] loss: 884.848
[39,     1] loss: 875.339
[40,     1] loss: 829.582
[41,     1] loss: 876.515
[42,     1] loss: 901.081
[43,     1] loss: 850.288
[44,     1] loss: 858.092
[45,     1] loss: 842.244
[46,     1] loss: 818.978
[47,     1] loss: 797.787
[48,     1] loss: 826.025
[49,     1] loss: 820.412
[50,     1] loss: 754.655
[51,     1] loss: 783.659
[52,     1] loss: 733.301
[53,     1] loss: 753.231
[54,     1] loss: 739.166
[55,     1] loss: 764.448
[56,     1] loss: 798.809
[57,     1] loss: 740.953
[58,     1] loss: 832.454
[59,     1] loss: 712.153
[60,     1] loss: 753.554
[61,     1] loss: 806.624
[62,     1] loss: 698.115
[63,     1] loss: 785.396
[64,     1] loss: 689.050
[65,     1] loss: 764.014
[66,     1] loss: 651.790
[67,     1] loss: 719.274
[68,     1] loss: 662.051
[69,     1] loss: 737.930
[70,     1] loss: 617.942
[71,     1] loss: 677.210
[72,     1] loss: 589.187
[73,     1] loss: 592.226
[74,     1] loss: 610.602
[75,     1] loss: 548.966
[76,     1] loss: 623.272
[77,     1] loss: 643.477
[78,     1] loss: 534.884
[79,     1] loss: 573.579
[80,     1] loss: 615.044
[81,     1] loss: 521.358
[82,     1] loss: 577.569
[83,     1] loss: 575.438
[84,     1] loss: 523.503
[85,     1] loss: 472.724
[86,     1] loss: 527.308
[87,     1] loss: 450.318
[88,     1] loss: 514.294
[89,     1] loss: 683.772
[90,     1] loss: 618.638
[91,     1] loss: 558.666
[92,     1] loss: 513.679
[93,     1] loss: 581.615
[94,     1] loss: 535.052
[95,     1] loss: 479.621
[96,     1] loss: 517.872
[97,     1] loss: 438.256
[98,     1] loss: 476.926
[99,     1] loss: 499.044
[100,     1] loss: 424.972
[101,     1] loss: 592.734
[102,     1] loss: 579.877
[103,     1] loss: 413.977
[104,     1] loss: 489.656
[105,     1] loss: 434.179
[106,     1] loss: 452.036
[107,     1] loss: 361.146
[108,     1] loss: 418.493
[109,     1] loss: 384.231
[110,     1] loss: 413.769
[111,     1] loss: 429.366
[112,     1] loss: 351.305
[113,     1] loss: 312.481
[114,     1] loss: 382.611
[115,     1] loss: 378.903
[116,     1] loss: 377.015
[117,     1] loss: 337.358
[118,     1] loss: 351.610
[119,     1] loss: 376.786
[120,     1] loss: 360.408
[121,     1] loss: 313.461
[122,     1] loss: 338.544
[123,     1] loss: 367.713
[124,     1] loss: 296.148
[125,     1] loss: 331.838
[126,     1] loss: 266.986
[127,     1] loss: 444.658
[128,     1] loss: 469.687
[129,     1] loss: 520.400
[130,     1] loss: 331.363
[131,     1] loss: 383.562
[132,     1] loss: 350.657
[133,     1] loss: 375.600
[134,     1] loss: 376.347
[135,     1] loss: 332.192
[136,     1] loss: 365.411
[137,     1] loss: 263.469
[138,     1] loss: 313.666
[139,     1] loss: 293.065
[140,     1] loss: 291.479
Early stopping applied (best metric=0.3106366991996765)
Finished Training
Total time taken: 20.293395280838013
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.537
[2,     1] loss: 1263.295
[3,     1] loss: 1257.637
[4,     1] loss: 1258.621
[5,     1] loss: 1260.513
[6,     1] loss: 1256.473
[7,     1] loss: 1256.710
[8,     1] loss: 1256.922
[9,     1] loss: 1255.258
[10,     1] loss: 1254.909
[11,     1] loss: 1254.657
[12,     1] loss: 1253.413
[13,     1] loss: 1249.715
[14,     1] loss: 1245.676
[15,     1] loss: 1239.936
[16,     1] loss: 1230.809
[17,     1] loss: 1216.922
[18,     1] loss: 1192.468
[19,     1] loss: 1165.647
[20,     1] loss: 1131.459
[21,     1] loss: 1103.664
[22,     1] loss: 1067.802
[23,     1] loss: 1054.670
[24,     1] loss: 998.984
[25,     1] loss: 1023.283
[26,     1] loss: 1021.000
[27,     1] loss: 1057.819
[28,     1] loss: 1030.503
[29,     1] loss: 1023.160
[30,     1] loss: 994.078
[31,     1] loss: 979.591
[32,     1] loss: 997.854
[33,     1] loss: 999.634
[34,     1] loss: 946.705
[35,     1] loss: 940.196
[36,     1] loss: 947.932
[37,     1] loss: 944.427
[38,     1] loss: 958.411
[39,     1] loss: 935.242
[40,     1] loss: 913.535
[41,     1] loss: 872.552
[42,     1] loss: 883.986
[43,     1] loss: 916.112
[44,     1] loss: 906.892
[45,     1] loss: 901.614
[46,     1] loss: 881.170
[47,     1] loss: 848.161
[48,     1] loss: 859.690
[49,     1] loss: 810.985
[50,     1] loss: 869.891
[51,     1] loss: 823.418
[52,     1] loss: 769.961
[53,     1] loss: 784.265
[54,     1] loss: 829.105
[55,     1] loss: 792.533
[56,     1] loss: 743.683
[57,     1] loss: 785.026
[58,     1] loss: 724.834
[59,     1] loss: 732.124
[60,     1] loss: 750.270
[61,     1] loss: 886.085
[62,     1] loss: 772.379
[63,     1] loss: 727.236
[64,     1] loss: 778.930
[65,     1] loss: 728.204
[66,     1] loss: 725.634
[67,     1] loss: 697.696
[68,     1] loss: 709.526
[69,     1] loss: 696.036
[70,     1] loss: 694.093
[71,     1] loss: 693.915
[72,     1] loss: 631.131
[73,     1] loss: 693.451
[74,     1] loss: 624.694
[75,     1] loss: 573.060
[76,     1] loss: 589.512
[77,     1] loss: 620.599
[78,     1] loss: 594.483
[79,     1] loss: 583.671
[80,     1] loss: 576.683
[81,     1] loss: 532.816
[82,     1] loss: 574.116
[83,     1] loss: 601.398
[84,     1] loss: 628.695
[85,     1] loss: 835.281
[86,     1] loss: 564.236
[87,     1] loss: 738.126
[88,     1] loss: 608.342
[89,     1] loss: 682.524
[90,     1] loss: 589.648
[91,     1] loss: 680.037
[92,     1] loss: 545.894
[93,     1] loss: 586.223
[94,     1] loss: 505.970
[95,     1] loss: 604.264
[96,     1] loss: 465.983
[97,     1] loss: 569.871
[98,     1] loss: 520.875
[99,     1] loss: 490.961
[100,     1] loss: 544.095
[101,     1] loss: 409.417
[102,     1] loss: 475.761
[103,     1] loss: 415.976
[104,     1] loss: 413.697
[105,     1] loss: 425.519
[106,     1] loss: 435.780
[107,     1] loss: 444.390
[108,     1] loss: 403.765
[109,     1] loss: 417.379
[110,     1] loss: 385.251
[111,     1] loss: 428.451
[112,     1] loss: 435.537
[113,     1] loss: 424.303
[114,     1] loss: 441.100
[115,     1] loss: 319.205
[116,     1] loss: 453.349
[117,     1] loss: 534.137
[118,     1] loss: 427.541
[119,     1] loss: 358.060
[120,     1] loss: 410.884
[121,     1] loss: 412.009
[122,     1] loss: 351.677
[123,     1] loss: 382.783
[124,     1] loss: 376.741
[125,     1] loss: 386.929
[126,     1] loss: 372.771
[127,     1] loss: 390.193
[128,     1] loss: 436.931
[129,     1] loss: 371.484
[130,     1] loss: 407.153
[131,     1] loss: 391.703
[132,     1] loss: 316.195
[133,     1] loss: 387.340
[134,     1] loss: 398.849
[135,     1] loss: 362.337
[136,     1] loss: 326.288
[137,     1] loss: 326.152
[138,     1] loss: 300.703
[139,     1] loss: 326.698
[140,     1] loss: 351.603
[141,     1] loss: 471.477
[142,     1] loss: 453.723
[143,     1] loss: 366.267
[144,     1] loss: 341.860
[145,     1] loss: 374.427
[146,     1] loss: 310.541
Early stopping applied (best metric=0.3692564070224762)
Finished Training
Total time taken: 19.582963466644287
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1259.942
[2,     1] loss: 1256.106
[3,     1] loss: 1254.729
[4,     1] loss: 1248.780
[5,     1] loss: 1256.106
[6,     1] loss: 1241.242
[7,     1] loss: 1220.748
[8,     1] loss: 1195.413
[9,     1] loss: 1165.787
[10,     1] loss: 1103.268
[11,     1] loss: 1111.826
[12,     1] loss: 1139.323
[13,     1] loss: 1080.232
[14,     1] loss: 1062.241
[15,     1] loss: 1100.698
[16,     1] loss: 1058.987
[17,     1] loss: 1088.254
[18,     1] loss: 1045.339
[19,     1] loss: 1059.573
[20,     1] loss: 1041.788
[21,     1] loss: 1050.195
[22,     1] loss: 1002.833
[23,     1] loss: 1019.628
[24,     1] loss: 987.468
[25,     1] loss: 1018.923
[26,     1] loss: 1004.898
[27,     1] loss: 978.740
[28,     1] loss: 975.778
[29,     1] loss: 925.191
[30,     1] loss: 950.568
[31,     1] loss: 951.824
[32,     1] loss: 939.682
[33,     1] loss: 921.518
[34,     1] loss: 912.990
[35,     1] loss: 964.786
[36,     1] loss: 947.141
[37,     1] loss: 890.448
[38,     1] loss: 896.716
[39,     1] loss: 867.806
[40,     1] loss: 862.040
[41,     1] loss: 864.062
[42,     1] loss: 871.320
[43,     1] loss: 927.642
[44,     1] loss: 811.367
[45,     1] loss: 929.246
[46,     1] loss: 774.112
[47,     1] loss: 886.904
[48,     1] loss: 805.151
[49,     1] loss: 857.792
[50,     1] loss: 725.199
[51,     1] loss: 841.581
[52,     1] loss: 779.553
[53,     1] loss: 831.839
[54,     1] loss: 671.286
[55,     1] loss: 837.964
[56,     1] loss: 790.467
[57,     1] loss: 673.484
[58,     1] loss: 668.581
[59,     1] loss: 675.803
[60,     1] loss: 654.902
[61,     1] loss: 662.356
[62,     1] loss: 654.804
[63,     1] loss: 772.592
[64,     1] loss: 753.620
[65,     1] loss: 813.435
[66,     1] loss: 626.176
[67,     1] loss: 714.369
[68,     1] loss: 624.011
[69,     1] loss: 698.092
[70,     1] loss: 600.641
[71,     1] loss: 646.621
[72,     1] loss: 565.206
[73,     1] loss: 586.302
[74,     1] loss: 545.182
[75,     1] loss: 544.936
[76,     1] loss: 579.455
[77,     1] loss: 618.364
[78,     1] loss: 585.495
[79,     1] loss: 608.363
[80,     1] loss: 524.262
[81,     1] loss: 493.110
[82,     1] loss: 520.129
Early stopping applied (best metric=0.3633609414100647)
Finished Training
Total time taken: 11.020336151123047
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.327
[2,     1] loss: 1263.466
[3,     1] loss: 1257.329
[4,     1] loss: 1254.716
[5,     1] loss: 1252.767
[6,     1] loss: 1253.922
[7,     1] loss: 1253.886
[8,     1] loss: 1253.063
[9,     1] loss: 1255.394
[10,     1] loss: 1252.473
[11,     1] loss: 1250.229
[12,     1] loss: 1250.659
[13,     1] loss: 1242.844
[14,     1] loss: 1234.783
[15,     1] loss: 1217.365
[16,     1] loss: 1197.500
[17,     1] loss: 1156.277
[18,     1] loss: 1127.539
[19,     1] loss: 1093.689
[20,     1] loss: 1092.644
[21,     1] loss: 1045.608
[22,     1] loss: 1078.098
[23,     1] loss: 1066.528
[24,     1] loss: 1026.020
[25,     1] loss: 1039.480
[26,     1] loss: 1007.501
[27,     1] loss: 1057.446
[28,     1] loss: 1055.920
[29,     1] loss: 1058.080
[30,     1] loss: 1001.415
[31,     1] loss: 965.306
[32,     1] loss: 999.638
[33,     1] loss: 974.240
[34,     1] loss: 987.618
[35,     1] loss: 938.154
[36,     1] loss: 978.900
[37,     1] loss: 968.215
[38,     1] loss: 920.928
[39,     1] loss: 942.719
[40,     1] loss: 922.795
[41,     1] loss: 885.852
[42,     1] loss: 919.086
[43,     1] loss: 901.963
[44,     1] loss: 857.197
[45,     1] loss: 942.600
[46,     1] loss: 850.575
[47,     1] loss: 877.941
[48,     1] loss: 878.953
[49,     1] loss: 817.043
[50,     1] loss: 852.225
[51,     1] loss: 803.331
[52,     1] loss: 806.722
[53,     1] loss: 802.715
[54,     1] loss: 757.761
[55,     1] loss: 799.723
[56,     1] loss: 806.006
[57,     1] loss: 814.167
[58,     1] loss: 769.691
[59,     1] loss: 861.097
[60,     1] loss: 818.104
[61,     1] loss: 756.364
[62,     1] loss: 838.122
[63,     1] loss: 743.741
[64,     1] loss: 744.397
[65,     1] loss: 758.065
[66,     1] loss: 750.876
[67,     1] loss: 733.242
[68,     1] loss: 740.852
[69,     1] loss: 685.841
[70,     1] loss: 647.011
[71,     1] loss: 651.078
[72,     1] loss: 666.108
[73,     1] loss: 595.303
[74,     1] loss: 655.133
[75,     1] loss: 730.853
[76,     1] loss: 646.431
[77,     1] loss: 651.895
[78,     1] loss: 723.791
[79,     1] loss: 556.883
[80,     1] loss: 714.642
[81,     1] loss: 581.888
[82,     1] loss: 619.188
[83,     1] loss: 509.623
[84,     1] loss: 595.200
[85,     1] loss: 525.668
[86,     1] loss: 532.413
[87,     1] loss: 611.647
[88,     1] loss: 453.896
[89,     1] loss: 552.007
[90,     1] loss: 533.326
[91,     1] loss: 528.837
[92,     1] loss: 588.051
[93,     1] loss: 601.903
[94,     1] loss: 497.386
[95,     1] loss: 599.198
[96,     1] loss: 513.940
[97,     1] loss: 562.898
[98,     1] loss: 442.668
[99,     1] loss: 599.882
[100,     1] loss: 556.596
[101,     1] loss: 509.912
[102,     1] loss: 498.387
Early stopping applied (best metric=0.3274390995502472)
Finished Training
Total time taken: 13.585770606994629
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.872
[2,     1] loss: 1259.159
[3,     1] loss: 1258.577
[4,     1] loss: 1257.533
[5,     1] loss: 1252.677
[6,     1] loss: 1257.809
[7,     1] loss: 1253.753
[8,     1] loss: 1254.284
[9,     1] loss: 1252.536
[10,     1] loss: 1247.674
[11,     1] loss: 1247.264
[12,     1] loss: 1240.298
[13,     1] loss: 1231.016
[14,     1] loss: 1212.768
[15,     1] loss: 1189.294
[16,     1] loss: 1172.649
[17,     1] loss: 1140.473
[18,     1] loss: 1094.955
[19,     1] loss: 1084.356
[20,     1] loss: 1048.682
[21,     1] loss: 1059.406
[22,     1] loss: 1023.943
[23,     1] loss: 1036.479
[24,     1] loss: 1008.600
[25,     1] loss: 984.408
[26,     1] loss: 999.889
[27,     1] loss: 956.124
[28,     1] loss: 970.602
[29,     1] loss: 987.444
[30,     1] loss: 952.669
[31,     1] loss: 946.746
[32,     1] loss: 937.815
[33,     1] loss: 941.196
[34,     1] loss: 928.485
[35,     1] loss: 916.005
[36,     1] loss: 949.215
[37,     1] loss: 866.904
[38,     1] loss: 903.648
[39,     1] loss: 886.241
[40,     1] loss: 970.484
[41,     1] loss: 852.445
[42,     1] loss: 962.071
[43,     1] loss: 872.924
[44,     1] loss: 921.551
[45,     1] loss: 855.236
[46,     1] loss: 888.512
[47,     1] loss: 819.042
[48,     1] loss: 827.924
[49,     1] loss: 783.550
[50,     1] loss: 878.759
[51,     1] loss: 825.344
[52,     1] loss: 764.946
[53,     1] loss: 784.626
[54,     1] loss: 698.175
[55,     1] loss: 748.802
[56,     1] loss: 725.223
[57,     1] loss: 745.548
[58,     1] loss: 750.315
[59,     1] loss: 742.838
[60,     1] loss: 824.235
[61,     1] loss: 674.823
[62,     1] loss: 755.916
[63,     1] loss: 702.805
[64,     1] loss: 759.513
[65,     1] loss: 643.050
[66,     1] loss: 737.952
[67,     1] loss: 631.243
[68,     1] loss: 797.490
[69,     1] loss: 662.523
[70,     1] loss: 657.797
[71,     1] loss: 605.107
[72,     1] loss: 561.727
[73,     1] loss: 623.913
[74,     1] loss: 543.626
[75,     1] loss: 544.203
[76,     1] loss: 496.728
[77,     1] loss: 506.833
[78,     1] loss: 536.743
[79,     1] loss: 542.832
[80,     1] loss: 522.990
[81,     1] loss: 596.294
[82,     1] loss: 564.288
[83,     1] loss: 507.644
[84,     1] loss: 523.918
[85,     1] loss: 580.461
[86,     1] loss: 487.664
[87,     1] loss: 581.021
[88,     1] loss: 527.513
[89,     1] loss: 532.159
[90,     1] loss: 522.231
[91,     1] loss: 493.063
[92,     1] loss: 448.075
[93,     1] loss: 446.539
[94,     1] loss: 465.349
[95,     1] loss: 478.895
[96,     1] loss: 625.657
[97,     1] loss: 659.993
[98,     1] loss: 506.085
[99,     1] loss: 501.213
[100,     1] loss: 530.017
[101,     1] loss: 478.787
[102,     1] loss: 507.686
[103,     1] loss: 485.260
[104,     1] loss: 423.396
[105,     1] loss: 409.320
[106,     1] loss: 466.804
[107,     1] loss: 436.818
[108,     1] loss: 430.337
[109,     1] loss: 407.194
[110,     1] loss: 374.321
[111,     1] loss: 442.387
[112,     1] loss: 392.596
[113,     1] loss: 422.512
[114,     1] loss: 390.496
[115,     1] loss: 368.561
[116,     1] loss: 344.079
Early stopping applied (best metric=0.36055129766464233)
Finished Training
Total time taken: 17.52583408355713
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.215
[2,     1] loss: 1264.907
[3,     1] loss: 1258.054
[4,     1] loss: 1257.774
[5,     1] loss: 1258.524
[6,     1] loss: 1260.561
[7,     1] loss: 1255.516
[8,     1] loss: 1253.651
[9,     1] loss: 1252.362
[10,     1] loss: 1258.285
[11,     1] loss: 1252.945
[12,     1] loss: 1244.681
[13,     1] loss: 1240.379
[14,     1] loss: 1237.284
[15,     1] loss: 1218.843
[16,     1] loss: 1205.658
[17,     1] loss: 1183.034
[18,     1] loss: 1150.960
[19,     1] loss: 1122.001
[20,     1] loss: 1129.214
[21,     1] loss: 1075.294
[22,     1] loss: 1058.973
[23,     1] loss: 1070.083
[24,     1] loss: 1081.891
[25,     1] loss: 1042.013
[26,     1] loss: 1034.986
[27,     1] loss: 1063.461
[28,     1] loss: 983.685
[29,     1] loss: 998.922
[30,     1] loss: 948.122
[31,     1] loss: 1059.144
[32,     1] loss: 996.998
[33,     1] loss: 963.662
[34,     1] loss: 971.465
[35,     1] loss: 938.551
[36,     1] loss: 973.230
[37,     1] loss: 936.740
[38,     1] loss: 933.612
[39,     1] loss: 907.899
[40,     1] loss: 956.574
[41,     1] loss: 901.768
[42,     1] loss: 961.711
[43,     1] loss: 883.605
[44,     1] loss: 965.896
[45,     1] loss: 931.013
[46,     1] loss: 914.795
[47,     1] loss: 905.940
[48,     1] loss: 835.234
[49,     1] loss: 891.653
[50,     1] loss: 853.644
[51,     1] loss: 839.543
[52,     1] loss: 852.333
[53,     1] loss: 805.453
[54,     1] loss: 773.023
[55,     1] loss: 772.216
[56,     1] loss: 732.924
[57,     1] loss: 827.597
[58,     1] loss: 847.606
[59,     1] loss: 859.964
[60,     1] loss: 782.509
[61,     1] loss: 807.436
[62,     1] loss: 816.791
[63,     1] loss: 817.876
[64,     1] loss: 818.461
[65,     1] loss: 739.975
[66,     1] loss: 764.241
[67,     1] loss: 682.896
[68,     1] loss: 729.113
[69,     1] loss: 765.099
[70,     1] loss: 660.835
[71,     1] loss: 840.518
[72,     1] loss: 742.075
[73,     1] loss: 777.229
[74,     1] loss: 734.566
[75,     1] loss: 766.257
[76,     1] loss: 661.193
[77,     1] loss: 730.093
[78,     1] loss: 690.250
[79,     1] loss: 642.586
[80,     1] loss: 674.884
[81,     1] loss: 679.893
[82,     1] loss: 646.710
[83,     1] loss: 601.946
[84,     1] loss: 563.733
[85,     1] loss: 649.900
[86,     1] loss: 588.333
[87,     1] loss: 556.167
[88,     1] loss: 495.808
[89,     1] loss: 546.671
[90,     1] loss: 552.196
[91,     1] loss: 512.912
[92,     1] loss: 543.799
[93,     1] loss: 476.957
[94,     1] loss: 538.894
[95,     1] loss: 513.293
[96,     1] loss: 498.419
[97,     1] loss: 473.783
[98,     1] loss: 506.012
[99,     1] loss: 512.334
[100,     1] loss: 617.124
[101,     1] loss: 552.262
[102,     1] loss: 482.964
[103,     1] loss: 494.811
[104,     1] loss: 468.032
[105,     1] loss: 457.324
[106,     1] loss: 459.774
[107,     1] loss: 413.389
[108,     1] loss: 510.660
Early stopping applied (best metric=0.3163025677204132)
Finished Training
Total time taken: 14.48221206665039
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.820
[2,     1] loss: 1260.359
[3,     1] loss: 1254.417
[4,     1] loss: 1258.221
[5,     1] loss: 1258.945
[6,     1] loss: 1255.451
[7,     1] loss: 1254.983
[8,     1] loss: 1251.476
[9,     1] loss: 1250.486
[10,     1] loss: 1251.059
[11,     1] loss: 1242.005
[12,     1] loss: 1235.933
[13,     1] loss: 1230.647
[14,     1] loss: 1202.791
[15,     1] loss: 1188.229
[16,     1] loss: 1148.041
[17,     1] loss: 1153.414
[18,     1] loss: 1124.029
[19,     1] loss: 1076.800
[20,     1] loss: 1098.314
[21,     1] loss: 1091.419
[22,     1] loss: 1069.186
[23,     1] loss: 1062.284
[24,     1] loss: 1028.541
[25,     1] loss: 1011.353
[26,     1] loss: 999.480
[27,     1] loss: 1002.438
[28,     1] loss: 1020.512
[29,     1] loss: 996.339
[30,     1] loss: 986.459
[31,     1] loss: 962.765
[32,     1] loss: 975.260
[33,     1] loss: 948.067
[34,     1] loss: 935.566
[35,     1] loss: 945.333
[36,     1] loss: 941.427
[37,     1] loss: 930.462
[38,     1] loss: 976.557
[39,     1] loss: 907.439
[40,     1] loss: 865.641
[41,     1] loss: 891.926
[42,     1] loss: 899.300
[43,     1] loss: 832.288
[44,     1] loss: 833.886
[45,     1] loss: 818.705
[46,     1] loss: 830.114
[47,     1] loss: 836.912
[48,     1] loss: 791.676
[49,     1] loss: 846.342
[50,     1] loss: 849.016
[51,     1] loss: 804.154
[52,     1] loss: 752.576
[53,     1] loss: 848.037
[54,     1] loss: 786.850
[55,     1] loss: 738.533
[56,     1] loss: 809.093
[57,     1] loss: 820.480
[58,     1] loss: 789.573
[59,     1] loss: 726.647
[60,     1] loss: 753.964
[61,     1] loss: 733.872
[62,     1] loss: 693.105
[63,     1] loss: 672.829
[64,     1] loss: 695.075
[65,     1] loss: 649.660
[66,     1] loss: 721.379
[67,     1] loss: 740.255
[68,     1] loss: 608.289
[69,     1] loss: 710.486
[70,     1] loss: 841.893
[71,     1] loss: 656.803
[72,     1] loss: 808.915
[73,     1] loss: 651.354
[74,     1] loss: 730.043
[75,     1] loss: 673.787
[76,     1] loss: 627.493
[77,     1] loss: 676.037
[78,     1] loss: 603.367
[79,     1] loss: 680.753
[80,     1] loss: 588.960
[81,     1] loss: 697.442
[82,     1] loss: 537.634
[83,     1] loss: 661.895
[84,     1] loss: 549.185
[85,     1] loss: 608.189
[86,     1] loss: 483.617
[87,     1] loss: 604.492
[88,     1] loss: 533.502
[89,     1] loss: 697.578
[90,     1] loss: 506.376
[91,     1] loss: 550.470
[92,     1] loss: 461.592
[93,     1] loss: 605.083
[94,     1] loss: 493.803
[95,     1] loss: 479.823
[96,     1] loss: 463.460
[97,     1] loss: 457.009
[98,     1] loss: 546.754
[99,     1] loss: 460.447
[100,     1] loss: 492.431
[101,     1] loss: 458.197
[102,     1] loss: 413.915
Early stopping applied (best metric=0.3126581907272339)
Finished Training
Total time taken: 17.074601411819458
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1258.805
[2,     1] loss: 1257.056
[3,     1] loss: 1254.039
[4,     1] loss: 1252.244
[5,     1] loss: 1247.742
[6,     1] loss: 1239.414
[7,     1] loss: 1202.769
[8,     1] loss: 1164.873
[9,     1] loss: 1127.872
[10,     1] loss: 1075.361
[11,     1] loss: 1041.549
[12,     1] loss: 1132.224
[13,     1] loss: 1058.206
[14,     1] loss: 1071.271
[15,     1] loss: 1043.762
[16,     1] loss: 1054.169
[17,     1] loss: 1006.198
[18,     1] loss: 1003.862
[19,     1] loss: 1001.811
[20,     1] loss: 1006.911
[21,     1] loss: 967.861
[22,     1] loss: 975.167
[23,     1] loss: 970.373
[24,     1] loss: 949.944
[25,     1] loss: 939.179
[26,     1] loss: 947.545
[27,     1] loss: 955.968
[28,     1] loss: 893.784
[29,     1] loss: 902.791
[30,     1] loss: 959.038
[31,     1] loss: 931.915
[32,     1] loss: 881.814
[33,     1] loss: 887.894
[34,     1] loss: 909.424
[35,     1] loss: 898.555
[36,     1] loss: 879.761
[37,     1] loss: 885.392
[38,     1] loss: 830.321
[39,     1] loss: 846.365
[40,     1] loss: 829.948
[41,     1] loss: 856.941
[42,     1] loss: 804.262
[43,     1] loss: 824.110
[44,     1] loss: 781.805
[45,     1] loss: 794.671
[46,     1] loss: 783.675
[47,     1] loss: 731.825
[48,     1] loss: 699.830
[49,     1] loss: 781.402
[50,     1] loss: 716.102
[51,     1] loss: 722.850
[52,     1] loss: 821.260
[53,     1] loss: 898.605
[54,     1] loss: 791.206
[55,     1] loss: 791.617
[56,     1] loss: 775.308
[57,     1] loss: 786.891
[58,     1] loss: 707.688
[59,     1] loss: 702.132
[60,     1] loss: 726.191
[61,     1] loss: 702.497
[62,     1] loss: 733.889
[63,     1] loss: 651.986
[64,     1] loss: 730.447
[65,     1] loss: 656.311
[66,     1] loss: 654.455
[67,     1] loss: 643.807
[68,     1] loss: 649.545
[69,     1] loss: 663.110
[70,     1] loss: 573.895
[71,     1] loss: 631.208
[72,     1] loss: 563.394
[73,     1] loss: 555.638
[74,     1] loss: 681.024
[75,     1] loss: 833.453
[76,     1] loss: 537.364
[77,     1] loss: 648.001
[78,     1] loss: 501.845
[79,     1] loss: 604.019
[80,     1] loss: 542.348
[81,     1] loss: 558.909
[82,     1] loss: 586.918
[83,     1] loss: 552.588
[84,     1] loss: 501.530
[85,     1] loss: 501.738
[86,     1] loss: 564.215
[87,     1] loss: 610.370
[88,     1] loss: 492.993
[89,     1] loss: 662.210
[90,     1] loss: 437.641
[91,     1] loss: 519.567
[92,     1] loss: 457.979
[93,     1] loss: 497.451
[94,     1] loss: 449.458
[95,     1] loss: 499.697
[96,     1] loss: 423.621
[97,     1] loss: 487.368
[98,     1] loss: 413.894
[99,     1] loss: 494.100
[100,     1] loss: 413.620
[101,     1] loss: 522.445
[102,     1] loss: 458.626
[103,     1] loss: 473.735
[104,     1] loss: 473.094
Early stopping applied (best metric=0.3946659564971924)
Finished Training
Total time taken: 17.441061973571777
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.812
[2,     1] loss: 1257.533
[3,     1] loss: 1257.449
[4,     1] loss: 1259.340
[5,     1] loss: 1254.642
[6,     1] loss: 1255.027
[7,     1] loss: 1256.190
[8,     1] loss: 1252.353
[9,     1] loss: 1245.927
[10,     1] loss: 1241.786
[11,     1] loss: 1226.942
[12,     1] loss: 1212.327
[13,     1] loss: 1183.756
[14,     1] loss: 1151.225
[15,     1] loss: 1105.881
[16,     1] loss: 1069.432
[17,     1] loss: 1063.273
[18,     1] loss: 1054.796
[19,     1] loss: 1044.637
[20,     1] loss: 1077.395
[21,     1] loss: 1027.314
[22,     1] loss: 989.587
[23,     1] loss: 1050.291
[24,     1] loss: 1022.349
[25,     1] loss: 1040.542
[26,     1] loss: 1019.491
[27,     1] loss: 1000.510
[28,     1] loss: 1058.896
[29,     1] loss: 989.724
[30,     1] loss: 987.720
[31,     1] loss: 957.919
[32,     1] loss: 973.904
[33,     1] loss: 945.057
[34,     1] loss: 938.016
[35,     1] loss: 944.573
[36,     1] loss: 909.713
[37,     1] loss: 881.801
[38,     1] loss: 967.182
[39,     1] loss: 922.041
[40,     1] loss: 892.168
[41,     1] loss: 968.044
[42,     1] loss: 914.419
[43,     1] loss: 846.846
[44,     1] loss: 858.964
[45,     1] loss: 829.851
[46,     1] loss: 877.430
[47,     1] loss: 913.810
[48,     1] loss: 877.243
[49,     1] loss: 838.032
[50,     1] loss: 838.551
[51,     1] loss: 813.028
[52,     1] loss: 850.124
[53,     1] loss: 756.672
[54,     1] loss: 780.494
[55,     1] loss: 900.206
[56,     1] loss: 815.838
[57,     1] loss: 759.455
[58,     1] loss: 790.737
[59,     1] loss: 733.950
[60,     1] loss: 716.370
[61,     1] loss: 704.477
[62,     1] loss: 761.078
[63,     1] loss: 696.127
[64,     1] loss: 691.943
[65,     1] loss: 678.015
[66,     1] loss: 635.295
[67,     1] loss: 700.896
[68,     1] loss: 646.783
[69,     1] loss: 628.395
[70,     1] loss: 620.061
[71,     1] loss: 679.260
[72,     1] loss: 717.381
[73,     1] loss: 608.614
[74,     1] loss: 692.071
[75,     1] loss: 736.446
[76,     1] loss: 647.809
[77,     1] loss: 670.656
[78,     1] loss: 728.032
[79,     1] loss: 610.813
[80,     1] loss: 679.642
[81,     1] loss: 612.892
[82,     1] loss: 635.884
[83,     1] loss: 570.537
[84,     1] loss: 652.627
[85,     1] loss: 527.063
[86,     1] loss: 635.283
[87,     1] loss: 576.191
[88,     1] loss: 599.385
[89,     1] loss: 554.781
[90,     1] loss: 543.086
[91,     1] loss: 599.881
[92,     1] loss: 509.013
[93,     1] loss: 569.974
[94,     1] loss: 507.231
[95,     1] loss: 505.936
[96,     1] loss: 488.294
Early stopping applied (best metric=0.3442046046257019)
Finished Training
Total time taken: 13.529869079589844
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.285
[2,     1] loss: 1253.087
[3,     1] loss: 1267.324
[4,     1] loss: 1255.291
[5,     1] loss: 1259.292
[6,     1] loss: 1251.875
[7,     1] loss: 1256.954
[8,     1] loss: 1249.470
[9,     1] loss: 1244.083
[10,     1] loss: 1240.962
[11,     1] loss: 1222.633
[12,     1] loss: 1202.308
[13,     1] loss: 1171.611
[14,     1] loss: 1124.198
[15,     1] loss: 1090.520
[16,     1] loss: 1158.795
[17,     1] loss: 1108.398
[18,     1] loss: 1097.664
[19,     1] loss: 1091.975
[20,     1] loss: 1036.794
[21,     1] loss: 996.162
[22,     1] loss: 1051.115
[23,     1] loss: 1044.740
[24,     1] loss: 1009.848
[25,     1] loss: 1048.049
[26,     1] loss: 992.146
[27,     1] loss: 989.373
[28,     1] loss: 984.253
[29,     1] loss: 978.704
[30,     1] loss: 1004.193
[31,     1] loss: 953.918
[32,     1] loss: 953.353
[33,     1] loss: 931.346
[34,     1] loss: 954.527
[35,     1] loss: 917.354
[36,     1] loss: 932.587
[37,     1] loss: 920.348
[38,     1] loss: 922.529
[39,     1] loss: 908.092
[40,     1] loss: 921.367
[41,     1] loss: 890.654
[42,     1] loss: 897.604
[43,     1] loss: 896.789
[44,     1] loss: 859.564
[45,     1] loss: 844.516
[46,     1] loss: 852.323
[47,     1] loss: 865.445
[48,     1] loss: 905.304
[49,     1] loss: 785.346
[50,     1] loss: 864.452
[51,     1] loss: 899.433
[52,     1] loss: 834.888
[53,     1] loss: 905.537
[54,     1] loss: 792.274
[55,     1] loss: 865.321
[56,     1] loss: 783.024
[57,     1] loss: 808.238
[58,     1] loss: 725.108
[59,     1] loss: 750.141
[60,     1] loss: 767.579
[61,     1] loss: 700.484
[62,     1] loss: 751.193
[63,     1] loss: 677.021
[64,     1] loss: 668.322
[65,     1] loss: 702.272
[66,     1] loss: 695.552
[67,     1] loss: 746.315
[68,     1] loss: 758.562
[69,     1] loss: 658.275
[70,     1] loss: 737.379
[71,     1] loss: 665.200
[72,     1] loss: 582.196
[73,     1] loss: 659.927
[74,     1] loss: 613.020
[75,     1] loss: 543.018
[76,     1] loss: 560.841
[77,     1] loss: 554.717
[78,     1] loss: 638.335
[79,     1] loss: 774.616
[80,     1] loss: 558.412
[81,     1] loss: 700.633
[82,     1] loss: 636.556
[83,     1] loss: 699.490
[84,     1] loss: 550.399
[85,     1] loss: 702.459
[86,     1] loss: 572.396
[87,     1] loss: 628.762
[88,     1] loss: 520.842
[89,     1] loss: 619.938
[90,     1] loss: 534.482
[91,     1] loss: 613.501
[92,     1] loss: 467.368
[93,     1] loss: 571.629
[94,     1] loss: 499.883
[95,     1] loss: 486.344
[96,     1] loss: 470.866
[97,     1] loss: 451.620
[98,     1] loss: 408.986
[99,     1] loss: 457.578
[100,     1] loss: 454.629
[101,     1] loss: 436.349
[102,     1] loss: 409.687
[103,     1] loss: 384.997
[104,     1] loss: 379.993
[105,     1] loss: 375.214
[106,     1] loss: 518.758
[107,     1] loss: 552.381
[108,     1] loss: 384.992
[109,     1] loss: 508.090
[110,     1] loss: 434.763
Early stopping applied (best metric=0.3547801375389099)
Finished Training
Total time taken: 16.727906942367554
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.374
[2,     1] loss: 1263.643
[3,     1] loss: 1255.746
[4,     1] loss: 1259.239
[5,     1] loss: 1253.534
[6,     1] loss: 1250.550
[7,     1] loss: 1248.790
[8,     1] loss: 1234.269
[9,     1] loss: 1213.208
[10,     1] loss: 1189.995
[11,     1] loss: 1128.987
[12,     1] loss: 1113.848
[13,     1] loss: 1136.057
[14,     1] loss: 1088.460
[15,     1] loss: 1042.455
[16,     1] loss: 1051.648
[17,     1] loss: 1064.634
[18,     1] loss: 1050.226
[19,     1] loss: 1003.451
[20,     1] loss: 1016.326
[21,     1] loss: 1044.813
[22,     1] loss: 985.675
[23,     1] loss: 984.307
[24,     1] loss: 956.193
[25,     1] loss: 964.603
[26,     1] loss: 949.640
[27,     1] loss: 1002.590
[28,     1] loss: 958.033
[29,     1] loss: 921.083
[30,     1] loss: 923.788
[31,     1] loss: 884.529
[32,     1] loss: 922.956
[33,     1] loss: 896.608
[34,     1] loss: 972.296
[35,     1] loss: 842.628
[36,     1] loss: 920.427
[37,     1] loss: 847.845
[38,     1] loss: 875.515
[39,     1] loss: 830.903
[40,     1] loss: 941.797
[41,     1] loss: 836.983
[42,     1] loss: 901.906
[43,     1] loss: 780.971
[44,     1] loss: 878.315
[45,     1] loss: 796.343
[46,     1] loss: 816.591
[47,     1] loss: 778.461
[48,     1] loss: 801.264
[49,     1] loss: 822.972
[50,     1] loss: 770.928
[51,     1] loss: 810.790
[52,     1] loss: 687.205
[53,     1] loss: 733.762
[54,     1] loss: 700.332
[55,     1] loss: 766.948
[56,     1] loss: 741.186
[57,     1] loss: 702.809
[58,     1] loss: 658.937
[59,     1] loss: 690.314
[60,     1] loss: 664.424
[61,     1] loss: 685.577
[62,     1] loss: 610.041
[63,     1] loss: 637.428
[64,     1] loss: 696.818
[65,     1] loss: 660.410
[66,     1] loss: 702.405
[67,     1] loss: 632.800
[68,     1] loss: 593.449
[69,     1] loss: 625.454
[70,     1] loss: 595.327
[71,     1] loss: 596.875
[72,     1] loss: 708.911
[73,     1] loss: 584.895
[74,     1] loss: 578.828
[75,     1] loss: 752.439
[76,     1] loss: 518.345
[77,     1] loss: 695.264
[78,     1] loss: 601.457
Early stopping applied (best metric=0.39166688919067383)
Finished Training
Total time taken: 12.968798875808716
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.826
[2,     1] loss: 1255.234
[3,     1] loss: 1253.608
[4,     1] loss: 1250.936
[5,     1] loss: 1247.806
[6,     1] loss: 1238.317
[7,     1] loss: 1226.041
[8,     1] loss: 1199.732
[9,     1] loss: 1155.677
[10,     1] loss: 1125.130
[11,     1] loss: 1080.648
[12,     1] loss: 1081.450
[13,     1] loss: 1020.191
[14,     1] loss: 1075.883
[15,     1] loss: 1007.704
[16,     1] loss: 980.863
[17,     1] loss: 1027.743
[18,     1] loss: 1000.415
[19,     1] loss: 982.725
[20,     1] loss: 952.563
[21,     1] loss: 997.213
[22,     1] loss: 935.433
[23,     1] loss: 978.235
[24,     1] loss: 1003.403
[25,     1] loss: 985.074
[26,     1] loss: 949.807
[27,     1] loss: 948.711
[28,     1] loss: 904.475
[29,     1] loss: 904.305
[30,     1] loss: 898.231
[31,     1] loss: 874.778
[32,     1] loss: 893.451
[33,     1] loss: 857.491
[34,     1] loss: 875.919
[35,     1] loss: 865.641
[36,     1] loss: 873.995
[37,     1] loss: 873.640
[38,     1] loss: 835.361
[39,     1] loss: 859.408
[40,     1] loss: 958.596
[41,     1] loss: 835.926
[42,     1] loss: 887.674
[43,     1] loss: 802.040
[44,     1] loss: 821.613
[45,     1] loss: 806.428
[46,     1] loss: 821.011
[47,     1] loss: 736.407
[48,     1] loss: 798.894
[49,     1] loss: 741.215
[50,     1] loss: 706.616
[51,     1] loss: 718.971
[52,     1] loss: 701.949
[53,     1] loss: 715.276
[54,     1] loss: 673.435
[55,     1] loss: 654.169
[56,     1] loss: 666.003
[57,     1] loss: 593.229
[58,     1] loss: 590.502
[59,     1] loss: 656.630
[60,     1] loss: 748.602
[61,     1] loss: 1168.276
[62,     1] loss: 747.338
[63,     1] loss: 865.187
[64,     1] loss: 737.330
[65,     1] loss: 830.942
[66,     1] loss: 846.999
[67,     1] loss: 763.795
[68,     1] loss: 692.515
[69,     1] loss: 795.170
[70,     1] loss: 757.238
[71,     1] loss: 673.383
[72,     1] loss: 744.145
[73,     1] loss: 621.240
[74,     1] loss: 659.249
[75,     1] loss: 630.795
[76,     1] loss: 637.023
[77,     1] loss: 618.195
[78,     1] loss: 605.406
[79,     1] loss: 559.984
[80,     1] loss: 544.689
[81,     1] loss: 480.502
[82,     1] loss: 538.431
[83,     1] loss: 529.677
[84,     1] loss: 489.502
[85,     1] loss: 450.308
[86,     1] loss: 479.928
[87,     1] loss: 425.177
[88,     1] loss: 481.279
[89,     1] loss: 566.614
[90,     1] loss: 583.964
[91,     1] loss: 453.713
[92,     1] loss: 504.859
[93,     1] loss: 503.150
[94,     1] loss: 507.523
[95,     1] loss: 496.986
[96,     1] loss: 439.071
[97,     1] loss: 458.941
[98,     1] loss: 413.877
Early stopping applied (best metric=0.36563462018966675)
Finished Training
Total time taken: 14.669829845428467
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1256.612
[2,     1] loss: 1261.806
[3,     1] loss: 1257.683
[4,     1] loss: 1259.378
[5,     1] loss: 1249.640
[6,     1] loss: 1244.574
[7,     1] loss: 1236.746
[8,     1] loss: 1213.060
[9,     1] loss: 1177.631
[10,     1] loss: 1129.978
[11,     1] loss: 1104.642
[12,     1] loss: 1019.412
[13,     1] loss: 1059.894
[14,     1] loss: 1078.469
[15,     1] loss: 1052.262
[16,     1] loss: 979.648
[17,     1] loss: 1009.524
[18,     1] loss: 1025.092
[19,     1] loss: 928.722
[20,     1] loss: 995.283
[21,     1] loss: 968.878
[22,     1] loss: 1007.916
[23,     1] loss: 985.657
[24,     1] loss: 935.034
[25,     1] loss: 912.054
[26,     1] loss: 893.238
[27,     1] loss: 901.320
[28,     1] loss: 895.693
[29,     1] loss: 909.745
[30,     1] loss: 890.438
[31,     1] loss: 906.585
[32,     1] loss: 906.247
[33,     1] loss: 844.564
[34,     1] loss: 870.753
[35,     1] loss: 859.342
[36,     1] loss: 876.063
[37,     1] loss: 806.986
[38,     1] loss: 826.099
[39,     1] loss: 805.412
[40,     1] loss: 756.789
[41,     1] loss: 776.288
[42,     1] loss: 839.612
[43,     1] loss: 769.953
[44,     1] loss: 783.684
[45,     1] loss: 795.864
[46,     1] loss: 730.564
[47,     1] loss: 749.091
[48,     1] loss: 730.621
[49,     1] loss: 752.225
[50,     1] loss: 737.578
[51,     1] loss: 702.666
[52,     1] loss: 720.026
[53,     1] loss: 873.555
[54,     1] loss: 639.388
[55,     1] loss: 717.507
[56,     1] loss: 680.931
[57,     1] loss: 658.737
[58,     1] loss: 632.665
[59,     1] loss: 643.460
[60,     1] loss: 644.948
[61,     1] loss: 676.668
[62,     1] loss: 575.140
[63,     1] loss: 556.917
[64,     1] loss: 620.442
[65,     1] loss: 567.780
[66,     1] loss: 579.906
[67,     1] loss: 639.653
[68,     1] loss: 534.615
[69,     1] loss: 646.604
[70,     1] loss: 570.841
[71,     1] loss: 586.766
[72,     1] loss: 561.651
[73,     1] loss: 490.492
[74,     1] loss: 512.957
[75,     1] loss: 484.852
[76,     1] loss: 504.355
[77,     1] loss: 440.755
[78,     1] loss: 532.177
[79,     1] loss: 540.769
[80,     1] loss: 548.538
[81,     1] loss: 480.824
Early stopping applied (best metric=0.4336673319339752)
Finished Training
Total time taken: 11.19381332397461
{'Hydroxylation-K Validation Accuracy': 0.7197104018912529, 'Hydroxylation-K Validation Sensitivity': 0.6814814814814815, 'Hydroxylation-K Validation Specificity': 0.7298245614035088, 'Hydroxylation-K Validation Precision': 0.3921371837083911, 'Hydroxylation-K AUC ROC': 0.7976218323586745, 'Hydroxylation-K AUC PR': 0.6358572258383132, 'Hydroxylation-K MCC': 0.34764093092186904, 'Hydroxylation-K F1': 0.49366655023742606, 'Validation Loss (Hydroxylation-K)': 0.4679320534070333, 'Hydroxylation-P Validation Accuracy': 0.7962302082804595, 'Hydroxylation-P Validation Sensitivity': 0.8104761904761905, 'Hydroxylation-P Validation Specificity': 0.7931692353733353, 'Hydroxylation-P Validation Precision': 0.4645141858220517, 'Hydroxylation-P AUC ROC': 0.8626085237992316, 'Hydroxylation-P AUC PR': 0.6181310689927326, 'Hydroxylation-P MCC': 0.500699921754891, 'Hydroxylation-P F1': 0.5876285158820921, 'Validation Loss (Hydroxylation-P)': 0.3556502898534139, 'Validation Loss (total)': 0.8235823512077332, 'TimeToTrain': 15.143393230438232}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009563137164001421,
 'learning_rate_Hydroxylation-K': 0.001347776105865998,
 'learning_rate_Hydroxylation-P': 0.006489060959087138,
 'log_base': 2.3173484760002534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 273146503,
 'sample_weights': [1.6534230431323391, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.8953534287078275,
 'weight_decay_Hydroxylation-K': 2.551463560634252,
 'weight_decay_Hydroxylation-P': 8.820839847053447}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.272
[2,     1] loss: 1340.033
[3,     1] loss: 1325.870
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032771571175180055,
 'learning_rate_Hydroxylation-K': 0.001241175113633801,
 'learning_rate_Hydroxylation-P': 0.009835887124966084,
 'log_base': 1.4172501289169501,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1731037613,
 'sample_weights': [1.9864305099117978, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.8927179390685858,
 'weight_decay_Hydroxylation-K': 5.7042557252768145,
 'weight_decay_Hydroxylation-P': 8.606181229854089}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1923.240
[2,     1] loss: 1922.511
[3,     1] loss: 1917.076
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009682237454485319,
 'learning_rate_Hydroxylation-K': 0.0070252670873973445,
 'learning_rate_Hydroxylation-P': 0.006372834307035403,
 'log_base': 1.9939877413759,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1360677067,
 'sample_weights': [4.787366641275877, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.7255232096656306,
 'weight_decay_Hydroxylation-K': 3.7924929659561277,
 'weight_decay_Hydroxylation-P': 6.452925804238787}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1416.455
[2,     1] loss: 1422.836
[3,     1] loss: 1419.391
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002614483840128411,
 'learning_rate_Hydroxylation-K': 0.0007971183982068508,
 'learning_rate_Hydroxylation-P': 0.001237843956347812,
 'log_base': 1.7071709707094636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 714209043,
 'sample_weights': [2.419004196901613, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.607958687965898,
 'weight_decay_Hydroxylation-K': 6.463276184292692,
 'weight_decay_Hydroxylation-P': 2.735686330250875}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1568.351
[2,     1] loss: 1567.973
[3,     1] loss: 1567.146
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009404273090056968,
 'learning_rate_Hydroxylation-K': 0.00187044468411829,
 'learning_rate_Hydroxylation-P': 0.003486534136152286,
 'log_base': 2.380279166398921,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 969201067,
 'sample_weights': [3.12140200237853, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5633652180830262,
 'weight_decay_Hydroxylation-K': 9.139281708209232,
 'weight_decay_Hydroxylation-P': 8.872896737576978}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.961
[2,     1] loss: 1316.994
[3,     1] loss: 1314.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004678267900575035,
 'learning_rate_Hydroxylation-K': 0.0020306990980439718,
 'learning_rate_Hydroxylation-P': 0.007716454446100119,
 'log_base': 2.959149803392674,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1954630508,
 'sample_weights': [1.925056416650943, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.964724180158125,
 'weight_decay_Hydroxylation-K': 8.51868412155982,
 'weight_decay_Hydroxylation-P': 1.6889639422187575}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.216
[2,     1] loss: 1234.281
[3,     1] loss: 1230.064
[4,     1] loss: 1230.752
[5,     1] loss: 1229.623
[6,     1] loss: 1222.204
[7,     1] loss: 1202.843
[8,     1] loss: 1174.342
[9,     1] loss: 1141.055
[10,     1] loss: 1104.602
[11,     1] loss: 1102.045
[12,     1] loss: 1143.632
[13,     1] loss: 1010.843
[14,     1] loss: 1085.572
[15,     1] loss: 1002.710
[16,     1] loss: 991.673
[17,     1] loss: 991.623
[18,     1] loss: 990.661
[19,     1] loss: 989.518
[20,     1] loss: 980.493
[21,     1] loss: 959.368
[22,     1] loss: 925.533
[23,     1] loss: 876.549
[24,     1] loss: 904.864
[25,     1] loss: 885.519
[26,     1] loss: 897.685
[27,     1] loss: 868.655
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008179929881261573,
 'learning_rate_Hydroxylation-K': 0.009716445058177429,
 'learning_rate_Hydroxylation-P': 0.0004618444071486037,
 'log_base': 1.0313928531985215,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 918523630,
 'sample_weights': [1.5387962689169257, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.531739118343933,
 'weight_decay_Hydroxylation-K': 2.6123172863855726,
 'weight_decay_Hydroxylation-P': 2.192493420784707}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17595.920
Exploding loss, terminate run (best metric=0.5338972210884094)
Finished Training
Total time taken: 0.20205187797546387
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17632.504
Exploding loss, terminate run (best metric=0.526348888874054)
Finished Training
Total time taken: 0.2030017375946045
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17612.328
Exploding loss, terminate run (best metric=0.5351144671440125)
Finished Training
Total time taken: 0.2220005989074707
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17485.816
Exploding loss, terminate run (best metric=0.5282904505729675)
Finished Training
Total time taken: 0.23600530624389648
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17582.477
Exploding loss, terminate run (best metric=0.5288436412811279)
Finished Training
Total time taken: 0.22300338745117188
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17549.004
Exploding loss, terminate run (best metric=0.5321622490882874)
Finished Training
Total time taken: 0.20051360130310059
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17514.693
Exploding loss, terminate run (best metric=0.5271427631378174)
Finished Training
Total time taken: 0.22200322151184082
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17572.564
Exploding loss, terminate run (best metric=0.5328929424285889)
Finished Training
Total time taken: 0.23200273513793945
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17508.949
Exploding loss, terminate run (best metric=0.5362783074378967)
Finished Training
Total time taken: 0.23500347137451172
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17569.469
Exploding loss, terminate run (best metric=0.5367261171340942)
Finished Training
Total time taken: 0.23900556564331055
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17636.346
Exploding loss, terminate run (best metric=0.5363062024116516)
Finished Training
Total time taken: 0.19900250434875488
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17542.654
Exploding loss, terminate run (best metric=0.5334786772727966)
Finished Training
Total time taken: 0.20200467109680176
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17547.775
Exploding loss, terminate run (best metric=0.5289173126220703)
Finished Training
Total time taken: 0.22100210189819336
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17519.631
Exploding loss, terminate run (best metric=0.5324164032936096)
Finished Training
Total time taken: 0.22200298309326172
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17563.729
Exploding loss, terminate run (best metric=0.5415562987327576)
Finished Training
Total time taken: 0.20302987098693848
{'Hydroxylation-K Validation Accuracy': 0.4828014184397163, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5960428849902534, 'Hydroxylation-K AUC PR': 0.3425500898551406, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1807881773399015, 'Validation Loss (Hydroxylation-K)': 0.560230537255605, 'Hydroxylation-P Validation Accuracy': 0.4786074480144832, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5592555619684155, 'Hydroxylation-P AUC PR': 0.2566311926602327, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16042167812008507, 'Validation Loss (Hydroxylation-P)': 0.532691462834676, 'Validation Loss (total)': 1.0929220120112102, 'TimeToTrain': 0.21744224230448406}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004564828219393549,
 'learning_rate_Hydroxylation-K': 0.009493838620071828,
 'learning_rate_Hydroxylation-P': 0.004898715667840144,
 'log_base': 1.7895923099522355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1475125982,
 'sample_weights': [54.04956862470324, 6.742157896324407],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0668479584452284,
 'weight_decay_Hydroxylation-K': 7.700326080857197,
 'weight_decay_Hydroxylation-P': 7.133988603630601}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1512.570
[2,     1] loss: 1521.711
[3,     1] loss: 1513.081
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005631446192553843,
 'learning_rate_Hydroxylation-K': 0.003351451994814946,
 'learning_rate_Hydroxylation-P': 0.006836964385626072,
 'log_base': 2.445176130861964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3688722797,
 'sample_weights': [2.8685189785482406, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.033400887492066,
 'weight_decay_Hydroxylation-K': 3.0865874177625185,
 'weight_decay_Hydroxylation-P': 8.606021862519441}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.573
[2,     1] loss: 1302.949
[3,     1] loss: 1299.734
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008332069105286293,
 'learning_rate_Hydroxylation-K': 0.0014096309653109115,
 'learning_rate_Hydroxylation-P': 0.008464524675202369,
 'log_base': 1.1188448213389863,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 143051473,
 'sample_weights': [1.8671413845826779, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.426308294319664,
 'weight_decay_Hydroxylation-K': 3.304655698459702,
 'weight_decay_Hydroxylation-P': 4.921569805208965}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4835.827
[2,     1] loss: 4826.120
[3,     1] loss: 4814.326
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008726286717365622,
 'learning_rate_Hydroxylation-K': 0.006151368750617768,
 'learning_rate_Hydroxylation-P': 0.0026796226455070254,
 'log_base': 1.2410638965687215,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2784092967,
 'sample_weights': [14.866354046455161, 1.8583658447638491],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.561742589903784,
 'weight_decay_Hydroxylation-K': 3.1930015341858753,
 'weight_decay_Hydroxylation-P': 8.611654438344422}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2542.864
[2,     1] loss: 2554.032
[3,     1] loss: 2531.532
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004391479320625414,
 'learning_rate_Hydroxylation-K': 0.0017761945835313856,
 'learning_rate_Hydroxylation-P': 7.642916149180494e-05,
 'log_base': 2.1745434738032525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3494000614,
 'sample_weights': [7.7300131156533425, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.720618210866603,
 'weight_decay_Hydroxylation-K': 0.3522805320091227,
 'weight_decay_Hydroxylation-P': 0.18126513623272444}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1362.680
[2,     1] loss: 1361.656
[3,     1] loss: 1358.585
[4,     1] loss: 1363.741
[5,     1] loss: 1363.552
[6,     1] loss: 1357.226
[7,     1] loss: 1357.353
[8,     1] loss: 1341.400
[9,     1] loss: 1332.128
[10,     1] loss: 1296.961
[11,     1] loss: 1254.275
[12,     1] loss: 1226.072
[13,     1] loss: 1244.667
[14,     1] loss: 1173.947
[15,     1] loss: 1200.832
[16,     1] loss: 1134.905
[17,     1] loss: 1140.949
[18,     1] loss: 1093.299
[19,     1] loss: 1101.706
[20,     1] loss: 1085.688
[21,     1] loss: 1082.778
[22,     1] loss: 1091.578
[23,     1] loss: 1058.272
[24,     1] loss: 1031.681
[25,     1] loss: 984.901
[26,     1] loss: 996.871
[27,     1] loss: 982.092
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009512225599526221,
 'learning_rate_Hydroxylation-K': 0.0024887295642726888,
 'learning_rate_Hydroxylation-P': 0.007891664598337948,
 'log_base': 2.6144112278310634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 777637060,
 'sample_weights': [2.149076804974031, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.086366496022072,
 'weight_decay_Hydroxylation-K': 4.746104896193472,
 'weight_decay_Hydroxylation-P': 4.560611676188533}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.601
[2,     1] loss: 1275.577
[3,     1] loss: 1273.109
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005112066747714725,
 'learning_rate_Hydroxylation-K': 0.007366148685537852,
 'learning_rate_Hydroxylation-P': 0.00018714606594053733,
 'log_base': 1.7761064001759967,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2871608689,
 'sample_weights': [1.737123348686008, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.686198161761343,
 'weight_decay_Hydroxylation-K': 2.2959159897411254,
 'weight_decay_Hydroxylation-P': 7.821265870116467}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1519.406
[2,     1] loss: 1518.919
[3,     1] loss: 1516.369
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005496269922289356,
 'learning_rate_Hydroxylation-K': 0.0023133264900247813,
 'learning_rate_Hydroxylation-P': 0.004595558602534282,
 'log_base': 2.6590403761572627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3050291016,
 'sample_weights': [2.9062929946574045, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4717145562428,
 'weight_decay_Hydroxylation-K': 0.7227918359157541,
 'weight_decay_Hydroxylation-P': 1.651995386814837}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.009
[2,     1] loss: 1269.445
[3,     1] loss: 1265.115
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00511970510989238,
 'learning_rate_Hydroxylation-K': 0.002458586288099826,
 'learning_rate_Hydroxylation-P': 0.001072229021845411,
 'log_base': 1.8985004720584024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1272181810,
 'sample_weights': [1.7070576561180297, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4934311907353734,
 'weight_decay_Hydroxylation-K': 5.708628730925646,
 'weight_decay_Hydroxylation-P': 9.719237693568687}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.453
[2,     1] loss: 1459.140
[3,     1] loss: 1449.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018029225389361331,
 'learning_rate_Hydroxylation-K': 0.004685471681830436,
 'learning_rate_Hydroxylation-P': 0.004108950530224071,
 'log_base': 2.951334907810986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3035192433,
 'sample_weights': [2.6041740570165945, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.3612090016121967,
 'weight_decay_Hydroxylation-K': 2.304450817962919,
 'weight_decay_Hydroxylation-P': 8.975449891710692}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.841
[2,     1] loss: 1233.851
[3,     1] loss: 1236.146
[4,     1] loss: 1234.517
[5,     1] loss: 1233.123
[6,     1] loss: 1233.273
[7,     1] loss: 1233.022
[8,     1] loss: 1230.842
[9,     1] loss: 1230.559
[10,     1] loss: 1226.288
[11,     1] loss: 1223.183
[12,     1] loss: 1222.978
[13,     1] loss: 1217.480
[14,     1] loss: 1204.469
[15,     1] loss: 1196.830
[16,     1] loss: 1182.212
[17,     1] loss: 1159.154
[18,     1] loss: 1136.772
[19,     1] loss: 1117.082
[20,     1] loss: 1102.460
[21,     1] loss: 1072.901
[22,     1] loss: 1073.835
[23,     1] loss: 1050.525
[24,     1] loss: 1042.575
[25,     1] loss: 1025.740
[26,     1] loss: 1036.116
[27,     1] loss: 975.077
[28,     1] loss: 979.311
[29,     1] loss: 978.191
[30,     1] loss: 997.878
[31,     1] loss: 1003.067
[32,     1] loss: 988.095
[33,     1] loss: 1022.007
[34,     1] loss: 976.489
[35,     1] loss: 956.084
[36,     1] loss: 945.787
[37,     1] loss: 954.117
[38,     1] loss: 951.423
[39,     1] loss: 932.896
[40,     1] loss: 933.797
[41,     1] loss: 945.496
[42,     1] loss: 902.366
[43,     1] loss: 917.256
[44,     1] loss: 906.763
[45,     1] loss: 868.786
[46,     1] loss: 874.510
[47,     1] loss: 867.591
[48,     1] loss: 915.612
[49,     1] loss: 910.340
[50,     1] loss: 870.710
[51,     1] loss: 869.467
[52,     1] loss: 883.701
[53,     1] loss: 854.917
[54,     1] loss: 861.883
[55,     1] loss: 836.302
[56,     1] loss: 885.764
[57,     1] loss: 826.819
[58,     1] loss: 906.684
[59,     1] loss: 806.391
[60,     1] loss: 837.504
[61,     1] loss: 826.913
[62,     1] loss: 776.867
[63,     1] loss: 806.846
[64,     1] loss: 792.515
[65,     1] loss: 755.470
[66,     1] loss: 776.696
[67,     1] loss: 793.158
[68,     1] loss: 789.044
[69,     1] loss: 767.145
[70,     1] loss: 757.765
[71,     1] loss: 718.754
[72,     1] loss: 778.612
[73,     1] loss: 684.376
[74,     1] loss: 738.300
[75,     1] loss: 715.417
[76,     1] loss: 738.362
[77,     1] loss: 728.243
[78,     1] loss: 737.720
[79,     1] loss: 654.379
[80,     1] loss: 755.747
[81,     1] loss: 677.851
[82,     1] loss: 732.537
[83,     1] loss: 710.225
[84,     1] loss: 689.516
[85,     1] loss: 710.562
[86,     1] loss: 681.260
[87,     1] loss: 711.696
[88,     1] loss: 646.368
[89,     1] loss: 585.837
[90,     1] loss: 662.539
[91,     1] loss: 614.807
[92,     1] loss: 644.042
[93,     1] loss: 669.753
[94,     1] loss: 590.637
[95,     1] loss: 595.014
[96,     1] loss: 566.783
[97,     1] loss: 555.275
[98,     1] loss: 563.740
[99,     1] loss: 580.385
[100,     1] loss: 501.244
[101,     1] loss: 539.572
[102,     1] loss: 564.865
[103,     1] loss: 515.524
[104,     1] loss: 534.178
[105,     1] loss: 535.899
[106,     1] loss: 527.669
[107,     1] loss: 548.696
[108,     1] loss: 518.761
[109,     1] loss: 505.332
[110,     1] loss: 480.772
[111,     1] loss: 498.169
[112,     1] loss: 516.504
[113,     1] loss: 509.550
[114,     1] loss: 515.548
[115,     1] loss: 463.734
[116,     1] loss: 486.715
[117,     1] loss: 498.773
[118,     1] loss: 458.323
[119,     1] loss: 468.689
[120,     1] loss: 529.355
Early stopping applied (best metric=0.3530657887458801)
Finished Training
Total time taken: 18.384531259536743
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.237
[2,     1] loss: 1233.396
[3,     1] loss: 1236.473
[4,     1] loss: 1232.990
[5,     1] loss: 1234.484
[6,     1] loss: 1232.445
[7,     1] loss: 1230.191
[8,     1] loss: 1228.311
[9,     1] loss: 1222.903
[10,     1] loss: 1219.160
[11,     1] loss: 1217.703
[12,     1] loss: 1203.531
[13,     1] loss: 1184.564
[14,     1] loss: 1167.418
[15,     1] loss: 1152.019
[16,     1] loss: 1131.946
[17,     1] loss: 1097.522
[18,     1] loss: 1077.180
[19,     1] loss: 1069.851
[20,     1] loss: 1035.706
[21,     1] loss: 1016.682
[22,     1] loss: 985.284
[23,     1] loss: 1014.536
[24,     1] loss: 1003.842
[25,     1] loss: 1004.393
[26,     1] loss: 1010.519
[27,     1] loss: 990.293
[28,     1] loss: 983.192
[29,     1] loss: 954.338
[30,     1] loss: 979.266
[31,     1] loss: 982.275
[32,     1] loss: 964.129
[33,     1] loss: 985.758
[34,     1] loss: 948.562
[35,     1] loss: 978.384
[36,     1] loss: 972.647
[37,     1] loss: 975.954
[38,     1] loss: 956.530
[39,     1] loss: 920.149
[40,     1] loss: 884.491
[41,     1] loss: 924.488
[42,     1] loss: 900.363
[43,     1] loss: 926.853
[44,     1] loss: 911.298
[45,     1] loss: 887.954
[46,     1] loss: 895.832
[47,     1] loss: 899.359
[48,     1] loss: 894.841
[49,     1] loss: 890.855
[50,     1] loss: 902.413
[51,     1] loss: 858.107
[52,     1] loss: 844.867
[53,     1] loss: 861.072
[54,     1] loss: 862.360
[55,     1] loss: 848.526
[56,     1] loss: 797.141
[57,     1] loss: 786.488
[58,     1] loss: 775.932
[59,     1] loss: 835.953
[60,     1] loss: 798.542
[61,     1] loss: 814.144
[62,     1] loss: 737.762
[63,     1] loss: 789.713
[64,     1] loss: 726.860
[65,     1] loss: 749.239
[66,     1] loss: 713.025
[67,     1] loss: 713.593
[68,     1] loss: 727.158
[69,     1] loss: 748.600
[70,     1] loss: 694.651
[71,     1] loss: 664.433
[72,     1] loss: 675.854
[73,     1] loss: 739.982
[74,     1] loss: 690.162
[75,     1] loss: 701.949
[76,     1] loss: 684.178
[77,     1] loss: 658.170
[78,     1] loss: 669.626
[79,     1] loss: 670.115
[80,     1] loss: 645.789
[81,     1] loss: 627.643
[82,     1] loss: 624.363
[83,     1] loss: 625.236
[84,     1] loss: 633.441
[85,     1] loss: 638.902
[86,     1] loss: 587.437
[87,     1] loss: 694.334
[88,     1] loss: 607.112
[89,     1] loss: 700.193
[90,     1] loss: 608.556
[91,     1] loss: 607.185
[92,     1] loss: 580.527
[93,     1] loss: 570.669
[94,     1] loss: 565.249
[95,     1] loss: 547.541
Early stopping applied (best metric=0.3901391923427582)
Finished Training
Total time taken: 13.263269901275635
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.451
[2,     1] loss: 1238.613
[3,     1] loss: 1231.649
[4,     1] loss: 1232.632
[5,     1] loss: 1230.223
[6,     1] loss: 1225.988
[7,     1] loss: 1220.559
[8,     1] loss: 1212.443
[9,     1] loss: 1204.348
[10,     1] loss: 1181.234
[11,     1] loss: 1162.978
[12,     1] loss: 1124.698
[13,     1] loss: 1111.649
[14,     1] loss: 1089.780
[15,     1] loss: 1061.285
[16,     1] loss: 1026.898
[17,     1] loss: 1033.802
[18,     1] loss: 1020.320
[19,     1] loss: 1009.991
[20,     1] loss: 984.486
[21,     1] loss: 988.842
[22,     1] loss: 960.012
[23,     1] loss: 965.199
[24,     1] loss: 976.092
[25,     1] loss: 1004.666
[26,     1] loss: 1017.074
[27,     1] loss: 967.615
[28,     1] loss: 982.193
[29,     1] loss: 997.192
[30,     1] loss: 946.454
[31,     1] loss: 988.040
[32,     1] loss: 931.311
[33,     1] loss: 966.290
[34,     1] loss: 964.725
[35,     1] loss: 936.394
[36,     1] loss: 941.305
[37,     1] loss: 945.298
[38,     1] loss: 945.122
[39,     1] loss: 921.859
[40,     1] loss: 932.513
[41,     1] loss: 855.688
[42,     1] loss: 840.408
[43,     1] loss: 881.579
[44,     1] loss: 879.333
[45,     1] loss: 861.024
[46,     1] loss: 838.226
[47,     1] loss: 884.802
[48,     1] loss: 871.750
[49,     1] loss: 853.344
[50,     1] loss: 843.856
[51,     1] loss: 839.757
[52,     1] loss: 813.087
[53,     1] loss: 814.530
[54,     1] loss: 838.238
[55,     1] loss: 796.352
[56,     1] loss: 833.923
[57,     1] loss: 795.832
[58,     1] loss: 846.669
[59,     1] loss: 798.374
[60,     1] loss: 763.002
[61,     1] loss: 758.788
[62,     1] loss: 793.294
[63,     1] loss: 775.051
[64,     1] loss: 765.822
[65,     1] loss: 763.109
[66,     1] loss: 753.029
[67,     1] loss: 733.027
[68,     1] loss: 728.592
[69,     1] loss: 735.159
[70,     1] loss: 737.152
[71,     1] loss: 665.966
[72,     1] loss: 671.652
[73,     1] loss: 708.221
[74,     1] loss: 670.395
[75,     1] loss: 689.238
[76,     1] loss: 683.231
[77,     1] loss: 687.128
[78,     1] loss: 683.751
[79,     1] loss: 656.812
[80,     1] loss: 670.868
[81,     1] loss: 647.888
[82,     1] loss: 713.362
[83,     1] loss: 621.881
[84,     1] loss: 648.460
[85,     1] loss: 630.512
[86,     1] loss: 623.413
[87,     1] loss: 611.783
[88,     1] loss: 631.693
[89,     1] loss: 648.178
[90,     1] loss: 605.431
[91,     1] loss: 556.260
[92,     1] loss: 606.046
[93,     1] loss: 593.536
[94,     1] loss: 600.797
[95,     1] loss: 569.499
[96,     1] loss: 636.332
[97,     1] loss: 635.613
[98,     1] loss: 507.031
[99,     1] loss: 621.964
[100,     1] loss: 533.868
[101,     1] loss: 578.339
[102,     1] loss: 545.280
[103,     1] loss: 539.010
[104,     1] loss: 553.831
[105,     1] loss: 543.276
[106,     1] loss: 584.401
[107,     1] loss: 520.698
[108,     1] loss: 507.523
[109,     1] loss: 494.208
[110,     1] loss: 517.744
[111,     1] loss: 515.113
[112,     1] loss: 541.745
[113,     1] loss: 477.098
[114,     1] loss: 546.646
[115,     1] loss: 452.668
[116,     1] loss: 502.594
Early stopping applied (best metric=0.3794613480567932)
Finished Training
Total time taken: 19.278571844100952
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.008
[2,     1] loss: 1232.736
[3,     1] loss: 1231.659
[4,     1] loss: 1236.887
[5,     1] loss: 1231.743
[6,     1] loss: 1230.452
[7,     1] loss: 1228.311
[8,     1] loss: 1226.867
[9,     1] loss: 1222.890
[10,     1] loss: 1219.600
[11,     1] loss: 1211.390
[12,     1] loss: 1199.561
[13,     1] loss: 1194.418
[14,     1] loss: 1181.983
[15,     1] loss: 1163.137
[16,     1] loss: 1135.280
[17,     1] loss: 1136.357
[18,     1] loss: 1099.071
[19,     1] loss: 1075.577
[20,     1] loss: 1067.822
[21,     1] loss: 1061.065
[22,     1] loss: 1062.350
[23,     1] loss: 1062.282
[24,     1] loss: 1038.640
[25,     1] loss: 1061.840
[26,     1] loss: 1028.897
[27,     1] loss: 1039.275
[28,     1] loss: 1055.207
[29,     1] loss: 1003.717
[30,     1] loss: 1011.889
[31,     1] loss: 1014.767
[32,     1] loss: 1000.579
[33,     1] loss: 998.193
[34,     1] loss: 979.746
[35,     1] loss: 953.975
[36,     1] loss: 987.771
[37,     1] loss: 997.792
[38,     1] loss: 974.154
[39,     1] loss: 995.905
[40,     1] loss: 968.300
[41,     1] loss: 893.492
[42,     1] loss: 929.646
[43,     1] loss: 975.898
[44,     1] loss: 931.131
[45,     1] loss: 979.474
[46,     1] loss: 912.050
[47,     1] loss: 891.315
[48,     1] loss: 932.825
[49,     1] loss: 926.224
[50,     1] loss: 926.430
[51,     1] loss: 864.451
[52,     1] loss: 889.717
[53,     1] loss: 879.466
[54,     1] loss: 881.161
[55,     1] loss: 882.627
[56,     1] loss: 838.165
[57,     1] loss: 852.769
[58,     1] loss: 854.771
[59,     1] loss: 849.998
[60,     1] loss: 801.006
[61,     1] loss: 837.607
[62,     1] loss: 830.742
[63,     1] loss: 859.335
[64,     1] loss: 813.500
[65,     1] loss: 808.677
[66,     1] loss: 800.790
[67,     1] loss: 762.404
[68,     1] loss: 821.483
[69,     1] loss: 709.821
[70,     1] loss: 752.960
[71,     1] loss: 767.488
[72,     1] loss: 739.503
[73,     1] loss: 726.237
[74,     1] loss: 735.825
[75,     1] loss: 676.277
[76,     1] loss: 727.881
[77,     1] loss: 718.117
[78,     1] loss: 674.837
[79,     1] loss: 732.461
[80,     1] loss: 664.460
[81,     1] loss: 754.487
[82,     1] loss: 669.089
[83,     1] loss: 662.004
[84,     1] loss: 661.711
[85,     1] loss: 690.455
[86,     1] loss: 673.194
[87,     1] loss: 663.215
[88,     1] loss: 700.011
[89,     1] loss: 628.855
[90,     1] loss: 645.385
[91,     1] loss: 676.214
[92,     1] loss: 649.645
[93,     1] loss: 577.286
[94,     1] loss: 584.285
[95,     1] loss: 655.724
[96,     1] loss: 598.429
[97,     1] loss: 598.110
[98,     1] loss: 555.627
[99,     1] loss: 587.151
[100,     1] loss: 576.415
[101,     1] loss: 550.841
[102,     1] loss: 573.403
[103,     1] loss: 546.984
[104,     1] loss: 561.545
[105,     1] loss: 488.765
[106,     1] loss: 513.370
[107,     1] loss: 516.451
[108,     1] loss: 524.786
[109,     1] loss: 519.063
[110,     1] loss: 540.196
[111,     1] loss: 532.303
[112,     1] loss: 491.804
[113,     1] loss: 512.170
[114,     1] loss: 489.701
[115,     1] loss: 522.536
[116,     1] loss: 511.192
[117,     1] loss: 516.292
[118,     1] loss: 486.742
[119,     1] loss: 504.109
[120,     1] loss: 493.608
[121,     1] loss: 492.910
[122,     1] loss: 498.539
[123,     1] loss: 502.316
[124,     1] loss: 484.341
[125,     1] loss: 510.083
[126,     1] loss: 475.305
[127,     1] loss: 526.085
[128,     1] loss: 397.430
[129,     1] loss: 504.555
[130,     1] loss: 461.746
Early stopping applied (best metric=0.30622267723083496)
Finished Training
Total time taken: 23.05558490753174
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1244.887
[2,     1] loss: 1237.451
[3,     1] loss: 1234.873
[4,     1] loss: 1234.026
[5,     1] loss: 1232.400
[6,     1] loss: 1226.676
[7,     1] loss: 1229.654
[8,     1] loss: 1224.794
[9,     1] loss: 1213.366
[10,     1] loss: 1202.826
[11,     1] loss: 1191.194
[12,     1] loss: 1164.154
[13,     1] loss: 1146.505
[14,     1] loss: 1121.179
[15,     1] loss: 1106.499
[16,     1] loss: 1083.188
[17,     1] loss: 1074.583
[18,     1] loss: 1045.878
[19,     1] loss: 1024.799
[20,     1] loss: 992.296
[21,     1] loss: 1049.898
[22,     1] loss: 989.553
[23,     1] loss: 998.547
[24,     1] loss: 925.761
[25,     1] loss: 951.732
[26,     1] loss: 988.020
[27,     1] loss: 986.178
[28,     1] loss: 986.230
[29,     1] loss: 959.578
[30,     1] loss: 953.736
[31,     1] loss: 945.117
[32,     1] loss: 989.811
[33,     1] loss: 931.992
[34,     1] loss: 963.748
[35,     1] loss: 934.618
[36,     1] loss: 943.641
[37,     1] loss: 901.288
[38,     1] loss: 902.406
[39,     1] loss: 927.888
[40,     1] loss: 898.177
[41,     1] loss: 916.238
[42,     1] loss: 837.821
[43,     1] loss: 854.309
[44,     1] loss: 894.074
[45,     1] loss: 831.202
[46,     1] loss: 882.108
[47,     1] loss: 888.003
[48,     1] loss: 840.710
[49,     1] loss: 835.202
[50,     1] loss: 852.151
[51,     1] loss: 812.579
[52,     1] loss: 798.258
[53,     1] loss: 851.393
[54,     1] loss: 817.449
[55,     1] loss: 781.570
[56,     1] loss: 775.304
[57,     1] loss: 771.082
[58,     1] loss: 774.861
[59,     1] loss: 704.535
[60,     1] loss: 726.661
[61,     1] loss: 761.024
[62,     1] loss: 757.809
[63,     1] loss: 734.195
[64,     1] loss: 736.216
[65,     1] loss: 695.194
[66,     1] loss: 709.016
[67,     1] loss: 709.889
[68,     1] loss: 718.419
[69,     1] loss: 655.554
[70,     1] loss: 691.581
[71,     1] loss: 704.860
[72,     1] loss: 700.544
[73,     1] loss: 684.273
[74,     1] loss: 690.567
[75,     1] loss: 654.464
[76,     1] loss: 678.425
[77,     1] loss: 631.801
[78,     1] loss: 664.934
[79,     1] loss: 750.151
[80,     1] loss: 622.965
[81,     1] loss: 640.666
[82,     1] loss: 625.919
[83,     1] loss: 602.066
[84,     1] loss: 660.111
[85,     1] loss: 622.766
[86,     1] loss: 557.147
[87,     1] loss: 600.957
[88,     1] loss: 637.461
[89,     1] loss: 574.860
[90,     1] loss: 644.303
[91,     1] loss: 621.392
[92,     1] loss: 603.660
[93,     1] loss: 582.368
[94,     1] loss: 572.548
[95,     1] loss: 563.567
[96,     1] loss: 593.596
[97,     1] loss: 613.734
[98,     1] loss: 555.569
[99,     1] loss: 610.735
[100,     1] loss: 524.202
[101,     1] loss: 570.246
Early stopping applied (best metric=0.4130673408508301)
Finished Training
Total time taken: 16.464747190475464
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.180
[2,     1] loss: 1232.859
[3,     1] loss: 1230.899
[4,     1] loss: 1231.682
[5,     1] loss: 1232.476
[6,     1] loss: 1228.757
[7,     1] loss: 1232.923
[8,     1] loss: 1223.212
[9,     1] loss: 1222.583
[10,     1] loss: 1208.353
[11,     1] loss: 1195.732
[12,     1] loss: 1175.070
[13,     1] loss: 1155.000
[14,     1] loss: 1133.215
[15,     1] loss: 1106.112
[16,     1] loss: 1099.164
[17,     1] loss: 1076.470
[18,     1] loss: 1093.927
[19,     1] loss: 1060.472
[20,     1] loss: 1065.710
[21,     1] loss: 1031.061
[22,     1] loss: 1038.655
[23,     1] loss: 1016.481
[24,     1] loss: 991.377
[25,     1] loss: 1011.886
[26,     1] loss: 993.952
[27,     1] loss: 993.831
[28,     1] loss: 998.849
[29,     1] loss: 960.348
[30,     1] loss: 987.925
[31,     1] loss: 975.963
[32,     1] loss: 1004.114
[33,     1] loss: 944.020
[34,     1] loss: 988.771
[35,     1] loss: 971.062
[36,     1] loss: 987.007
[37,     1] loss: 989.557
[38,     1] loss: 975.533
[39,     1] loss: 946.846
[40,     1] loss: 954.454
[41,     1] loss: 962.166
[42,     1] loss: 921.304
[43,     1] loss: 942.709
[44,     1] loss: 942.076
[45,     1] loss: 903.381
[46,     1] loss: 904.619
[47,     1] loss: 886.736
[48,     1] loss: 867.052
[49,     1] loss: 860.736
[50,     1] loss: 861.953
[51,     1] loss: 866.677
[52,     1] loss: 864.773
[53,     1] loss: 920.631
[54,     1] loss: 861.412
[55,     1] loss: 845.938
[56,     1] loss: 878.198
[57,     1] loss: 834.904
[58,     1] loss: 839.697
[59,     1] loss: 845.125
[60,     1] loss: 832.048
[61,     1] loss: 818.524
[62,     1] loss: 824.202
[63,     1] loss: 788.611
[64,     1] loss: 791.737
[65,     1] loss: 797.855
[66,     1] loss: 768.349
[67,     1] loss: 756.050
[68,     1] loss: 760.128
[69,     1] loss: 752.010
[70,     1] loss: 798.357
[71,     1] loss: 758.031
[72,     1] loss: 743.521
[73,     1] loss: 765.951
[74,     1] loss: 707.465
[75,     1] loss: 709.943
[76,     1] loss: 724.722
[77,     1] loss: 723.192
[78,     1] loss: 671.820
[79,     1] loss: 714.145
[80,     1] loss: 771.337
[81,     1] loss: 676.162
[82,     1] loss: 660.886
[83,     1] loss: 665.836
[84,     1] loss: 644.647
[85,     1] loss: 630.758
[86,     1] loss: 651.471
[87,     1] loss: 633.302
[88,     1] loss: 641.184
[89,     1] loss: 591.835
[90,     1] loss: 627.091
[91,     1] loss: 611.448
[92,     1] loss: 623.956
[93,     1] loss: 647.534
[94,     1] loss: 630.342
[95,     1] loss: 614.238
[96,     1] loss: 547.465
[97,     1] loss: 605.513
[98,     1] loss: 601.891
[99,     1] loss: 624.180
[100,     1] loss: 566.372
[101,     1] loss: 601.789
[102,     1] loss: 571.403
[103,     1] loss: 560.946
[104,     1] loss: 560.388
[105,     1] loss: 592.398
[106,     1] loss: 552.926
[107,     1] loss: 547.850
[108,     1] loss: 531.565
[109,     1] loss: 545.614
[110,     1] loss: 537.169
[111,     1] loss: 518.694
[112,     1] loss: 537.998
[113,     1] loss: 513.168
[114,     1] loss: 541.662
[115,     1] loss: 489.247
[116,     1] loss: 462.726
[117,     1] loss: 516.052
[118,     1] loss: 507.305
[119,     1] loss: 497.063
[120,     1] loss: 510.421
Early stopping applied (best metric=0.26134276390075684)
Finished Training
Total time taken: 16.735076189041138
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.509
[2,     1] loss: 1238.681
[3,     1] loss: 1231.404
[4,     1] loss: 1233.460
[5,     1] loss: 1231.964
[6,     1] loss: 1229.967
[7,     1] loss: 1227.578
[8,     1] loss: 1226.110
[9,     1] loss: 1227.217
[10,     1] loss: 1213.254
[11,     1] loss: 1206.645
[12,     1] loss: 1196.089
[13,     1] loss: 1175.867
[14,     1] loss: 1151.470
[15,     1] loss: 1139.631
[16,     1] loss: 1120.968
[17,     1] loss: 1078.232
[18,     1] loss: 1085.024
[19,     1] loss: 1035.540
[20,     1] loss: 1028.311
[21,     1] loss: 1021.742
[22,     1] loss: 997.490
[23,     1] loss: 986.540
[24,     1] loss: 981.303
[25,     1] loss: 959.442
[26,     1] loss: 946.949
[27,     1] loss: 980.272
[28,     1] loss: 907.930
[29,     1] loss: 942.517
[30,     1] loss: 928.550
[31,     1] loss: 960.977
[32,     1] loss: 920.950
[33,     1] loss: 939.087
[34,     1] loss: 949.009
[35,     1] loss: 891.475
[36,     1] loss: 877.634
[37,     1] loss: 892.360
[38,     1] loss: 905.419
[39,     1] loss: 863.697
[40,     1] loss: 874.616
[41,     1] loss: 896.028
[42,     1] loss: 792.765
[43,     1] loss: 851.512
[44,     1] loss: 868.130
[45,     1] loss: 851.072
[46,     1] loss: 841.589
[47,     1] loss: 795.675
[48,     1] loss: 783.543
[49,     1] loss: 808.773
[50,     1] loss: 834.522
[51,     1] loss: 800.473
[52,     1] loss: 765.255
[53,     1] loss: 825.304
[54,     1] loss: 800.462
[55,     1] loss: 751.823
[56,     1] loss: 781.253
[57,     1] loss: 744.720
[58,     1] loss: 795.819
[59,     1] loss: 722.495
[60,     1] loss: 733.223
[61,     1] loss: 731.519
[62,     1] loss: 752.718
[63,     1] loss: 706.222
[64,     1] loss: 744.172
[65,     1] loss: 734.776
[66,     1] loss: 720.628
[67,     1] loss: 764.428
[68,     1] loss: 672.101
[69,     1] loss: 730.687
[70,     1] loss: 635.265
[71,     1] loss: 694.713
[72,     1] loss: 699.187
[73,     1] loss: 735.333
[74,     1] loss: 641.605
[75,     1] loss: 678.428
[76,     1] loss: 656.931
[77,     1] loss: 692.847
[78,     1] loss: 702.111
[79,     1] loss: 663.881
[80,     1] loss: 667.572
[81,     1] loss: 624.979
[82,     1] loss: 658.475
[83,     1] loss: 661.989
[84,     1] loss: 602.470
[85,     1] loss: 642.559
[86,     1] loss: 629.097
[87,     1] loss: 616.394
[88,     1] loss: 615.569
[89,     1] loss: 613.993
[90,     1] loss: 600.833
[91,     1] loss: 600.699
[92,     1] loss: 588.871
[93,     1] loss: 516.500
[94,     1] loss: 526.267
[95,     1] loss: 551.034
[96,     1] loss: 541.257
[97,     1] loss: 566.930
Early stopping applied (best metric=0.42218315601348877)
Finished Training
Total time taken: 14.79603099822998
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.341
[2,     1] loss: 1235.833
[3,     1] loss: 1236.380
[4,     1] loss: 1233.656
[5,     1] loss: 1231.148
[6,     1] loss: 1232.891
[7,     1] loss: 1231.397
[8,     1] loss: 1229.465
[9,     1] loss: 1229.793
[10,     1] loss: 1228.402
[11,     1] loss: 1226.004
[12,     1] loss: 1220.757
[13,     1] loss: 1214.075
[14,     1] loss: 1206.769
[15,     1] loss: 1194.722
[16,     1] loss: 1179.729
[17,     1] loss: 1155.516
[18,     1] loss: 1128.770
[19,     1] loss: 1101.500
[20,     1] loss: 1081.187
[21,     1] loss: 1086.160
[22,     1] loss: 1064.095
[23,     1] loss: 1050.356
[24,     1] loss: 998.979
[25,     1] loss: 983.549
[26,     1] loss: 1003.495
[27,     1] loss: 1000.153
[28,     1] loss: 997.703
[29,     1] loss: 970.614
[30,     1] loss: 966.269
[31,     1] loss: 1021.346
[32,     1] loss: 947.340
[33,     1] loss: 947.079
[34,     1] loss: 979.458
[35,     1] loss: 929.886
[36,     1] loss: 923.346
[37,     1] loss: 939.627
[38,     1] loss: 894.268
[39,     1] loss: 874.909
[40,     1] loss: 909.348
[41,     1] loss: 900.197
[42,     1] loss: 863.946
[43,     1] loss: 868.875
[44,     1] loss: 857.807
[45,     1] loss: 893.494
[46,     1] loss: 837.291
[47,     1] loss: 857.365
[48,     1] loss: 840.654
[49,     1] loss: 808.741
[50,     1] loss: 833.440
[51,     1] loss: 832.456
[52,     1] loss: 801.989
[53,     1] loss: 839.560
[54,     1] loss: 821.826
[55,     1] loss: 769.953
[56,     1] loss: 810.780
[57,     1] loss: 794.702
[58,     1] loss: 742.755
[59,     1] loss: 787.114
[60,     1] loss: 780.350
[61,     1] loss: 750.629
[62,     1] loss: 712.084
[63,     1] loss: 745.219
[64,     1] loss: 728.767
[65,     1] loss: 718.746
[66,     1] loss: 752.406
[67,     1] loss: 690.504
[68,     1] loss: 717.696
[69,     1] loss: 736.922
[70,     1] loss: 702.091
[71,     1] loss: 765.562
[72,     1] loss: 712.419
[73,     1] loss: 702.370
[74,     1] loss: 663.382
[75,     1] loss: 711.900
[76,     1] loss: 657.225
[77,     1] loss: 640.507
[78,     1] loss: 616.357
[79,     1] loss: 632.236
[80,     1] loss: 604.453
[81,     1] loss: 600.795
[82,     1] loss: 640.800
[83,     1] loss: 648.164
[84,     1] loss: 592.335
[85,     1] loss: 566.103
[86,     1] loss: 568.453
Early stopping applied (best metric=0.4495766758918762)
Finished Training
Total time taken: 12.24422574043274
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.759
[2,     1] loss: 1232.316
[3,     1] loss: 1237.434
[4,     1] loss: 1233.126
[5,     1] loss: 1230.425
[6,     1] loss: 1230.359
[7,     1] loss: 1228.930
[8,     1] loss: 1228.556
[9,     1] loss: 1225.688
[10,     1] loss: 1219.670
[11,     1] loss: 1208.639
[12,     1] loss: 1199.933
[13,     1] loss: 1191.997
[14,     1] loss: 1171.837
[15,     1] loss: 1152.960
[16,     1] loss: 1147.327
[17,     1] loss: 1136.785
[18,     1] loss: 1124.883
[19,     1] loss: 1087.861
[20,     1] loss: 1077.269
[21,     1] loss: 1060.349
[22,     1] loss: 1042.118
[23,     1] loss: 1037.333
[24,     1] loss: 1060.082
[25,     1] loss: 1046.062
[26,     1] loss: 1032.652
[27,     1] loss: 1014.952
[28,     1] loss: 991.767
[29,     1] loss: 982.036
[30,     1] loss: 964.729
[31,     1] loss: 955.455
[32,     1] loss: 993.803
[33,     1] loss: 931.036
[34,     1] loss: 954.374
[35,     1] loss: 919.540
[36,     1] loss: 933.134
[37,     1] loss: 940.941
[38,     1] loss: 932.624
[39,     1] loss: 936.102
[40,     1] loss: 947.407
[41,     1] loss: 896.085
[42,     1] loss: 893.868
[43,     1] loss: 885.529
[44,     1] loss: 909.692
[45,     1] loss: 926.227
[46,     1] loss: 867.779
[47,     1] loss: 873.415
[48,     1] loss: 887.858
[49,     1] loss: 890.288
[50,     1] loss: 866.507
[51,     1] loss: 825.542
[52,     1] loss: 863.581
[53,     1] loss: 835.232
[54,     1] loss: 886.050
[55,     1] loss: 878.687
[56,     1] loss: 855.294
[57,     1] loss: 842.024
[58,     1] loss: 809.273
[59,     1] loss: 877.114
[60,     1] loss: 789.312
[61,     1] loss: 793.764
[62,     1] loss: 841.517
[63,     1] loss: 821.309
[64,     1] loss: 828.618
[65,     1] loss: 803.116
[66,     1] loss: 782.842
[67,     1] loss: 752.055
[68,     1] loss: 802.475
[69,     1] loss: 748.625
[70,     1] loss: 818.883
[71,     1] loss: 750.973
[72,     1] loss: 774.000
[73,     1] loss: 735.089
[74,     1] loss: 740.513
[75,     1] loss: 709.898
[76,     1] loss: 705.580
[77,     1] loss: 713.699
[78,     1] loss: 700.469
[79,     1] loss: 710.789
[80,     1] loss: 717.578
[81,     1] loss: 724.069
[82,     1] loss: 674.442
[83,     1] loss: 685.579
[84,     1] loss: 666.069
[85,     1] loss: 651.179
[86,     1] loss: 642.541
[87,     1] loss: 687.198
[88,     1] loss: 651.754
[89,     1] loss: 652.729
[90,     1] loss: 660.539
[91,     1] loss: 616.977
[92,     1] loss: 602.418
[93,     1] loss: 608.570
[94,     1] loss: 609.321
[95,     1] loss: 581.776
[96,     1] loss: 621.250
[97,     1] loss: 572.743
[98,     1] loss: 599.839
[99,     1] loss: 567.198
[100,     1] loss: 559.083
[101,     1] loss: 545.430
[102,     1] loss: 578.193
[103,     1] loss: 551.866
[104,     1] loss: 562.605
[105,     1] loss: 556.398
[106,     1] loss: 547.154
Early stopping applied (best metric=0.3702954053878784)
Finished Training
Total time taken: 14.989737749099731
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1230.692
[2,     1] loss: 1233.330
[3,     1] loss: 1238.818
[4,     1] loss: 1238.220
[5,     1] loss: 1233.192
[6,     1] loss: 1233.063
[7,     1] loss: 1232.328
[8,     1] loss: 1231.953
[9,     1] loss: 1229.269
[10,     1] loss: 1231.402
[11,     1] loss: 1227.228
[12,     1] loss: 1217.688
[13,     1] loss: 1211.366
[14,     1] loss: 1202.777
[15,     1] loss: 1183.481
[16,     1] loss: 1149.465
[17,     1] loss: 1138.925
[18,     1] loss: 1121.786
[19,     1] loss: 1092.891
[20,     1] loss: 1076.358
[21,     1] loss: 1074.764
[22,     1] loss: 1065.166
[23,     1] loss: 1057.490
[24,     1] loss: 1049.266
[25,     1] loss: 1042.615
[26,     1] loss: 1040.187
[27,     1] loss: 1009.310
[28,     1] loss: 994.505
[29,     1] loss: 1046.432
[30,     1] loss: 998.098
[31,     1] loss: 985.872
[32,     1] loss: 990.982
[33,     1] loss: 1005.837
[34,     1] loss: 958.208
[35,     1] loss: 952.159
[36,     1] loss: 938.607
[37,     1] loss: 967.419
[38,     1] loss: 910.166
[39,     1] loss: 941.429
[40,     1] loss: 944.122
[41,     1] loss: 959.390
[42,     1] loss: 903.394
[43,     1] loss: 933.290
[44,     1] loss: 936.912
[45,     1] loss: 900.001
[46,     1] loss: 894.650
[47,     1] loss: 858.836
[48,     1] loss: 885.816
[49,     1] loss: 868.655
[50,     1] loss: 925.048
[51,     1] loss: 904.029
[52,     1] loss: 918.091
[53,     1] loss: 856.389
[54,     1] loss: 821.783
[55,     1] loss: 847.939
[56,     1] loss: 845.055
[57,     1] loss: 828.449
[58,     1] loss: 767.298
[59,     1] loss: 803.477
[60,     1] loss: 777.028
[61,     1] loss: 847.267
[62,     1] loss: 746.581
[63,     1] loss: 752.395
[64,     1] loss: 804.404
[65,     1] loss: 740.655
[66,     1] loss: 726.322
[67,     1] loss: 734.397
[68,     1] loss: 734.571
[69,     1] loss: 753.283
[70,     1] loss: 748.123
[71,     1] loss: 665.978
[72,     1] loss: 735.867
[73,     1] loss: 714.873
[74,     1] loss: 617.387
[75,     1] loss: 721.293
[76,     1] loss: 690.423
[77,     1] loss: 790.970
[78,     1] loss: 662.546
[79,     1] loss: 702.084
[80,     1] loss: 633.129
[81,     1] loss: 693.536
[82,     1] loss: 653.655
[83,     1] loss: 665.421
[84,     1] loss: 623.317
[85,     1] loss: 646.103
[86,     1] loss: 672.978
[87,     1] loss: 631.039
[88,     1] loss: 630.661
[89,     1] loss: 555.156
[90,     1] loss: 652.151
[91,     1] loss: 637.084
[92,     1] loss: 648.020
Early stopping applied (best metric=0.32659974694252014)
Finished Training
Total time taken: 15.2942533493042
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.238
[2,     1] loss: 1234.438
[3,     1] loss: 1229.678
[4,     1] loss: 1233.178
[5,     1] loss: 1229.325
[6,     1] loss: 1225.260
[7,     1] loss: 1222.613
[8,     1] loss: 1224.170
[9,     1] loss: 1208.455
[10,     1] loss: 1194.945
[11,     1] loss: 1175.729
[12,     1] loss: 1151.447
[13,     1] loss: 1148.637
[14,     1] loss: 1115.026
[15,     1] loss: 1091.134
[16,     1] loss: 1085.071
[17,     1] loss: 1040.271
[18,     1] loss: 1058.426
[19,     1] loss: 1004.368
[20,     1] loss: 1021.830
[21,     1] loss: 1005.274
[22,     1] loss: 986.135
[23,     1] loss: 994.520
[24,     1] loss: 1020.540
[25,     1] loss: 1003.721
[26,     1] loss: 1002.755
[27,     1] loss: 982.707
[28,     1] loss: 964.968
[29,     1] loss: 980.366
[30,     1] loss: 973.401
[31,     1] loss: 955.181
[32,     1] loss: 956.961
[33,     1] loss: 982.025
[34,     1] loss: 949.629
[35,     1] loss: 934.695
[36,     1] loss: 953.849
[37,     1] loss: 931.624
[38,     1] loss: 927.313
[39,     1] loss: 910.346
[40,     1] loss: 899.041
[41,     1] loss: 931.045
[42,     1] loss: 871.705
[43,     1] loss: 882.033
[44,     1] loss: 879.160
[45,     1] loss: 845.276
[46,     1] loss: 837.448
[47,     1] loss: 848.033
[48,     1] loss: 834.138
[49,     1] loss: 851.233
[50,     1] loss: 816.721
[51,     1] loss: 843.732
[52,     1] loss: 804.895
[53,     1] loss: 806.386
[54,     1] loss: 841.100
[55,     1] loss: 811.488
[56,     1] loss: 783.509
[57,     1] loss: 743.094
[58,     1] loss: 794.903
[59,     1] loss: 767.515
[60,     1] loss: 760.783
[61,     1] loss: 715.398
[62,     1] loss: 737.364
[63,     1] loss: 718.432
[64,     1] loss: 751.661
[65,     1] loss: 726.741
[66,     1] loss: 725.141
[67,     1] loss: 699.643
[68,     1] loss: 674.589
[69,     1] loss: 713.486
[70,     1] loss: 693.584
[71,     1] loss: 682.757
[72,     1] loss: 650.172
[73,     1] loss: 686.532
[74,     1] loss: 617.120
[75,     1] loss: 694.651
[76,     1] loss: 627.261
[77,     1] loss: 616.631
[78,     1] loss: 638.314
[79,     1] loss: 651.536
[80,     1] loss: 632.275
[81,     1] loss: 698.185
[82,     1] loss: 648.335
[83,     1] loss: 676.372
[84,     1] loss: 569.725
[85,     1] loss: 674.934
[86,     1] loss: 590.066
[87,     1] loss: 650.542
[88,     1] loss: 594.261
[89,     1] loss: 575.313
[90,     1] loss: 607.312
[91,     1] loss: 610.585
[92,     1] loss: 582.964
[93,     1] loss: 585.920
[94,     1] loss: 550.080
[95,     1] loss: 504.681
[96,     1] loss: 555.155
[97,     1] loss: 592.280
Early stopping applied (best metric=0.39598020911216736)
Finished Training
Total time taken: 16.80181860923767
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.894
[2,     1] loss: 1235.757
[3,     1] loss: 1233.772
[4,     1] loss: 1228.709
[5,     1] loss: 1233.510
[6,     1] loss: 1228.044
[7,     1] loss: 1222.196
[8,     1] loss: 1220.248
[9,     1] loss: 1209.843
[10,     1] loss: 1199.005
[11,     1] loss: 1168.582
[12,     1] loss: 1148.092
[13,     1] loss: 1133.014
[14,     1] loss: 1100.756
[15,     1] loss: 1070.596
[16,     1] loss: 1077.759
[17,     1] loss: 1038.592
[18,     1] loss: 1009.448
[19,     1] loss: 1017.959
[20,     1] loss: 1007.081
[21,     1] loss: 967.876
[22,     1] loss: 1055.536
[23,     1] loss: 1021.202
[24,     1] loss: 969.174
[25,     1] loss: 1017.773
[26,     1] loss: 999.954
[27,     1] loss: 994.137
[28,     1] loss: 952.244
[29,     1] loss: 973.347
[30,     1] loss: 955.665
[31,     1] loss: 975.589
[32,     1] loss: 967.863
[33,     1] loss: 948.366
[34,     1] loss: 968.452
[35,     1] loss: 929.458
[36,     1] loss: 924.921
[37,     1] loss: 902.147
[38,     1] loss: 904.111
[39,     1] loss: 906.336
[40,     1] loss: 891.906
[41,     1] loss: 901.564
[42,     1] loss: 904.942
[43,     1] loss: 895.558
[44,     1] loss: 885.666
[45,     1] loss: 862.268
[46,     1] loss: 821.079
[47,     1] loss: 843.279
[48,     1] loss: 888.017
[49,     1] loss: 831.409
[50,     1] loss: 818.901
[51,     1] loss: 861.259
[52,     1] loss: 859.135
[53,     1] loss: 819.296
[54,     1] loss: 827.775
[55,     1] loss: 781.397
[56,     1] loss: 831.346
[57,     1] loss: 782.335
[58,     1] loss: 763.834
[59,     1] loss: 776.844
[60,     1] loss: 748.230
[61,     1] loss: 721.006
[62,     1] loss: 736.656
[63,     1] loss: 739.370
[64,     1] loss: 697.031
[65,     1] loss: 693.135
[66,     1] loss: 770.865
[67,     1] loss: 732.725
[68,     1] loss: 718.490
[69,     1] loss: 727.591
[70,     1] loss: 703.367
[71,     1] loss: 760.385
[72,     1] loss: 675.226
[73,     1] loss: 704.857
[74,     1] loss: 672.977
[75,     1] loss: 661.455
[76,     1] loss: 653.983
[77,     1] loss: 651.985
[78,     1] loss: 663.502
[79,     1] loss: 646.793
[80,     1] loss: 617.985
[81,     1] loss: 632.792
[82,     1] loss: 599.644
[83,     1] loss: 626.561
[84,     1] loss: 601.613
[85,     1] loss: 620.721
[86,     1] loss: 559.506
[87,     1] loss: 557.379
[88,     1] loss: 561.154
[89,     1] loss: 511.803
[90,     1] loss: 562.988
[91,     1] loss: 584.147
[92,     1] loss: 583.432
[93,     1] loss: 549.316
[94,     1] loss: 533.932
[95,     1] loss: 556.366
[96,     1] loss: 498.719
[97,     1] loss: 500.785
[98,     1] loss: 509.989
[99,     1] loss: 544.097
[100,     1] loss: 535.366
Early stopping applied (best metric=0.398057222366333)
Finished Training
Total time taken: 16.327570915222168
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.049
[2,     1] loss: 1237.612
[3,     1] loss: 1233.077
[4,     1] loss: 1236.235
[5,     1] loss: 1236.797
[6,     1] loss: 1235.902
[7,     1] loss: 1232.980
[8,     1] loss: 1231.580
[9,     1] loss: 1225.832
[10,     1] loss: 1229.317
[11,     1] loss: 1226.527
[12,     1] loss: 1221.694
[13,     1] loss: 1213.155
[14,     1] loss: 1202.416
[15,     1] loss: 1192.133
[16,     1] loss: 1172.500
[17,     1] loss: 1166.585
[18,     1] loss: 1157.502
[19,     1] loss: 1128.431
[20,     1] loss: 1113.934
[21,     1] loss: 1101.397
[22,     1] loss: 1070.755
[23,     1] loss: 1045.247
[24,     1] loss: 1069.179
[25,     1] loss: 1071.007
[26,     1] loss: 1001.466
[27,     1] loss: 1047.425
[28,     1] loss: 1016.757
[29,     1] loss: 995.826
[30,     1] loss: 953.222
[31,     1] loss: 990.570
[32,     1] loss: 1004.036
[33,     1] loss: 983.536
[34,     1] loss: 949.812
[35,     1] loss: 1002.163
[36,     1] loss: 1027.559
[37,     1] loss: 952.884
[38,     1] loss: 965.299
[39,     1] loss: 938.646
[40,     1] loss: 968.174
[41,     1] loss: 959.133
[42,     1] loss: 926.973
[43,     1] loss: 890.884
[44,     1] loss: 916.747
[45,     1] loss: 935.500
[46,     1] loss: 907.797
[47,     1] loss: 889.331
[48,     1] loss: 915.243
[49,     1] loss: 919.094
[50,     1] loss: 888.984
[51,     1] loss: 863.689
[52,     1] loss: 839.074
[53,     1] loss: 846.253
[54,     1] loss: 819.027
[55,     1] loss: 837.219
[56,     1] loss: 848.826
[57,     1] loss: 850.176
[58,     1] loss: 856.953
[59,     1] loss: 842.117
[60,     1] loss: 840.653
[61,     1] loss: 814.258
[62,     1] loss: 818.155
[63,     1] loss: 830.903
[64,     1] loss: 807.642
[65,     1] loss: 750.020
[66,     1] loss: 749.407
[67,     1] loss: 748.271
[68,     1] loss: 780.790
[69,     1] loss: 735.969
[70,     1] loss: 711.589
[71,     1] loss: 730.073
[72,     1] loss: 701.608
[73,     1] loss: 774.436
[74,     1] loss: 775.231
[75,     1] loss: 719.839
[76,     1] loss: 715.663
[77,     1] loss: 666.396
[78,     1] loss: 745.877
[79,     1] loss: 711.823
[80,     1] loss: 745.134
[81,     1] loss: 629.491
[82,     1] loss: 693.271
[83,     1] loss: 647.807
[84,     1] loss: 704.929
[85,     1] loss: 684.148
[86,     1] loss: 669.016
[87,     1] loss: 597.509
[88,     1] loss: 687.285
[89,     1] loss: 617.817
[90,     1] loss: 643.185
[91,     1] loss: 631.882
[92,     1] loss: 620.400
[93,     1] loss: 568.946
[94,     1] loss: 588.812
Early stopping applied (best metric=0.33028677105903625)
Finished Training
Total time taken: 14.713820457458496
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.749
[2,     1] loss: 1233.051
[3,     1] loss: 1235.223
[4,     1] loss: 1230.567
[5,     1] loss: 1228.664
[6,     1] loss: 1231.173
[7,     1] loss: 1227.502
[8,     1] loss: 1220.464
[9,     1] loss: 1216.455
[10,     1] loss: 1200.163
[11,     1] loss: 1188.646
[12,     1] loss: 1171.300
[13,     1] loss: 1149.707
[14,     1] loss: 1129.342
[15,     1] loss: 1114.066
[16,     1] loss: 1089.955
[17,     1] loss: 1075.552
[18,     1] loss: 1034.176
[19,     1] loss: 1057.062
[20,     1] loss: 1016.041
[21,     1] loss: 1032.046
[22,     1] loss: 978.619
[23,     1] loss: 1029.467
[24,     1] loss: 994.308
[25,     1] loss: 1009.260
[26,     1] loss: 986.948
[27,     1] loss: 989.555
[28,     1] loss: 957.059
[29,     1] loss: 985.257
[30,     1] loss: 964.545
[31,     1] loss: 988.045
[32,     1] loss: 975.024
[33,     1] loss: 993.376
[34,     1] loss: 942.792
[35,     1] loss: 974.294
[36,     1] loss: 932.187
[37,     1] loss: 911.713
[38,     1] loss: 919.068
[39,     1] loss: 913.610
[40,     1] loss: 914.544
[41,     1] loss: 921.794
[42,     1] loss: 855.624
[43,     1] loss: 918.887
[44,     1] loss: 890.692
[45,     1] loss: 878.975
[46,     1] loss: 886.246
[47,     1] loss: 893.647
[48,     1] loss: 865.740
[49,     1] loss: 829.436
[50,     1] loss: 852.903
[51,     1] loss: 839.789
[52,     1] loss: 878.465
[53,     1] loss: 874.291
[54,     1] loss: 866.028
[55,     1] loss: 868.990
[56,     1] loss: 816.906
[57,     1] loss: 857.645
[58,     1] loss: 795.772
[59,     1] loss: 791.759
[60,     1] loss: 874.257
[61,     1] loss: 799.578
[62,     1] loss: 786.106
[63,     1] loss: 786.615
[64,     1] loss: 812.819
[65,     1] loss: 747.102
[66,     1] loss: 729.665
[67,     1] loss: 762.838
[68,     1] loss: 763.047
[69,     1] loss: 707.541
[70,     1] loss: 740.840
[71,     1] loss: 723.723
[72,     1] loss: 682.462
[73,     1] loss: 750.131
[74,     1] loss: 716.358
[75,     1] loss: 670.052
[76,     1] loss: 677.594
[77,     1] loss: 662.222
[78,     1] loss: 708.854
[79,     1] loss: 663.868
[80,     1] loss: 654.341
[81,     1] loss: 690.778
[82,     1] loss: 670.550
[83,     1] loss: 618.523
[84,     1] loss: 655.892
[85,     1] loss: 651.901
[86,     1] loss: 596.874
[87,     1] loss: 606.461
[88,     1] loss: 625.820
[89,     1] loss: 577.547
[90,     1] loss: 606.340
[91,     1] loss: 583.667
[92,     1] loss: 590.579
[93,     1] loss: 612.961
[94,     1] loss: 534.007
[95,     1] loss: 576.400
[96,     1] loss: 574.289
[97,     1] loss: 527.380
[98,     1] loss: 536.603
[99,     1] loss: 523.892
[100,     1] loss: 537.415
Early stopping applied (best metric=0.37705984711647034)
Finished Training
Total time taken: 14.669610500335693
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.588
[2,     1] loss: 1235.199
[3,     1] loss: 1232.638
[4,     1] loss: 1231.489
[5,     1] loss: 1232.718
[6,     1] loss: 1229.799
[7,     1] loss: 1232.117
[8,     1] loss: 1229.209
[9,     1] loss: 1222.684
[10,     1] loss: 1217.408
[11,     1] loss: 1207.527
[12,     1] loss: 1187.353
[13,     1] loss: 1171.560
[14,     1] loss: 1148.524
[15,     1] loss: 1137.135
[16,     1] loss: 1095.302
[17,     1] loss: 1084.724
[18,     1] loss: 1058.404
[19,     1] loss: 1066.963
[20,     1] loss: 1021.377
[21,     1] loss: 1043.815
[22,     1] loss: 985.643
[23,     1] loss: 1027.571
[24,     1] loss: 1044.469
[25,     1] loss: 1010.132
[26,     1] loss: 1026.958
[27,     1] loss: 1037.498
[28,     1] loss: 989.336
[29,     1] loss: 953.115
[30,     1] loss: 970.596
[31,     1] loss: 952.186
[32,     1] loss: 972.536
[33,     1] loss: 952.812
[34,     1] loss: 953.862
[35,     1] loss: 917.566
[36,     1] loss: 951.942
[37,     1] loss: 938.671
[38,     1] loss: 914.734
[39,     1] loss: 962.579
[40,     1] loss: 902.470
[41,     1] loss: 902.675
[42,     1] loss: 857.322
[43,     1] loss: 874.425
[44,     1] loss: 888.861
[45,     1] loss: 897.115
[46,     1] loss: 907.118
[47,     1] loss: 898.913
[48,     1] loss: 900.721
[49,     1] loss: 818.132
[50,     1] loss: 855.461
[51,     1] loss: 803.972
[52,     1] loss: 815.941
[53,     1] loss: 800.428
[54,     1] loss: 756.286
[55,     1] loss: 825.984
[56,     1] loss: 831.312
[57,     1] loss: 808.382
[58,     1] loss: 745.058
[59,     1] loss: 771.918
[60,     1] loss: 762.485
[61,     1] loss: 801.470
[62,     1] loss: 743.598
[63,     1] loss: 735.105
[64,     1] loss: 703.982
[65,     1] loss: 737.146
[66,     1] loss: 700.083
[67,     1] loss: 701.025
[68,     1] loss: 727.210
[69,     1] loss: 723.418
[70,     1] loss: 723.606
[71,     1] loss: 687.017
[72,     1] loss: 761.702
[73,     1] loss: 696.628
[74,     1] loss: 707.552
[75,     1] loss: 670.982
[76,     1] loss: 656.881
[77,     1] loss: 670.236
[78,     1] loss: 658.631
[79,     1] loss: 664.785
[80,     1] loss: 639.063
[81,     1] loss: 625.554
[82,     1] loss: 633.296
[83,     1] loss: 697.971
[84,     1] loss: 576.277
[85,     1] loss: 590.497
[86,     1] loss: 669.244
[87,     1] loss: 621.912
[88,     1] loss: 608.957
[89,     1] loss: 675.618
[90,     1] loss: 567.500
[91,     1] loss: 622.354
[92,     1] loss: 578.498
[93,     1] loss: 579.761
[94,     1] loss: 570.678
[95,     1] loss: 603.712
[96,     1] loss: 552.209
[97,     1] loss: 525.054
[98,     1] loss: 592.371
[99,     1] loss: 521.023
[100,     1] loss: 484.658
[101,     1] loss: 499.175
[102,     1] loss: 496.676
[103,     1] loss: 482.210
[104,     1] loss: 487.074
[105,     1] loss: 515.184
[106,     1] loss: 492.988
[107,     1] loss: 527.930
[108,     1] loss: 524.941
Early stopping applied (best metric=0.33703890442848206)
Finished Training
Total time taken: 17.968628644943237
{'Hydroxylation-K Validation Accuracy': 0.7702718676122932, 'Hydroxylation-K Validation Sensitivity': 0.6325925925925926, 'Hydroxylation-K Validation Specificity': 0.8052631578947368, 'Hydroxylation-K Validation Precision': 0.4627238502238502, 'Hydroxylation-K AUC ROC': 0.7776413255360625, 'Hydroxylation-K AUC PR': 0.5620095242184417, 'Hydroxylation-K MCC': 0.3958281771809938, 'Hydroxylation-K F1': 0.5305813056339372, 'Validation Loss (Hydroxylation-K)': 0.5000117202599843, 'Hydroxylation-P Validation Accuracy': 0.8009712028154239, 'Hydroxylation-P Validation Sensitivity': 0.7668253968253969, 'Hydroxylation-P Validation Specificity': 0.8083021597087137, 'Hydroxylation-P Validation Precision': 0.46997000367383085, 'Hydroxylation-P AUC ROC': 0.844355513515696, 'Hydroxylation-P AUC PR': 0.5992825472414796, 'Hydroxylation-P MCC': 0.4860473249440905, 'Hydroxylation-P F1': 0.5808024047184881, 'Validation Loss (Hydroxylation-P)': 0.36735846996307375, 'Validation Loss (total)': 0.8673701882362366, 'TimeToTrain': 16.33249855041504}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004637148407022604,
 'learning_rate_Hydroxylation-K': 0.0023141506149901105,
 'learning_rate_Hydroxylation-P': 0.00958933662991093,
 'log_base': 2.8782287352362026,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3471179464,
 'sample_weights': [1.5437004738100095, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.499546637777245,
 'weight_decay_Hydroxylation-K': 6.2363556063569,
 'weight_decay_Hydroxylation-P': 0.004078433706186102}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.172
[2,     1] loss: 1238.273
[3,     1] loss: 1238.765
[4,     1] loss: 1238.482
[5,     1] loss: 1226.775
[6,     1] loss: 1221.117
[7,     1] loss: 1193.422
[8,     1] loss: 1140.232
[9,     1] loss: 1109.549
[10,     1] loss: 1110.961
[11,     1] loss: 1049.120
[12,     1] loss: 1055.334
[13,     1] loss: 1047.972
[14,     1] loss: 1017.514
[15,     1] loss: 1055.154
[16,     1] loss: 1014.107
[17,     1] loss: 993.963
[18,     1] loss: 1091.618
[19,     1] loss: 1005.443
[20,     1] loss: 957.924
[21,     1] loss: 971.444
[22,     1] loss: 952.410
[23,     1] loss: 967.329
[24,     1] loss: 964.949
[25,     1] loss: 937.748
[26,     1] loss: 913.458
[27,     1] loss: 909.027
[28,     1] loss: 904.812
[29,     1] loss: 848.635
[30,     1] loss: 875.475
[31,     1] loss: 856.532
[32,     1] loss: 940.901
[33,     1] loss: 997.942
[34,     1] loss: 850.680
[35,     1] loss: 894.662
[36,     1] loss: 832.623
[37,     1] loss: 863.444
[38,     1] loss: 766.471
[39,     1] loss: 940.920
[40,     1] loss: 933.589
[41,     1] loss: 790.431
[42,     1] loss: 890.162
[43,     1] loss: 819.130
[44,     1] loss: 829.504
[45,     1] loss: 761.848
[46,     1] loss: 733.820
[47,     1] loss: 745.730
[48,     1] loss: 831.493
[49,     1] loss: 725.984
[50,     1] loss: 813.422
[51,     1] loss: 735.514
[52,     1] loss: 754.658
[53,     1] loss: 709.141
[54,     1] loss: 736.373
[55,     1] loss: 800.908
[56,     1] loss: 648.746
[57,     1] loss: 829.463
[58,     1] loss: 852.431
[59,     1] loss: 736.969
[60,     1] loss: 751.466
[61,     1] loss: 805.710
[62,     1] loss: 709.668
[63,     1] loss: 778.240
[64,     1] loss: 651.302
[65,     1] loss: 779.724
[66,     1] loss: 622.220
[67,     1] loss: 654.780
[68,     1] loss: 717.950
[69,     1] loss: 595.473
[70,     1] loss: 626.521
[71,     1] loss: 521.615
[72,     1] loss: 608.219
[73,     1] loss: 782.124
[74,     1] loss: 664.803
[75,     1] loss: 569.715
[76,     1] loss: 609.392
[77,     1] loss: 544.805
[78,     1] loss: 710.857
[79,     1] loss: 1071.983
[80,     1] loss: 570.552
[81,     1] loss: 918.688
[82,     1] loss: 752.938
[83,     1] loss: 760.347
[84,     1] loss: 776.547
[85,     1] loss: 586.416
[86,     1] loss: 766.027
[87,     1] loss: 674.711
[88,     1] loss: 564.787
[89,     1] loss: 649.602
[90,     1] loss: 557.992
[91,     1] loss: 614.297
[92,     1] loss: 518.745
[93,     1] loss: 600.050
[94,     1] loss: 730.170
[95,     1] loss: 508.587
[96,     1] loss: 561.463
[97,     1] loss: 799.981
[98,     1] loss: 479.257
[99,     1] loss: 841.546
[100,     1] loss: 1472.098
[101,     1] loss: 912.418
[102,     1] loss: 919.431
[103,     1] loss: 962.243
[104,     1] loss: 883.672
[105,     1] loss: 838.285
[106,     1] loss: 780.875
[107,     1] loss: 899.902
[108,     1] loss: 1151.333
[109,     1] loss: 786.247
[110,     1] loss: 961.780
[111,     1] loss: 902.490
[112,     1] loss: 902.532
[113,     1] loss: 909.724
[114,     1] loss: 873.936
[115,     1] loss: 787.810
[116,     1] loss: 745.152
[117,     1] loss: 729.349
[118,     1] loss: 754.914
[119,     1] loss: 718.573
[120,     1] loss: 726.245
[121,     1] loss: 716.005
[122,     1] loss: 751.196
[123,     1] loss: 698.122
[124,     1] loss: 714.245
[125,     1] loss: 685.855
[126,     1] loss: 663.965
[127,     1] loss: 686.203
[128,     1] loss: 636.992
[129,     1] loss: 547.065
[130,     1] loss: 593.694
[131,     1] loss: 650.839
[132,     1] loss: 672.293
[133,     1] loss: 608.449
[134,     1] loss: 591.425
[135,     1] loss: 969.042
[136,     1] loss: 1398.670
[137,     1] loss: 1143.326
[138,     1] loss: 1115.119
[139,     1] loss: 1162.651
[140,     1] loss: 1194.317
[141,     1] loss: 1180.162
[142,     1] loss: 1129.533
[143,     1] loss: 1078.198
[144,     1] loss: 1062.662
[145,     1] loss: 1014.697
[146,     1] loss: 1001.959
[147,     1] loss: 994.580
[148,     1] loss: 1032.608
[149,     1] loss: 991.674
[150,     1] loss: 995.605
[151,     1] loss: 940.982
[152,     1] loss: 984.109
[153,     1] loss: 905.267
[154,     1] loss: 963.317
[155,     1] loss: 906.086
[156,     1] loss: 908.562
[157,     1] loss: 940.232
[158,     1] loss: 858.575
[159,     1] loss: 906.310
[160,     1] loss: 864.252
[161,     1] loss: 872.755
[162,     1] loss: 862.997
[163,     1] loss: 830.563
[164,     1] loss: 849.914
[165,     1] loss: 816.539
[166,     1] loss: 787.380
[167,     1] loss: 767.496
[168,     1] loss: 750.575
[169,     1] loss: 740.313
[170,     1] loss: 757.996
[171,     1] loss: 813.836
[172,     1] loss: 945.499
[173,     1] loss: 737.127
[174,     1] loss: 805.725
[175,     1] loss: 813.144
[176,     1] loss: 714.987
[177,     1] loss: 833.367
[178,     1] loss: 852.280
Early stopping applied (best metric=0.31159794330596924)
Finished Training
Total time taken: 24.39542269706726
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.891
[2,     1] loss: 1243.532
[3,     1] loss: 1241.538
[4,     1] loss: 1242.697
[5,     1] loss: 1240.159
[6,     1] loss: 1244.283
[7,     1] loss: 1241.928
[8,     1] loss: 1241.143
[9,     1] loss: 1239.369
[10,     1] loss: 1236.249
[11,     1] loss: 1231.815
[12,     1] loss: 1219.975
[13,     1] loss: 1208.189
[14,     1] loss: 1167.999
[15,     1] loss: 1122.213
[16,     1] loss: 1101.790
[17,     1] loss: 1076.993
[18,     1] loss: 1057.212
[19,     1] loss: 1013.240
[20,     1] loss: 1030.142
[21,     1] loss: 1023.849
[22,     1] loss: 1020.637
[23,     1] loss: 1000.224
[24,     1] loss: 961.484
[25,     1] loss: 1017.573
[26,     1] loss: 984.985
[27,     1] loss: 998.661
[28,     1] loss: 969.322
[29,     1] loss: 979.997
[30,     1] loss: 956.746
[31,     1] loss: 957.091
[32,     1] loss: 905.648
[33,     1] loss: 913.627
[34,     1] loss: 873.543
[35,     1] loss: 867.160
[36,     1] loss: 884.852
[37,     1] loss: 864.946
[38,     1] loss: 869.210
[39,     1] loss: 840.777
[40,     1] loss: 851.743
[41,     1] loss: 844.577
[42,     1] loss: 849.025
[43,     1] loss: 834.113
[44,     1] loss: 800.312
[45,     1] loss: 890.261
[46,     1] loss: 868.449
[47,     1] loss: 803.116
[48,     1] loss: 859.422
[49,     1] loss: 779.645
[50,     1] loss: 842.981
[51,     1] loss: 771.015
[52,     1] loss: 703.288
[53,     1] loss: 710.332
[54,     1] loss: 746.518
[55,     1] loss: 808.507
[56,     1] loss: 939.661
[57,     1] loss: 733.886
[58,     1] loss: 847.381
[59,     1] loss: 777.793
[60,     1] loss: 796.632
[61,     1] loss: 733.189
[62,     1] loss: 801.779
[63,     1] loss: 655.960
[64,     1] loss: 718.380
[65,     1] loss: 635.839
[66,     1] loss: 646.809
[67,     1] loss: 587.065
[68,     1] loss: 656.938
[69,     1] loss: 645.311
[70,     1] loss: 766.889
[71,     1] loss: 1433.900
[72,     1] loss: 789.730
[73,     1] loss: 970.515
[74,     1] loss: 1025.018
[75,     1] loss: 945.678
[76,     1] loss: 923.461
[77,     1] loss: 903.043
[78,     1] loss: 868.587
[79,     1] loss: 890.872
[80,     1] loss: 850.371
[81,     1] loss: 823.138
[82,     1] loss: 786.049
[83,     1] loss: 759.745
[84,     1] loss: 759.847
[85,     1] loss: 762.156
[86,     1] loss: 775.789
[87,     1] loss: 670.683
[88,     1] loss: 748.812
[89,     1] loss: 876.941
[90,     1] loss: 695.079
[91,     1] loss: 829.389
[92,     1] loss: 754.744
[93,     1] loss: 912.592
[94,     1] loss: 708.007
[95,     1] loss: 840.208
[96,     1] loss: 589.452
[97,     1] loss: 849.477
[98,     1] loss: 644.844
[99,     1] loss: 681.816
[100,     1] loss: 627.810
[101,     1] loss: 704.415
[102,     1] loss: 584.043
[103,     1] loss: 622.939
[104,     1] loss: 693.863
[105,     1] loss: 515.743
[106,     1] loss: 671.597
[107,     1] loss: 1094.914
[108,     1] loss: 765.579
[109,     1] loss: 861.618
[110,     1] loss: 782.242
[111,     1] loss: 705.718
[112,     1] loss: 721.439
[113,     1] loss: 666.306
[114,     1] loss: 661.385
[115,     1] loss: 695.645
[116,     1] loss: 593.625
[117,     1] loss: 689.200
[118,     1] loss: 686.771
Early stopping applied (best metric=0.34542116522789)
Finished Training
Total time taken: 19.91543698310852
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.368
[2,     1] loss: 1239.702
[3,     1] loss: 1245.772
[4,     1] loss: 1234.616
[5,     1] loss: 1228.112
[6,     1] loss: 1219.598
[7,     1] loss: 1186.619
[8,     1] loss: 1145.692
[9,     1] loss: 1108.831
[10,     1] loss: 1052.194
[11,     1] loss: 1070.673
[12,     1] loss: 1074.014
[13,     1] loss: 1027.918
[14,     1] loss: 1052.107
[15,     1] loss: 1027.251
[16,     1] loss: 1000.069
[17,     1] loss: 969.805
[18,     1] loss: 979.536
[19,     1] loss: 953.036
[20,     1] loss: 982.131
[21,     1] loss: 910.051
[22,     1] loss: 945.218
[23,     1] loss: 910.166
[24,     1] loss: 880.974
[25,     1] loss: 838.390
[26,     1] loss: 836.383
[27,     1] loss: 867.156
[28,     1] loss: 984.309
[29,     1] loss: 1365.556
[30,     1] loss: 892.874
[31,     1] loss: 1019.685
[32,     1] loss: 986.555
[33,     1] loss: 967.575
[34,     1] loss: 989.257
[35,     1] loss: 994.922
[36,     1] loss: 948.111
[37,     1] loss: 949.096
[38,     1] loss: 892.796
[39,     1] loss: 979.196
[40,     1] loss: 877.988
[41,     1] loss: 880.612
[42,     1] loss: 851.924
[43,     1] loss: 880.940
[44,     1] loss: 815.458
[45,     1] loss: 857.113
[46,     1] loss: 850.790
[47,     1] loss: 816.924
[48,     1] loss: 790.573
[49,     1] loss: 760.580
[50,     1] loss: 837.446
[51,     1] loss: 831.279
[52,     1] loss: 730.213
[53,     1] loss: 757.962
[54,     1] loss: 769.817
[55,     1] loss: 711.223
[56,     1] loss: 735.587
[57,     1] loss: 747.202
[58,     1] loss: 799.927
[59,     1] loss: 877.259
[60,     1] loss: 804.612
[61,     1] loss: 714.214
[62,     1] loss: 744.698
[63,     1] loss: 700.916
[64,     1] loss: 679.362
[65,     1] loss: 603.957
[66,     1] loss: 583.695
[67,     1] loss: 558.103
[68,     1] loss: 632.786
[69,     1] loss: 1099.282
[70,     1] loss: 1832.882
[71,     1] loss: 810.426
[72,     1] loss: 1037.136
[73,     1] loss: 1061.015
[74,     1] loss: 1074.792
[75,     1] loss: 1071.025
[76,     1] loss: 1080.373
[77,     1] loss: 1064.974
[78,     1] loss: 1051.593
[79,     1] loss: 984.259
[80,     1] loss: 1046.987
[81,     1] loss: 1008.224
[82,     1] loss: 1002.524
[83,     1] loss: 978.760
[84,     1] loss: 954.496
[85,     1] loss: 888.885
[86,     1] loss: 910.454
[87,     1] loss: 916.493
[88,     1] loss: 888.994
[89,     1] loss: 893.677
[90,     1] loss: 894.012
[91,     1] loss: 850.639
[92,     1] loss: 857.070
[93,     1] loss: 877.896
[94,     1] loss: 897.658
[95,     1] loss: 828.834
[96,     1] loss: 837.779
[97,     1] loss: 834.117
[98,     1] loss: 824.201
[99,     1] loss: 861.469
[100,     1] loss: 865.182
[101,     1] loss: 776.134
[102,     1] loss: 796.020
[103,     1] loss: 760.452
[104,     1] loss: 700.401
[105,     1] loss: 757.074
[106,     1] loss: 758.408
[107,     1] loss: 733.694
[108,     1] loss: 706.985
[109,     1] loss: 758.133
[110,     1] loss: 769.087
[111,     1] loss: 1136.345
[112,     1] loss: 895.761
[113,     1] loss: 865.485
[114,     1] loss: 792.759
[115,     1] loss: 822.576
Early stopping applied (best metric=0.325281023979187)
Finished Training
Total time taken: 15.747787475585938
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.069
[2,     1] loss: 1237.099
[3,     1] loss: 1234.567
[4,     1] loss: 1236.166
[5,     1] loss: 1226.583
[6,     1] loss: 1189.200
[7,     1] loss: 1154.193
[8,     1] loss: 1143.319
[9,     1] loss: 1108.997
[10,     1] loss: 1081.972
[11,     1] loss: 1088.298
[12,     1] loss: 1057.014
[13,     1] loss: 1042.788
[14,     1] loss: 1043.395
[15,     1] loss: 1027.152
[16,     1] loss: 998.400
[17,     1] loss: 1037.835
[18,     1] loss: 996.136
[19,     1] loss: 1013.248
[20,     1] loss: 957.404
[21,     1] loss: 929.482
[22,     1] loss: 940.120
[23,     1] loss: 976.651
[24,     1] loss: 906.090
[25,     1] loss: 917.854
[26,     1] loss: 936.439
[27,     1] loss: 856.475
[28,     1] loss: 924.763
[29,     1] loss: 864.395
[30,     1] loss: 888.215
[31,     1] loss: 942.844
[32,     1] loss: 930.715
[33,     1] loss: 862.103
[34,     1] loss: 846.846
[35,     1] loss: 830.652
[36,     1] loss: 806.494
[37,     1] loss: 842.485
[38,     1] loss: 893.894
[39,     1] loss: 828.264
[40,     1] loss: 789.509
[41,     1] loss: 788.628
[42,     1] loss: 771.139
[43,     1] loss: 781.875
[44,     1] loss: 712.151
[45,     1] loss: 734.984
[46,     1] loss: 760.708
[47,     1] loss: 1148.248
[48,     1] loss: 872.673
[49,     1] loss: 858.233
[50,     1] loss: 844.037
[51,     1] loss: 864.019
[52,     1] loss: 873.747
[53,     1] loss: 808.152
[54,     1] loss: 835.009
[55,     1] loss: 809.178
[56,     1] loss: 763.642
[57,     1] loss: 767.331
[58,     1] loss: 742.152
[59,     1] loss: 793.999
[60,     1] loss: 691.304
[61,     1] loss: 713.259
[62,     1] loss: 671.761
[63,     1] loss: 631.819
[64,     1] loss: 698.068
[65,     1] loss: 690.073
[66,     1] loss: 646.052
[67,     1] loss: 623.341
[68,     1] loss: 685.865
[69,     1] loss: 707.122
[70,     1] loss: 564.442
[71,     1] loss: 592.049
[72,     1] loss: 649.131
[73,     1] loss: 609.924
[74,     1] loss: 739.487
[75,     1] loss: 611.893
[76,     1] loss: 532.532
[77,     1] loss: 535.007
[78,     1] loss: 526.487
[79,     1] loss: 668.934
[80,     1] loss: 988.120
[81,     1] loss: 1650.195
[82,     1] loss: 1115.822
[83,     1] loss: 1136.541
[84,     1] loss: 1202.865
[85,     1] loss: 1227.465
[86,     1] loss: 1237.494
[87,     1] loss: 1240.695
[88,     1] loss: 1237.746
[89,     1] loss: 1226.991
[90,     1] loss: 1209.430
[91,     1] loss: 1228.655
[92,     1] loss: 1224.105
[93,     1] loss: 1208.192
[94,     1] loss: 1194.820
[95,     1] loss: 1173.594
[96,     1] loss: 1156.519
[97,     1] loss: 1136.302
[98,     1] loss: 1137.541
[99,     1] loss: 1138.493
[100,     1] loss: 1100.698
[101,     1] loss: 1169.131
[102,     1] loss: 1136.624
[103,     1] loss: 1167.003
[104,     1] loss: 1122.387
[105,     1] loss: 1059.368
[106,     1] loss: 1149.287
[107,     1] loss: 1067.816
[108,     1] loss: 1097.266
[109,     1] loss: 1085.871
[110,     1] loss: 1045.469
[111,     1] loss: 1080.831
[112,     1] loss: 1036.278
[113,     1] loss: 1053.242
[114,     1] loss: 1043.659
[115,     1] loss: 1039.364
Early stopping applied (best metric=0.34528329968452454)
Finished Training
Total time taken: 15.996338367462158
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1247.508
[2,     1] loss: 1247.480
[3,     1] loss: 1244.562
[4,     1] loss: 1241.035
[5,     1] loss: 1235.059
[6,     1] loss: 1230.160
[7,     1] loss: 1212.204
[8,     1] loss: 1190.821
[9,     1] loss: 1150.106
[10,     1] loss: 1099.443
[11,     1] loss: 1116.199
[12,     1] loss: 1119.040
[13,     1] loss: 1098.986
[14,     1] loss: 1058.119
[15,     1] loss: 1024.671
[16,     1] loss: 1080.345
[17,     1] loss: 1052.819
[18,     1] loss: 1046.570
[19,     1] loss: 1011.856
[20,     1] loss: 1020.129
[21,     1] loss: 998.576
[22,     1] loss: 967.723
[23,     1] loss: 1019.851
[24,     1] loss: 974.310
[25,     1] loss: 988.444
[26,     1] loss: 953.408
[27,     1] loss: 931.981
[28,     1] loss: 900.811
[29,     1] loss: 962.615
[30,     1] loss: 897.425
[31,     1] loss: 895.878
[32,     1] loss: 872.067
[33,     1] loss: 923.061
[34,     1] loss: 883.065
[35,     1] loss: 878.026
[36,     1] loss: 875.145
[37,     1] loss: 848.924
[38,     1] loss: 963.979
[39,     1] loss: 960.497
[40,     1] loss: 846.126
[41,     1] loss: 837.750
[42,     1] loss: 817.371
[43,     1] loss: 850.921
[44,     1] loss: 790.151
[45,     1] loss: 772.680
[46,     1] loss: 841.426
[47,     1] loss: 753.781
[48,     1] loss: 747.514
[49,     1] loss: 872.305
[50,     1] loss: 936.769
[51,     1] loss: 749.861
[52,     1] loss: 832.299
[53,     1] loss: 776.949
[54,     1] loss: 836.593
[55,     1] loss: 765.604
[56,     1] loss: 754.730
[57,     1] loss: 719.593
[58,     1] loss: 751.457
[59,     1] loss: 717.912
[60,     1] loss: 684.795
[61,     1] loss: 659.067
[62,     1] loss: 694.255
[63,     1] loss: 1091.909
[64,     1] loss: 755.470
[65,     1] loss: 758.453
[66,     1] loss: 774.043
[67,     1] loss: 791.440
[68,     1] loss: 698.782
[69,     1] loss: 775.833
[70,     1] loss: 636.047
[71,     1] loss: 625.016
[72,     1] loss: 722.893
[73,     1] loss: 851.381
[74,     1] loss: 617.034
[75,     1] loss: 871.619
[76,     1] loss: 711.978
[77,     1] loss: 847.411
[78,     1] loss: 644.022
[79,     1] loss: 774.472
[80,     1] loss: 645.792
[81,     1] loss: 808.727
[82,     1] loss: 626.375
[83,     1] loss: 821.622
[84,     1] loss: 629.794
[85,     1] loss: 731.241
[86,     1] loss: 597.618
[87,     1] loss: 685.139
[88,     1] loss: 573.587
[89,     1] loss: 569.777
[90,     1] loss: 610.075
[91,     1] loss: 545.011
[92,     1] loss: 494.185
[93,     1] loss: 611.372
[94,     1] loss: 769.628
[95,     1] loss: 501.985
[96,     1] loss: 553.676
[97,     1] loss: 821.221
[98,     1] loss: 537.446
[99,     1] loss: 608.033
[100,     1] loss: 797.570
[101,     1] loss: 540.332
[102,     1] loss: 796.023
[103,     1] loss: 861.867
[104,     1] loss: 735.022
[105,     1] loss: 692.269
[106,     1] loss: 825.018
[107,     1] loss: 853.199
[108,     1] loss: 611.016
[109,     1] loss: 603.027
[110,     1] loss: 656.221
[111,     1] loss: 743.320
[112,     1] loss: 629.494
[113,     1] loss: 576.754
[114,     1] loss: 594.214
[115,     1] loss: 540.286
[116,     1] loss: 513.054
[117,     1] loss: 539.393
[118,     1] loss: 946.851
[119,     1] loss: 2376.197
[120,     1] loss: 1168.663
[121,     1] loss: 1234.728
[122,     1] loss: 1243.347
[123,     1] loss: 1240.823
[124,     1] loss: 1243.620
[125,     1] loss: 1253.097
[126,     1] loss: 1255.249
[127,     1] loss: 1247.376
[128,     1] loss: 1252.340
[129,     1] loss: 1251.965
[130,     1] loss: 1249.872
[131,     1] loss: 1247.392
[132,     1] loss: 1246.392
[133,     1] loss: 1241.184
[134,     1] loss: 1243.739
[135,     1] loss: 1240.689
[136,     1] loss: 1244.181
[137,     1] loss: 1243.671
[138,     1] loss: 1245.183
[139,     1] loss: 1245.115
[140,     1] loss: 1242.924
[141,     1] loss: 1244.827
[142,     1] loss: 1243.940
[143,     1] loss: 1244.083
[144,     1] loss: 1243.089
[145,     1] loss: 1241.522
[146,     1] loss: 1241.814
[147,     1] loss: 1243.012
[148,     1] loss: 1242.854
[149,     1] loss: 1242.328
[150,     1] loss: 1243.106
[151,     1] loss: 1243.307
[152,     1] loss: 1242.523
[153,     1] loss: 1241.538
[154,     1] loss: 1241.783
[155,     1] loss: 1242.512
[156,     1] loss: 1244.472
[157,     1] loss: 1243.304
[158,     1] loss: 1242.067
[159,     1] loss: 1241.495
[160,     1] loss: 1242.495
[161,     1] loss: 1243.096
[162,     1] loss: 1241.905
[163,     1] loss: 1242.881
[164,     1] loss: 1242.960
[165,     1] loss: 1241.271
Early stopping applied (best metric=0.2970851957798004)
Finished Training
Total time taken: 23.494018077850342
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.720
[2,     1] loss: 1242.306
[3,     1] loss: 1240.146
[4,     1] loss: 1245.516
[5,     1] loss: 1244.427
[6,     1] loss: 1239.635
[7,     1] loss: 1244.193
[8,     1] loss: 1242.139
[9,     1] loss: 1228.120
[10,     1] loss: 1221.901
[11,     1] loss: 1201.521
[12,     1] loss: 1170.910
[13,     1] loss: 1120.333
[14,     1] loss: 1102.237
[15,     1] loss: 1080.850
[16,     1] loss: 1057.036
[17,     1] loss: 1040.772
[18,     1] loss: 1046.246
[19,     1] loss: 1018.520
[20,     1] loss: 1034.665
[21,     1] loss: 1028.510
[22,     1] loss: 990.618
[23,     1] loss: 982.040
[24,     1] loss: 990.979
[25,     1] loss: 953.909
[26,     1] loss: 950.828
[27,     1] loss: 941.376
[28,     1] loss: 947.474
[29,     1] loss: 894.800
[30,     1] loss: 885.173
[31,     1] loss: 891.750
[32,     1] loss: 903.986
[33,     1] loss: 1010.329
[34,     1] loss: 1152.855
[35,     1] loss: 877.765
[36,     1] loss: 986.894
[37,     1] loss: 976.451
[38,     1] loss: 943.938
[39,     1] loss: 934.435
[40,     1] loss: 905.551
[41,     1] loss: 923.333
[42,     1] loss: 862.295
[43,     1] loss: 889.883
[44,     1] loss: 853.593
[45,     1] loss: 879.209
[46,     1] loss: 877.827
[47,     1] loss: 838.229
[48,     1] loss: 862.977
[49,     1] loss: 813.304
[50,     1] loss: 838.840
[51,     1] loss: 778.378
[52,     1] loss: 809.354
[53,     1] loss: 802.345
[54,     1] loss: 738.237
[55,     1] loss: 751.288
[56,     1] loss: 725.176
[57,     1] loss: 663.099
[58,     1] loss: 705.599
[59,     1] loss: 853.471
[60,     1] loss: 1370.328
[61,     1] loss: 817.860
[62,     1] loss: 955.283
[63,     1] loss: 931.806
[64,     1] loss: 924.133
[65,     1] loss: 948.341
[66,     1] loss: 919.892
[67,     1] loss: 895.926
[68,     1] loss: 860.710
[69,     1] loss: 886.258
[70,     1] loss: 891.636
[71,     1] loss: 789.359
[72,     1] loss: 832.410
[73,     1] loss: 852.688
[74,     1] loss: 866.257
[75,     1] loss: 829.474
[76,     1] loss: 841.134
[77,     1] loss: 791.181
[78,     1] loss: 798.730
[79,     1] loss: 795.052
[80,     1] loss: 744.435
[81,     1] loss: 698.611
[82,     1] loss: 708.461
[83,     1] loss: 761.762
[84,     1] loss: 755.758
[85,     1] loss: 678.022
[86,     1] loss: 655.464
[87,     1] loss: 711.364
[88,     1] loss: 962.025
[89,     1] loss: 1359.395
[90,     1] loss: 791.517
[91,     1] loss: 1007.773
[92,     1] loss: 1029.333
[93,     1] loss: 906.432
[94,     1] loss: 870.540
[95,     1] loss: 870.721
[96,     1] loss: 850.519
[97,     1] loss: 920.222
[98,     1] loss: 832.386
[99,     1] loss: 805.532
[100,     1] loss: 789.242
[101,     1] loss: 775.083
[102,     1] loss: 777.138
Early stopping applied (best metric=0.3262473940849304)
Finished Training
Total time taken: 17.544432163238525
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.093
[2,     1] loss: 1247.977
[3,     1] loss: 1240.345
[4,     1] loss: 1240.861
[5,     1] loss: 1243.978
[6,     1] loss: 1242.687
[7,     1] loss: 1240.051
[8,     1] loss: 1237.352
[9,     1] loss: 1231.494
[10,     1] loss: 1222.380
[11,     1] loss: 1205.602
[12,     1] loss: 1174.517
[13,     1] loss: 1172.365
[14,     1] loss: 1113.398
[15,     1] loss: 1079.219
[16,     1] loss: 1115.155
[17,     1] loss: 1030.114
[18,     1] loss: 1074.985
[19,     1] loss: 970.574
[20,     1] loss: 1037.593
[21,     1] loss: 979.511
[22,     1] loss: 994.596
[23,     1] loss: 965.171
[24,     1] loss: 1019.653
[25,     1] loss: 941.176
[26,     1] loss: 961.780
[27,     1] loss: 937.094
[28,     1] loss: 960.263
[29,     1] loss: 912.661
[30,     1] loss: 949.667
[31,     1] loss: 901.666
[32,     1] loss: 910.203
[33,     1] loss: 861.703
[34,     1] loss: 855.510
[35,     1] loss: 799.054
[36,     1] loss: 793.487
[37,     1] loss: 842.880
[38,     1] loss: 808.047
[39,     1] loss: 804.897
[40,     1] loss: 792.932
[41,     1] loss: 798.287
[42,     1] loss: 1330.969
[43,     1] loss: 961.688
[44,     1] loss: 1041.129
[45,     1] loss: 990.940
[46,     1] loss: 1003.387
[47,     1] loss: 993.454
[48,     1] loss: 962.741
[49,     1] loss: 955.805
[50,     1] loss: 882.995
[51,     1] loss: 868.912
[52,     1] loss: 879.109
[53,     1] loss: 898.371
[54,     1] loss: 811.154
[55,     1] loss: 831.354
[56,     1] loss: 834.146
[57,     1] loss: 779.501
[58,     1] loss: 828.182
[59,     1] loss: 820.862
[60,     1] loss: 841.997
[61,     1] loss: 814.319
[62,     1] loss: 757.042
[63,     1] loss: 780.958
[64,     1] loss: 763.111
[65,     1] loss: 768.516
[66,     1] loss: 690.417
[67,     1] loss: 693.659
[68,     1] loss: 682.996
[69,     1] loss: 727.388
[70,     1] loss: 676.709
[71,     1] loss: 938.592
[72,     1] loss: 772.173
[73,     1] loss: 714.703
[74,     1] loss: 695.623
[75,     1] loss: 722.542
[76,     1] loss: 735.788
[77,     1] loss: 650.384
[78,     1] loss: 689.026
[79,     1] loss: 721.369
[80,     1] loss: 817.879
[81,     1] loss: 641.587
[82,     1] loss: 774.384
[83,     1] loss: 679.278
[84,     1] loss: 620.194
[85,     1] loss: 730.209
[86,     1] loss: 654.672
[87,     1] loss: 640.032
[88,     1] loss: 782.475
[89,     1] loss: 652.119
[90,     1] loss: 662.304
[91,     1] loss: 636.902
[92,     1] loss: 552.647
[93,     1] loss: 629.480
[94,     1] loss: 770.201
[95,     1] loss: 751.295
[96,     1] loss: 629.674
[97,     1] loss: 682.885
[98,     1] loss: 533.188
[99,     1] loss: 624.272
[100,     1] loss: 539.464
[101,     1] loss: 500.056
[102,     1] loss: 499.880
[103,     1] loss: 493.798
[104,     1] loss: 459.393
[105,     1] loss: 1235.300
[106,     1] loss: 2482.879
[107,     1] loss: 1251.757
[108,     1] loss: 1236.865
[109,     1] loss: 1243.544
[110,     1] loss: 1240.142
[111,     1] loss: 1242.765
[112,     1] loss: 1240.701
[113,     1] loss: 1243.678
[114,     1] loss: 1242.737
[115,     1] loss: 1243.012
[116,     1] loss: 1241.031
[117,     1] loss: 1240.626
[118,     1] loss: 1242.327
[119,     1] loss: 1243.369
[120,     1] loss: 1243.075
[121,     1] loss: 1241.848
[122,     1] loss: 1241.793
[123,     1] loss: 1240.248
[124,     1] loss: 1240.904
[125,     1] loss: 1243.099
[126,     1] loss: 1241.096
[127,     1] loss: 1241.420
[128,     1] loss: 1240.350
[129,     1] loss: 1240.203
[130,     1] loss: 1240.964
[131,     1] loss: 1242.118
[132,     1] loss: 1240.757
[133,     1] loss: 1242.166
[134,     1] loss: 1242.121
[135,     1] loss: 1239.454
[136,     1] loss: 1240.818
[137,     1] loss: 1241.137
[138,     1] loss: 1241.004
[139,     1] loss: 1241.130
[140,     1] loss: 1238.497
[141,     1] loss: 1241.551
[142,     1] loss: 1241.772
Early stopping applied (best metric=0.39136603474617004)
Finished Training
Total time taken: 19.721415281295776
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.607
[2,     1] loss: 1244.970
[3,     1] loss: 1236.877
[4,     1] loss: 1234.353
[5,     1] loss: 1229.861
[6,     1] loss: 1228.548
[7,     1] loss: 1192.710
[8,     1] loss: 1155.465
[9,     1] loss: 1163.766
[10,     1] loss: 1067.479
[11,     1] loss: 1036.514
[12,     1] loss: 1056.451
[13,     1] loss: 1041.481
[14,     1] loss: 1011.745
[15,     1] loss: 1006.838
[16,     1] loss: 1005.622
[17,     1] loss: 968.000
[18,     1] loss: 1010.083
[19,     1] loss: 977.493
[20,     1] loss: 923.357
[21,     1] loss: 891.143
[22,     1] loss: 932.253
[23,     1] loss: 952.927
[24,     1] loss: 931.695
[25,     1] loss: 913.295
[26,     1] loss: 880.447
[27,     1] loss: 904.711
[28,     1] loss: 884.951
[29,     1] loss: 894.515
[30,     1] loss: 944.853
[31,     1] loss: 884.975
[32,     1] loss: 866.986
[33,     1] loss: 858.284
[34,     1] loss: 888.340
[35,     1] loss: 801.936
[36,     1] loss: 836.920
[37,     1] loss: 818.130
[38,     1] loss: 780.988
[39,     1] loss: 740.808
[40,     1] loss: 767.726
[41,     1] loss: 873.022
[42,     1] loss: 1066.820
[43,     1] loss: 820.208
[44,     1] loss: 933.505
[45,     1] loss: 859.390
[46,     1] loss: 868.599
[47,     1] loss: 857.931
[48,     1] loss: 751.545
[49,     1] loss: 825.087
[50,     1] loss: 732.148
[51,     1] loss: 798.806
[52,     1] loss: 747.544
[53,     1] loss: 823.679
[54,     1] loss: 739.823
[55,     1] loss: 808.629
[56,     1] loss: 741.275
[57,     1] loss: 760.009
[58,     1] loss: 699.661
[59,     1] loss: 614.855
[60,     1] loss: 679.878
[61,     1] loss: 860.228
[62,     1] loss: 1064.851
[63,     1] loss: 700.047
[64,     1] loss: 807.320
[65,     1] loss: 901.730
[66,     1] loss: 805.812
[67,     1] loss: 850.344
[68,     1] loss: 802.213
[69,     1] loss: 776.504
[70,     1] loss: 907.358
[71,     1] loss: 740.927
[72,     1] loss: 704.290
[73,     1] loss: 713.746
[74,     1] loss: 695.566
[75,     1] loss: 721.642
[76,     1] loss: 697.740
[77,     1] loss: 595.538
[78,     1] loss: 720.206
[79,     1] loss: 768.103
[80,     1] loss: 628.662
[81,     1] loss: 644.035
[82,     1] loss: 683.094
[83,     1] loss: 592.540
[84,     1] loss: 823.573
[85,     1] loss: 827.393
[86,     1] loss: 611.144
[87,     1] loss: 837.727
Early stopping applied (best metric=0.38234493136405945)
Finished Training
Total time taken: 14.122238159179688
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.329
[2,     1] loss: 1244.938
[3,     1] loss: 1243.901
[4,     1] loss: 1241.443
[5,     1] loss: 1240.411
[6,     1] loss: 1229.558
[7,     1] loss: 1225.576
[8,     1] loss: 1191.205
[9,     1] loss: 1153.927
[10,     1] loss: 1105.636
[11,     1] loss: 1051.916
[12,     1] loss: 1095.737
[13,     1] loss: 1102.422
[14,     1] loss: 1016.181
[15,     1] loss: 1091.134
[16,     1] loss: 993.481
[17,     1] loss: 1025.533
[18,     1] loss: 1009.219
[19,     1] loss: 1008.878
[20,     1] loss: 997.350
[21,     1] loss: 973.023
[22,     1] loss: 989.588
[23,     1] loss: 952.413
[24,     1] loss: 938.402
[25,     1] loss: 956.745
[26,     1] loss: 957.213
[27,     1] loss: 991.788
[28,     1] loss: 960.299
[29,     1] loss: 886.603
[30,     1] loss: 903.992
[31,     1] loss: 907.507
[32,     1] loss: 868.764
[33,     1] loss: 914.275
[34,     1] loss: 877.362
[35,     1] loss: 856.714
[36,     1] loss: 858.395
[37,     1] loss: 883.063
[38,     1] loss: 830.936
[39,     1] loss: 826.242
[40,     1] loss: 819.821
[41,     1] loss: 791.982
[42,     1] loss: 792.080
[43,     1] loss: 823.552
[44,     1] loss: 925.871
[45,     1] loss: 834.448
[46,     1] loss: 811.476
[47,     1] loss: 842.351
[48,     1] loss: 796.911
[49,     1] loss: 815.150
[50,     1] loss: 827.586
[51,     1] loss: 795.929
[52,     1] loss: 736.637
[53,     1] loss: 711.452
[54,     1] loss: 699.845
[55,     1] loss: 736.799
[56,     1] loss: 807.069
[57,     1] loss: 800.660
[58,     1] loss: 702.015
[59,     1] loss: 709.966
[60,     1] loss: 685.130
[61,     1] loss: 650.655
[62,     1] loss: 784.619
[63,     1] loss: 731.071
[64,     1] loss: 636.312
[65,     1] loss: 698.182
[66,     1] loss: 610.734
[67,     1] loss: 692.153
[68,     1] loss: 813.850
[69,     1] loss: 699.081
[70,     1] loss: 592.482
[71,     1] loss: 665.780
[72,     1] loss: 614.206
[73,     1] loss: 485.279
[74,     1] loss: 651.936
[75,     1] loss: 949.686
[76,     1] loss: 1847.616
[77,     1] loss: 1047.674
[78,     1] loss: 1089.982
[79,     1] loss: 1164.107
[80,     1] loss: 1194.111
[81,     1] loss: 1197.822
[82,     1] loss: 1205.021
[83,     1] loss: 1206.390
[84,     1] loss: 1186.407
[85,     1] loss: 1176.178
[86,     1] loss: 1169.784
[87,     1] loss: 1145.141
[88,     1] loss: 1142.470
[89,     1] loss: 1120.770
[90,     1] loss: 1113.984
[91,     1] loss: 1112.662
[92,     1] loss: 1070.378
[93,     1] loss: 1092.665
[94,     1] loss: 1108.096
[95,     1] loss: 1109.658
[96,     1] loss: 1085.598
[97,     1] loss: 1070.570
[98,     1] loss: 1058.183
[99,     1] loss: 1050.355
[100,     1] loss: 1037.150
[101,     1] loss: 1017.467
[102,     1] loss: 1009.744
Early stopping applied (best metric=0.3822689354419708)
Finished Training
Total time taken: 17.294060468673706
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1244.418
[2,     1] loss: 1243.524
[3,     1] loss: 1242.064
[4,     1] loss: 1241.975
[5,     1] loss: 1238.789
[6,     1] loss: 1237.899
[7,     1] loss: 1233.771
[8,     1] loss: 1224.964
[9,     1] loss: 1201.378
[10,     1] loss: 1186.755
[11,     1] loss: 1126.284
[12,     1] loss: 1073.237
[13,     1] loss: 1098.588
[14,     1] loss: 1043.884
[15,     1] loss: 1025.211
[16,     1] loss: 1046.899
[17,     1] loss: 994.451
[18,     1] loss: 1065.702
[19,     1] loss: 962.958
[20,     1] loss: 1021.665
[21,     1] loss: 935.812
[22,     1] loss: 982.965
[23,     1] loss: 928.366
[24,     1] loss: 973.384
[25,     1] loss: 980.754
[26,     1] loss: 922.651
[27,     1] loss: 913.003
[28,     1] loss: 967.409
[29,     1] loss: 883.536
[30,     1] loss: 939.190
[31,     1] loss: 894.466
[32,     1] loss: 852.017
[33,     1] loss: 893.253
[34,     1] loss: 818.289
[35,     1] loss: 844.010
[36,     1] loss: 858.344
[37,     1] loss: 770.012
[38,     1] loss: 778.234
[39,     1] loss: 909.793
[40,     1] loss: 945.180
[41,     1] loss: 786.398
[42,     1] loss: 901.531
[43,     1] loss: 800.669
[44,     1] loss: 807.435
[45,     1] loss: 779.850
[46,     1] loss: 734.571
[47,     1] loss: 740.996
[48,     1] loss: 749.482
[49,     1] loss: 676.268
[50,     1] loss: 855.332
[51,     1] loss: 937.307
[52,     1] loss: 779.421
[53,     1] loss: 829.385
[54,     1] loss: 807.001
[55,     1] loss: 782.482
[56,     1] loss: 829.410
[57,     1] loss: 738.047
[58,     1] loss: 797.794
[59,     1] loss: 723.191
[60,     1] loss: 735.280
[61,     1] loss: 697.613
[62,     1] loss: 676.687
[63,     1] loss: 753.068
[64,     1] loss: 628.743
[65,     1] loss: 762.346
[66,     1] loss: 780.792
[67,     1] loss: 636.830
[68,     1] loss: 765.746
[69,     1] loss: 663.283
[70,     1] loss: 718.848
[71,     1] loss: 596.355
[72,     1] loss: 671.204
[73,     1] loss: 625.912
[74,     1] loss: 544.712
[75,     1] loss: 601.870
[76,     1] loss: 1050.986
[77,     1] loss: 707.857
[78,     1] loss: 665.715
[79,     1] loss: 702.198
[80,     1] loss: 760.346
[81,     1] loss: 657.385
[82,     1] loss: 690.884
[83,     1] loss: 618.676
[84,     1] loss: 639.713
[85,     1] loss: 571.856
[86,     1] loss: 621.134
[87,     1] loss: 737.578
[88,     1] loss: 744.417
[89,     1] loss: 838.829
[90,     1] loss: 633.734
[91,     1] loss: 824.178
[92,     1] loss: 626.525
[93,     1] loss: 688.997
[94,     1] loss: 653.848
[95,     1] loss: 629.409
[96,     1] loss: 646.376
[97,     1] loss: 726.774
[98,     1] loss: 599.124
[99,     1] loss: 702.099
[100,     1] loss: 742.853
[101,     1] loss: 541.883
[102,     1] loss: 791.029
[103,     1] loss: 617.850
[104,     1] loss: 678.374
[105,     1] loss: 565.868
[106,     1] loss: 748.826
[107,     1] loss: 683.055
[108,     1] loss: 590.447
[109,     1] loss: 596.916
[110,     1] loss: 515.629
[111,     1] loss: 528.295
[112,     1] loss: 528.300
[113,     1] loss: 915.706
[114,     1] loss: 1453.115
[115,     1] loss: 928.804
[116,     1] loss: 1053.373
[117,     1] loss: 1133.893
[118,     1] loss: 1102.662
[119,     1] loss: 997.648
[120,     1] loss: 936.283
[121,     1] loss: 935.018
[122,     1] loss: 888.232
Early stopping applied (best metric=0.4119957685470581)
Finished Training
Total time taken: 16.48695731163025
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.429
[2,     1] loss: 1242.964
[3,     1] loss: 1240.284
[4,     1] loss: 1240.951
[5,     1] loss: 1237.548
[6,     1] loss: 1231.088
[7,     1] loss: 1219.648
[8,     1] loss: 1190.803
[9,     1] loss: 1133.537
[10,     1] loss: 1108.296
[11,     1] loss: 1120.279
[12,     1] loss: 1094.840
[13,     1] loss: 1038.201
[14,     1] loss: 1078.687
[15,     1] loss: 975.258
[16,     1] loss: 1051.516
[17,     1] loss: 962.784
[18,     1] loss: 1015.145
[19,     1] loss: 962.080
[20,     1] loss: 990.548
[21,     1] loss: 940.894
[22,     1] loss: 973.177
[23,     1] loss: 932.664
[24,     1] loss: 924.206
[25,     1] loss: 878.373
[26,     1] loss: 845.563
[27,     1] loss: 871.365
[28,     1] loss: 867.488
[29,     1] loss: 892.277
[30,     1] loss: 813.074
[31,     1] loss: 791.096
[32,     1] loss: 831.924
[33,     1] loss: 867.061
[34,     1] loss: 1134.408
[35,     1] loss: 841.469
[36,     1] loss: 900.211
[37,     1] loss: 924.452
[38,     1] loss: 833.901
[39,     1] loss: 871.589
[40,     1] loss: 822.367
[41,     1] loss: 877.633
[42,     1] loss: 788.543
[43,     1] loss: 840.244
[44,     1] loss: 753.253
[45,     1] loss: 778.133
[46,     1] loss: 743.793
[47,     1] loss: 782.538
[48,     1] loss: 693.093
[49,     1] loss: 713.254
[50,     1] loss: 713.572
[51,     1] loss: 720.672
[52,     1] loss: 633.899
[53,     1] loss: 708.617
[54,     1] loss: 811.331
[55,     1] loss: 1048.930
[56,     1] loss: 783.903
[57,     1] loss: 872.199
[58,     1] loss: 778.668
[59,     1] loss: 730.292
[60,     1] loss: 724.244
[61,     1] loss: 690.093
[62,     1] loss: 767.726
[63,     1] loss: 669.455
[64,     1] loss: 636.651
[65,     1] loss: 646.181
[66,     1] loss: 592.704
[67,     1] loss: 607.792
[68,     1] loss: 648.489
[69,     1] loss: 585.276
[70,     1] loss: 688.989
[71,     1] loss: 1052.795
[72,     1] loss: 629.016
[73,     1] loss: 826.454
[74,     1] loss: 718.771
[75,     1] loss: 809.032
[76,     1] loss: 664.963
[77,     1] loss: 688.694
[78,     1] loss: 630.236
[79,     1] loss: 605.405
[80,     1] loss: 538.240
[81,     1] loss: 516.386
[82,     1] loss: 540.667
[83,     1] loss: 566.392
[84,     1] loss: 562.678
[85,     1] loss: 653.381
[86,     1] loss: 1117.515
[87,     1] loss: 552.965
[88,     1] loss: 919.064
[89,     1] loss: 732.671
[90,     1] loss: 858.493
[91,     1] loss: 808.714
[92,     1] loss: 644.767
[93,     1] loss: 854.676
[94,     1] loss: 724.511
[95,     1] loss: 736.310
[96,     1] loss: 641.068
[97,     1] loss: 652.012
[98,     1] loss: 564.567
[99,     1] loss: 627.843
[100,     1] loss: 515.613
[101,     1] loss: 680.172
[102,     1] loss: 998.345
[103,     1] loss: 544.195
[104,     1] loss: 765.278
[105,     1] loss: 669.985
[106,     1] loss: 785.033
[107,     1] loss: 599.437
[108,     1] loss: 731.748
[109,     1] loss: 684.395
[110,     1] loss: 555.213
[111,     1] loss: 727.093
[112,     1] loss: 616.781
[113,     1] loss: 691.857
[114,     1] loss: 589.704
[115,     1] loss: 571.413
[116,     1] loss: 629.032
[117,     1] loss: 542.997
[118,     1] loss: 621.796
[119,     1] loss: 518.653
[120,     1] loss: 512.338
[121,     1] loss: 557.466
[122,     1] loss: 551.291
[123,     1] loss: 454.462
[124,     1] loss: 532.107
[125,     1] loss: 573.783
[126,     1] loss: 488.274
[127,     1] loss: 484.084
[128,     1] loss: 657.944
Early stopping applied (best metric=0.3720703423023224)
Finished Training
Total time taken: 17.400473833084106
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.201
[2,     1] loss: 1245.931
[3,     1] loss: 1238.313
[4,     1] loss: 1240.410
[5,     1] loss: 1241.111
[6,     1] loss: 1239.589
[7,     1] loss: 1228.960
[8,     1] loss: 1215.855
[9,     1] loss: 1179.717
[10,     1] loss: 1141.086
[11,     1] loss: 1110.317
[12,     1] loss: 1103.575
[13,     1] loss: 1087.181
[14,     1] loss: 1037.230
[15,     1] loss: 1039.457
[16,     1] loss: 1055.209
[17,     1] loss: 1024.014
[18,     1] loss: 1031.372
[19,     1] loss: 1047.120
[20,     1] loss: 989.265
[21,     1] loss: 991.189
[22,     1] loss: 985.176
[23,     1] loss: 940.967
[24,     1] loss: 948.317
[25,     1] loss: 957.916
[26,     1] loss: 948.418
[27,     1] loss: 912.344
[28,     1] loss: 866.896
[29,     1] loss: 923.213
[30,     1] loss: 880.083
[31,     1] loss: 858.558
[32,     1] loss: 917.454
[33,     1] loss: 862.736
[34,     1] loss: 931.447
[35,     1] loss: 945.016
[36,     1] loss: 863.294
[37,     1] loss: 894.890
[38,     1] loss: 842.520
[39,     1] loss: 891.208
[40,     1] loss: 924.435
[41,     1] loss: 798.526
[42,     1] loss: 829.993
[43,     1] loss: 775.223
[44,     1] loss: 784.192
[45,     1] loss: 764.913
[46,     1] loss: 755.461
[47,     1] loss: 670.967
[48,     1] loss: 786.383
[49,     1] loss: 1089.418
[50,     1] loss: 1366.387
[51,     1] loss: 1123.850
[52,     1] loss: 1019.463
[53,     1] loss: 1053.180
[54,     1] loss: 1078.193
[55,     1] loss: 1076.769
[56,     1] loss: 1091.888
[57,     1] loss: 1088.188
[58,     1] loss: 1036.931
[59,     1] loss: 1053.146
[60,     1] loss: 1047.252
[61,     1] loss: 1014.514
[62,     1] loss: 989.297
[63,     1] loss: 953.175
[64,     1] loss: 1000.345
[65,     1] loss: 964.810
[66,     1] loss: 967.763
[67,     1] loss: 881.619
[68,     1] loss: 865.718
[69,     1] loss: 880.984
[70,     1] loss: 889.979
[71,     1] loss: 866.758
[72,     1] loss: 817.456
[73,     1] loss: 851.479
[74,     1] loss: 826.275
[75,     1] loss: 765.480
[76,     1] loss: 763.647
[77,     1] loss: 689.217
[78,     1] loss: 729.157
[79,     1] loss: 683.500
[80,     1] loss: 677.209
[81,     1] loss: 782.775
[82,     1] loss: 1320.630
[83,     1] loss: 857.590
[84,     1] loss: 1063.884
[85,     1] loss: 898.445
[86,     1] loss: 988.530
[87,     1] loss: 933.725
[88,     1] loss: 839.811
[89,     1] loss: 894.224
[90,     1] loss: 917.518
[91,     1] loss: 863.309
[92,     1] loss: 913.180
[93,     1] loss: 845.394
[94,     1] loss: 843.627
[95,     1] loss: 804.334
[96,     1] loss: 761.386
[97,     1] loss: 749.282
Early stopping applied (best metric=0.3873048722743988)
Finished Training
Total time taken: 17.070382595062256
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.029
[2,     1] loss: 1240.749
[3,     1] loss: 1243.456
[4,     1] loss: 1240.554
[5,     1] loss: 1240.269
[6,     1] loss: 1239.989
[7,     1] loss: 1239.642
[8,     1] loss: 1236.071
[9,     1] loss: 1236.671
[10,     1] loss: 1230.838
[11,     1] loss: 1217.960
[12,     1] loss: 1198.734
[13,     1] loss: 1185.771
[14,     1] loss: 1117.387
[15,     1] loss: 1077.678
[16,     1] loss: 1077.823
[17,     1] loss: 1106.540
[18,     1] loss: 1101.469
[19,     1] loss: 1021.127
[20,     1] loss: 1042.529
[21,     1] loss: 1004.442
[22,     1] loss: 1027.008
[23,     1] loss: 1025.441
[24,     1] loss: 990.124
[25,     1] loss: 971.431
[26,     1] loss: 965.529
[27,     1] loss: 922.787
[28,     1] loss: 968.576
[29,     1] loss: 893.681
[30,     1] loss: 982.089
[31,     1] loss: 898.877
[32,     1] loss: 942.188
[33,     1] loss: 874.955
[34,     1] loss: 894.347
[35,     1] loss: 886.711
[36,     1] loss: 904.876
[37,     1] loss: 829.247
[38,     1] loss: 852.202
[39,     1] loss: 829.265
[40,     1] loss: 903.273
[41,     1] loss: 912.382
[42,     1] loss: 822.477
[43,     1] loss: 848.264
[44,     1] loss: 824.197
[45,     1] loss: 837.473
[46,     1] loss: 830.906
[47,     1] loss: 725.889
[48,     1] loss: 795.692
[49,     1] loss: 747.460
[50,     1] loss: 780.052
[51,     1] loss: 799.868
[52,     1] loss: 695.217
[53,     1] loss: 682.747
[54,     1] loss: 767.850
[55,     1] loss: 1109.517
[56,     1] loss: 722.501
[57,     1] loss: 883.102
[58,     1] loss: 813.544
[59,     1] loss: 829.457
[60,     1] loss: 757.723
[61,     1] loss: 725.777
[62,     1] loss: 731.645
[63,     1] loss: 709.908
[64,     1] loss: 655.072
[65,     1] loss: 643.396
[66,     1] loss: 632.811
[67,     1] loss: 611.667
[68,     1] loss: 643.806
[69,     1] loss: 697.842
[70,     1] loss: 1128.783
[71,     1] loss: 621.540
[72,     1] loss: 1035.866
[73,     1] loss: 756.339
[74,     1] loss: 931.224
[75,     1] loss: 980.462
[76,     1] loss: 852.246
[77,     1] loss: 761.305
[78,     1] loss: 836.039
[79,     1] loss: 737.058
[80,     1] loss: 827.448
[81,     1] loss: 728.995
[82,     1] loss: 741.285
[83,     1] loss: 699.611
[84,     1] loss: 648.184
[85,     1] loss: 697.359
[86,     1] loss: 606.365
[87,     1] loss: 584.737
[88,     1] loss: 770.299
[89,     1] loss: 568.764
[90,     1] loss: 583.047
[91,     1] loss: 623.932
[92,     1] loss: 543.262
[93,     1] loss: 727.624
[94,     1] loss: 770.995
[95,     1] loss: 589.186
[96,     1] loss: 894.665
[97,     1] loss: 757.323
[98,     1] loss: 846.142
[99,     1] loss: 600.334
[100,     1] loss: 841.299
[101,     1] loss: 589.780
[102,     1] loss: 773.386
[103,     1] loss: 606.185
[104,     1] loss: 677.978
[105,     1] loss: 547.514
[106,     1] loss: 568.189
[107,     1] loss: 507.034
[108,     1] loss: 510.056
[109,     1] loss: 434.381
[110,     1] loss: 444.038
[111,     1] loss: 515.226
[112,     1] loss: 522.239
[113,     1] loss: 550.925
[114,     1] loss: 670.801
[115,     1] loss: 462.353
[116,     1] loss: 556.531
[117,     1] loss: 1042.650
[118,     1] loss: 516.367
[119,     1] loss: 710.553
[120,     1] loss: 729.179
[121,     1] loss: 791.654
[122,     1] loss: 542.439
[123,     1] loss: 929.357
[124,     1] loss: 870.306
[125,     1] loss: 1055.148
[126,     1] loss: 991.762
[127,     1] loss: 998.366
[128,     1] loss: 1000.856
[129,     1] loss: 992.084
[130,     1] loss: 867.041
[131,     1] loss: 808.851
[132,     1] loss: 782.006
[133,     1] loss: 863.522
[134,     1] loss: 1055.003
[135,     1] loss: 807.812
[136,     1] loss: 823.014
[137,     1] loss: 769.437
[138,     1] loss: 777.120
[139,     1] loss: 730.216
[140,     1] loss: 704.915
[141,     1] loss: 699.473
[142,     1] loss: 657.695
[143,     1] loss: 654.774
[144,     1] loss: 640.670
[145,     1] loss: 596.883
[146,     1] loss: 547.399
[147,     1] loss: 570.864
[148,     1] loss: 558.787
[149,     1] loss: 583.275
[150,     1] loss: 805.912
[151,     1] loss: 591.877
[152,     1] loss: 537.555
[153,     1] loss: 592.350
[154,     1] loss: 538.462
[155,     1] loss: 555.165
[156,     1] loss: 728.560
[157,     1] loss: 608.659
[158,     1] loss: 547.850
[159,     1] loss: 512.174
[160,     1] loss: 521.885
[161,     1] loss: 486.497
[162,     1] loss: 517.224
[163,     1] loss: 751.141
Early stopping applied (best metric=0.3776174485683441)
Finished Training
Total time taken: 23.289323329925537
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.879
[2,     1] loss: 1239.134
[3,     1] loss: 1248.668
[4,     1] loss: 1242.508
[5,     1] loss: 1244.562
[6,     1] loss: 1242.553
[7,     1] loss: 1243.906
[8,     1] loss: 1238.375
[9,     1] loss: 1236.390
[10,     1] loss: 1224.261
[11,     1] loss: 1208.855
[12,     1] loss: 1192.836
[13,     1] loss: 1147.690
[14,     1] loss: 1114.246
[15,     1] loss: 1088.184
[16,     1] loss: 1090.080
[17,     1] loss: 1023.116
[18,     1] loss: 1032.345
[19,     1] loss: 1079.427
[20,     1] loss: 989.566
[21,     1] loss: 1008.323
[22,     1] loss: 931.007
[23,     1] loss: 993.942
[24,     1] loss: 945.951
[25,     1] loss: 995.906
[26,     1] loss: 935.426
[27,     1] loss: 934.764
[28,     1] loss: 940.806
[29,     1] loss: 930.432
[30,     1] loss: 873.270
[31,     1] loss: 897.990
[32,     1] loss: 854.076
[33,     1] loss: 856.335
[34,     1] loss: 839.656
[35,     1] loss: 828.507
[36,     1] loss: 847.116
[37,     1] loss: 1081.708
[38,     1] loss: 962.507
[39,     1] loss: 925.646
[40,     1] loss: 923.769
[41,     1] loss: 894.982
[42,     1] loss: 866.712
[43,     1] loss: 836.289
[44,     1] loss: 864.217
[45,     1] loss: 818.761
[46,     1] loss: 855.074
[47,     1] loss: 763.714
[48,     1] loss: 786.438
[49,     1] loss: 764.802
[50,     1] loss: 821.305
[51,     1] loss: 773.824
[52,     1] loss: 753.362
[53,     1] loss: 753.002
[54,     1] loss: 719.524
[55,     1] loss: 723.612
[56,     1] loss: 674.506
[57,     1] loss: 677.870
[58,     1] loss: 631.208
[59,     1] loss: 714.218
[60,     1] loss: 761.216
[61,     1] loss: 906.388
[62,     1] loss: 684.271
[63,     1] loss: 771.412
[64,     1] loss: 697.097
[65,     1] loss: 730.870
[66,     1] loss: 690.068
[67,     1] loss: 637.951
[68,     1] loss: 662.509
[69,     1] loss: 609.682
[70,     1] loss: 697.958
[71,     1] loss: 629.097
[72,     1] loss: 657.686
[73,     1] loss: 536.609
[74,     1] loss: 605.031
[75,     1] loss: 766.973
[76,     1] loss: 586.509
[77,     1] loss: 587.588
[78,     1] loss: 708.796
[79,     1] loss: 532.032
[80,     1] loss: 483.475
[81,     1] loss: 551.428
[82,     1] loss: 845.604
[83,     1] loss: 2191.202
[84,     1] loss: 1162.441
[85,     1] loss: 1144.475
[86,     1] loss: 1213.444
[87,     1] loss: 1240.715
[88,     1] loss: 1241.925
[89,     1] loss: 1250.806
[90,     1] loss: 1241.549
[91,     1] loss: 1234.097
[92,     1] loss: 1238.360
[93,     1] loss: 1224.265
[94,     1] loss: 1211.995
[95,     1] loss: 1196.413
[96,     1] loss: 1169.912
[97,     1] loss: 1157.838
[98,     1] loss: 1156.671
[99,     1] loss: 1140.625
[100,     1] loss: 1119.804
[101,     1] loss: 1141.450
[102,     1] loss: 1113.471
[103,     1] loss: 1150.549
[104,     1] loss: 1179.755
[105,     1] loss: 1189.879
[106,     1] loss: 1139.358
[107,     1] loss: 1181.123
[108,     1] loss: 1108.823
[109,     1] loss: 1138.426
[110,     1] loss: 1126.692
[111,     1] loss: 1093.289
[112,     1] loss: 1080.392
[113,     1] loss: 1034.224
[114,     1] loss: 1054.097
[115,     1] loss: 1047.073
[116,     1] loss: 1021.337
[117,     1] loss: 1063.415
[118,     1] loss: 1008.031
[119,     1] loss: 1000.625
[120,     1] loss: 969.030
[121,     1] loss: 976.562
[122,     1] loss: 985.531
[123,     1] loss: 974.545
[124,     1] loss: 963.616
[125,     1] loss: 977.604
[126,     1] loss: 926.581
[127,     1] loss: 936.370
Early stopping applied (best metric=0.38371741771698)
Finished Training
Total time taken: 17.694392204284668
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1244.003
[2,     1] loss: 1244.140
[3,     1] loss: 1251.556
[4,     1] loss: 1242.969
[5,     1] loss: 1242.605
[6,     1] loss: 1243.132
[7,     1] loss: 1239.982
[8,     1] loss: 1238.792
[9,     1] loss: 1236.827
[10,     1] loss: 1231.087
[11,     1] loss: 1223.357
[12,     1] loss: 1202.259
[13,     1] loss: 1170.228
[14,     1] loss: 1149.831
[15,     1] loss: 1086.746
[16,     1] loss: 1122.579
[17,     1] loss: 1113.485
[18,     1] loss: 1034.696
[19,     1] loss: 1023.045
[20,     1] loss: 1050.299
[21,     1] loss: 998.869
[22,     1] loss: 997.662
[23,     1] loss: 970.605
[24,     1] loss: 989.735
[25,     1] loss: 974.939
[26,     1] loss: 933.248
[27,     1] loss: 974.195
[28,     1] loss: 984.003
[29,     1] loss: 957.721
[30,     1] loss: 986.779
[31,     1] loss: 948.983
[32,     1] loss: 939.350
[33,     1] loss: 865.686
[34,     1] loss: 912.383
[35,     1] loss: 927.119
[36,     1] loss: 896.084
[37,     1] loss: 906.848
[38,     1] loss: 839.279
[39,     1] loss: 868.874
[40,     1] loss: 875.914
[41,     1] loss: 963.123
[42,     1] loss: 889.680
[43,     1] loss: 811.208
[44,     1] loss: 843.871
[45,     1] loss: 826.255
[46,     1] loss: 819.065
[47,     1] loss: 752.307
[48,     1] loss: 829.405
[49,     1] loss: 853.875
[50,     1] loss: 766.993
[51,     1] loss: 788.144
[52,     1] loss: 785.771
[53,     1] loss: 702.350
[54,     1] loss: 716.424
[55,     1] loss: 874.380
[56,     1] loss: 1230.044
[57,     1] loss: 775.466
[58,     1] loss: 937.192
[59,     1] loss: 998.557
[60,     1] loss: 894.380
[61,     1] loss: 899.008
[62,     1] loss: 887.807
[63,     1] loss: 826.442
[64,     1] loss: 928.926
[65,     1] loss: 826.598
[66,     1] loss: 803.599
[67,     1] loss: 813.140
[68,     1] loss: 756.000
[69,     1] loss: 779.828
[70,     1] loss: 743.518
[71,     1] loss: 705.910
[72,     1] loss: 722.258
[73,     1] loss: 791.118
[74,     1] loss: 833.426
[75,     1] loss: 782.411
[76,     1] loss: 734.677
[77,     1] loss: 712.190
[78,     1] loss: 784.112
[79,     1] loss: 689.667
[80,     1] loss: 764.666
[81,     1] loss: 750.997
[82,     1] loss: 623.628
[83,     1] loss: 611.680
[84,     1] loss: 675.905
[85,     1] loss: 822.337
[86,     1] loss: 563.362
[87,     1] loss: 692.103
[88,     1] loss: 779.577
[89,     1] loss: 655.765
[90,     1] loss: 693.125
[91,     1] loss: 588.023
[92,     1] loss: 640.018
[93,     1] loss: 525.613
[94,     1] loss: 756.754
[95,     1] loss: 1393.407
[96,     1] loss: 837.247
[97,     1] loss: 911.796
[98,     1] loss: 1067.705
[99,     1] loss: 916.715
[100,     1] loss: 875.088
[101,     1] loss: 854.840
[102,     1] loss: 805.043
[103,     1] loss: 899.735
[104,     1] loss: 801.873
[105,     1] loss: 786.321
[106,     1] loss: 803.034
[107,     1] loss: 743.133
[108,     1] loss: 756.081
[109,     1] loss: 725.370
[110,     1] loss: 639.061
[111,     1] loss: 675.653
[112,     1] loss: 704.609
[113,     1] loss: 1087.670
[114,     1] loss: 693.911
[115,     1] loss: 740.658
[116,     1] loss: 783.785
[117,     1] loss: 816.586
[118,     1] loss: 664.860
[119,     1] loss: 898.692
[120,     1] loss: 1124.930
[121,     1] loss: 930.763
[122,     1] loss: 761.168
[123,     1] loss: 978.901
[124,     1] loss: 812.668
[125,     1] loss: 839.596
[126,     1] loss: 811.732
[127,     1] loss: 753.860
[128,     1] loss: 735.786
[129,     1] loss: 690.634
[130,     1] loss: 704.373
[131,     1] loss: 664.229
Early stopping applied (best metric=0.36062091588974)
Finished Training
Total time taken: 18.15548539161682
{'Hydroxylation-K Validation Accuracy': 0.7338652482269503, 'Hydroxylation-K Validation Sensitivity': 0.6681481481481482, 'Hydroxylation-K Validation Specificity': 0.7508771929824561, 'Hydroxylation-K Validation Precision': 0.4127431827431827, 'Hydroxylation-K AUC ROC': 0.7769590643274854, 'Hydroxylation-K AUC PR': 0.5911727408996864, 'Hydroxylation-K MCC': 0.3627008930885765, 'Hydroxylation-K F1': 0.5020797134346998, 'Validation Loss (Hydroxylation-K)': 0.47963427702585854, 'Hydroxylation-P Validation Accuracy': 0.7761955738287396, 'Hydroxylation-P Validation Sensitivity': 0.8085714285714286, 'Hydroxylation-P Validation Specificity': 0.7692603122350242, 'Hydroxylation-P Validation Precision': 0.4385200275645716, 'Hydroxylation-P AUC ROC': 0.8660564335631671, 'Hydroxylation-P AUC PR': 0.6115627751311091, 'Hydroxylation-P MCC': 0.4729645608190159, 'Hydroxylation-P F1': 0.5656145556498002, 'Validation Loss (Hydroxylation-P)': 0.36001484592755634, 'Validation Loss (total)': 0.8396491169929504, 'TimeToTrain': 18.555210955937703}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031067885756241548,
 'learning_rate_Hydroxylation-K': 0.005316465209263671,
 'learning_rate_Hydroxylation-P': 0.007000112702876377,
 'log_base': 1.7466915420106819,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1482545067,
 'sample_weights': [1.580326253805306, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.8437119703401965,
 'weight_decay_Hydroxylation-K': 4.837833543055707,
 'weight_decay_Hydroxylation-P': 5.0649573379187185}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1542.293
[2,     1] loss: 1538.964
[3,     1] loss: 1541.008
[4,     1] loss: 1537.505
[5,     1] loss: 1536.279
[6,     1] loss: 1540.561
[7,     1] loss: 1535.292
[8,     1] loss: 1538.857
[9,     1] loss: 1533.388
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009566018164593699,
 'learning_rate_Hydroxylation-K': 0.005969489897594251,
 'learning_rate_Hydroxylation-P': 0.009044695882697568,
 'log_base': 1.5211612437117712,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1829045947,
 'sample_weights': [2.9933171070961975, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4002916446343723,
 'weight_decay_Hydroxylation-K': 5.5026117146696,
 'weight_decay_Hydroxylation-P': 5.7667404038410375}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1760.250
[2,     1] loss: 1753.762
[3,     1] loss: 1748.158
[4,     1] loss: 1750.273
[5,     1] loss: 1747.598
[6,     1] loss: 1752.131
[7,     1] loss: 1751.324
[8,     1] loss: 1747.194
[9,     1] loss: 1746.214
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009106176176449047,
 'learning_rate_Hydroxylation-K': 0.008283197657167634,
 'learning_rate_Hydroxylation-P': 0.0012257365377800963,
 'log_base': 1.2297230787423794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2792919815,
 'sample_weights': [3.979848740565806, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.598779881914016,
 'weight_decay_Hydroxylation-K': 3.3411555681063754,
 'weight_decay_Hydroxylation-P': 4.148006934536652}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2616.941
[2,     1] loss: 2632.507
[3,     1] loss: 2635.765
[4,     1] loss: 2631.865
[5,     1] loss: 2618.006
[6,     1] loss: 2615.618
[7,     1] loss: 2622.169
[8,     1] loss: 2612.344
[9,     1] loss: 2612.650
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007531311496774147,
 'learning_rate_Hydroxylation-K': 0.006722328118194345,
 'learning_rate_Hydroxylation-P': 0.004092627941494322,
 'log_base': 2.9529256685379894,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2662998858,
 'sample_weights': [8.07317172890951, 1.0091853424879214],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9043519272762355,
 'weight_decay_Hydroxylation-K': 1.309445978541366,
 'weight_decay_Hydroxylation-P': 7.359661048617481}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.814
[2,     1] loss: 1242.412
[3,     1] loss: 1236.559
[4,     1] loss: 1236.974
[5,     1] loss: 1229.761
[6,     1] loss: 1230.207
[7,     1] loss: 1236.574
[8,     1] loss: 1232.496
[9,     1] loss: 1236.860
[10,     1] loss: 1232.810
[11,     1] loss: 1232.172
[12,     1] loss: 1233.481
[13,     1] loss: 1234.304
[14,     1] loss: 1233.470
[15,     1] loss: 1235.326
[16,     1] loss: 1231.679
[17,     1] loss: 1231.892
[18,     1] loss: 1232.670
[19,     1] loss: 1233.664
[20,     1] loss: 1234.859
[21,     1] loss: 1234.165
[22,     1] loss: 1232.752
[23,     1] loss: 1231.901
[24,     1] loss: 1234.674
[25,     1] loss: 1231.918
[26,     1] loss: 1233.126
[27,     1] loss: 1232.402
[28,     1] loss: 1232.327
[29,     1] loss: 1232.125
[30,     1] loss: 1232.640
[31,     1] loss: 1232.486
[32,     1] loss: 1233.609
[33,     1] loss: 1231.822
[34,     1] loss: 1231.451
[35,     1] loss: 1230.691
[36,     1] loss: 1231.975
[37,     1] loss: 1233.013
[38,     1] loss: 1230.627
[39,     1] loss: 1229.305
[40,     1] loss: 1229.360
[41,     1] loss: 1230.006
[42,     1] loss: 1223.470
[43,     1] loss: 1216.635
[44,     1] loss: 1212.636
[45,     1] loss: 1198.619
[46,     1] loss: 1179.574
[47,     1] loss: 1177.181
[48,     1] loss: 1141.466
[49,     1] loss: 1124.514
[50,     1] loss: 1092.014
[51,     1] loss: 1087.486
[52,     1] loss: 1039.748
[53,     1] loss: 1030.809
[54,     1] loss: 1124.242
[55,     1] loss: 1009.443
[56,     1] loss: 1046.275
[57,     1] loss: 993.964
[58,     1] loss: 1028.973
[59,     1] loss: 967.089
[60,     1] loss: 977.552
[61,     1] loss: 968.442
[62,     1] loss: 928.888
[63,     1] loss: 964.253
[64,     1] loss: 956.639
[65,     1] loss: 956.832
[66,     1] loss: 940.157
[67,     1] loss: 946.193
[68,     1] loss: 883.934
[69,     1] loss: 910.965
[70,     1] loss: 932.268
[71,     1] loss: 912.092
[72,     1] loss: 899.318
[73,     1] loss: 868.391
[74,     1] loss: 907.390
[75,     1] loss: 897.951
[76,     1] loss: 870.248
[77,     1] loss: 846.725
[78,     1] loss: 904.007
[79,     1] loss: 946.431
[80,     1] loss: 840.182
[81,     1] loss: 886.654
[82,     1] loss: 870.675
[83,     1] loss: 887.135
[84,     1] loss: 853.219
[85,     1] loss: 912.579
[86,     1] loss: 827.872
[87,     1] loss: 817.447
[88,     1] loss: 794.483
[89,     1] loss: 802.678
[90,     1] loss: 782.001
[91,     1] loss: 790.081
[92,     1] loss: 782.116
[93,     1] loss: 778.817
[94,     1] loss: 756.382
[95,     1] loss: 845.593
[96,     1] loss: 729.566
[97,     1] loss: 726.434
[98,     1] loss: 701.303
[99,     1] loss: 724.089
[100,     1] loss: 719.760
[101,     1] loss: 782.711
[102,     1] loss: 653.722
[103,     1] loss: 714.150
[104,     1] loss: 750.552
[105,     1] loss: 683.663
[106,     1] loss: 745.175
[107,     1] loss: 659.017
[108,     1] loss: 695.279
[109,     1] loss: 754.337
[110,     1] loss: 603.809
[111,     1] loss: 708.380
[112,     1] loss: 610.293
[113,     1] loss: 595.007
[114,     1] loss: 615.805
[115,     1] loss: 568.805
[116,     1] loss: 557.544
[117,     1] loss: 548.623
[118,     1] loss: 522.121
[119,     1] loss: 540.077
[120,     1] loss: 529.453
[121,     1] loss: 554.720
[122,     1] loss: 507.420
[123,     1] loss: 481.345
[124,     1] loss: 505.703
[125,     1] loss: 510.203
[126,     1] loss: 572.785
[127,     1] loss: 701.744
[128,     1] loss: 468.012
[129,     1] loss: 614.970
[130,     1] loss: 567.140
[131,     1] loss: 611.466
[132,     1] loss: 509.561
[133,     1] loss: 565.976
[134,     1] loss: 514.866
[135,     1] loss: 563.055
[136,     1] loss: 465.761
[137,     1] loss: 487.449
[138,     1] loss: 518.221
[139,     1] loss: 443.219
[140,     1] loss: 453.841
[141,     1] loss: 419.352
[142,     1] loss: 500.114
[143,     1] loss: 429.651
[144,     1] loss: 448.260
[145,     1] loss: 442.907
[146,     1] loss: 432.343
[147,     1] loss: 442.225
[148,     1] loss: 433.987
[149,     1] loss: 384.177
[150,     1] loss: 410.175
[151,     1] loss: 375.041
[152,     1] loss: 435.836
[153,     1] loss: 384.840
[154,     1] loss: 379.241
[155,     1] loss: 463.418
[156,     1] loss: 409.704
[157,     1] loss: 442.920
[158,     1] loss: 478.738
[159,     1] loss: 340.375
Early stopping applied (best metric=0.31466829776763916)
Finished Training
Total time taken: 29.404228925704956
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.475
[2,     1] loss: 1253.465
[3,     1] loss: 1231.369
[4,     1] loss: 1239.485
[5,     1] loss: 1232.182
[6,     1] loss: 1236.086
[7,     1] loss: 1236.720
[8,     1] loss: 1233.959
[9,     1] loss: 1233.736
[10,     1] loss: 1232.686
[11,     1] loss: 1233.597
[12,     1] loss: 1231.341
[13,     1] loss: 1229.598
[14,     1] loss: 1236.151
[15,     1] loss: 1234.149
[16,     1] loss: 1229.929
[17,     1] loss: 1231.243
[18,     1] loss: 1232.760
[19,     1] loss: 1233.119
[20,     1] loss: 1231.410
[21,     1] loss: 1230.880
[22,     1] loss: 1230.355
[23,     1] loss: 1228.718
[24,     1] loss: 1234.109
[25,     1] loss: 1228.154
[26,     1] loss: 1225.481
[27,     1] loss: 1225.610
[28,     1] loss: 1222.651
[29,     1] loss: 1221.735
[30,     1] loss: 1219.370
[31,     1] loss: 1210.689
[32,     1] loss: 1202.082
[33,     1] loss: 1191.197
[34,     1] loss: 1192.506
[35,     1] loss: 1177.590
[36,     1] loss: 1148.392
[37,     1] loss: 1132.045
[38,     1] loss: 1119.035
[39,     1] loss: 1086.111
[40,     1] loss: 1080.860
[41,     1] loss: 1100.806
[42,     1] loss: 1051.524
[43,     1] loss: 986.262
[44,     1] loss: 1097.773
[45,     1] loss: 987.828
[46,     1] loss: 1087.021
[47,     1] loss: 1001.228
[48,     1] loss: 1003.354
[49,     1] loss: 1022.075
[50,     1] loss: 961.462
[51,     1] loss: 1011.100
[52,     1] loss: 990.787
[53,     1] loss: 925.063
[54,     1] loss: 969.312
[55,     1] loss: 921.464
[56,     1] loss: 917.095
[57,     1] loss: 899.286
[58,     1] loss: 888.285
[59,     1] loss: 869.810
[60,     1] loss: 867.419
[61,     1] loss: 897.971
[62,     1] loss: 857.999
[63,     1] loss: 854.972
[64,     1] loss: 862.543
[65,     1] loss: 813.063
[66,     1] loss: 826.752
[67,     1] loss: 785.021
[68,     1] loss: 805.918
[69,     1] loss: 826.526
[70,     1] loss: 770.833
[71,     1] loss: 799.669
[72,     1] loss: 774.066
[73,     1] loss: 834.999
[74,     1] loss: 707.687
[75,     1] loss: 832.327
[76,     1] loss: 697.709
[77,     1] loss: 863.722
[78,     1] loss: 746.400
[79,     1] loss: 799.788
[80,     1] loss: 709.031
[81,     1] loss: 815.607
[82,     1] loss: 703.884
[83,     1] loss: 755.467
[84,     1] loss: 684.635
[85,     1] loss: 731.899
[86,     1] loss: 651.207
[87,     1] loss: 760.093
[88,     1] loss: 658.957
[89,     1] loss: 771.119
[90,     1] loss: 653.495
[91,     1] loss: 678.309
[92,     1] loss: 653.799
[93,     1] loss: 670.948
[94,     1] loss: 564.166
[95,     1] loss: 641.250
[96,     1] loss: 612.974
[97,     1] loss: 627.655
[98,     1] loss: 661.759
[99,     1] loss: 542.682
[100,     1] loss: 647.205
[101,     1] loss: 704.203
[102,     1] loss: 643.201
[103,     1] loss: 600.424
[104,     1] loss: 582.615
[105,     1] loss: 614.770
[106,     1] loss: 555.600
Early stopping applied (best metric=0.32048267126083374)
Finished Training
Total time taken: 17.920443296432495
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.524
[2,     1] loss: 1241.000
[3,     1] loss: 1239.425
[4,     1] loss: 1233.054
[5,     1] loss: 1231.381
[6,     1] loss: 1231.414
[7,     1] loss: 1232.481
[8,     1] loss: 1231.838
[9,     1] loss: 1232.948
[10,     1] loss: 1232.155
[11,     1] loss: 1231.248
[12,     1] loss: 1229.488
[13,     1] loss: 1231.274
[14,     1] loss: 1224.268
[15,     1] loss: 1221.912
[16,     1] loss: 1220.328
[17,     1] loss: 1207.865
[18,     1] loss: 1184.804
[19,     1] loss: 1164.309
[20,     1] loss: 1126.504
[21,     1] loss: 1125.420
[22,     1] loss: 1088.271
[23,     1] loss: 1044.363
[24,     1] loss: 1040.981
[25,     1] loss: 997.834
[26,     1] loss: 1061.004
[27,     1] loss: 993.786
[28,     1] loss: 1003.313
[29,     1] loss: 1034.302
[30,     1] loss: 951.948
[31,     1] loss: 1004.665
[32,     1] loss: 985.909
[33,     1] loss: 978.468
[34,     1] loss: 930.684
[35,     1] loss: 992.010
[36,     1] loss: 940.320
[37,     1] loss: 954.891
[38,     1] loss: 915.732
[39,     1] loss: 893.006
[40,     1] loss: 949.661
[41,     1] loss: 900.334
[42,     1] loss: 909.099
[43,     1] loss: 944.791
[44,     1] loss: 853.800
[45,     1] loss: 879.650
[46,     1] loss: 828.126
[47,     1] loss: 904.563
[48,     1] loss: 851.584
[49,     1] loss: 835.834
[50,     1] loss: 796.628
[51,     1] loss: 838.954
[52,     1] loss: 786.879
[53,     1] loss: 759.189
[54,     1] loss: 767.217
[55,     1] loss: 764.819
[56,     1] loss: 748.246
[57,     1] loss: 719.987
[58,     1] loss: 751.532
[59,     1] loss: 765.118
[60,     1] loss: 722.953
[61,     1] loss: 660.525
[62,     1] loss: 733.820
[63,     1] loss: 659.359
[64,     1] loss: 671.688
[65,     1] loss: 701.985
[66,     1] loss: 627.193
[67,     1] loss: 751.755
[68,     1] loss: 1040.842
[69,     1] loss: 1000.166
[70,     1] loss: 770.579
[71,     1] loss: 729.027
[72,     1] loss: 927.101
[73,     1] loss: 802.669
[74,     1] loss: 764.443
[75,     1] loss: 812.026
[76,     1] loss: 793.399
[77,     1] loss: 704.055
[78,     1] loss: 662.718
[79,     1] loss: 770.968
[80,     1] loss: 677.619
[81,     1] loss: 663.005
[82,     1] loss: 725.948
[83,     1] loss: 626.520
[84,     1] loss: 691.307
[85,     1] loss: 592.688
Early stopping applied (best metric=0.3987654447555542)
Finished Training
Total time taken: 12.877762794494629
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.004
[2,     1] loss: 1234.374
[3,     1] loss: 1235.420
[4,     1] loss: 1231.666
[5,     1] loss: 1229.612
[6,     1] loss: 1235.762
[7,     1] loss: 1232.749
[8,     1] loss: 1229.163
[9,     1] loss: 1226.524
[10,     1] loss: 1223.443
[11,     1] loss: 1211.823
[12,     1] loss: 1198.468
[13,     1] loss: 1176.604
[14,     1] loss: 1143.871
[15,     1] loss: 1115.951
[16,     1] loss: 1061.562
[17,     1] loss: 1060.219
[18,     1] loss: 994.131
[19,     1] loss: 1042.296
[20,     1] loss: 1026.200
[21,     1] loss: 989.132
[22,     1] loss: 980.896
[23,     1] loss: 1020.669
[24,     1] loss: 983.962
[25,     1] loss: 986.620
[26,     1] loss: 1011.559
[27,     1] loss: 957.453
[28,     1] loss: 998.245
[29,     1] loss: 965.072
[30,     1] loss: 951.048
[31,     1] loss: 1000.236
[32,     1] loss: 946.645
[33,     1] loss: 940.558
[34,     1] loss: 952.349
[35,     1] loss: 916.835
[36,     1] loss: 953.473
[37,     1] loss: 906.974
[38,     1] loss: 920.482
[39,     1] loss: 979.468
[40,     1] loss: 890.701
[41,     1] loss: 889.376
[42,     1] loss: 916.928
[43,     1] loss: 908.051
[44,     1] loss: 900.494
[45,     1] loss: 876.581
[46,     1] loss: 825.277
[47,     1] loss: 845.013
[48,     1] loss: 855.197
[49,     1] loss: 849.792
[50,     1] loss: 848.243
[51,     1] loss: 823.129
[52,     1] loss: 803.294
[53,     1] loss: 827.528
[54,     1] loss: 795.719
[55,     1] loss: 742.522
[56,     1] loss: 803.185
[57,     1] loss: 725.361
[58,     1] loss: 775.430
[59,     1] loss: 804.106
[60,     1] loss: 775.423
[61,     1] loss: 768.198
[62,     1] loss: 782.119
[63,     1] loss: 766.022
[64,     1] loss: 777.944
[65,     1] loss: 746.876
[66,     1] loss: 703.650
[67,     1] loss: 680.881
[68,     1] loss: 737.709
[69,     1] loss: 731.897
[70,     1] loss: 672.306
[71,     1] loss: 735.241
[72,     1] loss: 693.268
[73,     1] loss: 674.812
[74,     1] loss: 627.896
[75,     1] loss: 669.176
[76,     1] loss: 648.699
[77,     1] loss: 645.026
[78,     1] loss: 600.434
[79,     1] loss: 605.032
[80,     1] loss: 621.334
[81,     1] loss: 628.535
[82,     1] loss: 559.405
[83,     1] loss: 562.281
[84,     1] loss: 546.713
[85,     1] loss: 548.448
[86,     1] loss: 564.145
[87,     1] loss: 583.675
[88,     1] loss: 622.738
[89,     1] loss: 687.392
[90,     1] loss: 502.898
[91,     1] loss: 619.410
[92,     1] loss: 599.730
[93,     1] loss: 558.204
[94,     1] loss: 616.340
[95,     1] loss: 516.161
[96,     1] loss: 521.385
[97,     1] loss: 524.880
[98,     1] loss: 470.951
[99,     1] loss: 542.585
[100,     1] loss: 496.028
[101,     1] loss: 488.820
[102,     1] loss: 472.209
Early stopping applied (best metric=0.40065446496009827)
Finished Training
Total time taken: 18.2204110622406
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1239.571
[2,     1] loss: 1237.307
[3,     1] loss: 1235.640
[4,     1] loss: 1234.916
[5,     1] loss: 1234.255
[6,     1] loss: 1232.874
[7,     1] loss: 1230.472
[8,     1] loss: 1233.482
[9,     1] loss: 1231.384
[10,     1] loss: 1230.062
[11,     1] loss: 1232.953
[12,     1] loss: 1225.451
[13,     1] loss: 1217.825
[14,     1] loss: 1206.068
[15,     1] loss: 1187.755
[16,     1] loss: 1169.360
[17,     1] loss: 1145.702
[18,     1] loss: 1089.785
[19,     1] loss: 1064.042
[20,     1] loss: 1029.535
[21,     1] loss: 1009.006
[22,     1] loss: 1030.599
[23,     1] loss: 983.206
[24,     1] loss: 975.486
[25,     1] loss: 1003.966
[26,     1] loss: 987.653
[27,     1] loss: 989.705
[28,     1] loss: 984.990
[29,     1] loss: 975.984
[30,     1] loss: 955.736
[31,     1] loss: 993.332
[32,     1] loss: 926.216
[33,     1] loss: 973.568
[34,     1] loss: 923.651
[35,     1] loss: 907.314
[36,     1] loss: 894.672
[37,     1] loss: 913.753
[38,     1] loss: 885.719
[39,     1] loss: 898.415
[40,     1] loss: 910.353
[41,     1] loss: 890.969
[42,     1] loss: 879.479
[43,     1] loss: 908.286
[44,     1] loss: 860.145
[45,     1] loss: 866.390
[46,     1] loss: 849.071
[47,     1] loss: 844.866
[48,     1] loss: 872.083
[49,     1] loss: 853.094
[50,     1] loss: 761.494
[51,     1] loss: 762.119
[52,     1] loss: 861.391
[53,     1] loss: 818.856
[54,     1] loss: 766.132
[55,     1] loss: 804.387
[56,     1] loss: 803.090
[57,     1] loss: 784.453
[58,     1] loss: 812.561
[59,     1] loss: 745.727
[60,     1] loss: 784.800
[61,     1] loss: 728.455
[62,     1] loss: 685.083
[63,     1] loss: 713.853
[64,     1] loss: 706.441
[65,     1] loss: 722.666
[66,     1] loss: 699.423
[67,     1] loss: 705.390
[68,     1] loss: 743.028
[69,     1] loss: 620.900
[70,     1] loss: 783.175
[71,     1] loss: 636.827
[72,     1] loss: 667.329
[73,     1] loss: 697.157
[74,     1] loss: 629.807
[75,     1] loss: 622.548
[76,     1] loss: 654.929
[77,     1] loss: 610.534
[78,     1] loss: 594.728
[79,     1] loss: 715.698
[80,     1] loss: 577.821
[81,     1] loss: 631.440
[82,     1] loss: 550.990
[83,     1] loss: 519.625
[84,     1] loss: 667.542
[85,     1] loss: 730.104
[86,     1] loss: 544.749
[87,     1] loss: 663.783
[88,     1] loss: 638.067
[89,     1] loss: 582.215
[90,     1] loss: 574.464
[91,     1] loss: 507.649
[92,     1] loss: 551.964
[93,     1] loss: 507.152
[94,     1] loss: 568.405
[95,     1] loss: 516.050
[96,     1] loss: 483.066
[97,     1] loss: 526.289
[98,     1] loss: 464.317
[99,     1] loss: 511.373
[100,     1] loss: 450.926
[101,     1] loss: 415.474
[102,     1] loss: 405.036
[103,     1] loss: 451.135
[104,     1] loss: 429.882
[105,     1] loss: 414.804
[106,     1] loss: 451.648
[107,     1] loss: 531.655
[108,     1] loss: 610.811
[109,     1] loss: 450.636
[110,     1] loss: 449.568
[111,     1] loss: 515.888
[112,     1] loss: 448.043
[113,     1] loss: 472.443
[114,     1] loss: 425.323
[115,     1] loss: 470.146
[116,     1] loss: 402.994
[117,     1] loss: 411.313
[118,     1] loss: 430.776
[119,     1] loss: 385.975
[120,     1] loss: 354.464
[121,     1] loss: 375.734
[122,     1] loss: 405.490
[123,     1] loss: 380.029
[124,     1] loss: 345.830
[125,     1] loss: 391.343
[126,     1] loss: 357.185
[127,     1] loss: 356.991
[128,     1] loss: 354.714
[129,     1] loss: 362.282
[130,     1] loss: 313.264
[131,     1] loss: 396.279
[132,     1] loss: 333.829
[133,     1] loss: 337.045
[134,     1] loss: 343.038
[135,     1] loss: 408.320
[136,     1] loss: 364.238
[137,     1] loss: 409.388
Early stopping applied (best metric=0.3830207288265228)
Finished Training
Total time taken: 24.5200936794281
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.790
[2,     1] loss: 1246.540
[3,     1] loss: 1230.611
[4,     1] loss: 1238.719
[5,     1] loss: 1233.318
[6,     1] loss: 1233.203
[7,     1] loss: 1233.392
[8,     1] loss: 1230.681
[9,     1] loss: 1230.840
[10,     1] loss: 1237.547
[11,     1] loss: 1225.901
[12,     1] loss: 1225.828
[13,     1] loss: 1220.017
[14,     1] loss: 1221.030
[15,     1] loss: 1211.954
[16,     1] loss: 1198.716
[17,     1] loss: 1179.755
[18,     1] loss: 1161.393
[19,     1] loss: 1108.300
[20,     1] loss: 1103.790
[21,     1] loss: 1070.699
[22,     1] loss: 1043.020
[23,     1] loss: 1012.430
[24,     1] loss: 1033.735
[25,     1] loss: 1021.596
[26,     1] loss: 1028.644
[27,     1] loss: 1026.355
[28,     1] loss: 975.032
[29,     1] loss: 964.657
[30,     1] loss: 961.583
[31,     1] loss: 965.536
[32,     1] loss: 987.412
[33,     1] loss: 968.959
[34,     1] loss: 1000.226
[35,     1] loss: 977.589
[36,     1] loss: 953.189
[37,     1] loss: 901.078
[38,     1] loss: 914.417
[39,     1] loss: 911.352
[40,     1] loss: 876.004
[41,     1] loss: 916.213
[42,     1] loss: 844.632
[43,     1] loss: 866.586
[44,     1] loss: 846.324
[45,     1] loss: 890.055
[46,     1] loss: 878.571
[47,     1] loss: 821.736
[48,     1] loss: 841.422
[49,     1] loss: 810.091
[50,     1] loss: 793.741
[51,     1] loss: 790.454
[52,     1] loss: 816.776
[53,     1] loss: 752.957
[54,     1] loss: 756.032
[55,     1] loss: 772.858
[56,     1] loss: 685.200
[57,     1] loss: 682.655
[58,     1] loss: 788.505
[59,     1] loss: 832.578
[60,     1] loss: 780.817
[61,     1] loss: 764.360
[62,     1] loss: 841.247
[63,     1] loss: 757.369
[64,     1] loss: 775.942
[65,     1] loss: 696.905
[66,     1] loss: 741.651
[67,     1] loss: 692.549
[68,     1] loss: 710.809
[69,     1] loss: 743.846
[70,     1] loss: 712.214
[71,     1] loss: 642.419
[72,     1] loss: 631.479
[73,     1] loss: 619.759
[74,     1] loss: 627.214
[75,     1] loss: 676.981
[76,     1] loss: 612.752
[77,     1] loss: 624.596
[78,     1] loss: 728.127
[79,     1] loss: 629.456
[80,     1] loss: 603.699
[81,     1] loss: 643.450
[82,     1] loss: 586.112
[83,     1] loss: 618.762
[84,     1] loss: 539.992
[85,     1] loss: 558.880
Early stopping applied (best metric=0.3845665752887726)
Finished Training
Total time taken: 15.628879070281982
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.364
[2,     1] loss: 1236.464
[3,     1] loss: 1235.196
[4,     1] loss: 1232.849
[5,     1] loss: 1233.183
[6,     1] loss: 1232.461
[7,     1] loss: 1235.349
[8,     1] loss: 1230.891
[9,     1] loss: 1231.239
[10,     1] loss: 1231.964
[11,     1] loss: 1230.602
[12,     1] loss: 1231.263
[13,     1] loss: 1231.407
[14,     1] loss: 1232.415
[15,     1] loss: 1228.157
[16,     1] loss: 1228.676
[17,     1] loss: 1224.815
[18,     1] loss: 1221.853
[19,     1] loss: 1215.368
[20,     1] loss: 1204.247
[21,     1] loss: 1191.714
[22,     1] loss: 1167.497
[23,     1] loss: 1149.377
[24,     1] loss: 1099.534
[25,     1] loss: 1116.155
[26,     1] loss: 1043.221
[27,     1] loss: 1057.425
[28,     1] loss: 1083.498
[29,     1] loss: 1044.047
[30,     1] loss: 1033.382
[31,     1] loss: 1025.844
[32,     1] loss: 977.946
[33,     1] loss: 1021.536
[34,     1] loss: 992.013
[35,     1] loss: 956.732
[36,     1] loss: 971.187
[37,     1] loss: 959.436
[38,     1] loss: 972.792
[39,     1] loss: 960.196
[40,     1] loss: 1002.811
[41,     1] loss: 920.944
[42,     1] loss: 948.962
[43,     1] loss: 948.894
[44,     1] loss: 944.237
[45,     1] loss: 926.780
[46,     1] loss: 926.733
[47,     1] loss: 925.484
[48,     1] loss: 927.383
[49,     1] loss: 890.096
[50,     1] loss: 948.453
[51,     1] loss: 880.200
[52,     1] loss: 865.176
[53,     1] loss: 874.609
[54,     1] loss: 887.250
[55,     1] loss: 849.905
[56,     1] loss: 849.934
[57,     1] loss: 816.970
[58,     1] loss: 929.555
[59,     1] loss: 827.616
[60,     1] loss: 777.277
[61,     1] loss: 852.846
[62,     1] loss: 776.294
[63,     1] loss: 825.143
[64,     1] loss: 910.589
[65,     1] loss: 819.232
[66,     1] loss: 787.285
[67,     1] loss: 837.848
[68,     1] loss: 749.945
[69,     1] loss: 755.986
[70,     1] loss: 732.771
[71,     1] loss: 735.617
[72,     1] loss: 692.684
[73,     1] loss: 729.073
[74,     1] loss: 800.133
[75,     1] loss: 706.470
[76,     1] loss: 661.690
[77,     1] loss: 755.228
[78,     1] loss: 682.493
[79,     1] loss: 671.949
[80,     1] loss: 695.117
[81,     1] loss: 696.819
[82,     1] loss: 686.252
[83,     1] loss: 614.909
[84,     1] loss: 722.585
[85,     1] loss: 643.437
[86,     1] loss: 558.658
[87,     1] loss: 555.876
[88,     1] loss: 576.653
[89,     1] loss: 584.191
[90,     1] loss: 524.356
[91,     1] loss: 522.917
[92,     1] loss: 559.088
[93,     1] loss: 572.401
[94,     1] loss: 533.528
[95,     1] loss: 557.633
[96,     1] loss: 593.822
[97,     1] loss: 551.011
[98,     1] loss: 507.815
[99,     1] loss: 537.614
[100,     1] loss: 508.441
[101,     1] loss: 503.255
[102,     1] loss: 551.980
[103,     1] loss: 696.383
[104,     1] loss: 742.176
[105,     1] loss: 474.360
[106,     1] loss: 750.059
[107,     1] loss: 508.574
[108,     1] loss: 746.249
[109,     1] loss: 571.302
[110,     1] loss: 571.599
[111,     1] loss: 561.852
[112,     1] loss: 479.602
[113,     1] loss: 526.945
[114,     1] loss: 501.670
[115,     1] loss: 491.424
[116,     1] loss: 463.323
[117,     1] loss: 540.220
[118,     1] loss: 405.895
[119,     1] loss: 590.487
Early stopping applied (best metric=0.32558897137641907)
Finished Training
Total time taken: 19.899689197540283
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.300
[2,     1] loss: 1228.255
[3,     1] loss: 1240.963
[4,     1] loss: 1221.913
[5,     1] loss: 1225.100
[6,     1] loss: 1204.858
[7,     1] loss: 1197.518
[8,     1] loss: 1155.093
[9,     1] loss: 1096.929
[10,     1] loss: 1079.419
[11,     1] loss: 1025.695
[12,     1] loss: 1031.997
[13,     1] loss: 1009.268
[14,     1] loss: 1035.155
[15,     1] loss: 969.883
[16,     1] loss: 982.319
[17,     1] loss: 983.777
[18,     1] loss: 970.922
[19,     1] loss: 951.328
[20,     1] loss: 977.154
[21,     1] loss: 979.145
[22,     1] loss: 950.032
[23,     1] loss: 1006.061
[24,     1] loss: 936.414
[25,     1] loss: 932.857
[26,     1] loss: 919.152
[27,     1] loss: 919.476
[28,     1] loss: 934.233
[29,     1] loss: 884.972
[30,     1] loss: 879.652
[31,     1] loss: 888.993
[32,     1] loss: 851.477
[33,     1] loss: 866.140
[34,     1] loss: 890.655
[35,     1] loss: 858.489
[36,     1] loss: 826.163
[37,     1] loss: 847.002
[38,     1] loss: 848.877
[39,     1] loss: 810.552
[40,     1] loss: 812.692
[41,     1] loss: 826.105
[42,     1] loss: 812.843
[43,     1] loss: 845.083
[44,     1] loss: 809.631
[45,     1] loss: 798.628
[46,     1] loss: 784.178
[47,     1] loss: 786.097
[48,     1] loss: 773.817
[49,     1] loss: 754.028
[50,     1] loss: 756.880
[51,     1] loss: 771.939
[52,     1] loss: 621.258
[53,     1] loss: 662.411
[54,     1] loss: 673.637
[55,     1] loss: 679.858
[56,     1] loss: 683.989
[57,     1] loss: 673.739
[58,     1] loss: 661.528
[59,     1] loss: 604.855
[60,     1] loss: 615.423
[61,     1] loss: 651.567
[62,     1] loss: 683.547
[63,     1] loss: 655.688
[64,     1] loss: 674.953
[65,     1] loss: 679.006
[66,     1] loss: 642.118
[67,     1] loss: 637.989
[68,     1] loss: 634.169
[69,     1] loss: 678.243
[70,     1] loss: 553.593
[71,     1] loss: 605.178
[72,     1] loss: 618.871
[73,     1] loss: 575.031
[74,     1] loss: 603.606
[75,     1] loss: 531.713
[76,     1] loss: 571.264
[77,     1] loss: 556.928
[78,     1] loss: 548.961
[79,     1] loss: 579.853
[80,     1] loss: 510.989
[81,     1] loss: 518.685
[82,     1] loss: 486.534
[83,     1] loss: 446.119
[84,     1] loss: 480.551
[85,     1] loss: 608.811
[86,     1] loss: 728.143
[87,     1] loss: 479.137
[88,     1] loss: 600.026
[89,     1] loss: 565.718
[90,     1] loss: 589.383
[91,     1] loss: 529.178
[92,     1] loss: 578.954
[93,     1] loss: 503.734
[94,     1] loss: 575.062
[95,     1] loss: 493.169
[96,     1] loss: 498.291
Early stopping applied (best metric=0.4159524440765381)
Finished Training
Total time taken: 16.42978024482727
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.038
[2,     1] loss: 1236.406
[3,     1] loss: 1248.057
[4,     1] loss: 1236.038
[5,     1] loss: 1233.625
[6,     1] loss: 1234.415
[7,     1] loss: 1231.931
[8,     1] loss: 1230.804
[9,     1] loss: 1232.471
[10,     1] loss: 1232.506
[11,     1] loss: 1231.918
[12,     1] loss: 1231.745
[13,     1] loss: 1229.947
[14,     1] loss: 1229.285
[15,     1] loss: 1226.527
[16,     1] loss: 1230.822
[17,     1] loss: 1222.593
[18,     1] loss: 1219.652
[19,     1] loss: 1216.292
[20,     1] loss: 1202.813
[21,     1] loss: 1192.502
[22,     1] loss: 1167.355
[23,     1] loss: 1146.689
[24,     1] loss: 1112.883
[25,     1] loss: 1093.696
[26,     1] loss: 1042.722
[27,     1] loss: 1053.367
[28,     1] loss: 1082.578
[29,     1] loss: 1050.669
[30,     1] loss: 1123.460
[31,     1] loss: 1010.848
[32,     1] loss: 1040.881
[33,     1] loss: 998.719
[34,     1] loss: 1025.238
[35,     1] loss: 1018.787
[36,     1] loss: 1025.793
[37,     1] loss: 951.852
[38,     1] loss: 992.434
[39,     1] loss: 944.194
[40,     1] loss: 1019.068
[41,     1] loss: 934.036
[42,     1] loss: 977.058
[43,     1] loss: 926.806
[44,     1] loss: 918.567
[45,     1] loss: 920.879
[46,     1] loss: 914.834
[47,     1] loss: 914.351
[48,     1] loss: 930.210
[49,     1] loss: 884.835
[50,     1] loss: 871.828
[51,     1] loss: 892.059
[52,     1] loss: 917.940
[53,     1] loss: 858.616
[54,     1] loss: 854.396
[55,     1] loss: 891.873
[56,     1] loss: 812.612
[57,     1] loss: 861.352
[58,     1] loss: 845.301
[59,     1] loss: 819.975
[60,     1] loss: 877.524
[61,     1] loss: 804.769
[62,     1] loss: 800.383
[63,     1] loss: 800.930
[64,     1] loss: 783.821
[65,     1] loss: 780.959
[66,     1] loss: 760.252
[67,     1] loss: 822.736
[68,     1] loss: 819.357
[69,     1] loss: 750.736
[70,     1] loss: 823.724
[71,     1] loss: 747.462
[72,     1] loss: 761.484
[73,     1] loss: 749.823
[74,     1] loss: 742.360
[75,     1] loss: 754.449
[76,     1] loss: 728.599
[77,     1] loss: 662.500
[78,     1] loss: 720.436
[79,     1] loss: 781.543
[80,     1] loss: 711.518
[81,     1] loss: 716.719
[82,     1] loss: 743.475
[83,     1] loss: 707.736
[84,     1] loss: 649.987
[85,     1] loss: 651.871
[86,     1] loss: 593.724
[87,     1] loss: 584.984
[88,     1] loss: 545.855
[89,     1] loss: 591.119
[90,     1] loss: 621.857
[91,     1] loss: 629.837
[92,     1] loss: 733.938
[93,     1] loss: 665.806
[94,     1] loss: 589.293
[95,     1] loss: 623.144
[96,     1] loss: 626.056
[97,     1] loss: 572.555
[98,     1] loss: 657.336
[99,     1] loss: 593.909
[100,     1] loss: 682.241
[101,     1] loss: 586.725
[102,     1] loss: 592.090
[103,     1] loss: 585.551
[104,     1] loss: 524.689
[105,     1] loss: 661.951
[106,     1] loss: 485.620
[107,     1] loss: 758.637
[108,     1] loss: 566.143
[109,     1] loss: 648.405
[110,     1] loss: 505.744
[111,     1] loss: 607.861
[112,     1] loss: 462.838
[113,     1] loss: 578.255
[114,     1] loss: 519.027
[115,     1] loss: 501.830
[116,     1] loss: 540.935
[117,     1] loss: 464.873
[118,     1] loss: 500.455
[119,     1] loss: 435.950
[120,     1] loss: 457.946
[121,     1] loss: 452.055
[122,     1] loss: 431.720
[123,     1] loss: 488.746
[124,     1] loss: 392.980
[125,     1] loss: 379.410
[126,     1] loss: 357.702
[127,     1] loss: 418.164
[128,     1] loss: 440.203
[129,     1] loss: 386.791
[130,     1] loss: 458.113
[131,     1] loss: 447.141
[132,     1] loss: 352.861
[133,     1] loss: 427.935
[134,     1] loss: 372.464
[135,     1] loss: 361.056
[136,     1] loss: 390.900
[137,     1] loss: 356.057
[138,     1] loss: 378.745
[139,     1] loss: 336.746
[140,     1] loss: 401.105
[141,     1] loss: 468.359
[142,     1] loss: 340.225
[143,     1] loss: 450.881
[144,     1] loss: 370.241
[145,     1] loss: 404.728
[146,     1] loss: 416.764
[147,     1] loss: 349.022
Early stopping applied (best metric=0.3325311839580536)
Finished Training
Total time taken: 22.017834424972534
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.469
[2,     1] loss: 1259.961
[3,     1] loss: 1235.797
[4,     1] loss: 1233.551
[5,     1] loss: 1233.451
[6,     1] loss: 1234.547
[7,     1] loss: 1233.415
[8,     1] loss: 1231.916
[9,     1] loss: 1237.113
[10,     1] loss: 1234.458
[11,     1] loss: 1231.755
[12,     1] loss: 1235.102
[13,     1] loss: 1233.413
[14,     1] loss: 1234.856
[15,     1] loss: 1234.597
[16,     1] loss: 1233.839
[17,     1] loss: 1233.345
[18,     1] loss: 1231.779
[19,     1] loss: 1235.082
[20,     1] loss: 1233.190
[21,     1] loss: 1232.564
[22,     1] loss: 1230.770
[23,     1] loss: 1231.421
[24,     1] loss: 1230.133
[25,     1] loss: 1225.733
[26,     1] loss: 1224.323
[27,     1] loss: 1222.289
[28,     1] loss: 1212.784
[29,     1] loss: 1206.798
[30,     1] loss: 1190.138
[31,     1] loss: 1169.207
[32,     1] loss: 1151.796
[33,     1] loss: 1115.117
[34,     1] loss: 1104.209
[35,     1] loss: 1091.581
[36,     1] loss: 1094.644
[37,     1] loss: 1082.822
[38,     1] loss: 1090.165
[39,     1] loss: 1029.599
[40,     1] loss: 1017.021
[41,     1] loss: 998.533
[42,     1] loss: 1017.355
[43,     1] loss: 1008.573
[44,     1] loss: 1034.286
[45,     1] loss: 981.865
[46,     1] loss: 978.255
[47,     1] loss: 929.668
[48,     1] loss: 916.820
[49,     1] loss: 975.165
[50,     1] loss: 917.525
[51,     1] loss: 906.558
[52,     1] loss: 937.908
[53,     1] loss: 929.881
[54,     1] loss: 894.686
[55,     1] loss: 902.470
[56,     1] loss: 931.836
[57,     1] loss: 909.330
[58,     1] loss: 878.829
[59,     1] loss: 886.164
[60,     1] loss: 890.581
[61,     1] loss: 833.798
[62,     1] loss: 898.413
[63,     1] loss: 815.241
[64,     1] loss: 901.463
[65,     1] loss: 829.694
[66,     1] loss: 873.395
[67,     1] loss: 778.383
[68,     1] loss: 878.678
[69,     1] loss: 751.823
[70,     1] loss: 812.292
[71,     1] loss: 757.460
[72,     1] loss: 806.873
[73,     1] loss: 777.555
[74,     1] loss: 726.516
[75,     1] loss: 751.177
[76,     1] loss: 745.699
[77,     1] loss: 711.061
[78,     1] loss: 715.191
[79,     1] loss: 680.097
[80,     1] loss: 726.701
[81,     1] loss: 788.806
[82,     1] loss: 731.195
[83,     1] loss: 682.865
[84,     1] loss: 686.100
[85,     1] loss: 653.094
[86,     1] loss: 613.269
[87,     1] loss: 659.846
[88,     1] loss: 662.132
[89,     1] loss: 646.494
[90,     1] loss: 611.155
[91,     1] loss: 650.272
[92,     1] loss: 639.090
[93,     1] loss: 646.792
[94,     1] loss: 612.228
[95,     1] loss: 625.527
[96,     1] loss: 636.867
[97,     1] loss: 594.988
[98,     1] loss: 623.604
[99,     1] loss: 627.637
[100,     1] loss: 582.222
[101,     1] loss: 552.275
[102,     1] loss: 563.843
[103,     1] loss: 545.229
[104,     1] loss: 647.397
[105,     1] loss: 869.229
Early stopping applied (best metric=0.38948360085487366)
Finished Training
Total time taken: 14.007232427597046
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.888
[2,     1] loss: 1247.250
[3,     1] loss: 1234.072
[4,     1] loss: 1236.591
[5,     1] loss: 1232.547
[6,     1] loss: 1232.088
[7,     1] loss: 1232.700
[8,     1] loss: 1234.392
[9,     1] loss: 1234.497
[10,     1] loss: 1238.061
[11,     1] loss: 1232.836
[12,     1] loss: 1232.451
[13,     1] loss: 1230.194
[14,     1] loss: 1229.680
[15,     1] loss: 1234.356
[16,     1] loss: 1229.491
[17,     1] loss: 1226.973
[18,     1] loss: 1229.904
[19,     1] loss: 1225.571
[20,     1] loss: 1225.147
[21,     1] loss: 1221.277
[22,     1] loss: 1208.404
[23,     1] loss: 1205.396
[24,     1] loss: 1186.484
[25,     1] loss: 1160.286
[26,     1] loss: 1146.200
[27,     1] loss: 1121.339
[28,     1] loss: 1054.951
[29,     1] loss: 1105.001
[30,     1] loss: 1010.675
[31,     1] loss: 1025.553
[32,     1] loss: 997.089
[33,     1] loss: 1009.568
[34,     1] loss: 964.153
[35,     1] loss: 1014.326
[36,     1] loss: 964.436
[37,     1] loss: 958.701
[38,     1] loss: 961.721
[39,     1] loss: 959.951
[40,     1] loss: 964.532
[41,     1] loss: 946.683
[42,     1] loss: 922.702
[43,     1] loss: 937.145
[44,     1] loss: 935.464
[45,     1] loss: 957.370
[46,     1] loss: 897.555
[47,     1] loss: 869.429
[48,     1] loss: 879.206
[49,     1] loss: 861.802
[50,     1] loss: 910.852
[51,     1] loss: 844.715
[52,     1] loss: 853.136
[53,     1] loss: 863.482
[54,     1] loss: 798.107
[55,     1] loss: 859.649
[56,     1] loss: 818.373
[57,     1] loss: 830.293
[58,     1] loss: 834.119
[59,     1] loss: 827.033
[60,     1] loss: 742.072
[61,     1] loss: 803.468
[62,     1] loss: 809.655
[63,     1] loss: 799.281
[64,     1] loss: 826.523
[65,     1] loss: 791.846
[66,     1] loss: 769.273
[67,     1] loss: 780.719
[68,     1] loss: 771.527
[69,     1] loss: 756.508
[70,     1] loss: 745.157
[71,     1] loss: 723.620
[72,     1] loss: 705.459
[73,     1] loss: 776.673
[74,     1] loss: 683.647
[75,     1] loss: 671.698
[76,     1] loss: 730.157
[77,     1] loss: 723.631
[78,     1] loss: 715.304
[79,     1] loss: 677.850
[80,     1] loss: 643.249
[81,     1] loss: 642.067
[82,     1] loss: 660.528
[83,     1] loss: 714.939
[84,     1] loss: 659.989
[85,     1] loss: 599.255
[86,     1] loss: 672.442
[87,     1] loss: 710.169
[88,     1] loss: 629.862
[89,     1] loss: 702.879
[90,     1] loss: 687.574
[91,     1] loss: 622.718
[92,     1] loss: 660.901
[93,     1] loss: 647.198
[94,     1] loss: 695.106
[95,     1] loss: 645.155
[96,     1] loss: 628.621
[97,     1] loss: 561.794
[98,     1] loss: 565.969
[99,     1] loss: 580.438
[100,     1] loss: 586.094
[101,     1] loss: 563.421
[102,     1] loss: 584.352
[103,     1] loss: 486.383
[104,     1] loss: 524.677
[105,     1] loss: 473.316
[106,     1] loss: 518.569
[107,     1] loss: 496.236
[108,     1] loss: 505.719
Early stopping applied (best metric=0.43000051379203796)
Finished Training
Total time taken: 17.765332460403442
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.588
[2,     1] loss: 1230.680
[3,     1] loss: 1234.454
[4,     1] loss: 1234.797
[5,     1] loss: 1230.307
[6,     1] loss: 1228.422
[7,     1] loss: 1224.052
[8,     1] loss: 1218.295
[9,     1] loss: 1209.238
[10,     1] loss: 1190.368
[11,     1] loss: 1161.734
[12,     1] loss: 1139.429
[13,     1] loss: 1101.736
[14,     1] loss: 1044.953
[15,     1] loss: 1039.679
[16,     1] loss: 1023.475
[17,     1] loss: 1037.925
[18,     1] loss: 1024.291
[19,     1] loss: 1014.993
[20,     1] loss: 986.504
[21,     1] loss: 1020.573
[22,     1] loss: 1042.988
[23,     1] loss: 986.021
[24,     1] loss: 992.204
[25,     1] loss: 992.346
[26,     1] loss: 970.937
[27,     1] loss: 942.365
[28,     1] loss: 957.310
[29,     1] loss: 936.189
[30,     1] loss: 950.577
[31,     1] loss: 943.195
[32,     1] loss: 963.041
[33,     1] loss: 915.027
[34,     1] loss: 905.498
[35,     1] loss: 898.908
[36,     1] loss: 904.198
[37,     1] loss: 865.729
[38,     1] loss: 877.150
[39,     1] loss: 871.159
[40,     1] loss: 887.729
[41,     1] loss: 865.507
[42,     1] loss: 844.764
[43,     1] loss: 842.309
[44,     1] loss: 796.300
[45,     1] loss: 842.908
[46,     1] loss: 825.964
[47,     1] loss: 832.703
[48,     1] loss: 788.200
[49,     1] loss: 773.384
[50,     1] loss: 783.979
[51,     1] loss: 813.312
[52,     1] loss: 820.797
[53,     1] loss: 825.193
[54,     1] loss: 803.129
[55,     1] loss: 782.023
[56,     1] loss: 765.355
[57,     1] loss: 812.320
[58,     1] loss: 716.063
[59,     1] loss: 793.576
[60,     1] loss: 683.636
[61,     1] loss: 740.263
[62,     1] loss: 776.526
[63,     1] loss: 662.023
[64,     1] loss: 805.839
[65,     1] loss: 687.406
[66,     1] loss: 657.272
[67,     1] loss: 654.020
[68,     1] loss: 650.013
[69,     1] loss: 636.472
[70,     1] loss: 675.156
[71,     1] loss: 599.250
[72,     1] loss: 659.852
[73,     1] loss: 708.166
[74,     1] loss: 694.812
[75,     1] loss: 587.192
[76,     1] loss: 671.834
[77,     1] loss: 611.835
[78,     1] loss: 618.466
[79,     1] loss: 643.711
[80,     1] loss: 551.114
[81,     1] loss: 627.713
[82,     1] loss: 538.778
[83,     1] loss: 655.154
[84,     1] loss: 524.974
[85,     1] loss: 588.656
[86,     1] loss: 547.378
[87,     1] loss: 522.543
[88,     1] loss: 497.342
Early stopping applied (best metric=0.3992936313152313)
Finished Training
Total time taken: 14.744236707687378
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.539
[2,     1] loss: 1236.532
[3,     1] loss: 1231.682
[4,     1] loss: 1240.445
[5,     1] loss: 1233.729
[6,     1] loss: 1234.884
[7,     1] loss: 1237.200
[8,     1] loss: 1229.347
[9,     1] loss: 1232.515
[10,     1] loss: 1231.564
[11,     1] loss: 1230.498
[12,     1] loss: 1232.330
[13,     1] loss: 1232.846
[14,     1] loss: 1229.864
[15,     1] loss: 1227.884
[16,     1] loss: 1230.404
[17,     1] loss: 1223.694
[18,     1] loss: 1221.961
[19,     1] loss: 1215.672
[20,     1] loss: 1211.807
[21,     1] loss: 1189.744
[22,     1] loss: 1176.266
[23,     1] loss: 1154.162
[24,     1] loss: 1106.838
[25,     1] loss: 1099.018
[26,     1] loss: 1069.369
[27,     1] loss: 1068.870
[28,     1] loss: 1059.896
[29,     1] loss: 1044.835
[30,     1] loss: 997.159
[31,     1] loss: 1054.539
[32,     1] loss: 1001.256
[33,     1] loss: 977.884
[34,     1] loss: 1004.990
[35,     1] loss: 1007.444
[36,     1] loss: 991.497
[37,     1] loss: 997.361
[38,     1] loss: 974.153
[39,     1] loss: 972.554
[40,     1] loss: 957.992
[41,     1] loss: 975.196
[42,     1] loss: 980.486
[43,     1] loss: 960.067
[44,     1] loss: 933.114
[45,     1] loss: 917.430
[46,     1] loss: 956.293
[47,     1] loss: 992.330
[48,     1] loss: 911.027
[49,     1] loss: 951.054
[50,     1] loss: 914.448
[51,     1] loss: 934.878
[52,     1] loss: 857.632
[53,     1] loss: 884.451
[54,     1] loss: 886.650
[55,     1] loss: 818.832
[56,     1] loss: 859.492
[57,     1] loss: 846.335
[58,     1] loss: 846.923
[59,     1] loss: 847.898
[60,     1] loss: 829.868
[61,     1] loss: 820.825
[62,     1] loss: 842.464
[63,     1] loss: 870.207
[64,     1] loss: 837.247
[65,     1] loss: 813.158
[66,     1] loss: 849.980
[67,     1] loss: 789.424
[68,     1] loss: 797.747
[69,     1] loss: 774.137
[70,     1] loss: 743.441
[71,     1] loss: 747.014
[72,     1] loss: 702.731
[73,     1] loss: 820.263
[74,     1] loss: 1066.480
[75,     1] loss: 819.455
[76,     1] loss: 877.258
[77,     1] loss: 767.443
[78,     1] loss: 854.222
[79,     1] loss: 869.808
[80,     1] loss: 791.518
[81,     1] loss: 831.512
[82,     1] loss: 856.845
[83,     1] loss: 782.553
[84,     1] loss: 815.521
[85,     1] loss: 836.917
[86,     1] loss: 749.246
[87,     1] loss: 731.828
[88,     1] loss: 724.414
[89,     1] loss: 678.342
[90,     1] loss: 730.564
[91,     1] loss: 714.689
[92,     1] loss: 711.585
[93,     1] loss: 651.597
[94,     1] loss: 641.058
[95,     1] loss: 658.625
[96,     1] loss: 633.303
[97,     1] loss: 610.551
[98,     1] loss: 658.844
[99,     1] loss: 538.182
[100,     1] loss: 640.025
[101,     1] loss: 573.695
[102,     1] loss: 526.394
[103,     1] loss: 623.731
[104,     1] loss: 530.831
[105,     1] loss: 619.948
[106,     1] loss: 602.912
[107,     1] loss: 569.274
[108,     1] loss: 541.347
[109,     1] loss: 606.255
[110,     1] loss: 479.006
[111,     1] loss: 509.889
[112,     1] loss: 531.083
[113,     1] loss: 491.012
[114,     1] loss: 455.495
[115,     1] loss: 499.288
[116,     1] loss: 623.638
[117,     1] loss: 897.081
[118,     1] loss: 442.545
[119,     1] loss: 705.582
Early stopping applied (best metric=0.31087276339530945)
Finished Training
Total time taken: 19.578826427459717
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.333
[2,     1] loss: 1239.462
[3,     1] loss: 1233.590
[4,     1] loss: 1243.727
[5,     1] loss: 1240.481
[6,     1] loss: 1230.999
[7,     1] loss: 1235.177
[8,     1] loss: 1240.719
[9,     1] loss: 1234.788
[10,     1] loss: 1234.425
[11,     1] loss: 1233.505
[12,     1] loss: 1233.121
[13,     1] loss: 1231.895
[14,     1] loss: 1232.331
[15,     1] loss: 1231.430
[16,     1] loss: 1231.468
[17,     1] loss: 1230.817
[18,     1] loss: 1231.484
[19,     1] loss: 1228.730
[20,     1] loss: 1228.750
[21,     1] loss: 1228.797
[22,     1] loss: 1231.525
[23,     1] loss: 1225.266
[24,     1] loss: 1220.787
[25,     1] loss: 1222.762
[26,     1] loss: 1210.972
[27,     1] loss: 1214.791
[28,     1] loss: 1199.290
[29,     1] loss: 1191.294
[30,     1] loss: 1178.555
[31,     1] loss: 1146.901
[32,     1] loss: 1166.011
[33,     1] loss: 1133.349
[34,     1] loss: 1112.149
[35,     1] loss: 1093.676
[36,     1] loss: 1082.583
[37,     1] loss: 1080.523
[38,     1] loss: 1061.585
[39,     1] loss: 1059.243
[40,     1] loss: 1049.621
[41,     1] loss: 1020.565
[42,     1] loss: 1001.554
[43,     1] loss: 1034.289
[44,     1] loss: 1014.252
[45,     1] loss: 976.574
[46,     1] loss: 1007.479
[47,     1] loss: 981.740
[48,     1] loss: 1030.768
[49,     1] loss: 949.592
[50,     1] loss: 959.473
[51,     1] loss: 926.073
[52,     1] loss: 929.202
[53,     1] loss: 941.804
[54,     1] loss: 927.897
[55,     1] loss: 883.637
[56,     1] loss: 906.823
[57,     1] loss: 869.004
[58,     1] loss: 896.419
[59,     1] loss: 906.427
[60,     1] loss: 861.766
[61,     1] loss: 844.616
[62,     1] loss: 829.413
[63,     1] loss: 806.519
[64,     1] loss: 823.101
[65,     1] loss: 837.407
[66,     1] loss: 819.513
[67,     1] loss: 748.079
[68,     1] loss: 792.331
[69,     1] loss: 873.762
[70,     1] loss: 940.564
[71,     1] loss: 816.071
[72,     1] loss: 886.303
[73,     1] loss: 751.303
[74,     1] loss: 788.557
[75,     1] loss: 745.535
[76,     1] loss: 773.351
[77,     1] loss: 759.959
[78,     1] loss: 736.395
[79,     1] loss: 721.050
[80,     1] loss: 715.792
[81,     1] loss: 746.310
[82,     1] loss: 709.634
[83,     1] loss: 718.337
[84,     1] loss: 688.996
[85,     1] loss: 678.941
[86,     1] loss: 641.515
[87,     1] loss: 726.417
[88,     1] loss: 732.582
[89,     1] loss: 660.773
[90,     1] loss: 705.440
[91,     1] loss: 628.711
[92,     1] loss: 616.906
[93,     1] loss: 641.781
[94,     1] loss: 634.025
[95,     1] loss: 607.724
[96,     1] loss: 564.991
[97,     1] loss: 623.341
[98,     1] loss: 510.661
[99,     1] loss: 648.557
[100,     1] loss: 898.631
[101,     1] loss: 904.115
[102,     1] loss: 637.802
[103,     1] loss: 637.545
[104,     1] loss: 610.264
[105,     1] loss: 677.435
[106,     1] loss: 624.715
[107,     1] loss: 620.817
[108,     1] loss: 591.165
[109,     1] loss: 589.158
[110,     1] loss: 566.109
[111,     1] loss: 503.336
[112,     1] loss: 580.977
[113,     1] loss: 486.324
[114,     1] loss: 487.560
[115,     1] loss: 533.944
[116,     1] loss: 528.801
[117,     1] loss: 446.785
[118,     1] loss: 471.262
[119,     1] loss: 445.843
[120,     1] loss: 509.895
[121,     1] loss: 484.131
[122,     1] loss: 415.617
[123,     1] loss: 476.658
[124,     1] loss: 472.212
[125,     1] loss: 420.322
[126,     1] loss: 454.287
[127,     1] loss: 471.203
[128,     1] loss: 367.936
[129,     1] loss: 444.131
[130,     1] loss: 532.175
[131,     1] loss: 410.718
[132,     1] loss: 380.901
[133,     1] loss: 479.190
[134,     1] loss: 383.539
[135,     1] loss: 456.780
[136,     1] loss: 419.350
[137,     1] loss: 397.399
[138,     1] loss: 422.805
[139,     1] loss: 437.701
[140,     1] loss: 389.794
[141,     1] loss: 448.900
[142,     1] loss: 423.429
[143,     1] loss: 498.336
[144,     1] loss: 599.738
[145,     1] loss: 447.344
[146,     1] loss: 442.066
[147,     1] loss: 400.831
[148,     1] loss: 419.062
[149,     1] loss: 427.624
[150,     1] loss: 373.006
[151,     1] loss: 461.078
[152,     1] loss: 361.051
[153,     1] loss: 517.684
[154,     1] loss: 350.876
[155,     1] loss: 470.611
[156,     1] loss: 470.900
Early stopping applied (best metric=0.2922389805316925)
Finished Training
Total time taken: 21.925439834594727
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1238.029
[2,     1] loss: 1242.883
[3,     1] loss: 1233.731
[4,     1] loss: 1235.917
[5,     1] loss: 1233.112
[6,     1] loss: 1239.486
[7,     1] loss: 1235.951
[8,     1] loss: 1235.919
[9,     1] loss: 1235.066
[10,     1] loss: 1229.676
[11,     1] loss: 1232.025
[12,     1] loss: 1232.105
[13,     1] loss: 1225.058
[14,     1] loss: 1220.309
[15,     1] loss: 1216.068
[16,     1] loss: 1201.011
[17,     1] loss: 1177.956
[18,     1] loss: 1155.182
[19,     1] loss: 1131.005
[20,     1] loss: 1088.402
[21,     1] loss: 1057.959
[22,     1] loss: 1018.314
[23,     1] loss: 999.519
[24,     1] loss: 978.700
[25,     1] loss: 976.858
[26,     1] loss: 989.610
[27,     1] loss: 1005.492
[28,     1] loss: 953.810
[29,     1] loss: 953.751
[30,     1] loss: 949.925
[31,     1] loss: 953.352
[32,     1] loss: 900.866
[33,     1] loss: 933.859
[34,     1] loss: 924.271
[35,     1] loss: 943.189
[36,     1] loss: 941.859
[37,     1] loss: 872.720
[38,     1] loss: 902.339
[39,     1] loss: 912.632
[40,     1] loss: 934.801
[41,     1] loss: 859.047
[42,     1] loss: 865.139
[43,     1] loss: 830.020
[44,     1] loss: 838.838
[45,     1] loss: 813.660
[46,     1] loss: 848.552
[47,     1] loss: 806.762
[48,     1] loss: 839.778
[49,     1] loss: 857.904
[50,     1] loss: 780.060
[51,     1] loss: 810.760
[52,     1] loss: 766.313
[53,     1] loss: 831.793
[54,     1] loss: 810.851
[55,     1] loss: 854.017
[56,     1] loss: 785.122
[57,     1] loss: 787.412
[58,     1] loss: 709.249
[59,     1] loss: 785.573
[60,     1] loss: 742.178
[61,     1] loss: 772.098
[62,     1] loss: 733.084
[63,     1] loss: 744.820
[64,     1] loss: 719.418
[65,     1] loss: 699.350
[66,     1] loss: 728.701
[67,     1] loss: 688.029
[68,     1] loss: 725.621
[69,     1] loss: 686.409
[70,     1] loss: 638.487
[71,     1] loss: 642.977
[72,     1] loss: 610.986
[73,     1] loss: 683.552
[74,     1] loss: 702.513
[75,     1] loss: 772.252
[76,     1] loss: 620.936
[77,     1] loss: 702.393
[78,     1] loss: 625.245
[79,     1] loss: 619.764
[80,     1] loss: 609.299
[81,     1] loss: 585.124
[82,     1] loss: 567.147
[83,     1] loss: 592.908
[84,     1] loss: 546.700
[85,     1] loss: 527.781
[86,     1] loss: 554.604
[87,     1] loss: 551.529
[88,     1] loss: 522.972
[89,     1] loss: 570.473
[90,     1] loss: 533.488
[91,     1] loss: 512.171
[92,     1] loss: 504.784
[93,     1] loss: 473.644
[94,     1] loss: 518.671
Early stopping applied (best metric=0.44340789318084717)
Finished Training
Total time taken: 12.59826374053955
{'Hydroxylation-K Validation Accuracy': 0.7534574468085107, 'Hydroxylation-K Validation Sensitivity': 0.6696296296296296, 'Hydroxylation-K Validation Specificity': 0.775438596491228, 'Hydroxylation-K Validation Precision': 0.44480128803658214, 'Hydroxylation-K AUC ROC': 0.8013255360623782, 'Hydroxylation-K AUC PR': 0.5755172529136144, 'Hydroxylation-K MCC': 0.39217779094642774, 'Hydroxylation-K F1': 0.5264524242602046, 'Validation Loss (Hydroxylation-K)': 0.4721068859100342, 'Hydroxylation-P Validation Accuracy': 0.7993078016344348, 'Hydroxylation-P Validation Sensitivity': 0.7746031746031746, 'Hydroxylation-P Validation Specificity': 0.8046411292333783, 'Hydroxylation-P Validation Precision': 0.4653288752293637, 'Hydroxylation-P AUC ROC': 0.8455550567775616, 'Hydroxylation-P AUC PR': 0.5742076837461562, 'Hydroxylation-P MCC': 0.48620241959485233, 'Hydroxylation-P F1': 0.5793298583309536, 'Validation Loss (Hydroxylation-P)': 0.3694352110226949, 'Validation Loss (total)': 0.8415420969327291, 'TimeToTrain': 18.502563619613646}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009517473071176744,
 'learning_rate_Hydroxylation-K': 0.004666137212927801,
 'learning_rate_Hydroxylation-P': 0.005761818814313386,
 'log_base': 1.8049888710828998,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1583832672,
 'sample_weights': [1.5429322538757757, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6041101182501112,
 'weight_decay_Hydroxylation-K': 0.6841447136197678,
 'weight_decay_Hydroxylation-P': 0.8232825224002838}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.129
[2,     1] loss: 1518.511
[3,     1] loss: 1509.445
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00969868672477292,
 'learning_rate_Hydroxylation-K': 0.008163982979308733,
 'learning_rate_Hydroxylation-P': 0.0013426147042947603,
 'log_base': 1.289028452824031,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1472937592,
 'sample_weights': [2.8269081958740903, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.025419200272652,
 'weight_decay_Hydroxylation-K': 4.239179814736297,
 'weight_decay_Hydroxylation-P': 8.112384464035767}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2296.116
[2,     1] loss: 2302.728
[3,     1] loss: 2324.038
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006669062094419856,
 'learning_rate_Hydroxylation-K': 0.007767127639934602,
 'learning_rate_Hydroxylation-P': 0.00041862046262465357,
 'log_base': 1.531321682117701,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2633431991,
 'sample_weights': [6.575489605393016, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2659261813728273,
 'weight_decay_Hydroxylation-K': 7.270917799393643,
 'weight_decay_Hydroxylation-P': 1.8776561061769348}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1729.262
[2,     1] loss: 1738.332
[3,     1] loss: 1729.188
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007051357858191652,
 'learning_rate_Hydroxylation-K': 0.0089409445622779,
 'learning_rate_Hydroxylation-P': 0.0019593570990262214,
 'log_base': 2.64532456494525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 877089492,
 'sample_weights': [3.9176739931154096, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.658240605776218,
 'weight_decay_Hydroxylation-K': 6.7423479776211614,
 'weight_decay_Hydroxylation-P': 3.2528332960064654}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.052
[2,     1] loss: 1273.856
[3,     1] loss: 1270.454
[4,     1] loss: 1269.480
[5,     1] loss: 1269.052
[6,     1] loss: 1268.762
[7,     1] loss: 1269.618
[8,     1] loss: 1269.346
[9,     1] loss: 1264.056
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0059226822091437235,
 'learning_rate_Hydroxylation-K': 0.0027934359351967186,
 'learning_rate_Hydroxylation-P': 0.002919642786569956,
 'log_base': 2.9593307318818827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 217723053,
 'sample_weights': [1.7161326528431102, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.544137814661501,
 'weight_decay_Hydroxylation-K': 1.556787725286618,
 'weight_decay_Hydroxylation-P': 4.1050010031294}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.205
[2,     1] loss: 1232.427
[3,     1] loss: 1230.883
[4,     1] loss: 1228.963
[5,     1] loss: 1226.880
[6,     1] loss: 1224.992
[7,     1] loss: 1217.321
[8,     1] loss: 1209.063
[9,     1] loss: 1188.207
[10,     1] loss: 1154.739
[11,     1] loss: 1127.571
[12,     1] loss: 1076.463
[13,     1] loss: 1088.129
[14,     1] loss: 1024.818
[15,     1] loss: 1110.581
[16,     1] loss: 1016.141
[17,     1] loss: 1038.495
[18,     1] loss: 1057.421
[19,     1] loss: 998.428
[20,     1] loss: 1041.459
[21,     1] loss: 999.232
[22,     1] loss: 967.839
[23,     1] loss: 1023.761
[24,     1] loss: 974.173
[25,     1] loss: 959.095
[26,     1] loss: 989.997
[27,     1] loss: 1001.762
[28,     1] loss: 942.843
[29,     1] loss: 964.781
[30,     1] loss: 973.237
[31,     1] loss: 931.334
[32,     1] loss: 953.336
[33,     1] loss: 950.496
[34,     1] loss: 912.707
[35,     1] loss: 940.823
[36,     1] loss: 861.543
[37,     1] loss: 846.395
[38,     1] loss: 881.626
[39,     1] loss: 863.679
[40,     1] loss: 846.971
[41,     1] loss: 808.954
[42,     1] loss: 858.421
[43,     1] loss: 832.666
[44,     1] loss: 815.951
[45,     1] loss: 830.370
[46,     1] loss: 796.967
[47,     1] loss: 764.885
[48,     1] loss: 768.712
[49,     1] loss: 729.394
[50,     1] loss: 784.366
[51,     1] loss: 764.183
[52,     1] loss: 774.846
[53,     1] loss: 756.326
[54,     1] loss: 773.097
[55,     1] loss: 700.220
[56,     1] loss: 662.732
[57,     1] loss: 702.734
[58,     1] loss: 636.892
[59,     1] loss: 643.833
[60,     1] loss: 763.354
[61,     1] loss: 928.078
[62,     1] loss: 899.422
[63,     1] loss: 766.963
[64,     1] loss: 740.013
[65,     1] loss: 835.684
[66,     1] loss: 713.081
[67,     1] loss: 792.588
[68,     1] loss: 757.750
[69,     1] loss: 614.393
[70,     1] loss: 650.798
[71,     1] loss: 685.348
[72,     1] loss: 658.674
[73,     1] loss: 628.141
Early stopping applied (best metric=0.3755723834037781)
Finished Training
Total time taken: 12.183775424957275
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.387
[2,     1] loss: 1235.020
[3,     1] loss: 1233.906
[4,     1] loss: 1230.560
[5,     1] loss: 1228.392
[6,     1] loss: 1229.875
[7,     1] loss: 1228.121
[8,     1] loss: 1229.929
[9,     1] loss: 1229.022
[10,     1] loss: 1216.283
[11,     1] loss: 1210.949
[12,     1] loss: 1185.167
[13,     1] loss: 1172.850
[14,     1] loss: 1133.368
[15,     1] loss: 1097.455
[16,     1] loss: 1062.838
[17,     1] loss: 1059.579
[18,     1] loss: 1016.404
[19,     1] loss: 1060.725
[20,     1] loss: 1010.119
[21,     1] loss: 995.048
[22,     1] loss: 1003.377
[23,     1] loss: 966.230
[24,     1] loss: 944.017
[25,     1] loss: 995.055
[26,     1] loss: 926.536
[27,     1] loss: 911.196
[28,     1] loss: 934.553
[29,     1] loss: 921.209
[30,     1] loss: 923.226
[31,     1] loss: 956.428
[32,     1] loss: 950.593
[33,     1] loss: 921.186
[34,     1] loss: 916.359
[35,     1] loss: 892.903
[36,     1] loss: 913.158
[37,     1] loss: 837.356
[38,     1] loss: 855.752
[39,     1] loss: 871.699
[40,     1] loss: 873.447
[41,     1] loss: 826.067
[42,     1] loss: 845.407
[43,     1] loss: 846.341
[44,     1] loss: 828.667
[45,     1] loss: 817.594
[46,     1] loss: 802.838
[47,     1] loss: 829.472
[48,     1] loss: 826.073
[49,     1] loss: 788.083
[50,     1] loss: 829.687
[51,     1] loss: 811.230
[52,     1] loss: 820.472
[53,     1] loss: 787.911
[54,     1] loss: 796.993
[55,     1] loss: 751.128
[56,     1] loss: 737.085
[57,     1] loss: 797.917
[58,     1] loss: 708.283
[59,     1] loss: 774.494
[60,     1] loss: 735.762
[61,     1] loss: 710.347
[62,     1] loss: 708.825
[63,     1] loss: 693.336
[64,     1] loss: 740.921
[65,     1] loss: 732.647
[66,     1] loss: 664.896
[67,     1] loss: 653.410
[68,     1] loss: 622.561
[69,     1] loss: 675.823
[70,     1] loss: 672.067
[71,     1] loss: 697.119
[72,     1] loss: 696.403
[73,     1] loss: 647.827
[74,     1] loss: 648.725
[75,     1] loss: 596.931
[76,     1] loss: 608.276
[77,     1] loss: 677.310
[78,     1] loss: 614.482
[79,     1] loss: 593.606
[80,     1] loss: 490.133
[81,     1] loss: 636.684
[82,     1] loss: 777.647
[83,     1] loss: 578.196
[84,     1] loss: 594.834
[85,     1] loss: 643.178
[86,     1] loss: 586.622
[87,     1] loss: 610.723
[88,     1] loss: 539.143
[89,     1] loss: 621.691
[90,     1] loss: 553.217
[91,     1] loss: 574.086
[92,     1] loss: 481.618
[93,     1] loss: 583.001
[94,     1] loss: 578.302
[95,     1] loss: 490.856
[96,     1] loss: 516.712
[97,     1] loss: 458.242
[98,     1] loss: 508.481
[99,     1] loss: 473.607
[100,     1] loss: 479.824
[101,     1] loss: 500.900
[102,     1] loss: 426.334
[103,     1] loss: 517.547
[104,     1] loss: 539.099
[105,     1] loss: 415.088
[106,     1] loss: 446.478
[107,     1] loss: 444.675
[108,     1] loss: 394.519
[109,     1] loss: 445.896
[110,     1] loss: 431.033
[111,     1] loss: 418.418
[112,     1] loss: 400.736
[113,     1] loss: 384.341
[114,     1] loss: 359.070
[115,     1] loss: 450.615
[116,     1] loss: 396.699
[117,     1] loss: 372.509
[118,     1] loss: 342.312
[119,     1] loss: 425.373
[120,     1] loss: 394.037
[121,     1] loss: 324.131
[122,     1] loss: 412.336
[123,     1] loss: 465.522
[124,     1] loss: 364.374
[125,     1] loss: 462.200
[126,     1] loss: 421.843
[127,     1] loss: 385.073
[128,     1] loss: 549.256
[129,     1] loss: 418.933
[130,     1] loss: 369.498
[131,     1] loss: 376.126
[132,     1] loss: 346.971
[133,     1] loss: 368.474
[134,     1] loss: 306.410
[135,     1] loss: 412.758
[136,     1] loss: 399.235
[137,     1] loss: 305.718
[138,     1] loss: 367.494
Early stopping applied (best metric=0.36500465869903564)
Finished Training
Total time taken: 22.809447050094604
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.563
[2,     1] loss: 1240.019
[3,     1] loss: 1229.778
[4,     1] loss: 1233.352
[5,     1] loss: 1230.593
[6,     1] loss: 1231.831
[7,     1] loss: 1235.761
[8,     1] loss: 1231.954
[9,     1] loss: 1228.962
[10,     1] loss: 1230.503
[11,     1] loss: 1227.580
[12,     1] loss: 1228.619
[13,     1] loss: 1230.789
[14,     1] loss: 1222.353
[15,     1] loss: 1215.816
[16,     1] loss: 1205.136
[17,     1] loss: 1183.584
[18,     1] loss: 1167.595
[19,     1] loss: 1131.051
[20,     1] loss: 1105.301
[21,     1] loss: 1066.265
[22,     1] loss: 1081.701
[23,     1] loss: 1058.740
[24,     1] loss: 987.746
[25,     1] loss: 1048.246
[26,     1] loss: 995.975
[27,     1] loss: 1011.901
[28,     1] loss: 1003.045
[29,     1] loss: 1005.162
[30,     1] loss: 992.609
[31,     1] loss: 977.661
[32,     1] loss: 1005.875
[33,     1] loss: 972.189
[34,     1] loss: 956.758
[35,     1] loss: 950.051
[36,     1] loss: 896.791
[37,     1] loss: 913.006
[38,     1] loss: 960.566
[39,     1] loss: 963.698
[40,     1] loss: 920.590
[41,     1] loss: 922.347
[42,     1] loss: 866.337
[43,     1] loss: 909.185
[44,     1] loss: 871.292
[45,     1] loss: 859.165
[46,     1] loss: 872.202
[47,     1] loss: 868.046
[48,     1] loss: 861.558
[49,     1] loss: 896.347
[50,     1] loss: 846.530
[51,     1] loss: 850.518
[52,     1] loss: 855.428
[53,     1] loss: 825.079
[54,     1] loss: 834.603
[55,     1] loss: 842.885
[56,     1] loss: 878.721
[57,     1] loss: 786.248
[58,     1] loss: 814.311
[59,     1] loss: 809.191
[60,     1] loss: 792.076
[61,     1] loss: 790.061
[62,     1] loss: 770.805
[63,     1] loss: 805.224
[64,     1] loss: 718.277
[65,     1] loss: 773.583
[66,     1] loss: 728.897
[67,     1] loss: 795.955
[68,     1] loss: 721.124
[69,     1] loss: 742.070
[70,     1] loss: 686.065
[71,     1] loss: 690.643
[72,     1] loss: 652.067
[73,     1] loss: 654.569
[74,     1] loss: 701.223
[75,     1] loss: 674.282
[76,     1] loss: 642.865
[77,     1] loss: 676.163
[78,     1] loss: 664.651
[79,     1] loss: 624.928
[80,     1] loss: 649.675
[81,     1] loss: 649.747
[82,     1] loss: 583.731
[83,     1] loss: 671.675
[84,     1] loss: 668.171
[85,     1] loss: 568.514
[86,     1] loss: 746.801
[87,     1] loss: 616.895
[88,     1] loss: 562.102
[89,     1] loss: 562.676
[90,     1] loss: 556.926
[91,     1] loss: 606.514
[92,     1] loss: 533.400
[93,     1] loss: 571.005
[94,     1] loss: 536.141
[95,     1] loss: 480.323
[96,     1] loss: 475.058
[97,     1] loss: 465.420
[98,     1] loss: 481.723
[99,     1] loss: 518.821
[100,     1] loss: 464.870
[101,     1] loss: 386.307
[102,     1] loss: 458.280
[103,     1] loss: 598.665
[104,     1] loss: 587.655
[105,     1] loss: 551.936
[106,     1] loss: 425.323
[107,     1] loss: 517.374
[108,     1] loss: 470.632
[109,     1] loss: 507.307
[110,     1] loss: 398.567
[111,     1] loss: 409.926
[112,     1] loss: 401.070
[113,     1] loss: 380.159
[114,     1] loss: 399.925
[115,     1] loss: 388.501
[116,     1] loss: 403.499
[117,     1] loss: 438.469
[118,     1] loss: 444.739
[119,     1] loss: 471.789
[120,     1] loss: 382.211
[121,     1] loss: 441.531
[122,     1] loss: 496.444
[123,     1] loss: 364.336
[124,     1] loss: 500.245
[125,     1] loss: 392.400
[126,     1] loss: 401.996
[127,     1] loss: 370.252
[128,     1] loss: 372.894
[129,     1] loss: 374.995
[130,     1] loss: 325.470
[131,     1] loss: 351.133
[132,     1] loss: 386.185
[133,     1] loss: 334.610
[134,     1] loss: 324.391
[135,     1] loss: 336.921
[136,     1] loss: 385.154
[137,     1] loss: 373.345
[138,     1] loss: 331.824
[139,     1] loss: 313.694
[140,     1] loss: 299.265
[141,     1] loss: 344.097
[142,     1] loss: 424.276
[143,     1] loss: 325.338
[144,     1] loss: 292.735
Early stopping applied (best metric=0.30610862374305725)
Finished Training
Total time taken: 23.733399391174316
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.042
[2,     1] loss: 1232.291
[3,     1] loss: 1228.325
[4,     1] loss: 1232.729
[5,     1] loss: 1234.322
[6,     1] loss: 1227.997
[7,     1] loss: 1220.077
[8,     1] loss: 1206.583
[9,     1] loss: 1192.057
[10,     1] loss: 1156.590
[11,     1] loss: 1116.507
[12,     1] loss: 1069.224
[13,     1] loss: 1069.643
[14,     1] loss: 1038.372
[15,     1] loss: 1065.726
[16,     1] loss: 973.392
[17,     1] loss: 1015.618
[18,     1] loss: 991.641
[19,     1] loss: 997.625
[20,     1] loss: 969.065
[21,     1] loss: 933.451
[22,     1] loss: 967.618
[23,     1] loss: 966.971
[24,     1] loss: 996.833
[25,     1] loss: 912.819
[26,     1] loss: 921.015
[27,     1] loss: 942.821
[28,     1] loss: 918.441
[29,     1] loss: 908.110
[30,     1] loss: 901.820
[31,     1] loss: 900.914
[32,     1] loss: 909.771
[33,     1] loss: 885.799
[34,     1] loss: 902.281
[35,     1] loss: 880.975
[36,     1] loss: 863.449
[37,     1] loss: 840.688
[38,     1] loss: 848.062
[39,     1] loss: 806.323
[40,     1] loss: 797.274
[41,     1] loss: 773.369
[42,     1] loss: 824.862
[43,     1] loss: 876.624
[44,     1] loss: 805.866
[45,     1] loss: 813.370
[46,     1] loss: 755.324
[47,     1] loss: 804.511
[48,     1] loss: 760.195
[49,     1] loss: 746.325
[50,     1] loss: 722.950
[51,     1] loss: 706.582
[52,     1] loss: 716.912
[53,     1] loss: 727.123
[54,     1] loss: 761.962
[55,     1] loss: 810.259
[56,     1] loss: 885.589
[57,     1] loss: 708.887
[58,     1] loss: 811.494
[59,     1] loss: 697.697
[60,     1] loss: 695.818
[61,     1] loss: 652.888
[62,     1] loss: 730.193
[63,     1] loss: 619.203
[64,     1] loss: 768.214
[65,     1] loss: 581.551
[66,     1] loss: 814.131
[67,     1] loss: 648.523
[68,     1] loss: 681.246
[69,     1] loss: 609.237
[70,     1] loss: 617.822
[71,     1] loss: 621.652
[72,     1] loss: 545.783
[73,     1] loss: 577.963
[74,     1] loss: 535.042
[75,     1] loss: 571.685
[76,     1] loss: 543.971
[77,     1] loss: 541.854
[78,     1] loss: 581.474
[79,     1] loss: 505.359
[80,     1] loss: 510.315
[81,     1] loss: 646.763
[82,     1] loss: 618.246
[83,     1] loss: 467.638
[84,     1] loss: 490.650
[85,     1] loss: 466.127
Early stopping applied (best metric=0.4115007817745209)
Finished Training
Total time taken: 11.723317861557007
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1243.830
[2,     1] loss: 1237.580
[3,     1] loss: 1237.112
[4,     1] loss: 1238.303
[5,     1] loss: 1233.979
[6,     1] loss: 1232.637
[7,     1] loss: 1232.596
[8,     1] loss: 1234.829
[9,     1] loss: 1234.528
[10,     1] loss: 1226.476
[11,     1] loss: 1228.135
[12,     1] loss: 1225.794
[13,     1] loss: 1220.851
[14,     1] loss: 1208.251
[15,     1] loss: 1192.824
[16,     1] loss: 1172.056
[17,     1] loss: 1140.928
[18,     1] loss: 1101.217
[19,     1] loss: 1088.600
[20,     1] loss: 1047.736
[21,     1] loss: 1085.676
[22,     1] loss: 1017.707
[23,     1] loss: 1032.197
[24,     1] loss: 1001.506
[25,     1] loss: 997.779
[26,     1] loss: 965.082
[27,     1] loss: 968.226
[28,     1] loss: 927.199
[29,     1] loss: 982.008
[30,     1] loss: 934.958
[31,     1] loss: 907.250
[32,     1] loss: 945.161
[33,     1] loss: 890.098
[34,     1] loss: 911.638
[35,     1] loss: 894.148
[36,     1] loss: 891.808
[37,     1] loss: 872.048
[38,     1] loss: 849.569
[39,     1] loss: 879.181
[40,     1] loss: 877.648
[41,     1] loss: 845.732
[42,     1] loss: 819.980
[43,     1] loss: 828.895
[44,     1] loss: 853.966
[45,     1] loss: 836.491
[46,     1] loss: 784.569
[47,     1] loss: 830.923
[48,     1] loss: 769.669
[49,     1] loss: 777.484
[50,     1] loss: 775.492
[51,     1] loss: 787.087
[52,     1] loss: 826.202
[53,     1] loss: 791.456
[54,     1] loss: 752.754
[55,     1] loss: 757.014
[56,     1] loss: 808.030
[57,     1] loss: 765.235
[58,     1] loss: 747.279
[59,     1] loss: 811.260
[60,     1] loss: 782.302
[61,     1] loss: 743.381
[62,     1] loss: 681.578
[63,     1] loss: 693.882
[64,     1] loss: 668.312
[65,     1] loss: 667.224
[66,     1] loss: 646.095
[67,     1] loss: 635.349
[68,     1] loss: 665.629
[69,     1] loss: 602.468
[70,     1] loss: 619.153
[71,     1] loss: 519.400
[72,     1] loss: 581.812
[73,     1] loss: 641.524
[74,     1] loss: 577.264
[75,     1] loss: 537.608
[76,     1] loss: 532.671
[77,     1] loss: 533.245
[78,     1] loss: 586.013
[79,     1] loss: 589.669
[80,     1] loss: 661.932
[81,     1] loss: 476.365
[82,     1] loss: 569.015
[83,     1] loss: 558.266
[84,     1] loss: 508.377
[85,     1] loss: 589.301
[86,     1] loss: 514.775
[87,     1] loss: 511.285
[88,     1] loss: 455.197
[89,     1] loss: 485.173
[90,     1] loss: 470.075
[91,     1] loss: 456.556
[92,     1] loss: 414.225
Early stopping applied (best metric=0.4323880672454834)
Finished Training
Total time taken: 13.033961772918701
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.537
[2,     1] loss: 1232.171
[3,     1] loss: 1235.496
[4,     1] loss: 1231.167
[5,     1] loss: 1228.222
[6,     1] loss: 1215.469
[7,     1] loss: 1204.105
[8,     1] loss: 1178.828
[9,     1] loss: 1130.225
[10,     1] loss: 1094.849
[11,     1] loss: 1090.869
[12,     1] loss: 1116.624
[13,     1] loss: 1039.982
[14,     1] loss: 1045.611
[15,     1] loss: 1031.207
[16,     1] loss: 989.886
[17,     1] loss: 1006.417
[18,     1] loss: 1009.072
[19,     1] loss: 969.894
[20,     1] loss: 982.117
[21,     1] loss: 976.726
[22,     1] loss: 992.845
[23,     1] loss: 985.886
[24,     1] loss: 950.204
[25,     1] loss: 987.025
[26,     1] loss: 965.039
[27,     1] loss: 944.127
[28,     1] loss: 931.474
[29,     1] loss: 921.797
[30,     1] loss: 952.600
[31,     1] loss: 898.084
[32,     1] loss: 888.351
[33,     1] loss: 884.056
[34,     1] loss: 840.008
[35,     1] loss: 915.259
[36,     1] loss: 898.772
[37,     1] loss: 856.261
[38,     1] loss: 848.158
[39,     1] loss: 891.563
[40,     1] loss: 855.894
[41,     1] loss: 871.202
[42,     1] loss: 838.529
[43,     1] loss: 814.645
[44,     1] loss: 823.793
[45,     1] loss: 808.688
[46,     1] loss: 786.423
[47,     1] loss: 771.176
[48,     1] loss: 749.259
[49,     1] loss: 795.487
[50,     1] loss: 905.009
[51,     1] loss: 977.106
[52,     1] loss: 801.926
[53,     1] loss: 846.047
[54,     1] loss: 751.062
[55,     1] loss: 794.510
[56,     1] loss: 724.955
[57,     1] loss: 721.996
[58,     1] loss: 832.841
[59,     1] loss: 724.547
[60,     1] loss: 739.491
[61,     1] loss: 663.331
[62,     1] loss: 703.328
[63,     1] loss: 655.511
[64,     1] loss: 626.835
[65,     1] loss: 646.472
[66,     1] loss: 624.475
[67,     1] loss: 605.014
[68,     1] loss: 606.809
[69,     1] loss: 564.170
[70,     1] loss: 631.247
[71,     1] loss: 671.530
[72,     1] loss: 653.016
[73,     1] loss: 585.892
[74,     1] loss: 635.417
[75,     1] loss: 574.000
[76,     1] loss: 521.688
[77,     1] loss: 608.512
[78,     1] loss: 484.676
[79,     1] loss: 577.606
[80,     1] loss: 490.873
[81,     1] loss: 433.108
[82,     1] loss: 582.334
[83,     1] loss: 554.141
[84,     1] loss: 449.732
[85,     1] loss: 504.446
[86,     1] loss: 460.941
[87,     1] loss: 416.844
[88,     1] loss: 419.579
[89,     1] loss: 458.098
[90,     1] loss: 430.410
[91,     1] loss: 373.902
[92,     1] loss: 462.132
[93,     1] loss: 549.353
[94,     1] loss: 506.197
[95,     1] loss: 379.933
[96,     1] loss: 508.848
[97,     1] loss: 379.110
[98,     1] loss: 454.152
[99,     1] loss: 461.260
[100,     1] loss: 389.945
[101,     1] loss: 496.139
[102,     1] loss: 432.879
[103,     1] loss: 388.581
[104,     1] loss: 470.984
[105,     1] loss: 380.362
[106,     1] loss: 380.042
[107,     1] loss: 432.966
[108,     1] loss: 336.415
Early stopping applied (best metric=0.34374570846557617)
Finished Training
Total time taken: 17.75241255760193
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.713
[2,     1] loss: 1235.383
[3,     1] loss: 1233.040
[4,     1] loss: 1232.652
[5,     1] loss: 1229.647
[6,     1] loss: 1232.477
[7,     1] loss: 1230.592
[8,     1] loss: 1227.079
[9,     1] loss: 1229.497
[10,     1] loss: 1225.210
[11,     1] loss: 1216.315
[12,     1] loss: 1212.464
[13,     1] loss: 1204.945
[14,     1] loss: 1177.854
[15,     1] loss: 1199.521
[16,     1] loss: 1146.148
[17,     1] loss: 1113.234
[18,     1] loss: 1099.654
[19,     1] loss: 1093.742
[20,     1] loss: 1049.668
[21,     1] loss: 1044.820
[22,     1] loss: 1021.145
[23,     1] loss: 1013.357
[24,     1] loss: 1001.656
[25,     1] loss: 944.619
[26,     1] loss: 1011.535
[27,     1] loss: 980.450
[28,     1] loss: 970.069
[29,     1] loss: 955.244
[30,     1] loss: 964.898
[31,     1] loss: 921.369
[32,     1] loss: 989.928
[33,     1] loss: 959.316
[34,     1] loss: 916.444
[35,     1] loss: 972.853
[36,     1] loss: 921.468
[37,     1] loss: 908.845
[38,     1] loss: 915.190
[39,     1] loss: 967.773
[40,     1] loss: 871.154
[41,     1] loss: 893.108
[42,     1] loss: 847.691
[43,     1] loss: 895.612
[44,     1] loss: 943.216
[45,     1] loss: 807.221
[46,     1] loss: 873.182
[47,     1] loss: 819.163
[48,     1] loss: 906.626
[49,     1] loss: 784.672
[50,     1] loss: 819.238
[51,     1] loss: 718.512
[52,     1] loss: 832.921
[53,     1] loss: 712.628
[54,     1] loss: 774.391
[55,     1] loss: 797.099
[56,     1] loss: 726.805
[57,     1] loss: 752.152
[58,     1] loss: 713.481
[59,     1] loss: 686.626
[60,     1] loss: 700.359
[61,     1] loss: 676.202
[62,     1] loss: 681.970
[63,     1] loss: 621.172
[64,     1] loss: 646.333
[65,     1] loss: 655.966
[66,     1] loss: 621.319
[67,     1] loss: 598.515
[68,     1] loss: 639.886
[69,     1] loss: 639.141
[70,     1] loss: 699.316
[71,     1] loss: 892.735
[72,     1] loss: 550.013
[73,     1] loss: 782.712
[74,     1] loss: 609.899
[75,     1] loss: 781.172
[76,     1] loss: 619.307
[77,     1] loss: 719.682
[78,     1] loss: 567.962
[79,     1] loss: 597.647
[80,     1] loss: 593.887
[81,     1] loss: 569.987
[82,     1] loss: 533.616
[83,     1] loss: 552.967
[84,     1] loss: 535.464
[85,     1] loss: 552.943
[86,     1] loss: 517.040
[87,     1] loss: 479.569
[88,     1] loss: 483.379
[89,     1] loss: 473.425
[90,     1] loss: 476.438
[91,     1] loss: 457.277
[92,     1] loss: 512.633
[93,     1] loss: 460.750
[94,     1] loss: 497.604
[95,     1] loss: 480.033
[96,     1] loss: 486.946
[97,     1] loss: 463.009
[98,     1] loss: 400.243
[99,     1] loss: 499.116
[100,     1] loss: 481.664
Early stopping applied (best metric=0.36056455969810486)
Finished Training
Total time taken: 17.00989842414856
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.460
[2,     1] loss: 1234.705
[3,     1] loss: 1231.881
[4,     1] loss: 1233.112
[5,     1] loss: 1227.748
[6,     1] loss: 1232.073
[7,     1] loss: 1220.556
[8,     1] loss: 1208.421
[9,     1] loss: 1176.829
[10,     1] loss: 1160.554
[11,     1] loss: 1095.695
[12,     1] loss: 1055.836
[13,     1] loss: 1067.693
[14,     1] loss: 1038.618
[15,     1] loss: 1032.511
[16,     1] loss: 1003.380
[17,     1] loss: 976.042
[18,     1] loss: 1005.593
[19,     1] loss: 993.206
[20,     1] loss: 981.766
[21,     1] loss: 996.035
[22,     1] loss: 980.074
[23,     1] loss: 921.823
[24,     1] loss: 954.583
[25,     1] loss: 914.177
[26,     1] loss: 955.181
[27,     1] loss: 923.073
[28,     1] loss: 886.582
[29,     1] loss: 956.772
[30,     1] loss: 906.230
[31,     1] loss: 929.485
[32,     1] loss: 893.699
[33,     1] loss: 890.448
[34,     1] loss: 859.123
[35,     1] loss: 893.402
[36,     1] loss: 868.642
[37,     1] loss: 863.799
[38,     1] loss: 871.941
[39,     1] loss: 823.021
[40,     1] loss: 847.757
[41,     1] loss: 790.756
[42,     1] loss: 833.459
[43,     1] loss: 840.458
[44,     1] loss: 795.499
[45,     1] loss: 829.659
[46,     1] loss: 832.864
[47,     1] loss: 762.627
[48,     1] loss: 801.212
[49,     1] loss: 790.549
[50,     1] loss: 790.458
[51,     1] loss: 740.515
[52,     1] loss: 837.527
[53,     1] loss: 692.678
[54,     1] loss: 828.983
[55,     1] loss: 707.682
[56,     1] loss: 793.627
[57,     1] loss: 684.418
[58,     1] loss: 722.677
[59,     1] loss: 714.043
[60,     1] loss: 729.022
[61,     1] loss: 688.455
[62,     1] loss: 660.919
[63,     1] loss: 751.081
[64,     1] loss: 662.023
[65,     1] loss: 679.430
[66,     1] loss: 660.273
[67,     1] loss: 638.389
[68,     1] loss: 606.383
[69,     1] loss: 550.638
[70,     1] loss: 624.235
Early stopping applied (best metric=0.42400938272476196)
Finished Training
Total time taken: 11.841379404067993
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.636
[2,     1] loss: 1234.277
[3,     1] loss: 1229.625
[4,     1] loss: 1234.915
[5,     1] loss: 1224.910
[6,     1] loss: 1225.973
[7,     1] loss: 1225.510
[8,     1] loss: 1215.601
[9,     1] loss: 1205.035
[10,     1] loss: 1183.748
[11,     1] loss: 1153.162
[12,     1] loss: 1124.848
[13,     1] loss: 1067.725
[14,     1] loss: 1068.293
[15,     1] loss: 1062.700
[16,     1] loss: 1038.502
[17,     1] loss: 1026.941
[18,     1] loss: 1019.400
[19,     1] loss: 995.947
[20,     1] loss: 968.260
[21,     1] loss: 993.636
[22,     1] loss: 987.093
[23,     1] loss: 957.627
[24,     1] loss: 938.978
[25,     1] loss: 925.290
[26,     1] loss: 954.440
[27,     1] loss: 952.990
[28,     1] loss: 906.307
[29,     1] loss: 900.758
[30,     1] loss: 938.518
[31,     1] loss: 924.387
[32,     1] loss: 934.005
[33,     1] loss: 914.097
[34,     1] loss: 927.589
[35,     1] loss: 872.339
[36,     1] loss: 857.657
[37,     1] loss: 813.201
[38,     1] loss: 886.235
[39,     1] loss: 814.833
[40,     1] loss: 938.117
[41,     1] loss: 879.374
[42,     1] loss: 823.369
[43,     1] loss: 918.519
[44,     1] loss: 838.458
[45,     1] loss: 888.121
[46,     1] loss: 800.898
[47,     1] loss: 870.183
[48,     1] loss: 757.755
[49,     1] loss: 813.659
[50,     1] loss: 750.391
[51,     1] loss: 810.698
[52,     1] loss: 738.671
[53,     1] loss: 785.348
[54,     1] loss: 718.086
[55,     1] loss: 720.667
[56,     1] loss: 724.038
[57,     1] loss: 661.364
[58,     1] loss: 750.438
[59,     1] loss: 718.246
[60,     1] loss: 632.822
[61,     1] loss: 617.201
[62,     1] loss: 739.426
[63,     1] loss: 766.196
[64,     1] loss: 591.148
[65,     1] loss: 632.464
[66,     1] loss: 593.691
[67,     1] loss: 632.633
[68,     1] loss: 647.120
[69,     1] loss: 582.556
[70,     1] loss: 574.600
[71,     1] loss: 745.178
Early stopping applied (best metric=0.3634102940559387)
Finished Training
Total time taken: 9.640830278396606
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.390
[2,     1] loss: 1231.856
[3,     1] loss: 1232.103
[4,     1] loss: 1231.047
[5,     1] loss: 1228.545
[6,     1] loss: 1213.630
[7,     1] loss: 1191.839
[8,     1] loss: 1164.930
[9,     1] loss: 1126.703
[10,     1] loss: 1070.107
[11,     1] loss: 1046.034
[12,     1] loss: 1044.168
[13,     1] loss: 1076.213
[14,     1] loss: 1068.431
[15,     1] loss: 1034.324
[16,     1] loss: 1016.352
[17,     1] loss: 1039.635
[18,     1] loss: 1022.531
[19,     1] loss: 1000.141
[20,     1] loss: 1000.515
[21,     1] loss: 992.514
[22,     1] loss: 971.447
[23,     1] loss: 969.376
[24,     1] loss: 981.035
[25,     1] loss: 950.500
[26,     1] loss: 876.958
[27,     1] loss: 933.645
[28,     1] loss: 895.753
[29,     1] loss: 974.501
[30,     1] loss: 912.742
[31,     1] loss: 983.824
[32,     1] loss: 897.511
[33,     1] loss: 934.750
[34,     1] loss: 893.305
[35,     1] loss: 869.149
[36,     1] loss: 904.062
[37,     1] loss: 867.684
[38,     1] loss: 840.510
[39,     1] loss: 801.979
[40,     1] loss: 780.454
[41,     1] loss: 855.261
[42,     1] loss: 861.242
[43,     1] loss: 747.879
[44,     1] loss: 817.970
[45,     1] loss: 799.572
[46,     1] loss: 792.185
[47,     1] loss: 711.369
[48,     1] loss: 782.589
[49,     1] loss: 747.372
[50,     1] loss: 722.720
[51,     1] loss: 669.442
[52,     1] loss: 755.349
[53,     1] loss: 850.092
[54,     1] loss: 1392.909
[55,     1] loss: 920.636
[56,     1] loss: 1043.440
[57,     1] loss: 868.323
[58,     1] loss: 880.934
[59,     1] loss: 961.046
[60,     1] loss: 978.213
[61,     1] loss: 928.023
[62,     1] loss: 899.896
[63,     1] loss: 866.233
[64,     1] loss: 875.470
[65,     1] loss: 879.744
[66,     1] loss: 853.209
[67,     1] loss: 827.222
[68,     1] loss: 818.006
[69,     1] loss: 883.472
[70,     1] loss: 726.161
[71,     1] loss: 745.657
[72,     1] loss: 766.144
[73,     1] loss: 760.743
[74,     1] loss: 708.661
[75,     1] loss: 746.503
[76,     1] loss: 747.361
[77,     1] loss: 677.453
[78,     1] loss: 720.066
[79,     1] loss: 647.142
[80,     1] loss: 660.180
[81,     1] loss: 594.939
[82,     1] loss: 678.481
[83,     1] loss: 580.419
[84,     1] loss: 633.858
[85,     1] loss: 552.060
[86,     1] loss: 580.348
[87,     1] loss: 586.581
[88,     1] loss: 543.065
[89,     1] loss: 579.606
[90,     1] loss: 504.301
[91,     1] loss: 536.371
[92,     1] loss: 656.249
[93,     1] loss: 493.652
Early stopping applied (best metric=0.3424079716205597)
Finished Training
Total time taken: 12.669873237609863
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.283
[2,     1] loss: 1233.099
[3,     1] loss: 1237.046
[4,     1] loss: 1232.047
[5,     1] loss: 1232.121
[6,     1] loss: 1231.221
[7,     1] loss: 1236.966
[8,     1] loss: 1228.949
[9,     1] loss: 1227.747
[10,     1] loss: 1226.424
[11,     1] loss: 1224.947
[12,     1] loss: 1220.294
[13,     1] loss: 1208.605
[14,     1] loss: 1199.445
[15,     1] loss: 1178.868
[16,     1] loss: 1151.298
[17,     1] loss: 1118.934
[18,     1] loss: 1106.164
[19,     1] loss: 1059.492
[20,     1] loss: 1038.133
[21,     1] loss: 1063.748
[22,     1] loss: 1071.821
[23,     1] loss: 1037.109
[24,     1] loss: 1026.928
[25,     1] loss: 1010.635
[26,     1] loss: 1023.872
[27,     1] loss: 1017.161
[28,     1] loss: 969.361
[29,     1] loss: 987.415
[30,     1] loss: 968.190
[31,     1] loss: 978.018
[32,     1] loss: 905.619
[33,     1] loss: 936.886
[34,     1] loss: 946.891
[35,     1] loss: 945.877
[36,     1] loss: 902.029
[37,     1] loss: 919.264
[38,     1] loss: 917.700
[39,     1] loss: 895.644
[40,     1] loss: 886.385
[41,     1] loss: 874.853
[42,     1] loss: 904.473
[43,     1] loss: 840.264
[44,     1] loss: 890.331
[45,     1] loss: 827.482
[46,     1] loss: 861.728
[47,     1] loss: 866.128
[48,     1] loss: 815.383
[49,     1] loss: 842.551
[50,     1] loss: 892.016
[51,     1] loss: 830.343
[52,     1] loss: 786.051
[53,     1] loss: 796.525
[54,     1] loss: 765.636
[55,     1] loss: 757.861
[56,     1] loss: 775.911
[57,     1] loss: 730.392
[58,     1] loss: 762.695
[59,     1] loss: 745.948
[60,     1] loss: 705.115
[61,     1] loss: 711.011
[62,     1] loss: 712.876
[63,     1] loss: 879.161
[64,     1] loss: 1060.162
[65,     1] loss: 700.231
[66,     1] loss: 945.442
[67,     1] loss: 760.953
[68,     1] loss: 812.041
[69,     1] loss: 883.513
[70,     1] loss: 771.010
[71,     1] loss: 691.277
[72,     1] loss: 782.803
[73,     1] loss: 725.223
[74,     1] loss: 663.311
[75,     1] loss: 717.678
[76,     1] loss: 627.838
[77,     1] loss: 721.210
[78,     1] loss: 683.456
[79,     1] loss: 632.392
[80,     1] loss: 682.048
[81,     1] loss: 581.250
[82,     1] loss: 641.276
[83,     1] loss: 561.513
[84,     1] loss: 550.569
[85,     1] loss: 558.095
[86,     1] loss: 626.927
[87,     1] loss: 503.934
[88,     1] loss: 580.112
[89,     1] loss: 476.737
[90,     1] loss: 643.966
[91,     1] loss: 540.887
[92,     1] loss: 532.359
Early stopping applied (best metric=0.35040441155433655)
Finished Training
Total time taken: 15.546944856643677
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.512
[2,     1] loss: 1233.420
[3,     1] loss: 1235.380
[4,     1] loss: 1234.264
[5,     1] loss: 1232.524
[6,     1] loss: 1233.315
[7,     1] loss: 1230.816
[8,     1] loss: 1232.794
[9,     1] loss: 1233.047
[10,     1] loss: 1234.291
[11,     1] loss: 1229.988
[12,     1] loss: 1229.164
[13,     1] loss: 1225.679
[14,     1] loss: 1224.960
[15,     1] loss: 1221.372
[16,     1] loss: 1215.087
[17,     1] loss: 1205.109
[18,     1] loss: 1187.292
[19,     1] loss: 1162.602
[20,     1] loss: 1119.725
[21,     1] loss: 1082.114
[22,     1] loss: 1059.822
[23,     1] loss: 1059.804
[24,     1] loss: 1058.722
[25,     1] loss: 1031.781
[26,     1] loss: 989.728
[27,     1] loss: 981.661
[28,     1] loss: 1041.445
[29,     1] loss: 953.830
[30,     1] loss: 1003.109
[31,     1] loss: 968.252
[32,     1] loss: 964.595
[33,     1] loss: 993.209
[34,     1] loss: 920.840
[35,     1] loss: 962.927
[36,     1] loss: 966.117
[37,     1] loss: 934.961
[38,     1] loss: 930.023
[39,     1] loss: 957.158
[40,     1] loss: 939.834
[41,     1] loss: 949.115
[42,     1] loss: 887.189
[43,     1] loss: 925.569
[44,     1] loss: 853.947
[45,     1] loss: 948.687
[46,     1] loss: 907.372
[47,     1] loss: 892.064
[48,     1] loss: 896.237
[49,     1] loss: 877.496
[50,     1] loss: 867.176
[51,     1] loss: 831.460
[52,     1] loss: 837.466
[53,     1] loss: 821.165
[54,     1] loss: 796.861
[55,     1] loss: 835.143
[56,     1] loss: 828.409
[57,     1] loss: 872.149
[58,     1] loss: 795.050
[59,     1] loss: 821.627
[60,     1] loss: 764.905
[61,     1] loss: 784.008
[62,     1] loss: 714.934
[63,     1] loss: 762.447
[64,     1] loss: 714.292
[65,     1] loss: 749.943
[66,     1] loss: 733.859
[67,     1] loss: 642.896
[68,     1] loss: 761.872
[69,     1] loss: 682.568
[70,     1] loss: 629.958
[71,     1] loss: 720.012
[72,     1] loss: 664.711
[73,     1] loss: 655.940
[74,     1] loss: 700.545
[75,     1] loss: 600.000
[76,     1] loss: 679.108
[77,     1] loss: 627.965
[78,     1] loss: 563.867
[79,     1] loss: 626.727
[80,     1] loss: 692.919
[81,     1] loss: 731.269
[82,     1] loss: 581.211
[83,     1] loss: 577.344
[84,     1] loss: 585.993
[85,     1] loss: 500.666
[86,     1] loss: 545.295
[87,     1] loss: 526.422
[88,     1] loss: 570.929
[89,     1] loss: 507.736
[90,     1] loss: 498.368
[91,     1] loss: 526.017
[92,     1] loss: 525.101
[93,     1] loss: 539.237
[94,     1] loss: 501.432
[95,     1] loss: 478.328
[96,     1] loss: 485.602
[97,     1] loss: 468.727
[98,     1] loss: 499.862
[99,     1] loss: 492.561
[100,     1] loss: 503.778
[101,     1] loss: 733.070
[102,     1] loss: 502.648
[103,     1] loss: 472.051
[104,     1] loss: 472.795
[105,     1] loss: 409.340
[106,     1] loss: 438.483
[107,     1] loss: 444.430
[108,     1] loss: 444.977
[109,     1] loss: 461.168
[110,     1] loss: 380.601
[111,     1] loss: 442.831
Early stopping applied (best metric=0.3404957354068756)
Finished Training
Total time taken: 16.86802864074707
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.736
[2,     1] loss: 1238.981
[3,     1] loss: 1233.963
[4,     1] loss: 1231.984
[5,     1] loss: 1235.756
[6,     1] loss: 1233.149
[7,     1] loss: 1231.101
[8,     1] loss: 1231.048
[9,     1] loss: 1231.240
[10,     1] loss: 1228.835
[11,     1] loss: 1227.101
[12,     1] loss: 1226.899
[13,     1] loss: 1224.002
[14,     1] loss: 1222.018
[15,     1] loss: 1211.978
[16,     1] loss: 1205.052
[17,     1] loss: 1183.534
[18,     1] loss: 1160.182
[19,     1] loss: 1133.865
[20,     1] loss: 1124.943
[21,     1] loss: 1096.522
[22,     1] loss: 1051.237
[23,     1] loss: 1019.407
[24,     1] loss: 1022.140
[25,     1] loss: 1044.533
[26,     1] loss: 1033.740
[27,     1] loss: 1036.776
[28,     1] loss: 1014.164
[29,     1] loss: 1022.060
[30,     1] loss: 1026.181
[31,     1] loss: 1028.665
[32,     1] loss: 995.373
[33,     1] loss: 990.634
[34,     1] loss: 969.963
[35,     1] loss: 934.529
[36,     1] loss: 932.965
[37,     1] loss: 914.424
[38,     1] loss: 961.293
[39,     1] loss: 938.378
[40,     1] loss: 931.591
[41,     1] loss: 928.079
[42,     1] loss: 930.375
[43,     1] loss: 927.434
[44,     1] loss: 967.201
[45,     1] loss: 891.584
[46,     1] loss: 866.780
[47,     1] loss: 875.771
[48,     1] loss: 857.758
[49,     1] loss: 858.201
[50,     1] loss: 871.547
[51,     1] loss: 805.277
[52,     1] loss: 847.826
[53,     1] loss: 811.049
[54,     1] loss: 819.208
[55,     1] loss: 854.273
[56,     1] loss: 874.491
[57,     1] loss: 794.634
[58,     1] loss: 753.875
[59,     1] loss: 792.918
[60,     1] loss: 772.544
[61,     1] loss: 764.439
[62,     1] loss: 733.566
[63,     1] loss: 804.631
[64,     1] loss: 769.076
[65,     1] loss: 698.581
[66,     1] loss: 693.023
[67,     1] loss: 690.329
[68,     1] loss: 697.667
[69,     1] loss: 704.283
[70,     1] loss: 688.843
[71,     1] loss: 696.794
[72,     1] loss: 742.514
[73,     1] loss: 847.023
[74,     1] loss: 711.853
[75,     1] loss: 714.809
[76,     1] loss: 682.541
[77,     1] loss: 657.502
[78,     1] loss: 705.153
[79,     1] loss: 682.164
[80,     1] loss: 648.440
[81,     1] loss: 620.731
[82,     1] loss: 696.227
[83,     1] loss: 558.981
[84,     1] loss: 598.238
[85,     1] loss: 671.879
[86,     1] loss: 521.425
[87,     1] loss: 610.640
[88,     1] loss: 535.736
[89,     1] loss: 610.724
[90,     1] loss: 603.980
[91,     1] loss: 524.219
[92,     1] loss: 535.009
[93,     1] loss: 543.745
[94,     1] loss: 488.297
[95,     1] loss: 507.018
[96,     1] loss: 454.767
[97,     1] loss: 498.313
[98,     1] loss: 493.613
[99,     1] loss: 536.447
[100,     1] loss: 463.777
[101,     1] loss: 496.048
[102,     1] loss: 497.749
[103,     1] loss: 410.946
[104,     1] loss: 489.805
[105,     1] loss: 520.234
[106,     1] loss: 447.848
[107,     1] loss: 581.466
[108,     1] loss: 542.166
[109,     1] loss: 454.029
[110,     1] loss: 544.778
[111,     1] loss: 408.356
[112,     1] loss: 514.975
[113,     1] loss: 441.435
[114,     1] loss: 469.802
[115,     1] loss: 385.772
[116,     1] loss: 473.126
[117,     1] loss: 385.786
[118,     1] loss: 419.257
[119,     1] loss: 366.584
[120,     1] loss: 435.309
[121,     1] loss: 442.208
[122,     1] loss: 360.366
[123,     1] loss: 465.695
[124,     1] loss: 399.063
[125,     1] loss: 321.031
[126,     1] loss: 366.001
[127,     1] loss: 348.413
[128,     1] loss: 355.515
[129,     1] loss: 346.085
[130,     1] loss: 378.459
[131,     1] loss: 326.982
[132,     1] loss: 345.888
[133,     1] loss: 364.855
[134,     1] loss: 340.177
[135,     1] loss: 390.448
[136,     1] loss: 382.876
[137,     1] loss: 330.846
[138,     1] loss: 328.987
[139,     1] loss: 314.669
[140,     1] loss: 316.497
[141,     1] loss: 344.477
[142,     1] loss: 506.880
[143,     1] loss: 443.376
[144,     1] loss: 364.678
[145,     1] loss: 327.556
[146,     1] loss: 440.514
[147,     1] loss: 357.414
[148,     1] loss: 345.338
[149,     1] loss: 339.024
[150,     1] loss: 313.297
[151,     1] loss: 365.451
[152,     1] loss: 325.916
[153,     1] loss: 308.954
[154,     1] loss: 291.115
[155,     1] loss: 268.133
[156,     1] loss: 354.975
[157,     1] loss: 291.521
[158,     1] loss: 273.771
[159,     1] loss: 333.450
[160,     1] loss: 351.111
[161,     1] loss: 355.825
[162,     1] loss: 343.479
[163,     1] loss: 277.446
[164,     1] loss: 343.094
[165,     1] loss: 506.356
[166,     1] loss: 483.259
[167,     1] loss: 402.759
[168,     1] loss: 403.503
[169,     1] loss: 467.877
[170,     1] loss: 354.366
[171,     1] loss: 490.197
[172,     1] loss: 344.862
[173,     1] loss: 402.002
[174,     1] loss: 382.753
[175,     1] loss: 299.174
[176,     1] loss: 326.557
[177,     1] loss: 329.926
[178,     1] loss: 275.487
[179,     1] loss: 334.386
[180,     1] loss: 292.145
[181,     1] loss: 283.805
[182,     1] loss: 380.048
[183,     1] loss: 433.724
[184,     1] loss: 312.044
[185,     1] loss: 298.665
[186,     1] loss: 354.346
[187,     1] loss: 274.779
[188,     1] loss: 421.885
[189,     1] loss: 360.554
[190,     1] loss: 308.004
[191,     1] loss: 368.440
Early stopping applied (best metric=0.21347463130950928)
Finished Training
Total time taken: 29.510745525360107
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.126
[2,     1] loss: 1237.381
[3,     1] loss: 1230.350
[4,     1] loss: 1243.275
[5,     1] loss: 1234.819
[6,     1] loss: 1233.375
[7,     1] loss: 1230.582
[8,     1] loss: 1235.313
[9,     1] loss: 1229.039
[10,     1] loss: 1235.083
[11,     1] loss: 1227.620
[12,     1] loss: 1228.720
[13,     1] loss: 1229.352
[14,     1] loss: 1224.591
[15,     1] loss: 1221.968
[16,     1] loss: 1222.031
[17,     1] loss: 1209.914
[18,     1] loss: 1200.068
[19,     1] loss: 1185.974
[20,     1] loss: 1156.775
[21,     1] loss: 1127.731
[22,     1] loss: 1121.204
[23,     1] loss: 1074.230
[24,     1] loss: 1053.453
[25,     1] loss: 1067.110
[26,     1] loss: 1057.733
[27,     1] loss: 1035.387
[28,     1] loss: 1050.961
[29,     1] loss: 1071.444
[30,     1] loss: 994.644
[31,     1] loss: 1024.783
[32,     1] loss: 1013.286
[33,     1] loss: 986.075
[34,     1] loss: 941.376
[35,     1] loss: 1013.216
[36,     1] loss: 970.814
[37,     1] loss: 900.433
[38,     1] loss: 969.630
[39,     1] loss: 911.075
[40,     1] loss: 951.496
[41,     1] loss: 917.927
[42,     1] loss: 940.277
[43,     1] loss: 898.545
[44,     1] loss: 897.943
[45,     1] loss: 875.730
[46,     1] loss: 859.570
[47,     1] loss: 883.701
[48,     1] loss: 927.214
[49,     1] loss: 872.438
[50,     1] loss: 857.539
[51,     1] loss: 842.454
[52,     1] loss: 852.010
[53,     1] loss: 797.081
[54,     1] loss: 838.829
[55,     1] loss: 832.935
[56,     1] loss: 847.358
[57,     1] loss: 772.777
[58,     1] loss: 824.898
[59,     1] loss: 775.839
[60,     1] loss: 753.281
[61,     1] loss: 753.945
[62,     1] loss: 807.134
[63,     1] loss: 750.070
[64,     1] loss: 715.521
[65,     1] loss: 853.518
[66,     1] loss: 733.839
[67,     1] loss: 706.819
[68,     1] loss: 677.343
[69,     1] loss: 665.152
[70,     1] loss: 633.542
[71,     1] loss: 646.516
[72,     1] loss: 623.374
[73,     1] loss: 588.205
[74,     1] loss: 649.494
[75,     1] loss: 668.962
[76,     1] loss: 764.215
[77,     1] loss: 1127.269
[78,     1] loss: 648.609
[79,     1] loss: 894.842
[80,     1] loss: 678.904
[81,     1] loss: 766.232
[82,     1] loss: 819.502
[83,     1] loss: 763.103
[84,     1] loss: 696.716
[85,     1] loss: 750.559
[86,     1] loss: 650.103
[87,     1] loss: 632.174
[88,     1] loss: 671.698
[89,     1] loss: 637.145
[90,     1] loss: 615.091
[91,     1] loss: 553.099
[92,     1] loss: 538.672
[93,     1] loss: 534.480
[94,     1] loss: 474.475
[95,     1] loss: 477.127
[96,     1] loss: 550.109
[97,     1] loss: 509.583
[98,     1] loss: 460.287
[99,     1] loss: 496.065
[100,     1] loss: 465.863
[101,     1] loss: 428.579
[102,     1] loss: 412.019
[103,     1] loss: 441.559
[104,     1] loss: 423.356
[105,     1] loss: 450.184
[106,     1] loss: 499.901
[107,     1] loss: 495.926
[108,     1] loss: 535.942
[109,     1] loss: 418.590
[110,     1] loss: 406.722
[111,     1] loss: 452.511
Early stopping applied (best metric=0.3121102750301361)
Finished Training
Total time taken: 17.68127989768982
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1242.416
[2,     1] loss: 1234.842
[3,     1] loss: 1233.796
[4,     1] loss: 1236.417
[5,     1] loss: 1236.360
[6,     1] loss: 1231.762
[7,     1] loss: 1229.472
[8,     1] loss: 1229.982
[9,     1] loss: 1230.196
[10,     1] loss: 1223.478
[11,     1] loss: 1217.787
[12,     1] loss: 1199.522
[13,     1] loss: 1176.846
[14,     1] loss: 1143.563
[15,     1] loss: 1135.497
[16,     1] loss: 1105.126
[17,     1] loss: 1055.934
[18,     1] loss: 1021.279
[19,     1] loss: 999.281
[20,     1] loss: 1030.505
[21,     1] loss: 993.201
[22,     1] loss: 1009.773
[23,     1] loss: 981.424
[24,     1] loss: 931.327
[25,     1] loss: 1001.458
[26,     1] loss: 961.835
[27,     1] loss: 977.074
[28,     1] loss: 965.081
[29,     1] loss: 934.948
[30,     1] loss: 904.061
[31,     1] loss: 919.595
[32,     1] loss: 908.901
[33,     1] loss: 890.991
[34,     1] loss: 923.407
[35,     1] loss: 888.443
[36,     1] loss: 948.188
[37,     1] loss: 864.314
[38,     1] loss: 859.172
[39,     1] loss: 855.000
[40,     1] loss: 837.295
[41,     1] loss: 882.541
[42,     1] loss: 819.357
[43,     1] loss: 816.357
[44,     1] loss: 891.302
[45,     1] loss: 814.715
[46,     1] loss: 810.921
[47,     1] loss: 818.583
[48,     1] loss: 843.171
[49,     1] loss: 804.267
[50,     1] loss: 803.851
[51,     1] loss: 796.619
[52,     1] loss: 785.866
[53,     1] loss: 787.178
[54,     1] loss: 736.877
[55,     1] loss: 787.345
[56,     1] loss: 754.466
[57,     1] loss: 782.083
[58,     1] loss: 752.274
[59,     1] loss: 732.331
[60,     1] loss: 726.606
[61,     1] loss: 741.319
[62,     1] loss: 695.475
[63,     1] loss: 677.642
[64,     1] loss: 711.034
[65,     1] loss: 631.889
[66,     1] loss: 662.958
[67,     1] loss: 673.672
[68,     1] loss: 597.488
[69,     1] loss: 607.319
[70,     1] loss: 689.525
[71,     1] loss: 1239.949
[72,     1] loss: 868.908
[73,     1] loss: 882.884
[74,     1] loss: 709.389
[75,     1] loss: 840.438
[76,     1] loss: 857.699
[77,     1] loss: 830.034
[78,     1] loss: 768.436
[79,     1] loss: 790.931
[80,     1] loss: 832.455
[81,     1] loss: 731.862
[82,     1] loss: 692.618
[83,     1] loss: 768.344
[84,     1] loss: 707.895
[85,     1] loss: 676.873
[86,     1] loss: 690.029
[87,     1] loss: 656.133
[88,     1] loss: 626.211
[89,     1] loss: 642.354
[90,     1] loss: 593.182
[91,     1] loss: 575.318
[92,     1] loss: 590.640
[93,     1] loss: 513.704
[94,     1] loss: 553.189
[95,     1] loss: 495.321
[96,     1] loss: 507.095
[97,     1] loss: 501.060
[98,     1] loss: 491.155
[99,     1] loss: 414.531
[100,     1] loss: 534.647
[101,     1] loss: 435.127
[102,     1] loss: 419.436
[103,     1] loss: 509.699
[104,     1] loss: 435.831
[105,     1] loss: 479.354
[106,     1] loss: 452.114
[107,     1] loss: 406.274
[108,     1] loss: 469.436
[109,     1] loss: 431.355
[110,     1] loss: 369.574
[111,     1] loss: 459.181
[112,     1] loss: 415.290
[113,     1] loss: 455.593
[114,     1] loss: 587.282
[115,     1] loss: 496.403
[116,     1] loss: 452.544
[117,     1] loss: 489.750
[118,     1] loss: 415.959
[119,     1] loss: 495.629
[120,     1] loss: 448.609
[121,     1] loss: 507.394
[122,     1] loss: 366.186
[123,     1] loss: 457.346
[124,     1] loss: 390.155
[125,     1] loss: 418.679
[126,     1] loss: 356.214
[127,     1] loss: 437.884
[128,     1] loss: 335.946
[129,     1] loss: 424.298
[130,     1] loss: 354.384
[131,     1] loss: 429.182
[132,     1] loss: 477.876
[133,     1] loss: 350.074
[134,     1] loss: 415.851
[135,     1] loss: 331.687
[136,     1] loss: 369.926
Early stopping applied (best metric=0.38324451446533203)
Finished Training
Total time taken: 19.626339435577393
{'Hydroxylation-K Validation Accuracy': 0.7463947990543736, 'Hydroxylation-K Validation Sensitivity': 0.6733333333333333, 'Hydroxylation-K Validation Specificity': 0.7649122807017543, 'Hydroxylation-K Validation Precision': 0.4305279790728707, 'Hydroxylation-K AUC ROC': 0.8148732943469785, 'Hydroxylation-K AUC PR': 0.6067376838166891, 'Hydroxylation-K MCC': 0.3814737463197457, 'Hydroxylation-K F1': 0.5217612468417067, 'Validation Loss (Hydroxylation-K)': 0.4639725923538208, 'Hydroxylation-P Validation Accuracy': 0.8029844170346683, 'Hydroxylation-P Validation Sensitivity': 0.8202116402116403, 'Hydroxylation-P Validation Specificity': 0.7993316374881541, 'Hydroxylation-P Validation Precision': 0.47434119191841634, 'Hydroxylation-P AUC ROC': 0.8578284314540487, 'Hydroxylation-P AUC PR': 0.5987648149193673, 'Hydroxylation-P MCC': 0.5147258350616399, 'Hydroxylation-P F1': 0.5989196915565348, 'Validation Loss (Hydroxylation-P)': 0.35496279994646707, 'Validation Loss (total)': 0.8189353903134664, 'TimeToTrain': 16.77544225056966}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035177713539965337,
 'learning_rate_Hydroxylation-K': 0.007491928875356395,
 'learning_rate_Hydroxylation-P': 0.0022211430427132115,
 'log_base': 1.179973743377684,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2751863410,
 'sample_weights': [1.5398509664693472, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1696403642796014,
 'weight_decay_Hydroxylation-K': 1.2687999562052439,
 'weight_decay_Hydroxylation-P': 7.214018405721061}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3253.387
[2,     1] loss: 3256.658
[3,     1] loss: 3283.539
[4,     1] loss: 3281.451
[5,     1] loss: 3285.884
[6,     1] loss: 3270.038
[7,     1] loss: 3251.578
[8,     1] loss: 3251.220
[9,     1] loss: 3260.524
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007818360110891062,
 'learning_rate_Hydroxylation-K': 0.004801200756939251,
 'learning_rate_Hydroxylation-P': 0.004957609377551617,
 'log_base': 1.7775278465270419,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 344800927,
 'sample_weights': [10.087746008726944, 1.2610168286514807],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3605298491453706,
 'weight_decay_Hydroxylation-K': 0.8461692091548512,
 'weight_decay_Hydroxylation-P': 2.3596182099572838}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.388
[2,     1] loss: 1533.361
[3,     1] loss: 1523.290
[4,     1] loss: 1525.035
[5,     1] loss: 1521.099
[6,     1] loss: 1518.450
[7,     1] loss: 1520.985
[8,     1] loss: 1512.509
[9,     1] loss: 1507.000
[10,     1] loss: 1501.325
[11,     1] loss: 1496.388
[12,     1] loss: 1467.800
[13,     1] loss: 1438.051
[14,     1] loss: 1387.774
[15,     1] loss: 1390.393
[16,     1] loss: 1333.132
[17,     1] loss: 1282.613
[18,     1] loss: 1329.808
[19,     1] loss: 1297.401
[20,     1] loss: 1245.302
[21,     1] loss: 1238.340
[22,     1] loss: 1245.209
[23,     1] loss: 1248.549
[24,     1] loss: 1234.742
[25,     1] loss: 1216.156
[26,     1] loss: 1179.756
[27,     1] loss: 1160.301
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007572711060340918,
 'learning_rate_Hydroxylation-K': 0.008606182787597,
 'learning_rate_Hydroxylation-P': 0.0018955222933964237,
 'log_base': 1.805235423692421,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1103378347,
 'sample_weights': [2.902251048692782, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.626201320632613,
 'weight_decay_Hydroxylation-K': 8.286386209321572,
 'weight_decay_Hydroxylation-P': 8.999014699151019}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1504.938
[2,     1] loss: 1507.680
[3,     1] loss: 1514.250
[4,     1] loss: 1507.447
[5,     1] loss: 1504.269
[6,     1] loss: 1500.483
[7,     1] loss: 1501.602
[8,     1] loss: 1503.805
[9,     1] loss: 1500.088
[10,     1] loss: 1493.252
[11,     1] loss: 1495.312
[12,     1] loss: 1480.298
[13,     1] loss: 1464.124
[14,     1] loss: 1449.854
[15,     1] loss: 1400.578
[16,     1] loss: 1373.672
[17,     1] loss: 1323.560
[18,     1] loss: 1320.005
[19,     1] loss: 1368.380
[20,     1] loss: 1239.309
[21,     1] loss: 1258.301
[22,     1] loss: 1227.184
[23,     1] loss: 1191.898
[24,     1] loss: 1233.005
[25,     1] loss: 1165.299
[26,     1] loss: 1155.977
[27,     1] loss: 1188.849
[28,     1] loss: 1137.627
[29,     1] loss: 1098.814
[30,     1] loss: 1131.829
[31,     1] loss: 1089.581
[32,     1] loss: 1094.130
[33,     1] loss: 1066.413
[34,     1] loss: 1124.643
[35,     1] loss: 1290.878
[36,     1] loss: 1071.434
[37,     1] loss: 1145.082
[38,     1] loss: 1084.144
[39,     1] loss: 1098.521
[40,     1] loss: 1135.604
[41,     1] loss: 1050.525
[42,     1] loss: 1069.352
[43,     1] loss: 1036.824
[44,     1] loss: 1012.162
[45,     1] loss: 1020.746
[46,     1] loss: 946.546
[47,     1] loss: 948.644
[48,     1] loss: 930.702
[49,     1] loss: 940.253
[50,     1] loss: 1087.584
[51,     1] loss: 1287.803
[52,     1] loss: 1188.637
[53,     1] loss: 1205.971
[54,     1] loss: 1135.839
[55,     1] loss: 1059.705
[56,     1] loss: 1101.818
[57,     1] loss: 1011.886
[58,     1] loss: 1082.489
[59,     1] loss: 1027.398
[60,     1] loss: 939.296
[61,     1] loss: 993.439
[62,     1] loss: 994.108
[63,     1] loss: 976.770
[64,     1] loss: 1163.309
[65,     1] loss: 1014.929
[66,     1] loss: 941.996
[67,     1] loss: 1041.663
[68,     1] loss: 1016.038
[69,     1] loss: 1034.978
[70,     1] loss: 965.580
[71,     1] loss: 1140.805
[72,     1] loss: 985.853
[73,     1] loss: 963.220
[74,     1] loss: 1008.479
[75,     1] loss: 921.257
[76,     1] loss: 944.881
[77,     1] loss: 917.242
[78,     1] loss: 850.873
[79,     1] loss: 931.257
[80,     1] loss: 1210.595
[81,     1] loss: 1288.046
[82,     1] loss: 997.391
[83,     1] loss: 1150.368
[84,     1] loss: 1087.443
[85,     1] loss: 975.270
[86,     1] loss: 1055.696
[87,     1] loss: 872.430
[88,     1] loss: 1063.661
[89,     1] loss: 975.594
[90,     1] loss: 912.529
[91,     1] loss: 970.522
[92,     1] loss: 938.759
[93,     1] loss: 873.739
[94,     1] loss: 934.535
[95,     1] loss: 902.620
[96,     1] loss: 787.008
[97,     1] loss: 860.619
[98,     1] loss: 926.032
[99,     1] loss: 1071.909
[100,     1] loss: 1224.369
[101,     1] loss: 943.543
[102,     1] loss: 1118.605
[103,     1] loss: 916.272
[104,     1] loss: 954.702
[105,     1] loss: 898.435
[106,     1] loss: 970.934
[107,     1] loss: 845.439
[108,     1] loss: 887.979
[109,     1] loss: 1085.664
[110,     1] loss: 994.598
[111,     1] loss: 933.557
[112,     1] loss: 967.310
[113,     1] loss: 881.219
[114,     1] loss: 965.041
[115,     1] loss: 861.244
[116,     1] loss: 856.584
[117,     1] loss: 1050.426
[118,     1] loss: 921.697
[119,     1] loss: 866.660
[120,     1] loss: 895.748
[121,     1] loss: 792.038
[122,     1] loss: 909.202
[123,     1] loss: 1155.143
[124,     1] loss: 918.042
[125,     1] loss: 829.570
[126,     1] loss: 954.667
[127,     1] loss: 858.638
[128,     1] loss: 979.494
[129,     1] loss: 875.383
[130,     1] loss: 740.684
[131,     1] loss: 912.571
[132,     1] loss: 864.880
[133,     1] loss: 725.287
[134,     1] loss: 1054.689
[135,     1] loss: 1629.716
[136,     1] loss: 920.417
[137,     1] loss: 1202.029
[138,     1] loss: 1071.644
[139,     1] loss: 1107.940
[140,     1] loss: 1092.708
[141,     1] loss: 1038.720
[142,     1] loss: 1040.776
[143,     1] loss: 1077.274
[144,     1] loss: 1008.792
[145,     1] loss: 941.155
[146,     1] loss: 907.891
[147,     1] loss: 920.703
[148,     1] loss: 894.164
[149,     1] loss: 922.930
[150,     1] loss: 994.335
[151,     1] loss: 1075.556
[152,     1] loss: 1004.867
[153,     1] loss: 867.318
[154,     1] loss: 940.975
[155,     1] loss: 844.913
[156,     1] loss: 1045.297
[157,     1] loss: 1071.608
[158,     1] loss: 880.652
[159,     1] loss: 1079.178
[160,     1] loss: 888.975
Early stopping applied (best metric=0.392341285943985)
Finished Training
Total time taken: 27.12183141708374
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1508.396
[2,     1] loss: 1507.681
[3,     1] loss: 1501.478
[4,     1] loss: 1503.474
[5,     1] loss: 1499.862
[6,     1] loss: 1502.301
[7,     1] loss: 1498.050
[8,     1] loss: 1492.146
[9,     1] loss: 1479.036
[10,     1] loss: 1464.054
[11,     1] loss: 1439.286
[12,     1] loss: 1410.636
[13,     1] loss: 1376.504
[14,     1] loss: 1319.984
[15,     1] loss: 1306.313
[16,     1] loss: 1316.711
[17,     1] loss: 1290.997
[18,     1] loss: 1259.980
[19,     1] loss: 1265.949
[20,     1] loss: 1269.195
[21,     1] loss: 1197.880
[22,     1] loss: 1184.819
[23,     1] loss: 1210.631
[24,     1] loss: 1161.518
[25,     1] loss: 1218.796
[26,     1] loss: 1287.828
[27,     1] loss: 1179.743
[28,     1] loss: 1231.411
[29,     1] loss: 1168.737
[30,     1] loss: 1196.745
[31,     1] loss: 1145.863
[32,     1] loss: 1174.489
[33,     1] loss: 1110.071
[34,     1] loss: 1152.354
[35,     1] loss: 1085.100
[36,     1] loss: 1135.752
[37,     1] loss: 1088.015
[38,     1] loss: 1093.969
[39,     1] loss: 1062.957
[40,     1] loss: 1051.620
[41,     1] loss: 1038.009
[42,     1] loss: 1109.137
[43,     1] loss: 1098.603
[44,     1] loss: 976.297
[45,     1] loss: 968.853
[46,     1] loss: 929.829
[47,     1] loss: 1369.144
[48,     1] loss: 1969.956
[49,     1] loss: 1605.273
[50,     1] loss: 1337.835
[51,     1] loss: 1337.755
[52,     1] loss: 1338.970
[53,     1] loss: 1379.149
[54,     1] loss: 1386.934
[55,     1] loss: 1374.866
[56,     1] loss: 1345.212
[57,     1] loss: 1382.287
[58,     1] loss: 1385.842
[59,     1] loss: 1350.657
[60,     1] loss: 1332.282
[61,     1] loss: 1320.587
[62,     1] loss: 1310.172
[63,     1] loss: 1310.235
[64,     1] loss: 1294.305
[65,     1] loss: 1299.168
[66,     1] loss: 1262.895
[67,     1] loss: 1227.220
[68,     1] loss: 1253.701
[69,     1] loss: 1182.186
[70,     1] loss: 1185.474
[71,     1] loss: 1161.372
[72,     1] loss: 1141.730
[73,     1] loss: 1170.727
[74,     1] loss: 1148.865
[75,     1] loss: 1208.492
[76,     1] loss: 1081.124
[77,     1] loss: 1101.049
[78,     1] loss: 1124.902
[79,     1] loss: 1077.614
[80,     1] loss: 1115.499
[81,     1] loss: 1291.268
[82,     1] loss: 1272.892
[83,     1] loss: 1106.564
[84,     1] loss: 1140.407
[85,     1] loss: 1171.755
[86,     1] loss: 1123.132
[87,     1] loss: 1144.876
[88,     1] loss: 1049.898
[89,     1] loss: 1174.609
[90,     1] loss: 1017.257
[91,     1] loss: 1200.949
[92,     1] loss: 1069.586
[93,     1] loss: 1107.971
[94,     1] loss: 1089.489
Early stopping applied (best metric=0.3423878252506256)
Finished Training
Total time taken: 14.605052947998047
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1506.299
[2,     1] loss: 1506.982
[3,     1] loss: 1504.988
[4,     1] loss: 1502.062
[5,     1] loss: 1500.731
[6,     1] loss: 1502.103
[7,     1] loss: 1502.194
[8,     1] loss: 1504.316
[9,     1] loss: 1507.859
[10,     1] loss: 1495.884
[11,     1] loss: 1499.164
[12,     1] loss: 1493.367
[13,     1] loss: 1489.226
[14,     1] loss: 1489.107
[15,     1] loss: 1475.971
[16,     1] loss: 1453.687
[17,     1] loss: 1421.115
[18,     1] loss: 1384.261
[19,     1] loss: 1336.601
[20,     1] loss: 1351.605
[21,     1] loss: 1236.562
[22,     1] loss: 1314.267
[23,     1] loss: 1263.104
[24,     1] loss: 1232.723
[25,     1] loss: 1242.813
[26,     1] loss: 1269.860
[27,     1] loss: 1159.826
[28,     1] loss: 1265.609
[29,     1] loss: 1136.088
[30,     1] loss: 1199.150
[31,     1] loss: 1119.824
[32,     1] loss: 1173.449
[33,     1] loss: 1108.306
[34,     1] loss: 1139.411
[35,     1] loss: 1092.454
[36,     1] loss: 1128.618
[37,     1] loss: 1036.087
[38,     1] loss: 1093.196
[39,     1] loss: 1073.383
[40,     1] loss: 1076.170
[41,     1] loss: 1083.524
[42,     1] loss: 1004.669
[43,     1] loss: 1141.019
[44,     1] loss: 1200.574
[45,     1] loss: 1068.189
[46,     1] loss: 1203.833
[47,     1] loss: 1156.838
[48,     1] loss: 1064.887
[49,     1] loss: 1093.503
[50,     1] loss: 1043.308
[51,     1] loss: 1064.372
[52,     1] loss: 963.388
[53,     1] loss: 1009.775
[54,     1] loss: 1031.428
[55,     1] loss: 940.113
[56,     1] loss: 920.160
[57,     1] loss: 977.171
[58,     1] loss: 908.429
[59,     1] loss: 902.777
[60,     1] loss: 829.708
[61,     1] loss: 1008.223
[62,     1] loss: 2175.963
[63,     1] loss: 1181.900
[64,     1] loss: 1319.522
[65,     1] loss: 1273.392
[66,     1] loss: 1341.057
[67,     1] loss: 1349.851
[68,     1] loss: 1296.798
[69,     1] loss: 1290.186
[70,     1] loss: 1332.079
[71,     1] loss: 1275.857
[72,     1] loss: 1320.438
[73,     1] loss: 1273.592
[74,     1] loss: 1276.951
[75,     1] loss: 1265.462
[76,     1] loss: 1209.211
[77,     1] loss: 1258.094
[78,     1] loss: 1240.083
[79,     1] loss: 1241.836
[80,     1] loss: 1227.542
[81,     1] loss: 1176.664
[82,     1] loss: 1211.671
[83,     1] loss: 1237.356
[84,     1] loss: 1114.462
[85,     1] loss: 1134.075
[86,     1] loss: 1146.416
[87,     1] loss: 1109.094
[88,     1] loss: 990.063
[89,     1] loss: 1210.476
[90,     1] loss: 1184.315
[91,     1] loss: 1110.538
[92,     1] loss: 1088.461
[93,     1] loss: 1060.927
[94,     1] loss: 1033.960
[95,     1] loss: 1011.398
[96,     1] loss: 1313.927
[97,     1] loss: 1092.735
[98,     1] loss: 1136.081
[99,     1] loss: 1044.868
[100,     1] loss: 1044.411
[101,     1] loss: 981.876
[102,     1] loss: 1009.971
[103,     1] loss: 1135.651
[104,     1] loss: 975.235
[105,     1] loss: 880.830
[106,     1] loss: 952.620
[107,     1] loss: 1007.824
Early stopping applied (best metric=0.37200799584388733)
Finished Training
Total time taken: 18.810497760772705
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1509.359
[2,     1] loss: 1502.980
[3,     1] loss: 1503.564
[4,     1] loss: 1501.607
[5,     1] loss: 1496.934
[6,     1] loss: 1507.228
[7,     1] loss: 1505.583
[8,     1] loss: 1501.205
[9,     1] loss: 1501.679
[10,     1] loss: 1501.761
[11,     1] loss: 1502.698
[12,     1] loss: 1498.716
[13,     1] loss: 1498.437
[14,     1] loss: 1493.459
[15,     1] loss: 1484.165
[16,     1] loss: 1464.401
[17,     1] loss: 1443.091
[18,     1] loss: 1404.922
[19,     1] loss: 1368.638
[20,     1] loss: 1303.446
[21,     1] loss: 1310.467
[22,     1] loss: 1276.521
[23,     1] loss: 1294.258
[24,     1] loss: 1222.937
[25,     1] loss: 1291.649
[26,     1] loss: 1207.473
[27,     1] loss: 1231.076
[28,     1] loss: 1159.545
[29,     1] loss: 1210.339
[30,     1] loss: 1181.346
[31,     1] loss: 1123.746
[32,     1] loss: 1127.339
[33,     1] loss: 1126.832
[34,     1] loss: 1052.568
[35,     1] loss: 1132.689
[36,     1] loss: 1227.075
[37,     1] loss: 1190.000
[38,     1] loss: 1128.710
[39,     1] loss: 1141.454
[40,     1] loss: 1053.073
[41,     1] loss: 1128.177
[42,     1] loss: 1028.070
[43,     1] loss: 1044.744
[44,     1] loss: 1013.823
[45,     1] loss: 968.753
[46,     1] loss: 962.593
[47,     1] loss: 945.748
[48,     1] loss: 1153.320
[49,     1] loss: 1605.179
[50,     1] loss: 1017.844
[51,     1] loss: 1223.774
[52,     1] loss: 1277.488
[53,     1] loss: 1192.645
[54,     1] loss: 1247.715
[55,     1] loss: 1204.292
[56,     1] loss: 1177.935
[57,     1] loss: 1108.472
[58,     1] loss: 1160.676
[59,     1] loss: 1088.318
[60,     1] loss: 1106.831
[61,     1] loss: 1096.702
[62,     1] loss: 1107.653
[63,     1] loss: 1061.584
[64,     1] loss: 1088.572
[65,     1] loss: 1037.004
[66,     1] loss: 1062.423
[67,     1] loss: 1009.341
[68,     1] loss: 966.305
[69,     1] loss: 876.517
[70,     1] loss: 936.344
[71,     1] loss: 982.179
[72,     1] loss: 1220.771
[73,     1] loss: 1158.093
[74,     1] loss: 1047.361
[75,     1] loss: 1108.262
[76,     1] loss: 1196.764
[77,     1] loss: 1051.903
[78,     1] loss: 1064.855
[79,     1] loss: 1058.191
[80,     1] loss: 1020.637
[81,     1] loss: 1136.924
[82,     1] loss: 910.932
Early stopping applied (best metric=0.38402849435806274)
Finished Training
Total time taken: 14.001346111297607
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1510.341
[2,     1] loss: 1517.912
[3,     1] loss: 1509.652
[4,     1] loss: 1505.417
[5,     1] loss: 1508.090
[6,     1] loss: 1504.745
[7,     1] loss: 1504.261
[8,     1] loss: 1505.338
[9,     1] loss: 1505.482
[10,     1] loss: 1500.352
[11,     1] loss: 1499.507
[12,     1] loss: 1499.207
[13,     1] loss: 1492.764
[14,     1] loss: 1483.134
[15,     1] loss: 1475.849
[16,     1] loss: 1452.843
[17,     1] loss: 1432.636
[18,     1] loss: 1426.022
[19,     1] loss: 1392.921
[20,     1] loss: 1348.268
[21,     1] loss: 1420.142
[22,     1] loss: 1269.340
[23,     1] loss: 1281.417
[24,     1] loss: 1320.309
[25,     1] loss: 1232.869
[26,     1] loss: 1198.551
[27,     1] loss: 1229.391
[28,     1] loss: 1234.223
[29,     1] loss: 1180.948
[30,     1] loss: 1203.895
[31,     1] loss: 1145.393
[32,     1] loss: 1183.002
[33,     1] loss: 1132.843
[34,     1] loss: 1170.218
[35,     1] loss: 1252.721
[36,     1] loss: 1166.615
[37,     1] loss: 1148.126
[38,     1] loss: 1170.671
[39,     1] loss: 1081.578
[40,     1] loss: 1132.898
[41,     1] loss: 1045.675
[42,     1] loss: 1129.536
[43,     1] loss: 1180.280
[44,     1] loss: 1063.175
[45,     1] loss: 1099.043
[46,     1] loss: 1091.589
[47,     1] loss: 1023.192
[48,     1] loss: 1085.348
[49,     1] loss: 1040.531
[50,     1] loss: 1010.456
[51,     1] loss: 995.518
[52,     1] loss: 1035.386
[53,     1] loss: 1013.775
[54,     1] loss: 941.114
[55,     1] loss: 945.459
[56,     1] loss: 917.407
[57,     1] loss: 894.167
[58,     1] loss: 1191.628
[59,     1] loss: 1977.757
[60,     1] loss: 979.742
[61,     1] loss: 1267.943
[62,     1] loss: 1251.685
[63,     1] loss: 1224.473
[64,     1] loss: 1261.792
[65,     1] loss: 1258.848
[66,     1] loss: 1262.176
[67,     1] loss: 1202.171
[68,     1] loss: 1214.922
[69,     1] loss: 1178.834
[70,     1] loss: 1209.377
[71,     1] loss: 1157.712
[72,     1] loss: 1142.534
[73,     1] loss: 1148.336
[74,     1] loss: 1143.303
[75,     1] loss: 1066.449
[76,     1] loss: 1167.912
[77,     1] loss: 1085.019
[78,     1] loss: 1053.154
[79,     1] loss: 1112.356
[80,     1] loss: 1028.145
[81,     1] loss: 1051.179
[82,     1] loss: 962.130
[83,     1] loss: 1011.200
Early stopping applied (best metric=0.3615969121456146)
Finished Training
Total time taken: 11.87829041481018
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1504.591
[2,     1] loss: 1509.495
[3,     1] loss: 1501.311
[4,     1] loss: 1503.370
[5,     1] loss: 1498.036
[6,     1] loss: 1498.354
[7,     1] loss: 1495.046
[8,     1] loss: 1479.534
[9,     1] loss: 1465.346
[10,     1] loss: 1431.726
[11,     1] loss: 1406.259
[12,     1] loss: 1350.294
[13,     1] loss: 1277.678
[14,     1] loss: 1320.240
[15,     1] loss: 1240.592
[16,     1] loss: 1226.418
[17,     1] loss: 1216.687
[18,     1] loss: 1260.402
[19,     1] loss: 1192.120
[20,     1] loss: 1216.677
[21,     1] loss: 1212.979
[22,     1] loss: 1149.150
[23,     1] loss: 1216.270
[24,     1] loss: 1170.528
[25,     1] loss: 1180.229
[26,     1] loss: 1115.899
[27,     1] loss: 1088.510
[28,     1] loss: 1120.402
[29,     1] loss: 1171.994
[30,     1] loss: 1431.272
[31,     1] loss: 1132.922
[32,     1] loss: 1196.870
[33,     1] loss: 1116.478
[34,     1] loss: 1212.381
[35,     1] loss: 1171.442
[36,     1] loss: 1120.592
[37,     1] loss: 1172.841
[38,     1] loss: 1101.603
[39,     1] loss: 1049.435
[40,     1] loss: 1066.258
[41,     1] loss: 1053.516
[42,     1] loss: 1005.845
[43,     1] loss: 1045.295
[44,     1] loss: 1024.967
[45,     1] loss: 1016.297
[46,     1] loss: 988.139
[47,     1] loss: 959.104
[48,     1] loss: 992.953
[49,     1] loss: 983.633
[50,     1] loss: 901.888
[51,     1] loss: 1007.890
[52,     1] loss: 1835.885
[53,     1] loss: 975.257
[54,     1] loss: 1409.984
[55,     1] loss: 1179.220
[56,     1] loss: 1236.788
[57,     1] loss: 1269.722
[58,     1] loss: 1266.795
[59,     1] loss: 1204.896
[60,     1] loss: 1218.576
[61,     1] loss: 1142.437
[62,     1] loss: 1152.356
[63,     1] loss: 1076.632
[64,     1] loss: 1143.327
[65,     1] loss: 1139.550
[66,     1] loss: 1054.473
[67,     1] loss: 1075.602
[68,     1] loss: 1030.559
[69,     1] loss: 1009.943
[70,     1] loss: 1062.677
[71,     1] loss: 978.732
[72,     1] loss: 1022.254
[73,     1] loss: 996.510
[74,     1] loss: 952.241
[75,     1] loss: 972.154
[76,     1] loss: 958.045
[77,     1] loss: 1010.800
[78,     1] loss: 982.105
[79,     1] loss: 986.985
[80,     1] loss: 968.113
[81,     1] loss: 909.271
[82,     1] loss: 906.762
[83,     1] loss: 925.546
[84,     1] loss: 1162.419
[85,     1] loss: 1821.549
[86,     1] loss: 1008.833
[87,     1] loss: 1235.352
[88,     1] loss: 1232.207
[89,     1] loss: 1199.370
[90,     1] loss: 1209.161
[91,     1] loss: 1161.718
[92,     1] loss: 1145.425
[93,     1] loss: 1129.886
[94,     1] loss: 1102.145
[95,     1] loss: 1094.948
[96,     1] loss: 1065.752
[97,     1] loss: 1044.178
[98,     1] loss: 1033.210
[99,     1] loss: 973.746
[100,     1] loss: 1063.551
Early stopping applied (best metric=0.3621417284011841)
Finished Training
Total time taken: 17.758224487304688
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1503.543
[2,     1] loss: 1507.550
[3,     1] loss: 1509.274
[4,     1] loss: 1502.515
[5,     1] loss: 1506.851
[6,     1] loss: 1505.336
[7,     1] loss: 1502.600
[8,     1] loss: 1502.980
[9,     1] loss: 1501.781
[10,     1] loss: 1501.345
[11,     1] loss: 1504.966
[12,     1] loss: 1498.812
[13,     1] loss: 1501.991
[14,     1] loss: 1501.126
[15,     1] loss: 1496.670
[16,     1] loss: 1494.825
[17,     1] loss: 1490.203
[18,     1] loss: 1476.926
[19,     1] loss: 1464.783
[20,     1] loss: 1448.883
[21,     1] loss: 1416.741
[22,     1] loss: 1381.323
[23,     1] loss: 1343.626
[24,     1] loss: 1314.414
[25,     1] loss: 1271.811
[26,     1] loss: 1345.241
[27,     1] loss: 1361.508
[28,     1] loss: 1267.677
[29,     1] loss: 1241.104
[30,     1] loss: 1261.103
[31,     1] loss: 1259.731
[32,     1] loss: 1209.535
[33,     1] loss: 1216.649
[34,     1] loss: 1195.416
[35,     1] loss: 1202.325
[36,     1] loss: 1202.820
[37,     1] loss: 1158.691
[38,     1] loss: 1204.771
[39,     1] loss: 1154.388
[40,     1] loss: 1129.081
[41,     1] loss: 1057.775
[42,     1] loss: 1107.379
[43,     1] loss: 1106.762
[44,     1] loss: 997.817
[45,     1] loss: 1113.164
[46,     1] loss: 1464.665
[47,     1] loss: 1139.906
[48,     1] loss: 1257.509
[49,     1] loss: 1216.971
[50,     1] loss: 1199.198
[51,     1] loss: 1219.015
[52,     1] loss: 1210.635
[53,     1] loss: 1131.267
[54,     1] loss: 1136.531
[55,     1] loss: 1151.188
[56,     1] loss: 1198.548
[57,     1] loss: 1095.100
[58,     1] loss: 1076.910
[59,     1] loss: 1099.236
[60,     1] loss: 1015.162
[61,     1] loss: 1054.655
[62,     1] loss: 1064.376
[63,     1] loss: 919.249
[64,     1] loss: 917.642
[65,     1] loss: 977.373
[66,     1] loss: 930.101
[67,     1] loss: 1588.560
[68,     1] loss: 1394.838
[69,     1] loss: 1262.770
[70,     1] loss: 1344.180
[71,     1] loss: 1316.793
[72,     1] loss: 1242.792
[73,     1] loss: 1156.748
[74,     1] loss: 1183.434
[75,     1] loss: 1220.436
[76,     1] loss: 1181.109
[77,     1] loss: 1196.826
[78,     1] loss: 1141.424
[79,     1] loss: 1134.043
[80,     1] loss: 1121.569
[81,     1] loss: 1108.140
[82,     1] loss: 1104.133
[83,     1] loss: 1015.300
[84,     1] loss: 1039.667
[85,     1] loss: 1319.519
[86,     1] loss: 1048.561
[87,     1] loss: 1123.674
[88,     1] loss: 1095.688
[89,     1] loss: 1070.578
[90,     1] loss: 1044.951
[91,     1] loss: 1079.732
[92,     1] loss: 1036.377
[93,     1] loss: 1074.066
[94,     1] loss: 1433.773
[95,     1] loss: 1274.133
[96,     1] loss: 1225.339
[97,     1] loss: 1164.979
[98,     1] loss: 1207.477
[99,     1] loss: 1280.763
[100,     1] loss: 1189.675
[101,     1] loss: 1160.464
[102,     1] loss: 1130.172
[103,     1] loss: 1099.404
[104,     1] loss: 1014.789
[105,     1] loss: 1055.884
[106,     1] loss: 935.896
[107,     1] loss: 1003.985
[108,     1] loss: 1030.400
[109,     1] loss: 941.440
[110,     1] loss: 949.511
[111,     1] loss: 1015.422
Early stopping applied (best metric=0.3555416762828827)
Finished Training
Total time taken: 19.722968339920044
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1506.831
[2,     1] loss: 1508.196
[3,     1] loss: 1505.198
[4,     1] loss: 1506.332
[5,     1] loss: 1501.902
[6,     1] loss: 1498.477
[7,     1] loss: 1504.766
[8,     1] loss: 1510.437
[9,     1] loss: 1502.174
[10,     1] loss: 1498.443
[11,     1] loss: 1502.126
[12,     1] loss: 1501.402
[13,     1] loss: 1493.566
[14,     1] loss: 1486.741
[15,     1] loss: 1474.729
[16,     1] loss: 1458.007
[17,     1] loss: 1435.163
[18,     1] loss: 1399.965
[19,     1] loss: 1366.585
[20,     1] loss: 1324.552
[21,     1] loss: 1284.738
[22,     1] loss: 1277.615
[23,     1] loss: 1250.925
[24,     1] loss: 1273.265
[25,     1] loss: 1311.627
[26,     1] loss: 1232.167
[27,     1] loss: 1231.542
[28,     1] loss: 1231.792
[29,     1] loss: 1237.671
[30,     1] loss: 1199.432
[31,     1] loss: 1215.970
[32,     1] loss: 1172.737
[33,     1] loss: 1120.038
[34,     1] loss: 1240.486
[35,     1] loss: 1175.737
[36,     1] loss: 1269.708
[37,     1] loss: 1121.721
[38,     1] loss: 1185.540
[39,     1] loss: 1088.362
[40,     1] loss: 1123.117
[41,     1] loss: 1092.263
[42,     1] loss: 1114.038
[43,     1] loss: 1051.777
[44,     1] loss: 1201.760
[45,     1] loss: 1025.529
[46,     1] loss: 1220.586
[47,     1] loss: 1023.354
[48,     1] loss: 1196.115
[49,     1] loss: 1000.028
[50,     1] loss: 1167.657
[51,     1] loss: 1131.254
[52,     1] loss: 1099.329
[53,     1] loss: 1119.645
[54,     1] loss: 1010.446
[55,     1] loss: 1072.031
[56,     1] loss: 999.468
[57,     1] loss: 1034.103
[58,     1] loss: 1041.993
[59,     1] loss: 918.797
[60,     1] loss: 1149.646
[61,     1] loss: 960.301
[62,     1] loss: 1054.406
[63,     1] loss: 1027.701
[64,     1] loss: 968.335
[65,     1] loss: 1073.727
[66,     1] loss: 1028.475
[67,     1] loss: 957.303
[68,     1] loss: 1145.797
[69,     1] loss: 925.726
[70,     1] loss: 1045.758
[71,     1] loss: 1009.718
[72,     1] loss: 890.542
[73,     1] loss: 958.214
[74,     1] loss: 884.150
[75,     1] loss: 993.504
[76,     1] loss: 1170.479
[77,     1] loss: 882.751
[78,     1] loss: 978.457
[79,     1] loss: 881.854
[80,     1] loss: 982.115
[81,     1] loss: 1086.942
[82,     1] loss: 865.487
[83,     1] loss: 892.492
[84,     1] loss: 925.439
[85,     1] loss: 881.348
[86,     1] loss: 839.557
[87,     1] loss: 780.945
[88,     1] loss: 779.786
[89,     1] loss: 786.814
[90,     1] loss: 1302.577
[91,     1] loss: 1969.437
[92,     1] loss: 1191.152
[93,     1] loss: 1277.661
[94,     1] loss: 1291.388
[95,     1] loss: 1350.812
[96,     1] loss: 1249.897
[97,     1] loss: 1268.587
[98,     1] loss: 1276.487
[99,     1] loss: 1238.244
[100,     1] loss: 1197.189
[101,     1] loss: 1135.128
[102,     1] loss: 1133.029
[103,     1] loss: 1169.732
[104,     1] loss: 1184.978
[105,     1] loss: 1153.793
[106,     1] loss: 1088.520
[107,     1] loss: 1102.462
[108,     1] loss: 1037.038
[109,     1] loss: 1128.146
[110,     1] loss: 1031.289
[111,     1] loss: 1061.678
[112,     1] loss: 1040.955
[113,     1] loss: 1095.987
[114,     1] loss: 1098.221
[115,     1] loss: 1052.325
[116,     1] loss: 1149.774
[117,     1] loss: 983.089
[118,     1] loss: 1127.489
[119,     1] loss: 1020.947
[120,     1] loss: 1057.510
[121,     1] loss: 1214.179
[122,     1] loss: 934.990
[123,     1] loss: 1092.715
[124,     1] loss: 961.979
[125,     1] loss: 1212.734
[126,     1] loss: 1013.442
[127,     1] loss: 1118.774
[128,     1] loss: 1097.765
Early stopping applied (best metric=0.39109617471694946)
Finished Training
Total time taken: 19.02127480506897
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1509.246
[2,     1] loss: 1505.235
[3,     1] loss: 1507.899
[4,     1] loss: 1506.121
[5,     1] loss: 1502.934
[6,     1] loss: 1503.882
[7,     1] loss: 1502.694
[8,     1] loss: 1503.738
[9,     1] loss: 1504.438
[10,     1] loss: 1502.940
[11,     1] loss: 1505.523
[12,     1] loss: 1502.969
[13,     1] loss: 1504.688
[14,     1] loss: 1503.030
[15,     1] loss: 1503.714
[16,     1] loss: 1503.122
[17,     1] loss: 1503.327
[18,     1] loss: 1501.706
[19,     1] loss: 1500.703
[20,     1] loss: 1500.890
[21,     1] loss: 1498.782
[22,     1] loss: 1492.016
[23,     1] loss: 1484.253
[24,     1] loss: 1466.876
[25,     1] loss: 1445.063
[26,     1] loss: 1429.407
[27,     1] loss: 1402.877
[28,     1] loss: 1363.009
[29,     1] loss: 1334.430
[30,     1] loss: 1330.882
[31,     1] loss: 1270.384
[32,     1] loss: 1339.651
[33,     1] loss: 1300.136
[34,     1] loss: 1296.049
[35,     1] loss: 1216.219
[36,     1] loss: 1273.101
[37,     1] loss: 1195.537
[38,     1] loss: 1224.217
[39,     1] loss: 1197.402
[40,     1] loss: 1187.091
[41,     1] loss: 1118.719
[42,     1] loss: 1197.050
[43,     1] loss: 1116.735
[44,     1] loss: 1129.350
[45,     1] loss: 1120.770
[46,     1] loss: 1105.865
[47,     1] loss: 1114.957
[48,     1] loss: 1109.283
[49,     1] loss: 1096.733
[50,     1] loss: 1048.660
[51,     1] loss: 1041.755
[52,     1] loss: 1113.831
[53,     1] loss: 1294.350
[54,     1] loss: 1027.572
[55,     1] loss: 1208.307
[56,     1] loss: 1070.368
[57,     1] loss: 1111.389
[58,     1] loss: 1041.593
[59,     1] loss: 1098.952
[60,     1] loss: 1040.632
[61,     1] loss: 1175.797
[62,     1] loss: 973.739
[63,     1] loss: 1249.539
[64,     1] loss: 1116.903
[65,     1] loss: 1016.208
[66,     1] loss: 1022.990
[67,     1] loss: 1084.123
[68,     1] loss: 1023.681
[69,     1] loss: 1011.384
[70,     1] loss: 1051.973
[71,     1] loss: 988.408
[72,     1] loss: 1000.230
[73,     1] loss: 1149.431
[74,     1] loss: 954.726
[75,     1] loss: 1017.519
[76,     1] loss: 927.737
[77,     1] loss: 1028.762
[78,     1] loss: 1060.334
[79,     1] loss: 932.833
[80,     1] loss: 1028.823
[81,     1] loss: 914.414
[82,     1] loss: 892.393
[83,     1] loss: 864.633
[84,     1] loss: 847.932
[85,     1] loss: 1099.262
[86,     1] loss: 1967.711
[87,     1] loss: 1008.416
[88,     1] loss: 1185.281
[89,     1] loss: 1131.781
[90,     1] loss: 1076.570
[91,     1] loss: 1070.244
[92,     1] loss: 1128.033
[93,     1] loss: 1115.618
[94,     1] loss: 1109.659
[95,     1] loss: 1126.316
[96,     1] loss: 1068.272
[97,     1] loss: 1028.983
[98,     1] loss: 929.484
[99,     1] loss: 1014.204
[100,     1] loss: 944.939
[101,     1] loss: 926.718
[102,     1] loss: 919.080
[103,     1] loss: 1068.991
[104,     1] loss: 1971.159
[105,     1] loss: 982.208
[106,     1] loss: 1437.862
[107,     1] loss: 1165.600
[108,     1] loss: 1174.435
[109,     1] loss: 1275.755
[110,     1] loss: 1272.208
[111,     1] loss: 1286.294
[112,     1] loss: 1226.373
[113,     1] loss: 1195.238
[114,     1] loss: 1236.233
[115,     1] loss: 1215.704
[116,     1] loss: 1163.198
[117,     1] loss: 1204.101
[118,     1] loss: 1160.184
[119,     1] loss: 1121.115
[120,     1] loss: 1110.979
[121,     1] loss: 1110.712
[122,     1] loss: 1040.299
[123,     1] loss: 1118.350
[124,     1] loss: 1058.753
[125,     1] loss: 1053.608
[126,     1] loss: 1011.294
[127,     1] loss: 1016.886
[128,     1] loss: 960.259
[129,     1] loss: 966.058
[130,     1] loss: 1009.112
Early stopping applied (best metric=0.33954206109046936)
Finished Training
Total time taken: 20.429298639297485
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1503.528
[2,     1] loss: 1514.672
[3,     1] loss: 1512.341
[4,     1] loss: 1504.846
[5,     1] loss: 1510.055
[6,     1] loss: 1506.011
[7,     1] loss: 1504.026
[8,     1] loss: 1506.633
[9,     1] loss: 1503.860
[10,     1] loss: 1505.140
[11,     1] loss: 1502.288
[12,     1] loss: 1501.833
[13,     1] loss: 1492.680
[14,     1] loss: 1488.752
[15,     1] loss: 1472.852
[16,     1] loss: 1455.464
[17,     1] loss: 1419.547
[18,     1] loss: 1378.004
[19,     1] loss: 1337.220
[20,     1] loss: 1309.839
[21,     1] loss: 1272.845
[22,     1] loss: 1334.453
[23,     1] loss: 1241.384
[24,     1] loss: 1226.958
[25,     1] loss: 1351.877
[26,     1] loss: 1218.176
[27,     1] loss: 1325.193
[28,     1] loss: 1195.175
[29,     1] loss: 1220.541
[30,     1] loss: 1223.561
[31,     1] loss: 1193.444
[32,     1] loss: 1187.241
[33,     1] loss: 1210.582
[34,     1] loss: 1066.639
[35,     1] loss: 1199.032
[36,     1] loss: 1082.575
[37,     1] loss: 1120.648
[38,     1] loss: 1001.591
[39,     1] loss: 1047.942
[40,     1] loss: 1000.776
[41,     1] loss: 1042.593
[42,     1] loss: 1014.255
[43,     1] loss: 1033.837
[44,     1] loss: 1032.736
[45,     1] loss: 972.610
[46,     1] loss: 1021.508
[47,     1] loss: 964.412
[48,     1] loss: 922.689
[49,     1] loss: 1111.857
[50,     1] loss: 1755.217
[51,     1] loss: 1002.855
[52,     1] loss: 1268.432
[53,     1] loss: 1253.220
[54,     1] loss: 1215.929
[55,     1] loss: 1231.237
[56,     1] loss: 1240.554
[57,     1] loss: 1179.854
[58,     1] loss: 1143.096
[59,     1] loss: 1152.099
[60,     1] loss: 1086.580
[61,     1] loss: 1092.628
[62,     1] loss: 1187.802
[63,     1] loss: 1124.784
[64,     1] loss: 1090.774
[65,     1] loss: 1059.417
[66,     1] loss: 1055.146
[67,     1] loss: 1063.504
[68,     1] loss: 1001.935
[69,     1] loss: 1043.446
[70,     1] loss: 1011.164
[71,     1] loss: 1115.362
[72,     1] loss: 1501.119
[73,     1] loss: 994.479
[74,     1] loss: 1205.702
[75,     1] loss: 1086.950
[76,     1] loss: 1096.961
[77,     1] loss: 1093.796
[78,     1] loss: 1007.921
[79,     1] loss: 1080.411
[80,     1] loss: 1005.061
[81,     1] loss: 955.623
[82,     1] loss: 1022.633
[83,     1] loss: 1037.886
[84,     1] loss: 955.261
[85,     1] loss: 865.168
[86,     1] loss: 932.514
[87,     1] loss: 944.263
[88,     1] loss: 1080.543
[89,     1] loss: 904.087
[90,     1] loss: 850.204
[91,     1] loss: 879.235
[92,     1] loss: 887.944
[93,     1] loss: 1355.713
[94,     1] loss: 1164.155
Early stopping applied (best metric=0.42700186371803284)
Finished Training
Total time taken: 15.978847026824951
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1504.482
[2,     1] loss: 1501.309
[3,     1] loss: 1507.562
[4,     1] loss: 1495.295
[5,     1] loss: 1498.309
[6,     1] loss: 1497.191
[7,     1] loss: 1471.620
[8,     1] loss: 1463.993
[9,     1] loss: 1414.129
[10,     1] loss: 1366.344
[11,     1] loss: 1371.787
[12,     1] loss: 1321.930
[13,     1] loss: 1328.227
[14,     1] loss: 1215.279
[15,     1] loss: 1288.879
[16,     1] loss: 1230.156
[17,     1] loss: 1248.795
[18,     1] loss: 1218.151
[19,     1] loss: 1215.921
[20,     1] loss: 1158.767
[21,     1] loss: 1204.663
[22,     1] loss: 1252.881
[23,     1] loss: 1130.475
[24,     1] loss: 1197.089
[25,     1] loss: 1187.950
[26,     1] loss: 1109.092
[27,     1] loss: 1141.586
[28,     1] loss: 1097.310
[29,     1] loss: 1150.218
[30,     1] loss: 1071.840
[31,     1] loss: 1086.969
[32,     1] loss: 1087.276
[33,     1] loss: 1177.894
[34,     1] loss: 1080.264
[35,     1] loss: 1127.156
[36,     1] loss: 1069.876
[37,     1] loss: 1054.442
[38,     1] loss: 1056.812
[39,     1] loss: 1038.197
[40,     1] loss: 1046.160
[41,     1] loss: 1005.179
[42,     1] loss: 941.740
[43,     1] loss: 944.843
[44,     1] loss: 1008.989
[45,     1] loss: 995.793
[46,     1] loss: 937.264
[47,     1] loss: 843.935
[48,     1] loss: 908.586
[49,     1] loss: 1108.188
[50,     1] loss: 1522.685
[51,     1] loss: 945.176
[52,     1] loss: 1152.973
[53,     1] loss: 1137.937
[54,     1] loss: 1066.280
[55,     1] loss: 1154.379
[56,     1] loss: 1108.459
[57,     1] loss: 1082.619
[58,     1] loss: 1079.300
[59,     1] loss: 1018.330
[60,     1] loss: 1027.880
[61,     1] loss: 1045.006
[62,     1] loss: 1030.129
[63,     1] loss: 1057.952
[64,     1] loss: 996.691
[65,     1] loss: 1076.010
[66,     1] loss: 957.374
[67,     1] loss: 1027.253
[68,     1] loss: 924.488
[69,     1] loss: 916.952
[70,     1] loss: 903.872
[71,     1] loss: 978.121
[72,     1] loss: 955.630
[73,     1] loss: 1078.394
[74,     1] loss: 842.440
[75,     1] loss: 1024.406
[76,     1] loss: 1043.058
[77,     1] loss: 943.038
[78,     1] loss: 1065.035
[79,     1] loss: 870.504
[80,     1] loss: 985.265
[81,     1] loss: 907.729
[82,     1] loss: 1023.017
[83,     1] loss: 902.098
[84,     1] loss: 879.476
[85,     1] loss: 880.614
[86,     1] loss: 822.183
[87,     1] loss: 810.482
[88,     1] loss: 1082.849
[89,     1] loss: 1918.821
[90,     1] loss: 1002.402
[91,     1] loss: 1175.916
[92,     1] loss: 1170.254
[93,     1] loss: 1145.375
[94,     1] loss: 1189.059
[95,     1] loss: 1157.698
[96,     1] loss: 1101.705
[97,     1] loss: 1154.591
[98,     1] loss: 1109.034
[99,     1] loss: 1039.689
[100,     1] loss: 1147.682
[101,     1] loss: 1101.498
[102,     1] loss: 1102.978
[103,     1] loss: 1110.679
[104,     1] loss: 1072.730
[105,     1] loss: 1046.089
[106,     1] loss: 1066.841
[107,     1] loss: 1025.680
[108,     1] loss: 1011.372
[109,     1] loss: 1026.593
[110,     1] loss: 964.574
[111,     1] loss: 957.357
[112,     1] loss: 960.103
[113,     1] loss: 1052.594
[114,     1] loss: 1447.928
[115,     1] loss: 1042.767
[116,     1] loss: 1181.543
[117,     1] loss: 1164.360
[118,     1] loss: 1141.320
[119,     1] loss: 1109.041
[120,     1] loss: 1048.677
[121,     1] loss: 1024.729
[122,     1] loss: 1078.250
[123,     1] loss: 1050.834
[124,     1] loss: 973.921
[125,     1] loss: 1007.487
[126,     1] loss: 996.730
[127,     1] loss: 987.417
[128,     1] loss: 1005.336
Early stopping applied (best metric=0.411803662776947)
Finished Training
Total time taken: 20.462477445602417
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1503.729
[2,     1] loss: 1503.965
[3,     1] loss: 1501.513
[4,     1] loss: 1508.351
[5,     1] loss: 1499.095
[6,     1] loss: 1505.588
[7,     1] loss: 1503.778
[8,     1] loss: 1498.617
[9,     1] loss: 1497.095
[10,     1] loss: 1497.065
[11,     1] loss: 1488.758
[12,     1] loss: 1476.604
[13,     1] loss: 1446.852
[14,     1] loss: 1410.892
[15,     1] loss: 1354.672
[16,     1] loss: 1352.811
[17,     1] loss: 1359.821
[18,     1] loss: 1277.781
[19,     1] loss: 1245.554
[20,     1] loss: 1231.106
[21,     1] loss: 1277.792
[22,     1] loss: 1207.202
[23,     1] loss: 1190.820
[24,     1] loss: 1168.042
[25,     1] loss: 1189.225
[26,     1] loss: 1245.372
[27,     1] loss: 1136.617
[28,     1] loss: 1116.711
[29,     1] loss: 1108.925
[30,     1] loss: 1199.634
[31,     1] loss: 1345.493
[32,     1] loss: 1135.588
[33,     1] loss: 1146.794
[34,     1] loss: 1236.007
[35,     1] loss: 1210.269
[36,     1] loss: 1160.211
[37,     1] loss: 1166.136
[38,     1] loss: 1151.206
[39,     1] loss: 1093.906
[40,     1] loss: 1123.582
[41,     1] loss: 1110.182
[42,     1] loss: 1119.073
[43,     1] loss: 1078.621
[44,     1] loss: 1054.449
[45,     1] loss: 1021.466
[46,     1] loss: 1013.734
[47,     1] loss: 979.076
[48,     1] loss: 924.474
[49,     1] loss: 930.662
[50,     1] loss: 997.530
[51,     1] loss: 1440.332
[52,     1] loss: 1774.596
[53,     1] loss: 1143.443
[54,     1] loss: 1244.543
[55,     1] loss: 1298.412
[56,     1] loss: 1362.516
[57,     1] loss: 1330.959
[58,     1] loss: 1343.609
[59,     1] loss: 1293.325
[60,     1] loss: 1308.212
[61,     1] loss: 1302.821
[62,     1] loss: 1260.987
[63,     1] loss: 1235.670
[64,     1] loss: 1203.754
[65,     1] loss: 1137.417
[66,     1] loss: 1163.401
[67,     1] loss: 1157.904
[68,     1] loss: 1205.072
[69,     1] loss: 1149.065
[70,     1] loss: 1069.136
[71,     1] loss: 1116.738
[72,     1] loss: 1128.605
[73,     1] loss: 1171.656
[74,     1] loss: 1096.118
[75,     1] loss: 1086.536
[76,     1] loss: 1068.210
[77,     1] loss: 1102.406
[78,     1] loss: 1200.087
[79,     1] loss: 961.603
[80,     1] loss: 1104.246
[81,     1] loss: 1079.214
[82,     1] loss: 1070.454
[83,     1] loss: 1015.275
[84,     1] loss: 957.023
[85,     1] loss: 1137.713
[86,     1] loss: 1025.314
[87,     1] loss: 953.596
[88,     1] loss: 993.472
[89,     1] loss: 996.871
[90,     1] loss: 1268.352
[91,     1] loss: 1123.959
[92,     1] loss: 1008.451
[93,     1] loss: 1127.672
[94,     1] loss: 1015.742
[95,     1] loss: 1115.918
[96,     1] loss: 951.231
[97,     1] loss: 1103.952
Early stopping applied (best metric=0.36969485878944397)
Finished Training
Total time taken: 16.683531522750854
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1507.851
[2,     1] loss: 1504.848
[3,     1] loss: 1508.332
[4,     1] loss: 1501.310
[5,     1] loss: 1500.106
[6,     1] loss: 1501.784
[7,     1] loss: 1501.079
[8,     1] loss: 1495.452
[9,     1] loss: 1487.002
[10,     1] loss: 1469.464
[11,     1] loss: 1434.705
[12,     1] loss: 1394.741
[13,     1] loss: 1337.338
[14,     1] loss: 1338.125
[15,     1] loss: 1239.208
[16,     1] loss: 1245.386
[17,     1] loss: 1237.359
[18,     1] loss: 1242.129
[19,     1] loss: 1293.248
[20,     1] loss: 1202.055
[21,     1] loss: 1235.513
[22,     1] loss: 1162.450
[23,     1] loss: 1205.449
[24,     1] loss: 1099.486
[25,     1] loss: 1086.519
[26,     1] loss: 1071.096
[27,     1] loss: 1057.484
[28,     1] loss: 1088.707
[29,     1] loss: 1201.826
[30,     1] loss: 1243.115
[31,     1] loss: 1006.889
[32,     1] loss: 1096.631
[33,     1] loss: 1031.753
[34,     1] loss: 1005.629
[35,     1] loss: 1019.397
[36,     1] loss: 997.323
[37,     1] loss: 932.116
[38,     1] loss: 944.254
[39,     1] loss: 934.991
[40,     1] loss: 919.279
[41,     1] loss: 1166.091
[42,     1] loss: 966.741
[43,     1] loss: 929.043
[44,     1] loss: 975.220
[45,     1] loss: 873.712
[46,     1] loss: 908.508
[47,     1] loss: 814.607
[48,     1] loss: 784.278
[49,     1] loss: 848.631
[50,     1] loss: 1175.469
[51,     1] loss: 1255.971
[52,     1] loss: 921.281
[53,     1] loss: 1062.252
[54,     1] loss: 923.801
[55,     1] loss: 1076.749
[56,     1] loss: 1094.411
[57,     1] loss: 999.655
[58,     1] loss: 1066.127
[59,     1] loss: 1004.598
[60,     1] loss: 907.320
[61,     1] loss: 943.921
[62,     1] loss: 845.806
[63,     1] loss: 957.159
[64,     1] loss: 824.928
[65,     1] loss: 783.572
[66,     1] loss: 841.304
[67,     1] loss: 834.422
[68,     1] loss: 782.791
[69,     1] loss: 704.857
Early stopping applied (best metric=0.40604230761528015)
Finished Training
Total time taken: 9.996845006942749
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1511.737
[2,     1] loss: 1502.831
[3,     1] loss: 1504.545
[4,     1] loss: 1503.289
[5,     1] loss: 1507.572
[6,     1] loss: 1503.625
[7,     1] loss: 1506.951
[8,     1] loss: 1501.262
[9,     1] loss: 1501.400
[10,     1] loss: 1503.861
[11,     1] loss: 1502.087
[12,     1] loss: 1497.487
[13,     1] loss: 1495.986
[14,     1] loss: 1490.138
[15,     1] loss: 1480.478
[16,     1] loss: 1460.247
[17,     1] loss: 1428.032
[18,     1] loss: 1391.894
[19,     1] loss: 1369.780
[20,     1] loss: 1323.690
[21,     1] loss: 1279.051
[22,     1] loss: 1272.845
[23,     1] loss: 1252.792
[24,     1] loss: 1370.345
[25,     1] loss: 1256.276
[26,     1] loss: 1242.499
[27,     1] loss: 1220.240
[28,     1] loss: 1225.798
[29,     1] loss: 1225.313
[30,     1] loss: 1161.201
[31,     1] loss: 1205.401
[32,     1] loss: 1184.120
[33,     1] loss: 1216.616
[34,     1] loss: 1173.517
[35,     1] loss: 1126.636
[36,     1] loss: 1086.299
[37,     1] loss: 1145.956
[38,     1] loss: 1185.935
[39,     1] loss: 1078.411
[40,     1] loss: 1096.775
[41,     1] loss: 1071.362
[42,     1] loss: 1006.551
[43,     1] loss: 1126.208
[44,     1] loss: 1172.053
[45,     1] loss: 1073.930
[46,     1] loss: 1153.155
[47,     1] loss: 1039.615
[48,     1] loss: 1129.735
[49,     1] loss: 1046.285
[50,     1] loss: 1037.831
[51,     1] loss: 1016.486
[52,     1] loss: 1099.238
[53,     1] loss: 989.001
[54,     1] loss: 996.661
[55,     1] loss: 970.234
[56,     1] loss: 963.461
[57,     1] loss: 879.476
[58,     1] loss: 1087.628
[59,     1] loss: 1889.940
[60,     1] loss: 948.202
[61,     1] loss: 1342.478
[62,     1] loss: 1117.038
[63,     1] loss: 1226.390
[64,     1] loss: 1333.888
[65,     1] loss: 1258.154
[66,     1] loss: 1200.618
[67,     1] loss: 1180.133
[68,     1] loss: 1214.708
[69,     1] loss: 1183.385
[70,     1] loss: 1135.400
[71,     1] loss: 1174.344
[72,     1] loss: 1126.329
[73,     1] loss: 1125.060
[74,     1] loss: 1148.045
[75,     1] loss: 1099.754
[76,     1] loss: 1065.816
[77,     1] loss: 1085.890
[78,     1] loss: 1018.188
[79,     1] loss: 1042.442
[80,     1] loss: 955.900
[81,     1] loss: 988.514
[82,     1] loss: 1060.501
[83,     1] loss: 1202.626
[84,     1] loss: 1087.392
[85,     1] loss: 958.726
[86,     1] loss: 1032.298
Early stopping applied (best metric=0.41183140873908997)
Finished Training
Total time taken: 12.506758451461792
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1513.116
[2,     1] loss: 1508.110
[3,     1] loss: 1509.171
[4,     1] loss: 1507.228
[5,     1] loss: 1503.780
[6,     1] loss: 1503.211
[7,     1] loss: 1503.788
[8,     1] loss: 1503.567
[9,     1] loss: 1504.575
[10,     1] loss: 1502.656
[11,     1] loss: 1502.094
[12,     1] loss: 1499.163
[13,     1] loss: 1491.021
[14,     1] loss: 1489.462
[15,     1] loss: 1469.625
[16,     1] loss: 1436.562
[17,     1] loss: 1412.169
[18,     1] loss: 1356.745
[19,     1] loss: 1339.274
[20,     1] loss: 1323.373
[21,     1] loss: 1269.350
[22,     1] loss: 1307.031
[23,     1] loss: 1293.816
[24,     1] loss: 1261.009
[25,     1] loss: 1253.538
[26,     1] loss: 1180.594
[27,     1] loss: 1262.346
[28,     1] loss: 1200.174
[29,     1] loss: 1197.617
[30,     1] loss: 1154.317
[31,     1] loss: 1189.135
[32,     1] loss: 1149.389
[33,     1] loss: 1155.221
[34,     1] loss: 1125.297
[35,     1] loss: 1104.162
[36,     1] loss: 1038.060
[37,     1] loss: 1045.657
[38,     1] loss: 1042.983
[39,     1] loss: 1059.183
[40,     1] loss: 1021.982
[41,     1] loss: 919.960
[42,     1] loss: 965.285
[43,     1] loss: 969.102
[44,     1] loss: 1051.265
[45,     1] loss: 2682.117
[46,     1] loss: 1144.144
[47,     1] loss: 1274.783
[48,     1] loss: 1352.140
[49,     1] loss: 1320.298
[50,     1] loss: 1331.667
[51,     1] loss: 1316.811
[52,     1] loss: 1288.412
[53,     1] loss: 1294.003
[54,     1] loss: 1323.905
[55,     1] loss: 1299.940
[56,     1] loss: 1298.088
[57,     1] loss: 1273.943
[58,     1] loss: 1273.878
[59,     1] loss: 1276.079
[60,     1] loss: 1296.789
[61,     1] loss: 1245.951
[62,     1] loss: 1203.542
[63,     1] loss: 1239.683
[64,     1] loss: 1201.127
[65,     1] loss: 1163.797
[66,     1] loss: 1084.900
[67,     1] loss: 1185.764
[68,     1] loss: 1114.167
[69,     1] loss: 1092.545
[70,     1] loss: 1138.456
[71,     1] loss: 1288.315
[72,     1] loss: 1055.423
[73,     1] loss: 1176.028
[74,     1] loss: 1067.557
[75,     1] loss: 1123.316
[76,     1] loss: 1053.406
[77,     1] loss: 1138.461
[78,     1] loss: 1063.782
[79,     1] loss: 955.121
[80,     1] loss: 1054.708
[81,     1] loss: 1162.519
[82,     1] loss: 960.893
[83,     1] loss: 1046.602
[84,     1] loss: 1030.232
[85,     1] loss: 1000.641
[86,     1] loss: 1150.361
[87,     1] loss: 1016.898
[88,     1] loss: 1019.243
[89,     1] loss: 952.099
[90,     1] loss: 1009.664
Early stopping applied (best metric=0.37210676074028015)
Finished Training
Total time taken: 12.273783445358276
{'Hydroxylation-K Validation Accuracy': 0.7170212765957447, 'Hydroxylation-K Validation Sensitivity': 0.6674074074074074, 'Hydroxylation-K Validation Specificity': 0.7298245614035088, 'Hydroxylation-K Validation Precision': 0.3965687199510729, 'Hydroxylation-K AUC ROC': 0.7867446393762183, 'Hydroxylation-K AUC PR': 0.5805595779410907, 'Hydroxylation-K MCC': 0.3407540644392856, 'Hydroxylation-K F1': 0.48798858525818767, 'Validation Loss (Hydroxylation-K)': 0.47288065354029335, 'Hydroxylation-P Validation Accuracy': 0.7642038475204305, 'Hydroxylation-P Validation Sensitivity': 0.7934391534391535, 'Hydroxylation-P Validation Specificity': 0.7578682228540077, 'Hydroxylation-P Validation Precision': 0.42292398190401825, 'Hydroxylation-P AUC ROC': 0.8415017393892898, 'Hydroxylation-P AUC PR': 0.5693912671355686, 'Hydroxylation-P MCC': 0.4499535684377355, 'Hydroxylation-P F1': 0.5476817251452198, 'Validation Loss (Hydroxylation-P)': 0.3799443344275157, 'Validation Loss (total)': 0.8528249780337016, 'TimeToTrain': 16.750068521499635}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006816412171918187,
 'learning_rate_Hydroxylation-K': 0.0027066678098848058,
 'learning_rate_Hydroxylation-P': 0.003476212513812147,
 'log_base': 2.8120852825367444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 201945714,
 'sample_weights': [2.8283510396567455, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9761483911365452,
 'weight_decay_Hydroxylation-K': 8.284353500869376,
 'weight_decay_Hydroxylation-P': 3.007435804144889}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.104
[2,     1] loss: 1248.488
[3,     1] loss: 1255.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007331042564047501,
 'learning_rate_Hydroxylation-K': 0.0016412167805026259,
 'learning_rate_Hydroxylation-P': 0.007127585814556287,
 'log_base': 1.9742757165995244,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4127829445,
 'sample_weights': [1.6146635837631038, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3195768514853885,
 'weight_decay_Hydroxylation-K': 1.307095532020138,
 'weight_decay_Hydroxylation-P': 7.361088085909743}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1424.515
[2,     1] loss: 1436.012
[3,     1] loss: 1426.559
[4,     1] loss: 1426.622
[5,     1] loss: 1418.677
[6,     1] loss: 1416.511
[7,     1] loss: 1417.073
[8,     1] loss: 1405.113
[9,     1] loss: 1395.799
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004739098581359007,
 'learning_rate_Hydroxylation-K': 0.004562882388608877,
 'learning_rate_Hydroxylation-P': 0.009611753468135144,
 'log_base': 2.252937316579288,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1882476962,
 'sample_weights': [2.4543357945140714, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.677355704664109,
 'weight_decay_Hydroxylation-K': 2.096978077007129,
 'weight_decay_Hydroxylation-P': 4.586930953581384}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1340.517
[2,     1] loss: 1346.891
[3,     1] loss: 1339.780
[4,     1] loss: 1346.148
[5,     1] loss: 1340.340
[6,     1] loss: 1338.287
[7,     1] loss: 1337.273
[8,     1] loss: 1328.374
[9,     1] loss: 1310.672
[10,     1] loss: 1281.557
[11,     1] loss: 1245.198
[12,     1] loss: 1241.579
[13,     1] loss: 1189.891
[14,     1] loss: 1156.122
[15,     1] loss: 1115.167
[16,     1] loss: 1095.911
[17,     1] loss: 1086.591
[18,     1] loss: 1077.383
[19,     1] loss: 1106.078
[20,     1] loss: 1121.556
[21,     1] loss: 1096.797
[22,     1] loss: 1020.631
[23,     1] loss: 1006.118
[24,     1] loss: 1084.358
[25,     1] loss: 986.788
[26,     1] loss: 960.891
[27,     1] loss: 993.333
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009628104182810186,
 'learning_rate_Hydroxylation-K': 0.001454351753780138,
 'learning_rate_Hydroxylation-P': 0.002552346995734248,
 'log_base': 2.3768217045878504,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3491407397,
 'sample_weights': [2.0553700328395794, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2879843708333274,
 'weight_decay_Hydroxylation-K': 2.8820938584136093,
 'weight_decay_Hydroxylation-P': 2.15535033727902}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.510
[2,     1] loss: 1335.595
[3,     1] loss: 1314.548
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004077554920988865,
 'learning_rate_Hydroxylation-K': 0.004439736082745296,
 'learning_rate_Hydroxylation-P': 0.003498483618068191,
 'log_base': 1.2875442836915822,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1055659325,
 'sample_weights': [1.9282885476413334, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.71809104693688,
 'weight_decay_Hydroxylation-K': 0.6829961358009662,
 'weight_decay_Hydroxylation-P': 7.268007362559328}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2307.514
[2,     1] loss: 2302.040
[3,     1] loss: 2300.477
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026482486762654817,
 'learning_rate_Hydroxylation-K': 0.003869287937309834,
 'learning_rate_Hydroxylation-P': 9.896149722661758e-05,
 'log_base': 1.265346688162728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2341325044,
 'sample_weights': [6.605462641299498, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.681286749712911,
 'weight_decay_Hydroxylation-K': 3.6225008488321198,
 'weight_decay_Hydroxylation-P': 9.129676874820289}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2389.878
[2,     1] loss: 2399.564
[3,     1] loss: 2398.885
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008019472153570684,
 'learning_rate_Hydroxylation-K': 0.006911024453822396,
 'learning_rate_Hydroxylation-P': 0.0013017223779433456,
 'log_base': 1.0995526527616988,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3075104801,
 'sample_weights': [7.093564831282415, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.852811175748584,
 'weight_decay_Hydroxylation-K': 0.9920350220214211,
 'weight_decay_Hydroxylation-P': 4.2722120241734975}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5728.348
[2,     1] loss: 5742.463
[3,     1] loss: 5753.407
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004676334475891008,
 'learning_rate_Hydroxylation-K': 0.009446150372132426,
 'learning_rate_Hydroxylation-P': 0.00016732043126963257,
 'log_base': 1.3828145973834425,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2044250067,
 'sample_weights': [17.590969708678177, 2.1989559229337705],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.291372517573687,
 'weight_decay_Hydroxylation-K': 4.206256822088673,
 'weight_decay_Hydroxylation-P': 1.5788946341457732}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1998.891
[2,     1] loss: 1988.426
[3,     1] loss: 2002.098
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007890000892721561,
 'learning_rate_Hydroxylation-K': 0.003354915087150879,
 'learning_rate_Hydroxylation-P': 0.001243968934342518,
 'log_base': 1.0804379221874454,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1644435751,
 'sample_weights': [5.150678982843426, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.944475496636583,
 'weight_decay_Hydroxylation-K': 2.6432865025845653,
 'weight_decay_Hydroxylation-P': 9.569694273583199}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7029.062
[2,     1] loss: 7044.173
[3,     1] loss: 7016.991
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008914034497446956,
 'learning_rate_Hydroxylation-K': 0.004539885767680706,
 'learning_rate_Hydroxylation-P': 0.006760455923597235,
 'log_base': 2.73575434079538,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3800880997,
 'sample_weights': [21.578388438318544, 2.697402465561499],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5575021695347147,
 'weight_decay_Hydroxylation-K': 0.16078186755693813,
 'weight_decay_Hydroxylation-P': 4.559058667907749}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.630
[2,     1] loss: 1260.736
[3,     1] loss: 1266.302
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008692091364096469,
 'learning_rate_Hydroxylation-K': 0.009743684056214072,
 'learning_rate_Hydroxylation-P': 0.0006500814586570464,
 'log_base': 1.1532256159526506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 413302922,
 'sample_weights': [1.6588147761061534, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.916728578899244,
 'weight_decay_Hydroxylation-K': 1.8983125920274124,
 'weight_decay_Hydroxylation-P': 0.7298729268553881}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3802.659
[2,     1] loss: 3839.013
[3,     1] loss: 3829.717
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005898304093786055,
 'learning_rate_Hydroxylation-K': 0.005728342919439139,
 'learning_rate_Hydroxylation-P': 0.004454779985365356,
 'log_base': 1.5928034295316187,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3716366520,
 'sample_weights': [11.710221619129939, 1.4638340929863267],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7131359641487567,
 'weight_decay_Hydroxylation-K': 3.496878195595676,
 'weight_decay_Hydroxylation-P': 9.347135115941205}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1667.029
[2,     1] loss: 1672.077
[3,     1] loss: 1664.301
[4,     1] loss: 1666.274
[5,     1] loss: 1670.368
[6,     1] loss: 1662.790
[7,     1] loss: 1666.603
[8,     1] loss: 1663.140
[9,     1] loss: 1663.911
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006615350719361874,
 'learning_rate_Hydroxylation-K': 0.004890155073472921,
 'learning_rate_Hydroxylation-P': 0.0022009729715950473,
 'log_base': 1.3470486599816802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2965829666,
 'sample_weights': [3.5863777249607667, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.391537257524464,
 'weight_decay_Hydroxylation-K': 6.170350200239852,
 'weight_decay_Hydroxylation-P': 9.30222547026614}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2099.833
[2,     1] loss: 2097.993
[3,     1] loss: 2092.891
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005827234385086165,
 'learning_rate_Hydroxylation-K': 0.009799380894453062,
 'learning_rate_Hydroxylation-P': 0.00877525627605784,
 'log_base': 1.2011575105751744,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1327577583,
 'sample_weights': [5.603737385821465, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.717226244680365,
 'weight_decay_Hydroxylation-K': 0.21280306647056935,
 'weight_decay_Hydroxylation-P': 9.70430203170634}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2943.214
[2,     1] loss: 2974.386
[3,     1] loss: 2957.544
[4,     1] loss: 2969.064
[5,     1] loss: 2945.076
[6,     1] loss: 2954.244
[7,     1] loss: 2961.373
[8,     1] loss: 2963.173
[9,     1] loss: 2961.247
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00739966319025891,
 'learning_rate_Hydroxylation-K': 0.00738074190606939,
 'learning_rate_Hydroxylation-P': 0.0011919630242425623,
 'log_base': 1.3420158116438943,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2653280618,
 'sample_weights': [9.10842085867776, 1.1385964689531596],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.191585984628782,
 'weight_decay_Hydroxylation-K': 7.349295600637559,
 'weight_decay_Hydroxylation-P': 4.145876444135034}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2105.171
[2,     1] loss: 2106.957
[3,     1] loss: 2102.768
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0072507856149643946,
 'learning_rate_Hydroxylation-K': 0.0057864845494462885,
 'learning_rate_Hydroxylation-P': 0.0023408673848055576,
 'log_base': 2.718282750436636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2135153462,
 'sample_weights': [5.675042119302117, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2818652742445136,
 'weight_decay_Hydroxylation-K': 2.2746668825564065,
 'weight_decay_Hydroxylation-P': 4.234188999880153}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.936
[2,     1] loss: 1265.052
[3,     1] loss: 1267.655
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009586897123536425,
 'learning_rate_Hydroxylation-K': 0.008342289935431547,
 'learning_rate_Hydroxylation-P': 0.001173344273803981,
 'log_base': 1.369139694309913,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2906012564,
 'sample_weights': [1.6694425811879086, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.6465717344964546,
 'weight_decay_Hydroxylation-K': 4.296630149466269,
 'weight_decay_Hydroxylation-P': 3.0906352796212477}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2026.934
[2,     1] loss: 2035.879
[3,     1] loss: 2027.555
[4,     1] loss: 2012.248
[5,     1] loss: 2043.352
[6,     1] loss: 2024.295
[7,     1] loss: 2029.558
[8,     1] loss: 2022.107
[9,     1] loss: 2030.989
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037563975671387265,
 'learning_rate_Hydroxylation-K': 0.003965225910268481,
 'learning_rate_Hydroxylation-P': 0.005548576046876194,
 'log_base': 2.2173180628751568,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2640792242,
 'sample_weights': [5.313608206923527, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.424483631546281,
 'weight_decay_Hydroxylation-K': 0.6074066339517911,
 'weight_decay_Hydroxylation-P': 9.528789369786299}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.350
[2,     1] loss: 1349.775
[3,     1] loss: 1355.051
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003971030797938557,
 'learning_rate_Hydroxylation-K': 0.001762336301983461,
 'learning_rate_Hydroxylation-P': 0.008393687643575795,
 'log_base': 2.9554757124829223,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3619109163,
 'sample_weights': [2.0965044978523495, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.039942298479264,
 'weight_decay_Hydroxylation-K': 6.400276649982486,
 'weight_decay_Hydroxylation-P': 0.20070128433201861}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.188
[2,     1] loss: 1233.502
[3,     1] loss: 1233.762
[4,     1] loss: 1235.405
[5,     1] loss: 1228.759
[6,     1] loss: 1233.399
[7,     1] loss: 1227.916
[8,     1] loss: 1221.246
[9,     1] loss: 1208.235
[10,     1] loss: 1182.510
[11,     1] loss: 1137.709
[12,     1] loss: 1112.274
[13,     1] loss: 1096.185
[14,     1] loss: 1097.538
[15,     1] loss: 1050.312
[16,     1] loss: 1037.794
[17,     1] loss: 1024.211
[18,     1] loss: 1045.493
[19,     1] loss: 1004.384
[20,     1] loss: 1027.622
[21,     1] loss: 999.612
[22,     1] loss: 996.199
[23,     1] loss: 962.742
[24,     1] loss: 930.240
[25,     1] loss: 1017.072
[26,     1] loss: 927.170
[27,     1] loss: 957.066
[28,     1] loss: 960.176
[29,     1] loss: 912.144
[30,     1] loss: 911.681
[31,     1] loss: 922.531
[32,     1] loss: 906.761
[33,     1] loss: 861.981
[34,     1] loss: 900.458
[35,     1] loss: 876.461
[36,     1] loss: 856.625
[37,     1] loss: 901.625
[38,     1] loss: 865.130
[39,     1] loss: 837.977
[40,     1] loss: 852.687
[41,     1] loss: 853.522
[42,     1] loss: 933.018
[43,     1] loss: 858.138
[44,     1] loss: 870.751
[45,     1] loss: 855.341
[46,     1] loss: 794.821
[47,     1] loss: 855.226
[48,     1] loss: 784.785
[49,     1] loss: 829.850
[50,     1] loss: 743.636
[51,     1] loss: 754.211
[52,     1] loss: 803.342
[53,     1] loss: 756.541
[54,     1] loss: 710.107
[55,     1] loss: 719.189
[56,     1] loss: 720.865
[57,     1] loss: 734.527
[58,     1] loss: 845.465
[59,     1] loss: 1482.012
[60,     1] loss: 799.161
[61,     1] loss: 976.872
[62,     1] loss: 1009.065
[63,     1] loss: 970.472
[64,     1] loss: 976.211
[65,     1] loss: 963.250
[66,     1] loss: 955.565
[67,     1] loss: 897.209
[68,     1] loss: 884.983
[69,     1] loss: 890.517
[70,     1] loss: 880.463
[71,     1] loss: 804.335
[72,     1] loss: 769.730
[73,     1] loss: 730.534
[74,     1] loss: 770.769
[75,     1] loss: 756.728
[76,     1] loss: 734.543
[77,     1] loss: 682.891
[78,     1] loss: 712.665
[79,     1] loss: 706.406
[80,     1] loss: 629.221
[81,     1] loss: 610.424
[82,     1] loss: 635.214
[83,     1] loss: 617.888
[84,     1] loss: 608.487
[85,     1] loss: 610.749
[86,     1] loss: 627.342
[87,     1] loss: 672.093
[88,     1] loss: 702.585
[89,     1] loss: 588.550
[90,     1] loss: 580.311
[91,     1] loss: 744.007
[92,     1] loss: 553.745
[93,     1] loss: 635.267
[94,     1] loss: 684.116
[95,     1] loss: 556.042
[96,     1] loss: 799.477
[97,     1] loss: 672.129
[98,     1] loss: 706.727
[99,     1] loss: 598.616
[100,     1] loss: 654.852
[101,     1] loss: 546.675
[102,     1] loss: 637.230
[103,     1] loss: 481.324
[104,     1] loss: 634.874
[105,     1] loss: 666.570
[106,     1] loss: 561.517
[107,     1] loss: 643.329
[108,     1] loss: 524.146
[109,     1] loss: 579.111
[110,     1] loss: 483.206
[111,     1] loss: 455.454
[112,     1] loss: 444.012
[113,     1] loss: 438.570
[114,     1] loss: 633.664
[115,     1] loss: 818.773
[116,     1] loss: 453.638
[117,     1] loss: 706.954
[118,     1] loss: 559.025
[119,     1] loss: 621.327
[120,     1] loss: 553.657
[121,     1] loss: 522.927
[122,     1] loss: 498.000
[123,     1] loss: 541.589
[124,     1] loss: 473.927
[125,     1] loss: 601.317
[126,     1] loss: 693.225
[127,     1] loss: 526.449
[128,     1] loss: 736.656
[129,     1] loss: 515.936
[130,     1] loss: 584.165
[131,     1] loss: 547.503
[132,     1] loss: 468.753
[133,     1] loss: 472.333
[134,     1] loss: 469.993
[135,     1] loss: 393.353
[136,     1] loss: 444.684
[137,     1] loss: 597.744
[138,     1] loss: 513.948
[139,     1] loss: 396.381
[140,     1] loss: 500.889
[141,     1] loss: 427.444
[142,     1] loss: 504.731
[143,     1] loss: 599.440
[144,     1] loss: 341.357
[145,     1] loss: 826.906
[146,     1] loss: 1502.477
[147,     1] loss: 982.955
[148,     1] loss: 886.967
[149,     1] loss: 1041.992
[150,     1] loss: 1013.646
[151,     1] loss: 865.734
[152,     1] loss: 874.443
[153,     1] loss: 842.839
[154,     1] loss: 754.349
[155,     1] loss: 884.347
[156,     1] loss: 785.873
[157,     1] loss: 824.324
[158,     1] loss: 733.534
[159,     1] loss: 782.144
[160,     1] loss: 712.790
[161,     1] loss: 703.749
[162,     1] loss: 694.526
[163,     1] loss: 634.962
[164,     1] loss: 605.681
[165,     1] loss: 628.577
[166,     1] loss: 617.663
[167,     1] loss: 601.238
[168,     1] loss: 563.335
[169,     1] loss: 547.497
[170,     1] loss: 650.244
[171,     1] loss: 633.091
[172,     1] loss: 530.235
[173,     1] loss: 643.653
[174,     1] loss: 550.642
[175,     1] loss: 570.049
[176,     1] loss: 509.338
[177,     1] loss: 489.188
[178,     1] loss: 636.985
[179,     1] loss: 462.361
[180,     1] loss: 474.589
[181,     1] loss: 565.524
[182,     1] loss: 453.099
[183,     1] loss: 487.195
Early stopping applied (best metric=0.3014748990535736)
Finished Training
Total time taken: 26.683505535125732
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.056
[2,     1] loss: 1231.089
[3,     1] loss: 1234.376
[4,     1] loss: 1233.684
[5,     1] loss: 1230.553
[6,     1] loss: 1225.745
[7,     1] loss: 1224.202
[8,     1] loss: 1207.987
[9,     1] loss: 1182.082
[10,     1] loss: 1151.130
[11,     1] loss: 1127.940
[12,     1] loss: 1082.509
[13,     1] loss: 1076.844
[14,     1] loss: 1069.046
[15,     1] loss: 1026.634
[16,     1] loss: 1026.266
[17,     1] loss: 996.976
[18,     1] loss: 974.402
[19,     1] loss: 969.534
[20,     1] loss: 1012.026
[21,     1] loss: 1017.897
[22,     1] loss: 1009.118
[23,     1] loss: 1029.170
[24,     1] loss: 954.855
[25,     1] loss: 953.576
[26,     1] loss: 935.902
[27,     1] loss: 967.766
[28,     1] loss: 878.101
[29,     1] loss: 936.016
[30,     1] loss: 901.443
[31,     1] loss: 856.348
[32,     1] loss: 868.797
[33,     1] loss: 894.339
[34,     1] loss: 978.809
[35,     1] loss: 878.382
[36,     1] loss: 908.768
[37,     1] loss: 916.701
[38,     1] loss: 869.522
[39,     1] loss: 826.897
[40,     1] loss: 863.220
[41,     1] loss: 817.793
[42,     1] loss: 764.393
[43,     1] loss: 843.534
[44,     1] loss: 832.461
[45,     1] loss: 764.119
[46,     1] loss: 824.198
[47,     1] loss: 731.501
[48,     1] loss: 782.245
[49,     1] loss: 786.548
[50,     1] loss: 680.209
[51,     1] loss: 791.188
[52,     1] loss: 1054.656
[53,     1] loss: 732.451
[54,     1] loss: 871.769
[55,     1] loss: 759.206
[56,     1] loss: 806.535
[57,     1] loss: 805.597
[58,     1] loss: 723.050
[59,     1] loss: 888.525
[60,     1] loss: 699.425
[61,     1] loss: 787.822
[62,     1] loss: 653.677
[63,     1] loss: 735.458
[64,     1] loss: 640.578
[65,     1] loss: 680.724
[66,     1] loss: 608.786
[67,     1] loss: 614.063
[68,     1] loss: 632.600
[69,     1] loss: 539.209
[70,     1] loss: 560.823
[71,     1] loss: 758.546
[72,     1] loss: 867.048
[73,     1] loss: 727.794
[74,     1] loss: 813.362
[75,     1] loss: 671.386
[76,     1] loss: 774.593
[77,     1] loss: 829.928
[78,     1] loss: 688.407
[79,     1] loss: 709.277
[80,     1] loss: 713.209
[81,     1] loss: 645.814
[82,     1] loss: 646.802
[83,     1] loss: 544.401
[84,     1] loss: 680.845
[85,     1] loss: 538.364
[86,     1] loss: 523.107
[87,     1] loss: 583.514
[88,     1] loss: 525.328
[89,     1] loss: 570.185
[90,     1] loss: 594.578
[91,     1] loss: 469.234
[92,     1] loss: 707.745
[93,     1] loss: 953.269
[94,     1] loss: 636.756
[95,     1] loss: 726.186
[96,     1] loss: 785.171
[97,     1] loss: 647.127
[98,     1] loss: 671.477
[99,     1] loss: 631.294
[100,     1] loss: 566.331
[101,     1] loss: 622.276
[102,     1] loss: 599.206
[103,     1] loss: 461.414
[104,     1] loss: 554.124
[105,     1] loss: 497.122
[106,     1] loss: 483.180
[107,     1] loss: 531.995
[108,     1] loss: 417.278
[109,     1] loss: 389.823
[110,     1] loss: 421.757
[111,     1] loss: 401.322
[112,     1] loss: 362.257
[113,     1] loss: 392.881
[114,     1] loss: 725.834
[115,     1] loss: 1103.718
[116,     1] loss: 912.698
[117,     1] loss: 972.574
[118,     1] loss: 906.420
[119,     1] loss: 961.894
[120,     1] loss: 956.107
[121,     1] loss: 926.399
[122,     1] loss: 793.560
[123,     1] loss: 796.899
[124,     1] loss: 841.343
[125,     1] loss: 755.812
[126,     1] loss: 737.926
[127,     1] loss: 738.592
[128,     1] loss: 701.585
[129,     1] loss: 702.642
[130,     1] loss: 676.634
[131,     1] loss: 664.328
[132,     1] loss: 576.970
Early stopping applied (best metric=0.3443980813026428)
Finished Training
Total time taken: 19.729053258895874
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.354
[2,     1] loss: 1232.402
[3,     1] loss: 1234.143
[4,     1] loss: 1238.881
[5,     1] loss: 1236.252
[6,     1] loss: 1226.038
[7,     1] loss: 1221.526
[8,     1] loss: 1208.901
[9,     1] loss: 1189.124
[10,     1] loss: 1159.516
[11,     1] loss: 1092.036
[12,     1] loss: 1087.906
[13,     1] loss: 1058.021
[14,     1] loss: 1036.467
[15,     1] loss: 1099.610
[16,     1] loss: 1037.506
[17,     1] loss: 1044.491
[18,     1] loss: 1030.077
[19,     1] loss: 981.370
[20,     1] loss: 1006.861
[21,     1] loss: 1024.814
[22,     1] loss: 1001.533
[23,     1] loss: 962.263
[24,     1] loss: 995.033
[25,     1] loss: 954.791
[26,     1] loss: 951.938
[27,     1] loss: 943.207
[28,     1] loss: 1005.920
[29,     1] loss: 941.943
[30,     1] loss: 958.562
[31,     1] loss: 954.156
[32,     1] loss: 932.550
[33,     1] loss: 912.550
[34,     1] loss: 909.609
[35,     1] loss: 861.337
[36,     1] loss: 907.321
[37,     1] loss: 825.491
[38,     1] loss: 865.854
[39,     1] loss: 851.948
[40,     1] loss: 810.423
[41,     1] loss: 815.733
[42,     1] loss: 848.402
[43,     1] loss: 860.043
[44,     1] loss: 793.675
[45,     1] loss: 749.334
[46,     1] loss: 794.130
[47,     1] loss: 781.061
[48,     1] loss: 744.546
[49,     1] loss: 756.524
[50,     1] loss: 735.403
[51,     1] loss: 878.238
[52,     1] loss: 794.882
[53,     1] loss: 723.738
[54,     1] loss: 730.605
[55,     1] loss: 667.805
[56,     1] loss: 652.011
[57,     1] loss: 788.602
[58,     1] loss: 738.566
[59,     1] loss: 796.112
[60,     1] loss: 703.165
[61,     1] loss: 657.970
[62,     1] loss: 703.702
[63,     1] loss: 682.690
[64,     1] loss: 589.277
[65,     1] loss: 630.870
[66,     1] loss: 654.349
[67,     1] loss: 628.049
[68,     1] loss: 770.912
[69,     1] loss: 1387.625
[70,     1] loss: 723.759
[71,     1] loss: 963.490
[72,     1] loss: 927.645
[73,     1] loss: 882.287
[74,     1] loss: 922.280
[75,     1] loss: 914.353
[76,     1] loss: 864.109
[77,     1] loss: 789.587
[78,     1] loss: 817.602
[79,     1] loss: 757.917
[80,     1] loss: 724.757
[81,     1] loss: 754.912
[82,     1] loss: 721.280
[83,     1] loss: 650.091
[84,     1] loss: 656.504
[85,     1] loss: 650.885
[86,     1] loss: 643.887
[87,     1] loss: 646.710
[88,     1] loss: 601.561
[89,     1] loss: 566.363
[90,     1] loss: 546.780
[91,     1] loss: 568.379
[92,     1] loss: 558.201
[93,     1] loss: 621.977
[94,     1] loss: 1099.902
[95,     1] loss: 603.946
[96,     1] loss: 1084.275
[97,     1] loss: 727.056
[98,     1] loss: 897.737
[99,     1] loss: 901.462
[100,     1] loss: 725.328
[101,     1] loss: 719.540
[102,     1] loss: 709.227
[103,     1] loss: 636.749
[104,     1] loss: 634.943
[105,     1] loss: 591.362
[106,     1] loss: 677.118
[107,     1] loss: 576.158
[108,     1] loss: 602.049
[109,     1] loss: 572.912
[110,     1] loss: 497.449
[111,     1] loss: 598.381
[112,     1] loss: 549.653
[113,     1] loss: 475.377
[114,     1] loss: 525.261
[115,     1] loss: 554.833
Early stopping applied (best metric=0.29627057909965515)
Finished Training
Total time taken: 16.14127540588379
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.567
[2,     1] loss: 1234.158
[3,     1] loss: 1230.970
[4,     1] loss: 1232.641
[5,     1] loss: 1232.468
[6,     1] loss: 1227.562
[7,     1] loss: 1223.405
[8,     1] loss: 1216.579
[9,     1] loss: 1192.180
[10,     1] loss: 1150.564
[11,     1] loss: 1102.090
[12,     1] loss: 1080.096
[13,     1] loss: 1019.478
[14,     1] loss: 1038.462
[15,     1] loss: 1020.133
[16,     1] loss: 970.622
[17,     1] loss: 1016.292
[18,     1] loss: 982.497
[19,     1] loss: 958.060
[20,     1] loss: 932.154
[21,     1] loss: 981.551
[22,     1] loss: 910.943
[23,     1] loss: 927.357
[24,     1] loss: 920.820
[25,     1] loss: 944.815
[26,     1] loss: 880.721
[27,     1] loss: 941.834
[28,     1] loss: 878.936
[29,     1] loss: 855.877
[30,     1] loss: 889.862
[31,     1] loss: 831.977
[32,     1] loss: 856.892
[33,     1] loss: 867.305
[34,     1] loss: 828.849
[35,     1] loss: 777.148
[36,     1] loss: 820.269
[37,     1] loss: 891.412
[38,     1] loss: 805.178
[39,     1] loss: 754.144
[40,     1] loss: 836.531
[41,     1] loss: 739.131
[42,     1] loss: 764.027
[43,     1] loss: 828.701
[44,     1] loss: 813.042
[45,     1] loss: 774.408
[46,     1] loss: 771.936
[47,     1] loss: 679.939
[48,     1] loss: 722.027
[49,     1] loss: 759.164
[50,     1] loss: 690.295
[51,     1] loss: 703.375
[52,     1] loss: 855.632
[53,     1] loss: 708.753
[54,     1] loss: 750.202
[55,     1] loss: 692.520
[56,     1] loss: 773.147
[57,     1] loss: 679.881
[58,     1] loss: 736.606
[59,     1] loss: 639.835
[60,     1] loss: 735.686
[61,     1] loss: 584.358
[62,     1] loss: 736.313
[63,     1] loss: 547.434
[64,     1] loss: 672.376
[65,     1] loss: 604.590
[66,     1] loss: 642.296
[67,     1] loss: 756.885
[68,     1] loss: 598.510
[69,     1] loss: 611.936
[70,     1] loss: 534.531
[71,     1] loss: 605.694
[72,     1] loss: 570.362
[73,     1] loss: 522.547
[74,     1] loss: 522.907
[75,     1] loss: 540.436
[76,     1] loss: 531.745
[77,     1] loss: 600.430
[78,     1] loss: 510.176
[79,     1] loss: 508.067
[80,     1] loss: 527.953
[81,     1] loss: 463.528
[82,     1] loss: 449.534
[83,     1] loss: 564.708
[84,     1] loss: 786.660
[85,     1] loss: 1327.696
[86,     1] loss: 663.435
[87,     1] loss: 757.627
[88,     1] loss: 901.412
[89,     1] loss: 886.420
[90,     1] loss: 814.195
[91,     1] loss: 825.268
Early stopping applied (best metric=0.39988401532173157)
Finished Training
Total time taken: 13.919183731079102
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1232.538
[2,     1] loss: 1237.252
[3,     1] loss: 1247.981
[4,     1] loss: 1235.002
[5,     1] loss: 1231.427
[6,     1] loss: 1233.164
[7,     1] loss: 1228.777
[8,     1] loss: 1229.999
[9,     1] loss: 1224.329
[10,     1] loss: 1219.123
[11,     1] loss: 1187.781
[12,     1] loss: 1160.175
[13,     1] loss: 1114.006
[14,     1] loss: 1088.777
[15,     1] loss: 1074.098
[16,     1] loss: 1033.812
[17,     1] loss: 1063.524
[18,     1] loss: 1059.308
[19,     1] loss: 1050.861
[20,     1] loss: 986.963
[21,     1] loss: 1039.398
[22,     1] loss: 952.452
[23,     1] loss: 1006.629
[24,     1] loss: 966.255
[25,     1] loss: 944.726
[26,     1] loss: 963.716
[27,     1] loss: 926.222
[28,     1] loss: 922.496
[29,     1] loss: 923.366
[30,     1] loss: 976.648
[31,     1] loss: 851.580
[32,     1] loss: 908.400
[33,     1] loss: 861.575
[34,     1] loss: 882.658
[35,     1] loss: 915.234
[36,     1] loss: 871.307
[37,     1] loss: 882.760
[38,     1] loss: 834.455
[39,     1] loss: 829.741
[40,     1] loss: 780.748
[41,     1] loss: 780.373
[42,     1] loss: 722.689
[43,     1] loss: 734.300
[44,     1] loss: 676.149
[45,     1] loss: 720.971
[46,     1] loss: 710.724
[47,     1] loss: 868.021
[48,     1] loss: 1611.138
[49,     1] loss: 772.190
[50,     1] loss: 1015.786
[51,     1] loss: 954.043
[52,     1] loss: 919.448
[53,     1] loss: 962.785
[54,     1] loss: 964.276
[55,     1] loss: 963.926
[56,     1] loss: 892.789
[57,     1] loss: 859.201
[58,     1] loss: 885.645
[59,     1] loss: 842.740
[60,     1] loss: 784.435
[61,     1] loss: 829.694
[62,     1] loss: 775.665
[63,     1] loss: 775.628
[64,     1] loss: 720.202
[65,     1] loss: 721.854
[66,     1] loss: 766.394
[67,     1] loss: 708.498
[68,     1] loss: 706.700
[69,     1] loss: 658.070
[70,     1] loss: 671.701
[71,     1] loss: 643.153
[72,     1] loss: 653.131
[73,     1] loss: 589.189
[74,     1] loss: 572.810
[75,     1] loss: 594.766
[76,     1] loss: 576.635
[77,     1] loss: 658.121
[78,     1] loss: 609.346
[79,     1] loss: 544.122
[80,     1] loss: 526.093
[81,     1] loss: 562.250
[82,     1] loss: 872.009
[83,     1] loss: 677.835
[84,     1] loss: 592.449
[85,     1] loss: 544.941
[86,     1] loss: 575.430
[87,     1] loss: 544.109
[88,     1] loss: 567.597
[89,     1] loss: 510.776
[90,     1] loss: 649.590
[91,     1] loss: 925.377
[92,     1] loss: 508.465
[93,     1] loss: 972.498
[94,     1] loss: 721.612
[95,     1] loss: 896.534
[96,     1] loss: 791.825
[97,     1] loss: 664.097
[98,     1] loss: 759.824
[99,     1] loss: 769.988
[100,     1] loss: 678.345
[101,     1] loss: 727.320
[102,     1] loss: 739.495
[103,     1] loss: 607.490
[104,     1] loss: 708.987
[105,     1] loss: 618.417
[106,     1] loss: 623.785
[107,     1] loss: 644.272
[108,     1] loss: 508.446
[109,     1] loss: 622.377
[110,     1] loss: 502.193
[111,     1] loss: 574.279
[112,     1] loss: 478.362
[113,     1] loss: 491.233
[114,     1] loss: 528.501
[115,     1] loss: 428.503
[116,     1] loss: 509.707
[117,     1] loss: 569.633
[118,     1] loss: 388.612
[119,     1] loss: 525.449
[120,     1] loss: 587.372
[121,     1] loss: 373.383
[122,     1] loss: 559.574
[123,     1] loss: 596.145
[124,     1] loss: 415.069
[125,     1] loss: 602.714
[126,     1] loss: 535.046
[127,     1] loss: 533.433
[128,     1] loss: 494.498
[129,     1] loss: 397.202
[130,     1] loss: 519.932
[131,     1] loss: 363.387
[132,     1] loss: 580.731
[133,     1] loss: 887.308
[134,     1] loss: 464.991
[135,     1] loss: 681.587
[136,     1] loss: 486.625
[137,     1] loss: 542.619
[138,     1] loss: 437.194
[139,     1] loss: 559.783
[140,     1] loss: 748.182
[141,     1] loss: 416.977
[142,     1] loss: 875.088
[143,     1] loss: 630.481
[144,     1] loss: 835.636
[145,     1] loss: 651.560
[146,     1] loss: 696.709
[147,     1] loss: 541.361
[148,     1] loss: 632.055
[149,     1] loss: 496.903
[150,     1] loss: 509.726
[151,     1] loss: 491.528
[152,     1] loss: 535.052
[153,     1] loss: 449.469
[154,     1] loss: 446.833
[155,     1] loss: 381.842
[156,     1] loss: 394.936
[157,     1] loss: 559.414
[158,     1] loss: 1270.287
[159,     1] loss: 514.102
[160,     1] loss: 691.680
[161,     1] loss: 594.388
[162,     1] loss: 702.048
[163,     1] loss: 545.443
[164,     1] loss: 672.616
[165,     1] loss: 478.577
[166,     1] loss: 506.339
[167,     1] loss: 453.455
[168,     1] loss: 537.980
[169,     1] loss: 482.353
[170,     1] loss: 474.470
[171,     1] loss: 421.762
[172,     1] loss: 415.247
[173,     1] loss: 469.170
[174,     1] loss: 369.252
[175,     1] loss: 486.065
[176,     1] loss: 491.372
[177,     1] loss: 364.585
[178,     1] loss: 506.310
[179,     1] loss: 424.736
[180,     1] loss: 379.696
[181,     1] loss: 455.272
[182,     1] loss: 416.195
[183,     1] loss: 319.484
[184,     1] loss: 473.071
[185,     1] loss: 500.100
[186,     1] loss: 314.735
Early stopping applied (best metric=0.37738853693008423)
Finished Training
Total time taken: 28.137349605560303
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.078
[2,     1] loss: 1235.283
[3,     1] loss: 1232.872
[4,     1] loss: 1232.280
[5,     1] loss: 1234.773
[6,     1] loss: 1233.222
[7,     1] loss: 1230.661
[8,     1] loss: 1227.176
[9,     1] loss: 1219.094
[10,     1] loss: 1204.868
[11,     1] loss: 1183.793
[12,     1] loss: 1148.484
[13,     1] loss: 1110.324
[14,     1] loss: 1076.462
[15,     1] loss: 1066.627
[16,     1] loss: 1069.247
[17,     1] loss: 1000.586
[18,     1] loss: 1008.384
[19,     1] loss: 1016.932
[20,     1] loss: 1014.510
[21,     1] loss: 943.897
[22,     1] loss: 971.124
[23,     1] loss: 961.091
[24,     1] loss: 928.505
[25,     1] loss: 925.205
[26,     1] loss: 886.091
[27,     1] loss: 875.079
[28,     1] loss: 889.143
[29,     1] loss: 950.946
[30,     1] loss: 862.898
[31,     1] loss: 873.793
[32,     1] loss: 869.900
[33,     1] loss: 848.253
[34,     1] loss: 838.435
[35,     1] loss: 841.944
[36,     1] loss: 888.383
[37,     1] loss: 806.766
[38,     1] loss: 764.584
[39,     1] loss: 801.677
[40,     1] loss: 829.231
[41,     1] loss: 792.227
[42,     1] loss: 772.218
[43,     1] loss: 715.261
[44,     1] loss: 785.695
[45,     1] loss: 818.852
[46,     1] loss: 716.345
[47,     1] loss: 729.489
[48,     1] loss: 778.213
[49,     1] loss: 721.419
[50,     1] loss: 809.499
[51,     1] loss: 708.353
[52,     1] loss: 670.933
[53,     1] loss: 764.609
[54,     1] loss: 627.751
[55,     1] loss: 685.303
[56,     1] loss: 654.701
[57,     1] loss: 611.768
[58,     1] loss: 661.900
[59,     1] loss: 767.196
[60,     1] loss: 676.412
[61,     1] loss: 618.456
[62,     1] loss: 577.934
[63,     1] loss: 569.884
[64,     1] loss: 581.482
[65,     1] loss: 527.750
[66,     1] loss: 604.560
[67,     1] loss: 639.881
[68,     1] loss: 679.329
[69,     1] loss: 991.649
[70,     1] loss: 618.239
[71,     1] loss: 842.312
[72,     1] loss: 683.662
[73,     1] loss: 704.393
[74,     1] loss: 763.017
[75,     1] loss: 603.865
[76,     1] loss: 679.323
[77,     1] loss: 591.846
[78,     1] loss: 608.104
[79,     1] loss: 590.681
[80,     1] loss: 602.229
[81,     1] loss: 589.771
[82,     1] loss: 569.191
[83,     1] loss: 531.815
[84,     1] loss: 573.957
[85,     1] loss: 524.477
[86,     1] loss: 446.517
[87,     1] loss: 533.897
[88,     1] loss: 556.757
[89,     1] loss: 421.561
[90,     1] loss: 542.955
[91,     1] loss: 565.515
[92,     1] loss: 435.313
[93,     1] loss: 615.060
[94,     1] loss: 773.306
[95,     1] loss: 444.615
[96,     1] loss: 855.303
[97,     1] loss: 548.585
[98,     1] loss: 806.887
[99,     1] loss: 620.052
[100,     1] loss: 578.080
[101,     1] loss: 736.158
[102,     1] loss: 494.119
[103,     1] loss: 698.354
[104,     1] loss: 414.325
[105,     1] loss: 555.816
[106,     1] loss: 436.654
[107,     1] loss: 570.239
[108,     1] loss: 420.805
[109,     1] loss: 499.591
[110,     1] loss: 370.072
[111,     1] loss: 454.429
[112,     1] loss: 353.231
[113,     1] loss: 552.008
[114,     1] loss: 841.387
[115,     1] loss: 450.277
[116,     1] loss: 698.183
[117,     1] loss: 479.117
[118,     1] loss: 549.138
[119,     1] loss: 526.487
[120,     1] loss: 635.159
[121,     1] loss: 429.789
[122,     1] loss: 519.889
[123,     1] loss: 462.812
[124,     1] loss: 565.636
[125,     1] loss: 462.344
[126,     1] loss: 450.993
[127,     1] loss: 713.883
[128,     1] loss: 478.393
[129,     1] loss: 509.114
[130,     1] loss: 450.498
[131,     1] loss: 526.202
[132,     1] loss: 397.042
[133,     1] loss: 547.648
[134,     1] loss: 464.616
[135,     1] loss: 349.461
[136,     1] loss: 364.421
[137,     1] loss: 378.067
[138,     1] loss: 290.749
[139,     1] loss: 314.071
[140,     1] loss: 321.400
[141,     1] loss: 376.093
[142,     1] loss: 419.107
[143,     1] loss: 906.607
[144,     1] loss: 612.088
[145,     1] loss: 491.488
[146,     1] loss: 556.507
[147,     1] loss: 569.227
[148,     1] loss: 507.934
[149,     1] loss: 376.467
[150,     1] loss: 419.950
[151,     1] loss: 393.170
[152,     1] loss: 397.052
[153,     1] loss: 623.888
[154,     1] loss: 704.531
[155,     1] loss: 557.969
[156,     1] loss: 569.139
[157,     1] loss: 485.236
[158,     1] loss: 561.904
[159,     1] loss: 518.658
[160,     1] loss: 443.661
[161,     1] loss: 444.828
[162,     1] loss: 328.129
[163,     1] loss: 391.593
[164,     1] loss: 367.484
[165,     1] loss: 398.484
[166,     1] loss: 379.290
[167,     1] loss: 418.163
[168,     1] loss: 440.887
[169,     1] loss: 425.402
[170,     1] loss: 347.650
[171,     1] loss: 484.861
[172,     1] loss: 328.293
[173,     1] loss: 431.196
[174,     1] loss: 590.902
[175,     1] loss: 636.600
[176,     1] loss: 357.700
[177,     1] loss: 579.138
[178,     1] loss: 380.094
[179,     1] loss: 427.451
[180,     1] loss: 358.413
[181,     1] loss: 302.025
[182,     1] loss: 404.038
[183,     1] loss: 482.692
[184,     1] loss: 557.620
[185,     1] loss: 343.588
[186,     1] loss: 490.887
[187,     1] loss: 377.876
[188,     1] loss: 436.071
[189,     1] loss: 458.582
[190,     1] loss: 307.372
[191,     1] loss: 332.273
[192,     1] loss: 285.449
[193,     1] loss: 297.319
[194,     1] loss: 309.726
[195,     1] loss: 324.881
[196,     1] loss: 292.074
[197,     1] loss: 283.018
[198,     1] loss: 246.753
[199,     1] loss: 252.680
[200,     1] loss: 281.460
Finished Training
Total time taken: 31.97517490386963
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.866
[2,     1] loss: 1235.031
[3,     1] loss: 1232.096
[4,     1] loss: 1233.852
[5,     1] loss: 1235.316
[6,     1] loss: 1231.192
[7,     1] loss: 1226.985
[8,     1] loss: 1217.062
[9,     1] loss: 1204.546
[10,     1] loss: 1181.343
[11,     1] loss: 1145.181
[12,     1] loss: 1088.410
[13,     1] loss: 1095.647
[14,     1] loss: 1048.894
[15,     1] loss: 1038.392
[16,     1] loss: 1039.193
[17,     1] loss: 1018.839
[18,     1] loss: 1036.945
[19,     1] loss: 1007.719
[20,     1] loss: 998.290
[21,     1] loss: 962.282
[22,     1] loss: 957.922
[23,     1] loss: 952.857
[24,     1] loss: 917.623
[25,     1] loss: 1012.657
[26,     1] loss: 934.369
[27,     1] loss: 899.467
[28,     1] loss: 926.407
[29,     1] loss: 889.046
[30,     1] loss: 880.675
[31,     1] loss: 880.811
[32,     1] loss: 858.940
[33,     1] loss: 815.900
[34,     1] loss: 883.397
[35,     1] loss: 850.967
[36,     1] loss: 1022.409
[37,     1] loss: 979.982
[38,     1] loss: 877.055
[39,     1] loss: 876.294
[40,     1] loss: 919.399
[41,     1] loss: 849.813
[42,     1] loss: 842.096
[43,     1] loss: 817.984
[44,     1] loss: 794.887
[45,     1] loss: 787.152
[46,     1] loss: 775.283
[47,     1] loss: 793.793
[48,     1] loss: 765.895
[49,     1] loss: 704.632
[50,     1] loss: 726.453
[51,     1] loss: 706.822
[52,     1] loss: 708.459
[53,     1] loss: 680.002
[54,     1] loss: 680.508
[55,     1] loss: 737.879
[56,     1] loss: 1115.838
[57,     1] loss: 1036.002
[58,     1] loss: 989.484
[59,     1] loss: 864.943
[60,     1] loss: 907.101
[61,     1] loss: 968.162
[62,     1] loss: 1021.197
[63,     1] loss: 938.582
[64,     1] loss: 887.068
[65,     1] loss: 876.346
[66,     1] loss: 870.799
[67,     1] loss: 846.124
[68,     1] loss: 836.150
[69,     1] loss: 811.824
[70,     1] loss: 789.831
[71,     1] loss: 773.983
[72,     1] loss: 726.131
[73,     1] loss: 747.687
[74,     1] loss: 722.728
[75,     1] loss: 706.115
[76,     1] loss: 714.903
[77,     1] loss: 730.410
[78,     1] loss: 658.742
[79,     1] loss: 630.008
[80,     1] loss: 643.288
[81,     1] loss: 661.251
[82,     1] loss: 603.216
[83,     1] loss: 653.444
[84,     1] loss: 578.579
[85,     1] loss: 587.701
[86,     1] loss: 577.024
[87,     1] loss: 497.294
[88,     1] loss: 536.126
[89,     1] loss: 856.990
[90,     1] loss: 1575.518
[91,     1] loss: 730.467
[92,     1] loss: 786.219
[93,     1] loss: 997.362
[94,     1] loss: 917.286
[95,     1] loss: 937.016
[96,     1] loss: 942.959
[97,     1] loss: 904.417
[98,     1] loss: 858.810
[99,     1] loss: 816.077
[100,     1] loss: 862.398
[101,     1] loss: 822.819
[102,     1] loss: 822.631
[103,     1] loss: 788.080
[104,     1] loss: 730.923
[105,     1] loss: 771.815
[106,     1] loss: 739.986
[107,     1] loss: 747.492
[108,     1] loss: 746.114
[109,     1] loss: 628.829
[110,     1] loss: 664.814
[111,     1] loss: 704.809
[112,     1] loss: 624.448
[113,     1] loss: 610.848
[114,     1] loss: 556.260
[115,     1] loss: 625.316
[116,     1] loss: 563.583
[117,     1] loss: 567.918
[118,     1] loss: 554.380
Early stopping applied (best metric=0.36225709319114685)
Finished Training
Total time taken: 21.135476112365723
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.622
[2,     1] loss: 1233.248
[3,     1] loss: 1233.718
[4,     1] loss: 1230.508
[5,     1] loss: 1235.041
[6,     1] loss: 1235.113
[7,     1] loss: 1227.598
[8,     1] loss: 1222.240
[9,     1] loss: 1214.114
[10,     1] loss: 1185.877
[11,     1] loss: 1160.405
[12,     1] loss: 1124.872
[13,     1] loss: 1096.240
[14,     1] loss: 1073.403
[15,     1] loss: 1048.938
[16,     1] loss: 1066.915
[17,     1] loss: 1029.844
[18,     1] loss: 1036.632
[19,     1] loss: 985.773
[20,     1] loss: 992.165
[21,     1] loss: 977.506
[22,     1] loss: 970.802
[23,     1] loss: 990.692
[24,     1] loss: 941.583
[25,     1] loss: 940.468
[26,     1] loss: 926.043
[27,     1] loss: 936.785
[28,     1] loss: 948.417
[29,     1] loss: 944.510
[30,     1] loss: 875.763
[31,     1] loss: 898.507
[32,     1] loss: 891.789
[33,     1] loss: 884.583
[34,     1] loss: 850.362
[35,     1] loss: 800.955
[36,     1] loss: 831.938
[37,     1] loss: 859.560
[38,     1] loss: 854.051
[39,     1] loss: 783.597
[40,     1] loss: 842.038
[41,     1] loss: 817.461
[42,     1] loss: 789.002
[43,     1] loss: 798.032
[44,     1] loss: 776.076
[45,     1] loss: 936.067
[46,     1] loss: 1183.591
[47,     1] loss: 804.430
[48,     1] loss: 913.470
[49,     1] loss: 862.544
[50,     1] loss: 878.488
[51,     1] loss: 869.512
[52,     1] loss: 869.552
[53,     1] loss: 832.158
[54,     1] loss: 774.497
[55,     1] loss: 839.032
[56,     1] loss: 869.957
[57,     1] loss: 786.095
[58,     1] loss: 771.851
[59,     1] loss: 735.116
[60,     1] loss: 766.237
[61,     1] loss: 776.466
[62,     1] loss: 726.146
[63,     1] loss: 720.924
[64,     1] loss: 703.459
[65,     1] loss: 697.935
[66,     1] loss: 673.697
[67,     1] loss: 649.845
[68,     1] loss: 668.714
[69,     1] loss: 669.207
[70,     1] loss: 655.916
[71,     1] loss: 671.239
[72,     1] loss: 622.785
[73,     1] loss: 721.280
[74,     1] loss: 727.500
[75,     1] loss: 631.197
[76,     1] loss: 570.185
[77,     1] loss: 669.234
[78,     1] loss: 626.201
[79,     1] loss: 525.763
[80,     1] loss: 717.826
[81,     1] loss: 727.096
[82,     1] loss: 583.464
[83,     1] loss: 639.261
[84,     1] loss: 567.579
[85,     1] loss: 575.968
[86,     1] loss: 608.207
[87,     1] loss: 600.276
[88,     1] loss: 449.673
[89,     1] loss: 540.633
[90,     1] loss: 682.552
[91,     1] loss: 563.555
[92,     1] loss: 474.001
[93,     1] loss: 513.445
[94,     1] loss: 457.926
[95,     1] loss: 522.584
[96,     1] loss: 626.102
[97,     1] loss: 549.654
[98,     1] loss: 466.135
[99,     1] loss: 413.536
[100,     1] loss: 490.405
[101,     1] loss: 486.082
[102,     1] loss: 424.920
[103,     1] loss: 492.606
[104,     1] loss: 682.675
Early stopping applied (best metric=0.38747143745422363)
Finished Training
Total time taken: 16.071749687194824
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.557
[2,     1] loss: 1235.046
[3,     1] loss: 1230.911
[4,     1] loss: 1231.458
[5,     1] loss: 1229.361
[6,     1] loss: 1226.326
[7,     1] loss: 1220.573
[8,     1] loss: 1212.791
[9,     1] loss: 1179.197
[10,     1] loss: 1148.558
[11,     1] loss: 1117.905
[12,     1] loss: 1062.562
[13,     1] loss: 1048.807
[14,     1] loss: 1035.728
[15,     1] loss: 1024.677
[16,     1] loss: 994.924
[17,     1] loss: 1017.373
[18,     1] loss: 963.232
[19,     1] loss: 976.479
[20,     1] loss: 956.710
[21,     1] loss: 933.608
[22,     1] loss: 941.833
[23,     1] loss: 906.254
[24,     1] loss: 910.262
[25,     1] loss: 893.007
[26,     1] loss: 899.566
[27,     1] loss: 915.350
[28,     1] loss: 917.574
[29,     1] loss: 894.581
[30,     1] loss: 879.625
[31,     1] loss: 887.966
[32,     1] loss: 838.704
[33,     1] loss: 879.233
[34,     1] loss: 795.574
[35,     1] loss: 872.627
[36,     1] loss: 786.018
[37,     1] loss: 861.226
[38,     1] loss: 747.233
[39,     1] loss: 798.659
[40,     1] loss: 791.535
[41,     1] loss: 867.441
[42,     1] loss: 779.795
[43,     1] loss: 761.117
[44,     1] loss: 842.159
[45,     1] loss: 707.541
[46,     1] loss: 744.697
[47,     1] loss: 675.641
[48,     1] loss: 749.352
[49,     1] loss: 669.838
[50,     1] loss: 676.257
[51,     1] loss: 924.589
[52,     1] loss: 870.933
[53,     1] loss: 733.485
[54,     1] loss: 723.809
[55,     1] loss: 780.574
[56,     1] loss: 710.164
[57,     1] loss: 715.588
[58,     1] loss: 737.204
[59,     1] loss: 630.377
[60,     1] loss: 640.536
[61,     1] loss: 610.499
[62,     1] loss: 677.284
[63,     1] loss: 609.177
[64,     1] loss: 625.084
[65,     1] loss: 685.829
[66,     1] loss: 567.904
[67,     1] loss: 639.894
[68,     1] loss: 657.902
[69,     1] loss: 531.932
[70,     1] loss: 648.245
[71,     1] loss: 571.185
[72,     1] loss: 619.741
[73,     1] loss: 535.398
[74,     1] loss: 567.496
[75,     1] loss: 546.240
[76,     1] loss: 520.973
[77,     1] loss: 773.964
[78,     1] loss: 817.927
[79,     1] loss: 600.715
[80,     1] loss: 707.353
[81,     1] loss: 753.899
[82,     1] loss: 609.352
[83,     1] loss: 641.825
[84,     1] loss: 603.718
[85,     1] loss: 513.965
[86,     1] loss: 654.340
[87,     1] loss: 510.950
[88,     1] loss: 647.279
[89,     1] loss: 473.271
[90,     1] loss: 627.291
[91,     1] loss: 521.510
[92,     1] loss: 506.090
[93,     1] loss: 413.231
[94,     1] loss: 422.012
[95,     1] loss: 444.655
[96,     1] loss: 399.238
[97,     1] loss: 408.699
[98,     1] loss: 407.243
[99,     1] loss: 397.051
[100,     1] loss: 386.281
[101,     1] loss: 409.841
[102,     1] loss: 437.010
[103,     1] loss: 651.484
[104,     1] loss: 679.284
[105,     1] loss: 575.617
[106,     1] loss: 588.239
[107,     1] loss: 487.951
[108,     1] loss: 555.233
[109,     1] loss: 445.673
[110,     1] loss: 469.394
[111,     1] loss: 693.992
[112,     1] loss: 633.145
[113,     1] loss: 515.034
[114,     1] loss: 570.884
[115,     1] loss: 609.749
[116,     1] loss: 526.689
[117,     1] loss: 565.532
[118,     1] loss: 434.583
[119,     1] loss: 515.707
[120,     1] loss: 371.931
[121,     1] loss: 465.803
Early stopping applied (best metric=0.4156763255596161)
Finished Training
Total time taken: 17.118351936340332
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1240.534
[2,     1] loss: 1240.938
[3,     1] loss: 1234.807
[4,     1] loss: 1237.805
[5,     1] loss: 1232.557
[6,     1] loss: 1234.312
[7,     1] loss: 1233.264
[8,     1] loss: 1232.344
[9,     1] loss: 1223.710
[10,     1] loss: 1221.785
[11,     1] loss: 1202.691
[12,     1] loss: 1179.598
[13,     1] loss: 1164.684
[14,     1] loss: 1127.719
[15,     1] loss: 1120.851
[16,     1] loss: 1077.247
[17,     1] loss: 1064.239
[18,     1] loss: 1077.099
[19,     1] loss: 1013.642
[20,     1] loss: 984.996
[21,     1] loss: 1044.735
[22,     1] loss: 1013.868
[23,     1] loss: 1017.966
[24,     1] loss: 987.629
[25,     1] loss: 1027.308
[26,     1] loss: 979.249
[27,     1] loss: 963.578
[28,     1] loss: 964.599
[29,     1] loss: 914.115
[30,     1] loss: 933.687
[31,     1] loss: 931.197
[32,     1] loss: 890.109
[33,     1] loss: 916.702
[34,     1] loss: 987.270
[35,     1] loss: 908.492
[36,     1] loss: 871.140
[37,     1] loss: 886.311
[38,     1] loss: 859.175
[39,     1] loss: 858.734
[40,     1] loss: 856.507
[41,     1] loss: 883.231
[42,     1] loss: 777.040
[43,     1] loss: 919.880
[44,     1] loss: 816.281
[45,     1] loss: 876.653
[46,     1] loss: 809.176
[47,     1] loss: 777.703
[48,     1] loss: 749.629
[49,     1] loss: 849.771
[50,     1] loss: 776.786
[51,     1] loss: 777.821
[52,     1] loss: 778.686
[53,     1] loss: 706.807
[54,     1] loss: 663.035
[55,     1] loss: 698.182
[56,     1] loss: 677.026
[57,     1] loss: 763.035
[58,     1] loss: 873.711
[59,     1] loss: 714.338
[60,     1] loss: 700.771
[61,     1] loss: 713.938
[62,     1] loss: 779.654
[63,     1] loss: 673.894
[64,     1] loss: 772.475
[65,     1] loss: 620.263
[66,     1] loss: 777.331
[67,     1] loss: 615.779
[68,     1] loss: 643.970
[69,     1] loss: 712.537
[70,     1] loss: 593.399
[71,     1] loss: 730.794
[72,     1] loss: 577.191
[73,     1] loss: 626.109
[74,     1] loss: 562.660
[75,     1] loss: 709.232
[76,     1] loss: 619.991
[77,     1] loss: 600.383
[78,     1] loss: 651.716
[79,     1] loss: 532.135
[80,     1] loss: 597.789
[81,     1] loss: 508.507
[82,     1] loss: 658.021
[83,     1] loss: 682.634
[84,     1] loss: 446.338
[85,     1] loss: 584.540
[86,     1] loss: 499.040
[87,     1] loss: 563.972
[88,     1] loss: 481.182
[89,     1] loss: 525.729
[90,     1] loss: 507.764
[91,     1] loss: 448.501
[92,     1] loss: 754.550
[93,     1] loss: 1049.750
[94,     1] loss: 548.651
[95,     1] loss: 744.021
[96,     1] loss: 784.512
[97,     1] loss: 671.485
[98,     1] loss: 703.420
[99,     1] loss: 702.636
Early stopping applied (best metric=0.34546545147895813)
Finished Training
Total time taken: 15.485842943191528
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.592
[2,     1] loss: 1234.278
[3,     1] loss: 1232.826
[4,     1] loss: 1229.990
[5,     1] loss: 1229.524
[6,     1] loss: 1230.966
[7,     1] loss: 1224.953
[8,     1] loss: 1214.392
[9,     1] loss: 1187.971
[10,     1] loss: 1144.550
[11,     1] loss: 1115.040
[12,     1] loss: 1090.329
[13,     1] loss: 1020.364
[14,     1] loss: 1037.627
[15,     1] loss: 1020.783
[16,     1] loss: 970.568
[17,     1] loss: 1015.215
[18,     1] loss: 1013.091
[19,     1] loss: 970.818
[20,     1] loss: 963.574
[21,     1] loss: 988.205
[22,     1] loss: 950.521
[23,     1] loss: 919.960
[24,     1] loss: 920.823
[25,     1] loss: 908.980
[26,     1] loss: 928.945
[27,     1] loss: 895.374
[28,     1] loss: 911.280
[29,     1] loss: 909.367
[30,     1] loss: 915.032
[31,     1] loss: 895.255
[32,     1] loss: 891.775
[33,     1] loss: 881.509
[34,     1] loss: 856.645
[35,     1] loss: 833.498
[36,     1] loss: 783.279
[37,     1] loss: 842.618
[38,     1] loss: 823.438
[39,     1] loss: 791.105
[40,     1] loss: 846.903
[41,     1] loss: 835.497
[42,     1] loss: 759.366
[43,     1] loss: 762.127
[44,     1] loss: 737.792
[45,     1] loss: 746.793
[46,     1] loss: 898.474
[47,     1] loss: 1180.206
[48,     1] loss: 809.868
[49,     1] loss: 936.641
[50,     1] loss: 889.618
[51,     1] loss: 877.066
[52,     1] loss: 872.522
[53,     1] loss: 886.485
[54,     1] loss: 827.327
[55,     1] loss: 802.584
[56,     1] loss: 836.997
[57,     1] loss: 852.588
[58,     1] loss: 779.365
[59,     1] loss: 767.850
[60,     1] loss: 701.706
[61,     1] loss: 740.842
[62,     1] loss: 716.144
[63,     1] loss: 733.321
[64,     1] loss: 718.865
[65,     1] loss: 666.586
[66,     1] loss: 726.034
[67,     1] loss: 665.215
[68,     1] loss: 688.356
[69,     1] loss: 723.131
[70,     1] loss: 622.118
[71,     1] loss: 627.734
[72,     1] loss: 694.653
[73,     1] loss: 626.064
[74,     1] loss: 599.506
[75,     1] loss: 702.445
[76,     1] loss: 803.866
[77,     1] loss: 745.453
[78,     1] loss: 584.783
[79,     1] loss: 711.728
[80,     1] loss: 627.142
[81,     1] loss: 688.536
[82,     1] loss: 604.567
[83,     1] loss: 582.655
[84,     1] loss: 562.273
[85,     1] loss: 632.436
[86,     1] loss: 527.687
[87,     1] loss: 622.629
[88,     1] loss: 809.572
[89,     1] loss: 465.183
[90,     1] loss: 755.773
[91,     1] loss: 563.393
[92,     1] loss: 746.826
[93,     1] loss: 553.447
[94,     1] loss: 705.619
[95,     1] loss: 524.609
[96,     1] loss: 554.755
[97,     1] loss: 502.328
[98,     1] loss: 636.876
[99,     1] loss: 493.725
[100,     1] loss: 468.048
[101,     1] loss: 560.779
[102,     1] loss: 439.729
[103,     1] loss: 490.192
[104,     1] loss: 442.471
[105,     1] loss: 368.793
[106,     1] loss: 450.286
[107,     1] loss: 394.893
[108,     1] loss: 407.535
[109,     1] loss: 496.860
[110,     1] loss: 796.921
[111,     1] loss: 670.382
[112,     1] loss: 507.268
[113,     1] loss: 645.790
[114,     1] loss: 540.029
[115,     1] loss: 566.842
[116,     1] loss: 511.716
[117,     1] loss: 549.706
[118,     1] loss: 433.434
[119,     1] loss: 774.289
[120,     1] loss: 835.544
[121,     1] loss: 614.306
[122,     1] loss: 700.498
[123,     1] loss: 709.429
[124,     1] loss: 542.482
[125,     1] loss: 602.427
[126,     1] loss: 502.212
[127,     1] loss: 578.113
[128,     1] loss: 472.305
[129,     1] loss: 467.252
[130,     1] loss: 413.957
[131,     1] loss: 585.537
[132,     1] loss: 591.164
[133,     1] loss: 450.305
[134,     1] loss: 525.200
[135,     1] loss: 395.465
[136,     1] loss: 452.711
[137,     1] loss: 359.917
[138,     1] loss: 495.760
[139,     1] loss: 496.543
[140,     1] loss: 341.699
[141,     1] loss: 551.105
[142,     1] loss: 634.602
[143,     1] loss: 368.073
[144,     1] loss: 513.235
[145,     1] loss: 331.782
[146,     1] loss: 539.338
[147,     1] loss: 427.999
[148,     1] loss: 348.069
[149,     1] loss: 405.413
[150,     1] loss: 323.109
[151,     1] loss: 333.424
[152,     1] loss: 437.379
[153,     1] loss: 529.792
[154,     1] loss: 931.883
[155,     1] loss: 440.786
[156,     1] loss: 846.204
[157,     1] loss: 580.548
[158,     1] loss: 663.945
[159,     1] loss: 476.760
[160,     1] loss: 812.785
[161,     1] loss: 648.877
[162,     1] loss: 563.573
[163,     1] loss: 531.178
[164,     1] loss: 682.411
[165,     1] loss: 496.069
[166,     1] loss: 548.304
[167,     1] loss: 424.782
[168,     1] loss: 552.230
[169,     1] loss: 477.471
[170,     1] loss: 465.597
[171,     1] loss: 481.350
[172,     1] loss: 383.113
[173,     1] loss: 440.503
[174,     1] loss: 337.300
[175,     1] loss: 396.931
[176,     1] loss: 533.575
[177,     1] loss: 490.646
[178,     1] loss: 389.007
Early stopping applied (best metric=0.3510526716709137)
Finished Training
Total time taken: 25.18763494491577
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.719
[2,     1] loss: 1233.284
[3,     1] loss: 1235.827
[4,     1] loss: 1228.776
[5,     1] loss: 1231.298
[6,     1] loss: 1226.776
[7,     1] loss: 1217.324
[8,     1] loss: 1197.969
[9,     1] loss: 1181.344
[10,     1] loss: 1125.899
[11,     1] loss: 1055.882
[12,     1] loss: 1082.364
[13,     1] loss: 1064.935
[14,     1] loss: 1014.882
[15,     1] loss: 1020.628
[16,     1] loss: 1005.633
[17,     1] loss: 1019.866
[18,     1] loss: 976.483
[19,     1] loss: 985.541
[20,     1] loss: 975.660
[21,     1] loss: 1004.198
[22,     1] loss: 954.280
[23,     1] loss: 881.843
[24,     1] loss: 933.445
[25,     1] loss: 874.814
[26,     1] loss: 910.301
[27,     1] loss: 922.546
[28,     1] loss: 821.013
[29,     1] loss: 847.755
[30,     1] loss: 862.973
[31,     1] loss: 872.083
[32,     1] loss: 822.920
[33,     1] loss: 812.486
[34,     1] loss: 786.007
[35,     1] loss: 816.412
[36,     1] loss: 861.319
[37,     1] loss: 1230.849
[38,     1] loss: 797.341
[39,     1] loss: 992.392
[40,     1] loss: 872.348
[41,     1] loss: 905.001
[42,     1] loss: 933.307
[43,     1] loss: 861.172
[44,     1] loss: 823.290
[45,     1] loss: 852.486
[46,     1] loss: 812.289
[47,     1] loss: 763.232
[48,     1] loss: 797.314
[49,     1] loss: 746.862
[50,     1] loss: 763.417
[51,     1] loss: 780.421
[52,     1] loss: 688.697
[53,     1] loss: 732.670
[54,     1] loss: 699.985
[55,     1] loss: 670.833
[56,     1] loss: 659.596
[57,     1] loss: 575.762
[58,     1] loss: 580.891
[59,     1] loss: 597.555
[60,     1] loss: 606.410
[61,     1] loss: 599.619
[62,     1] loss: 853.323
[63,     1] loss: 1343.514
[64,     1] loss: 825.149
[65,     1] loss: 1108.202
[66,     1] loss: 1034.329
[67,     1] loss: 990.685
[68,     1] loss: 1006.661
[69,     1] loss: 1019.279
[70,     1] loss: 979.190
[71,     1] loss: 968.449
[72,     1] loss: 898.776
[73,     1] loss: 904.389
[74,     1] loss: 862.551
[75,     1] loss: 868.050
[76,     1] loss: 912.808
[77,     1] loss: 897.732
[78,     1] loss: 880.003
[79,     1] loss: 809.599
[80,     1] loss: 833.935
[81,     1] loss: 833.424
[82,     1] loss: 820.810
[83,     1] loss: 826.384
[84,     1] loss: 813.004
Early stopping applied (best metric=0.38491109013557434)
Finished Training
Total time taken: 15.165491580963135
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.576
[2,     1] loss: 1239.843
[3,     1] loss: 1232.868
[4,     1] loss: 1236.386
[5,     1] loss: 1227.856
[6,     1] loss: 1229.263
[7,     1] loss: 1224.878
[8,     1] loss: 1217.259
[9,     1] loss: 1201.492
[10,     1] loss: 1188.067
[11,     1] loss: 1136.795
[12,     1] loss: 1106.121
[13,     1] loss: 1075.031
[14,     1] loss: 1034.375
[15,     1] loss: 1000.437
[16,     1] loss: 1004.667
[17,     1] loss: 992.666
[18,     1] loss: 995.392
[19,     1] loss: 1009.034
[20,     1] loss: 976.620
[21,     1] loss: 962.279
[22,     1] loss: 958.073
[23,     1] loss: 955.608
[24,     1] loss: 906.288
[25,     1] loss: 1006.822
[26,     1] loss: 900.476
[27,     1] loss: 930.078
[28,     1] loss: 881.127
[29,     1] loss: 918.229
[30,     1] loss: 852.368
[31,     1] loss: 846.066
[32,     1] loss: 878.620
[33,     1] loss: 873.270
[34,     1] loss: 851.963
[35,     1] loss: 819.570
[36,     1] loss: 776.382
[37,     1] loss: 823.076
[38,     1] loss: 772.377
[39,     1] loss: 730.821
[40,     1] loss: 780.560
[41,     1] loss: 791.306
[42,     1] loss: 862.829
[43,     1] loss: 756.515
[44,     1] loss: 803.085
[45,     1] loss: 723.586
[46,     1] loss: 777.449
[47,     1] loss: 832.097
[48,     1] loss: 724.551
[49,     1] loss: 718.519
[50,     1] loss: 737.260
[51,     1] loss: 725.466
[52,     1] loss: 685.837
[53,     1] loss: 667.742
[54,     1] loss: 623.663
[55,     1] loss: 632.505
[56,     1] loss: 662.711
[57,     1] loss: 1268.704
[58,     1] loss: 990.772
[59,     1] loss: 722.075
[60,     1] loss: 806.666
[61,     1] loss: 872.538
[62,     1] loss: 828.230
[63,     1] loss: 793.363
[64,     1] loss: 729.216
[65,     1] loss: 785.794
[66,     1] loss: 806.150
[67,     1] loss: 711.400
[68,     1] loss: 757.593
[69,     1] loss: 700.863
[70,     1] loss: 696.370
[71,     1] loss: 705.624
[72,     1] loss: 682.920
[73,     1] loss: 655.911
[74,     1] loss: 636.027
[75,     1] loss: 608.829
[76,     1] loss: 634.122
[77,     1] loss: 698.311
[78,     1] loss: 591.489
[79,     1] loss: 576.184
[80,     1] loss: 546.264
[81,     1] loss: 560.562
[82,     1] loss: 580.902
[83,     1] loss: 581.680
[84,     1] loss: 518.118
[85,     1] loss: 522.841
[86,     1] loss: 512.359
[87,     1] loss: 509.564
[88,     1] loss: 690.752
[89,     1] loss: 972.268
[90,     1] loss: 1326.817
[91,     1] loss: 995.655
[92,     1] loss: 878.080
[93,     1] loss: 972.733
[94,     1] loss: 1034.273
[95,     1] loss: 1073.231
[96,     1] loss: 997.439
[97,     1] loss: 898.170
Early stopping applied (best metric=0.4121515452861786)
Finished Training
Total time taken: 17.168872117996216
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.224
[2,     1] loss: 1233.785
[3,     1] loss: 1230.547
[4,     1] loss: 1234.428
[5,     1] loss: 1227.807
[6,     1] loss: 1222.519
[7,     1] loss: 1209.284
[8,     1] loss: 1179.082
[9,     1] loss: 1148.234
[10,     1] loss: 1114.831
[11,     1] loss: 1082.096
[12,     1] loss: 1071.858
[13,     1] loss: 1047.962
[14,     1] loss: 1064.118
[15,     1] loss: 1059.106
[16,     1] loss: 1019.084
[17,     1] loss: 1037.351
[18,     1] loss: 1035.840
[19,     1] loss: 986.459
[20,     1] loss: 991.427
[21,     1] loss: 982.492
[22,     1] loss: 988.395
[23,     1] loss: 980.805
[24,     1] loss: 903.431
[25,     1] loss: 992.662
[26,     1] loss: 936.089
[27,     1] loss: 936.083
[28,     1] loss: 899.241
[29,     1] loss: 926.857
[30,     1] loss: 1011.702
[31,     1] loss: 876.755
[32,     1] loss: 944.014
[33,     1] loss: 869.573
[34,     1] loss: 942.051
[35,     1] loss: 922.679
[36,     1] loss: 858.987
[37,     1] loss: 896.828
[38,     1] loss: 855.060
[39,     1] loss: 860.388
[40,     1] loss: 841.572
[41,     1] loss: 831.296
[42,     1] loss: 816.128
[43,     1] loss: 962.533
[44,     1] loss: 1059.976
[45,     1] loss: 840.316
[46,     1] loss: 886.940
[47,     1] loss: 939.507
[48,     1] loss: 878.375
[49,     1] loss: 874.150
[50,     1] loss: 897.220
[51,     1] loss: 832.986
[52,     1] loss: 828.724
[53,     1] loss: 858.990
[54,     1] loss: 822.587
[55,     1] loss: 806.831
[56,     1] loss: 787.906
[57,     1] loss: 796.496
[58,     1] loss: 725.456
[59,     1] loss: 764.743
[60,     1] loss: 706.971
[61,     1] loss: 697.074
[62,     1] loss: 676.236
[63,     1] loss: 655.434
[64,     1] loss: 705.457
[65,     1] loss: 812.482
[66,     1] loss: 769.059
[67,     1] loss: 686.945
[68,     1] loss: 707.101
[69,     1] loss: 709.522
[70,     1] loss: 662.357
[71,     1] loss: 671.425
[72,     1] loss: 638.113
[73,     1] loss: 634.847
[74,     1] loss: 643.753
[75,     1] loss: 641.149
[76,     1] loss: 698.674
[77,     1] loss: 751.996
[78,     1] loss: 742.794
[79,     1] loss: 582.630
[80,     1] loss: 688.258
[81,     1] loss: 645.925
[82,     1] loss: 709.127
[83,     1] loss: 602.271
[84,     1] loss: 613.591
[85,     1] loss: 593.959
[86,     1] loss: 564.504
[87,     1] loss: 511.977
[88,     1] loss: 610.985
[89,     1] loss: 1056.567
[90,     1] loss: 716.363
[91,     1] loss: 584.459
[92,     1] loss: 709.539
[93,     1] loss: 738.326
[94,     1] loss: 624.627
[95,     1] loss: 706.652
[96,     1] loss: 603.692
[97,     1] loss: 637.726
[98,     1] loss: 581.749
[99,     1] loss: 546.906
[100,     1] loss: 499.209
[101,     1] loss: 510.666
[102,     1] loss: 526.319
[103,     1] loss: 455.193
[104,     1] loss: 459.090
[105,     1] loss: 732.009
[106,     1] loss: 1132.582
[107,     1] loss: 507.870
[108,     1] loss: 1021.379
[109,     1] loss: 701.770
[110,     1] loss: 801.684
[111,     1] loss: 865.477
[112,     1] loss: 660.196
[113,     1] loss: 729.276
[114,     1] loss: 655.908
[115,     1] loss: 631.534
[116,     1] loss: 678.343
[117,     1] loss: 591.222
[118,     1] loss: 580.138
[119,     1] loss: 624.819
[120,     1] loss: 566.053
[121,     1] loss: 536.889
[122,     1] loss: 469.082
[123,     1] loss: 534.969
[124,     1] loss: 447.350
[125,     1] loss: 410.361
[126,     1] loss: 426.760
[127,     1] loss: 453.749
[128,     1] loss: 748.210
[129,     1] loss: 619.750
[130,     1] loss: 509.356
[131,     1] loss: 546.019
[132,     1] loss: 545.105
[133,     1] loss: 429.754
[134,     1] loss: 461.566
[135,     1] loss: 401.298
[136,     1] loss: 374.228
[137,     1] loss: 441.425
[138,     1] loss: 627.138
[139,     1] loss: 2003.606
[140,     1] loss: 695.350
[141,     1] loss: 1120.099
[142,     1] loss: 1104.918
[143,     1] loss: 1049.436
[144,     1] loss: 1067.622
[145,     1] loss: 1005.045
[146,     1] loss: 979.381
[147,     1] loss: 941.040
[148,     1] loss: 948.184
[149,     1] loss: 896.152
[150,     1] loss: 912.022
[151,     1] loss: 956.471
[152,     1] loss: 879.026
[153,     1] loss: 943.329
[154,     1] loss: 864.680
[155,     1] loss: 878.831
[156,     1] loss: 806.800
[157,     1] loss: 843.497
[158,     1] loss: 805.360
[159,     1] loss: 816.824
[160,     1] loss: 761.120
[161,     1] loss: 697.915
[162,     1] loss: 751.725
[163,     1] loss: 692.480
[164,     1] loss: 728.549
Early stopping applied (best metric=0.28831300139427185)
Finished Training
Total time taken: 24.904133558273315
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.582
[2,     1] loss: 1233.222
[3,     1] loss: 1234.786
[4,     1] loss: 1232.739
[5,     1] loss: 1229.059
[6,     1] loss: 1221.655
[7,     1] loss: 1205.654
[8,     1] loss: 1172.046
[9,     1] loss: 1138.635
[10,     1] loss: 1108.721
[11,     1] loss: 1058.498
[12,     1] loss: 1064.409
[13,     1] loss: 1082.219
[14,     1] loss: 1040.112
[15,     1] loss: 1013.307
[16,     1] loss: 1014.527
[17,     1] loss: 980.139
[18,     1] loss: 981.089
[19,     1] loss: 983.457
[20,     1] loss: 1011.391
[21,     1] loss: 978.318
[22,     1] loss: 933.344
[23,     1] loss: 971.216
[24,     1] loss: 976.652
[25,     1] loss: 1014.422
[26,     1] loss: 907.049
[27,     1] loss: 935.163
[28,     1] loss: 948.002
[29,     1] loss: 854.334
[30,     1] loss: 925.435
[31,     1] loss: 893.938
[32,     1] loss: 876.890
[33,     1] loss: 869.849
[34,     1] loss: 905.618
[35,     1] loss: 837.338
[36,     1] loss: 872.765
[37,     1] loss: 843.399
[38,     1] loss: 861.493
[39,     1] loss: 823.772
[40,     1] loss: 864.168
[41,     1] loss: 769.331
[42,     1] loss: 820.567
[43,     1] loss: 908.426
[44,     1] loss: 799.152
[45,     1] loss: 791.881
[46,     1] loss: 842.309
[47,     1] loss: 777.562
[48,     1] loss: 768.470
[49,     1] loss: 800.887
[50,     1] loss: 771.141
[51,     1] loss: 687.282
[52,     1] loss: 748.277
[53,     1] loss: 660.735
[54,     1] loss: 757.241
[55,     1] loss: 711.424
[56,     1] loss: 661.463
[57,     1] loss: 671.253
[58,     1] loss: 781.792
[59,     1] loss: 702.907
[60,     1] loss: 610.396
[61,     1] loss: 621.765
[62,     1] loss: 648.495
[63,     1] loss: 599.861
[64,     1] loss: 637.826
[65,     1] loss: 621.715
[66,     1] loss: 990.543
[67,     1] loss: 658.635
[68,     1] loss: 633.471
[69,     1] loss: 618.805
[70,     1] loss: 650.049
[71,     1] loss: 640.429
[72,     1] loss: 627.493
[73,     1] loss: 716.120
[74,     1] loss: 537.434
[75,     1] loss: 725.090
[76,     1] loss: 760.955
[77,     1] loss: 675.250
[78,     1] loss: 626.081
[79,     1] loss: 659.206
[80,     1] loss: 557.295
[81,     1] loss: 670.467
[82,     1] loss: 519.856
[83,     1] loss: 687.445
[84,     1] loss: 552.119
[85,     1] loss: 625.773
[86,     1] loss: 549.430
[87,     1] loss: 567.489
[88,     1] loss: 553.116
[89,     1] loss: 562.916
[90,     1] loss: 534.146
[91,     1] loss: 455.808
[92,     1] loss: 588.296
[93,     1] loss: 417.847
[94,     1] loss: 603.542
[95,     1] loss: 671.943
[96,     1] loss: 476.179
[97,     1] loss: 621.853
[98,     1] loss: 458.056
[99,     1] loss: 517.764
[100,     1] loss: 471.636
[101,     1] loss: 587.033
[102,     1] loss: 450.740
[103,     1] loss: 535.556
[104,     1] loss: 456.226
[105,     1] loss: 436.825
[106,     1] loss: 445.093
[107,     1] loss: 386.247
[108,     1] loss: 429.251
[109,     1] loss: 469.416
[110,     1] loss: 442.262
[111,     1] loss: 410.071
[112,     1] loss: 346.679
[113,     1] loss: 357.808
[114,     1] loss: 419.285
[115,     1] loss: 382.014
[116,     1] loss: 333.935
[117,     1] loss: 308.490
[118,     1] loss: 310.156
[119,     1] loss: 372.595
[120,     1] loss: 1063.734
[121,     1] loss: 2422.237
[122,     1] loss: 920.485
[123,     1] loss: 1140.515
[124,     1] loss: 1139.988
[125,     1] loss: 1124.611
[126,     1] loss: 1155.125
[127,     1] loss: 1156.890
[128,     1] loss: 1114.035
[129,     1] loss: 1109.180
[130,     1] loss: 1104.270
[131,     1] loss: 1085.490
[132,     1] loss: 1072.306
[133,     1] loss: 1070.509
[134,     1] loss: 1046.461
[135,     1] loss: 1054.501
[136,     1] loss: 1042.061
[137,     1] loss: 1022.940
[138,     1] loss: 1027.774
[139,     1] loss: 1059.585
[140,     1] loss: 1018.479
[141,     1] loss: 1003.223
[142,     1] loss: 984.867
[143,     1] loss: 998.635
[144,     1] loss: 977.617
[145,     1] loss: 974.708
[146,     1] loss: 978.655
[147,     1] loss: 968.697
[148,     1] loss: 998.252
[149,     1] loss: 938.198
[150,     1] loss: 937.106
[151,     1] loss: 925.291
[152,     1] loss: 933.641
[153,     1] loss: 932.460
[154,     1] loss: 900.424
[155,     1] loss: 914.024
[156,     1] loss: 922.232
[157,     1] loss: 951.157
[158,     1] loss: 895.156
[159,     1] loss: 893.468
[160,     1] loss: 919.851
[161,     1] loss: 892.585
[162,     1] loss: 899.711
[163,     1] loss: 901.606
Early stopping applied (best metric=0.3267042338848114)
Finished Training
Total time taken: 29.706064701080322
{'Hydroxylation-K Validation Accuracy': 0.7253841607565011, 'Hydroxylation-K Validation Sensitivity': 0.697037037037037, 'Hydroxylation-K Validation Specificity': 0.7333333333333333, 'Hydroxylation-K Validation Precision': 0.40260604898409447, 'Hydroxylation-K AUC ROC': 0.7866666666666666, 'Hydroxylation-K AUC PR': 0.6047361997541596, 'Hydroxylation-K MCC': 0.36461701515195233, 'Hydroxylation-K F1': 0.5048931862092781, 'Validation Loss (Hydroxylation-K)': 0.47897971073786416, 'Hydroxylation-P Validation Accuracy': 0.7879292083312184, 'Hydroxylation-P Validation Sensitivity': 0.8292592592592593, 'Hydroxylation-P Validation Specificity': 0.779003940346152, 'Hydroxylation-P Validation Precision': 0.452576833900552, 'Hydroxylation-P AUC ROC': 0.8631466161871668, 'Hydroxylation-P AUC PR': 0.6168648087036515, 'Hydroxylation-P MCC': 0.4976521735789511, 'Hydroxylation-P F1': 0.5838393251764417, 'Validation Loss (Hydroxylation-P)': 0.35549450715382896, 'Validation Loss (total)': 0.8344742139180501, 'TimeToTrain': 21.23527733484904}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004137106163289873,
 'learning_rate_Hydroxylation-K': 0.007281748274515407,
 'learning_rate_Hydroxylation-P': 0.0018702123734984136,
 'log_base': 2.834040231113834,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3545337208,
 'sample_weights': [1.5417032262018606, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.620800143307805,
 'weight_decay_Hydroxylation-K': 6.162195383019865,
 'weight_decay_Hydroxylation-P': 6.225703573117155}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.749
[2,     1] loss: 1245.533
[3,     1] loss: 1245.586
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0049621087572932305,
 'learning_rate_Hydroxylation-K': 0.003951950258965867,
 'learning_rate_Hydroxylation-P': 0.009383238357040143,
 'log_base': 2.3804190855089695,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1366099255,
 'sample_weights': [1.602609003980647, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.232321860895083,
 'weight_decay_Hydroxylation-K': 3.3687190613313804,
 'weight_decay_Hydroxylation-P': 0.11107701458071227}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.897
[2,     1] loss: 1314.597
[3,     1] loss: 1310.289
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021250454807886726,
 'learning_rate_Hydroxylation-K': 0.001215505875760629,
 'learning_rate_Hydroxylation-P': 0.00811502283997875,
 'log_base': 2.4784658050366835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1039232342,
 'sample_weights': [1.9249259431763261, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.985942134814497,
 'weight_decay_Hydroxylation-K': 5.697875668410429,
 'weight_decay_Hydroxylation-P': 0.39891765013688074}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.779
[2,     1] loss: 1298.584
[3,     1] loss: 1296.165
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0077907950903128975,
 'learning_rate_Hydroxylation-K': 0.0012899329712730942,
 'learning_rate_Hydroxylation-P': 0.006628959693344575,
 'log_base': 2.745421125750514,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3271564022,
 'sample_weights': [1.839323545056022, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.489808496801711,
 'weight_decay_Hydroxylation-K': 3.4228591132070116,
 'weight_decay_Hydroxylation-P': 1.1194194868086}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.685
[2,     1] loss: 1263.232
[3,     1] loss: 1256.654
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008886907280160379,
 'learning_rate_Hydroxylation-K': 0.0009709174555365858,
 'learning_rate_Hydroxylation-P': 0.006235728307756732,
 'log_base': 2.3886814056917682,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1236099917,
 'sample_weights': [1.6530212429814175, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7497054530604856,
 'weight_decay_Hydroxylation-K': 0.5357868464740141,
 'weight_decay_Hydroxylation-P': 4.192625591597078}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.558
[2,     1] loss: 1311.648
[3,     1] loss: 1321.079
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006835934277047166,
 'learning_rate_Hydroxylation-K': 0.0014861819022848046,
 'learning_rate_Hydroxylation-P': 0.009806800679544269,
 'log_base': 2.8383486338087636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 624790301,
 'sample_weights': [1.917266084463285, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.144676954852981,
 'weight_decay_Hydroxylation-K': 3.1060530123091588,
 'weight_decay_Hydroxylation-P': 2.923390010461402}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.315
[2,     1] loss: 1250.368
[3,     1] loss: 1250.283
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008390995022314256,
 'learning_rate_Hydroxylation-K': 0.00653839615434458,
 'learning_rate_Hydroxylation-P': 0.006647037067974843,
 'log_base': 2.7620417286832724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3629965842,
 'sample_weights': [1.6002753791331714, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6203799904975673,
 'weight_decay_Hydroxylation-K': 6.229373310149284,
 'weight_decay_Hydroxylation-P': 2.3750843129617367}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.280
[2,     1] loss: 1251.584
[3,     1] loss: 1257.147
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003449941848674365,
 'learning_rate_Hydroxylation-K': 2.214583982640601e-05,
 'learning_rate_Hydroxylation-P': 0.009112504895225085,
 'log_base': 2.8203577634714274,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1031129393,
 'sample_weights': [1.643200960225445, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.821406891510257,
 'weight_decay_Hydroxylation-K': 7.933409700111847,
 'weight_decay_Hydroxylation-P': 0.23543394051258135}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.805
[2,     1] loss: 1246.958
[3,     1] loss: 1247.162
[4,     1] loss: 1247.327
[5,     1] loss: 1247.268
[6,     1] loss: 1245.938
[7,     1] loss: 1244.418
[8,     1] loss: 1220.588
[9,     1] loss: 1207.372
[10,     1] loss: 1176.423
[11,     1] loss: 1138.944
[12,     1] loss: 1119.878
[13,     1] loss: 1111.726
[14,     1] loss: 1078.566
[15,     1] loss: 1026.605
[16,     1] loss: 1079.418
[17,     1] loss: 1044.572
[18,     1] loss: 1054.295
[19,     1] loss: 1035.481
[20,     1] loss: 1073.952
[21,     1] loss: 989.924
[22,     1] loss: 1000.267
[23,     1] loss: 960.514
[24,     1] loss: 984.226
[25,     1] loss: 992.369
[26,     1] loss: 969.592
[27,     1] loss: 979.319
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002682343550016463,
 'learning_rate_Hydroxylation-K': 0.007313412462124153,
 'learning_rate_Hydroxylation-P': 0.0022475103201743296,
 'log_base': 1.0631496351597185,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3761862918,
 'sample_weights': [1.6100892311826216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.334594262468354,
 'weight_decay_Hydroxylation-K': 5.334250178462526,
 'weight_decay_Hydroxylation-P': 1.7291518006022655}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8831.152
[2,     1] loss: 8922.342
[3,     1] loss: 8827.536
[4,     1] loss: 8848.201
[5,     1] loss: 8847.199
[6,     1] loss: 8847.072
[7,     1] loss: 8851.436
[8,     1] loss: 8834.516
[9,     1] loss: 8815.261
[10,     1] loss: 8813.389
[11,     1] loss: 8817.554
[12,     1] loss: 8792.273
[13,     1] loss: 8775.407
[14,     1] loss: 8723.910
[15,     1] loss: 8709.478
[16,     1] loss: 8648.115
[17,     1] loss: 8580.115
[18,     1] loss: 8330.140
[19,     1] loss: 8302.354
[20,     1] loss: 7996.916
[21,     1] loss: 7725.740
[22,     1] loss: 7862.514
[23,     1] loss: 7963.088
[24,     1] loss: 7882.382
[25,     1] loss: 7285.575
[26,     1] loss: 7039.381
[27,     1] loss: 7089.436
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00653404669500067,
 'learning_rate_Hydroxylation-K': 0.009837412481784874,
 'learning_rate_Hydroxylation-P': 0.0007568493025193414,
 'log_base': 1.0857065801394263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 921570426,
 'sample_weights': [27.262510043261436, 3.407945038078054],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3982783592041743,
 'weight_decay_Hydroxylation-K': 1.5763022601114434,
 'weight_decay_Hydroxylation-P': 1.1548997675743011}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6588.216
[2,     1] loss: 6575.549
[3,     1] loss: 6646.597
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027702082064741448,
 'learning_rate_Hydroxylation-K': 0.0035934688414729415,
 'learning_rate_Hydroxylation-P': 0.005921625710040733,
 'log_base': 2.7658308267365017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3633841493,
 'sample_weights': [20.301870677659632, 2.5378315984053645],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.553469235691308,
 'weight_decay_Hydroxylation-K': 5.59650971547978,
 'weight_decay_Hydroxylation-P': 4.144792393083653}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.919
[2,     1] loss: 1249.829
[3,     1] loss: 1251.588
[4,     1] loss: 1252.191
[5,     1] loss: 1250.013
[6,     1] loss: 1253.545
[7,     1] loss: 1250.246
[8,     1] loss: 1240.405
[9,     1] loss: 1221.991
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0069366970696945595,
 'learning_rate_Hydroxylation-K': 0.0011732178686072229,
 'learning_rate_Hydroxylation-P': 0.007899201106572312,
 'log_base': 2.8285387668574633,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1908398620,
 'sample_weights': [1.6409866830231283, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.563038843591401,
 'weight_decay_Hydroxylation-K': 6.801983489327787,
 'weight_decay_Hydroxylation-P': 0.7332532249691738}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.859
[2,     1] loss: 1246.950
[3,     1] loss: 1254.033
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025471391919324893,
 'learning_rate_Hydroxylation-K': 0.0023208754133838836,
 'learning_rate_Hydroxylation-P': 0.00715108459479754,
 'log_base': 2.979766900445603,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1210028031,
 'sample_weights': [1.6056039467354115, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.106904732258293,
 'weight_decay_Hydroxylation-K': 5.999124568870684,
 'weight_decay_Hydroxylation-P': 5.122938078491831}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.434
[2,     1] loss: 1230.168
[3,     1] loss: 1229.850
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014576178514456987,
 'learning_rate_Hydroxylation-K': 0.004861152384466817,
 'learning_rate_Hydroxylation-P': 0.008549261171086817,
 'log_base': 1.8236140533162883,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4091506110,
 'sample_weights': [1.5290110144010443, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5743586434449375,
 'weight_decay_Hydroxylation-K': 5.010512537305063,
 'weight_decay_Hydroxylation-P': 9.9824933691438}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1497.267
[2,     1] loss: 1495.013
[3,     1] loss: 1492.339
[4,     1] loss: 1493.729
[5,     1] loss: 1491.436
[6,     1] loss: 1492.474
[7,     1] loss: 1490.544
[8,     1] loss: 1492.096
[9,     1] loss: 1489.287
[10,     1] loss: 1487.656
[11,     1] loss: 1488.645
[12,     1] loss: 1484.373
[13,     1] loss: 1483.422
[14,     1] loss: 1476.368
[15,     1] loss: 1475.353
[16,     1] loss: 1459.917
[17,     1] loss: 1443.932
[18,     1] loss: 1436.958
[19,     1] loss: 1419.498
[20,     1] loss: 1392.331
[21,     1] loss: 1371.945
[22,     1] loss: 1333.041
[23,     1] loss: 1331.674
[24,     1] loss: 1317.856
[25,     1] loss: 1294.910
[26,     1] loss: 1264.873
[27,     1] loss: 1226.949
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003466961105810425,
 'learning_rate_Hydroxylation-K': 0.0034041254099013265,
 'learning_rate_Hydroxylation-P': 0.006176029117427624,
 'log_base': 2.0908938554667285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2080631805,
 'sample_weights': [2.7786065400610163, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.26500883688613586,
 'weight_decay_Hydroxylation-K': 7.331732847900824,
 'weight_decay_Hydroxylation-P': 2.461972298916758}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.961
[2,     1] loss: 1396.553
[3,     1] loss: 1385.859
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030386530342801526,
 'learning_rate_Hydroxylation-K': 0.0045304066241135905,
 'learning_rate_Hydroxylation-P': 0.007753733122303612,
 'log_base': 2.9288760525271895,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2437729448,
 'sample_weights': [2.2633704333717226, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.67179390753663,
 'weight_decay_Hydroxylation-K': 7.267713910638383,
 'weight_decay_Hydroxylation-P': 1.091384338207873}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.435
[2,     1] loss: 1237.482
[3,     1] loss: 1238.003
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002109684260845995,
 'learning_rate_Hydroxylation-K': 0.0050242434557598,
 'learning_rate_Hydroxylation-P': 0.0022476632920277787,
 'log_base': 2.990661403348363,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2990725312,
 'sample_weights': [1.5535213283973817, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6712409661593837,
 'weight_decay_Hydroxylation-K': 8.961957151182109,
 'weight_decay_Hydroxylation-P': 0.07087982614057076}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.533
[2,     1] loss: 1234.292
[3,     1] loss: 1229.285
[4,     1] loss: 1229.487
[5,     1] loss: 1229.555
[6,     1] loss: 1227.492
[7,     1] loss: 1227.080
[8,     1] loss: 1225.745
[9,     1] loss: 1225.592
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003621436501111748,
 'learning_rate_Hydroxylation-K': 0.005058536541904039,
 'learning_rate_Hydroxylation-P': 6.992978261533955e-05,
 'log_base': 2.560824151504564,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3854977758,
 'sample_weights': [1.5239173210684003, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.04219692676516562,
 'weight_decay_Hydroxylation-K': 7.657938815960107,
 'weight_decay_Hydroxylation-P': 2.831774489895563}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.103
[2,     1] loss: 1285.533
[3,     1] loss: 1283.415
[4,     1] loss: 1282.645
[5,     1] loss: 1279.142
[6,     1] loss: 1275.421
[7,     1] loss: 1272.736
[8,     1] loss: 1265.478
[9,     1] loss: 1252.789
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006942843740474018,
 'learning_rate_Hydroxylation-K': 0.004205025459188257,
 'learning_rate_Hydroxylation-P': 0.00907549910553392,
 'log_base': 2.0132287539312035,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3970601836,
 'sample_weights': [1.7753816986795392, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.847221481116446,
 'weight_decay_Hydroxylation-K': 2.1208435192497483,
 'weight_decay_Hydroxylation-P': 9.059334139572304}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.530
[2,     1] loss: 1413.084
[3,     1] loss: 1422.001
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00538180345975897,
 'learning_rate_Hydroxylation-K': 0.00033739821765108177,
 'learning_rate_Hydroxylation-P': 0.008776225273402737,
 'log_base': 2.4237630391007516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3824209712,
 'sample_weights': [2.3858056933104246, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.486319724201861,
 'weight_decay_Hydroxylation-K': 8.567165986395143,
 'weight_decay_Hydroxylation-P': 0.8115381496557811}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1311.607
[2,     1] loss: 1305.795
[3,     1] loss: 1305.687
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016238247273747438,
 'learning_rate_Hydroxylation-K': 0.0007909252861317465,
 'learning_rate_Hydroxylation-P': 0.009926138210832474,
 'log_base': 2.7123351955644224,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1636614292,
 'sample_weights': [1.8856918202373902, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.148619775179718,
 'weight_decay_Hydroxylation-K': 9.961658678748673,
 'weight_decay_Hydroxylation-P': 0.5308100249470644}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.342
[2,     1] loss: 1258.842
[3,     1] loss: 1260.044
[4,     1] loss: 1257.020
[5,     1] loss: 1259.665
[6,     1] loss: 1251.959
[7,     1] loss: 1257.130
[8,     1] loss: 1249.980
[9,     1] loss: 1245.481
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00827717110679391,
 'learning_rate_Hydroxylation-K': 0.001897087902133628,
 'learning_rate_Hydroxylation-P': 0.008287531882917719,
 'log_base': 1.3891406588773685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4108720169,
 'sample_weights': [1.6731073200059123, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.955472899035506,
 'weight_decay_Hydroxylation-K': 0.5931022617132161,
 'weight_decay_Hydroxylation-P': 4.372343183170325}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1981.088
[2,     1] loss: 1981.424
[3,     1] loss: 1978.546
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00725788128426135,
 'learning_rate_Hydroxylation-K': 0.002873756892490617,
 'learning_rate_Hydroxylation-P': 0.006021921496442952,
 'log_base': 2.9335387780177493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3472507256,
 'sample_weights': [5.079153283619082, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.585675216263176,
 'weight_decay_Hydroxylation-K': 0.3235624201125198,
 'weight_decay_Hydroxylation-P': 4.41549524225705}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.322
[2,     1] loss: 1240.206
[3,     1] loss: 1246.552
[4,     1] loss: 1240.831
[5,     1] loss: 1237.676
[6,     1] loss: 1233.373
[7,     1] loss: 1232.532
[8,     1] loss: 1231.368
[9,     1] loss: 1229.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004729312363434627,
 'learning_rate_Hydroxylation-K': 0.0054198089268480635,
 'learning_rate_Hydroxylation-P': 0.0006881247064534489,
 'log_base': 1.73283959630748,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3896893014,
 'sample_weights': [1.5512251067846154, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.245577937237324,
 'weight_decay_Hydroxylation-K': 1.982227226854951,
 'weight_decay_Hydroxylation-P': 9.807322641839905}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1556.453
[2,     1] loss: 1550.655
[3,     1] loss: 1550.100
[4,     1] loss: 1548.679
[5,     1] loss: 1540.002
[6,     1] loss: 1544.226
[7,     1] loss: 1534.848
[8,     1] loss: 1527.140
[9,     1] loss: 1516.340
[10,     1] loss: 1508.199
[11,     1] loss: 1465.107
[12,     1] loss: 1430.788
[13,     1] loss: 1397.178
[14,     1] loss: 1320.591
[15,     1] loss: 1314.870
[16,     1] loss: 1347.814
[17,     1] loss: 1348.005
[18,     1] loss: 1341.308
[19,     1] loss: 1255.253
[20,     1] loss: 1279.760
[21,     1] loss: 1245.905
[22,     1] loss: 1237.526
[23,     1] loss: 1223.732
[24,     1] loss: 1258.845
[25,     1] loss: 1199.541
[26,     1] loss: 1246.922
[27,     1] loss: 1193.968
[28,     1] loss: 1202.871
[29,     1] loss: 1136.887
[30,     1] loss: 1192.659
[31,     1] loss: 1128.983
[32,     1] loss: 1108.878
[33,     1] loss: 1193.693
[34,     1] loss: 1137.623
[35,     1] loss: 1159.895
[36,     1] loss: 1091.076
[37,     1] loss: 1110.283
[38,     1] loss: 1181.536
[39,     1] loss: 1083.509
[40,     1] loss: 1090.873
[41,     1] loss: 1127.846
[42,     1] loss: 1046.220
[43,     1] loss: 1099.908
[44,     1] loss: 1065.385
[45,     1] loss: 979.508
[46,     1] loss: 1020.240
[47,     1] loss: 941.616
[48,     1] loss: 900.735
[49,     1] loss: 955.404
[50,     1] loss: 987.832
[51,     1] loss: 1078.900
[52,     1] loss: 993.947
[53,     1] loss: 945.053
[54,     1] loss: 923.389
[55,     1] loss: 915.382
[56,     1] loss: 992.129
[57,     1] loss: 879.889
[58,     1] loss: 899.739
[59,     1] loss: 877.054
[60,     1] loss: 835.185
[61,     1] loss: 836.193
[62,     1] loss: 810.545
[63,     1] loss: 744.970
[64,     1] loss: 817.144
[65,     1] loss: 889.441
[66,     1] loss: 1174.702
[67,     1] loss: 992.128
[68,     1] loss: 895.965
[69,     1] loss: 891.793
[70,     1] loss: 1030.711
[71,     1] loss: 885.244
[72,     1] loss: 934.239
[73,     1] loss: 944.748
[74,     1] loss: 841.419
[75,     1] loss: 913.016
[76,     1] loss: 888.500
[77,     1] loss: 770.104
[78,     1] loss: 803.602
[79,     1] loss: 758.724
[80,     1] loss: 822.011
[81,     1] loss: 733.727
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006182304739639253,
 'learning_rate_Hydroxylation-K': 0.005792735927416511,
 'learning_rate_Hydroxylation-P': 0.0018230091056673809,
 'log_base': 1.9565854496995059,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1155384309,
 'sample_weights': [3.0366682737038655, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8467983165470883,
 'weight_decay_Hydroxylation-K': 0.8896854567545931,
 'weight_decay_Hydroxylation-P': 8.011381864525863}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1434.469
[2,     1] loss: 1434.412
[3,     1] loss: 1430.729
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007325508708567648,
 'learning_rate_Hydroxylation-K': 0.008917055495081011,
 'learning_rate_Hydroxylation-P': 0.0002516214612217158,
 'log_base': 2.116175886234328,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3655705822,
 'sample_weights': [2.487248311775134, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.257055663288698,
 'weight_decay_Hydroxylation-K': 5.13650723853045,
 'weight_decay_Hydroxylation-P': 0.003462677504985079}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.745
[2,     1] loss: 1380.292
[3,     1] loss: 1378.490
[4,     1] loss: 1376.716
[5,     1] loss: 1373.857
[6,     1] loss: 1374.816
[7,     1] loss: 1375.747
[8,     1] loss: 1379.022
[9,     1] loss: 1376.461
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014836262510531158,
 'learning_rate_Hydroxylation-K': 0.0011240410337853685,
 'learning_rate_Hydroxylation-P': 0.003849637368439452,
 'log_base': 2.4440259378519835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2815295079,
 'sample_weights': [2.2270803999626554, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3301121601704604,
 'weight_decay_Hydroxylation-K': 1.8692337338060172,
 'weight_decay_Hydroxylation-P': 3.450689724148309}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.594
[2,     1] loss: 1302.962
[3,     1] loss: 1300.214
[4,     1] loss: 1299.678
[5,     1] loss: 1299.482
[6,     1] loss: 1300.882
[7,     1] loss: 1296.037
[8,     1] loss: 1296.855
[9,     1] loss: 1288.593
[10,     1] loss: 1283.257
[11,     1] loss: 1276.180
[12,     1] loss: 1266.046
[13,     1] loss: 1242.623
[14,     1] loss: 1226.828
[15,     1] loss: 1213.931
[16,     1] loss: 1192.654
[17,     1] loss: 1172.397
[18,     1] loss: 1145.656
[19,     1] loss: 1133.357
[20,     1] loss: 1111.833
[21,     1] loss: 1118.184
[22,     1] loss: 1122.572
[23,     1] loss: 1099.273
[24,     1] loss: 1095.282
[25,     1] loss: 1111.478
[26,     1] loss: 1070.744
[27,     1] loss: 1089.378
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005180203260447449,
 'learning_rate_Hydroxylation-K': 0.0043467874938217625,
 'learning_rate_Hydroxylation-P': 0.0005997464004081587,
 'log_base': 1.9420935991946982,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1616078198,
 'sample_weights': [1.8681244311978222, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3292957615004,
 'weight_decay_Hydroxylation-K': 2.77978087778801,
 'weight_decay_Hydroxylation-P': 9.60086607539144}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1442.012
[2,     1] loss: 1437.598
[3,     1] loss: 1442.499
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012349365764452013,
 'learning_rate_Hydroxylation-K': 0.009503804474176087,
 'learning_rate_Hydroxylation-P': 0.00822018013478214,
 'log_base': 2.1290106938862254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2750964263,
 'sample_weights': [2.5151058110917504, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.95825928209893,
 'weight_decay_Hydroxylation-K': 4.820110867874551,
 'weight_decay_Hydroxylation-P': 7.169227251780152}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1378.696
[2,     1] loss: 1377.275
[3,     1] loss: 1371.614
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007269767378479534,
 'learning_rate_Hydroxylation-K': 0.004984971971685168,
 'learning_rate_Hydroxylation-P': 0.0018696321020403456,
 'log_base': 1.4921572650339525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2967443411,
 'sample_weights': [2.209259285819071, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.30489859011344,
 'weight_decay_Hydroxylation-K': 0.24387982460381608,
 'weight_decay_Hydroxylation-P': 9.05402704099482}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1786.375
[2,     1] loss: 1796.057
[3,     1] loss: 1803.908
[4,     1] loss: 1792.271
[5,     1] loss: 1796.878
[6,     1] loss: 1788.728
[7,     1] loss: 1788.348
[8,     1] loss: 1786.996
[9,     1] loss: 1788.157
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029291913841771092,
 'learning_rate_Hydroxylation-K': 0.001773394706224973,
 'learning_rate_Hydroxylation-P': 0.0014104740150895855,
 'log_base': 2.193931315604528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 317289825,
 'sample_weights': [4.171283402680259, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.688194358675601,
 'weight_decay_Hydroxylation-K': 0.07909653557521262,
 'weight_decay_Hydroxylation-P': 2.9468554412328403}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1361.095
[2,     1] loss: 1355.069
[3,     1] loss: 1357.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006314177505630206,
 'learning_rate_Hydroxylation-K': 0.007119551520658832,
 'learning_rate_Hydroxylation-P': 0.0007818681283008126,
 'log_base': 1.2181099461169893,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2817522397,
 'sample_weights': [2.1247978272109296, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6141571749168495,
 'weight_decay_Hydroxylation-K': 5.096255266203041,
 'weight_decay_Hydroxylation-P': 8.840146257633581}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2764.703
[2,     1] loss: 2770.496
[3,     1] loss: 2743.687
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036270299462455124,
 'learning_rate_Hydroxylation-K': 0.005370162368900664,
 'learning_rate_Hydroxylation-P': 0.0014443568631954474,
 'log_base': 1.2989500827116554,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1736807475,
 'sample_weights': [8.461426679926605, 1.0577190810075334],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.822477380929591,
 'weight_decay_Hydroxylation-K': 0.41972310724773676,
 'weight_decay_Hydroxylation-P': 9.596576696119483}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2278.351
[2,     1] loss: 2252.525
[3,     1] loss: 2246.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007965822879051399,
 'learning_rate_Hydroxylation-K': 0.001237935870119797,
 'learning_rate_Hydroxylation-P': 0.0006181496326155831,
 'log_base': 2.1932422111376457,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 703132881,
 'sample_weights': [6.382729404485245, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.011257831358231,
 'weight_decay_Hydroxylation-K': 1.391504464057922,
 'weight_decay_Hydroxylation-P': 0.7749937570182281}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1360.512
[2,     1] loss: 1359.253
[3,     1] loss: 1360.127
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004189549611330373,
 'learning_rate_Hydroxylation-K': 0.00047440245794059263,
 'learning_rate_Hydroxylation-P': 0.009304028119536052,
 'log_base': 2.9400646029930737,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3796728510,
 'sample_weights': [2.1256477266140084, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.20774757678242,
 'weight_decay_Hydroxylation-K': 7.042364074581735,
 'weight_decay_Hydroxylation-P': 3.8767422521721375}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.832
[2,     1] loss: 1234.819
[3,     1] loss: 1235.317
[4,     1] loss: 1233.060
[5,     1] loss: 1233.541
[6,     1] loss: 1228.595
[7,     1] loss: 1227.085
[8,     1] loss: 1221.020
[9,     1] loss: 1209.976
[10,     1] loss: 1190.318
[11,     1] loss: 1160.891
[12,     1] loss: 1137.531
[13,     1] loss: 1107.828
[14,     1] loss: 1037.281
[15,     1] loss: 1077.432
[16,     1] loss: 1049.164
[17,     1] loss: 1039.731
[18,     1] loss: 1028.122
[19,     1] loss: 1062.627
[20,     1] loss: 998.815
[21,     1] loss: 973.791
[22,     1] loss: 973.183
[23,     1] loss: 961.147
[24,     1] loss: 987.770
[25,     1] loss: 989.275
[26,     1] loss: 995.027
[27,     1] loss: 955.943
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0042014198587476065,
 'learning_rate_Hydroxylation-K': 0.0011462104031178546,
 'learning_rate_Hydroxylation-P': 0.008264138307307654,
 'log_base': 2.7052432286347257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3285362679,
 'sample_weights': [1.5480288385600736, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.591049546795887,
 'weight_decay_Hydroxylation-K': 5.426551564676675,
 'weight_decay_Hydroxylation-P': 0.9800356285288098}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.795
[2,     1] loss: 1262.965
[3,     1] loss: 1260.725
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006866690316810442,
 'learning_rate_Hydroxylation-K': 0.0017566779751140961,
 'learning_rate_Hydroxylation-P': 0.009319006593185427,
 'log_base': 2.4739999654355005,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 966756350,
 'sample_weights': [1.6775089014028444, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.072406590839986,
 'weight_decay_Hydroxylation-K': 4.207759853018562,
 'weight_decay_Hydroxylation-P': 0.32568630129106757}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.974
[2,     1] loss: 1297.664
[3,     1] loss: 1296.617
[4,     1] loss: 1293.278
[5,     1] loss: 1292.214
[6,     1] loss: 1285.362
[7,     1] loss: 1288.558
[8,     1] loss: 1254.411
[9,     1] loss: 1208.891
[10,     1] loss: 1170.058
[11,     1] loss: 1111.756
[12,     1] loss: 1087.567
[13,     1] loss: 1168.228
[14,     1] loss: 1285.548
[15,     1] loss: 1048.460
[16,     1] loss: 1179.001
[17,     1] loss: 1120.652
[18,     1] loss: 1112.550
[19,     1] loss: 1143.351
[20,     1] loss: 1110.481
[21,     1] loss: 1062.276
[22,     1] loss: 1038.699
[23,     1] loss: 1058.468
[24,     1] loss: 1057.835
[25,     1] loss: 1020.642
[26,     1] loss: 1021.457
[27,     1] loss: 1052.876
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004160792553131572,
 'learning_rate_Hydroxylation-K': 0.00028312132129111665,
 'learning_rate_Hydroxylation-P': 0.004131717951318106,
 'log_base': 2.8927181254797345,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3009976149,
 'sample_weights': [1.84298556057488, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.066130253173116,
 'weight_decay_Hydroxylation-K': 3.123650737379217,
 'weight_decay_Hydroxylation-P': 1.0672831717544022}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.414
[2,     1] loss: 1242.118
[3,     1] loss: 1238.840
[4,     1] loss: 1234.298
[5,     1] loss: 1234.105
[6,     1] loss: 1223.924
[7,     1] loss: 1206.458
[8,     1] loss: 1182.889
[9,     1] loss: 1125.226
[10,     1] loss: 1093.031
[11,     1] loss: 1057.193
[12,     1] loss: 1037.700
[13,     1] loss: 1054.156
[14,     1] loss: 1005.233
[15,     1] loss: 1038.378
[16,     1] loss: 979.374
[17,     1] loss: 996.720
[18,     1] loss: 965.142
[19,     1] loss: 981.564
[20,     1] loss: 932.570
[21,     1] loss: 913.358
[22,     1] loss: 945.661
[23,     1] loss: 980.160
[24,     1] loss: 884.789
[25,     1] loss: 881.335
[26,     1] loss: 886.975
[27,     1] loss: 868.749
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036678468693977194,
 'learning_rate_Hydroxylation-K': 0.0012921891236647369,
 'learning_rate_Hydroxylation-P': 0.005091636506270453,
 'log_base': 2.982194054678651,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2799849497,
 'sample_weights': [1.571689427699264, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.2779654477544495,
 'weight_decay_Hydroxylation-K': 5.8041223516868445,
 'weight_decay_Hydroxylation-P': 0.4190432642789558}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.760
[2,     1] loss: 1232.552
[3,     1] loss: 1230.489
[4,     1] loss: 1230.547
[5,     1] loss: 1226.844
[6,     1] loss: 1228.366
[7,     1] loss: 1222.217
[8,     1] loss: 1210.963
[9,     1] loss: 1204.374
[10,     1] loss: 1170.572
[11,     1] loss: 1151.686
[12,     1] loss: 1130.102
[13,     1] loss: 1083.001
[14,     1] loss: 1087.267
[15,     1] loss: 1045.211
[16,     1] loss: 1031.467
[17,     1] loss: 1085.617
[18,     1] loss: 1016.163
[19,     1] loss: 1066.244
[20,     1] loss: 1002.936
[21,     1] loss: 1029.577
[22,     1] loss: 987.347
[23,     1] loss: 1033.488
[24,     1] loss: 972.109
[25,     1] loss: 977.351
[26,     1] loss: 1034.465
[27,     1] loss: 920.436
[28,     1] loss: 971.344
[29,     1] loss: 932.589
[30,     1] loss: 970.543
[31,     1] loss: 926.205
[32,     1] loss: 928.328
[33,     1] loss: 937.223
[34,     1] loss: 907.065
[35,     1] loss: 894.785
[36,     1] loss: 900.590
[37,     1] loss: 878.339
[38,     1] loss: 833.245
[39,     1] loss: 840.765
[40,     1] loss: 796.446
[41,     1] loss: 854.671
[42,     1] loss: 1114.885
[43,     1] loss: 1074.736
[44,     1] loss: 880.587
[45,     1] loss: 906.367
[46,     1] loss: 946.868
[47,     1] loss: 875.959
[48,     1] loss: 870.425
[49,     1] loss: 858.083
[50,     1] loss: 875.661
[51,     1] loss: 839.947
[52,     1] loss: 849.907
[53,     1] loss: 844.411
[54,     1] loss: 822.341
[55,     1] loss: 881.120
[56,     1] loss: 757.097
[57,     1] loss: 773.615
[58,     1] loss: 807.219
[59,     1] loss: 764.565
[60,     1] loss: 783.643
[61,     1] loss: 711.938
[62,     1] loss: 730.570
[63,     1] loss: 724.233
[64,     1] loss: 666.429
[65,     1] loss: 698.751
[66,     1] loss: 670.040
[67,     1] loss: 621.161
[68,     1] loss: 664.917
[69,     1] loss: 667.918
[70,     1] loss: 581.515
[71,     1] loss: 664.916
[72,     1] loss: 692.958
[73,     1] loss: 772.540
[74,     1] loss: 743.894
[75,     1] loss: 640.045
[76,     1] loss: 738.121
[77,     1] loss: 618.636
[78,     1] loss: 716.603
[79,     1] loss: 583.602
[80,     1] loss: 721.837
[81,     1] loss: 566.079
[82,     1] loss: 627.911
[83,     1] loss: 637.433
[84,     1] loss: 544.021
[85,     1] loss: 664.025
[86,     1] loss: 545.074
[87,     1] loss: 645.462
[88,     1] loss: 652.666
[89,     1] loss: 507.208
[90,     1] loss: 638.980
[91,     1] loss: 543.891
[92,     1] loss: 577.071
[93,     1] loss: 511.936
[94,     1] loss: 545.795
[95,     1] loss: 678.500
[96,     1] loss: 476.243
[97,     1] loss: 448.077
[98,     1] loss: 754.806
[99,     1] loss: 483.939
[100,     1] loss: 530.331
[101,     1] loss: 493.670
[102,     1] loss: 562.877
[103,     1] loss: 459.703
[104,     1] loss: 545.465
[105,     1] loss: 502.207
[106,     1] loss: 404.065
[107,     1] loss: 668.979
[108,     1] loss: 830.372
[109,     1] loss: 454.197
[110,     1] loss: 642.625
[111,     1] loss: 584.965
[112,     1] loss: 609.109
[113,     1] loss: 622.627
[114,     1] loss: 505.083
[115,     1] loss: 507.725
[116,     1] loss: 459.998
[117,     1] loss: 489.732
[118,     1] loss: 375.566
[119,     1] loss: 482.692
[120,     1] loss: 472.223
[121,     1] loss: 396.650
[122,     1] loss: 412.161
[123,     1] loss: 346.608
[124,     1] loss: 351.384
[125,     1] loss: 361.604
[126,     1] loss: 450.964
[127,     1] loss: 517.212
[128,     1] loss: 310.677
[129,     1] loss: 358.324
[130,     1] loss: 401.556
[131,     1] loss: 338.613
[132,     1] loss: 645.360
[133,     1] loss: 873.912
[134,     1] loss: 356.919
[135,     1] loss: 728.156
[136,     1] loss: 484.046
[137,     1] loss: 543.189
[138,     1] loss: 520.546
[139,     1] loss: 392.896
[140,     1] loss: 597.772
[141,     1] loss: 530.280
[142,     1] loss: 394.262
[143,     1] loss: 485.890
[144,     1] loss: 355.658
[145,     1] loss: 479.172
[146,     1] loss: 361.424
[147,     1] loss: 462.107
[148,     1] loss: 303.483
[149,     1] loss: 338.365
[150,     1] loss: 284.464
[151,     1] loss: 384.457
[152,     1] loss: 364.958
[153,     1] loss: 263.611
[154,     1] loss: 339.907
[155,     1] loss: 412.578
Early stopping applied (best metric=0.2603738605976105)
Finished Training
Total time taken: 22.36150097846985
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.529
[2,     1] loss: 1235.103
[3,     1] loss: 1234.388
[4,     1] loss: 1225.714
[5,     1] loss: 1230.178
[6,     1] loss: 1230.046
[7,     1] loss: 1225.728
[8,     1] loss: 1218.858
[9,     1] loss: 1211.781
[10,     1] loss: 1188.751
[11,     1] loss: 1177.184
[12,     1] loss: 1145.829
[13,     1] loss: 1142.696
[14,     1] loss: 1111.828
[15,     1] loss: 1101.813
[16,     1] loss: 1072.880
[17,     1] loss: 1057.201
[18,     1] loss: 1043.388
[19,     1] loss: 1052.720
[20,     1] loss: 1014.793
[21,     1] loss: 1051.159
[22,     1] loss: 986.068
[23,     1] loss: 979.103
[24,     1] loss: 1013.050
[25,     1] loss: 960.432
[26,     1] loss: 960.728
[27,     1] loss: 958.569
[28,     1] loss: 925.058
[29,     1] loss: 937.339
[30,     1] loss: 898.625
[31,     1] loss: 928.377
[32,     1] loss: 886.607
[33,     1] loss: 909.262
[34,     1] loss: 894.991
[35,     1] loss: 889.927
[36,     1] loss: 881.538
[37,     1] loss: 874.671
[38,     1] loss: 854.491
[39,     1] loss: 829.699
[40,     1] loss: 870.431
[41,     1] loss: 952.898
[42,     1] loss: 790.927
[43,     1] loss: 815.736
[44,     1] loss: 821.090
[45,     1] loss: 785.649
[46,     1] loss: 753.314
[47,     1] loss: 803.370
[48,     1] loss: 753.529
[49,     1] loss: 763.343
[50,     1] loss: 689.884
[51,     1] loss: 688.263
[52,     1] loss: 726.317
[53,     1] loss: 781.751
[54,     1] loss: 887.884
[55,     1] loss: 794.675
[56,     1] loss: 733.899
[57,     1] loss: 789.408
[58,     1] loss: 697.185
[59,     1] loss: 723.995
[60,     1] loss: 686.357
[61,     1] loss: 720.258
[62,     1] loss: 616.621
[63,     1] loss: 623.359
[64,     1] loss: 609.545
[65,     1] loss: 657.201
[66,     1] loss: 615.302
[67,     1] loss: 689.075
[68,     1] loss: 599.996
[69,     1] loss: 572.795
[70,     1] loss: 600.372
[71,     1] loss: 588.184
[72,     1] loss: 666.420
[73,     1] loss: 678.822
[74,     1] loss: 628.263
[75,     1] loss: 515.032
[76,     1] loss: 676.178
[77,     1] loss: 617.866
[78,     1] loss: 562.297
[79,     1] loss: 625.273
[80,     1] loss: 512.527
[81,     1] loss: 563.450
[82,     1] loss: 490.495
[83,     1] loss: 460.140
[84,     1] loss: 426.145
[85,     1] loss: 548.821
[86,     1] loss: 789.217
[87,     1] loss: 1000.520
[88,     1] loss: 646.097
[89,     1] loss: 833.539
[90,     1] loss: 685.853
[91,     1] loss: 726.380
[92,     1] loss: 753.180
[93,     1] loss: 734.301
[94,     1] loss: 644.249
[95,     1] loss: 680.783
[96,     1] loss: 719.709
[97,     1] loss: 594.935
[98,     1] loss: 632.798
[99,     1] loss: 542.155
[100,     1] loss: 558.601
[101,     1] loss: 550.274
[102,     1] loss: 519.594
[103,     1] loss: 478.518
[104,     1] loss: 449.056
[105,     1] loss: 421.237
[106,     1] loss: 427.495
[107,     1] loss: 550.607
[108,     1] loss: 459.078
[109,     1] loss: 416.918
[110,     1] loss: 489.125
[111,     1] loss: 368.167
[112,     1] loss: 504.732
[113,     1] loss: 500.714
[114,     1] loss: 414.731
[115,     1] loss: 627.329
[116,     1] loss: 657.691
[117,     1] loss: 455.776
[118,     1] loss: 605.016
[119,     1] loss: 468.848
[120,     1] loss: 472.376
[121,     1] loss: 438.810
[122,     1] loss: 430.466
[123,     1] loss: 347.774
[124,     1] loss: 431.877
[125,     1] loss: 369.598
[126,     1] loss: 326.567
[127,     1] loss: 359.318
[128,     1] loss: 332.790
[129,     1] loss: 305.960
[130,     1] loss: 303.188
[131,     1] loss: 314.605
[132,     1] loss: 258.827
[133,     1] loss: 322.468
[134,     1] loss: 411.512
[135,     1] loss: 1301.613
[136,     1] loss: 1701.823
[137,     1] loss: 709.447
[138,     1] loss: 1024.641
[139,     1] loss: 1098.998
[140,     1] loss: 1115.994
[141,     1] loss: 1076.047
[142,     1] loss: 1021.255
[143,     1] loss: 988.940
[144,     1] loss: 977.466
[145,     1] loss: 929.445
[146,     1] loss: 910.934
[147,     1] loss: 923.502
[148,     1] loss: 880.885
[149,     1] loss: 902.238
[150,     1] loss: 853.753
[151,     1] loss: 873.537
[152,     1] loss: 874.707
[153,     1] loss: 900.175
[154,     1] loss: 853.957
[155,     1] loss: 850.826
[156,     1] loss: 831.870
[157,     1] loss: 811.450
[158,     1] loss: 786.638
[159,     1] loss: 781.937
[160,     1] loss: 792.357
[161,     1] loss: 781.462
[162,     1] loss: 775.483
Early stopping applied (best metric=0.33332881331443787)
Finished Training
Total time taken: 27.142027378082275
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.626
[2,     1] loss: 1230.175
[3,     1] loss: 1231.264
[4,     1] loss: 1229.662
[5,     1] loss: 1232.362
[6,     1] loss: 1228.983
[7,     1] loss: 1231.404
[8,     1] loss: 1225.247
[9,     1] loss: 1221.310
[10,     1] loss: 1217.049
[11,     1] loss: 1209.433
[12,     1] loss: 1189.288
[13,     1] loss: 1157.556
[14,     1] loss: 1128.259
[15,     1] loss: 1095.972
[16,     1] loss: 1071.281
[17,     1] loss: 1060.030
[18,     1] loss: 988.533
[19,     1] loss: 1075.217
[20,     1] loss: 1021.242
[21,     1] loss: 1023.167
[22,     1] loss: 1004.638
[23,     1] loss: 1020.737
[24,     1] loss: 984.121
[25,     1] loss: 978.795
[26,     1] loss: 1000.562
[27,     1] loss: 975.393
[28,     1] loss: 1009.023
[29,     1] loss: 982.992
[30,     1] loss: 968.645
[31,     1] loss: 971.908
[32,     1] loss: 939.919
[33,     1] loss: 925.713
[34,     1] loss: 886.032
[35,     1] loss: 889.024
[36,     1] loss: 855.682
[37,     1] loss: 883.142
[38,     1] loss: 894.507
[39,     1] loss: 846.586
[40,     1] loss: 872.284
[41,     1] loss: 886.383
[42,     1] loss: 831.067
[43,     1] loss: 840.477
[44,     1] loss: 822.964
[45,     1] loss: 869.246
[46,     1] loss: 787.144
[47,     1] loss: 794.336
[48,     1] loss: 781.385
[49,     1] loss: 781.941
[50,     1] loss: 833.188
[51,     1] loss: 739.871
[52,     1] loss: 798.225
[53,     1] loss: 772.462
[54,     1] loss: 775.274
[55,     1] loss: 744.209
[56,     1] loss: 776.770
[57,     1] loss: 744.017
[58,     1] loss: 682.673
[59,     1] loss: 715.884
[60,     1] loss: 693.873
[61,     1] loss: 683.764
[62,     1] loss: 654.119
[63,     1] loss: 606.143
[64,     1] loss: 597.568
[65,     1] loss: 632.544
[66,     1] loss: 610.669
[67,     1] loss: 636.825
[68,     1] loss: 618.405
[69,     1] loss: 623.739
[70,     1] loss: 601.410
[71,     1] loss: 554.937
[72,     1] loss: 555.655
[73,     1] loss: 628.409
[74,     1] loss: 1234.829
[75,     1] loss: 612.860
[76,     1] loss: 729.559
[77,     1] loss: 647.775
[78,     1] loss: 780.484
[79,     1] loss: 742.200
[80,     1] loss: 617.451
[81,     1] loss: 719.134
[82,     1] loss: 648.763
[83,     1] loss: 634.307
[84,     1] loss: 651.536
[85,     1] loss: 557.814
[86,     1] loss: 629.072
[87,     1] loss: 556.120
[88,     1] loss: 581.751
[89,     1] loss: 505.531
[90,     1] loss: 640.814
[91,     1] loss: 549.722
[92,     1] loss: 537.521
[93,     1] loss: 468.555
[94,     1] loss: 535.968
[95,     1] loss: 459.431
[96,     1] loss: 495.877
[97,     1] loss: 469.758
[98,     1] loss: 427.138
Early stopping applied (best metric=0.3885594308376312)
Finished Training
Total time taken: 16.281961679458618
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.268
[2,     1] loss: 1236.697
[3,     1] loss: 1231.204
[4,     1] loss: 1231.006
[5,     1] loss: 1222.338
[6,     1] loss: 1233.649
[7,     1] loss: 1227.211
[8,     1] loss: 1228.877
[9,     1] loss: 1215.115
[10,     1] loss: 1205.423
[11,     1] loss: 1186.121
[12,     1] loss: 1157.962
[13,     1] loss: 1126.251
[14,     1] loss: 1106.626
[15,     1] loss: 1059.201
[16,     1] loss: 1075.141
[17,     1] loss: 1031.601
[18,     1] loss: 1012.049
[19,     1] loss: 1000.158
[20,     1] loss: 1006.915
[21,     1] loss: 1003.387
[22,     1] loss: 959.996
[23,     1] loss: 992.470
[24,     1] loss: 967.589
[25,     1] loss: 939.940
[26,     1] loss: 944.037
[27,     1] loss: 923.427
[28,     1] loss: 933.726
[29,     1] loss: 885.151
[30,     1] loss: 898.990
[31,     1] loss: 887.685
[32,     1] loss: 973.122
[33,     1] loss: 886.559
[34,     1] loss: 1007.800
[35,     1] loss: 870.838
[36,     1] loss: 882.743
[37,     1] loss: 830.185
[38,     1] loss: 867.448
[39,     1] loss: 759.735
[40,     1] loss: 798.185
[41,     1] loss: 735.040
[42,     1] loss: 835.506
[43,     1] loss: 743.864
[44,     1] loss: 808.349
[45,     1] loss: 782.702
[46,     1] loss: 752.531
[47,     1] loss: 788.169
[48,     1] loss: 722.475
[49,     1] loss: 735.372
[50,     1] loss: 720.458
[51,     1] loss: 652.040
[52,     1] loss: 649.812
[53,     1] loss: 679.499
[54,     1] loss: 719.856
[55,     1] loss: 833.984
[56,     1] loss: 682.189
[57,     1] loss: 719.504
[58,     1] loss: 745.548
[59,     1] loss: 659.616
[60,     1] loss: 593.500
[61,     1] loss: 696.255
[62,     1] loss: 704.309
[63,     1] loss: 560.882
[64,     1] loss: 657.383
[65,     1] loss: 610.708
[66,     1] loss: 541.794
[67,     1] loss: 564.753
[68,     1] loss: 537.352
[69,     1] loss: 528.126
[70,     1] loss: 650.050
[71,     1] loss: 748.669
[72,     1] loss: 518.375
[73,     1] loss: 663.680
[74,     1] loss: 525.589
[75,     1] loss: 558.417
[76,     1] loss: 508.303
[77,     1] loss: 544.617
[78,     1] loss: 499.896
[79,     1] loss: 505.856
[80,     1] loss: 549.204
[81,     1] loss: 752.460
[82,     1] loss: 542.248
[83,     1] loss: 496.927
[84,     1] loss: 545.724
[85,     1] loss: 546.825
[86,     1] loss: 516.357
[87,     1] loss: 507.249
[88,     1] loss: 527.698
[89,     1] loss: 410.737
Early stopping applied (best metric=0.4009366035461426)
Finished Training
Total time taken: 16.14656949043274
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1243.938
[2,     1] loss: 1231.906
[3,     1] loss: 1231.307
[4,     1] loss: 1230.790
[5,     1] loss: 1230.261
[6,     1] loss: 1226.806
[7,     1] loss: 1221.831
[8,     1] loss: 1213.743
[9,     1] loss: 1186.441
[10,     1] loss: 1177.422
[11,     1] loss: 1141.819
[12,     1] loss: 1088.317
[13,     1] loss: 1075.475
[14,     1] loss: 1063.283
[15,     1] loss: 1073.969
[16,     1] loss: 1056.266
[17,     1] loss: 1011.726
[18,     1] loss: 1009.127
[19,     1] loss: 971.815
[20,     1] loss: 1014.516
[21,     1] loss: 968.020
[22,     1] loss: 986.086
[23,     1] loss: 955.992
[24,     1] loss: 947.826
[25,     1] loss: 941.066
[26,     1] loss: 951.478
[27,     1] loss: 954.131
[28,     1] loss: 943.588
[29,     1] loss: 893.034
[30,     1] loss: 867.196
[31,     1] loss: 936.456
[32,     1] loss: 861.101
[33,     1] loss: 889.728
[34,     1] loss: 880.703
[35,     1] loss: 865.394
[36,     1] loss: 869.954
[37,     1] loss: 842.049
[38,     1] loss: 841.177
[39,     1] loss: 887.457
[40,     1] loss: 775.952
[41,     1] loss: 797.095
[42,     1] loss: 841.180
[43,     1] loss: 796.850
[44,     1] loss: 803.011
[45,     1] loss: 765.709
[46,     1] loss: 719.675
[47,     1] loss: 788.392
[48,     1] loss: 877.191
[49,     1] loss: 783.841
[50,     1] loss: 777.407
[51,     1] loss: 745.167
[52,     1] loss: 796.066
[53,     1] loss: 735.901
[54,     1] loss: 794.142
[55,     1] loss: 743.788
[56,     1] loss: 769.864
[57,     1] loss: 648.661
[58,     1] loss: 695.258
[59,     1] loss: 668.291
[60,     1] loss: 675.422
[61,     1] loss: 653.632
[62,     1] loss: 659.037
[63,     1] loss: 774.563
[64,     1] loss: 728.526
[65,     1] loss: 644.396
[66,     1] loss: 697.493
[67,     1] loss: 583.726
[68,     1] loss: 627.073
[69,     1] loss: 570.561
[70,     1] loss: 643.430
[71,     1] loss: 603.742
[72,     1] loss: 577.121
[73,     1] loss: 612.812
[74,     1] loss: 602.404
[75,     1] loss: 556.780
[76,     1] loss: 591.612
[77,     1] loss: 515.689
[78,     1] loss: 491.884
[79,     1] loss: 507.062
[80,     1] loss: 492.623
[81,     1] loss: 475.399
[82,     1] loss: 422.189
[83,     1] loss: 809.772
[84,     1] loss: 889.117
[85,     1] loss: 611.429
[86,     1] loss: 787.601
[87,     1] loss: 735.368
[88,     1] loss: 655.223
[89,     1] loss: 797.594
[90,     1] loss: 657.998
[91,     1] loss: 630.510
[92,     1] loss: 635.569
[93,     1] loss: 659.540
[94,     1] loss: 557.157
[95,     1] loss: 648.148
[96,     1] loss: 517.120
[97,     1] loss: 567.625
[98,     1] loss: 519.078
[99,     1] loss: 507.930
[100,     1] loss: 505.026
[101,     1] loss: 484.628
[102,     1] loss: 438.169
[103,     1] loss: 525.275
[104,     1] loss: 547.809
[105,     1] loss: 443.732
[106,     1] loss: 572.298
[107,     1] loss: 393.293
[108,     1] loss: 587.816
[109,     1] loss: 402.341
[110,     1] loss: 532.491
[111,     1] loss: 374.531
[112,     1] loss: 424.103
[113,     1] loss: 427.255
[114,     1] loss: 387.343
[115,     1] loss: 383.083
[116,     1] loss: 393.145
[117,     1] loss: 363.406
[118,     1] loss: 323.586
[119,     1] loss: 303.731
[120,     1] loss: 332.726
[121,     1] loss: 352.586
[122,     1] loss: 393.251
[123,     1] loss: 595.253
[124,     1] loss: 1390.793
[125,     1] loss: 1052.881
[126,     1] loss: 1113.923
[127,     1] loss: 1030.058
[128,     1] loss: 1088.592
[129,     1] loss: 1131.875
Early stopping applied (best metric=0.3040454387664795)
Finished Training
Total time taken: 19.210360050201416
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.833
[2,     1] loss: 1233.024
[3,     1] loss: 1229.530
[4,     1] loss: 1226.058
[5,     1] loss: 1221.582
[6,     1] loss: 1205.764
[7,     1] loss: 1184.175
[8,     1] loss: 1154.278
[9,     1] loss: 1091.772
[10,     1] loss: 1075.427
[11,     1] loss: 1075.738
[12,     1] loss: 1036.663
[13,     1] loss: 994.745
[14,     1] loss: 1100.561
[15,     1] loss: 1038.466
[16,     1] loss: 989.357
[17,     1] loss: 956.675
[18,     1] loss: 986.288
[19,     1] loss: 957.461
[20,     1] loss: 942.404
[21,     1] loss: 951.989
[22,     1] loss: 908.630
[23,     1] loss: 930.421
[24,     1] loss: 923.433
[25,     1] loss: 887.924
[26,     1] loss: 900.270
[27,     1] loss: 898.748
[28,     1] loss: 891.408
[29,     1] loss: 854.488
[30,     1] loss: 881.590
[31,     1] loss: 860.069
[32,     1] loss: 831.823
[33,     1] loss: 980.065
[34,     1] loss: 892.747
[35,     1] loss: 820.013
[36,     1] loss: 801.587
[37,     1] loss: 815.807
[38,     1] loss: 802.105
[39,     1] loss: 771.256
[40,     1] loss: 740.840
[41,     1] loss: 742.995
[42,     1] loss: 768.313
[43,     1] loss: 704.326
[44,     1] loss: 767.610
[45,     1] loss: 763.476
[46,     1] loss: 724.236
[47,     1] loss: 773.233
[48,     1] loss: 715.387
[49,     1] loss: 768.567
[50,     1] loss: 676.122
[51,     1] loss: 741.673
[52,     1] loss: 689.620
[53,     1] loss: 695.481
[54,     1] loss: 724.758
[55,     1] loss: 654.222
[56,     1] loss: 630.510
[57,     1] loss: 705.725
[58,     1] loss: 586.110
[59,     1] loss: 645.769
[60,     1] loss: 690.334
[61,     1] loss: 584.010
[62,     1] loss: 652.495
[63,     1] loss: 635.583
[64,     1] loss: 644.081
[65,     1] loss: 589.751
[66,     1] loss: 711.149
[67,     1] loss: 707.819
[68,     1] loss: 559.071
[69,     1] loss: 619.371
[70,     1] loss: 547.745
[71,     1] loss: 573.025
[72,     1] loss: 601.713
[73,     1] loss: 564.724
[74,     1] loss: 523.875
[75,     1] loss: 535.417
[76,     1] loss: 500.582
[77,     1] loss: 564.183
[78,     1] loss: 728.404
[79,     1] loss: 670.596
[80,     1] loss: 531.739
[81,     1] loss: 642.162
[82,     1] loss: 549.431
[83,     1] loss: 636.195
[84,     1] loss: 494.189
[85,     1] loss: 568.145
[86,     1] loss: 456.565
[87,     1] loss: 547.867
[88,     1] loss: 674.877
[89,     1] loss: 458.217
[90,     1] loss: 654.096
[91,     1] loss: 492.971
[92,     1] loss: 654.282
[93,     1] loss: 453.467
[94,     1] loss: 604.451
[95,     1] loss: 460.248
[96,     1] loss: 541.901
[97,     1] loss: 416.340
[98,     1] loss: 565.758
[99,     1] loss: 418.785
[100,     1] loss: 450.301
[101,     1] loss: 464.860
[102,     1] loss: 356.708
[103,     1] loss: 441.738
[104,     1] loss: 377.150
[105,     1] loss: 442.717
[106,     1] loss: 376.573
[107,     1] loss: 323.320
[108,     1] loss: 317.214
[109,     1] loss: 375.422
[110,     1] loss: 407.449
[111,     1] loss: 649.287
[112,     1] loss: 624.914
[113,     1] loss: 481.374
[114,     1] loss: 533.134
[115,     1] loss: 444.945
[116,     1] loss: 552.362
[117,     1] loss: 398.134
[118,     1] loss: 527.578
[119,     1] loss: 373.744
[120,     1] loss: 423.502
[121,     1] loss: 565.443
[122,     1] loss: 369.183
[123,     1] loss: 679.292
[124,     1] loss: 620.176
[125,     1] loss: 744.431
[126,     1] loss: 492.615
[127,     1] loss: 808.773
[128,     1] loss: 584.372
[129,     1] loss: 530.578
[130,     1] loss: 677.107
[131,     1] loss: 492.595
[132,     1] loss: 525.706
[133,     1] loss: 462.962
[134,     1] loss: 443.414
[135,     1] loss: 458.701
[136,     1] loss: 406.896
[137,     1] loss: 390.877
[138,     1] loss: 364.059
[139,     1] loss: 337.750
[140,     1] loss: 418.658
[141,     1] loss: 300.239
[142,     1] loss: 369.239
[143,     1] loss: 365.499
[144,     1] loss: 346.441
[145,     1] loss: 286.523
[146,     1] loss: 278.620
[147,     1] loss: 348.528
[148,     1] loss: 588.816
[149,     1] loss: 684.766
[150,     1] loss: 549.026
[151,     1] loss: 545.700
[152,     1] loss: 506.785
[153,     1] loss: 593.410
[154,     1] loss: 400.029
[155,     1] loss: 600.885
[156,     1] loss: 372.680
[157,     1] loss: 662.378
[158,     1] loss: 757.629
[159,     1] loss: 495.044
[160,     1] loss: 493.628
[161,     1] loss: 599.702
[162,     1] loss: 470.872
[163,     1] loss: 503.350
[164,     1] loss: 316.102
[165,     1] loss: 401.152
[166,     1] loss: 366.879
[167,     1] loss: 389.149
[168,     1] loss: 354.986
[169,     1] loss: 315.386
[170,     1] loss: 293.731
[171,     1] loss: 334.253
[172,     1] loss: 296.248
[173,     1] loss: 313.208
[174,     1] loss: 301.291
[175,     1] loss: 264.304
[176,     1] loss: 280.185
[177,     1] loss: 269.734
[178,     1] loss: 278.270
[179,     1] loss: 242.375
[180,     1] loss: 246.605
[181,     1] loss: 283.436
[182,     1] loss: 809.069
[183,     1] loss: 2881.899
[184,     1] loss: 1346.284
[185,     1] loss: 1305.178
[186,     1] loss: 1202.729
[187,     1] loss: 1175.447
[188,     1] loss: 1147.424
[189,     1] loss: 1142.655
[190,     1] loss: 1144.443
[191,     1] loss: 1123.597
[192,     1] loss: 1128.897
[193,     1] loss: 1126.320
[194,     1] loss: 1135.065
[195,     1] loss: 1115.326
[196,     1] loss: 1100.832
[197,     1] loss: 1105.733
[198,     1] loss: 1100.050
[199,     1] loss: 1095.272
[200,     1] loss: 1079.090
Finished Training
Total time taken: 29.573718547821045
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.129
[2,     1] loss: 1233.828
[3,     1] loss: 1233.120
[4,     1] loss: 1228.508
[5,     1] loss: 1219.268
[6,     1] loss: 1213.352
[7,     1] loss: 1191.578
[8,     1] loss: 1162.261
[9,     1] loss: 1125.386
[10,     1] loss: 1061.718
[11,     1] loss: 1032.394
[12,     1] loss: 1001.068
[13,     1] loss: 1029.400
[14,     1] loss: 990.780
[15,     1] loss: 1017.267
[16,     1] loss: 1013.336
[17,     1] loss: 951.173
[18,     1] loss: 991.068
[19,     1] loss: 922.859
[20,     1] loss: 973.783
[21,     1] loss: 949.001
[22,     1] loss: 965.046
[23,     1] loss: 916.203
[24,     1] loss: 918.250
[25,     1] loss: 860.694
[26,     1] loss: 900.688
[27,     1] loss: 841.065
[28,     1] loss: 869.150
[29,     1] loss: 856.350
[30,     1] loss: 827.424
[31,     1] loss: 853.372
[32,     1] loss: 809.313
[33,     1] loss: 752.774
[34,     1] loss: 757.153
[35,     1] loss: 799.740
[36,     1] loss: 810.264
[37,     1] loss: 741.385
[38,     1] loss: 791.917
[39,     1] loss: 902.595
[40,     1] loss: 792.515
[41,     1] loss: 802.980
[42,     1] loss: 815.216
[43,     1] loss: 787.854
[44,     1] loss: 760.900
[45,     1] loss: 727.390
[46,     1] loss: 737.766
[47,     1] loss: 749.345
[48,     1] loss: 712.961
[49,     1] loss: 692.367
[50,     1] loss: 717.489
[51,     1] loss: 678.116
[52,     1] loss: 709.282
[53,     1] loss: 622.769
[54,     1] loss: 651.164
[55,     1] loss: 670.541
[56,     1] loss: 608.240
[57,     1] loss: 613.536
[58,     1] loss: 658.026
[59,     1] loss: 690.230
[60,     1] loss: 607.510
[61,     1] loss: 605.182
[62,     1] loss: 635.049
[63,     1] loss: 602.811
[64,     1] loss: 589.380
[65,     1] loss: 598.965
[66,     1] loss: 549.935
[67,     1] loss: 550.032
[68,     1] loss: 604.732
[69,     1] loss: 643.564
[70,     1] loss: 594.567
[71,     1] loss: 516.977
[72,     1] loss: 545.428
[73,     1] loss: 550.244
[74,     1] loss: 497.279
[75,     1] loss: 515.744
[76,     1] loss: 474.046
[77,     1] loss: 552.674
[78,     1] loss: 698.679
[79,     1] loss: 623.836
[80,     1] loss: 492.393
[81,     1] loss: 545.911
[82,     1] loss: 522.411
[83,     1] loss: 499.123
[84,     1] loss: 482.726
[85,     1] loss: 426.461
[86,     1] loss: 481.380
[87,     1] loss: 767.698
[88,     1] loss: 727.809
[89,     1] loss: 509.154
[90,     1] loss: 589.191
[91,     1] loss: 509.693
[92,     1] loss: 582.291
[93,     1] loss: 483.622
[94,     1] loss: 497.777
[95,     1] loss: 485.599
[96,     1] loss: 483.566
[97,     1] loss: 435.438
[98,     1] loss: 512.742
[99,     1] loss: 425.128
[100,     1] loss: 408.301
[101,     1] loss: 396.147
[102,     1] loss: 381.652
[103,     1] loss: 345.633
[104,     1] loss: 372.062
[105,     1] loss: 402.980
[106,     1] loss: 500.949
[107,     1] loss: 808.137
[108,     1] loss: 743.258
[109,     1] loss: 477.796
[110,     1] loss: 588.082
[111,     1] loss: 516.042
[112,     1] loss: 510.882
[113,     1] loss: 515.918
[114,     1] loss: 489.408
[115,     1] loss: 468.212
[116,     1] loss: 408.635
[117,     1] loss: 534.149
[118,     1] loss: 574.675
[119,     1] loss: 396.249
[120,     1] loss: 516.258
[121,     1] loss: 377.451
[122,     1] loss: 493.551
[123,     1] loss: 385.745
[124,     1] loss: 477.389
[125,     1] loss: 420.709
[126,     1] loss: 391.424
[127,     1] loss: 408.046
[128,     1] loss: 316.934
[129,     1] loss: 331.936
[130,     1] loss: 262.038
[131,     1] loss: 299.833
[132,     1] loss: 293.997
[133,     1] loss: 309.752
[134,     1] loss: 323.443
[135,     1] loss: 398.773
[136,     1] loss: 405.316
[137,     1] loss: 310.013
[138,     1] loss: 322.098
[139,     1] loss: 407.888
[140,     1] loss: 335.204
[141,     1] loss: 281.025
[142,     1] loss: 287.449
[143,     1] loss: 523.689
[144,     1] loss: 1975.937
Early stopping applied (best metric=0.3723423480987549)
Finished Training
Total time taken: 22.215238094329834
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.325
[2,     1] loss: 1228.844
[3,     1] loss: 1229.301
[4,     1] loss: 1230.299
[5,     1] loss: 1223.655
[6,     1] loss: 1212.795
[7,     1] loss: 1195.082
[8,     1] loss: 1147.695
[9,     1] loss: 1128.931
[10,     1] loss: 1105.304
[11,     1] loss: 1070.899
[12,     1] loss: 1030.940
[13,     1] loss: 1046.737
[14,     1] loss: 977.354
[15,     1] loss: 1026.388
[16,     1] loss: 974.860
[17,     1] loss: 947.035
[18,     1] loss: 988.884
[19,     1] loss: 984.059
[20,     1] loss: 935.012
[21,     1] loss: 951.454
[22,     1] loss: 947.565
[23,     1] loss: 919.675
[24,     1] loss: 878.954
[25,     1] loss: 898.051
[26,     1] loss: 943.165
[27,     1] loss: 906.561
[28,     1] loss: 873.989
[29,     1] loss: 898.415
[30,     1] loss: 858.682
[31,     1] loss: 837.923
[32,     1] loss: 823.225
[33,     1] loss: 840.395
[34,     1] loss: 856.084
[35,     1] loss: 829.725
[36,     1] loss: 767.265
[37,     1] loss: 820.932
[38,     1] loss: 812.782
[39,     1] loss: 738.205
[40,     1] loss: 769.858
[41,     1] loss: 753.918
[42,     1] loss: 692.182
[43,     1] loss: 729.519
[44,     1] loss: 849.979
[45,     1] loss: 836.391
[46,     1] loss: 707.777
[47,     1] loss: 791.419
[48,     1] loss: 716.055
[49,     1] loss: 751.773
[50,     1] loss: 694.925
[51,     1] loss: 678.285
[52,     1] loss: 651.579
[53,     1] loss: 645.253
[54,     1] loss: 786.013
[55,     1] loss: 861.979
[56,     1] loss: 597.860
[57,     1] loss: 744.750
[58,     1] loss: 623.056
[59,     1] loss: 695.681
[60,     1] loss: 585.877
[61,     1] loss: 645.926
[62,     1] loss: 572.838
[63,     1] loss: 650.210
[64,     1] loss: 619.390
[65,     1] loss: 584.736
[66,     1] loss: 562.217
[67,     1] loss: 575.940
[68,     1] loss: 575.237
[69,     1] loss: 523.403
[70,     1] loss: 524.917
[71,     1] loss: 500.313
[72,     1] loss: 477.516
[73,     1] loss: 474.661
[74,     1] loss: 453.862
[75,     1] loss: 1158.648
[76,     1] loss: 2326.055
[77,     1] loss: 1430.305
[78,     1] loss: 909.534
[79,     1] loss: 990.395
[80,     1] loss: 1034.499
[81,     1] loss: 1094.625
[82,     1] loss: 1100.931
[83,     1] loss: 1142.740
[84,     1] loss: 1090.215
[85,     1] loss: 1088.028
[86,     1] loss: 1085.037
[87,     1] loss: 1077.558
[88,     1] loss: 1060.300
[89,     1] loss: 1046.853
[90,     1] loss: 1055.714
Early stopping applied (best metric=0.3464634418487549)
Finished Training
Total time taken: 12.571933031082153
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.041
[2,     1] loss: 1238.712
[3,     1] loss: 1231.015
[4,     1] loss: 1230.833
[5,     1] loss: 1221.377
[6,     1] loss: 1216.337
[7,     1] loss: 1203.215
[8,     1] loss: 1169.381
[9,     1] loss: 1141.670
[10,     1] loss: 1081.639
[11,     1] loss: 1053.369
[12,     1] loss: 1025.451
[13,     1] loss: 1009.191
[14,     1] loss: 1038.401
[15,     1] loss: 983.244
[16,     1] loss: 1100.713
[17,     1] loss: 975.066
[18,     1] loss: 1025.078
[19,     1] loss: 983.108
[20,     1] loss: 945.852
[21,     1] loss: 964.435
[22,     1] loss: 940.027
[23,     1] loss: 923.403
[24,     1] loss: 966.813
[25,     1] loss: 933.995
[26,     1] loss: 899.443
[27,     1] loss: 919.467
[28,     1] loss: 899.572
[29,     1] loss: 887.397
[30,     1] loss: 865.854
[31,     1] loss: 835.769
[32,     1] loss: 820.822
[33,     1] loss: 801.790
[34,     1] loss: 785.958
[35,     1] loss: 775.437
[36,     1] loss: 743.982
[37,     1] loss: 772.677
[38,     1] loss: 879.036
[39,     1] loss: 720.466
[40,     1] loss: 759.485
[41,     1] loss: 788.384
[42,     1] loss: 699.386
[43,     1] loss: 700.434
[44,     1] loss: 793.430
[45,     1] loss: 929.653
[46,     1] loss: 669.187
[47,     1] loss: 794.675
[48,     1] loss: 697.990
[49,     1] loss: 776.207
[50,     1] loss: 708.018
[51,     1] loss: 745.437
[52,     1] loss: 681.378
[53,     1] loss: 661.407
[54,     1] loss: 685.591
[55,     1] loss: 629.140
[56,     1] loss: 627.025
[57,     1] loss: 599.479
[58,     1] loss: 612.374
[59,     1] loss: 590.693
[60,     1] loss: 659.520
[61,     1] loss: 624.115
[62,     1] loss: 546.583
[63,     1] loss: 533.954
[64,     1] loss: 582.540
[65,     1] loss: 557.405
[66,     1] loss: 595.760
[67,     1] loss: 853.488
[68,     1] loss: 596.249
[69,     1] loss: 572.468
[70,     1] loss: 619.879
[71,     1] loss: 627.451
[72,     1] loss: 605.707
[73,     1] loss: 625.229
[74,     1] loss: 571.693
[75,     1] loss: 621.147
[76,     1] loss: 541.088
[77,     1] loss: 576.705
[78,     1] loss: 544.073
[79,     1] loss: 550.313
[80,     1] loss: 580.500
[81,     1] loss: 462.661
Early stopping applied (best metric=0.4466400444507599)
Finished Training
Total time taken: 11.590680837631226
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1233.577
[2,     1] loss: 1234.610
[3,     1] loss: 1238.508
[4,     1] loss: 1229.522
[5,     1] loss: 1233.408
[6,     1] loss: 1224.629
[7,     1] loss: 1222.412
[8,     1] loss: 1211.083
[9,     1] loss: 1194.241
[10,     1] loss: 1158.994
[11,     1] loss: 1125.393
[12,     1] loss: 1116.567
[13,     1] loss: 1053.819
[14,     1] loss: 1039.162
[15,     1] loss: 1005.694
[16,     1] loss: 1005.445
[17,     1] loss: 1017.751
[18,     1] loss: 1009.820
[19,     1] loss: 1013.040
[20,     1] loss: 991.346
[21,     1] loss: 992.939
[22,     1] loss: 1004.122
[23,     1] loss: 982.233
[24,     1] loss: 946.794
[25,     1] loss: 984.285
[26,     1] loss: 950.142
[27,     1] loss: 928.695
[28,     1] loss: 967.286
[29,     1] loss: 952.402
[30,     1] loss: 907.745
[31,     1] loss: 879.825
[32,     1] loss: 897.845
[33,     1] loss: 842.310
[34,     1] loss: 825.292
[35,     1] loss: 901.152
[36,     1] loss: 878.315
[37,     1] loss: 803.445
[38,     1] loss: 849.936
[39,     1] loss: 852.842
[40,     1] loss: 827.541
[41,     1] loss: 808.995
[42,     1] loss: 814.301
[43,     1] loss: 763.230
[44,     1] loss: 820.786
[45,     1] loss: 849.537
[46,     1] loss: 799.770
[47,     1] loss: 783.112
[48,     1] loss: 749.510
[49,     1] loss: 752.043
[50,     1] loss: 797.328
[51,     1] loss: 705.285
[52,     1] loss: 672.699
[53,     1] loss: 735.954
[54,     1] loss: 864.609
[55,     1] loss: 1256.498
[56,     1] loss: 734.162
[57,     1] loss: 943.142
[58,     1] loss: 874.784
[59,     1] loss: 830.883
[60,     1] loss: 831.884
[61,     1] loss: 867.390
[62,     1] loss: 800.242
[63,     1] loss: 797.049
[64,     1] loss: 821.375
[65,     1] loss: 699.587
[66,     1] loss: 775.787
[67,     1] loss: 706.773
[68,     1] loss: 685.705
[69,     1] loss: 731.917
[70,     1] loss: 663.069
[71,     1] loss: 661.706
[72,     1] loss: 636.769
[73,     1] loss: 624.123
[74,     1] loss: 685.494
[75,     1] loss: 631.557
[76,     1] loss: 572.429
[77,     1] loss: 586.002
[78,     1] loss: 575.221
[79,     1] loss: 612.333
[80,     1] loss: 522.031
[81,     1] loss: 546.553
[82,     1] loss: 673.853
[83,     1] loss: 566.525
[84,     1] loss: 512.807
[85,     1] loss: 478.392
[86,     1] loss: 547.081
[87,     1] loss: 700.725
[88,     1] loss: 804.650
[89,     1] loss: 688.250
[90,     1] loss: 646.304
[91,     1] loss: 601.414
[92,     1] loss: 662.815
[93,     1] loss: 549.783
[94,     1] loss: 574.037
[95,     1] loss: 550.697
[96,     1] loss: 550.801
[97,     1] loss: 513.489
[98,     1] loss: 424.643
[99,     1] loss: 446.878
[100,     1] loss: 495.406
[101,     1] loss: 472.145
[102,     1] loss: 422.779
[103,     1] loss: 419.384
[104,     1] loss: 435.829
[105,     1] loss: 382.628
[106,     1] loss: 454.222
[107,     1] loss: 716.148
[108,     1] loss: 484.835
[109,     1] loss: 381.050
[110,     1] loss: 430.661
[111,     1] loss: 364.595
[112,     1] loss: 399.397
[113,     1] loss: 443.121
[114,     1] loss: 380.504
[115,     1] loss: 317.837
[116,     1] loss: 357.736
[117,     1] loss: 366.473
[118,     1] loss: 426.697
[119,     1] loss: 925.887
[120,     1] loss: 904.912
[121,     1] loss: 561.499
[122,     1] loss: 762.397
[123,     1] loss: 641.750
[124,     1] loss: 582.901
[125,     1] loss: 596.355
[126,     1] loss: 517.520
[127,     1] loss: 593.826
[128,     1] loss: 450.010
[129,     1] loss: 504.655
[130,     1] loss: 433.023
[131,     1] loss: 602.297
[132,     1] loss: 410.687
[133,     1] loss: 492.857
[134,     1] loss: 372.676
[135,     1] loss: 407.434
[136,     1] loss: 411.732
[137,     1] loss: 315.973
[138,     1] loss: 355.286
[139,     1] loss: 410.897
[140,     1] loss: 321.112
[141,     1] loss: 305.755
[142,     1] loss: 348.128
[143,     1] loss: 366.766
[144,     1] loss: 300.164
[145,     1] loss: 332.457
[146,     1] loss: 315.887
[147,     1] loss: 287.420
[148,     1] loss: 257.841
[149,     1] loss: 269.306
[150,     1] loss: 412.627
[151,     1] loss: 1257.572
[152,     1] loss: 1680.933
[153,     1] loss: 821.507
[154,     1] loss: 1119.908
[155,     1] loss: 1174.814
[156,     1] loss: 1154.997
[157,     1] loss: 1140.800
[158,     1] loss: 1048.331
[159,     1] loss: 984.541
[160,     1] loss: 993.448
[161,     1] loss: 941.567
[162,     1] loss: 995.817
[163,     1] loss: 917.129
[164,     1] loss: 957.214
[165,     1] loss: 928.042
[166,     1] loss: 881.994
[167,     1] loss: 928.346
[168,     1] loss: 875.472
[169,     1] loss: 867.747
[170,     1] loss: 850.039
[171,     1] loss: 834.293
[172,     1] loss: 889.043
[173,     1] loss: 871.309
[174,     1] loss: 896.653
[175,     1] loss: 850.967
[176,     1] loss: 892.308
[177,     1] loss: 817.417
Early stopping applied (best metric=0.33634957671165466)
Finished Training
Total time taken: 27.895795106887817
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.298
[2,     1] loss: 1228.059
[3,     1] loss: 1234.410
[4,     1] loss: 1230.246
[5,     1] loss: 1220.513
[6,     1] loss: 1215.500
[7,     1] loss: 1196.082
[8,     1] loss: 1160.109
[9,     1] loss: 1127.299
[10,     1] loss: 1083.091
[11,     1] loss: 1081.427
[12,     1] loss: 1041.498
[13,     1] loss: 1027.290
[14,     1] loss: 1004.434
[15,     1] loss: 982.101
[16,     1] loss: 1034.626
[17,     1] loss: 1013.970
[18,     1] loss: 973.098
[19,     1] loss: 944.113
[20,     1] loss: 954.042
[21,     1] loss: 916.637
[22,     1] loss: 928.596
[23,     1] loss: 929.230
[24,     1] loss: 874.571
[25,     1] loss: 856.374
[26,     1] loss: 914.939
[27,     1] loss: 830.869
[28,     1] loss: 822.643
[29,     1] loss: 836.879
[30,     1] loss: 886.297
[31,     1] loss: 849.882
[32,     1] loss: 827.980
[33,     1] loss: 750.752
[34,     1] loss: 781.295
[35,     1] loss: 740.637
[36,     1] loss: 796.317
[37,     1] loss: 701.997
[38,     1] loss: 748.528
[39,     1] loss: 796.242
[40,     1] loss: 1006.298
[41,     1] loss: 794.841
[42,     1] loss: 805.144
[43,     1] loss: 785.396
[44,     1] loss: 856.693
[45,     1] loss: 744.035
[46,     1] loss: 801.427
[47,     1] loss: 736.476
[48,     1] loss: 733.059
[49,     1] loss: 767.676
[50,     1] loss: 669.471
[51,     1] loss: 721.320
[52,     1] loss: 673.342
[53,     1] loss: 656.735
[54,     1] loss: 665.395
[55,     1] loss: 647.332
[56,     1] loss: 627.909
[57,     1] loss: 617.521
[58,     1] loss: 669.442
[59,     1] loss: 700.555
[60,     1] loss: 596.929
[61,     1] loss: 610.621
[62,     1] loss: 611.496
[63,     1] loss: 579.974
[64,     1] loss: 550.159
[65,     1] loss: 536.067
[66,     1] loss: 567.530
[67,     1] loss: 838.122
[68,     1] loss: 920.718
[69,     1] loss: 621.866
[70,     1] loss: 757.330
[71,     1] loss: 659.135
[72,     1] loss: 687.607
[73,     1] loss: 763.385
[74,     1] loss: 668.158
[75,     1] loss: 682.932
[76,     1] loss: 688.676
[77,     1] loss: 582.465
[78,     1] loss: 607.193
[79,     1] loss: 575.722
[80,     1] loss: 520.375
[81,     1] loss: 594.541
[82,     1] loss: 534.103
[83,     1] loss: 519.294
[84,     1] loss: 509.976
[85,     1] loss: 522.315
[86,     1] loss: 545.954
[87,     1] loss: 422.567
[88,     1] loss: 484.221
[89,     1] loss: 431.283
[90,     1] loss: 444.558
[91,     1] loss: 510.914
[92,     1] loss: 529.520
[93,     1] loss: 467.142
[94,     1] loss: 395.459
[95,     1] loss: 515.111
[96,     1] loss: 443.483
[97,     1] loss: 398.488
Early stopping applied (best metric=0.41863906383514404)
Finished Training
Total time taken: 16.38199234008789
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.526
[2,     1] loss: 1230.747
[3,     1] loss: 1231.938
[4,     1] loss: 1226.665
[5,     1] loss: 1226.484
[6,     1] loss: 1214.317
[7,     1] loss: 1188.942
[8,     1] loss: 1155.964
[9,     1] loss: 1108.160
[10,     1] loss: 1064.222
[11,     1] loss: 1051.554
[12,     1] loss: 1019.797
[13,     1] loss: 1049.489
[14,     1] loss: 1024.879
[15,     1] loss: 966.157
[16,     1] loss: 971.240
[17,     1] loss: 969.134
[18,     1] loss: 944.961
[19,     1] loss: 915.936
[20,     1] loss: 913.632
[21,     1] loss: 910.797
[22,     1] loss: 893.234
[23,     1] loss: 937.366
[24,     1] loss: 901.293
[25,     1] loss: 880.483
[26,     1] loss: 836.834
[27,     1] loss: 830.268
[28,     1] loss: 811.766
[29,     1] loss: 800.785
[30,     1] loss: 818.351
[31,     1] loss: 781.696
[32,     1] loss: 833.683
[33,     1] loss: 948.797
[34,     1] loss: 825.109
[35,     1] loss: 842.822
[36,     1] loss: 803.263
[37,     1] loss: 857.784
[38,     1] loss: 758.391
[39,     1] loss: 800.474
[40,     1] loss: 768.078
[41,     1] loss: 741.033
[42,     1] loss: 704.015
[43,     1] loss: 683.938
[44,     1] loss: 710.327
[45,     1] loss: 692.337
[46,     1] loss: 663.449
[47,     1] loss: 689.389
[48,     1] loss: 647.289
[49,     1] loss: 686.315
[50,     1] loss: 760.440
[51,     1] loss: 752.732
[52,     1] loss: 645.779
[53,     1] loss: 787.018
[54,     1] loss: 645.623
[55,     1] loss: 743.500
[56,     1] loss: 675.078
[57,     1] loss: 701.966
[58,     1] loss: 607.871
[59,     1] loss: 641.357
[60,     1] loss: 563.453
[61,     1] loss: 650.779
[62,     1] loss: 621.170
[63,     1] loss: 570.923
[64,     1] loss: 652.148
[65,     1] loss: 648.870
[66,     1] loss: 595.275
[67,     1] loss: 798.747
[68,     1] loss: 633.654
[69,     1] loss: 683.157
[70,     1] loss: 614.463
[71,     1] loss: 709.107
[72,     1] loss: 537.690
[73,     1] loss: 666.865
[74,     1] loss: 554.870
[75,     1] loss: 651.631
[76,     1] loss: 525.719
[77,     1] loss: 619.041
[78,     1] loss: 504.464
[79,     1] loss: 636.833
[80,     1] loss: 517.365
[81,     1] loss: 581.141
[82,     1] loss: 506.990
[83,     1] loss: 492.467
[84,     1] loss: 499.652
[85,     1] loss: 416.284
[86,     1] loss: 479.707
[87,     1] loss: 425.149
[88,     1] loss: 425.845
[89,     1] loss: 381.393
[90,     1] loss: 437.816
[91,     1] loss: 470.626
[92,     1] loss: 513.056
[93,     1] loss: 468.401
[94,     1] loss: 385.417
[95,     1] loss: 456.306
[96,     1] loss: 409.536
[97,     1] loss: 394.066
[98,     1] loss: 401.297
[99,     1] loss: 372.951
[100,     1] loss: 442.244
[101,     1] loss: 785.366
[102,     1] loss: 1669.736
[103,     1] loss: 577.253
[104,     1] loss: 766.400
[105,     1] loss: 943.904
[106,     1] loss: 896.167
[107,     1] loss: 850.046
[108,     1] loss: 841.633
[109,     1] loss: 859.065
[110,     1] loss: 750.863
[111,     1] loss: 678.910
[112,     1] loss: 758.732
[113,     1] loss: 744.717
[114,     1] loss: 703.246
[115,     1] loss: 752.080
[116,     1] loss: 649.228
[117,     1] loss: 670.879
[118,     1] loss: 707.797
[119,     1] loss: 638.707
[120,     1] loss: 647.794
[121,     1] loss: 605.958
[122,     1] loss: 566.382
[123,     1] loss: 578.873
[124,     1] loss: 515.272
[125,     1] loss: 630.799
[126,     1] loss: 556.885
[127,     1] loss: 460.025
[128,     1] loss: 469.479
[129,     1] loss: 437.644
[130,     1] loss: 490.572
[131,     1] loss: 410.339
[132,     1] loss: 444.545
[133,     1] loss: 469.200
[134,     1] loss: 562.154
[135,     1] loss: 439.559
[136,     1] loss: 395.507
[137,     1] loss: 390.399
[138,     1] loss: 466.469
[139,     1] loss: 615.303
[140,     1] loss: 361.491
[141,     1] loss: 521.741
[142,     1] loss: 585.601
[143,     1] loss: 504.954
[144,     1] loss: 577.238
Early stopping applied (best metric=0.3502875864505768)
Finished Training
Total time taken: 24.386847257614136
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.399
[2,     1] loss: 1232.858
[3,     1] loss: 1227.931
[4,     1] loss: 1225.528
[5,     1] loss: 1217.143
[6,     1] loss: 1202.773
[7,     1] loss: 1171.166
[8,     1] loss: 1119.794
[9,     1] loss: 1085.812
[10,     1] loss: 1096.327
[11,     1] loss: 1038.302
[12,     1] loss: 980.874
[13,     1] loss: 990.477
[14,     1] loss: 1033.691
[15,     1] loss: 1012.288
[16,     1] loss: 968.237
[17,     1] loss: 1005.441
[18,     1] loss: 1000.711
[19,     1] loss: 972.499
[20,     1] loss: 938.551
[21,     1] loss: 918.297
[22,     1] loss: 937.105
[23,     1] loss: 900.589
[24,     1] loss: 890.953
[25,     1] loss: 862.672
[26,     1] loss: 839.116
[27,     1] loss: 902.280
[28,     1] loss: 881.505
[29,     1] loss: 828.930
[30,     1] loss: 817.076
[31,     1] loss: 916.712
[32,     1] loss: 924.703
[33,     1] loss: 809.683
[34,     1] loss: 925.964
[35,     1] loss: 795.490
[36,     1] loss: 912.589
[37,     1] loss: 803.999
[38,     1] loss: 848.532
[39,     1] loss: 767.314
[40,     1] loss: 796.562
[41,     1] loss: 772.363
[42,     1] loss: 714.801
[43,     1] loss: 744.183
[44,     1] loss: 741.648
[45,     1] loss: 730.906
[46,     1] loss: 697.888
[47,     1] loss: 693.872
[48,     1] loss: 674.207
[49,     1] loss: 656.710
[50,     1] loss: 661.696
[51,     1] loss: 656.383
[52,     1] loss: 615.264
[53,     1] loss: 691.370
[54,     1] loss: 893.703
[55,     1] loss: 651.227
[56,     1] loss: 692.969
[57,     1] loss: 676.014
[58,     1] loss: 699.237
[59,     1] loss: 627.106
[60,     1] loss: 712.273
[61,     1] loss: 712.240
[62,     1] loss: 584.990
[63,     1] loss: 670.786
[64,     1] loss: 616.593
[65,     1] loss: 653.332
[66,     1] loss: 559.051
[67,     1] loss: 705.660
[68,     1] loss: 710.020
[69,     1] loss: 596.818
[70,     1] loss: 648.563
[71,     1] loss: 603.884
[72,     1] loss: 605.222
[73,     1] loss: 660.802
[74,     1] loss: 598.798
[75,     1] loss: 639.955
[76,     1] loss: 585.929
[77,     1] loss: 575.081
[78,     1] loss: 513.792
[79,     1] loss: 514.631
[80,     1] loss: 564.745
[81,     1] loss: 507.580
[82,     1] loss: 582.623
[83,     1] loss: 567.216
[84,     1] loss: 482.692
[85,     1] loss: 644.790
[86,     1] loss: 514.228
[87,     1] loss: 587.452
[88,     1] loss: 451.805
[89,     1] loss: 558.152
[90,     1] loss: 437.808
[91,     1] loss: 508.741
[92,     1] loss: 485.557
[93,     1] loss: 415.706
[94,     1] loss: 431.294
[95,     1] loss: 434.283
[96,     1] loss: 446.821
[97,     1] loss: 410.374
[98,     1] loss: 421.256
[99,     1] loss: 375.185
[100,     1] loss: 407.788
[101,     1] loss: 404.306
[102,     1] loss: 508.698
[103,     1] loss: 693.325
[104,     1] loss: 933.491
[105,     1] loss: 457.140
[106,     1] loss: 733.805
[107,     1] loss: 625.200
[108,     1] loss: 579.349
[109,     1] loss: 590.518
[110,     1] loss: 518.053
Early stopping applied (best metric=0.40740928053855896)
Finished Training
Total time taken: 18.288394927978516
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.799
[2,     1] loss: 1234.091
[3,     1] loss: 1229.759
[4,     1] loss: 1234.427
[5,     1] loss: 1225.337
[6,     1] loss: 1220.470
[7,     1] loss: 1224.304
[8,     1] loss: 1208.136
[9,     1] loss: 1199.264
[10,     1] loss: 1149.718
[11,     1] loss: 1137.466
[12,     1] loss: 1084.062
[13,     1] loss: 1025.824
[14,     1] loss: 1008.288
[15,     1] loss: 1043.387
[16,     1] loss: 1041.813
[17,     1] loss: 973.917
[18,     1] loss: 966.173
[19,     1] loss: 983.768
[20,     1] loss: 967.567
[21,     1] loss: 981.485
[22,     1] loss: 970.014
[23,     1] loss: 964.635
[24,     1] loss: 913.909
[25,     1] loss: 928.987
[26,     1] loss: 863.762
[27,     1] loss: 965.622
[28,     1] loss: 887.780
[29,     1] loss: 904.646
[30,     1] loss: 845.759
[31,     1] loss: 919.662
[32,     1] loss: 855.244
[33,     1] loss: 892.966
[34,     1] loss: 880.274
[35,     1] loss: 947.977
[36,     1] loss: 811.317
[37,     1] loss: 902.282
[38,     1] loss: 797.152
[39,     1] loss: 853.935
[40,     1] loss: 824.513
[41,     1] loss: 870.025
[42,     1] loss: 841.036
[43,     1] loss: 825.775
[44,     1] loss: 792.464
[45,     1] loss: 786.273
[46,     1] loss: 777.886
[47,     1] loss: 764.337
[48,     1] loss: 723.067
[49,     1] loss: 721.223
[50,     1] loss: 697.250
[51,     1] loss: 723.595
[52,     1] loss: 770.462
[53,     1] loss: 996.638
[54,     1] loss: 755.539
[55,     1] loss: 798.318
[56,     1] loss: 756.560
[57,     1] loss: 784.355
[58,     1] loss: 741.608
[59,     1] loss: 703.187
[60,     1] loss: 774.977
[61,     1] loss: 726.591
[62,     1] loss: 673.444
[63,     1] loss: 638.619
[64,     1] loss: 676.438
[65,     1] loss: 657.255
[66,     1] loss: 681.958
[67,     1] loss: 627.339
[68,     1] loss: 583.542
[69,     1] loss: 557.397
[70,     1] loss: 612.448
[71,     1] loss: 588.096
[72,     1] loss: 566.458
[73,     1] loss: 522.845
[74,     1] loss: 548.761
[75,     1] loss: 546.670
[76,     1] loss: 529.675
[77,     1] loss: 496.441
[78,     1] loss: 593.096
[79,     1] loss: 788.055
[80,     1] loss: 1028.643
[81,     1] loss: 558.465
[82,     1] loss: 728.426
[83,     1] loss: 694.836
[84,     1] loss: 662.465
[85,     1] loss: 700.749
[86,     1] loss: 676.245
[87,     1] loss: 604.555
[88,     1] loss: 612.722
[89,     1] loss: 594.080
[90,     1] loss: 613.932
[91,     1] loss: 554.703
[92,     1] loss: 515.681
[93,     1] loss: 519.210
[94,     1] loss: 562.797
[95,     1] loss: 500.993
[96,     1] loss: 528.327
[97,     1] loss: 445.795
[98,     1] loss: 476.071
[99,     1] loss: 487.778
[100,     1] loss: 405.941
Early stopping applied (best metric=0.36417654156684875)
Finished Training
Total time taken: 16.773020029067993
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1232.437
[2,     1] loss: 1232.808
[3,     1] loss: 1230.979
[4,     1] loss: 1231.096
[5,     1] loss: 1220.229
[6,     1] loss: 1218.591
[7,     1] loss: 1198.680
[8,     1] loss: 1179.819
[9,     1] loss: 1137.335
[10,     1] loss: 1100.344
[11,     1] loss: 1062.145
[12,     1] loss: 1032.040
[13,     1] loss: 1044.983
[14,     1] loss: 949.500
[15,     1] loss: 1077.838
[16,     1] loss: 1023.441
[17,     1] loss: 981.871
[18,     1] loss: 983.875
[19,     1] loss: 999.867
[20,     1] loss: 980.044
[21,     1] loss: 959.876
[22,     1] loss: 940.926
[23,     1] loss: 951.411
[24,     1] loss: 925.621
[25,     1] loss: 885.696
[26,     1] loss: 860.587
[27,     1] loss: 877.008
[28,     1] loss: 890.135
[29,     1] loss: 835.780
[30,     1] loss: 865.896
[31,     1] loss: 815.569
[32,     1] loss: 823.043
[33,     1] loss: 816.516
[34,     1] loss: 812.250
[35,     1] loss: 815.055
[36,     1] loss: 739.699
[37,     1] loss: 819.665
[38,     1] loss: 936.795
[39,     1] loss: 863.231
[40,     1] loss: 781.540
[41,     1] loss: 771.582
[42,     1] loss: 790.309
[43,     1] loss: 752.898
[44,     1] loss: 804.225
[45,     1] loss: 736.165
[46,     1] loss: 762.396
[47,     1] loss: 727.161
[48,     1] loss: 754.285
[49,     1] loss: 725.354
[50,     1] loss: 733.670
[51,     1] loss: 713.206
[52,     1] loss: 660.134
[53,     1] loss: 656.066
[54,     1] loss: 632.007
[55,     1] loss: 569.748
[56,     1] loss: 638.083
[57,     1] loss: 629.103
[58,     1] loss: 786.143
[59,     1] loss: 794.607
[60,     1] loss: 663.353
[61,     1] loss: 690.112
[62,     1] loss: 661.522
[63,     1] loss: 738.212
[64,     1] loss: 602.616
[65,     1] loss: 667.828
[66,     1] loss: 585.102
[67,     1] loss: 692.757
[68,     1] loss: 577.628
[69,     1] loss: 579.518
[70,     1] loss: 576.544
[71,     1] loss: 511.556
[72,     1] loss: 646.359
[73,     1] loss: 512.989
[74,     1] loss: 539.111
[75,     1] loss: 484.788
[76,     1] loss: 509.985
[77,     1] loss: 534.990
[78,     1] loss: 620.142
[79,     1] loss: 501.860
[80,     1] loss: 582.234
[81,     1] loss: 677.399
[82,     1] loss: 519.694
[83,     1] loss: 638.457
[84,     1] loss: 464.705
[85,     1] loss: 641.182
Early stopping applied (best metric=0.39310070872306824)
Finished Training
Total time taken: 14.181358814239502
{'Hydroxylation-K Validation Accuracy': 0.7171394799054374, 'Hydroxylation-K Validation Sensitivity': 0.6637037037037037, 'Hydroxylation-K Validation Specificity': 0.7298245614035088, 'Hydroxylation-K Validation Precision': 0.3843313978762895, 'Hydroxylation-K AUC ROC': 0.7813255360623782, 'Hydroxylation-K AUC PR': 0.566950198073773, 'Hydroxylation-K MCC': 0.33351335812577626, 'Hydroxylation-K F1': 0.48280165124900654, 'Validation Loss (Hydroxylation-K)': 0.46525981426239016, 'Hydroxylation-P Validation Accuracy': 0.7739128301439859, 'Hydroxylation-P Validation Sensitivity': 0.804973544973545, 'Hydroxylation-P Validation Specificity': 0.7672652002593645, 'Hydroxylation-P Validation Precision': 0.43295314592864137, 'Hydroxylation-P AUC ROC': 0.8455327166695564, 'Hydroxylation-P AUC PR': 0.5931816858856338, 'Hydroxylation-P MCC': 0.4665763355408324, 'Hydroxylation-P F1': 0.5611366241031863, 'Validation Loss (Hydroxylation-P)': 0.3681258817513784, 'Validation Loss (total)': 0.833385701974233, 'TimeToTrain': 19.666759904225668}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0045229783993589075,
 'learning_rate_Hydroxylation-K': 0.002073778401698448,
 'learning_rate_Hydroxylation-P': 0.002969443560891196,
 'log_base': 2.909956154457818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3730279322,
 'sample_weights': [1.529005018876468, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.415075786848898,
 'weight_decay_Hydroxylation-K': 6.498960915979083,
 'weight_decay_Hydroxylation-P': 0.06809658567763821}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.811
[2,     1] loss: 1251.069
[3,     1] loss: 1235.428
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009829067780426025,
 'learning_rate_Hydroxylation-K': 0.004279358924935603,
 'learning_rate_Hydroxylation-P': 0.0035063739762100686,
 'log_base': 2.511375511043698,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1040657686,
 'sample_weights': [1.5629470403079901, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.26794951153258,
 'weight_decay_Hydroxylation-K': 5.123717548928652,
 'weight_decay_Hydroxylation-P': 0.5981732340243636}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.069
[2,     1] loss: 1293.956
[3,     1] loss: 1293.335
[4,     1] loss: 1291.812
[5,     1] loss: 1289.170
[6,     1] loss: 1288.256
[7,     1] loss: 1286.816
[8,     1] loss: 1286.101
[9,     1] loss: 1285.574
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00045934376130729723,
 'learning_rate_Hydroxylation-K': 0.007582051347773169,
 'learning_rate_Hydroxylation-P': 0.003116341450471227,
 'log_base': 2.6117910619078,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2686310518,
 'sample_weights': [1.812975285019619, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5088809402359544,
 'weight_decay_Hydroxylation-K': 1.4831277616052752,
 'weight_decay_Hydroxylation-P': 9.110139592028588}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.174
[2,     1] loss: 1280.120
[3,     1] loss: 1278.126
[4,     1] loss: 1277.694
[5,     1] loss: 1275.673
[6,     1] loss: 1274.645
[7,     1] loss: 1274.961
[8,     1] loss: 1272.827
[9,     1] loss: 1273.759
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002528669490254626,
 'learning_rate_Hydroxylation-K': 0.002628975779459533,
 'learning_rate_Hydroxylation-P': 0.005148773140591336,
 'log_base': 2.249009620680521,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4078420012,
 'sample_weights': [1.7389376760519453, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.25114461401473376,
 'weight_decay_Hydroxylation-K': 3.814677413754944,
 'weight_decay_Hydroxylation-P': 4.671560925636323}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.512
[2,     1] loss: 1344.858
[3,     1] loss: 1339.743
[4,     1] loss: 1340.351
[5,     1] loss: 1336.631
[6,     1] loss: 1333.132
[7,     1] loss: 1332.722
[8,     1] loss: 1310.985
[9,     1] loss: 1296.025
[10,     1] loss: 1261.651
[11,     1] loss: 1240.573
[12,     1] loss: 1205.964
[13,     1] loss: 1172.534
[14,     1] loss: 1162.745
[15,     1] loss: 1140.594
[16,     1] loss: 1173.981
[17,     1] loss: 1124.350
[18,     1] loss: 1098.749
[19,     1] loss: 1095.495
[20,     1] loss: 1110.229
[21,     1] loss: 1120.991
[22,     1] loss: 1080.333
[23,     1] loss: 1098.319
[24,     1] loss: 1059.559
[25,     1] loss: 1060.526
[26,     1] loss: 1082.190
[27,     1] loss: 1091.065
[28,     1] loss: 1007.598
[29,     1] loss: 1025.738
[30,     1] loss: 1045.915
[31,     1] loss: 1039.547
[32,     1] loss: 1039.889
[33,     1] loss: 1005.792
[34,     1] loss: 993.087
[35,     1] loss: 1012.626
[36,     1] loss: 964.747
[37,     1] loss: 923.618
[38,     1] loss: 977.816
[39,     1] loss: 1025.894
[40,     1] loss: 950.946
[41,     1] loss: 973.390
[42,     1] loss: 990.444
[43,     1] loss: 982.219
[44,     1] loss: 910.694
[45,     1] loss: 921.984
[46,     1] loss: 938.951
[47,     1] loss: 912.005
[48,     1] loss: 906.653
[49,     1] loss: 901.649
[50,     1] loss: 944.336
[51,     1] loss: 893.778
[52,     1] loss: 880.978
[53,     1] loss: 920.904
[54,     1] loss: 882.582
[55,     1] loss: 956.670
[56,     1] loss: 838.961
[57,     1] loss: 875.755
[58,     1] loss: 811.143
[59,     1] loss: 854.702
[60,     1] loss: 864.313
[61,     1] loss: 781.479
[62,     1] loss: 822.745
[63,     1] loss: 823.827
[64,     1] loss: 747.072
[65,     1] loss: 803.235
[66,     1] loss: 779.424
[67,     1] loss: 769.725
[68,     1] loss: 739.038
[69,     1] loss: 741.328
[70,     1] loss: 772.161
[71,     1] loss: 734.640
[72,     1] loss: 704.781
[73,     1] loss: 694.020
[74,     1] loss: 741.173
[75,     1] loss: 730.021
[76,     1] loss: 727.489
[77,     1] loss: 741.019
[78,     1] loss: 708.426
[79,     1] loss: 740.886
[80,     1] loss: 691.775
[81,     1] loss: 671.739
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00935604212697776,
 'learning_rate_Hydroxylation-K': 0.009325581641058036,
 'learning_rate_Hydroxylation-P': 0.001556037599051832,
 'log_base': 1.0299577374786548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 166275319,
 'sample_weights': [2.059794999217025, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.265429736360157,
 'weight_decay_Hydroxylation-K': 0.22436053262693,
 'weight_decay_Hydroxylation-P': 1.4442158154502922}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18529.035
Exploding loss, terminate run (best metric=0.5463272929191589)
Finished Training
Total time taken: 0.21200132369995117
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18359.445
Exploding loss, terminate run (best metric=0.5278489589691162)
Finished Training
Total time taken: 0.22600078582763672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18553.232
Exploding loss, terminate run (best metric=0.5271729230880737)
Finished Training
Total time taken: 0.22600388526916504
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18354.156
Exploding loss, terminate run (best metric=0.5264147520065308)
Finished Training
Total time taken: 0.20600104331970215
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18393.068
Exploding loss, terminate run (best metric=0.5559492111206055)
Finished Training
Total time taken: 0.22100090980529785
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18332.846
Exploding loss, terminate run (best metric=0.5328783988952637)
Finished Training
Total time taken: 0.2480008602142334
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18283.186
Exploding loss, terminate run (best metric=0.5277187824249268)
Finished Training
Total time taken: 0.22999835014343262
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18346.865
Exploding loss, terminate run (best metric=0.5269858241081238)
Finished Training
Total time taken: 0.20802998542785645
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18383.766
Exploding loss, terminate run (best metric=0.5274351239204407)
Finished Training
Total time taken: 0.20800113677978516
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18344.859
Exploding loss, terminate run (best metric=0.5274786353111267)
Finished Training
Total time taken: 0.22900104522705078
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18311.508
Exploding loss, terminate run (best metric=0.5319656729698181)
Finished Training
Total time taken: 0.22100067138671875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18324.834
Exploding loss, terminate run (best metric=0.5356181263923645)
Finished Training
Total time taken: 0.20699715614318848
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18426.180
Exploding loss, terminate run (best metric=0.6032443642616272)
Finished Training
Total time taken: 0.2370007038116455
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18326.021
Exploding loss, terminate run (best metric=0.5267335176467896)
Finished Training
Total time taken: 0.2340376377105713
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18530.943
Exploding loss, terminate run (best metric=0.5282996296882629)
Finished Training
Total time taken: 0.1979990005493164
{'Hydroxylation-K Validation Accuracy': 0.400531914893617, 'Hydroxylation-K Validation Sensitivity': 0.6962962962962963, 'Hydroxylation-K Validation Specificity': 0.3263157894736842, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6276998050682261, 'Hydroxylation-K AUC PR': 0.32254075343476885, 'Hydroxylation-K MCC': 0.023674258214165227, 'Hydroxylation-K F1': 0.2550178692166522, 'Validation Loss (Hydroxylation-K)': 0.5623842676480612, 'Hydroxylation-P Validation Accuracy': 0.39116518620036206, 'Hydroxylation-P Validation Sensitivity': 0.7028571428571428, 'Hydroxylation-P Validation Specificity': 0.3243353783231084, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5924042900449454, 'Hydroxylation-P AUC PR': 0.2589010994566091, 'Hydroxylation-P MCC': 0.025599739457896866, 'Hydroxylation-P F1': 0.23364105133945828, 'Validation Loss (Hydroxylation-P)': 0.5368047475814819, 'Validation Loss (total)': 1.0991890112559, 'TimeToTrain': 0.22073829968770345}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009864625853839126,
 'learning_rate_Hydroxylation-K': 0.0023505581908164737,
 'learning_rate_Hydroxylation-P': 0.004452111182585111,
 'log_base': 2.711717370729535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2745846082,
 'sample_weights': [56.599178987621485, 7.060196986705661],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.024242044511441074,
 'weight_decay_Hydroxylation-K': 2.0272506284893805,
 'weight_decay_Hydroxylation-P': 2.660181557133147}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.893
[2,     1] loss: 1268.053
[3,     1] loss: 1260.930
[4,     1] loss: 1261.410
[5,     1] loss: 1257.975
[6,     1] loss: 1260.842
[7,     1] loss: 1255.548
[8,     1] loss: 1257.743
[9,     1] loss: 1250.797
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004248366686661704,
 'learning_rate_Hydroxylation-K': 0.0008516685188417318,
 'learning_rate_Hydroxylation-P': 0.0035092236282414455,
 'log_base': 2.9816927439536944,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1414506805,
 'sample_weights': [1.6734893932264532, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.65995839619292,
 'weight_decay_Hydroxylation-K': 4.392384110126848,
 'weight_decay_Hydroxylation-P': 2.2018320161707847}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1227.004
[2,     1] loss: 1232.101
[3,     1] loss: 1227.896
[4,     1] loss: 1228.177
[5,     1] loss: 1227.670
[6,     1] loss: 1220.019
[7,     1] loss: 1213.884
[8,     1] loss: 1191.056
[9,     1] loss: 1156.627
[10,     1] loss: 1109.836
[11,     1] loss: 1117.704
[12,     1] loss: 1078.070
[13,     1] loss: 1027.149
[14,     1] loss: 1020.741
[15,     1] loss: 985.069
[16,     1] loss: 1053.366
[17,     1] loss: 1027.260
[18,     1] loss: 1030.666
[19,     1] loss: 1021.909
[20,     1] loss: 997.187
[21,     1] loss: 1017.764
[22,     1] loss: 988.922
[23,     1] loss: 944.829
[24,     1] loss: 943.326
[25,     1] loss: 929.037
[26,     1] loss: 920.272
[27,     1] loss: 985.671
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064117912401040434,
 'learning_rate_Hydroxylation-K': 0.0064329259656625065,
 'learning_rate_Hydroxylation-P': 0.0021795586974373395,
 'log_base': 2.444212188957492,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 913739485,
 'sample_weights': [1.5281067590798427, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.746872488484577,
 'weight_decay_Hydroxylation-K': 1.0178354732081543,
 'weight_decay_Hydroxylation-P': 0.4685666763090648}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.851
[2,     1] loss: 1301.568
[3,     1] loss: 1313.672
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002443642005162309,
 'learning_rate_Hydroxylation-K': 0.0030826933441647697,
 'learning_rate_Hydroxylation-P': 0.006623835421451,
 'log_base': 2.8639130531854295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3577745334,
 'sample_weights': [1.8679651445269334, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7116115687376918,
 'weight_decay_Hydroxylation-K': 4.492341427832545,
 'weight_decay_Hydroxylation-P': 9.924510829111384}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.070
[2,     1] loss: 1242.343
[3,     1] loss: 1241.172
[4,     1] loss: 1242.911
[5,     1] loss: 1239.811
[6,     1] loss: 1239.271
[7,     1] loss: 1240.569
[8,     1] loss: 1237.096
[9,     1] loss: 1234.472
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009056709467577067,
 'learning_rate_Hydroxylation-K': 0.007301133167236119,
 'learning_rate_Hydroxylation-P': 0.0028225780680742173,
 'log_base': 1.0540740331266696,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3369787354,
 'sample_weights': [1.586638257741435, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.51253758474804,
 'weight_decay_Hydroxylation-K': 0.9396358972661941,
 'weight_decay_Hydroxylation-P': 1.3874295697157097}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10267.930
[2,     1] loss: 10290.165
[3,     1] loss: 10273.968
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008585974547605669,
 'learning_rate_Hydroxylation-K': 0.009908273806833082,
 'learning_rate_Hydroxylation-P': 0.004112652805151506,
 'log_base': 1.1434375454159647,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3954386105,
 'sample_weights': [31.700682529317902, 3.962737970863389],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.1625291288041595,
 'weight_decay_Hydroxylation-K': 0.9552827909721747,
 'weight_decay_Hydroxylation-P': 2.825363132483492}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4043.935
[2,     1] loss: 4036.688
[3,     1] loss: 4034.072
[4,     1] loss: 4022.395
[5,     1] loss: 4030.516
[6,     1] loss: 4039.925
[7,     1] loss: 4038.394
[8,     1] loss: 4059.079
[9,     1] loss: 4013.940
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005351323359402535,
 'learning_rate_Hydroxylation-K': 0.0020078690919049288,
 'learning_rate_Hydroxylation-P': 0.006769486325303204,
 'log_base': 2.0177645337902232,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3777641193,
 'sample_weights': [12.454895241506085, 1.5569218817606285],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.232240071156708,
 'weight_decay_Hydroxylation-K': 0.49920793729127594,
 'weight_decay_Hydroxylation-P': 5.696510912267229}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.912
[2,     1] loss: 1407.867
[3,     1] loss: 1409.420
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00932233453434759,
 'learning_rate_Hydroxylation-K': 0.008748573620737471,
 'learning_rate_Hydroxylation-P': 0.0017563344466438677,
 'log_base': 1.302893657506853,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1467551157,
 'sample_weights': [2.378157232009574, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.691232125056695,
 'weight_decay_Hydroxylation-K': 2.0447970458545814,
 'weight_decay_Hydroxylation-P': 1.986243679568772}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2247.412
[2,     1] loss: 2238.786
[3,     1] loss: 2241.697
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0080271479552215,
 'learning_rate_Hydroxylation-K': 0.0055293746394619045,
 'learning_rate_Hydroxylation-P': 0.0031632148472975955,
 'log_base': 2.5612012729256124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3039691753,
 'sample_weights': [6.309602698211997, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.163005986256682,
 'weight_decay_Hydroxylation-K': 8.842592278704073,
 'weight_decay_Hydroxylation-P': 1.5461928308546542}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.715
[2,     1] loss: 1286.763
[3,     1] loss: 1287.948
[4,     1] loss: 1281.763
[5,     1] loss: 1280.461
[6,     1] loss: 1283.764
[7,     1] loss: 1279.135
[8,     1] loss: 1283.646
[9,     1] loss: 1276.974
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003662362218333083,
 'learning_rate_Hydroxylation-K': 0.0026616395562751127,
 'learning_rate_Hydroxylation-P': 0.00814797518255379,
 'log_base': 2.619690699183841,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3923368625,
 'sample_weights': [1.7751037188354766, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.7274007659879045,
 'weight_decay_Hydroxylation-K': 3.231067096019935,
 'weight_decay_Hydroxylation-P': 4.377413248269113}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.012
[2,     1] loss: 1272.359
[3,     1] loss: 1279.830
[4,     1] loss: 1275.373
[5,     1] loss: 1270.706
[6,     1] loss: 1270.559
[7,     1] loss: 1270.997
[8,     1] loss: 1258.531
[9,     1] loss: 1247.398
[10,     1] loss: 1222.339
[11,     1] loss: 1190.988
[12,     1] loss: 1142.783
[13,     1] loss: 1113.078
[14,     1] loss: 1083.440
[15,     1] loss: 1044.160
[16,     1] loss: 1077.735
[17,     1] loss: 1073.844
[18,     1] loss: 1056.878
[19,     1] loss: 1058.599
[20,     1] loss: 1041.852
[21,     1] loss: 1000.284
[22,     1] loss: 1039.583
[23,     1] loss: 1059.086
[24,     1] loss: 985.072
[25,     1] loss: 1005.367
[26,     1] loss: 1009.395
[27,     1] loss: 978.347
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006469983559626813,
 'learning_rate_Hydroxylation-K': 0.0032024105103320355,
 'learning_rate_Hydroxylation-P': 0.006331746727539092,
 'log_base': 2.974490059163249,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1243158570,
 'sample_weights': [1.7334845552267857, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.314238404668012,
 'weight_decay_Hydroxylation-K': 1.3914751219741612,
 'weight_decay_Hydroxylation-P': 8.880939950700212}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.209
[2,     1] loss: 1240.523
[3,     1] loss: 1231.947
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032272890660852496,
 'learning_rate_Hydroxylation-K': 0.0011964632058506632,
 'learning_rate_Hydroxylation-P': 0.00730204517855782,
 'log_base': 2.9287488199115153,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2699250480,
 'sample_weights': [1.531497189728854, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.584965901905119,
 'weight_decay_Hydroxylation-K': 6.29951216482509,
 'weight_decay_Hydroxylation-P': 1.2513224609905977}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.346
[2,     1] loss: 1242.499
[3,     1] loss: 1236.210
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00036022158653559356,
 'learning_rate_Hydroxylation-K': 0.0014422806562633089,
 'learning_rate_Hydroxylation-P': 0.009701125665694471,
 'log_base': 2.4992635387625812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2615802193,
 'sample_weights': [1.5535841323907194, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.50136229285818,
 'weight_decay_Hydroxylation-K': 6.40107725324879,
 'weight_decay_Hydroxylation-P': 0.46773008424585316}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.635
[2,     1] loss: 1294.696
[3,     1] loss: 1294.440
[4,     1] loss: 1291.574
[5,     1] loss: 1290.343
[6,     1] loss: 1291.246
[7,     1] loss: 1294.124
[8,     1] loss: 1292.213
[9,     1] loss: 1290.963
[10,     1] loss: 1289.570
[11,     1] loss: 1289.506
[12,     1] loss: 1290.608
[13,     1] loss: 1289.718
[14,     1] loss: 1290.091
[15,     1] loss: 1285.714
[16,     1] loss: 1291.359
[17,     1] loss: 1288.923
[18,     1] loss: 1286.676
[19,     1] loss: 1284.041
[20,     1] loss: 1281.537
[21,     1] loss: 1283.242
[22,     1] loss: 1279.888
[23,     1] loss: 1273.978
[24,     1] loss: 1275.335
[25,     1] loss: 1270.428
[26,     1] loss: 1265.793
[27,     1] loss: 1260.332
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022125956552207456,
 'learning_rate_Hydroxylation-K': 0.006808958893577558,
 'learning_rate_Hydroxylation-P': 0.006366910118842882,
 'log_base': 2.330177107776355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1265754300,
 'sample_weights': [1.8225439389630143, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.084219953550578,
 'weight_decay_Hydroxylation-K': 0.8922336902017776,
 'weight_decay_Hydroxylation-P': 5.437622150759092}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.740
[2,     1] loss: 1322.521
[3,     1] loss: 1326.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0076138676531254,
 'learning_rate_Hydroxylation-K': 0.0044050278822129335,
 'learning_rate_Hydroxylation-P': 0.004756350259671133,
 'log_base': 2.293787353249545,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3804497071,
 'sample_weights': [1.9734670398311136, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.262439998813351,
 'weight_decay_Hydroxylation-K': 2.566419784814811,
 'weight_decay_Hydroxylation-P': 2.436181828529981}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1339.483
[2,     1] loss: 1333.626
[3,     1] loss: 1331.439
[4,     1] loss: 1336.179
[5,     1] loss: 1331.290
[6,     1] loss: 1332.624
[7,     1] loss: 1332.697
[8,     1] loss: 1329.968
[9,     1] loss: 1326.427
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0051940921648160054,
 'learning_rate_Hydroxylation-K': 0.0005771664112986373,
 'learning_rate_Hydroxylation-P': 0.0013456843834948268,
 'log_base': 1.6597492857156755,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2537594978,
 'sample_weights': [2.0108822759244105, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.588906630128275,
 'weight_decay_Hydroxylation-K': 0.022724719319449305,
 'weight_decay_Hydroxylation-P': 1.514529253359976}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1606.450
[2,     1] loss: 1607.196
[3,     1] loss: 1598.756
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006427350081140342,
 'learning_rate_Hydroxylation-K': 0.009446021216803721,
 'learning_rate_Hydroxylation-P': 0.0012947907956559893,
 'log_base': 1.086689136073537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3927570051,
 'sample_weights': [3.2949542854285907, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.4446786080882745,
 'weight_decay_Hydroxylation-K': 3.9838694491119018,
 'weight_decay_Hydroxylation-P': 1.7818368399836286}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6526.512
[2,     1] loss: 6531.276
[3,     1] loss: 6476.014
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009097707974071588,
 'learning_rate_Hydroxylation-K': 0.0002534665029061919,
 'learning_rate_Hydroxylation-P': 0.0036104817772800273,
 'log_base': 2.8009739826964015,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3343606710,
 'sample_weights': [20.080969782256382, 2.510217922731265],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.932673476250444,
 'weight_decay_Hydroxylation-K': 8.47896500502872,
 'weight_decay_Hydroxylation-P': 1.8379990832863102}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.595
[2,     1] loss: 1252.032
[3,     1] loss: 1246.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005252050113224048,
 'learning_rate_Hydroxylation-K': 0.002966731162959151,
 'learning_rate_Hydroxylation-P': 0.002202635041105411,
 'log_base': 1.5274675752554985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3763180860,
 'sample_weights': [1.620870193741107, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.625759596045559,
 'weight_decay_Hydroxylation-K': 1.7693287152330772,
 'weight_decay_Hydroxylation-P': 6.1649050377754255}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1741.540
[2,     1] loss: 1741.415
[3,     1] loss: 1735.397
[4,     1] loss: 1738.689
[5,     1] loss: 1732.686
[6,     1] loss: 1725.198
[7,     1] loss: 1727.779
[8,     1] loss: 1713.980
[9,     1] loss: 1695.638
[10,     1] loss: 1665.600
[11,     1] loss: 1622.873
[12,     1] loss: 1559.385
[13,     1] loss: 1491.226
[14,     1] loss: 1484.546
[15,     1] loss: 1422.998
[16,     1] loss: 1379.025
[17,     1] loss: 1398.550
[18,     1] loss: 1435.223
[19,     1] loss: 1428.072
[20,     1] loss: 1472.660
[21,     1] loss: 1386.406
[22,     1] loss: 1405.443
[23,     1] loss: 1409.983
[24,     1] loss: 1397.948
[25,     1] loss: 1342.903
[26,     1] loss: 1354.987
[27,     1] loss: 1374.739
[28,     1] loss: 1308.630
[29,     1] loss: 1278.750
[30,     1] loss: 1299.230
[31,     1] loss: 1298.109
[32,     1] loss: 1307.422
[33,     1] loss: 1220.057
[34,     1] loss: 1227.517
[35,     1] loss: 1219.681
[36,     1] loss: 1227.641
[37,     1] loss: 1443.635
[38,     1] loss: 1165.481
[39,     1] loss: 1240.380
[40,     1] loss: 1116.173
[41,     1] loss: 1160.777
[42,     1] loss: 1165.605
[43,     1] loss: 1192.997
[44,     1] loss: 1149.864
[45,     1] loss: 1062.979
[46,     1] loss: 1123.846
[47,     1] loss: 1074.535
[48,     1] loss: 1104.703
[49,     1] loss: 965.486
[50,     1] loss: 1114.343
[51,     1] loss: 1006.876
[52,     1] loss: 988.803
[53,     1] loss: 988.424
[54,     1] loss: 879.223
[55,     1] loss: 892.555
[56,     1] loss: 951.761
[57,     1] loss: 911.271
[58,     1] loss: 957.350
[59,     1] loss: 936.158
[60,     1] loss: 933.633
[61,     1] loss: 872.184
[62,     1] loss: 1129.887
[63,     1] loss: 984.416
[64,     1] loss: 1018.362
[65,     1] loss: 951.687
[66,     1] loss: 987.320
[67,     1] loss: 1038.998
[68,     1] loss: 958.842
[69,     1] loss: 837.239
[70,     1] loss: 850.396
[71,     1] loss: 824.252
[72,     1] loss: 837.096
[73,     1] loss: 818.075
[74,     1] loss: 800.272
[75,     1] loss: 749.825
[76,     1] loss: 822.292
[77,     1] loss: 705.810
[78,     1] loss: 765.550
[79,     1] loss: 685.119
[80,     1] loss: 707.086
[81,     1] loss: 684.281
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035618203062094487,
 'learning_rate_Hydroxylation-K': 0.006658037132697474,
 'learning_rate_Hydroxylation-P': 0.0014170531408018874,
 'log_base': 1.8226823083207753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 504472443,
 'sample_weights': [3.9409798618892973, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.213986479665586,
 'weight_decay_Hydroxylation-K': 1.4390257468820244,
 'weight_decay_Hydroxylation-P': 0.004241089868146836}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1494.545
[2,     1] loss: 1493.857
[3,     1] loss: 1497.208
[4,     1] loss: 1494.892
[5,     1] loss: 1497.370
[6,     1] loss: 1488.496
[7,     1] loss: 1493.126
[8,     1] loss: 1490.211
[9,     1] loss: 1484.762
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004417428654704593,
 'learning_rate_Hydroxylation-K': 0.003452838919400468,
 'learning_rate_Hydroxylation-P': 0.00615667487949733,
 'log_base': 2.5818829224732207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2274535683,
 'sample_weights': [2.7809720629552475, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.43412608981723,
 'weight_decay_Hydroxylation-K': 7.689657852514105,
 'weight_decay_Hydroxylation-P': 0.1892285047058789}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.053
[2,     1] loss: 1283.806
[3,     1] loss: 1279.305
[4,     1] loss: 1277.787
[5,     1] loss: 1278.228
[6,     1] loss: 1276.040
[7,     1] loss: 1271.677
[8,     1] loss: 1270.807
[9,     1] loss: 1258.629
[10,     1] loss: 1232.642
[11,     1] loss: 1202.244
[12,     1] loss: 1177.191
[13,     1] loss: 1141.325
[14,     1] loss: 1109.375
[15,     1] loss: 1071.500
[16,     1] loss: 1046.718
[17,     1] loss: 1059.587
[18,     1] loss: 1036.472
[19,     1] loss: 1009.405
[20,     1] loss: 1028.158
[21,     1] loss: 1031.949
[22,     1] loss: 1022.566
[23,     1] loss: 1023.279
[24,     1] loss: 1005.829
[25,     1] loss: 972.369
[26,     1] loss: 973.468
[27,     1] loss: 932.501
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007692722859998275,
 'learning_rate_Hydroxylation-K': 0.009530474532151069,
 'learning_rate_Hydroxylation-P': 0.0004057300658858623,
 'log_base': 1.163921742884157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1713662591,
 'sample_weights': [1.7600525024215865, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.388151075824762,
 'weight_decay_Hydroxylation-K': 0.7647828358514022,
 'weight_decay_Hydroxylation-P': 4.875260035249424}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3620.399
[2,     1] loss: 3582.867
[3,     1] loss: 3560.992
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003612798167385908,
 'learning_rate_Hydroxylation-K': 0.0021557274491951967,
 'learning_rate_Hydroxylation-P': 0.0023096742155326844,
 'log_base': 1.2567383349777854,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2611010490,
 'sample_weights': [10.998003052179383, 1.3748033424276067],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.33000689046420906,
 'weight_decay_Hydroxylation-K': 9.359613752146288,
 'weight_decay_Hydroxylation-P': 0.17800028403620427}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2454.642
[2,     1] loss: 2454.481
[3,     1] loss: 2450.847
[4,     1] loss: 2446.211
[5,     1] loss: 2434.292
[6,     1] loss: 2447.584
[7,     1] loss: 2434.511
[8,     1] loss: 2436.142
[9,     1] loss: 2442.959
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004120906032764918,
 'learning_rate_Hydroxylation-K': 0.003971558587518348,
 'learning_rate_Hydroxylation-P': 0.00018384748407488435,
 'log_base': 2.2470965800629368,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2388222588,
 'sample_weights': [7.305465756943009, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.31634176888727,
 'weight_decay_Hydroxylation-K': 1.1592351694482992,
 'weight_decay_Hydroxylation-P': 9.08738639067566}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.216
[2,     1] loss: 1343.824
[3,     1] loss: 1338.929
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008946211460758455,
 'learning_rate_Hydroxylation-K': 0.00976405582811853,
 'learning_rate_Hydroxylation-P': 0.001928844731712739,
 'log_base': 1.1401819389993513,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 777355679,
 'sample_weights': [2.0619599610554014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.30201075287948,
 'weight_decay_Hydroxylation-K': 0.4850470153913729,
 'weight_decay_Hydroxylation-P': 2.5702688801764033}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4152.594
[2,     1] loss: 4153.875
[3,     1] loss: 4125.258
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008897991344807076,
 'learning_rate_Hydroxylation-K': 0.0013631179755745886,
 'learning_rate_Hydroxylation-P': 0.004680288812854599,
 'log_base': 2.819428754915869,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3760994236,
 'sample_weights': [12.725593165937932, 1.590760425860993],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.402280833691873,
 'weight_decay_Hydroxylation-K': 9.342106608503117,
 'weight_decay_Hydroxylation-P': 8.141983906001423}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.718
[2,     1] loss: 1246.400
[3,     1] loss: 1247.951
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007546835077249138,
 'learning_rate_Hydroxylation-K': 0.0009364239396137682,
 'learning_rate_Hydroxylation-P': 0.005812377571761416,
 'log_base': 2.997298028960373,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 426753964,
 'sample_weights': [1.610600975840221, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.621369386462606,
 'weight_decay_Hydroxylation-K': 3.988157950545773,
 'weight_decay_Hydroxylation-P': 0.8651170614530876}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.454
[2,     1] loss: 1230.356
[3,     1] loss: 1233.903
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00500912581932826,
 'learning_rate_Hydroxylation-K': 0.006840556875564652,
 'learning_rate_Hydroxylation-P': 0.0012067762060075627,
 'log_base': 1.4373746601735846,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 149388526,
 'sample_weights': [1.520840006147406, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.708155817865871,
 'weight_decay_Hydroxylation-K': 2.9900886525168007,
 'weight_decay_Hydroxylation-P': 8.870076000629979}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1883.440
[2,     1] loss: 1885.297
[3,     1] loss: 1878.436
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006323402937235285,
 'learning_rate_Hydroxylation-K': 0.00594388965203287,
 'learning_rate_Hydroxylation-P': 0.00021652350682461183,
 'log_base': 1.205846748429059,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3116137563,
 'sample_weights': [4.601320168768011, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.441439401169592,
 'weight_decay_Hydroxylation-K': 2.983299228810103,
 'weight_decay_Hydroxylation-P': 9.335035658211988}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2893.808
[2,     1] loss: 2892.589
[3,     1] loss: 2895.591
[4,     1] loss: 2887.105
[5,     1] loss: 2915.583
[6,     1] loss: 2903.781
[7,     1] loss: 2882.305
[8,     1] loss: 2884.268
[9,     1] loss: 2884.508
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008762156278439901,
 'learning_rate_Hydroxylation-K': 0.001031939181990144,
 'learning_rate_Hydroxylation-P': 0.006337904263134054,
 'log_base': 1.9937292889809615,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2258816315,
 'sample_weights': [8.918822349829032, 1.1148957423350787],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.05741574919664,
 'weight_decay_Hydroxylation-K': 1.9746994833400742,
 'weight_decay_Hydroxylation-P': 1.8951113851338128}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1419.779
[2,     1] loss: 1420.027
[3,     1] loss: 1425.002
[4,     1] loss: 1416.025
[5,     1] loss: 1411.708
[6,     1] loss: 1422.680
[7,     1] loss: 1416.712
[8,     1] loss: 1418.800
[9,     1] loss: 1416.065
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009171308172044122,
 'learning_rate_Hydroxylation-K': 0.00968264173093834,
 'learning_rate_Hydroxylation-P': 0.0012042308039406677,
 'log_base': 1.01459498258632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 986317063,
 'sample_weights': [2.419458629431965, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.499818048409143,
 'weight_decay_Hydroxylation-K': 1.5784945699164779,
 'weight_decay_Hydroxylation-P': 0.3684023178791378}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37218.734
Exploding loss, terminate run (best metric=0.5411375164985657)
Finished Training
Total time taken: 0.28299975395202637
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37570.438
Exploding loss, terminate run (best metric=0.5257453322410583)
Finished Training
Total time taken: 0.24400043487548828
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37477.141
Exploding loss, terminate run (best metric=0.5331597328186035)
Finished Training
Total time taken: 0.30300211906433105
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37314.656
Exploding loss, terminate run (best metric=0.5283521413803101)
Finished Training
Total time taken: 0.2480010986328125
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37481.898
Exploding loss, terminate run (best metric=0.5311086773872375)
Finished Training
Total time taken: 0.30100154876708984
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37522.742
Exploding loss, terminate run (best metric=0.5520648956298828)
Finished Training
Total time taken: 0.30300116539001465
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37452.602
Exploding loss, terminate run (best metric=0.5298336744308472)
Finished Training
Total time taken: 0.2830009460449219
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37291.742
Exploding loss, terminate run (best metric=0.5469523072242737)
Finished Training
Total time taken: 0.28151774406433105
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37563.867
Exploding loss, terminate run (best metric=0.5316419005393982)
Finished Training
Total time taken: 0.2839987277984619
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37332.051
Exploding loss, terminate run (best metric=0.5434528589248657)
Finished Training
Total time taken: 0.26399874687194824
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37427.348
Exploding loss, terminate run (best metric=0.5446048378944397)
Finished Training
Total time taken: 0.2590012550354004
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37632.871
Exploding loss, terminate run (best metric=0.5508232712745667)
Finished Training
Total time taken: 0.2370014190673828
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37226.938
Exploding loss, terminate run (best metric=0.5701383352279663)
Finished Training
Total time taken: 0.2590010166168213
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37515.070
Exploding loss, terminate run (best metric=0.5347632765769958)
Finished Training
Total time taken: 0.25999999046325684
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37341.234
Exploding loss, terminate run (best metric=0.527683675289154)
Finished Training
Total time taken: 0.24200057983398438
{'Hydroxylation-K Validation Accuracy': 0.5143617021276595, 'Hydroxylation-K Validation Sensitivity': 0.5037037037037037, 'Hydroxylation-K Validation Specificity': 0.5210526315789473, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5690058479532164, 'Hydroxylation-K AUC PR': 0.3564497433535954, 'Hydroxylation-K MCC': 0.022339567647607098, 'Hydroxylation-K F1': 0.1864258347016968, 'Validation Loss (Hydroxylation-K)': 0.5653544584910075, 'Hydroxylation-P Validation Accuracy': 0.5193825017342605, 'Hydroxylation-P Validation Sensitivity': 0.4876190476190476, 'Hydroxylation-P Validation Specificity': 0.5264227642276422, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5726479074679733, 'Hydroxylation-P AUC PR': 0.2941022831206007, 'Hydroxylation-P MCC': 0.01537453624789256, 'Hydroxylation-P F1': 0.16344871070993136, 'Validation Loss (Hydroxylation-P)': 0.539430828889211, 'Validation Loss (total)': 1.1047852834065754, 'TimeToTrain': 0.2701017697652181}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007170494588284476,
 'learning_rate_Hydroxylation-K': 0.007797858884211605,
 'learning_rate_Hydroxylation-P': 0.00021648405186228017,
 'log_base': 1.1236113691320548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 821992767,
 'sample_weights': [115.30290424503848, 14.382915647721166],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.535910403921529,
 'weight_decay_Hydroxylation-K': 1.1054246515009245,
 'weight_decay_Hydroxylation-P': 0.17717974953845528}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4617.734
[2,     1] loss: 4651.254
[3,     1] loss: 4665.607
[4,     1] loss: 4648.546
[5,     1] loss: 4689.980
[6,     1] loss: 4679.718
[7,     1] loss: 4622.446
[8,     1] loss: 4639.144
[9,     1] loss: 4635.461
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0094831441732138,
 'learning_rate_Hydroxylation-K': 0.008423567416728148,
 'learning_rate_Hydroxylation-P': 0.001021857788136188,
 'log_base': 1.2150479229131936,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4159941784,
 'sample_weights': [14.324090357780573, 1.7905802724076734],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.5129720946557335,
 'weight_decay_Hydroxylation-K': 1.9964499562983202,
 'weight_decay_Hydroxylation-P': 0.18947244124123128}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2794.319
[2,     1] loss: 2780.487
[3,     1] loss: 2787.965
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032300682579563143,
 'learning_rate_Hydroxylation-K': 0.00016279968065031837,
 'learning_rate_Hydroxylation-P': 0.004110592485824133,
 'log_base': 2.3656599726168666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4070904987,
 'sample_weights': [8.570761829385376, 1.0713865011935053],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.987420900071025,
 'weight_decay_Hydroxylation-K': 1.7686464831694602,
 'weight_decay_Hydroxylation-P': 0.2931821460580557}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.185
[2,     1] loss: 1319.376
[3,     1] loss: 1313.861
[4,     1] loss: 1318.330
[5,     1] loss: 1316.301
[6,     1] loss: 1312.677
[7,     1] loss: 1312.388
[8,     1] loss: 1304.281
[9,     1] loss: 1296.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00994064897780165,
 'learning_rate_Hydroxylation-K': 0.009636933613666581,
 'learning_rate_Hydroxylation-P': 0.0031689694898328347,
 'log_base': 1.1033934443300033,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 493141267,
 'sample_weights': [1.938829912000461, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.6266704830206296,
 'weight_decay_Hydroxylation-K': 1.1038942537916032,
 'weight_decay_Hydroxylation-P': 0.14809475288033935}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5493.215
[2,     1] loss: 5692.098
[3,     1] loss: 5515.775
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026034637760780116,
 'learning_rate_Hydroxylation-K': 0.009523879347184357,
 'learning_rate_Hydroxylation-P': 0.008216965938067856,
 'log_base': 1.0687682791257345,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1704646070,
 'sample_weights': [16.967544376741934, 2.1210247543358363],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.923233217464347,
 'weight_decay_Hydroxylation-K': 2.103894327210611,
 'weight_decay_Hydroxylation-P': 9.75075441587979}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8134.110
[2,     1] loss: 8138.296
[3,     1] loss: 8150.525
[4,     1] loss: 8132.776
[5,     1] loss: 8088.752
[6,     1] loss: 8149.438
[7,     1] loss: 8097.917
[8,     1] loss: 8120.584
[9,     1] loss: 8120.243
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030212276825216542,
 'learning_rate_Hydroxylation-K': 0.0033502916961578866,
 'learning_rate_Hydroxylation-P': 0.00011099397126162724,
 'log_base': 2.269893441464448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 454152498,
 'sample_weights': [25.101824669807964, 3.1378489616117324],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.37661946407598,
 'weight_decay_Hydroxylation-K': 1.385348618021049,
 'weight_decay_Hydroxylation-P': 0.6367843764596519}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.642
[2,     1] loss: 1337.782
[3,     1] loss: 1335.492
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004818624333235757,
 'learning_rate_Hydroxylation-K': 0.0011138005927473888,
 'learning_rate_Hydroxylation-P': 0.004305656286194117,
 'log_base': 2.6642361917815625,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2778801329,
 'sample_weights': [2.0365696817105188, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.178840722249456,
 'weight_decay_Hydroxylation-K': 4.259976776174063,
 'weight_decay_Hydroxylation-P': 1.5900496931067898}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.075
[2,     1] loss: 1268.678
[3,     1] loss: 1264.502
[4,     1] loss: 1267.194
[5,     1] loss: 1265.944
[6,     1] loss: 1253.760
[7,     1] loss: 1247.343
[8,     1] loss: 1208.249
[9,     1] loss: 1163.558
[10,     1] loss: 1158.896
[11,     1] loss: 1119.897
[12,     1] loss: 1085.196
[13,     1] loss: 1102.741
[14,     1] loss: 1021.998
[15,     1] loss: 1013.795
[16,     1] loss: 1012.737
[17,     1] loss: 1002.716
[18,     1] loss: 976.413
[19,     1] loss: 954.483
[20,     1] loss: 943.724
[21,     1] loss: 982.864
[22,     1] loss: 902.433
[23,     1] loss: 908.725
[24,     1] loss: 904.237
[25,     1] loss: 861.579
[26,     1] loss: 905.473
[27,     1] loss: 963.349
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003627981449375455,
 'learning_rate_Hydroxylation-K': 0.00610704773631974,
 'learning_rate_Hydroxylation-P': 0.0009478109725654421,
 'log_base': 2.9735547537852423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 76390992,
 'sample_weights': [1.7036569934712773, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3452200838041035,
 'weight_decay_Hydroxylation-K': 3.623173097324015,
 'weight_decay_Hydroxylation-P': 6.841875218741938}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.860
[2,     1] loss: 1229.929
[3,     1] loss: 1232.115
[4,     1] loss: 1230.933
[5,     1] loss: 1227.989
[6,     1] loss: 1228.873
[7,     1] loss: 1223.436
[8,     1] loss: 1218.142
[9,     1] loss: 1211.451
[10,     1] loss: 1196.229
[11,     1] loss: 1183.015
[12,     1] loss: 1145.971
[13,     1] loss: 1117.591
[14,     1] loss: 1103.975
[15,     1] loss: 1079.687
[16,     1] loss: 1048.392
[17,     1] loss: 1014.761
[18,     1] loss: 1071.782
[19,     1] loss: 1016.120
[20,     1] loss: 1003.094
[21,     1] loss: 973.206
[22,     1] loss: 988.772
[23,     1] loss: 973.672
[24,     1] loss: 972.047
[25,     1] loss: 939.039
[26,     1] loss: 966.546
[27,     1] loss: 970.591
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031465774678758355,
 'learning_rate_Hydroxylation-K': 0.00651302447261757,
 'learning_rate_Hydroxylation-P': 0.0011848349895375212,
 'log_base': 1.8728553952908893,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3301618787,
 'sample_weights': [1.5319391622695078, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.153123730646356,
 'weight_decay_Hydroxylation-K': 3.151479011365435,
 'weight_decay_Hydroxylation-P': 9.908438151234128}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1468.840
[2,     1] loss: 1470.884
[3,     1] loss: 1472.670
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004389061541006536,
 'learning_rate_Hydroxylation-K': 0.009141052664809947,
 'learning_rate_Hydroxylation-P': 0.006619780425331772,
 'log_base': 2.539138402128463,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1557452135,
 'sample_weights': [2.66061889406567, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.023506309046391,
 'weight_decay_Hydroxylation-K': 6.312799595027437,
 'weight_decay_Hydroxylation-P': 9.425718622311486}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.508
[2,     1] loss: 1289.015
[3,     1] loss: 1287.570
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030841227092848643,
 'learning_rate_Hydroxylation-K': 0.00495586477703582,
 'learning_rate_Hydroxylation-P': 0.007056647839494321,
 'log_base': 2.584696441884088,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 791144932,
 'sample_weights': [1.7915847769000395, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.903058915809821,
 'weight_decay_Hydroxylation-K': 0.24968555531945524,
 'weight_decay_Hydroxylation-P': 8.577409514980719}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.198
[2,     1] loss: 1281.241
[3,     1] loss: 1278.179
[4,     1] loss: 1278.961
[5,     1] loss: 1279.370
[6,     1] loss: 1276.696
[7,     1] loss: 1274.385
[8,     1] loss: 1266.698
[9,     1] loss: 1262.973
[10,     1] loss: 1249.005
[11,     1] loss: 1229.252
[12,     1] loss: 1212.046
[13,     1] loss: 1176.408
[14,     1] loss: 1163.920
[15,     1] loss: 1130.304
[16,     1] loss: 1100.528
[17,     1] loss: 1073.623
[18,     1] loss: 1064.436
[19,     1] loss: 1036.708
[20,     1] loss: 1122.739
[21,     1] loss: 1104.452
[22,     1] loss: 1107.077
[23,     1] loss: 1027.039
[24,     1] loss: 1057.630
[25,     1] loss: 1078.484
[26,     1] loss: 1073.189
[27,     1] loss: 1033.751
[28,     1] loss: 1038.471
[29,     1] loss: 1028.828
[30,     1] loss: 990.952
[31,     1] loss: 1020.635
[32,     1] loss: 1057.321
[33,     1] loss: 1019.436
[34,     1] loss: 996.689
[35,     1] loss: 1038.913
[36,     1] loss: 972.703
[37,     1] loss: 1010.895
[38,     1] loss: 991.828
[39,     1] loss: 954.023
[40,     1] loss: 994.036
[41,     1] loss: 920.814
[42,     1] loss: 939.368
[43,     1] loss: 958.140
[44,     1] loss: 914.333
[45,     1] loss: 927.705
[46,     1] loss: 918.992
[47,     1] loss: 904.501
[48,     1] loss: 917.563
[49,     1] loss: 888.411
[50,     1] loss: 859.281
[51,     1] loss: 822.031
[52,     1] loss: 871.882
[53,     1] loss: 813.620
[54,     1] loss: 839.222
[55,     1] loss: 828.348
[56,     1] loss: 803.348
[57,     1] loss: 875.293
[58,     1] loss: 807.088
[59,     1] loss: 822.485
[60,     1] loss: 789.276
[61,     1] loss: 763.603
[62,     1] loss: 772.310
[63,     1] loss: 731.911
[64,     1] loss: 751.910
[65,     1] loss: 728.659
[66,     1] loss: 769.712
[67,     1] loss: 707.082
[68,     1] loss: 713.705
[69,     1] loss: 754.485
[70,     1] loss: 818.129
[71,     1] loss: 696.754
[72,     1] loss: 733.702
[73,     1] loss: 768.206
[74,     1] loss: 643.612
[75,     1] loss: 659.355
[76,     1] loss: 630.680
[77,     1] loss: 695.486
[78,     1] loss: 590.720
[79,     1] loss: 601.756
[80,     1] loss: 568.073
[81,     1] loss: 586.796
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006202287761357591,
 'learning_rate_Hydroxylation-K': 0.0024409818119559903,
 'learning_rate_Hydroxylation-P': 0.0047403194379959665,
 'log_base': 2.6810556106832526,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1231439643,
 'sample_weights': [1.7580338661365216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.042262898498922,
 'weight_decay_Hydroxylation-K': 7.395642144952136,
 'weight_decay_Hydroxylation-P': 1.8066826746710034}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.063
[2,     1] loss: 1265.194
[3,     1] loss: 1265.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003303327250419365,
 'learning_rate_Hydroxylation-K': 0.0002832759577018663,
 'learning_rate_Hydroxylation-P': 0.006918956208890936,
 'log_base': 2.8931207752624912,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2128237239,
 'sample_weights': [1.6927856431407786, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.195444016595893,
 'weight_decay_Hydroxylation-K': 6.4537640681487165,
 'weight_decay_Hydroxylation-P': 0.9185874579567602}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.663
[2,     1] loss: 1243.718
[3,     1] loss: 1240.127
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002350769235819801,
 'learning_rate_Hydroxylation-K': 0.004372166791383617,
 'learning_rate_Hydroxylation-P': 0.0038668446823858214,
 'log_base': 2.76141035775254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3645898920,
 'sample_weights': [1.5714835088729269, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.090308939697486,
 'weight_decay_Hydroxylation-K': 6.30512786312859,
 'weight_decay_Hydroxylation-P': 9.64415925661552}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.207
[2,     1] loss: 1250.690
[3,     1] loss: 1253.165
[4,     1] loss: 1255.917
[5,     1] loss: 1247.951
[6,     1] loss: 1248.137
[7,     1] loss: 1240.475
[8,     1] loss: 1231.276
[9,     1] loss: 1215.918
[10,     1] loss: 1203.241
[11,     1] loss: 1170.374
[12,     1] loss: 1144.604
[13,     1] loss: 1123.470
[14,     1] loss: 1081.677
[15,     1] loss: 1072.122
[16,     1] loss: 1047.398
[17,     1] loss: 1017.744
[18,     1] loss: 1044.797
[19,     1] loss: 1022.446
[20,     1] loss: 987.792
[21,     1] loss: 1005.404
[22,     1] loss: 970.476
[23,     1] loss: 991.257
[24,     1] loss: 1019.009
[25,     1] loss: 991.240
[26,     1] loss: 958.875
[27,     1] loss: 981.077
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009848018905241023,
 'learning_rate_Hydroxylation-K': 0.0002410057996485338,
 'learning_rate_Hydroxylation-P': 0.006848447031972655,
 'log_base': 2.0105833234599135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3580701,
 'sample_weights': [1.6435707981570729, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.25571107008150573,
 'weight_decay_Hydroxylation-K': 1.9867773637976582,
 'weight_decay_Hydroxylation-P': 6.750056539486319}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1414.797
[2,     1] loss: 1434.392
[3,     1] loss: 1416.144
[4,     1] loss: 1411.491
[5,     1] loss: 1412.264
[6,     1] loss: 1412.340
[7,     1] loss: 1414.640
[8,     1] loss: 1411.803
[9,     1] loss: 1409.784
[10,     1] loss: 1414.132
[11,     1] loss: 1417.235
[12,     1] loss: 1419.398
[13,     1] loss: 1413.854
[14,     1] loss: 1411.052
[15,     1] loss: 1413.739
[16,     1] loss: 1408.851
[17,     1] loss: 1413.880
[18,     1] loss: 1413.417
[19,     1] loss: 1410.159
[20,     1] loss: 1412.019
[21,     1] loss: 1413.094
[22,     1] loss: 1410.644
[23,     1] loss: 1411.556
[24,     1] loss: 1411.740
[25,     1] loss: 1411.642
[26,     1] loss: 1410.004
[27,     1] loss: 1410.563
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017318464931549388,
 'learning_rate_Hydroxylation-K': 0.0024002864968806956,
 'learning_rate_Hydroxylation-P': 0.0028274419851551744,
 'log_base': 1.9075058901102964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1940840523,
 'sample_weights': [2.3902973244308656, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0855927506777712,
 'weight_decay_Hydroxylation-K': 1.5869964997731274,
 'weight_decay_Hydroxylation-P': 6.529541183096676}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.353
[2,     1] loss: 1454.977
[3,     1] loss: 1454.017
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004891741174972586,
 'learning_rate_Hydroxylation-K': 0.009766042595607637,
 'learning_rate_Hydroxylation-P': 0.00010953113435798281,
 'log_base': 1.1079732430058604,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3704146940,
 'sample_weights': [2.585091373507937, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.002484418858703,
 'weight_decay_Hydroxylation-K': 2.005907226542165,
 'weight_decay_Hydroxylation-P': 1.839305428890846}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5258.861
[2,     1] loss: 5341.771
[3,     1] loss: 5275.780
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007103046534271972,
 'learning_rate_Hydroxylation-K': 0.0010092458455318656,
 'learning_rate_Hydroxylation-P': 0.005773810926738312,
 'log_base': 2.3322256342755567,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2901201028,
 'sample_weights': [16.282097273040787, 2.035340565601326],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.789451472546586,
 'weight_decay_Hydroxylation-K': 9.684031641712771,
 'weight_decay_Hydroxylation-P': 9.04821956707524}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.599
[2,     1] loss: 1317.920
[3,     1] loss: 1330.863
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00554827158076885,
 'learning_rate_Hydroxylation-K': 0.0048154472408440275,
 'learning_rate_Hydroxylation-P': 0.0035548620211788926,
 'log_base': 2.93395684537049,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 520399364,
 'sample_weights': [1.9714191855677352, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.008997045779375,
 'weight_decay_Hydroxylation-K': 4.428458262340717,
 'weight_decay_Hydroxylation-P': 2.5315513894879347}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.974
[2,     1] loss: 1242.306
[3,     1] loss: 1236.300
[4,     1] loss: 1238.493
[5,     1] loss: 1233.748
[6,     1] loss: 1242.130
[7,     1] loss: 1235.690
[8,     1] loss: 1235.048
[9,     1] loss: 1236.159
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00446520887375735,
 'learning_rate_Hydroxylation-K': 0.0017971549218999238,
 'learning_rate_Hydroxylation-P': 0.00507835810218314,
 'log_base': 2.7685791772757824,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1303990261,
 'sample_weights': [1.5510197334800229, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.881586720482302,
 'weight_decay_Hydroxylation-K': 6.267774331413179,
 'weight_decay_Hydroxylation-P': 4.55923475932069}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.112
[2,     1] loss: 1258.387
[3,     1] loss: 1254.938
[4,     1] loss: 1253.758
[5,     1] loss: 1252.280
[6,     1] loss: 1253.034
[7,     1] loss: 1248.039
[8,     1] loss: 1243.300
[9,     1] loss: 1235.010
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029668335252114333,
 'learning_rate_Hydroxylation-K': 0.00297321780571226,
 'learning_rate_Hydroxylation-P': 0.009879321591884874,
 'log_base': 1.025958356849929,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 469054117,
 'sample_weights': [1.6393862204262946, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.07987518875793,
 'weight_decay_Hydroxylation-K': 7.801908898637757,
 'weight_decay_Hydroxylation-P': 2.442448234167536}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21192.367
Exploding loss, terminate run (best metric=0.5350451469421387)
Finished Training
Total time taken: 0.25500035285949707
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21181.299
Exploding loss, terminate run (best metric=0.5264862179756165)
Finished Training
Total time taken: 0.2570004463195801
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21215.984
Exploding loss, terminate run (best metric=0.5257113575935364)
Finished Training
Total time taken: 0.2550039291381836
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21290.137
Exploding loss, terminate run (best metric=0.5274245738983154)
Finished Training
Total time taken: 0.26200079917907715
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21205.531
Exploding loss, terminate run (best metric=0.5282524824142456)
Finished Training
Total time taken: 0.27900147438049316
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21132.900
Exploding loss, terminate run (best metric=0.5321851372718811)
Finished Training
Total time taken: 0.26600122451782227
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21149.031
Exploding loss, terminate run (best metric=0.5386397838592529)
Finished Training
Total time taken: 0.2890009880065918
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21290.230
Exploding loss, terminate run (best metric=0.5294398665428162)
Finished Training
Total time taken: 0.2760009765625
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21059.283
Exploding loss, terminate run (best metric=0.5335260033607483)
Finished Training
Total time taken: 0.2435157299041748
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21131.064
Exploding loss, terminate run (best metric=0.5302976965904236)
Finished Training
Total time taken: 0.2590007781982422
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21025.316
Exploding loss, terminate run (best metric=0.5465445518493652)
Finished Training
Total time taken: 0.2960014343261719
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21144.959
Exploding loss, terminate run (best metric=0.5339255332946777)
Finished Training
Total time taken: 0.27700161933898926
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21247.740
Exploding loss, terminate run (best metric=0.5263236165046692)
Finished Training
Total time taken: 0.23751568794250488
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21391.148
Exploding loss, terminate run (best metric=0.529062807559967)
Finished Training
Total time taken: 0.22899866104125977
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21149.734
Exploding loss, terminate run (best metric=0.5299034714698792)
Finished Training
Total time taken: 0.2240743637084961
{'Hydroxylation-K Validation Accuracy': 0.5378250591016548, 'Hydroxylation-K Validation Sensitivity': 0.4711111111111111, 'Hydroxylation-K Validation Specificity': 0.5543859649122806, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6805068226120857, 'Hydroxylation-K AUC PR': 0.3504528800054644, 'Hydroxylation-K MCC': 0.02696724292628284, 'Hydroxylation-K F1': 0.18919713075792932, 'Validation Loss (Hydroxylation-K)': 0.5566814144452413, 'Hydroxylation-P Validation Accuracy': 0.5423796423193408, 'Hydroxylation-P Validation Sensitivity': 0.4438095238095238, 'Hydroxylation-P Validation Specificity': 0.5638186443214125, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.653856954231973, 'Hydroxylation-P AUC PR': 0.33345253595849544, 'Hydroxylation-P MCC': 0.009642977909029085, 'Hydroxylation-P F1': 0.15131624339956012, 'Validation Loss (Hydroxylation-P)': 0.5315178831418356, 'Validation Loss (total)': 1.088199297587077, 'TimeToTrain': 0.2603412310282389}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038250563509341423,
 'learning_rate_Hydroxylation-K': 0.005360742972073944,
 'learning_rate_Hydroxylation-P': 0.008203311754209753,
 'log_base': 1.4433046472718254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2939107339,
 'sample_weights': [65.19183804184682, 8.132047615063694],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.393120239576933,
 'weight_decay_Hydroxylation-K': 8.455831917132722,
 'weight_decay_Hydroxylation-P': 2.911784137880719}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1877.879
[2,     1] loss: 1864.167
[3,     1] loss: 1880.631
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003626316099298742,
 'learning_rate_Hydroxylation-K': 0.008191061459322954,
 'learning_rate_Hydroxylation-P': 0.0035591745127893683,
 'log_base': 2.1449126908057177,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3831420019,
 'sample_weights': [4.5496925242040795, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.108444565504772,
 'weight_decay_Hydroxylation-K': 8.610575647486243,
 'weight_decay_Hydroxylation-P': 3.800697116717212}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.657
[2,     1] loss: 1369.951
[3,     1] loss: 1370.632
[4,     1] loss: 1364.898
[5,     1] loss: 1363.357
[6,     1] loss: 1364.451
[7,     1] loss: 1346.388
[8,     1] loss: 1341.641
[9,     1] loss: 1308.512
[10,     1] loss: 1284.497
[11,     1] loss: 1238.560
[12,     1] loss: 1218.571
[13,     1] loss: 1184.867
[14,     1] loss: 1177.648
[15,     1] loss: 1165.791
[16,     1] loss: 1155.026
[17,     1] loss: 1112.309
[18,     1] loss: 1144.384
[19,     1] loss: 1070.573
[20,     1] loss: 1102.465
[21,     1] loss: 1104.871
[22,     1] loss: 1098.013
[23,     1] loss: 1070.216
[24,     1] loss: 1070.628
[25,     1] loss: 1036.197
[26,     1] loss: 1049.575
[27,     1] loss: 1068.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007985739020447948,
 'learning_rate_Hydroxylation-K': 0.001386523423527458,
 'learning_rate_Hydroxylation-P': 0.005100459745227295,
 'log_base': 2.0883917590148733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4152146130,
 'sample_weights': [2.1877154602929774, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.845051308845086,
 'weight_decay_Hydroxylation-K': 0.0045711380979555905,
 'weight_decay_Hydroxylation-P': 8.952488449105639}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.493
[2,     1] loss: 1390.909
[3,     1] loss: 1385.747
[4,     1] loss: 1385.747
[5,     1] loss: 1397.609
[6,     1] loss: 1383.592
[7,     1] loss: 1384.428
[8,     1] loss: 1387.818
[9,     1] loss: 1381.734
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009233817722609117,
 'learning_rate_Hydroxylation-K': 0.00941084273722281,
 'learning_rate_Hydroxylation-P': 0.0009912000541859888,
 'log_base': 1.0172661641733605,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3575988298,
 'sample_weights': [2.267050682991854, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.63244605069959,
 'weight_decay_Hydroxylation-K': 3.45726328142857,
 'weight_decay_Hydroxylation-P': 3.1436966162976097}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31511.023
Exploding loss, terminate run (best metric=0.5482894778251648)
Finished Training
Total time taken: 0.24000072479248047
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31647.074
Exploding loss, terminate run (best metric=0.5310395956039429)
Finished Training
Total time taken: 0.23800039291381836
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31630.650
Exploding loss, terminate run (best metric=0.5297396183013916)
Finished Training
Total time taken: 0.19900107383728027
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31613.629
Exploding loss, terminate run (best metric=0.526138961315155)
Finished Training
Total time taken: 0.2420046329498291
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31627.375
Exploding loss, terminate run (best metric=0.5437940955162048)
Finished Training
Total time taken: 0.2129979133605957
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31672.090
Exploding loss, terminate run (best metric=0.5332038998603821)
Finished Training
Total time taken: 0.2350001335144043
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31584.445
Exploding loss, terminate run (best metric=0.5283527970314026)
Finished Training
Total time taken: 0.22500038146972656
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31682.523
Exploding loss, terminate run (best metric=0.5393733978271484)
Finished Training
Total time taken: 0.22300148010253906
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31530.543
Exploding loss, terminate run (best metric=0.5425423979759216)
Finished Training
Total time taken: 0.25100040435791016
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31692.014
Exploding loss, terminate run (best metric=0.5275506377220154)
Finished Training
Total time taken: 0.2219991683959961
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31809.367
Exploding loss, terminate run (best metric=0.5332237482070923)
Finished Training
Total time taken: 0.23700237274169922
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31866.785
Exploding loss, terminate run (best metric=0.5287237763404846)
Finished Training
Total time taken: 0.19999980926513672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31813.270
Exploding loss, terminate run (best metric=0.5270559191703796)
Finished Training
Total time taken: 0.2420332431793213
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31661.836
Exploding loss, terminate run (best metric=0.5645499229431152)
Finished Training
Total time taken: 0.24001622200012207
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31712.359
Exploding loss, terminate run (best metric=0.5318301320075989)
Finished Training
Total time taken: 0.23399782180786133
{'Hydroxylation-K Validation Accuracy': 0.5578014184397163, 'Hydroxylation-K Validation Sensitivity': 0.4866666666666667, 'Hydroxylation-K Validation Specificity': 0.5736842105263158, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6107992202729045, 'Hydroxylation-K AUC PR': 0.34089691782806275, 'Hydroxylation-K MCC': 0.05360052093043696, 'Hydroxylation-K F1': 0.20613983717431994, 'Validation Loss (Hydroxylation-K)': 0.5611334323883057, 'Hydroxylation-P Validation Accuracy': 0.5631535116660745, 'Hydroxylation-P Validation Sensitivity': 0.4318518518518519, 'Hydroxylation-P Validation Specificity': 0.5914634146341463, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5670668053744516, 'Hydroxylation-P AUC PR': 0.28066893073989096, 'Hydroxylation-P MCC': 0.030294739221569927, 'Hydroxylation-P F1': 0.16142883446811535, 'Validation Loss (Hydroxylation-P)': 0.53569389184316, 'Validation Loss (total)': 1.0968273401260376, 'TimeToTrain': 0.22940371831258138}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004580744850641091,
 'learning_rate_Hydroxylation-K': 0.006510463530936135,
 'learning_rate_Hydroxylation-P': 0.006257396466604766,
 'log_base': 2.7351713281760923,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1077172308,
 'sample_weights': [97.5933913695127, 12.1738261931346],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.511787513067668,
 'weight_decay_Hydroxylation-K': 8.660316525713924,
 'weight_decay_Hydroxylation-P': 2.4195032212602743}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.089
[2,     1] loss: 1260.517
[3,     1] loss: 1261.904
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00391841620789947,
 'learning_rate_Hydroxylation-K': 0.0027393507184718174,
 'learning_rate_Hydroxylation-P': 0.00021299400661033604,
 'log_base': 2.8097204105445464,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2447090227,
 'sample_weights': [1.659166144972182, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6347918598474767,
 'weight_decay_Hydroxylation-K': 0.6615473773675509,
 'weight_decay_Hydroxylation-P': 1.3654774343527418}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.703
[2,     1] loss: 1246.774
[3,     1] loss: 1249.754
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004463254245747879,
 'learning_rate_Hydroxylation-K': 0.0017338615132405709,
 'learning_rate_Hydroxylation-P': 0.007612243097480147,
 'log_base': 1.0720247077617344,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2295966489,
 'sample_weights': [1.6159785294992002, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.021383018879075566,
 'weight_decay_Hydroxylation-K': 6.797999564652603,
 'weight_decay_Hydroxylation-P': 2.060588621922691}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7857.297
[2,     1] loss: 7802.142
[3,     1] loss: 7793.437
[4,     1] loss: 7796.659
[5,     1] loss: 7811.263
[6,     1] loss: 7808.054
[7,     1] loss: 7792.952
[8,     1] loss: 7813.854
[9,     1] loss: 7798.248
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014288856396246423,
 'learning_rate_Hydroxylation-K': 0.002850119404586136,
 'learning_rate_Hydroxylation-P': 0.009332986262467697,
 'log_base': 1.014626773626786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 983555061,
 'sample_weights': [24.00380294419682, 3.0005909583845747],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.625610143772928,
 'weight_decay_Hydroxylation-K': 7.125234376454538,
 'weight_decay_Hydroxylation-P': 3.140299598878281}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37675.285
Exploding loss, terminate run (best metric=0.5323317050933838)
Finished Training
Total time taken: 0.23400211334228516
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37434.188
Exploding loss, terminate run (best metric=0.531618595123291)
Finished Training
Total time taken: 0.24799871444702148
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37332.691
Exploding loss, terminate run (best metric=0.5307563543319702)
Finished Training
Total time taken: 0.22600150108337402
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37655.691
Exploding loss, terminate run (best metric=0.5322707295417786)
Finished Training
Total time taken: 0.2050008773803711
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37227.301
Exploding loss, terminate run (best metric=0.5358147621154785)
Finished Training
Total time taken: 0.21700072288513184
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37393.719
Exploding loss, terminate run (best metric=0.5340608358383179)
Finished Training
Total time taken: 0.24200177192687988
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37437.824
Exploding loss, terminate run (best metric=0.5283998250961304)
Finished Training
Total time taken: 0.22499775886535645
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37322.312
Exploding loss, terminate run (best metric=0.5269516706466675)
Finished Training
Total time taken: 0.20100069046020508
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37123.227
Exploding loss, terminate run (best metric=0.5350645184516907)
Finished Training
Total time taken: 0.21899914741516113
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37349.664
Exploding loss, terminate run (best metric=0.5297804474830627)
Finished Training
Total time taken: 0.2200000286102295
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37239.258
Exploding loss, terminate run (best metric=0.5392741560935974)
Finished Training
Total time taken: 0.20299816131591797
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37361.094
Exploding loss, terminate run (best metric=0.5290192365646362)
Finished Training
Total time taken: 0.2460007667541504
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37456.855
Exploding loss, terminate run (best metric=0.5290722846984863)
Finished Training
Total time taken: 0.2589991092681885
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37280.602
Exploding loss, terminate run (best metric=0.5316597819328308)
Finished Training
Total time taken: 0.2340230941772461
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37302.035
Exploding loss, terminate run (best metric=0.5282878875732422)
Finished Training
Total time taken: 0.22900176048278809
{'Hydroxylation-K Validation Accuracy': 0.5342789598108747, 'Hydroxylation-K Validation Sensitivity': 0.45925925925925926, 'Hydroxylation-K Validation Specificity': 0.5508771929824562, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6235867446393762, 'Hydroxylation-K AUC PR': 0.30845879963983736, 'Hydroxylation-K MCC': 0.019790899029713375, 'Hydroxylation-K F1': 0.15979559935824839, 'Validation Loss (Hydroxylation-K)': 0.556533670425415, 'Hydroxylation-P Validation Accuracy': 0.5428771635957566, 'Hydroxylation-P Validation Sensitivity': 0.44761904761904764, 'Hydroxylation-P Validation Specificity': 0.5634944386253679, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6006533853170842, 'Hydroxylation-P AUC PR': 0.2712876835878306, 'Hydroxylation-P MCC': 0.013568616583575744, 'Hydroxylation-P F1': 0.14186702581683083, 'Validation Loss (Hydroxylation-P)': 0.531624186038971, 'Validation Loss (total)': 1.088157844543457, 'TimeToTrain': 0.2272017478942871}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021938250089878915,
 'learning_rate_Hydroxylation-K': 0.001089333926063334,
 'learning_rate_Hydroxylation-P': 0.009729581160352116,
 'log_base': 1.1445673282501896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4248982861,
 'sample_weights': [115.05410222126102, 14.351880015579857],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5560434709123953,
 'weight_decay_Hydroxylation-K': 7.666478890301322,
 'weight_decay_Hydroxylation-P': 1.8734131289289793}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4019.107
[2,     1] loss: 4007.866
[3,     1] loss: 4004.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008477464541565639,
 'learning_rate_Hydroxylation-K': 0.0018474396759992765,
 'learning_rate_Hydroxylation-P': 0.0068323358931054404,
 'log_base': 1.3283995538401117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2058093797,
 'sample_weights': [12.363801502834805, 1.545534725764642],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.488782306987926,
 'weight_decay_Hydroxylation-K': 7.017340484835896,
 'weight_decay_Hydroxylation-P': 1.687320109204988}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2154.029
[2,     1] loss: 2151.118
[3,     1] loss: 2151.488
[4,     1] loss: 2151.201
[5,     1] loss: 2145.989
[6,     1] loss: 2149.814
[7,     1] loss: 2138.347
[8,     1] loss: 2143.830
[9,     1] loss: 2144.616
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014693404098076015,
 'learning_rate_Hydroxylation-K': 0.001142608570512441,
 'learning_rate_Hydroxylation-P': 0.000706567149263542,
 'log_base': 2.8428882547252323,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3093127457,
 'sample_weights': [5.878841039847169, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.108801621030612,
 'weight_decay_Hydroxylation-K': 0.8128949172200388,
 'weight_decay_Hydroxylation-P': 5.124683598178467}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.069
[2,     1] loss: 1245.541
[3,     1] loss: 1247.442
[4,     1] loss: 1243.964
[5,     1] loss: 1242.990
[6,     1] loss: 1244.367
[7,     1] loss: 1242.986
[8,     1] loss: 1242.582
[9,     1] loss: 1242.311
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001190631485295737,
 'learning_rate_Hydroxylation-K': 0.00207205636706008,
 'learning_rate_Hydroxylation-P': 0.009287789523746466,
 'log_base': 1.1389881392695695,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2170354671,
 'sample_weights': [1.5978276700157577, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.558521720000442,
 'weight_decay_Hydroxylation-K': 9.07738599193643,
 'weight_decay_Hydroxylation-P': 5.956617591991012}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4197.557
[2,     1] loss: 4162.195
[3,     1] loss: 4160.134
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00029054702712769716,
 'learning_rate_Hydroxylation-K': 0.006581155164360551,
 'learning_rate_Hydroxylation-P': 0.0014438847409022648,
 'log_base': 1.819726425134596,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3631388286,
 'sample_weights': [12.828028810514969, 1.6035653747121694],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.937924317413366,
 'weight_decay_Hydroxylation-K': 7.590010299914555,
 'weight_decay_Hydroxylation-P': 8.073470990704994}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1501.006
[2,     1] loss: 1499.007
[3,     1] loss: 1500.234
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017022950886209382,
 'learning_rate_Hydroxylation-K': 0.0031663237386621744,
 'learning_rate_Hydroxylation-P': 0.007963385450099348,
 'log_base': 1.2921752859827387,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1156020168,
 'sample_weights': [2.7885112770597953, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.271932650824709,
 'weight_decay_Hydroxylation-K': 9.503243385767867,
 'weight_decay_Hydroxylation-P': 2.277937534107713}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2290.094
[2,     1] loss: 2276.598
[3,     1] loss: 2280.094
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001225534298252615,
 'learning_rate_Hydroxylation-K': 0.00596593093549925,
 'learning_rate_Hydroxylation-P': 0.00873161271900164,
 'log_base': 1.2928238221716126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 866332118,
 'sample_weights': [6.512941339821999, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4123126362537812,
 'weight_decay_Hydroxylation-K': 7.420782433175597,
 'weight_decay_Hydroxylation-P': 5.005347874587079}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2282.678
[2,     1] loss: 2290.788
[3,     1] loss: 2283.573
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023861505012672886,
 'learning_rate_Hydroxylation-K': 6.902931497688829e-05,
 'learning_rate_Hydroxylation-P': 0.00944627795057482,
 'log_base': 1.1276249888597798,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1142203754,
 'sample_weights': [6.500216942944103, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.5102658016302986,
 'weight_decay_Hydroxylation-K': 6.761535489632287,
 'weight_decay_Hydroxylation-P': 2.9771680944844716}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4502.207
[2,     1] loss: 4525.958
[3,     1] loss: 4494.396
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006023668451131981,
 'learning_rate_Hydroxylation-K': 0.004754074372096551,
 'learning_rate_Hydroxylation-P': 0.009584514191026657,
 'log_base': 1.055586084215299,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2917617271,
 'sample_weights': [13.898863875262416, 1.7374249144139466],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9072052457620037,
 'weight_decay_Hydroxylation-K': 5.42189540061646,
 'weight_decay_Hydroxylation-P': 5.772670371513683}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10021.683
[2,     1] loss: 9994.693
[3,     1] loss: 10045.116
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052322354493612964,
 'learning_rate_Hydroxylation-K': 0.0028578721484413686,
 'learning_rate_Hydroxylation-P': 0.00211204068139798,
 'log_base': 2.477490122873621,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2246117519,
 'sample_weights': [30.86066885471909, 3.8577322164510464],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.905226240991745,
 'weight_decay_Hydroxylation-K': 3.193106365197794,
 'weight_decay_Hydroxylation-P': 4.929780749290303}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.598
[2,     1] loss: 1297.589
[3,     1] loss: 1295.833
[4,     1] loss: 1288.899
[5,     1] loss: 1295.203
[6,     1] loss: 1298.547
[7,     1] loss: 1289.284
[8,     1] loss: 1287.947
[9,     1] loss: 1267.597
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003381188292541266,
 'learning_rate_Hydroxylation-K': 0.0030426373491046583,
 'learning_rate_Hydroxylation-P': 0.0065981761267123115,
 'log_base': 2.8440801266420985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3654604477,
 'sample_weights': [1.8401218043783358, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.235851693424394,
 'weight_decay_Hydroxylation-K': 4.2004286154206065,
 'weight_decay_Hydroxylation-P': 0.8126488403776478}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.167
[2,     1] loss: 1248.956
[3,     1] loss: 1247.681
[4,     1] loss: 1245.056
[5,     1] loss: 1245.283
[6,     1] loss: 1236.685
[7,     1] loss: 1236.543
[8,     1] loss: 1234.490
[9,     1] loss: 1220.565
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006309490964000204,
 'learning_rate_Hydroxylation-K': 0.0032439760015299064,
 'learning_rate_Hydroxylation-P': 0.00861667125104369,
 'log_base': 2.8035051336981693,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1871307706,
 'sample_weights': [1.597186913807949, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.562668951088511,
 'weight_decay_Hydroxylation-K': 4.546127227454746,
 'weight_decay_Hydroxylation-P': 0.19220396485117286}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.096
[2,     1] loss: 1250.685
[3,     1] loss: 1252.710
[4,     1] loss: 1249.160
[5,     1] loss: 1247.957
[6,     1] loss: 1250.887
[7,     1] loss: 1246.271
[8,     1] loss: 1250.195
[9,     1] loss: 1249.576
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005075073007887739,
 'learning_rate_Hydroxylation-K': 0.0007844390613797121,
 'learning_rate_Hydroxylation-P': 0.004044652255572083,
 'log_base': 2.800388811928091,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1534908877,
 'sample_weights': [1.6194499693642619, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9187209849988465,
 'weight_decay_Hydroxylation-K': 4.652065649713794,
 'weight_decay_Hydroxylation-P': 1.219029284553026}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.393
[2,     1] loss: 1249.177
[3,     1] loss: 1251.528
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009016918842024905,
 'learning_rate_Hydroxylation-K': 0.009885115598525594,
 'learning_rate_Hydroxylation-P': 8.266968840423884e-05,
 'log_base': 1.4973356734733763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4137161951,
 'sample_weights': [1.6211990695021579, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.035113680639556,
 'weight_decay_Hydroxylation-K': 2.078476889056117,
 'weight_decay_Hydroxylation-P': 4.806318767097412}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1794.484
[2,     1] loss: 1784.243
[3,     1] loss: 1776.376
[4,     1] loss: 1784.987
[5,     1] loss: 1788.677
[6,     1] loss: 1776.617
[7,     1] loss: 1784.477
[8,     1] loss: 1789.493
[9,     1] loss: 1776.522
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006679036249584166,
 'learning_rate_Hydroxylation-K': 0.0098156565315227,
 'learning_rate_Hydroxylation-P': 0.0015045594720646276,
 'log_base': 1.0464650691259352,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3083474473,
 'sample_weights': [4.135485811934149, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.243451055148577,
 'weight_decay_Hydroxylation-K': 4.6035465149019,
 'weight_decay_Hydroxylation-P': 5.020870621006889}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11952.550
[2,     1] loss: 11955.215
[3,     1] loss: 11941.473
[4,     1] loss: 11962.502
[5,     1] loss: 12006.330
[6,     1] loss: 11916.166
[7,     1] loss: 11965.530
[8,     1] loss: 11896.180
[9,     1] loss: 11904.231
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004930196011600382,
 'learning_rate_Hydroxylation-K': 0.002588287950225498,
 'learning_rate_Hydroxylation-P': 0.008685441756353234,
 'log_base': 1.2009380570574002,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 547952707,
 'sample_weights': [36.757396330290455, 4.594851546600146],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3149368993702262,
 'weight_decay_Hydroxylation-K': 8.91692025681556,
 'weight_decay_Hydroxylation-P': 3.1439238930040063}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2960.967
[2,     1] loss: 2955.065
[3,     1] loss: 2960.196
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001549023742965568,
 'learning_rate_Hydroxylation-K': 0.001168889503533469,
 'learning_rate_Hydroxylation-P': 0.002772108689484926,
 'log_base': 2.369883975252268,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 741663662,
 'sample_weights': [9.117510148939681, 1.1397326740053946],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.195925600913196,
 'weight_decay_Hydroxylation-K': 4.480277772211836,
 'weight_decay_Hydroxylation-P': 1.0767948695064389}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.785
[2,     1] loss: 1311.892
[3,     1] loss: 1318.367
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005973535197878094,
 'learning_rate_Hydroxylation-K': 0.004024063276354105,
 'learning_rate_Hydroxylation-P': 0.006724816648653271,
 'log_base': 2.403841334273007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2952991580,
 'sample_weights': [1.9348213061257842, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.530955593039675,
 'weight_decay_Hydroxylation-K': 0.2400185158430377,
 'weight_decay_Hydroxylation-P': 7.170854623370755}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.668
[2,     1] loss: 1314.046
[3,     1] loss: 1310.433
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003091376794446355,
 'learning_rate_Hydroxylation-K': 0.002869749931517305,
 'learning_rate_Hydroxylation-P': 0.008094162989982032,
 'log_base': 1.0685190293787208,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2745896240,
 'sample_weights': [1.903436359781802, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.041523364885821,
 'weight_decay_Hydroxylation-K': 7.043975548695584,
 'weight_decay_Hydroxylation-P': 2.5714121041944624}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8209.887
[2,     1] loss: 8165.680
[3,     1] loss: 8164.872
[4,     1] loss: 8138.602
[5,     1] loss: 8188.705
[6,     1] loss: 8181.625
[7,     1] loss: 8184.068
[8,     1] loss: 8173.276
[9,     1] loss: 8172.243
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003801736285543821,
 'learning_rate_Hydroxylation-K': 0.006045772691369022,
 'learning_rate_Hydroxylation-P': 0.00885939161487312,
 'log_base': 1.206146454964329,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2811074130,
 'sample_weights': [25.19016652462574, 3.148892118874329],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.307312588724074,
 'weight_decay_Hydroxylation-K': 5.506228216476513,
 'weight_decay_Hydroxylation-P': 2.6097445463726157}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2891.833
[2,     1] loss: 2887.472
[3,     1] loss: 2893.854
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006632113045252061,
 'learning_rate_Hydroxylation-K': 0.0042916991215622945,
 'learning_rate_Hydroxylation-P': 0.008900900474946344,
 'log_base': 2.6514567385759498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1469594395,
 'sample_weights': [8.906996909728772, 1.113417505377116],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.662330350982517,
 'weight_decay_Hydroxylation-K': 6.850432661414201,
 'weight_decay_Hydroxylation-P': 1.1257404986159742}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.694
[2,     1] loss: 1269.362
[3,     1] loss: 1272.367
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020431684466228352,
 'learning_rate_Hydroxylation-K': 0.00037537948674215994,
 'learning_rate_Hydroxylation-P': 0.007474546335521871,
 'log_base': 1.0376051324268536,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 178316544,
 'sample_weights': [1.7120576286535003, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.0020971453055347,
 'weight_decay_Hydroxylation-K': 5.097813441731167,
 'weight_decay_Hydroxylation-P': 5.447202910284036}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14701.162
[2,     1] loss: 14743.889
[3,     1] loss: 14663.326
[4,     1] loss: 14617.468
[5,     1] loss: 14647.389
[6,     1] loss: 14737.445
[7,     1] loss: 14627.846
[8,     1] loss: 14620.291
[9,     1] loss: 14624.820
[10,     1] loss: 14497.756
[11,     1] loss: 14507.291
[12,     1] loss: 14531.455
[13,     1] loss: 14397.578
[14,     1] loss: 14194.365
[15,     1] loss: 14032.205
[16,     1] loss: 13880.514
[17,     1] loss: 13779.213
[18,     1] loss: 13586.027
[19,     1] loss: 13525.957
[20,     1] loss: 13205.520
[21,     1] loss: 13043.227
[22,     1] loss: 13201.234
[23,     1] loss: 12386.670
[24,     1] loss: 13156.432
[25,     1] loss: 12348.951
[26,     1] loss: 12746.441
[27,     1] loss: 12380.562
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008393727004827515,
 'learning_rate_Hydroxylation-K': 0.006231732285375712,
 'learning_rate_Hydroxylation-P': 0.0004057252199940164,
 'log_base': 1.3413243769622945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2609841221,
 'sample_weights': [45.22360991617951, 5.653169013904341],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.352597379874197,
 'weight_decay_Hydroxylation-K': 4.697579144817856,
 'weight_decay_Hydroxylation-P': 2.063586378801661}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2110.831
[2,     1] loss: 2108.252
[3,     1] loss: 2101.513
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006255162444036211,
 'learning_rate_Hydroxylation-K': 0.0037351112949003565,
 'learning_rate_Hydroxylation-P': 0.00939871217459607,
 'log_base': 2.795982293445206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 406237276,
 'sample_weights': [5.685001526071831, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.077153897195064,
 'weight_decay_Hydroxylation-K': 9.732794869586522,
 'weight_decay_Hydroxylation-P': 8.306092798164793}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.388
[2,     1] loss: 1252.412
[3,     1] loss: 1269.167
[4,     1] loss: 1248.792
[5,     1] loss: 1252.703
[6,     1] loss: 1250.041
[7,     1] loss: 1251.314
[8,     1] loss: 1248.569
[9,     1] loss: 1245.923
[10,     1] loss: 1246.068
[11,     1] loss: 1244.182
[12,     1] loss: 1239.113
[13,     1] loss: 1231.163
[14,     1] loss: 1216.782
[15,     1] loss: 1192.023
[16,     1] loss: 1174.690
[17,     1] loss: 1137.934
[18,     1] loss: 1089.778
[19,     1] loss: 1076.204
[20,     1] loss: 1067.468
[21,     1] loss: 1004.615
[22,     1] loss: 983.246
[23,     1] loss: 1070.278
[24,     1] loss: 1038.380
[25,     1] loss: 1007.220
[26,     1] loss: 958.968
[27,     1] loss: 978.276
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009133132930723603,
 'learning_rate_Hydroxylation-K': 0.009049321386276352,
 'learning_rate_Hydroxylation-P': 0.0016705856845959295,
 'log_base': 1.0586909946493352,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2685955190,
 'sample_weights': [1.623682116244855, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.326365384550265,
 'weight_decay_Hydroxylation-K': 4.857460506205124,
 'weight_decay_Hydroxylation-P': 3.540124646737636}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9479.262
[2,     1] loss: 9471.092
[3,     1] loss: 9555.640
[4,     1] loss: 9508.018
[5,     1] loss: 9487.213
[6,     1] loss: 9466.921
[7,     1] loss: 9496.077
[8,     1] loss: 9452.096
[9,     1] loss: 9435.145
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006581186350724848,
 'learning_rate_Hydroxylation-K': 0.0007836346613642002,
 'learning_rate_Hydroxylation-P': 0.005208057185427534,
 'log_base': 2.2888271873127763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3225537697,
 'sample_weights': [29.271409351856384, 3.6590671255109672],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.389550650963994,
 'weight_decay_Hydroxylation-K': 4.275663610875435,
 'weight_decay_Hydroxylation-P': 3.7361212558541057}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.957
[2,     1] loss: 1333.434
[3,     1] loss: 1329.381
[4,     1] loss: 1335.713
[5,     1] loss: 1323.785
[6,     1] loss: 1327.007
[7,     1] loss: 1305.605
[8,     1] loss: 1270.302
[9,     1] loss: 1280.724
[10,     1] loss: 1206.287
[11,     1] loss: 1167.848
[12,     1] loss: 1162.224
[13,     1] loss: 1098.157
[14,     1] loss: 1118.723
[15,     1] loss: 1057.960
[16,     1] loss: 1073.313
[17,     1] loss: 1080.854
[18,     1] loss: 1036.783
[19,     1] loss: 1048.763
[20,     1] loss: 1000.612
[21,     1] loss: 1023.561
[22,     1] loss: 1033.575
[23,     1] loss: 935.829
[24,     1] loss: 983.203
[25,     1] loss: 938.454
[26,     1] loss: 1037.126
[27,     1] loss: 1051.324
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009935315487172578,
 'learning_rate_Hydroxylation-K': 0.006901032104436416,
 'learning_rate_Hydroxylation-P': 0.0011259929441315949,
 'log_base': 1.0424309566611607,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3601457772,
 'sample_weights': [2.016139404662461, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.43758116368459,
 'weight_decay_Hydroxylation-K': 0.6232422772801185,
 'weight_decay_Hydroxylation-P': 1.5377022517141454}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13002.975
[2,     1] loss: 13034.738
[3,     1] loss: 12937.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009711980384353832,
 'learning_rate_Hydroxylation-K': 0.0003149303841094315,
 'learning_rate_Hydroxylation-P': 0.006842075600707316,
 'log_base': 2.230326273433605,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 850345685,
 'sample_weights': [40.17387354678691, 5.021927650714913],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.0697497539716263,
 'weight_decay_Hydroxylation-K': 3.219992701596338,
 'weight_decay_Hydroxylation-P': 3.052397149863973}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.835
[2,     1] loss: 1346.436
[3,     1] loss: 1348.007
[4,     1] loss: 1350.049
[5,     1] loss: 1346.573
[6,     1] loss: 1343.294
[7,     1] loss: 1344.656
[8,     1] loss: 1340.851
[9,     1] loss: 1337.239
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002131455863035721,
 'learning_rate_Hydroxylation-K': 0.0033589541769840723,
 'learning_rate_Hydroxylation-P': 0.009777892529641435,
 'log_base': 1.0302320672405556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1402397184,
 'sample_weights': [2.081216166158543, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.382793788719956,
 'weight_decay_Hydroxylation-K': 9.22527697436592,
 'weight_decay_Hydroxylation-P': 3.0364175851800352}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18185.654
Exploding loss, terminate run (best metric=0.5325269103050232)
Finished Training
Total time taken: 0.20399880409240723
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18356.912
Exploding loss, terminate run (best metric=0.5278893113136292)
Finished Training
Total time taken: 0.2259998321533203
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18328.938
Exploding loss, terminate run (best metric=0.5333462953567505)
Finished Training
Total time taken: 0.2330012321472168
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18278.471
Exploding loss, terminate run (best metric=0.5328009128570557)
Finished Training
Total time taken: 0.2220005989074707
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18158.836
Exploding loss, terminate run (best metric=0.5282503962516785)
Finished Training
Total time taken: 0.2070016860961914
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18155.855
Exploding loss, terminate run (best metric=0.5317999124526978)
Finished Training
Total time taken: 0.22700214385986328
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18186.350
Exploding loss, terminate run (best metric=0.5304446816444397)
Finished Training
Total time taken: 0.23000025749206543
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18331.934
Exploding loss, terminate run (best metric=0.5323037505149841)
Finished Training
Total time taken: 0.22200274467468262
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18185.846
Exploding loss, terminate run (best metric=0.5286037921905518)
Finished Training
Total time taken: 0.2089991569519043
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18248.438
Exploding loss, terminate run (best metric=0.5309374928474426)
Finished Training
Total time taken: 0.22600078582763672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18134.965
Exploding loss, terminate run (best metric=0.5314712524414062)
Finished Training
Total time taken: 0.21399831771850586
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18233.809
Exploding loss, terminate run (best metric=0.5264760851860046)
Finished Training
Total time taken: 0.21700000762939453
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18250.666
Exploding loss, terminate run (best metric=0.5294332504272461)
Finished Training
Total time taken: 0.20600104331970215
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18201.387
Exploding loss, terminate run (best metric=0.5275922417640686)
Finished Training
Total time taken: 0.22099900245666504
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18215.033
Exploding loss, terminate run (best metric=0.5343222618103027)
Finished Training
Total time taken: 0.23800253868103027
{'Hydroxylation-K Validation Accuracy': 0.47916666666666663, 'Hydroxylation-K Validation Sensitivity': 0.5466666666666666, 'Hydroxylation-K Validation Specificity': 0.4614035087719298, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6722222222222223, 'Hydroxylation-K AUC PR': 0.38079876975440036, 'Hydroxylation-K MCC': 0.010728908570966901, 'Hydroxylation-K F1': 0.19700602079912427, 'Validation Loss (Hydroxylation-K)': 0.5565858085950216, 'Hydroxylation-P Validation Accuracy': 0.4707816523695921, 'Hydroxylation-P Validation Sensitivity': 0.540952380952381, 'Hydroxylation-P Validation Specificity': 0.4548780487804878, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6108372186555637, 'Hydroxylation-P AUC PR': 0.29245824043231083, 'Hydroxylation-P MCC': -0.004268116148202492, 'Hydroxylation-P F1': 0.16866077848281455, 'Validation Loss (Hydroxylation-P)': 0.5305465698242188, 'Validation Loss (total)': 1.0871323664983115, 'TimeToTrain': 0.22013387680053711}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004513246395045454,
 'learning_rate_Hydroxylation-K': 0.0015737926618234438,
 'learning_rate_Hydroxylation-P': 0.0019792233470726857,
 'log_base': 2.8403155405656526,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3360445560,
 'sample_weights': [56.09309620560044, 6.997068082778803],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.472410151990825,
 'weight_decay_Hydroxylation-K': 1.1703150697092317,
 'weight_decay_Hydroxylation-P': 1.324642646895145}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.935
[2,     1] loss: 1250.521
[3,     1] loss: 1244.998
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002225220576834185,
 'learning_rate_Hydroxylation-K': 0.004615651661823361,
 'learning_rate_Hydroxylation-P': 0.00952180348310023,
 'log_base': 1.0507108086087016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927746494,
 'sample_weights': [1.5992134461217877, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.170207148860587,
 'weight_decay_Hydroxylation-K': 8.92080281769284,
 'weight_decay_Hydroxylation-P': 0.735443159597355}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10954.380
[2,     1] loss: 10905.709
[3,     1] loss: 10935.087
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00015100395931857925,
 'learning_rate_Hydroxylation-K': 0.0027921112161123016,
 'learning_rate_Hydroxylation-P': 0.008924597972818415,
 'log_base': 1.0499139374675788,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2489602399,
 'sample_weights': [33.74869442085857, 4.218749319511238],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9818870013870615,
 'weight_decay_Hydroxylation-K': 5.454134624899396,
 'weight_decay_Hydroxylation-P': 2.861763167991183}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11141.477
[2,     1] loss: 11157.407
[3,     1] loss: 11093.285
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005072221476973367,
 'learning_rate_Hydroxylation-K': 0.0051347109647025165,
 'learning_rate_Hydroxylation-P': 0.0017295215115801222,
 'log_base': 2.6634575312218582,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2408679674,
 'sample_weights': [34.274378186916415, 4.284462321699671],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.124636578570363,
 'weight_decay_Hydroxylation-K': 8.175025166833143,
 'weight_decay_Hydroxylation-P': 2.3487678751737517}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.434
[2,     1] loss: 1265.079
[3,     1] loss: 1268.699
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004443707939788697,
 'learning_rate_Hydroxylation-K': 0.009652914406633753,
 'learning_rate_Hydroxylation-P': 0.005067735927340844,
 'log_base': 1.0359704076668577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 456423296,
 'sample_weights': [1.7041653415438143, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.923894545895733,
 'weight_decay_Hydroxylation-K': 0.7351487124592537,
 'weight_decay_Hydroxylation-P': 4.085394122720924}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15384.625
Exploding loss, terminate run (best metric=0.5401353240013123)
Finished Training
Total time taken: 0.2220139503479004
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15412.041
Exploding loss, terminate run (best metric=0.5285643935203552)
Finished Training
Total time taken: 0.22899842262268066
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15333.324
Exploding loss, terminate run (best metric=0.5316967964172363)
Finished Training
Total time taken: 0.22331738471984863
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15325.339
Exploding loss, terminate run (best metric=0.5295116305351257)
Finished Training
Total time taken: 0.20999836921691895
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15361.342
Exploding loss, terminate run (best metric=0.5386992692947388)
Finished Training
Total time taken: 0.22699952125549316
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15432.734
Exploding loss, terminate run (best metric=0.5350108742713928)
Finished Training
Total time taken: 0.2259998321533203
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15314.270
Exploding loss, terminate run (best metric=0.5326163172721863)
Finished Training
Total time taken: 0.23000240325927734
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15490.305
Exploding loss, terminate run (best metric=0.5290082097053528)
Finished Training
Total time taken: 0.21000242233276367
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15334.531
Exploding loss, terminate run (best metric=0.5301227569580078)
Finished Training
Total time taken: 0.23600339889526367
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15332.145
Exploding loss, terminate run (best metric=0.5364312529563904)
Finished Training
Total time taken: 0.24499797821044922
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15491.125
Exploding loss, terminate run (best metric=0.5316162109375)
Finished Training
Total time taken: 0.22300171852111816
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15397.131
Exploding loss, terminate run (best metric=0.5308353304862976)
Finished Training
Total time taken: 0.20200085639953613
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15375.385
Exploding loss, terminate run (best metric=0.5368958711624146)
Finished Training
Total time taken: 0.22499966621398926
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15316.498
Exploding loss, terminate run (best metric=0.527138352394104)
Finished Training
Total time taken: 0.24200081825256348
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15370.842
Exploding loss, terminate run (best metric=0.5319018959999084)
Finished Training
Total time taken: 0.21799969673156738
{'Hydroxylation-K Validation Accuracy': 0.6058806146572103, 'Hydroxylation-K Validation Sensitivity': 0.34074074074074073, 'Hydroxylation-K Validation Specificity': 0.6701754385964912, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6030019493177388, 'Hydroxylation-K AUC PR': 0.2947161359543331, 'Hydroxylation-K MCC': 0.01984998319912602, 'Hydroxylation-K F1': 0.1267577250335871, 'Validation Loss (Hydroxylation-K)': 0.558450170358022, 'Hydroxylation-P Validation Accuracy': 0.6186431145627126, 'Hydroxylation-P Validation Sensitivity': 0.3314285714285714, 'Hydroxylation-P Validation Specificity': 0.6804878048780488, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.550066776872852, 'Hydroxylation-P AUC PR': 0.2516305005001707, 'Hydroxylation-P MCC': 0.01191637630662021, 'Hydroxylation-P F1': 0.10294861239414516, 'Validation Loss (Hydroxylation-P)': 0.5326789657274882, 'Validation Loss (total)': 1.0911291281382243, 'TimeToTrain': 0.22455576260884602}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013133411085920962,
 'learning_rate_Hydroxylation-K': 0.004372649530746414,
 'learning_rate_Hydroxylation-P': 0.009029667589342977,
 'log_base': 1.1074842147836768,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1444032991,
 'sample_weights': [47.276420444280554, 5.897273549426202],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4720344401073044,
 'weight_decay_Hydroxylation-K': 8.839599195553184,
 'weight_decay_Hydroxylation-P': 3.7702222043419074}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5359.304
[2,     1] loss: 5300.613
[3,     1] loss: 5301.277
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006895510148751441,
 'learning_rate_Hydroxylation-K': 0.004065535441884319,
 'learning_rate_Hydroxylation-P': 0.00036766404591455755,
 'log_base': 1.422394920962365,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2206994585,
 'sample_weights': [16.35250551924726, 2.0441419354281822],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.1765513786941293,
 'weight_decay_Hydroxylation-K': 1.1888400470466647,
 'weight_decay_Hydroxylation-P': 9.184881704719832}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1911.133
[2,     1] loss: 1907.609
[3,     1] loss: 1907.448
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004948855406258137,
 'learning_rate_Hydroxylation-K': 0.0012899699863845916,
 'learning_rate_Hydroxylation-P': 0.005001178808747827,
 'log_base': 2.9987863126777072,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 8519435,
 'sample_weights': [4.738132485656044, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.967659352813328,
 'weight_decay_Hydroxylation-K': 1.3884101414855463,
 'weight_decay_Hydroxylation-P': 4.378285509462673}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.122
[2,     1] loss: 1229.037
[3,     1] loss: 1229.398
[4,     1] loss: 1229.399
[5,     1] loss: 1225.010
[6,     1] loss: 1222.027
[7,     1] loss: 1204.745
[8,     1] loss: 1176.136
[9,     1] loss: 1148.582
[10,     1] loss: 1123.449
[11,     1] loss: 1083.140
[12,     1] loss: 1062.513
[13,     1] loss: 1116.204
[14,     1] loss: 1042.505
[15,     1] loss: 992.175
[16,     1] loss: 1027.417
[17,     1] loss: 967.538
[18,     1] loss: 976.781
[19,     1] loss: 964.333
[20,     1] loss: 960.325
[21,     1] loss: 926.728
[22,     1] loss: 972.210
[23,     1] loss: 919.427
[24,     1] loss: 873.513
[25,     1] loss: 889.785
[26,     1] loss: 896.304
[27,     1] loss: 889.724
[28,     1] loss: 878.430
[29,     1] loss: 863.360
[30,     1] loss: 875.552
[31,     1] loss: 867.640
[32,     1] loss: 874.539
[33,     1] loss: 902.680
[34,     1] loss: 913.372
[35,     1] loss: 820.367
[36,     1] loss: 786.521
[37,     1] loss: 830.255
[38,     1] loss: 820.721
[39,     1] loss: 765.689
[40,     1] loss: 769.330
[41,     1] loss: 845.609
[42,     1] loss: 801.953
[43,     1] loss: 771.996
[44,     1] loss: 784.603
[45,     1] loss: 767.288
[46,     1] loss: 741.379
[47,     1] loss: 638.602
[48,     1] loss: 712.704
[49,     1] loss: 729.138
[50,     1] loss: 814.755
[51,     1] loss: 842.050
[52,     1] loss: 720.611
[53,     1] loss: 746.086
[54,     1] loss: 783.997
[55,     1] loss: 761.700
[56,     1] loss: 718.987
[57,     1] loss: 723.921
[58,     1] loss: 681.967
[59,     1] loss: 649.396
[60,     1] loss: 712.553
[61,     1] loss: 738.652
[62,     1] loss: 684.174
[63,     1] loss: 650.211
[64,     1] loss: 608.856
[65,     1] loss: 639.752
[66,     1] loss: 562.208
[67,     1] loss: 573.195
[68,     1] loss: 643.317
[69,     1] loss: 1026.368
[70,     1] loss: 1297.258
[71,     1] loss: 724.024
[72,     1] loss: 823.979
[73,     1] loss: 929.770
[74,     1] loss: 925.891
[75,     1] loss: 908.050
[76,     1] loss: 890.985
[77,     1] loss: 867.742
[78,     1] loss: 821.369
[79,     1] loss: 773.602
[80,     1] loss: 748.829
[81,     1] loss: 775.580
[82,     1] loss: 773.230
[83,     1] loss: 742.276
[84,     1] loss: 727.864
[85,     1] loss: 710.471
[86,     1] loss: 684.761
[87,     1] loss: 665.602
[88,     1] loss: 624.026
[89,     1] loss: 608.143
Early stopping applied (best metric=0.294519305229187)
Finished Training
Total time taken: 12.871071100234985
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.896
[2,     1] loss: 1228.459
[3,     1] loss: 1226.292
[4,     1] loss: 1225.668
[5,     1] loss: 1217.745
[6,     1] loss: 1209.189
[7,     1] loss: 1173.134
[8,     1] loss: 1182.066
[9,     1] loss: 1156.510
[10,     1] loss: 1074.630
[11,     1] loss: 1053.795
[12,     1] loss: 1031.027
[13,     1] loss: 995.449
[14,     1] loss: 1028.483
[15,     1] loss: 1000.913
[16,     1] loss: 1027.073
[17,     1] loss: 1008.679
[18,     1] loss: 957.806
[19,     1] loss: 1011.556
[20,     1] loss: 972.557
[21,     1] loss: 912.640
[22,     1] loss: 983.346
[23,     1] loss: 924.676
[24,     1] loss: 921.410
[25,     1] loss: 901.385
[26,     1] loss: 917.624
[27,     1] loss: 858.228
[28,     1] loss: 950.733
[29,     1] loss: 880.541
[30,     1] loss: 857.190
[31,     1] loss: 821.489
[32,     1] loss: 897.413
[33,     1] loss: 798.370
[34,     1] loss: 783.187
[35,     1] loss: 798.078
[36,     1] loss: 837.924
[37,     1] loss: 816.658
[38,     1] loss: 822.950
[39,     1] loss: 734.919
[40,     1] loss: 804.768
[41,     1] loss: 836.407
[42,     1] loss: 733.857
[43,     1] loss: 783.879
[44,     1] loss: 717.655
[45,     1] loss: 641.763
[46,     1] loss: 732.930
[47,     1] loss: 693.649
[48,     1] loss: 658.880
[49,     1] loss: 657.623
[50,     1] loss: 689.241
[51,     1] loss: 596.592
[52,     1] loss: 581.644
[53,     1] loss: 649.663
[54,     1] loss: 668.560
[55,     1] loss: 1508.391
[56,     1] loss: 795.222
[57,     1] loss: 943.123
[58,     1] loss: 856.271
[59,     1] loss: 895.376
[60,     1] loss: 886.790
[61,     1] loss: 867.406
[62,     1] loss: 783.047
[63,     1] loss: 783.862
Early stopping applied (best metric=0.3419976234436035)
Finished Training
Total time taken: 9.776598691940308
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.629
[2,     1] loss: 1227.908
[3,     1] loss: 1225.557
[4,     1] loss: 1226.874
[5,     1] loss: 1222.839
[6,     1] loss: 1223.745
[7,     1] loss: 1193.548
[8,     1] loss: 1165.969
[9,     1] loss: 1134.225
[10,     1] loss: 1100.631
[11,     1] loss: 1023.671
[12,     1] loss: 1050.316
[13,     1] loss: 1022.262
[14,     1] loss: 980.712
[15,     1] loss: 1097.804
[16,     1] loss: 1014.210
[17,     1] loss: 999.633
[18,     1] loss: 949.528
[19,     1] loss: 956.987
[20,     1] loss: 932.498
[21,     1] loss: 906.667
[22,     1] loss: 953.743
[23,     1] loss: 926.080
[24,     1] loss: 956.453
[25,     1] loss: 889.273
[26,     1] loss: 897.429
[27,     1] loss: 943.411
[28,     1] loss: 868.663
[29,     1] loss: 832.749
[30,     1] loss: 851.706
[31,     1] loss: 865.585
[32,     1] loss: 841.340
[33,     1] loss: 863.734
[34,     1] loss: 860.081
[35,     1] loss: 803.505
[36,     1] loss: 797.989
[37,     1] loss: 790.371
[38,     1] loss: 848.358
[39,     1] loss: 758.838
[40,     1] loss: 771.396
[41,     1] loss: 779.734
[42,     1] loss: 736.592
[43,     1] loss: 709.084
[44,     1] loss: 830.011
[45,     1] loss: 792.880
[46,     1] loss: 729.461
[47,     1] loss: 748.989
[48,     1] loss: 738.223
[49,     1] loss: 712.442
[50,     1] loss: 753.656
[51,     1] loss: 637.663
[52,     1] loss: 699.268
[53,     1] loss: 806.642
[54,     1] loss: 647.189
[55,     1] loss: 610.499
[56,     1] loss: 625.736
[57,     1] loss: 605.855
[58,     1] loss: 585.935
[59,     1] loss: 647.979
[60,     1] loss: 1351.920
[61,     1] loss: 1021.145
[62,     1] loss: 832.934
[63,     1] loss: 805.256
[64,     1] loss: 898.997
[65,     1] loss: 936.415
[66,     1] loss: 892.665
[67,     1] loss: 873.702
[68,     1] loss: 781.907
[69,     1] loss: 826.490
[70,     1] loss: 882.731
[71,     1] loss: 807.924
[72,     1] loss: 782.938
[73,     1] loss: 803.250
[74,     1] loss: 775.958
[75,     1] loss: 720.887
[76,     1] loss: 741.780
[77,     1] loss: 757.033
[78,     1] loss: 685.382
[79,     1] loss: 691.428
[80,     1] loss: 701.937
[81,     1] loss: 689.594
[82,     1] loss: 728.213
[83,     1] loss: 627.853
[84,     1] loss: 648.460
[85,     1] loss: 590.389
[86,     1] loss: 610.757
[87,     1] loss: 607.102
[88,     1] loss: 554.465
Early stopping applied (best metric=0.35692891478538513)
Finished Training
Total time taken: 14.836206912994385
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.600
[2,     1] loss: 1229.055
[3,     1] loss: 1233.310
[4,     1] loss: 1231.476
[5,     1] loss: 1233.230
[6,     1] loss: 1225.816
[7,     1] loss: 1223.891
[8,     1] loss: 1221.707
[9,     1] loss: 1213.462
[10,     1] loss: 1200.678
[11,     1] loss: 1177.908
[12,     1] loss: 1137.689
[13,     1] loss: 1119.335
[14,     1] loss: 1060.600
[15,     1] loss: 1044.969
[16,     1] loss: 1058.944
[17,     1] loss: 1014.496
[18,     1] loss: 1029.344
[19,     1] loss: 1014.438
[20,     1] loss: 1015.043
[21,     1] loss: 975.404
[22,     1] loss: 1039.820
[23,     1] loss: 984.364
[24,     1] loss: 1036.887
[25,     1] loss: 944.900
[26,     1] loss: 1008.093
[27,     1] loss: 909.703
[28,     1] loss: 967.957
[29,     1] loss: 926.553
[30,     1] loss: 947.695
[31,     1] loss: 867.041
[32,     1] loss: 943.781
[33,     1] loss: 886.050
[34,     1] loss: 895.505
[35,     1] loss: 941.501
[36,     1] loss: 896.600
[37,     1] loss: 902.423
[38,     1] loss: 846.556
[39,     1] loss: 879.490
[40,     1] loss: 892.387
[41,     1] loss: 867.110
[42,     1] loss: 826.890
[43,     1] loss: 804.203
[44,     1] loss: 804.728
[45,     1] loss: 787.420
[46,     1] loss: 777.525
[47,     1] loss: 781.099
[48,     1] loss: 818.563
[49,     1] loss: 787.510
[50,     1] loss: 727.248
[51,     1] loss: 802.077
[52,     1] loss: 788.420
[53,     1] loss: 751.289
[54,     1] loss: 741.241
[55,     1] loss: 794.988
[56,     1] loss: 907.321
[57,     1] loss: 771.622
[58,     1] loss: 742.659
[59,     1] loss: 769.396
[60,     1] loss: 683.428
[61,     1] loss: 711.365
[62,     1] loss: 638.174
[63,     1] loss: 711.347
[64,     1] loss: 625.276
[65,     1] loss: 608.779
[66,     1] loss: 707.999
[67,     1] loss: 686.209
[68,     1] loss: 590.350
[69,     1] loss: 608.567
[70,     1] loss: 539.948
[71,     1] loss: 533.527
[72,     1] loss: 697.921
[73,     1] loss: 2198.341
[74,     1] loss: 948.762
[75,     1] loss: 1067.229
[76,     1] loss: 948.683
[77,     1] loss: 1023.051
[78,     1] loss: 1067.126
[79,     1] loss: 1066.133
[80,     1] loss: 1052.918
[81,     1] loss: 1031.400
[82,     1] loss: 1054.451
[83,     1] loss: 982.813
[84,     1] loss: 953.773
[85,     1] loss: 962.569
[86,     1] loss: 937.803
[87,     1] loss: 949.769
[88,     1] loss: 922.486
[89,     1] loss: 937.530
[90,     1] loss: 906.074
[91,     1] loss: 903.852
[92,     1] loss: 898.580
[93,     1] loss: 883.880
[94,     1] loss: 910.779
[95,     1] loss: 887.545
[96,     1] loss: 895.961
[97,     1] loss: 879.890
[98,     1] loss: 825.921
[99,     1] loss: 819.077
[100,     1] loss: 815.595
[101,     1] loss: 840.104
[102,     1] loss: 853.230
[103,     1] loss: 813.518
[104,     1] loss: 797.194
[105,     1] loss: 796.601
[106,     1] loss: 798.144
[107,     1] loss: 744.021
[108,     1] loss: 724.593
[109,     1] loss: 836.347
[110,     1] loss: 835.774
[111,     1] loss: 682.858
[112,     1] loss: 787.642
[113,     1] loss: 722.943
[114,     1] loss: 653.301
[115,     1] loss: 766.773
[116,     1] loss: 665.763
[117,     1] loss: 611.997
[118,     1] loss: 826.089
[119,     1] loss: 1027.874
[120,     1] loss: 752.476
Early stopping applied (best metric=0.31354817748069763)
Finished Training
Total time taken: 19.901754140853882
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1235.481
[2,     1] loss: 1230.640
[3,     1] loss: 1229.513
[4,     1] loss: 1234.875
[5,     1] loss: 1226.175
[6,     1] loss: 1225.425
[7,     1] loss: 1222.797
[8,     1] loss: 1211.765
[9,     1] loss: 1198.896
[10,     1] loss: 1167.239
[11,     1] loss: 1114.791
[12,     1] loss: 1106.889
[13,     1] loss: 1044.685
[14,     1] loss: 1022.153
[15,     1] loss: 992.286
[16,     1] loss: 964.417
[17,     1] loss: 1059.705
[18,     1] loss: 1006.325
[19,     1] loss: 1021.125
[20,     1] loss: 990.524
[21,     1] loss: 1019.580
[22,     1] loss: 950.267
[23,     1] loss: 976.898
[24,     1] loss: 954.327
[25,     1] loss: 939.470
[26,     1] loss: 937.652
[27,     1] loss: 864.397
[28,     1] loss: 933.029
[29,     1] loss: 867.668
[30,     1] loss: 900.656
[31,     1] loss: 863.560
[32,     1] loss: 818.678
[33,     1] loss: 868.719
[34,     1] loss: 830.692
[35,     1] loss: 844.125
[36,     1] loss: 839.357
[37,     1] loss: 902.697
[38,     1] loss: 829.376
[39,     1] loss: 812.664
[40,     1] loss: 846.901
[41,     1] loss: 811.905
[42,     1] loss: 812.642
[43,     1] loss: 767.376
[44,     1] loss: 756.933
[45,     1] loss: 751.240
[46,     1] loss: 731.533
[47,     1] loss: 725.948
[48,     1] loss: 720.212
[49,     1] loss: 754.088
[50,     1] loss: 931.784
[51,     1] loss: 763.752
[52,     1] loss: 709.616
[53,     1] loss: 781.938
[54,     1] loss: 769.973
[55,     1] loss: 758.521
[56,     1] loss: 730.123
[57,     1] loss: 652.961
[58,     1] loss: 677.268
[59,     1] loss: 682.008
[60,     1] loss: 624.735
[61,     1] loss: 645.092
[62,     1] loss: 699.997
[63,     1] loss: 803.832
[64,     1] loss: 704.499
[65,     1] loss: 651.629
[66,     1] loss: 661.535
[67,     1] loss: 679.909
[68,     1] loss: 647.933
[69,     1] loss: 594.381
[70,     1] loss: 595.074
[71,     1] loss: 515.082
[72,     1] loss: 682.766
[73,     1] loss: 1406.034
[74,     1] loss: 577.795
[75,     1] loss: 1018.125
[76,     1] loss: 726.252
[77,     1] loss: 736.563
[78,     1] loss: 872.463
[79,     1] loss: 880.507
[80,     1] loss: 825.728
[81,     1] loss: 724.308
[82,     1] loss: 770.080
[83,     1] loss: 809.719
[84,     1] loss: 638.376
[85,     1] loss: 708.734
[86,     1] loss: 732.550
[87,     1] loss: 658.352
[88,     1] loss: 685.886
[89,     1] loss: 597.852
[90,     1] loss: 594.951
[91,     1] loss: 565.294
[92,     1] loss: 529.804
[93,     1] loss: 578.786
[94,     1] loss: 553.333
[95,     1] loss: 494.651
[96,     1] loss: 535.863
[97,     1] loss: 641.525
[98,     1] loss: 504.638
[99,     1] loss: 511.551
[100,     1] loss: 592.115
[101,     1] loss: 492.398
[102,     1] loss: 733.177
[103,     1] loss: 971.161
[104,     1] loss: 592.648
[105,     1] loss: 747.876
[106,     1] loss: 647.767
[107,     1] loss: 598.375
[108,     1] loss: 583.594
[109,     1] loss: 503.169
[110,     1] loss: 639.844
[111,     1] loss: 591.915
[112,     1] loss: 512.563
[113,     1] loss: 641.051
[114,     1] loss: 494.291
[115,     1] loss: 521.442
Early stopping applied (best metric=0.3805489242076874)
Finished Training
Total time taken: 17.914211750030518
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.019
[2,     1] loss: 1227.521
[3,     1] loss: 1226.388
[4,     1] loss: 1235.610
[5,     1] loss: 1225.361
[6,     1] loss: 1225.371
[7,     1] loss: 1226.714
[8,     1] loss: 1220.901
[9,     1] loss: 1209.732
[10,     1] loss: 1194.291
[11,     1] loss: 1165.913
[12,     1] loss: 1138.321
[13,     1] loss: 1088.956
[14,     1] loss: 1052.635
[15,     1] loss: 1053.475
[16,     1] loss: 988.939
[17,     1] loss: 1013.260
[18,     1] loss: 1019.321
[19,     1] loss: 933.102
[20,     1] loss: 956.463
[21,     1] loss: 949.224
[22,     1] loss: 950.078
[23,     1] loss: 970.436
[24,     1] loss: 948.592
[25,     1] loss: 903.000
[26,     1] loss: 872.958
[27,     1] loss: 947.674
[28,     1] loss: 903.760
[29,     1] loss: 937.536
[30,     1] loss: 865.764
[31,     1] loss: 865.391
[32,     1] loss: 867.919
[33,     1] loss: 850.265
[34,     1] loss: 892.791
[35,     1] loss: 835.948
[36,     1] loss: 792.603
[37,     1] loss: 833.006
[38,     1] loss: 849.814
[39,     1] loss: 793.421
[40,     1] loss: 787.298
[41,     1] loss: 717.945
[42,     1] loss: 767.911
[43,     1] loss: 781.085
[44,     1] loss: 681.021
[45,     1] loss: 725.137
[46,     1] loss: 777.959
[47,     1] loss: 873.769
[48,     1] loss: 1334.682
[49,     1] loss: 739.798
[50,     1] loss: 903.366
[51,     1] loss: 952.056
[52,     1] loss: 915.719
[53,     1] loss: 877.707
[54,     1] loss: 890.888
[55,     1] loss: 828.844
[56,     1] loss: 838.977
[57,     1] loss: 832.821
[58,     1] loss: 841.831
[59,     1] loss: 823.670
[60,     1] loss: 772.720
[61,     1] loss: 792.618
[62,     1] loss: 747.014
[63,     1] loss: 744.023
[64,     1] loss: 745.059
[65,     1] loss: 714.277
[66,     1] loss: 725.846
[67,     1] loss: 649.321
[68,     1] loss: 690.431
[69,     1] loss: 627.024
[70,     1] loss: 703.670
[71,     1] loss: 625.092
[72,     1] loss: 633.624
[73,     1] loss: 593.765
[74,     1] loss: 604.975
[75,     1] loss: 646.736
[76,     1] loss: 704.481
[77,     1] loss: 687.162
[78,     1] loss: 580.498
[79,     1] loss: 603.352
[80,     1] loss: 625.378
[81,     1] loss: 545.993
[82,     1] loss: 594.276
[83,     1] loss: 559.925
[84,     1] loss: 621.980
[85,     1] loss: 840.292
[86,     1] loss: 653.454
Early stopping applied (best metric=0.3610062599182129)
Finished Training
Total time taken: 11.611629724502563
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.282
[2,     1] loss: 1230.666
[3,     1] loss: 1229.328
[4,     1] loss: 1225.761
[5,     1] loss: 1216.805
[6,     1] loss: 1202.168
[7,     1] loss: 1171.128
[8,     1] loss: 1140.757
[9,     1] loss: 1092.193
[10,     1] loss: 1117.602
[11,     1] loss: 1128.469
[12,     1] loss: 1032.616
[13,     1] loss: 1112.469
[14,     1] loss: 997.981
[15,     1] loss: 1065.383
[16,     1] loss: 1031.117
[17,     1] loss: 1007.162
[18,     1] loss: 999.705
[19,     1] loss: 1011.842
[20,     1] loss: 983.466
[21,     1] loss: 986.396
[22,     1] loss: 933.185
[23,     1] loss: 970.225
[24,     1] loss: 994.029
[25,     1] loss: 966.586
[26,     1] loss: 911.226
[27,     1] loss: 920.436
[28,     1] loss: 933.187
[29,     1] loss: 922.010
[30,     1] loss: 950.504
[31,     1] loss: 909.665
[32,     1] loss: 928.723
[33,     1] loss: 888.344
[34,     1] loss: 840.865
[35,     1] loss: 882.108
[36,     1] loss: 926.266
[37,     1] loss: 832.738
[38,     1] loss: 899.434
[39,     1] loss: 848.848
[40,     1] loss: 805.207
[41,     1] loss: 848.477
[42,     1] loss: 806.542
[43,     1] loss: 839.210
[44,     1] loss: 846.875
[45,     1] loss: 785.707
[46,     1] loss: 759.507
[47,     1] loss: 815.456
[48,     1] loss: 795.470
[49,     1] loss: 704.235
[50,     1] loss: 797.472
[51,     1] loss: 828.922
[52,     1] loss: 692.442
[53,     1] loss: 847.049
[54,     1] loss: 744.963
[55,     1] loss: 728.313
[56,     1] loss: 679.364
[57,     1] loss: 724.978
[58,     1] loss: 650.086
[59,     1] loss: 660.509
[60,     1] loss: 674.164
[61,     1] loss: 776.422
[62,     1] loss: 880.009
[63,     1] loss: 696.432
[64,     1] loss: 779.038
[65,     1] loss: 665.443
[66,     1] loss: 775.421
[67,     1] loss: 671.938
[68,     1] loss: 768.342
[69,     1] loss: 660.125
[70,     1] loss: 670.285
[71,     1] loss: 620.747
[72,     1] loss: 622.862
[73,     1] loss: 604.239
[74,     1] loss: 592.935
[75,     1] loss: 534.657
[76,     1] loss: 565.572
[77,     1] loss: 681.378
[78,     1] loss: 798.854
[79,     1] loss: 522.850
[80,     1] loss: 639.723
[81,     1] loss: 504.282
[82,     1] loss: 667.920
[83,     1] loss: 591.984
[84,     1] loss: 558.979
[85,     1] loss: 498.120
[86,     1] loss: 447.306
[87,     1] loss: 609.993
[88,     1] loss: 903.964
[89,     1] loss: 514.003
[90,     1] loss: 449.258
[91,     1] loss: 526.745
[92,     1] loss: 483.974
[93,     1] loss: 524.131
[94,     1] loss: 473.285
[95,     1] loss: 489.138
[96,     1] loss: 436.522
[97,     1] loss: 402.790
[98,     1] loss: 370.921
[99,     1] loss: 426.038
[100,     1] loss: 802.478
[101,     1] loss: 1391.245
Early stopping applied (best metric=0.39079564809799194)
Finished Training
Total time taken: 15.211207151412964
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.745
[2,     1] loss: 1228.316
[3,     1] loss: 1232.948
[4,     1] loss: 1226.049
[5,     1] loss: 1228.433
[6,     1] loss: 1217.365
[7,     1] loss: 1219.674
[8,     1] loss: 1200.615
[9,     1] loss: 1167.852
[10,     1] loss: 1129.234
[11,     1] loss: 1094.172
[12,     1] loss: 1046.097
[13,     1] loss: 1026.300
[14,     1] loss: 1024.076
[15,     1] loss: 1027.574
[16,     1] loss: 957.177
[17,     1] loss: 1056.541
[18,     1] loss: 989.878
[19,     1] loss: 984.368
[20,     1] loss: 981.630
[21,     1] loss: 974.079
[22,     1] loss: 937.772
[23,     1] loss: 927.865
[24,     1] loss: 946.868
[25,     1] loss: 904.238
[26,     1] loss: 907.900
[27,     1] loss: 868.393
[28,     1] loss: 852.065
[29,     1] loss: 845.057
[30,     1] loss: 867.439
[31,     1] loss: 846.229
[32,     1] loss: 849.029
[33,     1] loss: 813.692
[34,     1] loss: 825.556
[35,     1] loss: 861.359
[36,     1] loss: 891.482
[37,     1] loss: 782.217
[38,     1] loss: 848.799
[39,     1] loss: 814.178
[40,     1] loss: 747.874
[41,     1] loss: 810.564
[42,     1] loss: 795.785
[43,     1] loss: 704.529
[44,     1] loss: 753.934
[45,     1] loss: 841.172
[46,     1] loss: 711.519
[47,     1] loss: 818.796
[48,     1] loss: 734.338
[49,     1] loss: 765.172
[50,     1] loss: 737.528
[51,     1] loss: 742.787
[52,     1] loss: 725.178
[53,     1] loss: 702.633
[54,     1] loss: 707.349
[55,     1] loss: 651.913
[56,     1] loss: 654.903
[57,     1] loss: 636.089
[58,     1] loss: 579.239
[59,     1] loss: 599.957
[60,     1] loss: 795.133
[61,     1] loss: 1182.443
[62,     1] loss: 631.267
[63,     1] loss: 915.647
[64,     1] loss: 781.278
[65,     1] loss: 793.369
[66,     1] loss: 834.886
[67,     1] loss: 795.424
[68,     1] loss: 723.932
[69,     1] loss: 772.447
[70,     1] loss: 750.407
[71,     1] loss: 710.434
[72,     1] loss: 646.739
[73,     1] loss: 611.103
[74,     1] loss: 665.721
[75,     1] loss: 629.223
[76,     1] loss: 592.251
[77,     1] loss: 576.835
[78,     1] loss: 537.103
[79,     1] loss: 579.395
[80,     1] loss: 472.355
[81,     1] loss: 538.381
[82,     1] loss: 526.155
[83,     1] loss: 571.683
Early stopping applied (best metric=0.36311331391334534)
Finished Training
Total time taken: 13.130153179168701
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.776
[2,     1] loss: 1236.268
[3,     1] loss: 1232.071
[4,     1] loss: 1230.178
[5,     1] loss: 1229.879
[6,     1] loss: 1229.736
[7,     1] loss: 1226.922
[8,     1] loss: 1226.056
[9,     1] loss: 1222.662
[10,     1] loss: 1216.171
[11,     1] loss: 1213.804
[12,     1] loss: 1201.873
[13,     1] loss: 1179.025
[14,     1] loss: 1164.135
[15,     1] loss: 1129.933
[16,     1] loss: 1115.917
[17,     1] loss: 1104.449
[18,     1] loss: 1082.834
[19,     1] loss: 1057.829
[20,     1] loss: 1045.793
[21,     1] loss: 1020.706
[22,     1] loss: 1027.951
[23,     1] loss: 1045.042
[24,     1] loss: 979.600
[25,     1] loss: 1061.867
[26,     1] loss: 998.608
[27,     1] loss: 1027.417
[28,     1] loss: 948.036
[29,     1] loss: 953.639
[30,     1] loss: 922.377
[31,     1] loss: 931.714
[32,     1] loss: 943.458
[33,     1] loss: 919.735
[34,     1] loss: 924.280
[35,     1] loss: 891.040
[36,     1] loss: 916.438
[37,     1] loss: 853.298
[38,     1] loss: 881.076
[39,     1] loss: 887.039
[40,     1] loss: 850.822
[41,     1] loss: 835.971
[42,     1] loss: 865.638
[43,     1] loss: 887.022
[44,     1] loss: 805.244
[45,     1] loss: 874.470
[46,     1] loss: 813.039
[47,     1] loss: 796.217
[48,     1] loss: 782.441
[49,     1] loss: 785.448
[50,     1] loss: 762.430
[51,     1] loss: 725.456
[52,     1] loss: 757.703
[53,     1] loss: 748.806
[54,     1] loss: 726.425
[55,     1] loss: 750.638
[56,     1] loss: 709.181
[57,     1] loss: 864.831
[58,     1] loss: 1181.604
[59,     1] loss: 781.510
[60,     1] loss: 915.881
[61,     1] loss: 902.989
[62,     1] loss: 871.293
[63,     1] loss: 900.248
[64,     1] loss: 885.897
[65,     1] loss: 812.707
[66,     1] loss: 791.617
[67,     1] loss: 809.794
[68,     1] loss: 751.516
[69,     1] loss: 850.846
[70,     1] loss: 760.870
[71,     1] loss: 772.447
[72,     1] loss: 729.145
[73,     1] loss: 711.258
[74,     1] loss: 757.850
[75,     1] loss: 679.518
[76,     1] loss: 689.758
[77,     1] loss: 637.038
[78,     1] loss: 625.785
[79,     1] loss: 578.482
[80,     1] loss: 584.359
[81,     1] loss: 614.771
[82,     1] loss: 1074.565
[83,     1] loss: 1312.468
[84,     1] loss: 740.868
[85,     1] loss: 868.509
[86,     1] loss: 1000.720
[87,     1] loss: 981.031
[88,     1] loss: 931.466
[89,     1] loss: 905.180
[90,     1] loss: 887.087
[91,     1] loss: 879.402
[92,     1] loss: 832.543
[93,     1] loss: 818.628
[94,     1] loss: 896.482
[95,     1] loss: 829.222
[96,     1] loss: 795.433
[97,     1] loss: 797.640
[98,     1] loss: 759.549
[99,     1] loss: 777.749
[100,     1] loss: 778.186
[101,     1] loss: 753.798
[102,     1] loss: 739.102
[103,     1] loss: 703.427
[104,     1] loss: 726.907
[105,     1] loss: 652.042
[106,     1] loss: 656.714
[107,     1] loss: 753.351
[108,     1] loss: 838.728
[109,     1] loss: 740.106
[110,     1] loss: 656.895
[111,     1] loss: 690.315
[112,     1] loss: 641.956
[113,     1] loss: 706.411
[114,     1] loss: 575.538
[115,     1] loss: 577.642
Early stopping applied (best metric=0.35875675082206726)
Finished Training
Total time taken: 19.44629192352295
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1235.952
[2,     1] loss: 1234.683
[3,     1] loss: 1249.677
[4,     1] loss: 1231.500
[5,     1] loss: 1230.477
[6,     1] loss: 1229.920
[7,     1] loss: 1230.706
[8,     1] loss: 1231.007
[9,     1] loss: 1227.577
[10,     1] loss: 1225.328
[11,     1] loss: 1219.933
[12,     1] loss: 1211.587
[13,     1] loss: 1200.930
[14,     1] loss: 1178.781
[15,     1] loss: 1158.608
[16,     1] loss: 1122.392
[17,     1] loss: 1083.849
[18,     1] loss: 1091.415
[19,     1] loss: 1095.474
[20,     1] loss: 1066.229
[21,     1] loss: 1051.944
[22,     1] loss: 1041.113
[23,     1] loss: 992.057
[24,     1] loss: 960.430
[25,     1] loss: 1029.807
[26,     1] loss: 1025.811
[27,     1] loss: 981.862
[28,     1] loss: 980.441
[29,     1] loss: 963.442
[30,     1] loss: 998.308
[31,     1] loss: 933.381
[32,     1] loss: 920.430
[33,     1] loss: 884.540
[34,     1] loss: 919.282
[35,     1] loss: 934.802
[36,     1] loss: 886.931
[37,     1] loss: 902.041
[38,     1] loss: 976.262
[39,     1] loss: 973.431
[40,     1] loss: 900.838
[41,     1] loss: 902.260
[42,     1] loss: 884.641
[43,     1] loss: 863.351
[44,     1] loss: 918.239
[45,     1] loss: 837.534
[46,     1] loss: 897.448
[47,     1] loss: 811.564
[48,     1] loss: 837.971
[49,     1] loss: 825.830
[50,     1] loss: 882.342
[51,     1] loss: 900.619
[52,     1] loss: 807.689
[53,     1] loss: 847.568
[54,     1] loss: 753.060
[55,     1] loss: 830.020
[56,     1] loss: 781.646
[57,     1] loss: 771.185
[58,     1] loss: 731.656
[59,     1] loss: 793.541
[60,     1] loss: 811.547
[61,     1] loss: 687.776
[62,     1] loss: 837.602
[63,     1] loss: 727.274
[64,     1] loss: 780.659
[65,     1] loss: 692.162
[66,     1] loss: 808.567
[67,     1] loss: 694.449
[68,     1] loss: 718.582
[69,     1] loss: 679.297
[70,     1] loss: 690.337
[71,     1] loss: 713.171
[72,     1] loss: 636.058
[73,     1] loss: 804.796
[74,     1] loss: 965.887
[75,     1] loss: 684.858
[76,     1] loss: 761.105
[77,     1] loss: 787.849
[78,     1] loss: 689.137
[79,     1] loss: 782.384
[80,     1] loss: 677.294
[81,     1] loss: 678.059
[82,     1] loss: 706.087
[83,     1] loss: 660.758
[84,     1] loss: 655.046
[85,     1] loss: 658.779
[86,     1] loss: 558.404
[87,     1] loss: 566.483
[88,     1] loss: 547.871
[89,     1] loss: 517.251
[90,     1] loss: 517.880
[91,     1] loss: 520.823
[92,     1] loss: 848.319
[93,     1] loss: 1063.970
[94,     1] loss: 726.025
[95,     1] loss: 879.984
[96,     1] loss: 779.738
[97,     1] loss: 736.603
[98,     1] loss: 810.045
[99,     1] loss: 696.658
[100,     1] loss: 789.957
[101,     1] loss: 684.663
[102,     1] loss: 677.724
[103,     1] loss: 669.255
[104,     1] loss: 665.729
[105,     1] loss: 634.339
[106,     1] loss: 610.136
[107,     1] loss: 553.825
[108,     1] loss: 548.073
[109,     1] loss: 514.728
[110,     1] loss: 697.010
[111,     1] loss: 710.939
[112,     1] loss: 651.794
[113,     1] loss: 594.574
[114,     1] loss: 630.867
[115,     1] loss: 533.410
[116,     1] loss: 558.512
[117,     1] loss: 533.650
[118,     1] loss: 424.003
[119,     1] loss: 466.594
[120,     1] loss: 928.702
[121,     1] loss: 1776.464
[122,     1] loss: 973.895
[123,     1] loss: 1053.381
[124,     1] loss: 1152.333
[125,     1] loss: 1166.224
[126,     1] loss: 1178.339
[127,     1] loss: 1149.472
[128,     1] loss: 1080.744
[129,     1] loss: 1053.523
Early stopping applied (best metric=0.2748088240623474)
Finished Training
Total time taken: 21.65443444252014
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.888
[2,     1] loss: 1227.173
[3,     1] loss: 1231.538
[4,     1] loss: 1227.924
[5,     1] loss: 1229.174
[6,     1] loss: 1229.870
[7,     1] loss: 1225.932
[8,     1] loss: 1222.677
[9,     1] loss: 1219.423
[10,     1] loss: 1209.045
[11,     1] loss: 1196.971
[12,     1] loss: 1159.373
[13,     1] loss: 1136.112
[14,     1] loss: 1118.290
[15,     1] loss: 1070.157
[16,     1] loss: 1064.234
[17,     1] loss: 1041.947
[18,     1] loss: 1055.498
[19,     1] loss: 1004.746
[20,     1] loss: 978.114
[21,     1] loss: 1001.544
[22,     1] loss: 970.418
[23,     1] loss: 949.891
[24,     1] loss: 985.431
[25,     1] loss: 933.524
[26,     1] loss: 1003.271
[27,     1] loss: 942.484
[28,     1] loss: 957.515
[29,     1] loss: 929.602
[30,     1] loss: 945.247
[31,     1] loss: 881.339
[32,     1] loss: 935.671
[33,     1] loss: 943.759
[34,     1] loss: 920.327
[35,     1] loss: 942.430
[36,     1] loss: 898.602
[37,     1] loss: 895.467
[38,     1] loss: 835.139
[39,     1] loss: 878.430
[40,     1] loss: 848.701
[41,     1] loss: 897.695
[42,     1] loss: 812.517
[43,     1] loss: 845.719
[44,     1] loss: 849.105
[45,     1] loss: 818.821
[46,     1] loss: 865.020
[47,     1] loss: 845.789
[48,     1] loss: 797.413
[49,     1] loss: 875.748
[50,     1] loss: 775.661
[51,     1] loss: 839.826
[52,     1] loss: 769.026
[53,     1] loss: 757.890
[54,     1] loss: 794.435
[55,     1] loss: 692.014
[56,     1] loss: 757.049
[57,     1] loss: 790.542
[58,     1] loss: 704.864
[59,     1] loss: 677.386
[60,     1] loss: 676.678
[61,     1] loss: 637.140
[62,     1] loss: 635.464
[63,     1] loss: 645.952
[64,     1] loss: 800.748
[65,     1] loss: 1238.974
[66,     1] loss: 711.429
[67,     1] loss: 936.104
[68,     1] loss: 906.698
[69,     1] loss: 912.843
[70,     1] loss: 905.838
[71,     1] loss: 858.793
[72,     1] loss: 771.038
[73,     1] loss: 768.006
[74,     1] loss: 795.513
[75,     1] loss: 792.173
[76,     1] loss: 748.580
[77,     1] loss: 758.356
[78,     1] loss: 725.170
[79,     1] loss: 734.786
[80,     1] loss: 678.802
[81,     1] loss: 681.019
[82,     1] loss: 639.510
[83,     1] loss: 621.754
[84,     1] loss: 646.452
[85,     1] loss: 650.923
[86,     1] loss: 679.325
[87,     1] loss: 793.511
[88,     1] loss: 721.398
[89,     1] loss: 611.139
[90,     1] loss: 694.791
[91,     1] loss: 626.430
[92,     1] loss: 661.818
[93,     1] loss: 576.539
[94,     1] loss: 599.485
[95,     1] loss: 541.062
[96,     1] loss: 491.662
[97,     1] loss: 589.376
[98,     1] loss: 815.962
[99,     1] loss: 751.409
[100,     1] loss: 540.255
[101,     1] loss: 638.771
[102,     1] loss: 564.242
[103,     1] loss: 638.816
[104,     1] loss: 534.488
[105,     1] loss: 542.148
[106,     1] loss: 531.290
[107,     1] loss: 584.363
[108,     1] loss: 456.822
[109,     1] loss: 438.326
[110,     1] loss: 535.544
[111,     1] loss: 467.547
[112,     1] loss: 456.298
[113,     1] loss: 509.459
[114,     1] loss: 475.760
[115,     1] loss: 399.536
[116,     1] loss: 369.044
[117,     1] loss: 398.771
[118,     1] loss: 627.438
[119,     1] loss: 2241.210
[120,     1] loss: 799.697
[121,     1] loss: 1036.400
[122,     1] loss: 1087.340
[123,     1] loss: 1084.489
[124,     1] loss: 1087.237
[125,     1] loss: 1058.350
[126,     1] loss: 1026.715
Early stopping applied (best metric=0.30381837487220764)
Finished Training
Total time taken: 21.624000787734985
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.214
[2,     1] loss: 1227.440
[3,     1] loss: 1226.754
[4,     1] loss: 1230.094
[5,     1] loss: 1228.982
[6,     1] loss: 1225.660
[7,     1] loss: 1222.098
[8,     1] loss: 1212.284
[9,     1] loss: 1194.191
[10,     1] loss: 1179.682
[11,     1] loss: 1152.424
[12,     1] loss: 1089.682
[13,     1] loss: 1073.583
[14,     1] loss: 1050.232
[15,     1] loss: 1076.475
[16,     1] loss: 1052.727
[17,     1] loss: 1046.964
[18,     1] loss: 1065.836
[19,     1] loss: 1033.899
[20,     1] loss: 1009.731
[21,     1] loss: 1042.169
[22,     1] loss: 1014.166
[23,     1] loss: 994.596
[24,     1] loss: 970.987
[25,     1] loss: 970.406
[26,     1] loss: 942.049
[27,     1] loss: 936.239
[28,     1] loss: 905.220
[29,     1] loss: 950.802
[30,     1] loss: 900.611
[31,     1] loss: 912.360
[32,     1] loss: 900.764
[33,     1] loss: 897.416
[34,     1] loss: 861.629
[35,     1] loss: 887.303
[36,     1] loss: 808.803
[37,     1] loss: 826.362
[38,     1] loss: 843.197
[39,     1] loss: 977.339
[40,     1] loss: 1058.178
[41,     1] loss: 831.761
[42,     1] loss: 914.578
[43,     1] loss: 946.065
[44,     1] loss: 897.674
[45,     1] loss: 879.434
[46,     1] loss: 901.946
[47,     1] loss: 850.052
[48,     1] loss: 843.195
[49,     1] loss: 927.990
[50,     1] loss: 793.102
[51,     1] loss: 866.627
[52,     1] loss: 816.603
[53,     1] loss: 798.237
[54,     1] loss: 815.079
[55,     1] loss: 775.317
[56,     1] loss: 812.878
[57,     1] loss: 782.817
[58,     1] loss: 697.858
[59,     1] loss: 809.027
[60,     1] loss: 769.864
[61,     1] loss: 764.655
[62,     1] loss: 777.075
[63,     1] loss: 699.915
[64,     1] loss: 712.770
[65,     1] loss: 711.252
[66,     1] loss: 659.347
[67,     1] loss: 616.317
[68,     1] loss: 680.077
[69,     1] loss: 667.437
[70,     1] loss: 788.055
[71,     1] loss: 749.667
[72,     1] loss: 653.607
[73,     1] loss: 639.706
[74,     1] loss: 678.108
[75,     1] loss: 580.230
[76,     1] loss: 623.058
[77,     1] loss: 595.617
[78,     1] loss: 576.945
[79,     1] loss: 685.322
[80,     1] loss: 1135.158
[81,     1] loss: 1363.753
[82,     1] loss: 1081.595
[83,     1] loss: 917.476
[84,     1] loss: 967.851
[85,     1] loss: 1023.339
[86,     1] loss: 1161.654
[87,     1] loss: 1064.360
[88,     1] loss: 1066.111
[89,     1] loss: 1055.827
[90,     1] loss: 1045.265
[91,     1] loss: 1017.712
[92,     1] loss: 973.425
[93,     1] loss: 942.240
[94,     1] loss: 902.757
[95,     1] loss: 953.966
[96,     1] loss: 898.397
[97,     1] loss: 933.929
[98,     1] loss: 939.307
[99,     1] loss: 945.257
[100,     1] loss: 902.549
[101,     1] loss: 903.183
[102,     1] loss: 916.632
[103,     1] loss: 906.711
[104,     1] loss: 887.134
[105,     1] loss: 873.350
[106,     1] loss: 866.207
[107,     1] loss: 855.735
[108,     1] loss: 867.932
[109,     1] loss: 795.731
[110,     1] loss: 764.412
[111,     1] loss: 839.289
[112,     1] loss: 787.437
[113,     1] loss: 769.629
[114,     1] loss: 790.638
[115,     1] loss: 746.036
[116,     1] loss: 778.559
[117,     1] loss: 805.151
Early stopping applied (best metric=0.3297629654407501)
Finished Training
Total time taken: 18.220888137817383
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.166
[2,     1] loss: 1228.808
[3,     1] loss: 1227.839
[4,     1] loss: 1229.581
[5,     1] loss: 1226.613
[6,     1] loss: 1231.085
[7,     1] loss: 1224.669
[8,     1] loss: 1222.235
[9,     1] loss: 1218.727
[10,     1] loss: 1204.420
[11,     1] loss: 1179.216
[12,     1] loss: 1152.118
[13,     1] loss: 1119.951
[14,     1] loss: 1077.605
[15,     1] loss: 1040.933
[16,     1] loss: 1122.608
[17,     1] loss: 1095.293
[18,     1] loss: 1017.000
[19,     1] loss: 996.992
[20,     1] loss: 996.452
[21,     1] loss: 956.965
[22,     1] loss: 973.014
[23,     1] loss: 978.935
[24,     1] loss: 949.391
[25,     1] loss: 947.471
[26,     1] loss: 921.761
[27,     1] loss: 906.600
[28,     1] loss: 882.688
[29,     1] loss: 867.388
[30,     1] loss: 842.977
[31,     1] loss: 884.250
[32,     1] loss: 901.874
[33,     1] loss: 891.852
[34,     1] loss: 863.899
[35,     1] loss: 830.107
[36,     1] loss: 897.308
[37,     1] loss: 792.083
[38,     1] loss: 878.858
[39,     1] loss: 861.271
[40,     1] loss: 833.261
[41,     1] loss: 830.438
[42,     1] loss: 746.274
[43,     1] loss: 793.148
[44,     1] loss: 708.760
[45,     1] loss: 774.764
[46,     1] loss: 903.131
[47,     1] loss: 801.822
[48,     1] loss: 695.191
[49,     1] loss: 774.350
[50,     1] loss: 700.471
[51,     1] loss: 727.851
[52,     1] loss: 703.459
[53,     1] loss: 682.845
[54,     1] loss: 597.529
[55,     1] loss: 699.376
[56,     1] loss: 743.648
[57,     1] loss: 843.654
[58,     1] loss: 663.326
[59,     1] loss: 682.100
[60,     1] loss: 655.866
[61,     1] loss: 724.155
[62,     1] loss: 617.280
[63,     1] loss: 701.640
[64,     1] loss: 594.332
[65,     1] loss: 696.168
[66,     1] loss: 577.117
[67,     1] loss: 569.250
[68,     1] loss: 675.162
[69,     1] loss: 506.232
[70,     1] loss: 631.159
[71,     1] loss: 584.017
[72,     1] loss: 514.210
[73,     1] loss: 495.769
[74,     1] loss: 515.618
[75,     1] loss: 457.261
[76,     1] loss: 545.180
[77,     1] loss: 663.381
[78,     1] loss: 655.913
[79,     1] loss: 564.154
[80,     1] loss: 563.995
[81,     1] loss: 560.358
[82,     1] loss: 508.063
[83,     1] loss: 516.952
[84,     1] loss: 497.281
[85,     1] loss: 552.134
[86,     1] loss: 492.671
Early stopping applied (best metric=0.37404048442840576)
Finished Training
Total time taken: 13.432788133621216
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.374
[2,     1] loss: 1235.037
[3,     1] loss: 1229.821
[4,     1] loss: 1226.484
[5,     1] loss: 1225.914
[6,     1] loss: 1223.184
[7,     1] loss: 1221.824
[8,     1] loss: 1214.479
[9,     1] loss: 1204.871
[10,     1] loss: 1179.350
[11,     1] loss: 1149.562
[12,     1] loss: 1129.497
[13,     1] loss: 1073.864
[14,     1] loss: 1052.976
[15,     1] loss: 1050.948
[16,     1] loss: 1041.760
[17,     1] loss: 1046.234
[18,     1] loss: 1024.101
[19,     1] loss: 965.712
[20,     1] loss: 1030.789
[21,     1] loss: 985.345
[22,     1] loss: 985.827
[23,     1] loss: 943.443
[24,     1] loss: 958.865
[25,     1] loss: 939.580
[26,     1] loss: 996.267
[27,     1] loss: 910.221
[28,     1] loss: 896.324
[29,     1] loss: 938.340
[30,     1] loss: 911.790
[31,     1] loss: 964.297
[32,     1] loss: 882.365
[33,     1] loss: 947.550
[34,     1] loss: 831.894
[35,     1] loss: 904.307
[36,     1] loss: 829.009
[37,     1] loss: 897.646
[38,     1] loss: 816.548
[39,     1] loss: 894.887
[40,     1] loss: 826.176
[41,     1] loss: 818.385
[42,     1] loss: 767.118
[43,     1] loss: 796.868
[44,     1] loss: 709.157
[45,     1] loss: 752.260
[46,     1] loss: 840.736
[47,     1] loss: 752.281
[48,     1] loss: 735.258
[49,     1] loss: 749.358
[50,     1] loss: 778.715
[51,     1] loss: 756.622
[52,     1] loss: 724.834
[53,     1] loss: 757.060
[54,     1] loss: 806.482
[55,     1] loss: 674.277
[56,     1] loss: 676.088
[57,     1] loss: 743.763
[58,     1] loss: 683.599
[59,     1] loss: 685.414
[60,     1] loss: 681.533
[61,     1] loss: 638.572
[62,     1] loss: 638.781
[63,     1] loss: 625.277
[64,     1] loss: 659.137
[65,     1] loss: 981.159
[66,     1] loss: 570.858
[67,     1] loss: 948.115
[68,     1] loss: 744.044
[69,     1] loss: 924.899
[70,     1] loss: 833.212
[71,     1] loss: 748.682
[72,     1] loss: 863.158
[73,     1] loss: 721.563
[74,     1] loss: 637.036
[75,     1] loss: 805.752
[76,     1] loss: 659.572
[77,     1] loss: 621.629
[78,     1] loss: 632.852
[79,     1] loss: 569.688
[80,     1] loss: 634.798
[81,     1] loss: 580.182
[82,     1] loss: 536.133
[83,     1] loss: 651.067
[84,     1] loss: 492.089
[85,     1] loss: 605.847
Early stopping applied (best metric=0.34377285838127136)
Finished Training
Total time taken: 13.395268440246582
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1230.794
[2,     1] loss: 1227.802
[3,     1] loss: 1234.338
[4,     1] loss: 1229.878
[5,     1] loss: 1229.314
[6,     1] loss: 1226.077
[7,     1] loss: 1222.362
[8,     1] loss: 1221.126
[9,     1] loss: 1204.354
[10,     1] loss: 1186.927
[11,     1] loss: 1155.194
[12,     1] loss: 1126.268
[13,     1] loss: 1057.472
[14,     1] loss: 1037.628
[15,     1] loss: 1116.061
[16,     1] loss: 1102.892
[17,     1] loss: 1009.726
[18,     1] loss: 990.662
[19,     1] loss: 1003.386
[20,     1] loss: 968.516
[21,     1] loss: 925.325
[22,     1] loss: 938.945
[23,     1] loss: 936.593
[24,     1] loss: 948.718
[25,     1] loss: 886.230
[26,     1] loss: 888.283
[27,     1] loss: 941.854
[28,     1] loss: 885.214
[29,     1] loss: 878.302
[30,     1] loss: 883.424
[31,     1] loss: 870.547
[32,     1] loss: 888.723
[33,     1] loss: 866.301
[34,     1] loss: 821.369
[35,     1] loss: 835.918
[36,     1] loss: 803.444
[37,     1] loss: 774.129
[38,     1] loss: 772.057
[39,     1] loss: 783.043
[40,     1] loss: 768.970
[41,     1] loss: 770.261
[42,     1] loss: 752.347
[43,     1] loss: 734.123
[44,     1] loss: 703.589
[45,     1] loss: 743.948
[46,     1] loss: 960.476
[47,     1] loss: 1213.251
[48,     1] loss: 800.578
[49,     1] loss: 957.981
[50,     1] loss: 993.873
[51,     1] loss: 987.927
[52,     1] loss: 955.792
[53,     1] loss: 912.220
[54,     1] loss: 898.746
[55,     1] loss: 863.046
[56,     1] loss: 826.250
[57,     1] loss: 821.370
[58,     1] loss: 823.962
[59,     1] loss: 794.680
[60,     1] loss: 883.133
[61,     1] loss: 770.372
[62,     1] loss: 762.372
[63,     1] loss: 794.682
[64,     1] loss: 742.146
[65,     1] loss: 760.328
[66,     1] loss: 751.716
[67,     1] loss: 750.330
[68,     1] loss: 706.142
[69,     1] loss: 707.655
[70,     1] loss: 680.826
[71,     1] loss: 651.075
[72,     1] loss: 649.988
[73,     1] loss: 615.175
[74,     1] loss: 640.204
[75,     1] loss: 812.833
[76,     1] loss: 978.525
[77,     1] loss: 684.732
[78,     1] loss: 841.036
[79,     1] loss: 711.042
[80,     1] loss: 739.767
[81,     1] loss: 770.150
[82,     1] loss: 627.163
[83,     1] loss: 719.732
[84,     1] loss: 564.308
[85,     1] loss: 692.291
[86,     1] loss: 684.588
[87,     1] loss: 650.922
[88,     1] loss: 630.226
[89,     1] loss: 630.698
[90,     1] loss: 579.866
[91,     1] loss: 589.328
[92,     1] loss: 587.528
[93,     1] loss: 523.410
[94,     1] loss: 445.542
[95,     1] loss: 483.922
[96,     1] loss: 515.071
[97,     1] loss: 719.755
[98,     1] loss: 1270.908
[99,     1] loss: 649.719
[100,     1] loss: 835.311
[101,     1] loss: 920.322
[102,     1] loss: 837.981
[103,     1] loss: 816.371
[104,     1] loss: 776.048
[105,     1] loss: 744.813
[106,     1] loss: 750.614
[107,     1] loss: 710.266
[108,     1] loss: 703.027
[109,     1] loss: 661.967
[110,     1] loss: 756.212
[111,     1] loss: 756.883
[112,     1] loss: 685.659
[113,     1] loss: 622.130
[114,     1] loss: 621.237
[115,     1] loss: 589.169
[116,     1] loss: 566.340
[117,     1] loss: 613.524
[118,     1] loss: 586.872
[119,     1] loss: 530.684
[120,     1] loss: 644.666
[121,     1] loss: 713.145
[122,     1] loss: 504.125
[123,     1] loss: 706.687
[124,     1] loss: 512.964
[125,     1] loss: 593.039
[126,     1] loss: 463.637
[127,     1] loss: 529.559
[128,     1] loss: 623.494
[129,     1] loss: 431.063
[130,     1] loss: 644.202
[131,     1] loss: 608.035
[132,     1] loss: 507.878
Early stopping applied (best metric=0.3714558482170105)
Finished Training
Total time taken: 20.559651374816895
{'Hydroxylation-K Validation Accuracy': 0.7170212765957447, 'Hydroxylation-K Validation Sensitivity': 0.5555555555555556, 'Hydroxylation-K Validation Specificity': 0.7578947368421053, 'Hydroxylation-K Validation Precision': 0.37133555986497163, 'Hydroxylation-K AUC ROC': 0.7500779727095517, 'Hydroxylation-K AUC PR': 0.5081403637386402, 'Hydroxylation-K MCC': 0.2754379516547903, 'Hydroxylation-K F1': 0.44210510234461414, 'Validation Loss (Hydroxylation-K)': 0.5257222493489583, 'Hydroxylation-P Validation Accuracy': 0.7985702756205268, 'Hydroxylation-P Validation Sensitivity': 0.8143386243386244, 'Hydroxylation-P Validation Specificity': 0.7951917801386603, 'Hydroxylation-P Validation Precision': 0.47036192073335553, 'Hydroxylation-P AUC ROC': 0.8695002244500972, 'Hydroxylation-P AUC PR': 0.6085757371953118, 'Hydroxylation-P MCC': 0.5071576499179309, 'Hydroxylation-P F1': 0.592657454203106, 'Validation Loss (Hydroxylation-P)': 0.34392495155334474, 'Validation Loss (total)': 0.8696472009023031, 'TimeToTrain': 16.239077059427895}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030673103541138955,
 'learning_rate_Hydroxylation-K': 0.001746536587822697,
 'learning_rate_Hydroxylation-P': 0.009539630257664945,
 'log_base': 2.9135408268612086,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3317721441,
 'sample_weights': [1.5212801934495765, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.523833686462496,
 'weight_decay_Hydroxylation-K': 4.049332810438257,
 'weight_decay_Hydroxylation-P': 0.15720988025877533}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.894
[2,     1] loss: 1238.209
[3,     1] loss: 1234.996
[4,     1] loss: 1242.678
[5,     1] loss: 1234.482
[6,     1] loss: 1230.869
[7,     1] loss: 1225.453
[8,     1] loss: 1215.172
[9,     1] loss: 1177.761
[10,     1] loss: 1166.450
[11,     1] loss: 1128.494
[12,     1] loss: 1081.690
[13,     1] loss: 1062.325
[14,     1] loss: 1069.996
[15,     1] loss: 1070.867
[16,     1] loss: 1021.024
[17,     1] loss: 1047.609
[18,     1] loss: 993.033
[19,     1] loss: 1013.297
[20,     1] loss: 1032.661
[21,     1] loss: 993.872
[22,     1] loss: 983.680
[23,     1] loss: 988.910
[24,     1] loss: 943.046
[25,     1] loss: 964.872
[26,     1] loss: 989.961
[27,     1] loss: 1018.410
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005865665511247905,
 'learning_rate_Hydroxylation-K': 0.0036845863330877537,
 'learning_rate_Hydroxylation-P': 0.0053672809347327256,
 'log_base': 2.8599800693358053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3909330980,
 'sample_weights': [1.5611477042149566, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.854753746568184,
 'weight_decay_Hydroxylation-K': 5.322802245340661,
 'weight_decay_Hydroxylation-P': 0.62970112666521}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.903
[2,     1] loss: 1255.072
[3,     1] loss: 1242.629
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007790703862025424,
 'learning_rate_Hydroxylation-K': 0.002466619447099288,
 'learning_rate_Hydroxylation-P': 0.004611493539452032,
 'log_base': 1.2336209974311498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3037791074,
 'sample_weights': [1.5887132310363654, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.875406338986291,
 'weight_decay_Hydroxylation-K': 7.34624665665151,
 'weight_decay_Hydroxylation-P': 4.1190032126315685}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2595.723
[2,     1] loss: 2595.129
[3,     1] loss: 2586.603
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066577754991096446,
 'learning_rate_Hydroxylation-K': 0.0012379969756453584,
 'learning_rate_Hydroxylation-P': 0.002579939523644661,
 'log_base': 2.911817620719211,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 34446830,
 'sample_weights': [7.95148068377294, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.59331162825179,
 'weight_decay_Hydroxylation-K': 0.29919131034681357,
 'weight_decay_Hydroxylation-P': 5.099443466219259}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.760
[2,     1] loss: 1237.052
[3,     1] loss: 1243.897
[4,     1] loss: 1234.678
[5,     1] loss: 1228.459
[6,     1] loss: 1228.888
[7,     1] loss: 1202.365
[8,     1] loss: 1166.765
[9,     1] loss: 1110.992
[10,     1] loss: 1085.375
[11,     1] loss: 1086.157
[12,     1] loss: 1194.344
[13,     1] loss: 1034.652
[14,     1] loss: 1036.177
[15,     1] loss: 1011.106
[16,     1] loss: 1035.009
[17,     1] loss: 1044.974
[18,     1] loss: 1024.817
[19,     1] loss: 1037.828
[20,     1] loss: 965.321
[21,     1] loss: 1002.637
[22,     1] loss: 985.085
[23,     1] loss: 980.622
[24,     1] loss: 929.000
[25,     1] loss: 924.421
[26,     1] loss: 934.097
[27,     1] loss: 958.761
[28,     1] loss: 913.841
[29,     1] loss: 919.204
[30,     1] loss: 880.958
[31,     1] loss: 852.757
[32,     1] loss: 928.252
[33,     1] loss: 855.178
[34,     1] loss: 822.478
[35,     1] loss: 782.743
[36,     1] loss: 862.317
[37,     1] loss: 791.014
[38,     1] loss: 828.838
[39,     1] loss: 763.611
[40,     1] loss: 839.934
[41,     1] loss: 1086.468
[42,     1] loss: 952.205
[43,     1] loss: 830.051
[44,     1] loss: 862.927
[45,     1] loss: 852.834
[46,     1] loss: 850.979
[47,     1] loss: 855.332
[48,     1] loss: 820.355
[49,     1] loss: 784.331
[50,     1] loss: 777.415
[51,     1] loss: 795.366
[52,     1] loss: 778.119
[53,     1] loss: 727.753
[54,     1] loss: 752.066
[55,     1] loss: 670.631
[56,     1] loss: 646.237
[57,     1] loss: 764.081
[58,     1] loss: 1040.784
[59,     1] loss: 699.458
[60,     1] loss: 969.945
[61,     1] loss: 786.009
[62,     1] loss: 874.945
[63,     1] loss: 899.726
[64,     1] loss: 806.304
[65,     1] loss: 708.448
[66,     1] loss: 795.028
[67,     1] loss: 694.264
[68,     1] loss: 789.087
[69,     1] loss: 683.935
[70,     1] loss: 756.245
[71,     1] loss: 624.907
[72,     1] loss: 671.113
[73,     1] loss: 611.838
[74,     1] loss: 521.193
[75,     1] loss: 547.714
[76,     1] loss: 644.070
[77,     1] loss: 857.156
[78,     1] loss: 979.202
[79,     1] loss: 863.505
[80,     1] loss: 923.676
[81,     1] loss: 872.177
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009397195533179295,
 'learning_rate_Hydroxylation-K': 0.0012856650382104973,
 'learning_rate_Hydroxylation-P': 0.003435712667865347,
 'log_base': 2.572418649357547,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2532860335,
 'sample_weights': [1.5620118783858847, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.5424919857573345,
 'weight_decay_Hydroxylation-K': 0.5270070711197294,
 'weight_decay_Hydroxylation-P': 0.2607950865995061}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.281
[2,     1] loss: 1288.941
[3,     1] loss: 1291.646
[4,     1] loss: 1285.449
[5,     1] loss: 1281.938
[6,     1] loss: 1285.738
[7,     1] loss: 1280.720
[8,     1] loss: 1278.151
[9,     1] loss: 1277.847
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007480496766416783,
 'learning_rate_Hydroxylation-K': 0.00038474175450621474,
 'learning_rate_Hydroxylation-P': 0.0038981230883127424,
 'log_base': 2.6813379114102402,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 372222643,
 'sample_weights': [1.766893386917793, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.230201948615892,
 'weight_decay_Hydroxylation-K': 0.9676663462491155,
 'weight_decay_Hydroxylation-P': 6.781249420165924}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.873
[2,     1] loss: 1262.613
[3,     1] loss: 1267.407
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005633555870253587,
 'learning_rate_Hydroxylation-K': 0.00918346472500583,
 'learning_rate_Hydroxylation-P': 0.0023602553114408283,
 'log_base': 1.7083072919816207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2638678444,
 'sample_weights': [1.6926049385242596, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.3750311490022735,
 'weight_decay_Hydroxylation-K': 2.6928833803925505,
 'weight_decay_Hydroxylation-P': 0.36193339683693176}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1565.739
[2,     1] loss: 1568.272
[3,     1] loss: 1568.280
[4,     1] loss: 1564.015
[5,     1] loss: 1566.498
[6,     1] loss: 1562.871
[7,     1] loss: 1557.515
[8,     1] loss: 1547.907
[9,     1] loss: 1519.085
[10,     1] loss: 1485.918
[11,     1] loss: 1405.119
[12,     1] loss: 1364.743
[13,     1] loss: 1322.186
[14,     1] loss: 1373.204
[15,     1] loss: 1285.457
[16,     1] loss: 1269.618
[17,     1] loss: 1261.238
[18,     1] loss: 1284.387
[19,     1] loss: 1265.390
[20,     1] loss: 1220.402
[21,     1] loss: 1225.853
[22,     1] loss: 1219.224
[23,     1] loss: 1141.775
[24,     1] loss: 1205.240
[25,     1] loss: 1247.518
[26,     1] loss: 1135.386
[27,     1] loss: 1171.810
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024641508111235984,
 'learning_rate_Hydroxylation-K': 0.009907892630572485,
 'learning_rate_Hydroxylation-P': 0.0009820931005370128,
 'log_base': 1.6518442436417384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 615592558,
 'sample_weights': [3.11752346996413, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.149989757341192,
 'weight_decay_Hydroxylation-K': 7.688070136180775,
 'weight_decay_Hydroxylation-P': 7.364050315739254}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1615.366
[2,     1] loss: 1609.616
[3,     1] loss: 1615.496
[4,     1] loss: 1611.778
[5,     1] loss: 1604.235
[6,     1] loss: 1603.895
[7,     1] loss: 1599.321
[8,     1] loss: 1602.735
[9,     1] loss: 1591.946
[10,     1] loss: 1581.163
[11,     1] loss: 1565.844
[12,     1] loss: 1531.130
[13,     1] loss: 1515.378
[14,     1] loss: 1501.430
[15,     1] loss: 1436.155
[16,     1] loss: 1438.524
[17,     1] loss: 1405.941
[18,     1] loss: 1393.142
[19,     1] loss: 1349.643
[20,     1] loss: 1343.281
[21,     1] loss: 1320.001
[22,     1] loss: 1366.867
[23,     1] loss: 1325.371
[24,     1] loss: 1272.376
[25,     1] loss: 1258.240
[26,     1] loss: 1287.706
[27,     1] loss: 1296.898
[28,     1] loss: 1234.893
[29,     1] loss: 1215.502
[30,     1] loss: 1184.247
[31,     1] loss: 1108.898
[32,     1] loss: 1183.162
[33,     1] loss: 1075.318
[34,     1] loss: 1150.179
[35,     1] loss: 1135.484
[36,     1] loss: 1133.323
[37,     1] loss: 1098.362
[38,     1] loss: 1076.908
[39,     1] loss: 1067.380
[40,     1] loss: 1049.494
[41,     1] loss: 1051.862
[42,     1] loss: 1022.901
[43,     1] loss: 1081.692
[44,     1] loss: 989.937
[45,     1] loss: 978.498
[46,     1] loss: 1008.005
[47,     1] loss: 980.896
[48,     1] loss: 873.533
[49,     1] loss: 967.148
[50,     1] loss: 990.246
[51,     1] loss: 923.641
[52,     1] loss: 875.962
[53,     1] loss: 886.291
[54,     1] loss: 878.956
[55,     1] loss: 911.867
[56,     1] loss: 812.400
[57,     1] loss: 931.792
[58,     1] loss: 813.237
[59,     1] loss: 803.567
[60,     1] loss: 851.289
[61,     1] loss: 840.376
[62,     1] loss: 926.476
[63,     1] loss: 754.312
[64,     1] loss: 738.759
[65,     1] loss: 755.982
[66,     1] loss: 821.463
[67,     1] loss: 673.000
[68,     1] loss: 694.292
[69,     1] loss: 772.595
[70,     1] loss: 769.856
[71,     1] loss: 806.713
[72,     1] loss: 736.896
[73,     1] loss: 748.984
[74,     1] loss: 861.266
[75,     1] loss: 675.943
Early stopping applied (best metric=0.3954254388809204)
Finished Training
Total time taken: 10.591107845306396
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1623.222
[2,     1] loss: 1614.648
[3,     1] loss: 1609.918
[4,     1] loss: 1605.236
[5,     1] loss: 1612.548
[6,     1] loss: 1610.702
[7,     1] loss: 1612.057
[8,     1] loss: 1602.580
[9,     1] loss: 1603.380
[10,     1] loss: 1600.427
[11,     1] loss: 1604.831
[12,     1] loss: 1591.734
[13,     1] loss: 1589.386
[14,     1] loss: 1566.408
[15,     1] loss: 1544.422
[16,     1] loss: 1526.622
[17,     1] loss: 1490.612
[18,     1] loss: 1445.128
[19,     1] loss: 1440.417
[20,     1] loss: 1403.328
[21,     1] loss: 1420.797
[22,     1] loss: 1363.825
[23,     1] loss: 1353.201
[24,     1] loss: 1345.603
[25,     1] loss: 1312.014
[26,     1] loss: 1312.934
[27,     1] loss: 1281.785
[28,     1] loss: 1269.779
[29,     1] loss: 1271.015
[30,     1] loss: 1216.551
[31,     1] loss: 1286.348
[32,     1] loss: 1229.354
[33,     1] loss: 1165.734
[34,     1] loss: 1237.571
[35,     1] loss: 1190.562
[36,     1] loss: 1174.353
[37,     1] loss: 1149.916
[38,     1] loss: 1187.038
[39,     1] loss: 1176.376
[40,     1] loss: 1156.877
[41,     1] loss: 1121.847
[42,     1] loss: 1175.677
[43,     1] loss: 1119.419
[44,     1] loss: 1106.348
[45,     1] loss: 1037.312
[46,     1] loss: 1060.661
[47,     1] loss: 1060.972
[48,     1] loss: 1006.660
[49,     1] loss: 951.303
[50,     1] loss: 958.136
[51,     1] loss: 930.733
[52,     1] loss: 939.480
[53,     1] loss: 981.317
[54,     1] loss: 1002.023
[55,     1] loss: 908.661
[56,     1] loss: 910.284
[57,     1] loss: 965.473
[58,     1] loss: 837.093
[59,     1] loss: 1075.260
[60,     1] loss: 874.415
[61,     1] loss: 1034.065
[62,     1] loss: 885.279
[63,     1] loss: 1016.957
[64,     1] loss: 845.190
[65,     1] loss: 887.849
[66,     1] loss: 948.439
[67,     1] loss: 792.284
[68,     1] loss: 841.907
[69,     1] loss: 841.213
[70,     1] loss: 843.100
[71,     1] loss: 803.161
[72,     1] loss: 796.992
[73,     1] loss: 799.631
[74,     1] loss: 788.625
[75,     1] loss: 755.905
[76,     1] loss: 730.850
[77,     1] loss: 726.871
[78,     1] loss: 787.221
[79,     1] loss: 670.508
[80,     1] loss: 796.945
[81,     1] loss: 746.235
Early stopping applied (best metric=0.3644295036792755)
Finished Training
Total time taken: 12.534789323806763
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1606.697
[2,     1] loss: 1605.669
[3,     1] loss: 1607.385
[4,     1] loss: 1611.073
[5,     1] loss: 1608.895
[6,     1] loss: 1612.605
[7,     1] loss: 1609.157
[8,     1] loss: 1601.985
[9,     1] loss: 1594.276
[10,     1] loss: 1590.828
[11,     1] loss: 1575.156
[12,     1] loss: 1559.851
[13,     1] loss: 1534.382
[14,     1] loss: 1487.432
[15,     1] loss: 1480.503
[16,     1] loss: 1465.008
[17,     1] loss: 1417.093
[18,     1] loss: 1433.590
[19,     1] loss: 1410.025
[20,     1] loss: 1346.370
[21,     1] loss: 1384.685
[22,     1] loss: 1334.652
[23,     1] loss: 1437.623
[24,     1] loss: 1339.726
[25,     1] loss: 1368.460
[26,     1] loss: 1337.273
[27,     1] loss: 1306.727
[28,     1] loss: 1240.655
[29,     1] loss: 1300.350
[30,     1] loss: 1282.389
[31,     1] loss: 1234.833
[32,     1] loss: 1186.078
[33,     1] loss: 1211.453
[34,     1] loss: 1303.635
[35,     1] loss: 1174.580
[36,     1] loss: 1184.113
[37,     1] loss: 1171.181
[38,     1] loss: 1149.650
[39,     1] loss: 1169.932
[40,     1] loss: 1107.709
[41,     1] loss: 1103.806
[42,     1] loss: 1138.356
[43,     1] loss: 1148.536
[44,     1] loss: 1052.690
[45,     1] loss: 1120.139
[46,     1] loss: 989.257
[47,     1] loss: 1107.557
[48,     1] loss: 1039.125
[49,     1] loss: 969.747
[50,     1] loss: 1006.732
[51,     1] loss: 1209.827
[52,     1] loss: 911.740
[53,     1] loss: 1101.429
[54,     1] loss: 986.416
[55,     1] loss: 932.752
[56,     1] loss: 958.966
[57,     1] loss: 975.641
[58,     1] loss: 1019.264
[59,     1] loss: 913.331
[60,     1] loss: 943.657
[61,     1] loss: 889.854
[62,     1] loss: 846.639
[63,     1] loss: 875.696
[64,     1] loss: 878.821
[65,     1] loss: 821.243
[66,     1] loss: 811.085
[67,     1] loss: 817.784
[68,     1] loss: 857.369
[69,     1] loss: 842.337
[70,     1] loss: 823.975
[71,     1] loss: 774.039
[72,     1] loss: 818.298
[73,     1] loss: 777.054
[74,     1] loss: 746.584
[75,     1] loss: 771.354
[76,     1] loss: 872.688
[77,     1] loss: 874.850
[78,     1] loss: 720.427
[79,     1] loss: 739.284
[80,     1] loss: 778.426
[81,     1] loss: 747.102
[82,     1] loss: 723.594
[83,     1] loss: 782.940
Early stopping applied (best metric=0.3900141417980194)
Finished Training
Total time taken: 14.735041856765747
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1609.391
[2,     1] loss: 1608.984
[3,     1] loss: 1610.464
[4,     1] loss: 1610.837
[5,     1] loss: 1609.435
[6,     1] loss: 1610.648
[7,     1] loss: 1608.796
[8,     1] loss: 1605.470
[9,     1] loss: 1608.363
[10,     1] loss: 1599.738
[11,     1] loss: 1595.198
[12,     1] loss: 1588.504
[13,     1] loss: 1586.279
[14,     1] loss: 1574.449
[15,     1] loss: 1554.604
[16,     1] loss: 1529.734
[17,     1] loss: 1502.666
[18,     1] loss: 1473.149
[19,     1] loss: 1450.772
[20,     1] loss: 1445.211
[21,     1] loss: 1364.764
[22,     1] loss: 1331.004
[23,     1] loss: 1304.531
[24,     1] loss: 1364.431
[25,     1] loss: 1357.761
[26,     1] loss: 1363.849
[27,     1] loss: 1370.960
[28,     1] loss: 1351.611
[29,     1] loss: 1267.871
[30,     1] loss: 1286.151
[31,     1] loss: 1248.095
[32,     1] loss: 1252.771
[33,     1] loss: 1243.601
[34,     1] loss: 1217.705
[35,     1] loss: 1219.317
[36,     1] loss: 1189.780
[37,     1] loss: 1234.322
[38,     1] loss: 1231.713
[39,     1] loss: 1183.291
[40,     1] loss: 1146.921
[41,     1] loss: 1298.766
[42,     1] loss: 1193.787
[43,     1] loss: 1125.021
[44,     1] loss: 1125.984
[45,     1] loss: 1191.304
[46,     1] loss: 1131.573
[47,     1] loss: 1129.578
[48,     1] loss: 1116.413
[49,     1] loss: 1166.375
[50,     1] loss: 1063.685
[51,     1] loss: 1047.329
[52,     1] loss: 995.717
[53,     1] loss: 1078.662
[54,     1] loss: 991.766
[55,     1] loss: 985.920
[56,     1] loss: 945.444
[57,     1] loss: 994.675
[58,     1] loss: 971.257
[59,     1] loss: 914.626
[60,     1] loss: 1006.549
[61,     1] loss: 839.717
[62,     1] loss: 891.122
[63,     1] loss: 882.162
[64,     1] loss: 822.560
[65,     1] loss: 876.737
[66,     1] loss: 870.575
[67,     1] loss: 837.815
[68,     1] loss: 757.829
[69,     1] loss: 810.375
[70,     1] loss: 891.552
[71,     1] loss: 887.543
[72,     1] loss: 860.501
[73,     1] loss: 761.798
[74,     1] loss: 855.702
[75,     1] loss: 767.499
[76,     1] loss: 933.465
[77,     1] loss: 858.132
[78,     1] loss: 879.782
[79,     1] loss: 785.556
[80,     1] loss: 840.859
[81,     1] loss: 770.710
[82,     1] loss: 814.934
[83,     1] loss: 799.514
[84,     1] loss: 689.434
[85,     1] loss: 700.219
[86,     1] loss: 805.854
Early stopping applied (best metric=0.43002283573150635)
Finished Training
Total time taken: 14.805365800857544
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1619.300
[2,     1] loss: 1613.343
[3,     1] loss: 1606.114
[4,     1] loss: 1609.461
[5,     1] loss: 1607.966
[6,     1] loss: 1599.061
[7,     1] loss: 1601.435
[8,     1] loss: 1594.715
[9,     1] loss: 1572.387
[10,     1] loss: 1556.863
[11,     1] loss: 1521.364
[12,     1] loss: 1484.512
[13,     1] loss: 1455.344
[14,     1] loss: 1396.297
[15,     1] loss: 1396.513
[16,     1] loss: 1362.037
[17,     1] loss: 1318.765
[18,     1] loss: 1318.067
[19,     1] loss: 1306.813
[20,     1] loss: 1363.146
[21,     1] loss: 1285.766
[22,     1] loss: 1321.569
[23,     1] loss: 1258.359
[24,     1] loss: 1316.635
[25,     1] loss: 1288.101
[26,     1] loss: 1264.411
[27,     1] loss: 1259.527
[28,     1] loss: 1227.137
[29,     1] loss: 1226.576
[30,     1] loss: 1215.309
[31,     1] loss: 1250.170
[32,     1] loss: 1166.159
[33,     1] loss: 1166.536
[34,     1] loss: 1157.792
[35,     1] loss: 1159.562
[36,     1] loss: 1134.381
[37,     1] loss: 1044.015
[38,     1] loss: 1078.344
[39,     1] loss: 1045.162
[40,     1] loss: 1065.046
[41,     1] loss: 1073.971
[42,     1] loss: 1042.404
[43,     1] loss: 1146.718
[44,     1] loss: 998.475
[45,     1] loss: 992.736
[46,     1] loss: 1011.039
[47,     1] loss: 924.048
[48,     1] loss: 979.043
[49,     1] loss: 946.835
[50,     1] loss: 995.309
[51,     1] loss: 1007.690
[52,     1] loss: 902.401
[53,     1] loss: 906.527
[54,     1] loss: 965.049
[55,     1] loss: 803.384
[56,     1] loss: 894.518
[57,     1] loss: 895.184
[58,     1] loss: 863.541
[59,     1] loss: 814.887
[60,     1] loss: 866.940
[61,     1] loss: 768.258
[62,     1] loss: 804.414
[63,     1] loss: 820.013
[64,     1] loss: 806.712
[65,     1] loss: 766.730
[66,     1] loss: 834.162
[67,     1] loss: 1010.938
[68,     1] loss: 771.562
[69,     1] loss: 807.007
[70,     1] loss: 768.895
[71,     1] loss: 789.294
[72,     1] loss: 878.060
[73,     1] loss: 775.325
[74,     1] loss: 849.767
[75,     1] loss: 763.833
[76,     1] loss: 816.362
[77,     1] loss: 737.871
[78,     1] loss: 751.503
[79,     1] loss: 784.893
[80,     1] loss: 685.155
Early stopping applied (best metric=0.41203799843788147)
Finished Training
Total time taken: 14.829211711883545
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1611.951
[2,     1] loss: 1611.586
[3,     1] loss: 1607.838
[4,     1] loss: 1608.894
[5,     1] loss: 1609.837
[6,     1] loss: 1601.788
[7,     1] loss: 1603.634
[8,     1] loss: 1599.669
[9,     1] loss: 1603.064
[10,     1] loss: 1587.352
[11,     1] loss: 1571.688
[12,     1] loss: 1551.930
[13,     1] loss: 1520.598
[14,     1] loss: 1490.143
[15,     1] loss: 1465.703
[16,     1] loss: 1416.953
[17,     1] loss: 1417.637
[18,     1] loss: 1409.915
[19,     1] loss: 1387.859
[20,     1] loss: 1383.273
[21,     1] loss: 1348.526
[22,     1] loss: 1405.153
[23,     1] loss: 1365.616
[24,     1] loss: 1347.473
[25,     1] loss: 1395.719
[26,     1] loss: 1373.499
[27,     1] loss: 1335.107
[28,     1] loss: 1302.810
[29,     1] loss: 1284.975
[30,     1] loss: 1277.997
[31,     1] loss: 1297.085
[32,     1] loss: 1308.098
[33,     1] loss: 1294.170
[34,     1] loss: 1199.571
[35,     1] loss: 1167.021
[36,     1] loss: 1219.086
[37,     1] loss: 1219.834
[38,     1] loss: 1177.526
[39,     1] loss: 1183.876
[40,     1] loss: 1164.454
[41,     1] loss: 1170.113
[42,     1] loss: 1061.922
[43,     1] loss: 1171.547
[44,     1] loss: 1090.628
[45,     1] loss: 1055.429
[46,     1] loss: 1103.247
[47,     1] loss: 1038.559
[48,     1] loss: 1030.283
[49,     1] loss: 1015.995
[50,     1] loss: 1008.726
[51,     1] loss: 1064.097
[52,     1] loss: 960.373
[53,     1] loss: 917.059
[54,     1] loss: 1032.611
[55,     1] loss: 938.309
[56,     1] loss: 975.367
[57,     1] loss: 876.848
[58,     1] loss: 1000.213
[59,     1] loss: 990.625
[60,     1] loss: 879.322
[61,     1] loss: 987.253
[62,     1] loss: 839.042
[63,     1] loss: 933.446
[64,     1] loss: 865.615
[65,     1] loss: 918.561
[66,     1] loss: 810.548
[67,     1] loss: 941.749
[68,     1] loss: 810.106
[69,     1] loss: 941.250
[70,     1] loss: 818.028
[71,     1] loss: 903.657
[72,     1] loss: 762.635
[73,     1] loss: 882.790
[74,     1] loss: 783.840
[75,     1] loss: 856.847
[76,     1] loss: 754.105
[77,     1] loss: 774.314
[78,     1] loss: 745.507
[79,     1] loss: 871.002
[80,     1] loss: 720.792
[81,     1] loss: 663.968
[82,     1] loss: 737.004
[83,     1] loss: 728.534
[84,     1] loss: 730.187
[85,     1] loss: 722.222
[86,     1] loss: 679.004
[87,     1] loss: 620.823
[88,     1] loss: 708.201
[89,     1] loss: 717.771
[90,     1] loss: 668.147
[91,     1] loss: 689.233
[92,     1] loss: 669.275
[93,     1] loss: 716.994
[94,     1] loss: 670.941
[95,     1] loss: 592.046
[96,     1] loss: 624.490
[97,     1] loss: 615.518
[98,     1] loss: 617.812
Early stopping applied (best metric=0.35927000641822815)
Finished Training
Total time taken: 15.478387594223022
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1614.419
[2,     1] loss: 1599.332
[3,     1] loss: 1619.043
[4,     1] loss: 1617.813
[5,     1] loss: 1606.755
[6,     1] loss: 1611.300
[7,     1] loss: 1609.146
[8,     1] loss: 1605.400
[9,     1] loss: 1604.821
[10,     1] loss: 1606.983
[11,     1] loss: 1602.212
[12,     1] loss: 1599.876
[13,     1] loss: 1588.897
[14,     1] loss: 1576.077
[15,     1] loss: 1566.797
[16,     1] loss: 1548.965
[17,     1] loss: 1520.186
[18,     1] loss: 1493.564
[19,     1] loss: 1486.213
[20,     1] loss: 1464.265
[21,     1] loss: 1419.373
[22,     1] loss: 1397.419
[23,     1] loss: 1401.699
[24,     1] loss: 1403.268
[25,     1] loss: 1360.160
[26,     1] loss: 1360.528
[27,     1] loss: 1362.958
[28,     1] loss: 1365.357
[29,     1] loss: 1346.063
[30,     1] loss: 1347.141
[31,     1] loss: 1368.721
[32,     1] loss: 1338.920
[33,     1] loss: 1325.554
[34,     1] loss: 1283.809
[35,     1] loss: 1295.116
[36,     1] loss: 1291.493
[37,     1] loss: 1275.988
[38,     1] loss: 1228.334
[39,     1] loss: 1251.692
[40,     1] loss: 1218.879
[41,     1] loss: 1240.229
[42,     1] loss: 1238.704
[43,     1] loss: 1280.658
[44,     1] loss: 1173.109
[45,     1] loss: 1220.103
[46,     1] loss: 1244.823
[47,     1] loss: 1192.580
[48,     1] loss: 1135.840
[49,     1] loss: 1131.958
[50,     1] loss: 1150.897
[51,     1] loss: 1126.487
[52,     1] loss: 1123.109
[53,     1] loss: 1072.303
[54,     1] loss: 1118.000
[55,     1] loss: 1083.138
[56,     1] loss: 1126.040
[57,     1] loss: 1035.480
[58,     1] loss: 1054.851
[59,     1] loss: 1030.826
[60,     1] loss: 1028.277
[61,     1] loss: 1013.261
[62,     1] loss: 1005.981
[63,     1] loss: 996.978
[64,     1] loss: 1192.682
[65,     1] loss: 1363.288
[66,     1] loss: 947.715
[67,     1] loss: 1328.670
[68,     1] loss: 1013.938
[69,     1] loss: 1093.909
[70,     1] loss: 1190.217
[71,     1] loss: 1030.230
[72,     1] loss: 993.035
[73,     1] loss: 1118.553
[74,     1] loss: 1006.773
[75,     1] loss: 923.099
[76,     1] loss: 1014.706
[77,     1] loss: 979.562
[78,     1] loss: 947.010
[79,     1] loss: 1056.380
[80,     1] loss: 883.866
[81,     1] loss: 953.807
[82,     1] loss: 928.724
[83,     1] loss: 869.820
[84,     1] loss: 914.594
[85,     1] loss: 825.781
[86,     1] loss: 885.657
[87,     1] loss: 803.852
[88,     1] loss: 804.559
[89,     1] loss: 819.203
[90,     1] loss: 791.588
[91,     1] loss: 734.619
[92,     1] loss: 753.385
[93,     1] loss: 775.259
[94,     1] loss: 737.891
[95,     1] loss: 732.698
[96,     1] loss: 739.253
[97,     1] loss: 689.160
[98,     1] loss: 710.562
[99,     1] loss: 819.547
[100,     1] loss: 823.239
[101,     1] loss: 713.230
[102,     1] loss: 861.608
[103,     1] loss: 707.998
[104,     1] loss: 748.802
Early stopping applied (best metric=0.3645487129688263)
Finished Training
Total time taken: 14.9906747341156
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1619.606
[2,     1] loss: 1606.470
[3,     1] loss: 1606.200
[4,     1] loss: 1606.418
[5,     1] loss: 1612.608
[6,     1] loss: 1603.183
[7,     1] loss: 1610.438
[8,     1] loss: 1604.629
[9,     1] loss: 1606.793
[10,     1] loss: 1597.886
[11,     1] loss: 1597.562
[12,     1] loss: 1590.919
[13,     1] loss: 1572.977
[14,     1] loss: 1557.352
[15,     1] loss: 1531.373
[16,     1] loss: 1514.011
[17,     1] loss: 1491.805
[18,     1] loss: 1468.159
[19,     1] loss: 1430.414
[20,     1] loss: 1397.154
[21,     1] loss: 1418.130
[22,     1] loss: 1380.567
[23,     1] loss: 1330.652
[24,     1] loss: 1353.327
[25,     1] loss: 1328.472
[26,     1] loss: 1293.858
[27,     1] loss: 1394.330
[28,     1] loss: 1275.815
[29,     1] loss: 1350.796
[30,     1] loss: 1262.353
[31,     1] loss: 1301.677
[32,     1] loss: 1249.802
[33,     1] loss: 1218.550
[34,     1] loss: 1273.938
[35,     1] loss: 1205.676
[36,     1] loss: 1218.412
[37,     1] loss: 1182.643
[38,     1] loss: 1154.375
[39,     1] loss: 1156.648
[40,     1] loss: 1141.995
[41,     1] loss: 1140.786
[42,     1] loss: 1092.717
[43,     1] loss: 1110.760
[44,     1] loss: 1100.838
[45,     1] loss: 1104.906
[46,     1] loss: 1154.411
[47,     1] loss: 1096.201
[48,     1] loss: 1108.007
[49,     1] loss: 1101.641
[50,     1] loss: 1019.565
[51,     1] loss: 1023.319
[52,     1] loss: 948.418
[53,     1] loss: 934.760
[54,     1] loss: 914.694
[55,     1] loss: 1013.677
[56,     1] loss: 959.578
[57,     1] loss: 947.175
[58,     1] loss: 978.521
[59,     1] loss: 905.182
[60,     1] loss: 951.661
[61,     1] loss: 901.516
[62,     1] loss: 964.973
[63,     1] loss: 833.748
[64,     1] loss: 898.132
[65,     1] loss: 898.295
[66,     1] loss: 948.412
[67,     1] loss: 837.030
[68,     1] loss: 856.568
[69,     1] loss: 837.402
[70,     1] loss: 823.728
[71,     1] loss: 788.310
[72,     1] loss: 874.770
[73,     1] loss: 882.519
[74,     1] loss: 873.276
[75,     1] loss: 746.284
[76,     1] loss: 821.517
[77,     1] loss: 739.898
[78,     1] loss: 782.946
[79,     1] loss: 813.478
[80,     1] loss: 780.003
[81,     1] loss: 739.524
[82,     1] loss: 793.604
[83,     1] loss: 751.049
[84,     1] loss: 747.113
[85,     1] loss: 716.775
[86,     1] loss: 678.890
Early stopping applied (best metric=0.4142550230026245)
Finished Training
Total time taken: 14.711189985275269
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1610.233
[2,     1] loss: 1605.429
[3,     1] loss: 1612.788
[4,     1] loss: 1607.019
[5,     1] loss: 1611.572
[6,     1] loss: 1605.455
[7,     1] loss: 1600.464
[8,     1] loss: 1599.570
[9,     1] loss: 1596.464
[10,     1] loss: 1590.922
[11,     1] loss: 1565.687
[12,     1] loss: 1551.887
[13,     1] loss: 1535.184
[14,     1] loss: 1479.692
[15,     1] loss: 1452.663
[16,     1] loss: 1431.954
[17,     1] loss: 1397.604
[18,     1] loss: 1300.828
[19,     1] loss: 1374.873
[20,     1] loss: 1306.896
[21,     1] loss: 1326.660
[22,     1] loss: 1300.695
[23,     1] loss: 1263.662
[24,     1] loss: 1338.849
[25,     1] loss: 1349.226
[26,     1] loss: 1292.135
[27,     1] loss: 1257.001
[28,     1] loss: 1262.823
[29,     1] loss: 1242.250
[30,     1] loss: 1227.319
[31,     1] loss: 1253.973
[32,     1] loss: 1174.384
[33,     1] loss: 1163.279
[34,     1] loss: 1182.318
[35,     1] loss: 1174.467
[36,     1] loss: 1165.932
[37,     1] loss: 1156.514
[38,     1] loss: 1130.266
[39,     1] loss: 1128.812
[40,     1] loss: 1151.705
[41,     1] loss: 1146.037
[42,     1] loss: 1060.832
[43,     1] loss: 1062.340
[44,     1] loss: 1092.278
[45,     1] loss: 1142.072
[46,     1] loss: 1095.764
[47,     1] loss: 1035.089
[48,     1] loss: 1006.491
[49,     1] loss: 1024.165
[50,     1] loss: 981.405
[51,     1] loss: 1071.253
[52,     1] loss: 1146.698
[53,     1] loss: 954.147
[54,     1] loss: 1128.915
[55,     1] loss: 957.480
[56,     1] loss: 1075.070
[57,     1] loss: 917.406
[58,     1] loss: 1034.932
[59,     1] loss: 922.869
[60,     1] loss: 952.073
[61,     1] loss: 928.250
[62,     1] loss: 894.179
[63,     1] loss: 909.187
[64,     1] loss: 836.921
[65,     1] loss: 882.454
[66,     1] loss: 830.778
[67,     1] loss: 896.474
[68,     1] loss: 786.780
[69,     1] loss: 885.079
[70,     1] loss: 822.905
[71,     1] loss: 896.191
[72,     1] loss: 774.183
[73,     1] loss: 812.452
[74,     1] loss: 844.474
[75,     1] loss: 919.539
[76,     1] loss: 812.623
[77,     1] loss: 759.500
[78,     1] loss: 912.948
[79,     1] loss: 732.392
[80,     1] loss: 834.694
[81,     1] loss: 820.395
[82,     1] loss: 781.162
[83,     1] loss: 740.015
[84,     1] loss: 684.708
[85,     1] loss: 754.942
[86,     1] loss: 671.913
Early stopping applied (best metric=0.37207645177841187)
Finished Training
Total time taken: 15.221150398254395
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1615.494
[2,     1] loss: 1614.407
[3,     1] loss: 1613.814
[4,     1] loss: 1614.264
[5,     1] loss: 1611.323
[6,     1] loss: 1610.305
[7,     1] loss: 1610.418
[8,     1] loss: 1608.148
[9,     1] loss: 1605.354
[10,     1] loss: 1603.073
[11,     1] loss: 1599.980
[12,     1] loss: 1589.870
[13,     1] loss: 1577.807
[14,     1] loss: 1558.770
[15,     1] loss: 1528.630
[16,     1] loss: 1504.601
[17,     1] loss: 1463.994
[18,     1] loss: 1428.716
[19,     1] loss: 1414.202
[20,     1] loss: 1385.557
[21,     1] loss: 1371.398
[22,     1] loss: 1324.309
[23,     1] loss: 1355.322
[24,     1] loss: 1398.769
[25,     1] loss: 1303.177
[26,     1] loss: 1391.952
[27,     1] loss: 1271.539
[28,     1] loss: 1287.486
[29,     1] loss: 1301.533
[30,     1] loss: 1283.811
[31,     1] loss: 1296.207
[32,     1] loss: 1264.903
[33,     1] loss: 1310.285
[34,     1] loss: 1255.267
[35,     1] loss: 1254.840
[36,     1] loss: 1257.917
[37,     1] loss: 1215.557
[38,     1] loss: 1256.843
[39,     1] loss: 1308.849
[40,     1] loss: 1176.698
[41,     1] loss: 1180.376
[42,     1] loss: 1179.861
[43,     1] loss: 1151.340
[44,     1] loss: 1134.140
[45,     1] loss: 1199.605
[46,     1] loss: 1166.137
[47,     1] loss: 1202.245
[48,     1] loss: 1118.154
[49,     1] loss: 1234.129
[50,     1] loss: 1166.228
[51,     1] loss: 1069.749
[52,     1] loss: 1120.836
[53,     1] loss: 1087.392
[54,     1] loss: 1158.025
[55,     1] loss: 1130.339
[56,     1] loss: 1057.446
[57,     1] loss: 999.458
[58,     1] loss: 1068.190
[59,     1] loss: 1096.830
[60,     1] loss: 1050.388
[61,     1] loss: 1038.182
[62,     1] loss: 1007.028
[63,     1] loss: 1031.245
[64,     1] loss: 1033.723
[65,     1] loss: 1088.984
[66,     1] loss: 991.178
[67,     1] loss: 985.528
[68,     1] loss: 1033.306
[69,     1] loss: 1032.527
[70,     1] loss: 1006.377
[71,     1] loss: 971.623
[72,     1] loss: 972.745
[73,     1] loss: 945.400
[74,     1] loss: 1047.844
[75,     1] loss: 878.319
[76,     1] loss: 948.154
[77,     1] loss: 883.564
[78,     1] loss: 864.768
[79,     1] loss: 962.672
[80,     1] loss: 1149.849
[81,     1] loss: 900.464
[82,     1] loss: 1013.826
[83,     1] loss: 910.878
[84,     1] loss: 950.117
[85,     1] loss: 840.141
[86,     1] loss: 995.397
[87,     1] loss: 874.340
[88,     1] loss: 978.707
[89,     1] loss: 923.125
[90,     1] loss: 930.425
[91,     1] loss: 813.909
[92,     1] loss: 916.889
[93,     1] loss: 850.100
[94,     1] loss: 843.380
[95,     1] loss: 789.212
[96,     1] loss: 790.459
[97,     1] loss: 695.403
[98,     1] loss: 835.777
[99,     1] loss: 696.851
[100,     1] loss: 856.412
[101,     1] loss: 844.089
[102,     1] loss: 728.536
[103,     1] loss: 836.066
[104,     1] loss: 805.495
[105,     1] loss: 777.546
[106,     1] loss: 851.487
[107,     1] loss: 688.178
[108,     1] loss: 852.397
[109,     1] loss: 671.371
[110,     1] loss: 862.071
[111,     1] loss: 678.400
[112,     1] loss: 853.063
[113,     1] loss: 701.351
[114,     1] loss: 788.041
[115,     1] loss: 631.616
[116,     1] loss: 861.659
[117,     1] loss: 701.897
[118,     1] loss: 801.082
[119,     1] loss: 682.775
[120,     1] loss: 701.187
[121,     1] loss: 618.422
[122,     1] loss: 667.465
[123,     1] loss: 662.694
[124,     1] loss: 680.392
[125,     1] loss: 694.838
[126,     1] loss: 703.910
[127,     1] loss: 666.926
[128,     1] loss: 592.866
[129,     1] loss: 626.508
[130,     1] loss: 594.777
[131,     1] loss: 638.882
[132,     1] loss: 609.484
[133,     1] loss: 617.506
[134,     1] loss: 598.046
[135,     1] loss: 623.107
[136,     1] loss: 587.407
[137,     1] loss: 681.518
[138,     1] loss: 702.117
[139,     1] loss: 579.822
[140,     1] loss: 621.862
[141,     1] loss: 776.711
[142,     1] loss: 824.197
[143,     1] loss: 534.159
[144,     1] loss: 879.877
[145,     1] loss: 566.927
[146,     1] loss: 670.754
[147,     1] loss: 600.022
[148,     1] loss: 702.153
[149,     1] loss: 635.047
[150,     1] loss: 591.690
[151,     1] loss: 631.574
[152,     1] loss: 565.951
[153,     1] loss: 549.056
[154,     1] loss: 589.181
[155,     1] loss: 579.403
[156,     1] loss: 526.151
[157,     1] loss: 608.098
[158,     1] loss: 561.332
[159,     1] loss: 529.287
[160,     1] loss: 526.097
[161,     1] loss: 508.229
[162,     1] loss: 517.389
[163,     1] loss: 484.071
Early stopping applied (best metric=0.315250426530838)
Finished Training
Total time taken: 25.632760524749756
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1613.430
[2,     1] loss: 1611.258
[3,     1] loss: 1610.597
[4,     1] loss: 1604.471
[5,     1] loss: 1608.625
[6,     1] loss: 1608.367
[7,     1] loss: 1600.679
[8,     1] loss: 1603.079
[9,     1] loss: 1610.690
[10,     1] loss: 1593.143
[11,     1] loss: 1598.762
[12,     1] loss: 1583.002
[13,     1] loss: 1563.493
[14,     1] loss: 1542.348
[15,     1] loss: 1508.597
[16,     1] loss: 1474.673
[17,     1] loss: 1424.568
[18,     1] loss: 1420.421
[19,     1] loss: 1410.200
[20,     1] loss: 1380.405
[21,     1] loss: 1344.986
[22,     1] loss: 1348.126
[23,     1] loss: 1342.919
[24,     1] loss: 1352.805
[25,     1] loss: 1339.661
[26,     1] loss: 1290.279
[27,     1] loss: 1287.178
[28,     1] loss: 1261.866
[29,     1] loss: 1267.660
[30,     1] loss: 1222.984
[31,     1] loss: 1204.582
[32,     1] loss: 1236.385
[33,     1] loss: 1137.840
[34,     1] loss: 1189.254
[35,     1] loss: 1215.396
[36,     1] loss: 1226.353
[37,     1] loss: 1160.133
[38,     1] loss: 1207.996
[39,     1] loss: 1163.006
[40,     1] loss: 1152.604
[41,     1] loss: 1176.419
[42,     1] loss: 1192.058
[43,     1] loss: 1028.433
[44,     1] loss: 1098.755
[45,     1] loss: 1086.205
[46,     1] loss: 1064.357
[47,     1] loss: 1024.604
[48,     1] loss: 1011.477
[49,     1] loss: 1002.054
[50,     1] loss: 970.498
[51,     1] loss: 1077.962
[52,     1] loss: 927.106
[53,     1] loss: 891.620
[54,     1] loss: 1007.710
[55,     1] loss: 933.992
[56,     1] loss: 944.040
[57,     1] loss: 992.954
[58,     1] loss: 880.158
[59,     1] loss: 894.372
[60,     1] loss: 940.236
[61,     1] loss: 904.184
[62,     1] loss: 813.601
[63,     1] loss: 788.499
[64,     1] loss: 873.888
[65,     1] loss: 837.467
[66,     1] loss: 922.689
[67,     1] loss: 918.292
[68,     1] loss: 787.296
[69,     1] loss: 842.997
[70,     1] loss: 849.855
[71,     1] loss: 779.760
[72,     1] loss: 766.593
[73,     1] loss: 747.110
[74,     1] loss: 792.349
[75,     1] loss: 950.907
[76,     1] loss: 806.215
Early stopping applied (best metric=0.38575077056884766)
Finished Training
Total time taken: 12.806248188018799
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1606.374
[2,     1] loss: 1609.456
[3,     1] loss: 1610.407
[4,     1] loss: 1610.319
[5,     1] loss: 1606.400
[6,     1] loss: 1607.754
[7,     1] loss: 1604.871
[8,     1] loss: 1603.734
[9,     1] loss: 1599.420
[10,     1] loss: 1596.378
[11,     1] loss: 1588.544
[12,     1] loss: 1571.316
[13,     1] loss: 1553.934
[14,     1] loss: 1534.229
[15,     1] loss: 1497.891
[16,     1] loss: 1474.197
[17,     1] loss: 1447.268
[18,     1] loss: 1409.726
[19,     1] loss: 1406.493
[20,     1] loss: 1371.941
[21,     1] loss: 1347.129
[22,     1] loss: 1354.980
[23,     1] loss: 1298.135
[24,     1] loss: 1337.562
[25,     1] loss: 1391.436
[26,     1] loss: 1336.288
[27,     1] loss: 1326.154
[28,     1] loss: 1339.092
[29,     1] loss: 1315.653
[30,     1] loss: 1257.477
[31,     1] loss: 1268.561
[32,     1] loss: 1296.850
[33,     1] loss: 1270.876
[34,     1] loss: 1233.925
[35,     1] loss: 1227.579
[36,     1] loss: 1194.734
[37,     1] loss: 1190.120
[38,     1] loss: 1133.143
[39,     1] loss: 1195.362
[40,     1] loss: 1165.013
[41,     1] loss: 1134.685
[42,     1] loss: 1141.767
[43,     1] loss: 1184.175
[44,     1] loss: 1067.823
[45,     1] loss: 1102.789
[46,     1] loss: 1026.774
[47,     1] loss: 1048.751
[48,     1] loss: 1048.745
[49,     1] loss: 1056.688
[50,     1] loss: 997.885
[51,     1] loss: 1044.214
[52,     1] loss: 1003.259
[53,     1] loss: 910.487
[54,     1] loss: 1023.574
[55,     1] loss: 1060.979
[56,     1] loss: 970.744
[57,     1] loss: 1001.721
[58,     1] loss: 1019.338
[59,     1] loss: 933.535
[60,     1] loss: 1020.132
[61,     1] loss: 1135.169
[62,     1] loss: 936.021
[63,     1] loss: 1047.221
[64,     1] loss: 907.572
[65,     1] loss: 960.792
[66,     1] loss: 888.212
[67,     1] loss: 945.625
[68,     1] loss: 1029.507
[69,     1] loss: 839.635
[70,     1] loss: 835.248
[71,     1] loss: 851.788
[72,     1] loss: 861.287
[73,     1] loss: 769.394
[74,     1] loss: 806.327
[75,     1] loss: 817.342
[76,     1] loss: 755.689
[77,     1] loss: 775.493
[78,     1] loss: 829.443
[79,     1] loss: 808.364
[80,     1] loss: 836.738
[81,     1] loss: 772.596
[82,     1] loss: 752.976
[83,     1] loss: 801.727
[84,     1] loss: 776.703
[85,     1] loss: 693.257
[86,     1] loss: 735.350
[87,     1] loss: 776.175
[88,     1] loss: 837.983
[89,     1] loss: 757.801
[90,     1] loss: 704.117
[91,     1] loss: 770.769
[92,     1] loss: 723.280
[93,     1] loss: 651.101
[94,     1] loss: 693.602
[95,     1] loss: 710.048
[96,     1] loss: 735.155
Early stopping applied (best metric=0.37433844804763794)
Finished Training
Total time taken: 16.06570839881897
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1611.545
[2,     1] loss: 1611.895
[3,     1] loss: 1610.855
[4,     1] loss: 1609.533
[5,     1] loss: 1611.197
[6,     1] loss: 1606.083
[7,     1] loss: 1604.835
[8,     1] loss: 1605.652
[9,     1] loss: 1598.630
[10,     1] loss: 1597.506
[11,     1] loss: 1592.277
[12,     1] loss: 1577.239
[13,     1] loss: 1562.710
[14,     1] loss: 1542.619
[15,     1] loss: 1514.807
[16,     1] loss: 1483.959
[17,     1] loss: 1476.960
[18,     1] loss: 1445.915
[19,     1] loss: 1414.059
[20,     1] loss: 1392.104
[21,     1] loss: 1343.364
[22,     1] loss: 1337.984
[23,     1] loss: 1315.665
[24,     1] loss: 1343.126
[25,     1] loss: 1306.680
[26,     1] loss: 1268.869
[27,     1] loss: 1317.920
[28,     1] loss: 1279.698
[29,     1] loss: 1308.869
[30,     1] loss: 1215.546
[31,     1] loss: 1309.539
[32,     1] loss: 1290.960
[33,     1] loss: 1241.386
[34,     1] loss: 1262.724
[35,     1] loss: 1214.313
[36,     1] loss: 1167.411
[37,     1] loss: 1190.643
[38,     1] loss: 1156.281
[39,     1] loss: 1154.821
[40,     1] loss: 1116.776
[41,     1] loss: 1143.997
[42,     1] loss: 1169.506
[43,     1] loss: 1102.919
[44,     1] loss: 1147.317
[45,     1] loss: 1102.533
[46,     1] loss: 1143.138
[47,     1] loss: 1073.544
[48,     1] loss: 1234.437
[49,     1] loss: 1043.802
[50,     1] loss: 1186.806
[51,     1] loss: 1016.976
[52,     1] loss: 1142.754
[53,     1] loss: 1010.937
[54,     1] loss: 1071.652
[55,     1] loss: 1015.431
[56,     1] loss: 1046.351
[57,     1] loss: 985.620
[58,     1] loss: 979.739
[59,     1] loss: 945.611
[60,     1] loss: 1014.425
[61,     1] loss: 872.942
[62,     1] loss: 954.214
[63,     1] loss: 901.577
[64,     1] loss: 829.703
[65,     1] loss: 938.480
[66,     1] loss: 875.622
[67,     1] loss: 857.461
[68,     1] loss: 947.605
[69,     1] loss: 973.062
[70,     1] loss: 845.246
[71,     1] loss: 835.069
[72,     1] loss: 869.218
[73,     1] loss: 904.499
[74,     1] loss: 783.001
[75,     1] loss: 844.416
[76,     1] loss: 812.736
[77,     1] loss: 899.648
[78,     1] loss: 928.897
[79,     1] loss: 817.566
[80,     1] loss: 760.171
[81,     1] loss: 820.916
[82,     1] loss: 755.500
Early stopping applied (best metric=0.42596396803855896)
Finished Training
Total time taken: 13.707157373428345
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1611.879
[2,     1] loss: 1611.792
[3,     1] loss: 1610.741
[4,     1] loss: 1608.206
[5,     1] loss: 1604.352
[6,     1] loss: 1608.501
[7,     1] loss: 1604.935
[8,     1] loss: 1601.232
[9,     1] loss: 1594.053
[10,     1] loss: 1592.774
[11,     1] loss: 1589.047
[12,     1] loss: 1564.046
[13,     1] loss: 1552.461
[14,     1] loss: 1531.232
[15,     1] loss: 1484.951
[16,     1] loss: 1465.163
[17,     1] loss: 1444.930
[18,     1] loss: 1441.427
[19,     1] loss: 1380.662
[20,     1] loss: 1370.609
[21,     1] loss: 1333.245
[22,     1] loss: 1278.855
[23,     1] loss: 1374.571
[24,     1] loss: 1286.433
[25,     1] loss: 1279.408
[26,     1] loss: 1288.949
[27,     1] loss: 1354.999
[28,     1] loss: 1267.737
[29,     1] loss: 1252.674
[30,     1] loss: 1250.728
[31,     1] loss: 1238.455
[32,     1] loss: 1209.125
[33,     1] loss: 1176.488
[34,     1] loss: 1220.303
[35,     1] loss: 1164.357
[36,     1] loss: 1131.004
[37,     1] loss: 1170.639
[38,     1] loss: 1100.495
[39,     1] loss: 1203.161
[40,     1] loss: 1063.521
[41,     1] loss: 1102.620
[42,     1] loss: 1040.370
[43,     1] loss: 1052.934
[44,     1] loss: 1084.869
[45,     1] loss: 1047.693
[46,     1] loss: 1060.607
[47,     1] loss: 1042.082
[48,     1] loss: 1000.889
[49,     1] loss: 1026.683
[50,     1] loss: 985.089
[51,     1] loss: 928.432
[52,     1] loss: 962.158
[53,     1] loss: 918.210
[54,     1] loss: 990.848
[55,     1] loss: 949.378
[56,     1] loss: 860.115
[57,     1] loss: 935.061
[58,     1] loss: 977.987
[59,     1] loss: 860.971
[60,     1] loss: 882.220
[61,     1] loss: 900.637
[62,     1] loss: 876.416
[63,     1] loss: 819.069
[64,     1] loss: 827.536
[65,     1] loss: 840.708
[66,     1] loss: 868.868
[67,     1] loss: 927.603
[68,     1] loss: 1091.127
[69,     1] loss: 904.139
[70,     1] loss: 948.073
[71,     1] loss: 957.942
[72,     1] loss: 866.383
[73,     1] loss: 813.249
[74,     1] loss: 939.540
[75,     1] loss: 834.415
[76,     1] loss: 927.549
[77,     1] loss: 811.693
Early stopping applied (best metric=0.38319703936576843)
Finished Training
Total time taken: 12.93672251701355
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1610.403
[2,     1] loss: 1615.578
[3,     1] loss: 1610.892
[4,     1] loss: 1608.049
[5,     1] loss: 1607.519
[6,     1] loss: 1610.496
[7,     1] loss: 1612.106
[8,     1] loss: 1616.536
[9,     1] loss: 1608.773
[10,     1] loss: 1609.650
[11,     1] loss: 1604.779
[12,     1] loss: 1601.762
[13,     1] loss: 1600.267
[14,     1] loss: 1587.857
[15,     1] loss: 1579.515
[16,     1] loss: 1553.399
[17,     1] loss: 1520.087
[18,     1] loss: 1507.876
[19,     1] loss: 1454.182
[20,     1] loss: 1443.913
[21,     1] loss: 1387.087
[22,     1] loss: 1382.074
[23,     1] loss: 1358.443
[24,     1] loss: 1379.973
[25,     1] loss: 1355.339
[26,     1] loss: 1317.089
[27,     1] loss: 1424.655
[28,     1] loss: 1404.199
[29,     1] loss: 1270.576
[30,     1] loss: 1321.437
[31,     1] loss: 1311.118
[32,     1] loss: 1319.496
[33,     1] loss: 1278.448
[34,     1] loss: 1256.762
[35,     1] loss: 1326.228
[36,     1] loss: 1284.347
[37,     1] loss: 1219.178
[38,     1] loss: 1224.853
[39,     1] loss: 1235.788
[40,     1] loss: 1241.254
[41,     1] loss: 1185.168
[42,     1] loss: 1135.080
[43,     1] loss: 1214.517
[44,     1] loss: 1189.677
[45,     1] loss: 1163.250
[46,     1] loss: 1136.336
[47,     1] loss: 1093.237
[48,     1] loss: 1081.292
[49,     1] loss: 1024.888
[50,     1] loss: 1087.187
[51,     1] loss: 1054.318
[52,     1] loss: 1049.448
[53,     1] loss: 1020.790
[54,     1] loss: 1034.133
[55,     1] loss: 1030.247
[56,     1] loss: 1049.178
[57,     1] loss: 1023.513
[58,     1] loss: 964.261
[59,     1] loss: 963.765
[60,     1] loss: 1098.713
[61,     1] loss: 1036.689
[62,     1] loss: 1050.984
[63,     1] loss: 909.645
[64,     1] loss: 997.140
[65,     1] loss: 980.025
[66,     1] loss: 929.193
[67,     1] loss: 957.468
[68,     1] loss: 948.267
[69,     1] loss: 906.700
[70,     1] loss: 872.473
[71,     1] loss: 865.750
[72,     1] loss: 781.885
[73,     1] loss: 854.115
[74,     1] loss: 872.247
[75,     1] loss: 813.884
[76,     1] loss: 881.895
[77,     1] loss: 892.782
[78,     1] loss: 834.130
[79,     1] loss: 894.924
[80,     1] loss: 934.807
[81,     1] loss: 761.935
[82,     1] loss: 823.262
[83,     1] loss: 753.065
[84,     1] loss: 833.481
[85,     1] loss: 858.125
[86,     1] loss: 758.948
[87,     1] loss: 727.790
[88,     1] loss: 804.740
[89,     1] loss: 769.375
[90,     1] loss: 763.681
[91,     1] loss: 679.644
[92,     1] loss: 871.869
[93,     1] loss: 1043.161
[94,     1] loss: 727.131
[95,     1] loss: 978.510
[96,     1] loss: 753.221
[97,     1] loss: 851.064
[98,     1] loss: 835.906
[99,     1] loss: 949.070
[100,     1] loss: 787.723
[101,     1] loss: 772.527
[102,     1] loss: 884.451
[103,     1] loss: 711.229
[104,     1] loss: 796.615
Early stopping applied (best metric=0.35789385437965393)
Finished Training
Total time taken: 17.344709634780884
{'Hydroxylation-K Validation Accuracy': 0.7574763593380615, 'Hydroxylation-K Validation Sensitivity': 0.6607407407407407, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.4359388006214632, 'Hydroxylation-K AUC ROC': 0.7801169590643274, 'Hydroxylation-K AUC PR': 0.5966175074172789, 'Hydroxylation-K MCC': 0.3867102728243464, 'Hydroxylation-K F1': 0.5215702172223912, 'Validation Loss (Hydroxylation-K)': 0.4655425210793813, 'Hydroxylation-P Validation Accuracy': 0.7731566079556029, 'Hydroxylation-P Validation Sensitivity': 0.7895767195767196, 'Hydroxylation-P Validation Specificity': 0.7695944934909472, 'Hydroxylation-P Validation Precision': 0.42844882923110994, 'Hydroxylation-P AUC ROC': 0.8311748220827228, 'Hydroxylation-P AUC PR': 0.5577972513489454, 'Hydroxylation-P MCC': 0.4565670738177914, 'Hydroxylation-P F1': 0.5537358357505999, 'Validation Loss (Hydroxylation-P)': 0.3829649746417999, 'Validation Loss (total)': 0.8485074917475383, 'TimeToTrain': 15.092681725819906}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006236405718182776,
 'learning_rate_Hydroxylation-K': 0.0018583473983310736,
 'learning_rate_Hydroxylation-P': 0.0029893748721201863,
 'log_base': 2.9498947111908116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1608624650,
 'sample_weights': [3.3287644532363987, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.378676551639979,
 'weight_decay_Hydroxylation-K': 0.8897625597799769,
 'weight_decay_Hydroxylation-P': 4.255642310591322}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.157
[2,     1] loss: 1232.981
[3,     1] loss: 1234.457
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000550285032042176,
 'learning_rate_Hydroxylation-K': 0.0015647087942785745,
 'learning_rate_Hydroxylation-P': 0.0043502828779202925,
 'log_base': 2.9578176822220694,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 347510703,
 'sample_weights': [1.54325221820363, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.341140865005025,
 'weight_decay_Hydroxylation-K': 3.7539929185172003,
 'weight_decay_Hydroxylation-P': 0.27672262448719903}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.821
[2,     1] loss: 1232.874
[3,     1] loss: 1232.686
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007819733881343115,
 'learning_rate_Hydroxylation-K': 0.0013851690184700205,
 'learning_rate_Hydroxylation-P': 0.007652696660177976,
 'log_base': 2.766631323364877,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1907806370,
 'sample_weights': [1.5394351873979566, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.844897024337921,
 'weight_decay_Hydroxylation-K': 1.099761473142378,
 'weight_decay_Hydroxylation-P': 3.868279278396333}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.838
[2,     1] loss: 1256.056
[3,     1] loss: 1254.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004909624803563412,
 'learning_rate_Hydroxylation-K': 0.0022811205859609913,
 'learning_rate_Hydroxylation-P': 0.009114410078445902,
 'log_base': 2.5964631461772725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3265303466,
 'sample_weights': [1.6405200386678893, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.812719469392468,
 'weight_decay_Hydroxylation-K': 7.041029563351396,
 'weight_decay_Hydroxylation-P': 0.043795696597333705}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.363
[2,     1] loss: 1280.933
[3,     1] loss: 1275.211
[4,     1] loss: 1277.276
[5,     1] loss: 1275.702
[6,     1] loss: 1267.732
[7,     1] loss: 1260.499
[8,     1] loss: 1249.497
[9,     1] loss: 1206.946
[10,     1] loss: 1175.250
[11,     1] loss: 1140.814
[12,     1] loss: 1086.241
[13,     1] loss: 1073.371
[14,     1] loss: 1166.726
[15,     1] loss: 1034.521
[16,     1] loss: 1077.875
[17,     1] loss: 1068.099
[18,     1] loss: 1027.760
[19,     1] loss: 1044.188
[20,     1] loss: 1027.837
[21,     1] loss: 1018.663
[22,     1] loss: 997.972
[23,     1] loss: 1022.861
[24,     1] loss: 988.700
[25,     1] loss: 971.900
[26,     1] loss: 946.580
[27,     1] loss: 959.275
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009146136966454468,
 'learning_rate_Hydroxylation-K': 0.009856009567250175,
 'learning_rate_Hydroxylation-P': 0.002315405391508479,
 'log_base': 1.0795645436376653,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 135896759,
 'sample_weights': [1.749664952082438, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.321654165764787,
 'weight_decay_Hydroxylation-K': 3.09004671529619,
 'weight_decay_Hydroxylation-P': 3.7179909302550325}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7088.589
[2,     1] loss: 7121.646
[3,     1] loss: 7167.331
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006732966846204827,
 'learning_rate_Hydroxylation-K': 0.0027150064970421547,
 'learning_rate_Hydroxylation-P': 0.0017058272274237017,
 'log_base': 2.627616702225539,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2689084302,
 'sample_weights': [21.80632189954484, 2.7258952458288137],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1248762016474565,
 'weight_decay_Hydroxylation-K': 4.273300077173362,
 'weight_decay_Hydroxylation-P': 3.225601733337258}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.968
[2,     1] loss: 1280.338
[3,     1] loss: 1274.576
[4,     1] loss: 1270.392
[5,     1] loss: 1273.668
[6,     1] loss: 1276.044
[7,     1] loss: 1274.257
[8,     1] loss: 1271.321
[9,     1] loss: 1269.323
[10,     1] loss: 1266.653
[11,     1] loss: 1262.423
[12,     1] loss: 1255.663
[13,     1] loss: 1239.306
[14,     1] loss: 1204.687
[15,     1] loss: 1174.854
[16,     1] loss: 1171.647
[17,     1] loss: 1099.989
[18,     1] loss: 1065.259
[19,     1] loss: 1014.893
[20,     1] loss: 1022.594
[21,     1] loss: 1084.417
[22,     1] loss: 1030.278
[23,     1] loss: 1000.374
[24,     1] loss: 997.269
[25,     1] loss: 979.011
[26,     1] loss: 1012.004
[27,     1] loss: 964.236
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003858985070232428,
 'learning_rate_Hydroxylation-K': 0.0002229295153612686,
 'learning_rate_Hydroxylation-P': 0.005051180268754089,
 'log_base': 2.772169514757609,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2460730183,
 'sample_weights': [1.728063845146624, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.458846141256624,
 'weight_decay_Hydroxylation-K': 2.9241814080226693,
 'weight_decay_Hydroxylation-P': 4.204877850662343}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.119
[2,     1] loss: 1252.814
[3,     1] loss: 1253.668
[4,     1] loss: 1248.270
[5,     1] loss: 1238.883
[6,     1] loss: 1224.672
[7,     1] loss: 1200.002
[8,     1] loss: 1162.619
[9,     1] loss: 1104.106
[10,     1] loss: 1100.970
[11,     1] loss: 1159.828
[12,     1] loss: 1067.998
[13,     1] loss: 1076.472
[14,     1] loss: 1039.516
[15,     1] loss: 1052.264
[16,     1] loss: 1060.532
[17,     1] loss: 1029.847
[18,     1] loss: 1028.114
[19,     1] loss: 980.768
[20,     1] loss: 1027.150
[21,     1] loss: 988.694
[22,     1] loss: 985.128
[23,     1] loss: 982.062
[24,     1] loss: 923.101
[25,     1] loss: 936.043
[26,     1] loss: 940.344
[27,     1] loss: 894.314
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005903386921424695,
 'learning_rate_Hydroxylation-K': 0.0026194374386324616,
 'learning_rate_Hydroxylation-P': 0.001450151886495252,
 'log_base': 1.4401610365469044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 143753873,
 'sample_weights': [1.6373025191707589, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.904401411725369,
 'weight_decay_Hydroxylation-K': 3.510757024936881,
 'weight_decay_Hydroxylation-P': 8.977355824838012}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1878.902
[2,     1] loss: 1878.360
[3,     1] loss: 1876.435
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019354050887548811,
 'learning_rate_Hydroxylation-K': 0.002892028742728225,
 'learning_rate_Hydroxylation-P': 0.004567726776977111,
 'log_base': 2.5131629263043114,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1918076559,
 'sample_weights': [4.576889775197375, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.336699939689783,
 'weight_decay_Hydroxylation-K': 6.922268147162821,
 'weight_decay_Hydroxylation-P': 0.41086157404684703}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.812
[2,     1] loss: 1289.710
[3,     1] loss: 1291.215
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003990456003283897,
 'learning_rate_Hydroxylation-K': 0.0056547790995204635,
 'learning_rate_Hydroxylation-P': 0.00950640444730838,
 'log_base': 2.847878669804428,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2120585957,
 'sample_weights': [1.8115755816417343, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.803532322138773,
 'weight_decay_Hydroxylation-K': 5.778399340832985,
 'weight_decay_Hydroxylation-P': 2.9858320314850735}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.115
[2,     1] loss: 1245.351
[3,     1] loss: 1247.439
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004066221144013276,
 'learning_rate_Hydroxylation-K': 0.0022588106471341392,
 'learning_rate_Hydroxylation-P': 0.009536064327595295,
 'log_base': 1.6794603960290992,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1138398721,
 'sample_weights': [1.5951500076654181, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8081582320826235,
 'weight_decay_Hydroxylation-K': 5.853100149729704,
 'weight_decay_Hydroxylation-P': 5.011355903236313}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1588.000
[2,     1] loss: 1585.500
[3,     1] loss: 1590.763
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054414664099047645,
 'learning_rate_Hydroxylation-K': 0.0031917160489386034,
 'learning_rate_Hydroxylation-P': 0.007463137968636348,
 'log_base': 2.4395553581636924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4215400117,
 'sample_weights': [3.219925819991726, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.304409741802277,
 'weight_decay_Hydroxylation-K': 7.366322556266661,
 'weight_decay_Hydroxylation-P': 0.8058169051699619}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.186
[2,     1] loss: 1301.943
[3,     1] loss: 1312.787
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007669606197112521,
 'learning_rate_Hydroxylation-K': 0.009666729207981192,
 'learning_rate_Hydroxylation-P': 0.0006312262793367683,
 'log_base': 1.1892834567195598,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 960275742,
 'sample_weights': [1.871959614954932, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.582099476209943,
 'weight_decay_Hydroxylation-K': 2.095278230974071,
 'weight_decay_Hydroxylation-P': 1.3596715956523742}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3125.032
[2,     1] loss: 3126.598
[3,     1] loss: 3122.423
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00014515540973079194,
 'learning_rate_Hydroxylation-K': 0.005096154950122833,
 'learning_rate_Hydroxylation-P': 9.56583161078435e-05,
 'log_base': 1.518215755216135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1920241085,
 'sample_weights': [9.630421847225405, 1.2038491062183951],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9560689139107148,
 'weight_decay_Hydroxylation-K': 5.047513524593179,
 'weight_decay_Hydroxylation-P': 0.532973423670734}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1755.171
[2,     1] loss: 1752.087
[3,     1] loss: 1752.791
[4,     1] loss: 1754.319
[5,     1] loss: 1752.588
[6,     1] loss: 1753.859
[7,     1] loss: 1754.479
[8,     1] loss: 1753.184
[9,     1] loss: 1752.271
[10,     1] loss: 1745.513
[11,     1] loss: 1750.637
[12,     1] loss: 1753.560
[13,     1] loss: 1748.669
[14,     1] loss: 1751.801
[15,     1] loss: 1750.857
[16,     1] loss: 1750.434
[17,     1] loss: 1748.161
[18,     1] loss: 1750.987
[19,     1] loss: 1747.496
[20,     1] loss: 1749.937
[21,     1] loss: 1750.865
[22,     1] loss: 1752.920
[23,     1] loss: 1754.106
[24,     1] loss: 1747.992
[25,     1] loss: 1746.734
[26,     1] loss: 1752.274
[27,     1] loss: 1750.333
[28,     1] loss: 1750.481
[29,     1] loss: 1752.651
[30,     1] loss: 1749.515
[31,     1] loss: 1748.296
[32,     1] loss: 1754.922
[33,     1] loss: 1749.334
[34,     1] loss: 1747.011
[35,     1] loss: 1751.990
[36,     1] loss: 1749.080
[37,     1] loss: 1750.676
[38,     1] loss: 1747.514
[39,     1] loss: 1749.236
[40,     1] loss: 1747.399
[41,     1] loss: 1750.354
[42,     1] loss: 1748.844
[43,     1] loss: 1746.891
[44,     1] loss: 1746.697
[45,     1] loss: 1749.085
[46,     1] loss: 1746.794
[47,     1] loss: 1748.261
[48,     1] loss: 1748.743
[49,     1] loss: 1749.996
[50,     1] loss: 1745.112
[51,     1] loss: 1747.882
[52,     1] loss: 1747.140
[53,     1] loss: 1746.455
[54,     1] loss: 1746.367
[55,     1] loss: 1747.953
[56,     1] loss: 1745.855
[57,     1] loss: 1743.100
[58,     1] loss: 1746.091
[59,     1] loss: 1743.665
[60,     1] loss: 1743.825
[61,     1] loss: 1744.599
[62,     1] loss: 1743.038
[63,     1] loss: 1746.560
[64,     1] loss: 1747.375
[65,     1] loss: 1742.729
[66,     1] loss: 1746.066
[67,     1] loss: 1745.534
[68,     1] loss: 1741.462
[69,     1] loss: 1740.904
[70,     1] loss: 1742.296
[71,     1] loss: 1743.279
[72,     1] loss: 1740.330
[73,     1] loss: 1741.139
[74,     1] loss: 1738.383
[75,     1] loss: 1733.883
[76,     1] loss: 1744.043
[77,     1] loss: 1731.013
[78,     1] loss: 1732.684
[79,     1] loss: 1733.874
[80,     1] loss: 1732.780
[81,     1] loss: 1727.900
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014086705977220371,
 'learning_rate_Hydroxylation-K': 0.001431492331850943,
 'learning_rate_Hydroxylation-P': 0.008136922716776063,
 'log_base': 2.914415906500532,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 790696890,
 'sample_weights': [3.998323370080241, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.27810837402747,
 'weight_decay_Hydroxylation-K': 3.339718263295549,
 'weight_decay_Hydroxylation-P': 3.8680304307694904}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.839
[2,     1] loss: 1242.924
[3,     1] loss: 1237.947
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004402015670675738,
 'learning_rate_Hydroxylation-K': 0.0005928812485540436,
 'learning_rate_Hydroxylation-P': 0.009854938380742083,
 'log_base': 2.8345828388162033,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1400191902,
 'sample_weights': [1.5607094201592828, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7452125430910606,
 'weight_decay_Hydroxylation-K': 9.012034590009078,
 'weight_decay_Hydroxylation-P': 4.986556404820808}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.830
[2,     1] loss: 1246.933
[3,     1] loss: 1244.835
[4,     1] loss: 1243.587
[5,     1] loss: 1240.625
[6,     1] loss: 1245.024
[7,     1] loss: 1233.765
[8,     1] loss: 1222.771
[9,     1] loss: 1204.439
[10,     1] loss: 1174.493
[11,     1] loss: 1173.781
[12,     1] loss: 1112.607
[13,     1] loss: 1083.165
[14,     1] loss: 1022.826
[15,     1] loss: 1044.884
[16,     1] loss: 1058.017
[17,     1] loss: 1072.415
[18,     1] loss: 1047.620
[19,     1] loss: 1066.688
[20,     1] loss: 1014.686
[21,     1] loss: 1018.821
[22,     1] loss: 1016.413
[23,     1] loss: 1013.480
[24,     1] loss: 971.331
[25,     1] loss: 1042.713
[26,     1] loss: 955.547
[27,     1] loss: 949.047
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003378261404831173,
 'learning_rate_Hydroxylation-K': 0.0001944220181975553,
 'learning_rate_Hydroxylation-P': 0.005394509233672656,
 'log_base': 2.940268066090475,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3170510174,
 'sample_weights': [1.6023145332812632, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.252272588867522,
 'weight_decay_Hydroxylation-K': 1.083720343728574,
 'weight_decay_Hydroxylation-P': 4.007687785183511}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.809
[2,     1] loss: 1237.337
[3,     1] loss: 1238.198
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007517090601058008,
 'learning_rate_Hydroxylation-K': 0.0003377758338092972,
 'learning_rate_Hydroxylation-P': 0.008845083851861297,
 'log_base': 2.706880540999549,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2886561119,
 'sample_weights': [1.5479295104106199, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.365959808599819,
 'weight_decay_Hydroxylation-K': 6.393638768120799,
 'weight_decay_Hydroxylation-P': 1.1905056597636312}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.112
[2,     1] loss: 1265.715
[3,     1] loss: 1262.429
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000517910058027515,
 'learning_rate_Hydroxylation-K': 0.0005516946746021217,
 'learning_rate_Hydroxylation-P': 0.0037086726119659897,
 'log_base': 1.6367008934518998,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 595449069,
 'sample_weights': [1.6764896345457778, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.125852586692792,
 'weight_decay_Hydroxylation-K': 4.104859823089135,
 'weight_decay_Hydroxylation-P': 7.711162022883558}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1619.522
[2,     1] loss: 1622.623
[3,     1] loss: 1624.713
[4,     1] loss: 1625.515
[5,     1] loss: 1623.424
[6,     1] loss: 1625.357
[7,     1] loss: 1619.331
[8,     1] loss: 1626.481
[9,     1] loss: 1618.104
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004329840212412079,
 'learning_rate_Hydroxylation-K': 0.007937432326742025,
 'learning_rate_Hydroxylation-P': 0.00395959842063083,
 'log_base': 1.088322048532049,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 749304118,
 'sample_weights': [3.388476200725708, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.861489624732253,
 'weight_decay_Hydroxylation-K': 3.6617563225340297,
 'weight_decay_Hydroxylation-P': 0.3033932985215904}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6416.144
[2,     1] loss: 6400.043
[3,     1] loss: 6384.169
[4,     1] loss: 6385.443
[5,     1] loss: 6377.127
[6,     1] loss: 6401.921
[7,     1] loss: 6371.730
[8,     1] loss: 6361.462
[9,     1] loss: 6364.636
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007275277480349508,
 'learning_rate_Hydroxylation-K': 0.0011326905326272806,
 'learning_rate_Hydroxylation-P': 0.001161394956834709,
 'log_base': 2.816582548148973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4288872369,
 'sample_weights': [19.724719376830176, 2.465684906528466],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.517160869678778,
 'weight_decay_Hydroxylation-K': 0.17529763235375873,
 'weight_decay_Hydroxylation-P': 7.264872307739419}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.810
[2,     1] loss: 1249.202
[3,     1] loss: 1253.229
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004677460222297721,
 'learning_rate_Hydroxylation-K': 0.003310246260436558,
 'learning_rate_Hydroxylation-P': 0.003778612744833154,
 'log_base': 2.660122977933802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 112503490,
 'sample_weights': [1.6121718894143198, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.654432272184942,
 'weight_decay_Hydroxylation-K': 9.031177519110615,
 'weight_decay_Hydroxylation-P': 8.300021157069448}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.665
[2,     1] loss: 1270.104
[3,     1] loss: 1267.178
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00432303836666963,
 'learning_rate_Hydroxylation-K': 0.00043106259018869014,
 'learning_rate_Hydroxylation-P': 0.008396894161653781,
 'log_base': 2.5425507711571935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2797998606,
 'sample_weights': [1.70634742544146, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.870101750133557,
 'weight_decay_Hydroxylation-K': 8.15643939933597,
 'weight_decay_Hydroxylation-P': 0.555574613649163}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.556
[2,     1] loss: 1289.432
[3,     1] loss: 1284.970
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005599692299630436,
 'learning_rate_Hydroxylation-K': 0.00993302612748667,
 'learning_rate_Hydroxylation-P': 0.007051401022762759,
 'log_base': 1.570300388870975,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2361286157,
 'sample_weights': [1.7890063455922496, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.393993614576061,
 'weight_decay_Hydroxylation-K': 4.114686784027675,
 'weight_decay_Hydroxylation-P': 3.7312297084932937}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1689.028
[2,     1] loss: 1706.019
[3,     1] loss: 1695.981
[4,     1] loss: 1685.611
[5,     1] loss: 1684.501
[6,     1] loss: 1687.790
[7,     1] loss: 1684.957
[8,     1] loss: 1684.939
[9,     1] loss: 1674.782
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002607661146761615,
 'learning_rate_Hydroxylation-K': 0.00011392379023050561,
 'learning_rate_Hydroxylation-P': 0.005966036095847918,
 'log_base': 2.551971378413083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 105739894,
 'sample_weights': [3.6994581936011484, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.652326608862691,
 'weight_decay_Hydroxylation-K': 1.9969501379277086,
 'weight_decay_Hydroxylation-P': 7.277531776899934}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.735
[2,     1] loss: 1286.506
[3,     1] loss: 1288.050
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00437028966056828,
 'learning_rate_Hydroxylation-K': 0.002728940530250844,
 'learning_rate_Hydroxylation-P': 0.00853249268481557,
 'log_base': 2.9581532640919246,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2433018865,
 'sample_weights': [1.781944141443403, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.979394101205076,
 'weight_decay_Hydroxylation-K': 7.771789340433081,
 'weight_decay_Hydroxylation-P': 0.5074845057882558}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.024
[2,     1] loss: 1230.580
[3,     1] loss: 1237.411
[4,     1] loss: 1230.475
[5,     1] loss: 1233.471
[6,     1] loss: 1226.316
[7,     1] loss: 1222.487
[8,     1] loss: 1207.099
[9,     1] loss: 1190.034
[10,     1] loss: 1148.615
[11,     1] loss: 1111.892
[12,     1] loss: 1077.982
[13,     1] loss: 1068.696
[14,     1] loss: 1084.667
[15,     1] loss: 1002.049
[16,     1] loss: 1053.574
[17,     1] loss: 1010.752
[18,     1] loss: 1027.939
[19,     1] loss: 1027.376
[20,     1] loss: 1007.841
[21,     1] loss: 967.844
[22,     1] loss: 989.777
[23,     1] loss: 984.272
[24,     1] loss: 963.115
[25,     1] loss: 964.295
[26,     1] loss: 1075.448
[27,     1] loss: 902.603
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004392980572472484,
 'learning_rate_Hydroxylation-K': 0.0029338378856823355,
 'learning_rate_Hydroxylation-P': 0.0028884558503530777,
 'log_base': 2.871283406616878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 667508158,
 'sample_weights': [1.5392741568737993, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.15188378636048183,
 'weight_decay_Hydroxylation-K': 1.9775340073656618,
 'weight_decay_Hydroxylation-P': 9.988658661363875}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.178
[2,     1] loss: 1245.130
[3,     1] loss: 1242.041
[4,     1] loss: 1241.425
[5,     1] loss: 1241.658
[6,     1] loss: 1241.383
[7,     1] loss: 1242.135
[8,     1] loss: 1235.334
[9,     1] loss: 1239.467
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074789247558653665,
 'learning_rate_Hydroxylation-K': 0.007183797648069628,
 'learning_rate_Hydroxylation-P': 0.00022782241217583505,
 'log_base': 1.4445511836993756,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1307260829,
 'sample_weights': [1.5827719636429998, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9144878291795884,
 'weight_decay_Hydroxylation-K': 2.0176233010743028,
 'weight_decay_Hydroxylation-P': 9.270990509316501}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1865.252
[2,     1] loss: 1868.696
[3,     1] loss: 1870.333
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002066336791637136,
 'learning_rate_Hydroxylation-K': 0.003660115729620764,
 'learning_rate_Hydroxylation-P': 0.0002104550112133115,
 'log_base': 1.8026945929195806,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3614717940,
 'sample_weights': [4.539013504765128, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7600202862147136,
 'weight_decay_Hydroxylation-K': 1.1634882274956753,
 'weight_decay_Hydroxylation-P': 9.613068346703255}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1504.684
[2,     1] loss: 1507.864
[3,     1] loss: 1501.487
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0098530437077696,
 'learning_rate_Hydroxylation-K': 0.00890947528548228,
 'learning_rate_Hydroxylation-P': 0.0003650639357747705,
 'log_base': 1.0360351250272448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1944750012,
 'sample_weights': [2.833009684841249, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.838777587540035,
 'weight_decay_Hydroxylation-K': 4.036049627664179,
 'weight_decay_Hydroxylation-P': 1.5172778344883726}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15355.151
Exploding loss, terminate run (best metric=0.5320394039154053)
Finished Training
Total time taken: 0.20804142951965332
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15311.349
Exploding loss, terminate run (best metric=0.534786581993103)
Finished Training
Total time taken: 0.23000288009643555
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15557.811
Exploding loss, terminate run (best metric=0.5314566493034363)
Finished Training
Total time taken: 0.2350006103515625
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15252.548
Exploding loss, terminate run (best metric=0.5349751710891724)
Finished Training
Total time taken: 0.21000146865844727
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15317.139
Exploding loss, terminate run (best metric=0.5369120836257935)
Finished Training
Total time taken: 0.24003243446350098
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15265.121
Exploding loss, terminate run (best metric=0.5572142601013184)
Finished Training
Total time taken: 0.24602603912353516
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15339.228
Exploding loss, terminate run (best metric=0.5347363948822021)
Finished Training
Total time taken: 0.24401140213012695
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15332.434
Exploding loss, terminate run (best metric=0.536708652973175)
Finished Training
Total time taken: 0.20099997520446777
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15292.939
Exploding loss, terminate run (best metric=0.5310811400413513)
Finished Training
Total time taken: 0.20700287818908691
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15391.309
Exploding loss, terminate run (best metric=0.5369771718978882)
Finished Training
Total time taken: 0.230513334274292
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15299.697
Exploding loss, terminate run (best metric=0.5331200361251831)
Finished Training
Total time taken: 0.22039556503295898
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15295.318
Exploding loss, terminate run (best metric=0.5268695950508118)
Finished Training
Total time taken: 0.20399856567382812
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15302.327
Exploding loss, terminate run (best metric=0.5288146734237671)
Finished Training
Total time taken: 0.2200005054473877
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15307.262
Exploding loss, terminate run (best metric=0.5380566716194153)
Finished Training
Total time taken: 0.2369999885559082
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15338.379
Exploding loss, terminate run (best metric=0.5300077795982361)
Finished Training
Total time taken: 0.22502660751342773
{'Hydroxylation-K Validation Accuracy': 0.4805555555555555, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5815009746588694, 'Hydroxylation-K AUC PR': 0.2895190086383511, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1792282430213465, 'Validation Loss (Hydroxylation-K)': 0.561144208908081, 'Hydroxylation-P Validation Accuracy': 0.47872588531885013, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5081431405754314, 'Hydroxylation-P AUC PR': 0.22469358914338924, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16050727048035554, 'Validation Loss (Hydroxylation-P)': 0.5349170843760173, 'Validation Loss (total)': 1.0960612773895264, 'TimeToTrain': 0.22387024561564128}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006546825784846941,
 'learning_rate_Hydroxylation-K': 0.0043659583972192615,
 'learning_rate_Hydroxylation-P': 0.004795374232937679,
 'log_base': 2.9053934089172384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3271248863,
 'sample_weights': [47.19299695489528, 5.886867280662],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9876866072268724,
 'weight_decay_Hydroxylation-K': 9.388874175814957,
 'weight_decay_Hydroxylation-P': 1.0111292817951383}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.562
[2,     1] loss: 1239.101
[3,     1] loss: 1237.747
[4,     1] loss: 1234.341
[5,     1] loss: 1237.806
[6,     1] loss: 1238.178
[7,     1] loss: 1236.291
[8,     1] loss: 1225.888
[9,     1] loss: 1220.505
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009815653325748509,
 'learning_rate_Hydroxylation-K': 0.008006649279220896,
 'learning_rate_Hydroxylation-P': 0.00048713046443787293,
 'log_base': 1.4022214712352266,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2964531017,
 'sample_weights': [1.56524655343621, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.531646180947679,
 'weight_decay_Hydroxylation-K': 4.432250856820274,
 'weight_decay_Hydroxylation-P': 0.8936562443570398}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1963.868
[2,     1] loss: 1953.171
[3,     1] loss: 1954.396
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024841556816104166,
 'learning_rate_Hydroxylation-K': 0.009148832467677637,
 'learning_rate_Hydroxylation-P': 0.0031367569136369526,
 'log_base': 1.4893055056144768,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3623870556,
 'sample_weights': [4.938337239802475, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.367190848691273,
 'weight_decay_Hydroxylation-K': 0.5836475360038264,
 'weight_decay_Hydroxylation-P': 0.209606449091166}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1789.285
[2,     1] loss: 1793.766
[3,     1] loss: 1789.709
[4,     1] loss: 1785.442
[5,     1] loss: 1788.748
[6,     1] loss: 1784.909
[7,     1] loss: 1784.966
[8,     1] loss: 1781.360
[9,     1] loss: 1766.053
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005040284098655337,
 'learning_rate_Hydroxylation-K': 0.0026017458619489128,
 'learning_rate_Hydroxylation-P': 0.009750633319958107,
 'log_base': 2.784572667864311,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1050748938,
 'sample_weights': [4.191317150847468, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.260870246877973,
 'weight_decay_Hydroxylation-K': 8.458977373342158,
 'weight_decay_Hydroxylation-P': 0.9412060533718939}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.417
[2,     1] loss: 1255.097
[3,     1] loss: 1250.866
[4,     1] loss: 1253.820
[5,     1] loss: 1250.837
[6,     1] loss: 1249.513
[7,     1] loss: 1244.329
[8,     1] loss: 1243.219
[9,     1] loss: 1222.281
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002859988586792246,
 'learning_rate_Hydroxylation-K': 0.005991178482707465,
 'learning_rate_Hydroxylation-P': 0.007361616881406473,
 'log_base': 2.983652882405626,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3135725112,
 'sample_weights': [1.6301652593923335, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.704580090252109,
 'weight_decay_Hydroxylation-K': 9.647779071377148,
 'weight_decay_Hydroxylation-P': 8.423483769701978}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.526
[2,     1] loss: 1228.065
[3,     1] loss: 1231.349
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009395258580695206,
 'learning_rate_Hydroxylation-K': 0.0036749918225296397,
 'learning_rate_Hydroxylation-P': 0.005866535279734014,
 'log_base': 2.4853607703884064,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2423491646,
 'sample_weights': [1.5271880971025886, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5694992997093613,
 'weight_decay_Hydroxylation-K': 3.882943188836083,
 'weight_decay_Hydroxylation-P': 1.3878014323207202}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.358
[2,     1] loss: 1298.493
[3,     1] loss: 1298.363
[4,     1] loss: 1294.166
[5,     1] loss: 1293.121
[6,     1] loss: 1295.276
[7,     1] loss: 1286.911
[8,     1] loss: 1284.610
[9,     1] loss: 1275.056
[10,     1] loss: 1255.178
[11,     1] loss: 1230.026
[12,     1] loss: 1196.019
[13,     1] loss: 1187.868
[14,     1] loss: 1110.655
[15,     1] loss: 1143.522
[16,     1] loss: 1141.061
[17,     1] loss: 1099.936
[18,     1] loss: 1054.276
[19,     1] loss: 1090.801
[20,     1] loss: 1096.854
[21,     1] loss: 1058.273
[22,     1] loss: 1023.864
[23,     1] loss: 1053.760
[24,     1] loss: 1017.254
[25,     1] loss: 1021.708
[26,     1] loss: 1031.336
[27,     1] loss: 1040.479
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008338956150541605,
 'learning_rate_Hydroxylation-K': 0.00890471615875701,
 'learning_rate_Hydroxylation-P': 0.001659452962220894,
 'log_base': 1.0154153446731466,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1953814715,
 'sample_weights': [1.8337109573607573, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.707814055686416,
 'weight_decay_Hydroxylation-K': 6.012489624889414,
 'weight_decay_Hydroxylation-P': 1.2314722267128002}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35436.570
Exploding loss, terminate run (best metric=0.5319291353225708)
Finished Training
Total time taken: 0.2230541706085205
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35307.574
Exploding loss, terminate run (best metric=0.5266835689544678)
Finished Training
Total time taken: 0.20900201797485352
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35424.488
Exploding loss, terminate run (best metric=0.5375539660453796)
Finished Training
Total time taken: 0.22299814224243164
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35273.227
Exploding loss, terminate run (best metric=0.5612097382545471)
Finished Training
Total time taken: 0.22800207138061523
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35582.070
Exploding loss, terminate run (best metric=0.5459113121032715)
Finished Training
Total time taken: 0.2180008888244629
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35546.922
Exploding loss, terminate run (best metric=0.5339066982269287)
Finished Training
Total time taken: 0.20600008964538574
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35320.297
Exploding loss, terminate run (best metric=0.5265754461288452)
Finished Training
Total time taken: 0.22600007057189941
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35467.742
Exploding loss, terminate run (best metric=0.5338220596313477)
Finished Training
Total time taken: 0.22499990463256836
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35588.070
Exploding loss, terminate run (best metric=0.5324430465698242)
Finished Training
Total time taken: 0.2220156192779541
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35451.035
Exploding loss, terminate run (best metric=0.53047114610672)
Finished Training
Total time taken: 0.2050004005432129
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35351.844
Exploding loss, terminate run (best metric=0.5324214100837708)
Finished Training
Total time taken: 0.2330002784729004
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35484.004
Exploding loss, terminate run (best metric=0.5286009907722473)
Finished Training
Total time taken: 0.22500085830688477
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35695.465
Exploding loss, terminate run (best metric=0.5461564660072327)
Finished Training
Total time taken: 0.2090013027191162
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35338.461
Exploding loss, terminate run (best metric=0.531120777130127)
Finished Training
Total time taken: 0.20200085639953613
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35381.195
Exploding loss, terminate run (best metric=0.5404636263847351)
Finished Training
Total time taken: 0.21600055694580078
{'Hydroxylation-K Validation Accuracy': 0.5669917257683215, 'Hydroxylation-K Validation Sensitivity': 0.3933333333333333, 'Hydroxylation-K Validation Specificity': 0.6070175438596491, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6056042884990254, 'Hydroxylation-K AUC PR': 0.31457590253807055, 'Hydroxylation-K MCC': 0.00046647428569421306, 'Hydroxylation-K F1': 0.13602410385103944, 'Validation Loss (Hydroxylation-K)': 0.5608221769332886, 'Hydroxylation-P Validation Accuracy': 0.5692762803918583, 'Hydroxylation-P Validation Sensitivity': 0.3980952380952381, 'Hydroxylation-P Validation Specificity': 0.6056910569105691, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6294893364431745, 'Hydroxylation-P AUC PR': 0.3092095860344397, 'Hydroxylation-P MCC': 0.00546029182010486, 'Hydroxylation-P F1': 0.12120717813775628, 'Validation Loss (Hydroxylation-P)': 0.535951292514801, 'Validation Loss (total)': 1.0967734654744465, 'TimeToTrain': 0.21800514856974285}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007370627982128228,
 'learning_rate_Hydroxylation-K': 0.00793756250241219,
 'learning_rate_Hydroxylation-P': 0.0026823346396197953,
 'log_base': 1.3791327481430882,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2180607853,
 'sample_weights': [109.21103664643002, 13.62301442596472],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.7874879667727095,
 'weight_decay_Hydroxylation-K': 6.975236478675203,
 'weight_decay_Hydroxylation-P': 1.8321458846286638}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2018.838
[2,     1] loss: 2000.780
[3,     1] loss: 2026.088
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009149472511843153,
 'learning_rate_Hydroxylation-K': 0.0049995686433928765,
 'learning_rate_Hydroxylation-P': 0.00016990181180218372,
 'log_base': 2.1513572211005423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1216342901,
 'sample_weights': [5.193398402990902, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.47216472661604,
 'weight_decay_Hydroxylation-K': 0.22354384256756532,
 'weight_decay_Hydroxylation-P': 5.520449834781559}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1371.257
[2,     1] loss: 1377.064
[3,     1] loss: 1368.015
[4,     1] loss: 1366.643
[5,     1] loss: 1367.984
[6,     1] loss: 1366.926
[7,     1] loss: 1364.226
[8,     1] loss: 1369.411
[9,     1] loss: 1371.330
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003832630652913488,
 'learning_rate_Hydroxylation-K': 0.007112368906783894,
 'learning_rate_Hydroxylation-P': 0.0009118988275752783,
 'log_base': 2.174484431902229,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2626354099,
 'sample_weights': [2.1791483171110086, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4149265288704331,
 'weight_decay_Hydroxylation-K': 1.3169066993089296,
 'weight_decay_Hydroxylation-P': 9.909529728918672}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1356.068
[2,     1] loss: 1366.936
[3,     1] loss: 1366.725
[4,     1] loss: 1357.981
[5,     1] loss: 1354.233
[6,     1] loss: 1355.980
[7,     1] loss: 1360.904
[8,     1] loss: 1347.027
[9,     1] loss: 1346.929
[10,     1] loss: 1345.846
[11,     1] loss: 1331.711
[12,     1] loss: 1309.794
[13,     1] loss: 1297.567
[14,     1] loss: 1270.256
[15,     1] loss: 1256.620
[16,     1] loss: 1212.978
[17,     1] loss: 1189.356
[18,     1] loss: 1149.100
[19,     1] loss: 1149.104
[20,     1] loss: 1124.022
[21,     1] loss: 1143.310
[22,     1] loss: 1122.674
[23,     1] loss: 1100.917
[24,     1] loss: 1110.335
[25,     1] loss: 1085.530
[26,     1] loss: 1145.877
[27,     1] loss: 1085.330
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00864274420875801,
 'learning_rate_Hydroxylation-K': 0.00916404425718611,
 'learning_rate_Hydroxylation-P': 0.0003056514013656792,
 'log_base': 1.0421528605912083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1629820539,
 'sample_weights': [2.149151923239903, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.549468562674692,
 'weight_decay_Hydroxylation-K': 3.688510966869768,
 'weight_decay_Hydroxylation-P': 2.486252211512665}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13128.172
[2,     1] loss: 13076.244
[3,     1] loss: 13171.946
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008105457350056522,
 'learning_rate_Hydroxylation-K': 0.009339951200156465,
 'learning_rate_Hydroxylation-P': 0.00638213226355544,
 'log_base': 1.4308032835671534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4046377539,
 'sample_weights': [40.433481925098576, 5.054379948148525],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.151222423709577,
 'weight_decay_Hydroxylation-K': 1.813490539177902,
 'weight_decay_Hydroxylation-P': 1.7862338847199455}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1889.531
[2,     1] loss: 1903.604
[3,     1] loss: 1892.310
[4,     1] loss: 1885.292
[5,     1] loss: 1881.242
[6,     1] loss: 1892.944
[7,     1] loss: 1891.654
[8,     1] loss: 1892.812
[9,     1] loss: 1889.528
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001501898780356556,
 'learning_rate_Hydroxylation-K': 0.0018843707681621695,
 'learning_rate_Hydroxylation-P': 0.009851467402161557,
 'log_base': 1.1471835569932303,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 534258654,
 'sample_weights': [4.660176639874824, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.244660755091781,
 'weight_decay_Hydroxylation-K': 8.643073119662464,
 'weight_decay_Hydroxylation-P': 3.965355208008877}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3947.028
[2,     1] loss: 3944.270
[3,     1] loss: 3945.871
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008368294319325667,
 'learning_rate_Hydroxylation-K': 0.009469323255666982,
 'learning_rate_Hydroxylation-P': 0.0011128004312491479,
 'log_base': 1.3555745657306206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 720593301,
 'sample_weights': [12.158217746758151, 1.5198357662660809],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.191372620148677,
 'weight_decay_Hydroxylation-K': 6.609012457613466,
 'weight_decay_Hydroxylation-P': 1.2707336256500863}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2066.596
[2,     1] loss: 2074.859
[3,     1] loss: 2063.491
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006297902177822529,
 'learning_rate_Hydroxylation-K': 0.005876447983797897,
 'learning_rate_Hydroxylation-P': 0.00833836721001807,
 'log_base': 2.906502760618238,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3818441946,
 'sample_weights': [5.487520625423246, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.1467595333016396,
 'weight_decay_Hydroxylation-K': 1.1906913469709304,
 'weight_decay_Hydroxylation-P': 9.214412413732994}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.244
[2,     1] loss: 1251.880
[3,     1] loss: 1238.229
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004773477980454486,
 'learning_rate_Hydroxylation-K': 0.0009561536363466128,
 'learning_rate_Hydroxylation-P': 0.005048228058401049,
 'log_base': 2.5263380386546017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 411664873,
 'sample_weights': [1.564686512376238, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.234824018278355,
 'weight_decay_Hydroxylation-K': 4.069146641506189,
 'weight_decay_Hydroxylation-P': 5.0675093704041085}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.696
[2,     1] loss: 1297.132
[3,     1] loss: 1284.807
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007665789974271571,
 'learning_rate_Hydroxylation-K': 0.0023052877248598037,
 'learning_rate_Hydroxylation-P': 0.002762695809011923,
 'log_base': 2.3432067817704096,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 141030855,
 'sample_weights': [1.8013548525321164, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.115578386274779,
 'weight_decay_Hydroxylation-K': 3.018626844631335,
 'weight_decay_Hydroxylation-P': 5.780716147683608}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.612
[2,     1] loss: 1325.360
[3,     1] loss: 1323.733
[4,     1] loss: 1322.905
[5,     1] loss: 1320.398
[6,     1] loss: 1326.017
[7,     1] loss: 1322.855
[8,     1] loss: 1321.441
[9,     1] loss: 1320.236
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035469082306832608,
 'learning_rate_Hydroxylation-K': 3.640162279161985e-05,
 'learning_rate_Hydroxylation-P': 0.002565969344166091,
 'log_base': 2.4495693553885665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 209700884,
 'sample_weights': [1.960543900445347, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.812582207497275,
 'weight_decay_Hydroxylation-K': 6.454389053428804,
 'weight_decay_Hydroxylation-P': 0.1755330606286762}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.817
[2,     1] loss: 1303.325
[3,     1] loss: 1301.652
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00222906813419246,
 'learning_rate_Hydroxylation-K': 0.0031213527049189834,
 'learning_rate_Hydroxylation-P': 0.009682089353851151,
 'log_base': 1.1120006544019256,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 64164289,
 'sample_weights': [1.8634003206135232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3216890414735216,
 'weight_decay_Hydroxylation-K': 6.661680030061864,
 'weight_decay_Hydroxylation-P': 1.9992687183270652}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5096.625
[2,     1] loss: 5081.312
[3,     1] loss: 5138.986
[4,     1] loss: 5079.763
[5,     1] loss: 5094.097
[6,     1] loss: 5095.767
[7,     1] loss: 5083.508
[8,     1] loss: 5102.038
[9,     1] loss: 5100.485
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00742997131755775,
 'learning_rate_Hydroxylation-K': 0.0018736303861996138,
 'learning_rate_Hydroxylation-P': 0.009973708348914501,
 'log_base': 2.652633844287734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3648122755,
 'sample_weights': [15.725610526809234, 1.9657770425593681],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.281890184854012,
 'weight_decay_Hydroxylation-K': 7.4479021037101205,
 'weight_decay_Hydroxylation-P': 2.4205109138341676}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.912
[2,     1] loss: 1268.295
[3,     1] loss: 1270.199
[4,     1] loss: 1267.638
[5,     1] loss: 1261.693
[6,     1] loss: 1251.652
[7,     1] loss: 1226.443
[8,     1] loss: 1198.921
[9,     1] loss: 1193.793
[10,     1] loss: 1231.852
[11,     1] loss: 1094.339
[12,     1] loss: 1158.009
[13,     1] loss: 1081.371
[14,     1] loss: 1105.770
[15,     1] loss: 1061.033
[16,     1] loss: 1082.346
[17,     1] loss: 1018.555
[18,     1] loss: 1054.754
[19,     1] loss: 1060.533
[20,     1] loss: 1018.631
[21,     1] loss: 986.239
[22,     1] loss: 989.053
[23,     1] loss: 950.880
[24,     1] loss: 973.065
[25,     1] loss: 908.329
[26,     1] loss: 911.739
[27,     1] loss: 952.539
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050008604102138796,
 'learning_rate_Hydroxylation-K': 0.004307461668415702,
 'learning_rate_Hydroxylation-P': 0.005597398230801642,
 'log_base': 2.8481417986912194,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1459150040,
 'sample_weights': [1.711278692205738, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.233108642229838,
 'weight_decay_Hydroxylation-K': 2.653845098684762,
 'weight_decay_Hydroxylation-P': 2.3678553799622724}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.232
[2,     1] loss: 1249.773
[3,     1] loss: 1244.049
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030592685017222887,
 'learning_rate_Hydroxylation-K': 0.003476944256531049,
 'learning_rate_Hydroxylation-P': 0.005025586189749244,
 'log_base': 2.7367624176155814,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3778995124,
 'sample_weights': [1.59500920201841, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.282297998008556,
 'weight_decay_Hydroxylation-K': 1.1371814296138583,
 'weight_decay_Hydroxylation-P': 6.78601248656376}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.088
[2,     1] loss: 1257.378
[3,     1] loss: 1258.571
[4,     1] loss: 1258.006
[5,     1] loss: 1254.748
[6,     1] loss: 1254.592
[7,     1] loss: 1248.896
[8,     1] loss: 1240.197
[9,     1] loss: 1230.839
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009969212099593979,
 'learning_rate_Hydroxylation-K': 0.003929290466368492,
 'learning_rate_Hydroxylation-P': 0.005616551726604454,
 'log_base': 2.457007641395121,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1141528815,
 'sample_weights': [1.6582077578774126, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.239395254219973,
 'weight_decay_Hydroxylation-K': 2.9773797827220596,
 'weight_decay_Hydroxylation-P': 2.3977859705928513}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1298.292
[2,     1] loss: 1300.682
[3,     1] loss: 1312.912
[4,     1] loss: 1291.936
[5,     1] loss: 1292.084
[6,     1] loss: 1285.815
[7,     1] loss: 1271.236
[8,     1] loss: 1258.672
[9,     1] loss: 1226.525
[10,     1] loss: 1176.342
[11,     1] loss: 1175.788
[12,     1] loss: 1108.367
[13,     1] loss: 1072.403
[14,     1] loss: 1146.564
[15,     1] loss: 1106.849
[16,     1] loss: 1057.406
[17,     1] loss: 1051.778
[18,     1] loss: 1072.139
[19,     1] loss: 1073.074
[20,     1] loss: 1052.781
[21,     1] loss: 1008.547
[22,     1] loss: 1001.281
[23,     1] loss: 1023.207
[24,     1] loss: 1122.906
[25,     1] loss: 986.556
[26,     1] loss: 1009.065
[27,     1] loss: 1022.302
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005659473846431495,
 'learning_rate_Hydroxylation-K': 0.007219759064286953,
 'learning_rate_Hydroxylation-P': 0.006941668819261627,
 'log_base': 2.9612599440083907,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3039192407,
 'sample_weights': [1.8571154258136064, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1553220526110743,
 'weight_decay_Hydroxylation-K': 2.256731612430081,
 'weight_decay_Hydroxylation-P': 9.498608573259903}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.140
[2,     1] loss: 1228.516
[3,     1] loss: 1232.499
[4,     1] loss: 1228.583
[5,     1] loss: 1230.993
[6,     1] loss: 1229.804
[7,     1] loss: 1225.055
[8,     1] loss: 1218.789
[9,     1] loss: 1210.111
[10,     1] loss: 1195.151
[11,     1] loss: 1173.968
[12,     1] loss: 1132.345
[13,     1] loss: 1117.788
[14,     1] loss: 1095.584
[15,     1] loss: 1050.659
[16,     1] loss: 1037.815
[17,     1] loss: 1035.919
[18,     1] loss: 1008.292
[19,     1] loss: 1029.875
[20,     1] loss: 1006.679
[21,     1] loss: 1004.166
[22,     1] loss: 1031.867
[23,     1] loss: 1004.567
[24,     1] loss: 975.392
[25,     1] loss: 1007.605
[26,     1] loss: 1005.195
[27,     1] loss: 950.215
[28,     1] loss: 987.398
[29,     1] loss: 1035.844
[30,     1] loss: 936.718
[31,     1] loss: 1018.000
[32,     1] loss: 947.532
[33,     1] loss: 956.184
[34,     1] loss: 965.513
[35,     1] loss: 957.448
[36,     1] loss: 943.427
[37,     1] loss: 892.577
[38,     1] loss: 930.609
[39,     1] loss: 888.576
[40,     1] loss: 895.220
[41,     1] loss: 867.740
[42,     1] loss: 887.950
[43,     1] loss: 882.080
[44,     1] loss: 882.571
[45,     1] loss: 809.916
[46,     1] loss: 864.365
[47,     1] loss: 882.023
[48,     1] loss: 877.229
[49,     1] loss: 849.247
[50,     1] loss: 834.328
[51,     1] loss: 871.545
[52,     1] loss: 809.536
[53,     1] loss: 800.966
[54,     1] loss: 848.521
[55,     1] loss: 795.888
[56,     1] loss: 814.766
[57,     1] loss: 766.980
[58,     1] loss: 767.192
[59,     1] loss: 742.257
[60,     1] loss: 830.156
[61,     1] loss: 780.040
[62,     1] loss: 805.118
[63,     1] loss: 799.616
[64,     1] loss: 809.815
[65,     1] loss: 716.402
[66,     1] loss: 749.998
[67,     1] loss: 721.588
[68,     1] loss: 745.493
[69,     1] loss: 696.807
[70,     1] loss: 691.619
[71,     1] loss: 695.300
[72,     1] loss: 704.096
[73,     1] loss: 680.682
[74,     1] loss: 669.344
[75,     1] loss: 665.824
[76,     1] loss: 689.911
[77,     1] loss: 666.852
[78,     1] loss: 715.100
[79,     1] loss: 638.343
[80,     1] loss: 723.113
[81,     1] loss: 699.444
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0056919400776966255,
 'learning_rate_Hydroxylation-K': 0.0011628377019078007,
 'learning_rate_Hydroxylation-P': 0.0025682464531774024,
 'log_base': 1.669768998343851,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2055163959,
 'sample_weights': [1.5377858650460703, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.5434159207549305,
 'weight_decay_Hydroxylation-K': 0.20225216058277729,
 'weight_decay_Hydroxylation-P': 1.1696461489885142}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1596.276
[2,     1] loss: 1598.279
[3,     1] loss: 1600.026
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009823928264053525,
 'learning_rate_Hydroxylation-K': 0.004782091986200893,
 'learning_rate_Hydroxylation-P': 0.0055609594129587495,
 'log_base': 2.6077816508482754,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3664741610,
 'sample_weights': [3.256272750152415, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6599304383068296,
 'weight_decay_Hydroxylation-K': 5.614208995228134,
 'weight_decay_Hydroxylation-P': 0.9495714542466065}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1276.373
[2,     1] loss: 1287.194
[3,     1] loss: 1283.319
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006523146247269743,
 'learning_rate_Hydroxylation-K': 0.0013920384388358673,
 'learning_rate_Hydroxylation-P': 0.005638751402477715,
 'log_base': 2.831809598444711,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1843146262,
 'sample_weights': [1.7417248729512058, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.189646548909971,
 'weight_decay_Hydroxylation-K': 1.2126676661790596,
 'weight_decay_Hydroxylation-P': 3.505827154091935}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.375
[2,     1] loss: 1243.282
[3,     1] loss: 1250.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00781713320191858,
 'learning_rate_Hydroxylation-K': 0.008345724175370078,
 'learning_rate_Hydroxylation-P': 0.0014517825490614241,
 'log_base': 2.2005162019138553,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 951281543,
 'sample_weights': [1.6038212895505153, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4646434861016138,
 'weight_decay_Hydroxylation-K': 6.164645840941452,
 'weight_decay_Hydroxylation-P': 5.575401728846645}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1354.965
[2,     1] loss: 1356.688
[3,     1] loss: 1357.492
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009740948085900543,
 'learning_rate_Hydroxylation-K': 0.0029860659880882176,
 'learning_rate_Hydroxylation-P': 0.0074817426306957235,
 'log_base': 2.9407609023717107,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4051739276,
 'sample_weights': [2.11672390586819, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7032474765296692,
 'weight_decay_Hydroxylation-K': 3.6617080398480715,
 'weight_decay_Hydroxylation-P': 1.977325963216269}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.839
[2,     1] loss: 1235.884
[3,     1] loss: 1238.765
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007227930686548409,
 'learning_rate_Hydroxylation-K': 0.004226446483326291,
 'learning_rate_Hydroxylation-P': 0.006230924125970622,
 'log_base': 1.7092390076598976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 269050264,
 'sample_weights': [1.5476889951759556, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.987548646948901,
 'weight_decay_Hydroxylation-K': 5.8432510878815656,
 'weight_decay_Hydroxylation-P': 0.12348882646183679}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1566.028
[2,     1] loss: 1560.766
[3,     1] loss: 1589.039
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008915562855716739,
 'learning_rate_Hydroxylation-K': 0.007906401816912505,
 'learning_rate_Hydroxylation-P': 0.00044403443821191505,
 'log_base': 2.1720931432558337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1090594551,
 'sample_weights': [3.1143524070767437, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.808960013935158,
 'weight_decay_Hydroxylation-K': 0.8261511747256937,
 'weight_decay_Hydroxylation-P': 0.8332512657865744}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1364.854
[2,     1] loss: 1367.948
[3,     1] loss: 1362.606
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009064316943389557,
 'learning_rate_Hydroxylation-K': 0.009159778548966824,
 'learning_rate_Hydroxylation-P': 0.002334379502158279,
 'log_base': 1.0360741637756996,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 802710970,
 'sample_weights': [2.152200469650792, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.983099814957371,
 'weight_decay_Hydroxylation-K': 4.282691519343297,
 'weight_decay_Hydroxylation-P': 0.5111242410068333}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15320.361
Exploding loss, terminate run (best metric=0.5320721864700317)
Finished Training
Total time taken: 0.2259993553161621
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15320.945
Exploding loss, terminate run (best metric=0.5406928062438965)
Finished Training
Total time taken: 0.2070004940032959
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15467.602
Exploding loss, terminate run (best metric=0.5366207957267761)
Finished Training
Total time taken: 0.2240006923675537
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15377.748
Exploding loss, terminate run (best metric=0.5287767648696899)
Finished Training
Total time taken: 0.24199843406677246
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15307.833
Exploding loss, terminate run (best metric=0.5276523232460022)
Finished Training
Total time taken: 0.21999812126159668
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15343.336
Exploding loss, terminate run (best metric=0.5353504419326782)
Finished Training
Total time taken: 0.20499849319458008
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15301.021
Exploding loss, terminate run (best metric=0.5272501111030579)
Finished Training
Total time taken: 0.22699832916259766
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15326.768
Exploding loss, terminate run (best metric=0.5308144688606262)
Finished Training
Total time taken: 0.24300265312194824
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15256.397
Exploding loss, terminate run (best metric=0.5653870105743408)
Finished Training
Total time taken: 0.23000049591064453
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15381.745
Exploding loss, terminate run (best metric=0.5298234820365906)
Finished Training
Total time taken: 0.205000638961792
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15270.066
Exploding loss, terminate run (best metric=0.5330960154533386)
Finished Training
Total time taken: 0.22600126266479492
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15275.829
Exploding loss, terminate run (best metric=0.5283549427986145)
Finished Training
Total time taken: 0.23600149154663086
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15440.262
Exploding loss, terminate run (best metric=0.530465841293335)
Finished Training
Total time taken: 0.20300054550170898
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15333.167
Exploding loss, terminate run (best metric=0.5268605351448059)
Finished Training
Total time taken: 0.23003292083740234
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15378.240
Exploding loss, terminate run (best metric=0.5664527416229248)
Finished Training
Total time taken: 0.23602628707885742
{'Hydroxylation-K Validation Accuracy': 0.3588652482269504, 'Hydroxylation-K Validation Sensitivity': 0.7666666666666667, 'Hydroxylation-K Validation Specificity': 0.2543859649122807, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6267446393762184, 'Hydroxylation-K AUC PR': 0.3248956381625106, 'Hydroxylation-K MCC': 0.019744962591969745, 'Hydroxylation-K F1': 0.2769368562472011, 'Validation Loss (Hydroxylation-K)': 0.5667014678319295, 'Hydroxylation-P Validation Accuracy': 0.342256873593557, 'Hydroxylation-P Validation Sensitivity': 0.7695238095238095, 'Hydroxylation-P Validation Specificity': 0.25, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5946279185440116, 'Hydroxylation-P AUC PR': 0.28993240435851925, 'Hydroxylation-P MCC': 0.016197108180951627, 'Hydroxylation-P F1': 0.24739896008390955, 'Validation Loss (Hydroxylation-P)': 0.5359780311584472, 'Validation Loss (total)': 1.1026795069376627, 'TimeToTrain': 0.2240040143330892}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008757337095139607,
 'learning_rate_Hydroxylation-K': 0.00849745601760324,
 'learning_rate_Hydroxylation-P': 0.0023450648757010546,
 'log_base': 2.5080641611271406,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 497917149,
 'sample_weights': [47.14281903457925, 5.880608073229117],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7533581085924648,
 'weight_decay_Hydroxylation-K': 0.5661279985761636,
 'weight_decay_Hydroxylation-P': 7.98773898909157}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.216
[2,     1] loss: 1297.723
[3,     1] loss: 1288.838
[4,     1] loss: 1294.114
[5,     1] loss: 1289.924
[6,     1] loss: 1293.375
[7,     1] loss: 1295.094
[8,     1] loss: 1292.460
[9,     1] loss: 1289.184
[10,     1] loss: 1290.622
[11,     1] loss: 1286.115
[12,     1] loss: 1290.477
[13,     1] loss: 1288.995
[14,     1] loss: 1288.765
[15,     1] loss: 1287.114
[16,     1] loss: 1286.977
[17,     1] loss: 1285.638
[18,     1] loss: 1284.055
[19,     1] loss: 1280.996
[20,     1] loss: 1279.139
[21,     1] loss: 1269.758
[22,     1] loss: 1261.832
[23,     1] loss: 1242.137
[24,     1] loss: 1213.905
[25,     1] loss: 1197.497
[26,     1] loss: 1195.708
[27,     1] loss: 1178.606
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00213557167975614,
 'learning_rate_Hydroxylation-K': 0.004612951504203951,
 'learning_rate_Hydroxylation-P': 0.0004996159482640867,
 'log_base': 2.975596150687488,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3338835507,
 'sample_weights': [1.81557673064915, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.6628577163575144,
 'weight_decay_Hydroxylation-K': 2.2866190115899125,
 'weight_decay_Hydroxylation-P': 9.269249519705953}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.625
[2,     1] loss: 1232.251
[3,     1] loss: 1229.905
[4,     1] loss: 1235.516
[5,     1] loss: 1231.661
[6,     1] loss: 1229.845
[7,     1] loss: 1230.712
[8,     1] loss: 1229.916
[9,     1] loss: 1230.519
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009450252373143642,
 'learning_rate_Hydroxylation-K': 0.0076843135296189415,
 'learning_rate_Hydroxylation-P': 0.003532696122724401,
 'log_base': 1.4778315088729812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2292299250,
 'sample_weights': [1.530975021453657, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.446094887190038,
 'weight_decay_Hydroxylation-K': 4.517246981087703,
 'weight_decay_Hydroxylation-P': 0.13664218911531253}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1816.991
[2,     1] loss: 1806.674
[3,     1] loss: 1823.868
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005973965171381074,
 'learning_rate_Hydroxylation-K': 0.00895099947736233,
 'learning_rate_Hydroxylation-P': 0.008072630644402416,
 'log_base': 1.2682226469005857,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2295235142,
 'sample_weights': [4.274312633788743, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.227403219457889,
 'weight_decay_Hydroxylation-K': 0.6057663599545888,
 'weight_decay_Hydroxylation-P': 3.2113051240053223}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2390.902
[2,     1] loss: 2398.571
[3,     1] loss: 2401.084
[4,     1] loss: 2387.051
[5,     1] loss: 2379.000
[6,     1] loss: 2371.481
[7,     1] loss: 2389.711
[8,     1] loss: 2394.973
[9,     1] loss: 2347.347
[10,     1] loss: 2354.580
[11,     1] loss: 2331.293
[12,     1] loss: 2328.953
[13,     1] loss: 2283.015
[14,     1] loss: 2211.600
[15,     1] loss: 2179.258
[16,     1] loss: 2176.588
[17,     1] loss: 2057.107
[18,     1] loss: 2105.614
[19,     1] loss: 2086.146
[20,     1] loss: 2070.102
[21,     1] loss: 2087.191
[22,     1] loss: 1944.250
[23,     1] loss: 2073.243
[24,     1] loss: 1983.352
[25,     1] loss: 2011.317
[26,     1] loss: 1981.664
[27,     1] loss: 2011.970
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00822361022116392,
 'learning_rate_Hydroxylation-K': 0.009842606553037288,
 'learning_rate_Hydroxylation-P': 0.0027786909849414486,
 'log_base': 1.212271909925418,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 163905096,
 'sample_weights': [7.025790051458863, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.403687610713694,
 'weight_decay_Hydroxylation-K': 4.6854105929247565,
 'weight_decay_Hydroxylation-P': 0.4384373966611809}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2823.787
[2,     1] loss: 2815.001
[3,     1] loss: 2823.529
[4,     1] loss: 2820.969
[5,     1] loss: 2805.824
[6,     1] loss: 2802.219
[7,     1] loss: 2814.802
[8,     1] loss: 2809.118
[9,     1] loss: 2809.016
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005328061924163078,
 'learning_rate_Hydroxylation-K': 0.0003897617786216733,
 'learning_rate_Hydroxylation-P': 0.004856099108861843,
 'log_base': 2.9475894893859107,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3918152606,
 'sample_weights': [8.672602656174995, 1.0841170949569083],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.374505263056553,
 'weight_decay_Hydroxylation-K': 6.811579967550866,
 'weight_decay_Hydroxylation-P': 1.0704637337309202}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.359
[2,     1] loss: 1235.629
[3,     1] loss: 1237.658
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008777894185893174,
 'learning_rate_Hydroxylation-K': 0.004584824080197107,
 'learning_rate_Hydroxylation-P': 0.009002971688576892,
 'log_base': 2.530555079210133,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1193883502,
 'sample_weights': [1.544368289920275, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0806519771323693,
 'weight_decay_Hydroxylation-K': 0.4268662849840701,
 'weight_decay_Hydroxylation-P': 0.25807382696215253}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.403
[2,     1] loss: 1288.810
[3,     1] loss: 1294.226
[4,     1] loss: 1288.419
[5,     1] loss: 1287.729
[6,     1] loss: 1287.246
[7,     1] loss: 1292.311
[8,     1] loss: 1286.967
[9,     1] loss: 1286.992
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00804947138167403,
 'learning_rate_Hydroxylation-K': 0.0036011745119867485,
 'learning_rate_Hydroxylation-P': 0.0008317416275207823,
 'log_base': 2.29177546540214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2038498573,
 'sample_weights': [1.798118914852807, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.13680767197515298,
 'weight_decay_Hydroxylation-K': 5.656089671986029,
 'weight_decay_Hydroxylation-P': 9.730959832797266}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.544
[2,     1] loss: 1338.027
[3,     1] loss: 1335.318
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025492265996927125,
 'learning_rate_Hydroxylation-K': 0.0008662510496625925,
 'learning_rate_Hydroxylation-P': 0.004910019808027045,
 'log_base': 2.915971585189142,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4066321560,
 'sample_weights': [2.013009935166838, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.68894338660276,
 'weight_decay_Hydroxylation-K': 2.8280068346375393,
 'weight_decay_Hydroxylation-P': 4.911542676881636}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.078
[2,     1] loss: 1235.464
[3,     1] loss: 1234.876
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007698536514803473,
 'learning_rate_Hydroxylation-K': 0.008136926154223722,
 'learning_rate_Hydroxylation-P': 0.002567685010507245,
 'log_base': 1.0765671992994414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2784040589,
 'sample_weights': [1.559931189478323, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.845494253117362,
 'weight_decay_Hydroxylation-K': 4.955540853922269,
 'weight_decay_Hydroxylation-P': 0.7543946808345716}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7346.591
[2,     1] loss: 7356.255
[3,     1] loss: 7331.352
[4,     1] loss: 7353.099
[5,     1] loss: 7339.896
[6,     1] loss: 7327.955
[7,     1] loss: 7349.406
[8,     1] loss: 7322.871
[9,     1] loss: 7309.112
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003673916218365471,
 'learning_rate_Hydroxylation-K': 0.0010283794836728347,
 'learning_rate_Hydroxylation-P': 0.009470087280710655,
 'log_base': 2.8369698391582814,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2932770941,
 'sample_weights': [22.628092010018594, 2.8286204668736055],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.443229737047327,
 'weight_decay_Hydroxylation-K': 6.905093585162596,
 'weight_decay_Hydroxylation-P': 0.6113037052427871}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.926
[2,     1] loss: 1247.318
[3,     1] loss: 1243.881
[4,     1] loss: 1253.537
[5,     1] loss: 1247.169
[6,     1] loss: 1240.343
[7,     1] loss: 1246.014
[8,     1] loss: 1244.423
[9,     1] loss: 1239.691
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00951228296814957,
 'learning_rate_Hydroxylation-K': 0.009455276468718396,
 'learning_rate_Hydroxylation-P': 0.00151726988413672,
 'log_base': 1.0553198609353611,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3838564395,
 'sample_weights': [1.601021071150872, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.013011911497595,
 'weight_decay_Hydroxylation-K': 1.1764894735549123,
 'weight_decay_Hydroxylation-P': 0.0721059769366812}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10084.400
[2,     1] loss: 10095.951
[3,     1] loss: 10063.117
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00020505038948338307,
 'learning_rate_Hydroxylation-K': 0.007654955682761486,
 'learning_rate_Hydroxylation-P': 0.0036504804111457756,
 'log_base': 1.439696031607818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 290516363,
 'sample_weights': [31.005238100767116, 3.8758040683805364],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.0642677882390155,
 'weight_decay_Hydroxylation-K': 2.5016316836064605,
 'weight_decay_Hydroxylation-P': 6.335171236345634}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1873.121
[2,     1] loss: 1891.973
[3,     1] loss: 1866.355
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007412459278335718,
 'learning_rate_Hydroxylation-K': 0.003037990291487546,
 'learning_rate_Hydroxylation-P': 0.0030323836742132218,
 'log_base': 2.0381333490861566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1373993087,
 'sample_weights': [4.580945519606187, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6590773223437005,
 'weight_decay_Hydroxylation-K': 1.2675696833176724,
 'weight_decay_Hydroxylation-P': 3.2148341188355167}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.337
[2,     1] loss: 1402.137
[3,     1] loss: 1404.322
[4,     1] loss: 1401.711
[5,     1] loss: 1402.856
[6,     1] loss: 1398.887
[7,     1] loss: 1398.649
[8,     1] loss: 1388.745
[9,     1] loss: 1390.254
[10,     1] loss: 1369.258
[11,     1] loss: 1361.069
[12,     1] loss: 1339.785
[13,     1] loss: 1310.342
[14,     1] loss: 1271.140
[15,     1] loss: 1226.290
[16,     1] loss: 1243.430
[17,     1] loss: 1183.923
[18,     1] loss: 1160.612
[19,     1] loss: 1162.569
[20,     1] loss: 1167.528
[21,     1] loss: 1159.599
[22,     1] loss: 1129.084
[23,     1] loss: 1107.702
[24,     1] loss: 1122.093
[25,     1] loss: 1140.234
[26,     1] loss: 1057.007
[27,     1] loss: 1093.599
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00591643854103773,
 'learning_rate_Hydroxylation-K': 0.0032327411316469615,
 'learning_rate_Hydroxylation-P': 0.0049334840123543,
 'log_base': 1.4342334459602575,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3339777986,
 'sample_weights': [2.3446103612580043, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.532792361543152,
 'weight_decay_Hydroxylation-K': 4.635404298551669,
 'weight_decay_Hydroxylation-P': 8.743579230995756}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1881.802
[2,     1] loss: 1899.080
[3,     1] loss: 1879.734
[4,     1] loss: 1886.453
[5,     1] loss: 1880.332
[6,     1] loss: 1877.350
[7,     1] loss: 1883.304
[8,     1] loss: 1872.501
[9,     1] loss: 1869.122
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009761629801283105,
 'learning_rate_Hydroxylation-K': 0.003553990469702899,
 'learning_rate_Hydroxylation-P': 0.004742560254343586,
 'log_base': 2.3043986499665934,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 75132628,
 'sample_weights': [4.629234197709049, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.023321599593164,
 'weight_decay_Hydroxylation-K': 3.3463507606461453,
 'weight_decay_Hydroxylation-P': 1.1818592608903717}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.194
[2,     1] loss: 1329.673
[3,     1] loss: 1324.780
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005145827573245527,
 'learning_rate_Hydroxylation-K': 0.0012353502184294439,
 'learning_rate_Hydroxylation-P': 0.000624727335495553,
 'log_base': 2.369871323011481,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1388700047,
 'sample_weights': [1.9997647891932644, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.725656172218915,
 'weight_decay_Hydroxylation-K': 1.7685018369845489,
 'weight_decay_Hydroxylation-P': 0.06283117807674538}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.967
[2,     1] loss: 1318.396
[3,     1] loss: 1314.883
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054551073676224505,
 'learning_rate_Hydroxylation-K': 0.00620147882551862,
 'learning_rate_Hydroxylation-P': 0.0011289571347216578,
 'log_base': 1.741819113854724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4259684782,
 'sample_weights': [1.9348332777837136, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.769541912535719,
 'weight_decay_Hydroxylation-K': 2.0140783629167935,
 'weight_decay_Hydroxylation-P': 9.95518586777524}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1541.090
[2,     1] loss: 1538.472
[3,     1] loss: 1543.952
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008781955647365806,
 'learning_rate_Hydroxylation-K': 0.007176318784303103,
 'learning_rate_Hydroxylation-P': 0.00456871640011531,
 'log_base': 2.9329548875638936,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3117102382,
 'sample_weights': [3.0083849181280686, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1313568525895068,
 'weight_decay_Hydroxylation-K': 6.682564810121891,
 'weight_decay_Hydroxylation-P': 5.188960729279632}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.060
[2,     1] loss: 1237.685
[3,     1] loss: 1235.224
[4,     1] loss: 1235.678
[5,     1] loss: 1234.791
[6,     1] loss: 1231.282
[7,     1] loss: 1232.439
[8,     1] loss: 1230.083
[9,     1] loss: 1224.806
[10,     1] loss: 1210.127
[11,     1] loss: 1193.366
[12,     1] loss: 1168.889
[13,     1] loss: 1133.765
[14,     1] loss: 1100.776
[15,     1] loss: 1079.746
[16,     1] loss: 1043.242
[17,     1] loss: 1030.955
[18,     1] loss: 1073.357
[19,     1] loss: 1000.408
[20,     1] loss: 1000.803
[21,     1] loss: 992.949
[22,     1] loss: 1003.432
[23,     1] loss: 1013.365
[24,     1] loss: 983.941
[25,     1] loss: 986.406
[26,     1] loss: 972.424
[27,     1] loss: 945.508
[28,     1] loss: 977.443
[29,     1] loss: 926.292
[30,     1] loss: 946.570
[31,     1] loss: 896.248
[32,     1] loss: 931.771
[33,     1] loss: 899.090
[34,     1] loss: 900.896
[35,     1] loss: 946.137
[36,     1] loss: 917.755
[37,     1] loss: 900.651
[38,     1] loss: 863.740
[39,     1] loss: 928.203
[40,     1] loss: 866.571
[41,     1] loss: 898.263
[42,     1] loss: 857.668
[43,     1] loss: 833.445
[44,     1] loss: 827.056
[45,     1] loss: 804.573
[46,     1] loss: 851.223
[47,     1] loss: 856.774
[48,     1] loss: 809.099
[49,     1] loss: 818.814
[50,     1] loss: 838.410
[51,     1] loss: 818.285
[52,     1] loss: 784.171
[53,     1] loss: 787.088
[54,     1] loss: 755.052
[55,     1] loss: 761.204
[56,     1] loss: 812.937
[57,     1] loss: 783.388
[58,     1] loss: 739.973
[59,     1] loss: 803.913
[60,     1] loss: 758.551
[61,     1] loss: 798.474
[62,     1] loss: 817.092
[63,     1] loss: 811.207
[64,     1] loss: 737.064
[65,     1] loss: 806.999
[66,     1] loss: 781.305
[67,     1] loss: 782.449
[68,     1] loss: 749.775
[69,     1] loss: 764.318
[70,     1] loss: 751.847
[71,     1] loss: 669.675
[72,     1] loss: 762.132
[73,     1] loss: 654.613
[74,     1] loss: 699.032
[75,     1] loss: 661.376
[76,     1] loss: 647.419
[77,     1] loss: 657.770
[78,     1] loss: 631.731
[79,     1] loss: 638.194
[80,     1] loss: 588.886
[81,     1] loss: 628.236
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008913914774053434,
 'learning_rate_Hydroxylation-K': 0.009796025823935075,
 'learning_rate_Hydroxylation-P': 9.878175080152603e-05,
 'log_base': 2.975654800630164,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4005117282,
 'sample_weights': [1.5515120798380242, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.684899641795809,
 'weight_decay_Hydroxylation-K': 8.140611395631472,
 'weight_decay_Hydroxylation-P': 9.58701592289274}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.171
[2,     1] loss: 1227.216
[3,     1] loss: 1231.753
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00420084068515078,
 'learning_rate_Hydroxylation-K': 0.0014173337677524812,
 'learning_rate_Hydroxylation-P': 0.009957906116658186,
 'log_base': 2.248257543783261,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1576705924,
 'sample_weights': [1.530947349103142, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.142599779142028,
 'weight_decay_Hydroxylation-K': 5.326287357300101,
 'weight_decay_Hydroxylation-P': 0.19282331189843113}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1347.688
[2,     1] loss: 1342.696
[3,     1] loss: 1342.773
[4,     1] loss: 1337.972
[5,     1] loss: 1344.079
[6,     1] loss: 1331.998
[7,     1] loss: 1327.390
[8,     1] loss: 1327.088
[9,     1] loss: 1305.769
[10,     1] loss: 1269.066
[11,     1] loss: 1262.584
[12,     1] loss: 1234.409
[13,     1] loss: 1153.954
[14,     1] loss: 1148.406
[15,     1] loss: 1114.488
[16,     1] loss: 1158.957
[17,     1] loss: 1087.715
[18,     1] loss: 1140.751
[19,     1] loss: 1073.801
[20,     1] loss: 1075.088
[21,     1] loss: 1064.619
[22,     1] loss: 1048.334
[23,     1] loss: 1078.688
[24,     1] loss: 990.102
[25,     1] loss: 1007.154
[26,     1] loss: 1045.117
[27,     1] loss: 965.831
[28,     1] loss: 988.101
[29,     1] loss: 980.479
[30,     1] loss: 977.382
[31,     1] loss: 949.445
[32,     1] loss: 899.343
[33,     1] loss: 987.684
[34,     1] loss: 1027.839
[35,     1] loss: 961.432
[36,     1] loss: 881.780
[37,     1] loss: 910.418
[38,     1] loss: 889.202
[39,     1] loss: 889.939
[40,     1] loss: 887.228
[41,     1] loss: 923.078
[42,     1] loss: 830.129
[43,     1] loss: 864.680
[44,     1] loss: 842.528
[45,     1] loss: 777.587
[46,     1] loss: 770.116
[47,     1] loss: 837.685
[48,     1] loss: 978.614
[49,     1] loss: 1003.327
[50,     1] loss: 733.590
[51,     1] loss: 911.793
[52,     1] loss: 827.905
[53,     1] loss: 815.812
[54,     1] loss: 814.742
[55,     1] loss: 751.001
[56,     1] loss: 857.223
[57,     1] loss: 752.887
[58,     1] loss: 762.482
[59,     1] loss: 812.674
[60,     1] loss: 640.822
[61,     1] loss: 656.333
[62,     1] loss: 608.175
[63,     1] loss: 625.650
[64,     1] loss: 595.135
[65,     1] loss: 738.670
[66,     1] loss: 1155.132
[67,     1] loss: 1112.038
[68,     1] loss: 663.065
[69,     1] loss: 914.023
[70,     1] loss: 899.271
[71,     1] loss: 850.429
[72,     1] loss: 799.275
[73,     1] loss: 827.737
[74,     1] loss: 860.715
[75,     1] loss: 705.243
[76,     1] loss: 791.216
[77,     1] loss: 712.855
[78,     1] loss: 724.801
[79,     1] loss: 752.202
[80,     1] loss: 711.752
[81,     1] loss: 648.018
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024864927491357724,
 'learning_rate_Hydroxylation-K': 0.002209548823638002,
 'learning_rate_Hydroxylation-P': 0.007679489256514482,
 'log_base': 1.887671563086506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1048190271,
 'sample_weights': [2.0606453520817207, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.911014103548114,
 'weight_decay_Hydroxylation-K': 5.705664908373053,
 'weight_decay_Hydroxylation-P': 0.1503833185745464}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1463.842
[2,     1] loss: 1461.961
[3,     1] loss: 1460.275
[4,     1] loss: 1462.790
[5,     1] loss: 1460.204
[6,     1] loss: 1456.845
[7,     1] loss: 1453.301
[8,     1] loss: 1452.049
[9,     1] loss: 1447.537
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003968478512419233,
 'learning_rate_Hydroxylation-K': 0.00879538311850863,
 'learning_rate_Hydroxylation-P': 0.0035340566911015534,
 'log_base': 1.2609700594761084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1045399888,
 'sample_weights': [2.627620477622144, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3317885407633536,
 'weight_decay_Hydroxylation-K': 3.321272971631738,
 'weight_decay_Hydroxylation-P': 7.164854085705837}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2429.662
[2,     1] loss: 2419.840
[3,     1] loss: 2419.890
[4,     1] loss: 2416.224
[5,     1] loss: 2440.259
[6,     1] loss: 2420.140
[7,     1] loss: 2411.459
[8,     1] loss: 2422.329
[9,     1] loss: 2413.306
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023239637881528026,
 'learning_rate_Hydroxylation-K': 0.00012131638150649372,
 'learning_rate_Hydroxylation-P': 0.004641062671301426,
 'log_base': 2.7312991342159414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1878092070,
 'sample_weights': [7.199558792425082, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.314887892483677,
 'weight_decay_Hydroxylation-K': 3.8227907312189866,
 'weight_decay_Hydroxylation-P': 0.6367347370427715}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.839
[2,     1] loss: 1262.617
[3,     1] loss: 1259.019
[4,     1] loss: 1262.669
[5,     1] loss: 1262.131
[6,     1] loss: 1257.847
[7,     1] loss: 1259.367
[8,     1] loss: 1259.872
[9,     1] loss: 1255.328
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002092960422723383,
 'learning_rate_Hydroxylation-K': 0.004274628734195828,
 'learning_rate_Hydroxylation-P': 0.003359302397623258,
 'log_base': 2.936506433281714,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2811244945,
 'sample_weights': [1.6615055218781791, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.844510528731924,
 'weight_decay_Hydroxylation-K': 1.6071628386587546,
 'weight_decay_Hydroxylation-P': 0.3870310272108797}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.698
[2,     1] loss: 1233.359
[3,     1] loss: 1232.803
[4,     1] loss: 1230.786
[5,     1] loss: 1230.318
[6,     1] loss: 1229.432
[7,     1] loss: 1222.963
[8,     1] loss: 1216.430
[9,     1] loss: 1187.350
[10,     1] loss: 1178.933
[11,     1] loss: 1146.157
[12,     1] loss: 1127.968
[13,     1] loss: 1089.940
[14,     1] loss: 1101.949
[15,     1] loss: 1067.420
[16,     1] loss: 1033.023
[17,     1] loss: 1039.161
[18,     1] loss: 1010.703
[19,     1] loss: 1022.084
[20,     1] loss: 1046.823
[21,     1] loss: 988.241
[22,     1] loss: 973.231
[23,     1] loss: 991.674
[24,     1] loss: 995.033
[25,     1] loss: 965.996
[26,     1] loss: 1005.055
[27,     1] loss: 991.532
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004290437351040044,
 'learning_rate_Hydroxylation-K': 0.00027610866656623104,
 'learning_rate_Hydroxylation-P': 0.006200625120244409,
 'log_base': 2.5342515398625665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2478479730,
 'sample_weights': [1.5497690704355016, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.033037238500999,
 'weight_decay_Hydroxylation-K': 2.65862955677141,
 'weight_decay_Hydroxylation-P': 2.623634172388024}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.260
[2,     1] loss: 1286.239
[3,     1] loss: 1283.362
[4,     1] loss: 1284.678
[5,     1] loss: 1283.737
[6,     1] loss: 1279.049
[7,     1] loss: 1274.515
[8,     1] loss: 1252.503
[9,     1] loss: 1227.750
[10,     1] loss: 1227.119
[11,     1] loss: 1152.783
[12,     1] loss: 1172.145
[13,     1] loss: 1092.607
[14,     1] loss: 1141.333
[15,     1] loss: 1053.052
[16,     1] loss: 1099.898
[17,     1] loss: 1079.651
[18,     1] loss: 1050.383
[19,     1] loss: 1068.858
[20,     1] loss: 1079.495
[21,     1] loss: 1042.647
[22,     1] loss: 1032.796
[23,     1] loss: 1021.413
[24,     1] loss: 1023.690
[25,     1] loss: 1007.247
[26,     1] loss: 979.506
[27,     1] loss: 959.033
[28,     1] loss: 950.739
[29,     1] loss: 948.647
[30,     1] loss: 963.729
[31,     1] loss: 1003.684
[32,     1] loss: 901.394
[33,     1] loss: 869.763
[34,     1] loss: 879.591
[35,     1] loss: 921.834
[36,     1] loss: 970.220
[37,     1] loss: 1147.846
[38,     1] loss: 882.123
[39,     1] loss: 1066.225
[40,     1] loss: 943.225
[41,     1] loss: 966.029
[42,     1] loss: 954.667
[43,     1] loss: 984.631
[44,     1] loss: 906.347
[45,     1] loss: 964.200
[46,     1] loss: 893.070
[47,     1] loss: 885.621
[48,     1] loss: 934.916
[49,     1] loss: 831.080
[50,     1] loss: 962.234
[51,     1] loss: 869.442
[52,     1] loss: 866.188
[53,     1] loss: 866.053
[54,     1] loss: 809.357
[55,     1] loss: 834.027
[56,     1] loss: 799.644
[57,     1] loss: 785.195
[58,     1] loss: 738.130
[59,     1] loss: 726.726
[60,     1] loss: 741.146
[61,     1] loss: 790.160
[62,     1] loss: 791.970
[63,     1] loss: 859.553
[64,     1] loss: 716.653
[65,     1] loss: 805.619
[66,     1] loss: 757.911
[67,     1] loss: 719.174
[68,     1] loss: 741.489
[69,     1] loss: 649.126
[70,     1] loss: 629.670
[71,     1] loss: 619.459
[72,     1] loss: 746.644
[73,     1] loss: 1169.288
[74,     1] loss: 1105.893
[75,     1] loss: 1031.027
[76,     1] loss: 930.294
[77,     1] loss: 989.034
[78,     1] loss: 1003.839
[79,     1] loss: 1032.723
[80,     1] loss: 998.671
[81,     1] loss: 945.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005536381683160844,
 'learning_rate_Hydroxylation-K': 0.0033150082454661924,
 'learning_rate_Hydroxylation-P': 0.0007816027165762209,
 'log_base': 1.5086986203647565,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3869614215,
 'sample_weights': [1.7952964000514897, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.162408397676601,
 'weight_decay_Hydroxylation-K': 3.559429510775453,
 'weight_decay_Hydroxylation-P': 8.75353549005314}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1774.482
[2,     1] loss: 1766.376
[3,     1] loss: 1765.032
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009200406677637775,
 'learning_rate_Hydroxylation-K': 0.006906011922949149,
 'learning_rate_Hydroxylation-P': 0.002017777278331103,
 'log_base': 1.147295612099724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2513059053,
 'sample_weights': [4.059461510268124, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.134347666803306,
 'weight_decay_Hydroxylation-K': 3.726274460708321,
 'weight_decay_Hydroxylation-P': 1.1334326891375812}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3961.227
[2,     1] loss: 3942.027
[3,     1] loss: 3963.639
[4,     1] loss: 3968.923
[5,     1] loss: 3935.004
[6,     1] loss: 3916.032
[7,     1] loss: 3937.361
[8,     1] loss: 3942.316
[9,     1] loss: 3918.816
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003489006652440001,
 'learning_rate_Hydroxylation-K': 0.0030835637790223277,
 'learning_rate_Hydroxylation-P': 0.009559590211813237,
 'log_base': 2.0880407825263667,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2433635825,
 'sample_weights': [12.149575296082471, 1.518755418313883],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.599289465058117,
 'weight_decay_Hydroxylation-K': 5.836836714249432,
 'weight_decay_Hydroxylation-P': 1.5333143012443768}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.474
[2,     1] loss: 1385.473
[3,     1] loss: 1386.531
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004781220384503792,
 'learning_rate_Hydroxylation-K': 0.0019332682502458736,
 'learning_rate_Hydroxylation-P': 0.006087538014319431,
 'log_base': 2.6194045558347137,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1416013219,
 'sample_weights': [2.2675682332269114, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.4544485161468454,
 'weight_decay_Hydroxylation-K': 2.22102536372642,
 'weight_decay_Hydroxylation-P': 0.1275027582062211}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.502
[2,     1] loss: 1272.340
[3,     1] loss: 1270.470
[4,     1] loss: 1270.712
[5,     1] loss: 1262.750
[6,     1] loss: 1248.606
[7,     1] loss: 1228.506
[8,     1] loss: 1156.441
[9,     1] loss: 1116.399
[10,     1] loss: 1126.225
[11,     1] loss: 1095.769
[12,     1] loss: 1068.442
[13,     1] loss: 1068.547
[14,     1] loss: 1030.331
[15,     1] loss: 1020.871
[16,     1] loss: 1044.635
[17,     1] loss: 1004.054
[18,     1] loss: 978.308
[19,     1] loss: 1013.346
[20,     1] loss: 1014.512
[21,     1] loss: 1028.018
[22,     1] loss: 927.251
[23,     1] loss: 992.220
[24,     1] loss: 977.319
[25,     1] loss: 983.912
[26,     1] loss: 963.011
[27,     1] loss: 963.277
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012621415484760192,
 'learning_rate_Hydroxylation-K': 0.0041329329151207714,
 'learning_rate_Hydroxylation-P': 0.00927235147738599,
 'log_base': 1.2887550990027434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 493511939,
 'sample_weights': [1.7336811966184178, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.334059601052107,
 'weight_decay_Hydroxylation-K': 7.003421601281497,
 'weight_decay_Hydroxylation-P': 3.577698332643732}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2295.929
[2,     1] loss: 2301.708
[3,     1] loss: 2287.172
[4,     1] loss: 2293.912
[5,     1] loss: 2299.781
[6,     1] loss: 2298.099
[7,     1] loss: 2289.357
[8,     1] loss: 2291.047
[9,     1] loss: 2276.697
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007905694216464953,
 'learning_rate_Hydroxylation-K': 0.001703685022177214,
 'learning_rate_Hydroxylation-P': 0.005664957068587526,
 'log_base': 2.9899586073346986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3990908593,
 'sample_weights': [6.580986990891461, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.482613526075427,
 'weight_decay_Hydroxylation-K': 0.2845624444841133,
 'weight_decay_Hydroxylation-P': 6.494153221860235}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.340
[2,     1] loss: 1234.673
[3,     1] loss: 1222.804
[4,     1] loss: 1232.060
[5,     1] loss: 1232.215
[6,     1] loss: 1229.562
[7,     1] loss: 1228.070
[8,     1] loss: 1228.315
[9,     1] loss: 1224.754
[10,     1] loss: 1218.953
[11,     1] loss: 1210.137
[12,     1] loss: 1190.792
[13,     1] loss: 1154.947
[14,     1] loss: 1104.309
[15,     1] loss: 1083.347
[16,     1] loss: 1078.826
[17,     1] loss: 1105.543
[18,     1] loss: 1028.083
[19,     1] loss: 1076.345
[20,     1] loss: 1003.060
[21,     1] loss: 1036.479
[22,     1] loss: 1005.336
[23,     1] loss: 1014.825
[24,     1] loss: 1006.752
[25,     1] loss: 928.047
[26,     1] loss: 967.080
[27,     1] loss: 922.425
[28,     1] loss: 908.039
[29,     1] loss: 896.128
[30,     1] loss: 926.376
[31,     1] loss: 911.713
[32,     1] loss: 926.920
[33,     1] loss: 927.412
[34,     1] loss: 873.128
[35,     1] loss: 936.106
[36,     1] loss: 893.253
[37,     1] loss: 867.970
[38,     1] loss: 853.084
[39,     1] loss: 799.888
[40,     1] loss: 905.035
[41,     1] loss: 935.597
[42,     1] loss: 857.917
[43,     1] loss: 873.389
[44,     1] loss: 810.890
[45,     1] loss: 884.952
[46,     1] loss: 817.649
[47,     1] loss: 801.395
[48,     1] loss: 760.603
[49,     1] loss: 758.115
[50,     1] loss: 848.083
[51,     1] loss: 966.941
[52,     1] loss: 757.748
[53,     1] loss: 992.263
[54,     1] loss: 802.185
[55,     1] loss: 935.027
[56,     1] loss: 823.226
[57,     1] loss: 825.093
[58,     1] loss: 748.211
[59,     1] loss: 798.520
[60,     1] loss: 769.582
[61,     1] loss: 782.479
[62,     1] loss: 732.091
[63,     1] loss: 792.115
[64,     1] loss: 673.874
[65,     1] loss: 655.255
[66,     1] loss: 646.826
[67,     1] loss: 814.719
[68,     1] loss: 1487.233
[69,     1] loss: 875.863
[70,     1] loss: 963.227
[71,     1] loss: 1050.712
[72,     1] loss: 978.621
[73,     1] loss: 996.336
[74,     1] loss: 981.879
[75,     1] loss: 863.015
[76,     1] loss: 923.270
[77,     1] loss: 940.368
[78,     1] loss: 999.672
[79,     1] loss: 869.800
[80,     1] loss: 938.260
[81,     1] loss: 889.740
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008667876635502175,
 'learning_rate_Hydroxylation-K': 0.00495336073709862,
 'learning_rate_Hydroxylation-P': 0.008176757481038788,
 'log_base': 2.1923501256027143,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1225480590,
 'sample_weights': [1.5242443283600586, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.591988316759471,
 'weight_decay_Hydroxylation-K': 4.8060133783640255,
 'weight_decay_Hydroxylation-P': 2.117176990483638}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1358.326
[2,     1] loss: 1356.844
[3,     1] loss: 1360.598
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005512571035400004,
 'learning_rate_Hydroxylation-K': 0.0008985992530463789,
 'learning_rate_Hydroxylation-P': 0.006561909982259079,
 'log_base': 2.319067701477703,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3877667692,
 'sample_weights': [2.1267493780705413, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.487375273828794,
 'weight_decay_Hydroxylation-K': 3.0942728284538754,
 'weight_decay_Hydroxylation-P': 1.989368062352372}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.881
[2,     1] loss: 1332.061
[3,     1] loss: 1331.698
[4,     1] loss: 1318.979
[5,     1] loss: 1325.266
[6,     1] loss: 1328.834
[7,     1] loss: 1323.012
[8,     1] loss: 1314.515
[9,     1] loss: 1312.632
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004631844628040644,
 'learning_rate_Hydroxylation-K': 0.0035770213359135354,
 'learning_rate_Hydroxylation-P': 0.002215234357046973,
 'log_base': 2.2605339609494512,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1311512257,
 'sample_weights': [1.9846791617355484, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.468754750563924,
 'weight_decay_Hydroxylation-K': 7.795545389551505,
 'weight_decay_Hydroxylation-P': 1.0769448260611467}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.933
[2,     1] loss: 1343.111
[3,     1] loss: 1339.484
[4,     1] loss: 1336.707
[5,     1] loss: 1333.949
[6,     1] loss: 1323.137
[7,     1] loss: 1302.451
[8,     1] loss: 1268.888
[9,     1] loss: 1245.104
[10,     1] loss: 1210.102
[11,     1] loss: 1173.382
[12,     1] loss: 1100.193
[13,     1] loss: 1086.358
[14,     1] loss: 1116.522
[15,     1] loss: 1033.320
[16,     1] loss: 1066.297
[17,     1] loss: 1043.985
[18,     1] loss: 1076.050
[19,     1] loss: 1008.201
[20,     1] loss: 1067.141
[21,     1] loss: 998.511
[22,     1] loss: 971.255
[23,     1] loss: 969.724
[24,     1] loss: 1110.444
[25,     1] loss: 1133.222
[26,     1] loss: 980.503
[27,     1] loss: 1063.335
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009126503369697161,
 'learning_rate_Hydroxylation-K': 0.008099331434717052,
 'learning_rate_Hydroxylation-P': 0.008820642506393939,
 'log_base': 2.0013876578420913,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3524514978,
 'sample_weights': [2.0468869489490786, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.616732332209405,
 'weight_decay_Hydroxylation-K': 4.100642593625858,
 'weight_decay_Hydroxylation-P': 9.576252766027899}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.690
[2,     1] loss: 1437.215
[3,     1] loss: 1419.501
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008597512254183787,
 'learning_rate_Hydroxylation-K': 0.009783393373259463,
 'learning_rate_Hydroxylation-P': 2.598250566337632e-05,
 'log_base': 1.108502518592796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4153496855,
 'sample_weights': [2.4060897287532614, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.866837668862231,
 'weight_decay_Hydroxylation-K': 4.803676217549409,
 'weight_decay_Hydroxylation-P': 3.619309851210216}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5283.597
[2,     1] loss: 5257.102
[3,     1] loss: 5256.998
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004043439672397365,
 'learning_rate_Hydroxylation-K': 0.0014797941285528422,
 'learning_rate_Hydroxylation-P': 0.00770812269957217,
 'log_base': 1.968993596611979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1861311534,
 'sample_weights': [16.206608955789186, 2.0259041624307366],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5238472740136397,
 'weight_decay_Hydroxylation-K': 3.3031746453230313,
 'weight_decay_Hydroxylation-P': 1.2218022183910018}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1427.085
[2,     1] loss: 1433.987
[3,     1] loss: 1430.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008933003795687358,
 'learning_rate_Hydroxylation-K': 0.009955287736055323,
 'learning_rate_Hydroxylation-P': 0.0012697510809476862,
 'log_base': 1.0646026224961755,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 912630327,
 'sample_weights': [2.4640407222760157, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.783843898687834,
 'weight_decay_Hydroxylation-K': 1.7121794020437928,
 'weight_decay_Hydroxylation-P': 0.8366003101008991}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8659.892
[2,     1] loss: 8627.471
[3,     1] loss: 8720.018
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004806599251103329,
 'learning_rate_Hydroxylation-K': 0.00690354988991436,
 'learning_rate_Hydroxylation-P': 0.004757418027821044,
 'log_base': 2.9402473304137975,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1425365309,
 'sample_weights': [26.667737102253483, 3.3335955563218005],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.06160123522829908,
 'weight_decay_Hydroxylation-K': 2.0941023033359736,
 'weight_decay_Hydroxylation-P': 3.9892457799073195}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.713
[2,     1] loss: 1234.712
[3,     1] loss: 1232.391
[4,     1] loss: 1230.872
[5,     1] loss: 1226.606
[6,     1] loss: 1220.326
[7,     1] loss: 1203.277
[8,     1] loss: 1181.250
[9,     1] loss: 1129.979
[10,     1] loss: 1098.911
[11,     1] loss: 1058.514
[12,     1] loss: 1011.709
[13,     1] loss: 1005.403
[14,     1] loss: 1035.166
[15,     1] loss: 994.041
[16,     1] loss: 926.708
[17,     1] loss: 1013.712
[18,     1] loss: 1033.241
[19,     1] loss: 976.797
[20,     1] loss: 1005.249
[21,     1] loss: 953.165
[22,     1] loss: 962.285
[23,     1] loss: 955.401
[24,     1] loss: 944.094
[25,     1] loss: 969.779
[26,     1] loss: 932.166
[27,     1] loss: 955.571
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00978215800068978,
 'learning_rate_Hydroxylation-K': 0.00987740250255447,
 'learning_rate_Hydroxylation-P': 0.000997756425903823,
 'log_base': 1.0728308230888444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2155652001,
 'sample_weights': [1.5479396324117765, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.443390305428229,
 'weight_decay_Hydroxylation-K': 4.288030011897896,
 'weight_decay_Hydroxylation-P': 1.4204158278761807}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7762.161
[2,     1] loss: 7749.073
[3,     1] loss: 7739.292
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003623000170252031,
 'learning_rate_Hydroxylation-K': 0.002108674785334797,
 'learning_rate_Hydroxylation-P': 0.009568393747534735,
 'log_base': 2.767654164230822,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3146384607,
 'sample_weights': [23.74714835505077, 2.968507898820643],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.544848821187797,
 'weight_decay_Hydroxylation-K': 7.13206055041611,
 'weight_decay_Hydroxylation-P': 1.2387483427016013}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.274
[2,     1] loss: 1251.855
[3,     1] loss: 1254.016
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006420387551172076,
 'learning_rate_Hydroxylation-K': 0.0004255590027000357,
 'learning_rate_Hydroxylation-P': 0.008057565494014889,
 'log_base': 2.919070916925032,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2871028175,
 'sample_weights': [1.6399243624838398, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.880755169629538,
 'weight_decay_Hydroxylation-K': 9.339624832294435,
 'weight_decay_Hydroxylation-P': 1.5759551449110403}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.464
[2,     1] loss: 1239.430
[3,     1] loss: 1243.187
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006529169611336387,
 'learning_rate_Hydroxylation-K': 0.0008105031129767359,
 'learning_rate_Hydroxylation-P': 0.00924486075131148,
 'log_base': 1.9169077711658735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1075525165,
 'sample_weights': [1.5583842887468995, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.791176945328113,
 'weight_decay_Hydroxylation-K': 4.6312709555397,
 'weight_decay_Hydroxylation-P': 0.05388546627619514}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.863
[2,     1] loss: 1453.863
[3,     1] loss: 1452.097
[4,     1] loss: 1452.865
[5,     1] loss: 1445.736
[6,     1] loss: 1448.083
[7,     1] loss: 1449.200
[8,     1] loss: 1448.470
[9,     1] loss: 1447.254
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026308113063546103,
 'learning_rate_Hydroxylation-K': 0.008126203618997713,
 'learning_rate_Hydroxylation-P': 0.0064842416899836865,
 'log_base': 1.9349989569224386,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2118548762,
 'sample_weights': [2.5655584637942987, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.169970035839176,
 'weight_decay_Hydroxylation-K': 2.569368544009561,
 'weight_decay_Hydroxylation-P': 0.7908783011692915}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1441.711
[2,     1] loss: 1443.499
[3,     1] loss: 1443.958
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004601878740605357,
 'learning_rate_Hydroxylation-K': 0.004354654760216629,
 'learning_rate_Hydroxylation-P': 0.008499605936129816,
 'log_base': 1.6577985664152073,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 168347953,
 'sample_weights': [2.5290501161788543, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.306676449490647,
 'weight_decay_Hydroxylation-K': 1.1488045522469026,
 'weight_decay_Hydroxylation-P': 6.125051784861939}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1611.567
[2,     1] loss: 1604.868
[3,     1] loss: 1605.069
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009697804812412003,
 'learning_rate_Hydroxylation-K': 0.009440129461665564,
 'learning_rate_Hydroxylation-P': 0.0001655185900877989,
 'log_base': 1.4249314142425116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1821362961,
 'sample_weights': [3.3026198470109263, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.800583771307669,
 'weight_decay_Hydroxylation-K': 2.546355572891036,
 'weight_decay_Hydroxylation-P': 3.2306094065915305}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1908.877
[2,     1] loss: 1926.134
[3,     1] loss: 1907.272
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004462211090596666,
 'learning_rate_Hydroxylation-K': 0.00388053308131451,
 'learning_rate_Hydroxylation-P': 0.0004692672929422293,
 'log_base': 2.7671961162166414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2617945309,
 'sample_weights': [4.714293991927049, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.710714525227312,
 'weight_decay_Hydroxylation-K': 1.2920084803511456,
 'weight_decay_Hydroxylation-P': 0.3798810775369219}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.615
[2,     1] loss: 1256.333
[3,     1] loss: 1254.162
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005761528628113844,
 'learning_rate_Hydroxylation-K': 0.002579521557280788,
 'learning_rate_Hydroxylation-P': 0.009138243504701573,
 'log_base': 2.0440503730492554,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4196951956,
 'sample_weights': [1.6401910371388946, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.289570053557622,
 'weight_decay_Hydroxylation-K': 7.390194623084064,
 'weight_decay_Hydroxylation-P': 2.235697701772357}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.856
[2,     1] loss: 1400.116
[3,     1] loss: 1405.331
[4,     1] loss: 1398.749
[5,     1] loss: 1399.468
[6,     1] loss: 1395.647
[7,     1] loss: 1393.771
[8,     1] loss: 1396.911
[9,     1] loss: 1381.841
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004702113151352556,
 'learning_rate_Hydroxylation-K': 0.0030391603208811345,
 'learning_rate_Hydroxylation-P': 0.009883450255109227,
 'log_base': 1.33861771915681,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 90321231,
 'sample_weights': [2.335103301467336, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.873261811436276,
 'weight_decay_Hydroxylation-K': 8.40245786024812,
 'weight_decay_Hydroxylation-P': 2.9961584234691543}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2124.277
[2,     1] loss: 2111.008
[3,     1] loss: 2119.208
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008280602988801363,
 'learning_rate_Hydroxylation-K': 0.00809582118583939,
 'learning_rate_Hydroxylation-P': 0.0002318147782016072,
 'log_base': 1.0905206104968934,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1270091095,
 'sample_weights': [5.7243769517307515, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.773094779550572,
 'weight_decay_Hydroxylation-K': 4.584721144344908,
 'weight_decay_Hydroxylation-P': 1.0800456932987736}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6319.756
[2,     1] loss: 6323.276
[3,     1] loss: 6260.887
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00018509155575809934,
 'learning_rate_Hydroxylation-K': 0.0008048270681089396,
 'learning_rate_Hydroxylation-P': 0.008173294698459252,
 'log_base': 2.1047472781368124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1137113179,
 'sample_weights': [19.26535305137569, 2.408261903767218],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.608578872216231,
 'weight_decay_Hydroxylation-K': 0.2433244575305249,
 'weight_decay_Hydroxylation-P': 0.3225022866860492}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1382.483
[2,     1] loss: 1384.795
[3,     1] loss: 1381.872
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008062882541648072,
 'learning_rate_Hydroxylation-K': 0.0031253026001056153,
 'learning_rate_Hydroxylation-P': 0.0032921756536575864,
 'log_base': 2.8175327296291917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1189163381,
 'sample_weights': [2.2432860279162616, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7448915680076107,
 'weight_decay_Hydroxylation-K': 0.6789833944479544,
 'weight_decay_Hydroxylation-P': 9.144734817547572}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.779
[2,     1] loss: 1274.204
[3,     1] loss: 1249.501
[4,     1] loss: 1249.360
[5,     1] loss: 1253.731
[6,     1] loss: 1248.920
[7,     1] loss: 1246.380
[8,     1] loss: 1247.872
[9,     1] loss: 1250.700
[10,     1] loss: 1247.210
[11,     1] loss: 1248.005
[12,     1] loss: 1247.705
[13,     1] loss: 1248.859
[14,     1] loss: 1247.529
[15,     1] loss: 1247.621
[16,     1] loss: 1247.461
[17,     1] loss: 1248.501
[18,     1] loss: 1247.162
[19,     1] loss: 1247.596
[20,     1] loss: 1247.378
[21,     1] loss: 1249.066
[22,     1] loss: 1247.488
[23,     1] loss: 1246.284
[24,     1] loss: 1247.956
[25,     1] loss: 1247.052
[26,     1] loss: 1247.004
[27,     1] loss: 1247.202
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007428944695305034,
 'learning_rate_Hydroxylation-K': 0.00830959068185829,
 'learning_rate_Hydroxylation-P': 0.0011694530277848374,
 'log_base': 1.0697967114715914,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2376766211,
 'sample_weights': [1.6116469363804742, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.481168810080996,
 'weight_decay_Hydroxylation-K': 8.346034410290223,
 'weight_decay_Hydroxylation-P': 1.5725164385021255}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8056.152
[2,     1] loss: 8079.418
[3,     1] loss: 8030.784
[4,     1] loss: 8050.224
[5,     1] loss: 8050.680
[6,     1] loss: 8038.605
[7,     1] loss: 8018.334
[8,     1] loss: 8020.432
[9,     1] loss: 8001.670
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061557688607867065,
 'learning_rate_Hydroxylation-K': 0.0010564243811185732,
 'learning_rate_Hydroxylation-P': 0.00289517885532269,
 'log_base': 2.951937908476186,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1966712433,
 'sample_weights': [24.74398652618094, 3.0931174704880746],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1948989218072885,
 'weight_decay_Hydroxylation-K': 5.740867288280652,
 'weight_decay_Hydroxylation-P': 4.160240962437809}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.747
[2,     1] loss: 1235.177
[3,     1] loss: 1245.668
[4,     1] loss: 1230.640
[5,     1] loss: 1232.078
[6,     1] loss: 1232.715
[7,     1] loss: 1229.310
[8,     1] loss: 1226.634
[9,     1] loss: 1224.232
[10,     1] loss: 1212.029
[11,     1] loss: 1195.490
[12,     1] loss: 1163.409
[13,     1] loss: 1126.266
[14,     1] loss: 1096.991
[15,     1] loss: 1060.930
[16,     1] loss: 1049.698
[17,     1] loss: 1106.638
[18,     1] loss: 1016.509
[19,     1] loss: 1115.922
[20,     1] loss: 1050.993
[21,     1] loss: 1033.593
[22,     1] loss: 1067.988
[23,     1] loss: 1013.757
[24,     1] loss: 1060.625
[25,     1] loss: 1029.997
[26,     1] loss: 986.076
[27,     1] loss: 1055.525
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0044003530016498114,
 'learning_rate_Hydroxylation-K': 0.009748360158401451,
 'learning_rate_Hydroxylation-P': 0.0033488816688113797,
 'log_base': 1.0592947346084942,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 533377434,
 'sample_weights': [1.5422650805249392, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.746727538571585,
 'weight_decay_Hydroxylation-K': 0.1352221364090116,
 'weight_decay_Hydroxylation-P': 5.318313923469531}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9400.535
[2,     1] loss: 9360.144
[3,     1] loss: 9431.764
[4,     1] loss: 9301.582
[5,     1] loss: 9345.896
[6,     1] loss: 9257.244
[7,     1] loss: 9206.192
[8,     1] loss: 9047.703
[9,     1] loss: 8530.854
[10,     1] loss: 8317.781
[11,     1] loss: 8071.491
[12,     1] loss: 8190.277
[13,     1] loss: 8191.623
[14,     1] loss: 8040.529
[15,     1] loss: 7585.710
[16,     1] loss: 7948.804
[17,     1] loss: 7593.530
[18,     1] loss: 7159.288
[19,     1] loss: 7726.980
[20,     1] loss: 7383.727
[21,     1] loss: 6905.844
[22,     1] loss: 7417.768
[23,     1] loss: 6514.496
[24,     1] loss: 6748.444
[25,     1] loss: 6585.399
[26,     1] loss: 8839.296
[27,     1] loss: 6319.961
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000573460949896889,
 'learning_rate_Hydroxylation-K': 0.00041006985387830004,
 'learning_rate_Hydroxylation-P': 0.006946366248953458,
 'log_base': 2.9553455421757198,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1919171281,
 'sample_weights': [28.98170644593036, 3.622852867205305],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.799650228985962,
 'weight_decay_Hydroxylation-K': 9.65941583333173,
 'weight_decay_Hydroxylation-P': 0.05230560756410882}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.803
[2,     1] loss: 1235.029
[3,     1] loss: 1238.507
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011561940191450073,
 'learning_rate_Hydroxylation-K': 0.0005775184924346898,
 'learning_rate_Hydroxylation-P': 0.0004226027109118515,
 'log_base': 2.301719583418903,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927289016,
 'sample_weights': [1.5406230587063618, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.940124668563238,
 'weight_decay_Hydroxylation-K': 0.3300579852084667,
 'weight_decay_Hydroxylation-P': 0.8728385664618582}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.406
[2,     1] loss: 1326.621
[3,     1] loss: 1327.298
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014169027828110298,
 'learning_rate_Hydroxylation-K': 0.0012105435167833203,
 'learning_rate_Hydroxylation-P': 0.009210935124219118,
 'log_base': 2.7883786068400074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 555868144,
 'sample_weights': [2.0025552135055324, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.466583004105825,
 'weight_decay_Hydroxylation-K': 6.959205417846722,
 'weight_decay_Hydroxylation-P': 0.6318786765676806}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.002
[2,     1] loss: 1251.939
[3,     1] loss: 1255.979
[4,     1] loss: 1253.270
[5,     1] loss: 1250.574
[6,     1] loss: 1249.442
[7,     1] loss: 1246.797
[8,     1] loss: 1247.400
[9,     1] loss: 1242.413
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004294327972788822,
 'learning_rate_Hydroxylation-K': 0.00631297611455384,
 'learning_rate_Hydroxylation-P': 0.0034377037113681105,
 'log_base': 2.2887778370869984,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 294555237,
 'sample_weights': [1.6279939614179257, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7150859131744394,
 'weight_decay_Hydroxylation-K': 7.313056702745503,
 'weight_decay_Hydroxylation-P': 3.4713047046435688}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.800
[2,     1] loss: 1335.017
[3,     1] loss: 1342.985
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005810682452787981,
 'learning_rate_Hydroxylation-K': 0.00054584947039811,
 'learning_rate_Hydroxylation-P': 0.002210920288421421,
 'log_base': 2.9395654293825397,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1033102185,
 'sample_weights': [2.0161919049416777, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.712497886752956,
 'weight_decay_Hydroxylation-K': 1.3449847434242757,
 'weight_decay_Hydroxylation-P': 1.9512106776452862}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.211
[2,     1] loss: 1236.070
[3,     1] loss: 1231.521
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008778886700706251,
 'learning_rate_Hydroxylation-K': 0.004633781720727666,
 'learning_rate_Hydroxylation-P': 0.008450929819756483,
 'log_base': 2.3399207086197586,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1774350980,
 'sample_weights': [1.548272612041394, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.7891866767664,
 'weight_decay_Hydroxylation-K': 2.051776006582962,
 'weight_decay_Hydroxylation-P': 7.970761454649063}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.379
[2,     1] loss: 1320.779
[3,     1] loss: 1325.299
[4,     1] loss: 1318.745
[5,     1] loss: 1317.148
[6,     1] loss: 1319.429
[7,     1] loss: 1308.954
[8,     1] loss: 1294.445
[9,     1] loss: 1267.128
[10,     1] loss: 1229.939
[11,     1] loss: 1267.133
[12,     1] loss: 1199.646
[13,     1] loss: 1124.701
[14,     1] loss: 1130.386
[15,     1] loss: 1104.801
[16,     1] loss: 1085.363
[17,     1] loss: 1118.269
[18,     1] loss: 1093.777
[19,     1] loss: 1066.965
[20,     1] loss: 1064.202
[21,     1] loss: 1094.915
[22,     1] loss: 1038.427
[23,     1] loss: 1018.894
[24,     1] loss: 1054.520
[25,     1] loss: 1035.641
[26,     1] loss: 1006.532
[27,     1] loss: 1039.037
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004941457006829785,
 'learning_rate_Hydroxylation-K': 0.0002174036854990407,
 'learning_rate_Hydroxylation-P': 0.009529976653017563,
 'log_base': 2.4930460355313713,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1500950997,
 'sample_weights': [1.9637803523773456, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.00163898562067,
 'weight_decay_Hydroxylation-K': 4.239572760183728,
 'weight_decay_Hydroxylation-P': 0.1725071404203561}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.874
[2,     1] loss: 1294.132
[3,     1] loss: 1292.280
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008968475709528126,
 'learning_rate_Hydroxylation-K': 0.009568663044770442,
 'learning_rate_Hydroxylation-P': 0.0013474888951059195,
 'log_base': 1.1802909210960524,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1354771847,
 'sample_weights': [1.8275134273689166, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.74273268263989,
 'weight_decay_Hydroxylation-K': 4.166334185464772,
 'weight_decay_Hydroxylation-P': 0.9442496693856989}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3267.047
[2,     1] loss: 3306.259
[3,     1] loss: 3259.003
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008881725932833682,
 'learning_rate_Hydroxylation-K': 0.0053970841849724505,
 'learning_rate_Hydroxylation-P': 0.00294813659880737,
 'log_base': 2.8694301911205233,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2197290546,
 'sample_weights': [10.071389753733854, 1.2589722180137666],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.819576544013262,
 'weight_decay_Hydroxylation-K': 1.1890799491649955,
 'weight_decay_Hydroxylation-P': 0.7172337014197927}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.024
[2,     1] loss: 1240.072
[3,     1] loss: 1244.543
[4,     1] loss: 1237.277
[5,     1] loss: 1248.532
[6,     1] loss: 1245.161
[7,     1] loss: 1235.369
[8,     1] loss: 1234.067
[9,     1] loss: 1226.227
[10,     1] loss: 1210.882
[11,     1] loss: 1182.859
[12,     1] loss: 1156.073
[13,     1] loss: 1132.160
[14,     1] loss: 1102.993
[15,     1] loss: 1058.512
[16,     1] loss: 1050.597
[17,     1] loss: 1022.383
[18,     1] loss: 1026.690
[19,     1] loss: 1013.572
[20,     1] loss: 990.430
[21,     1] loss: 965.585
[22,     1] loss: 978.089
[23,     1] loss: 986.113
[24,     1] loss: 971.099
[25,     1] loss: 933.789
[26,     1] loss: 902.030
[27,     1] loss: 906.247
[28,     1] loss: 898.500
[29,     1] loss: 874.270
[30,     1] loss: 903.697
[31,     1] loss: 886.304
[32,     1] loss: 931.611
[33,     1] loss: 1042.016
[34,     1] loss: 882.353
[35,     1] loss: 923.406
[36,     1] loss: 854.570
[37,     1] loss: 928.273
[38,     1] loss: 810.641
[39,     1] loss: 902.622
[40,     1] loss: 932.488
[41,     1] loss: 815.798
[42,     1] loss: 817.996
[43,     1] loss: 784.143
[44,     1] loss: 865.361
[45,     1] loss: 839.533
[46,     1] loss: 797.020
[47,     1] loss: 739.657
[48,     1] loss: 793.825
[49,     1] loss: 1015.326
[50,     1] loss: 923.613
[51,     1] loss: 824.064
[52,     1] loss: 899.736
[53,     1] loss: 873.909
[54,     1] loss: 788.564
[55,     1] loss: 843.129
[56,     1] loss: 720.904
[57,     1] loss: 830.310
[58,     1] loss: 777.915
[59,     1] loss: 672.852
[60,     1] loss: 691.227
[61,     1] loss: 702.363
[62,     1] loss: 669.513
[63,     1] loss: 595.140
[64,     1] loss: 644.638
[65,     1] loss: 833.172
[66,     1] loss: 1998.691
[67,     1] loss: 887.098
[68,     1] loss: 1019.125
[69,     1] loss: 1041.178
[70,     1] loss: 1051.680
[71,     1] loss: 1049.851
[72,     1] loss: 1012.313
[73,     1] loss: 972.851
[74,     1] loss: 996.450
[75,     1] loss: 984.347
[76,     1] loss: 923.670
[77,     1] loss: 895.435
[78,     1] loss: 858.366
[79,     1] loss: 1327.318
[80,     1] loss: 1975.945
[81,     1] loss: 1584.164
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028529324214303592,
 'learning_rate_Hydroxylation-K': 0.004833368717845808,
 'learning_rate_Hydroxylation-P': 0.0024307854720804134,
 'log_base': 2.321937209475444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1620359503,
 'sample_weights': [1.5837414038744881, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.481225694936466,
 'weight_decay_Hydroxylation-K': 3.6590961095309,
 'weight_decay_Hydroxylation-P': 8.297427267740545}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.881
[2,     1] loss: 1326.656
[3,     1] loss: 1328.449
[4,     1] loss: 1323.800
[5,     1] loss: 1318.393
[6,     1] loss: 1317.443
[7,     1] loss: 1310.549
[8,     1] loss: 1299.811
[9,     1] loss: 1284.062
[10,     1] loss: 1251.165
[11,     1] loss: 1228.219
[12,     1] loss: 1204.725
[13,     1] loss: 1164.404
[14,     1] loss: 1146.203
[15,     1] loss: 1110.637
[16,     1] loss: 1080.895
[17,     1] loss: 1061.178
[18,     1] loss: 1115.478
[19,     1] loss: 1069.215
[20,     1] loss: 1096.837
[21,     1] loss: 1096.452
[22,     1] loss: 1066.903
[23,     1] loss: 1067.909
[24,     1] loss: 1032.352
[25,     1] loss: 1038.585
[26,     1] loss: 1005.028
[27,     1] loss: 1026.188
[28,     1] loss: 1017.389
[29,     1] loss: 982.117
[30,     1] loss: 926.714
[31,     1] loss: 998.468
[32,     1] loss: 966.014
[33,     1] loss: 903.943
[34,     1] loss: 933.608
[35,     1] loss: 909.007
[36,     1] loss: 882.714
[37,     1] loss: 919.613
[38,     1] loss: 900.980
[39,     1] loss: 888.947
[40,     1] loss: 870.587
[41,     1] loss: 912.511
[42,     1] loss: 881.142
[43,     1] loss: 841.169
[44,     1] loss: 895.078
[45,     1] loss: 842.938
[46,     1] loss: 861.613
[47,     1] loss: 798.437
[48,     1] loss: 806.881
[49,     1] loss: 810.083
[50,     1] loss: 796.265
[51,     1] loss: 809.768
[52,     1] loss: 753.076
[53,     1] loss: 752.266
[54,     1] loss: 750.428
[55,     1] loss: 755.149
[56,     1] loss: 745.633
[57,     1] loss: 759.828
[58,     1] loss: 731.492
[59,     1] loss: 707.187
[60,     1] loss: 720.090
[61,     1] loss: 753.752
[62,     1] loss: 682.521
[63,     1] loss: 683.240
[64,     1] loss: 687.418
[65,     1] loss: 692.910
[66,     1] loss: 833.384
[67,     1] loss: 830.531
[68,     1] loss: 625.923
[69,     1] loss: 837.655
[70,     1] loss: 676.676
[71,     1] loss: 791.172
[72,     1] loss: 662.914
[73,     1] loss: 757.325
[74,     1] loss: 692.424
[75,     1] loss: 652.190
[76,     1] loss: 728.856
[77,     1] loss: 645.275
[78,     1] loss: 615.264
[79,     1] loss: 645.490
[80,     1] loss: 628.917
[81,     1] loss: 631.284
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004973880340920976,
 'learning_rate_Hydroxylation-K': 0.009485328270802184,
 'learning_rate_Hydroxylation-P': 0.004666095120709298,
 'log_base': 1.3709838675319281,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2560726986,
 'sample_weights': [1.9817657862275422, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.557164661138605,
 'weight_decay_Hydroxylation-K': 2.2556240020953666,
 'weight_decay_Hydroxylation-P': 1.979543157993723}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2040.796
[2,     1] loss: 2025.656
[3,     1] loss: 2018.797
[4,     1] loss: 2022.687
[5,     1] loss: 2024.491
[6,     1] loss: 2025.712
[7,     1] loss: 2023.921
[8,     1] loss: 2023.114
[9,     1] loss: 2016.575
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004049836070787236,
 'learning_rate_Hydroxylation-K': 0.00013246318137086807,
 'learning_rate_Hydroxylation-P': 0.004569667722095473,
 'log_base': 2.932065845568014,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3482764524,
 'sample_weights': [5.29094025003143, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9263076509699,
 'weight_decay_Hydroxylation-K': 2.7400141702733607,
 'weight_decay_Hydroxylation-P': 2.55100440187293}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.733
[2,     1] loss: 1239.325
[3,     1] loss: 1233.848
[4,     1] loss: 1234.366
[5,     1] loss: 1239.446
[6,     1] loss: 1239.256
[7,     1] loss: 1228.719
[8,     1] loss: 1228.571
[9,     1] loss: 1221.510
[10,     1] loss: 1210.347
[11,     1] loss: 1191.058
[12,     1] loss: 1146.628
[13,     1] loss: 1107.875
[14,     1] loss: 1056.382
[15,     1] loss: 1021.570
[16,     1] loss: 1015.704
[17,     1] loss: 1075.153
[18,     1] loss: 1013.165
[19,     1] loss: 999.861
[20,     1] loss: 954.880
[21,     1] loss: 1015.985
[22,     1] loss: 988.616
[23,     1] loss: 953.954
[24,     1] loss: 967.396
[25,     1] loss: 969.222
[26,     1] loss: 925.216
[27,     1] loss: 933.467
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0045874791770700455,
 'learning_rate_Hydroxylation-K': 0.0034534707786329742,
 'learning_rate_Hydroxylation-P': 0.0042974989340540705,
 'log_base': 2.6698239699674886,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 625031313,
 'sample_weights': [1.551949343907919, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.810859293840892,
 'weight_decay_Hydroxylation-K': 4.5209413792804325,
 'weight_decay_Hydroxylation-P': 1.033083382130823}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.587
[2,     1] loss: 1265.148
[3,     1] loss: 1270.002
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003557644572520594,
 'learning_rate_Hydroxylation-K': 0.006006018659093129,
 'learning_rate_Hydroxylation-P': 0.006548162408672096,
 'log_base': 2.9403680660168154,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3973408411,
 'sample_weights': [1.7000222268696017, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0451092989910173,
 'weight_decay_Hydroxylation-K': 0.6284919751078,
 'weight_decay_Hydroxylation-P': 9.7992816596455}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.123
[2,     1] loss: 1235.627
[3,     1] loss: 1233.827
[4,     1] loss: 1230.691
[5,     1] loss: 1229.324
[6,     1] loss: 1230.038
[7,     1] loss: 1222.772
[8,     1] loss: 1204.814
[9,     1] loss: 1185.031
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036512952993684174,
 'learning_rate_Hydroxylation-K': 0.002101483984032696,
 'learning_rate_Hydroxylation-P': 0.006018326567832414,
 'log_base': 2.9950716384841467,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1873333889,
 'sample_weights': [1.5478806988831013, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.875533932949761,
 'weight_decay_Hydroxylation-K': 5.131757975489195,
 'weight_decay_Hydroxylation-P': 1.208211650096551}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.584
[2,     1] loss: 1227.031
[3,     1] loss: 1228.125
[4,     1] loss: 1231.189
[5,     1] loss: 1220.290
[6,     1] loss: 1214.849
[7,     1] loss: 1186.589
[8,     1] loss: 1162.517
[9,     1] loss: 1106.831
[10,     1] loss: 1088.967
[11,     1] loss: 1075.471
[12,     1] loss: 1074.629
[13,     1] loss: 1051.124
[14,     1] loss: 1031.443
[15,     1] loss: 999.373
[16,     1] loss: 1027.917
[17,     1] loss: 983.696
[18,     1] loss: 983.199
[19,     1] loss: 982.959
[20,     1] loss: 994.345
[21,     1] loss: 940.173
[22,     1] loss: 951.763
[23,     1] loss: 1009.288
[24,     1] loss: 918.714
[25,     1] loss: 906.805
[26,     1] loss: 921.303
[27,     1] loss: 949.947
[28,     1] loss: 913.977
[29,     1] loss: 919.658
[30,     1] loss: 872.217
[31,     1] loss: 893.638
[32,     1] loss: 864.575
[33,     1] loss: 877.705
[34,     1] loss: 855.107
[35,     1] loss: 811.568
[36,     1] loss: 861.393
[37,     1] loss: 831.259
[38,     1] loss: 825.908
[39,     1] loss: 824.127
[40,     1] loss: 892.950
[41,     1] loss: 986.110
[42,     1] loss: 795.478
[43,     1] loss: 829.684
[44,     1] loss: 787.242
[45,     1] loss: 813.583
[46,     1] loss: 779.845
[47,     1] loss: 767.074
[48,     1] loss: 782.155
[49,     1] loss: 677.546
[50,     1] loss: 735.868
[51,     1] loss: 768.394
[52,     1] loss: 841.594
[53,     1] loss: 757.992
[54,     1] loss: 690.246
[55,     1] loss: 809.683
[56,     1] loss: 708.173
[57,     1] loss: 739.415
[58,     1] loss: 758.276
[59,     1] loss: 694.684
[60,     1] loss: 661.948
[61,     1] loss: 701.539
[62,     1] loss: 644.759
[63,     1] loss: 693.034
[64,     1] loss: 657.947
[65,     1] loss: 633.394
[66,     1] loss: 798.068
[67,     1] loss: 761.305
[68,     1] loss: 679.873
[69,     1] loss: 716.395
[70,     1] loss: 686.338
[71,     1] loss: 629.762
[72,     1] loss: 707.777
[73,     1] loss: 628.607
[74,     1] loss: 652.154
[75,     1] loss: 614.976
[76,     1] loss: 572.267
[77,     1] loss: 557.363
[78,     1] loss: 595.876
[79,     1] loss: 486.501
[80,     1] loss: 534.735
[81,     1] loss: 561.658
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023200277053619916,
 'learning_rate_Hydroxylation-K': 0.004363720282460869,
 'learning_rate_Hydroxylation-P': 0.005449307624208055,
 'log_base': 2.7034631583283555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3801646519,
 'sample_weights': [1.5218702078516506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.073665878277618,
 'weight_decay_Hydroxylation-K': 4.317460672697909,
 'weight_decay_Hydroxylation-P': 1.2732772759852922}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.619
[2,     1] loss: 1260.543
[3,     1] loss: 1262.327
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004117978681618618,
 'learning_rate_Hydroxylation-K': 0.008463068938722692,
 'learning_rate_Hydroxylation-P': 0.007103773516882195,
 'log_base': 1.353123591606598,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 191040226,
 'sample_weights': [1.6786191471873808, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.947237361453423,
 'weight_decay_Hydroxylation-K': 1.99540742735048,
 'weight_decay_Hydroxylation-P': 4.822063887248442}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2082.570
[2,     1] loss: 2075.640
[3,     1] loss: 2072.279
[4,     1] loss: 2075.554
[5,     1] loss: 2065.889
[6,     1] loss: 2052.025
[7,     1] loss: 2081.093
[8,     1] loss: 2054.725
[9,     1] loss: 2027.199
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007074437631582918,
 'learning_rate_Hydroxylation-K': 0.002508054549123345,
 'learning_rate_Hydroxylation-P': 0.0059542872748421845,
 'log_base': 2.439824837596726,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2617803369,
 'sample_weights': [5.520358880514759, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.976144550783112,
 'weight_decay_Hydroxylation-K': 4.150935302115288,
 'weight_decay_Hydroxylation-P': 1.2888685544254415}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.040
[2,     1] loss: 1300.664
[3,     1] loss: 1306.048
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004204843185166951,
 'learning_rate_Hydroxylation-K': 0.0010668756458517186,
 'learning_rate_Hydroxylation-P': 0.003442445814264351,
 'log_base': 2.716350709521385,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3168434496,
 'sample_weights': [1.871727790903232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.375619093878675,
 'weight_decay_Hydroxylation-K': 5.968601554841672,
 'weight_decay_Hydroxylation-P': 2.055107977774501}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.976
[2,     1] loss: 1264.148
[3,     1] loss: 1260.417
[4,     1] loss: 1265.276
[5,     1] loss: 1257.199
[6,     1] loss: 1260.961
[7,     1] loss: 1254.584
[8,     1] loss: 1249.189
[9,     1] loss: 1237.194
[10,     1] loss: 1223.053
[11,     1] loss: 1182.852
[12,     1] loss: 1145.765
[13,     1] loss: 1080.506
[14,     1] loss: 1097.264
[15,     1] loss: 1075.322
[16,     1] loss: 1040.778
[17,     1] loss: 1021.555
[18,     1] loss: 1042.304
[19,     1] loss: 1011.929
[20,     1] loss: 1028.768
[21,     1] loss: 1000.593
[22,     1] loss: 976.031
[23,     1] loss: 966.857
[24,     1] loss: 948.096
[25,     1] loss: 954.360
[26,     1] loss: 963.793
[27,     1] loss: 969.944
[28,     1] loss: 939.629
[29,     1] loss: 908.655
[30,     1] loss: 924.336
[31,     1] loss: 885.538
[32,     1] loss: 908.286
[33,     1] loss: 892.771
[34,     1] loss: 828.802
[35,     1] loss: 870.784
[36,     1] loss: 830.721
[37,     1] loss: 800.903
[38,     1] loss: 852.459
[39,     1] loss: 795.353
[40,     1] loss: 835.860
[41,     1] loss: 846.362
[42,     1] loss: 760.326
[43,     1] loss: 842.660
[44,     1] loss: 782.635
[45,     1] loss: 778.601
[46,     1] loss: 744.280
[47,     1] loss: 699.770
[48,     1] loss: 803.955
[49,     1] loss: 952.846
[50,     1] loss: 786.758
[51,     1] loss: 732.344
[52,     1] loss: 754.204
[53,     1] loss: 811.976
[54,     1] loss: 703.561
[55,     1] loss: 761.478
[56,     1] loss: 731.166
[57,     1] loss: 719.323
[58,     1] loss: 743.193
[59,     1] loss: 633.304
[60,     1] loss: 634.564
[61,     1] loss: 712.986
[62,     1] loss: 595.995
[63,     1] loss: 628.820
[64,     1] loss: 731.711
[65,     1] loss: 1113.515
[66,     1] loss: 588.689
[67,     1] loss: 934.631
[68,     1] loss: 681.664
[69,     1] loss: 808.591
[70,     1] loss: 829.993
[71,     1] loss: 713.284
[72,     1] loss: 672.479
[73,     1] loss: 731.090
[74,     1] loss: 647.407
[75,     1] loss: 694.162
[76,     1] loss: 627.220
[77,     1] loss: 641.837
[78,     1] loss: 643.280
[79,     1] loss: 640.486
[80,     1] loss: 623.272
[81,     1] loss: 606.709
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009367878749239378,
 'learning_rate_Hydroxylation-K': 0.00730790194563643,
 'learning_rate_Hydroxylation-P': 0.0034415804794427577,
 'log_base': 2.412789305200023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4022411044,
 'sample_weights': [1.6706304167188062, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.039075646968554,
 'weight_decay_Hydroxylation-K': 9.807889184759883,
 'weight_decay_Hydroxylation-P': 9.565123507305085}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1315.640
[2,     1] loss: 1312.901
[3,     1] loss: 1309.678
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032634607001305596,
 'learning_rate_Hydroxylation-K': 0.0032441792984087437,
 'learning_rate_Hydroxylation-P': 0.005554890403081476,
 'log_base': 2.965787948193959,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4275996548,
 'sample_weights': [1.8954069999132148, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.874518983772697,
 'weight_decay_Hydroxylation-K': 6.835862133915484,
 'weight_decay_Hydroxylation-P': 2.7500451639983106}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.016
[2,     1] loss: 1235.127
[3,     1] loss: 1229.617
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033937281121188805,
 'learning_rate_Hydroxylation-K': 0.0015599376727236853,
 'learning_rate_Hydroxylation-P': 8.463925162978901e-05,
 'log_base': 1.6811938931857666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1576993353,
 'sample_weights': [1.5356246013660027, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.7639609459026975,
 'weight_decay_Hydroxylation-K': 0.08937879494761652,
 'weight_decay_Hydroxylation-P': 0.053757126841996766}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1590.844
[2,     1] loss: 1592.435
[3,     1] loss: 1573.243
[4,     1] loss: 1581.450
[5,     1] loss: 1580.483
[6,     1] loss: 1588.643
[7,     1] loss: 1575.484
[8,     1] loss: 1580.441
[9,     1] loss: 1581.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022123069765635417,
 'learning_rate_Hydroxylation-K': 0.00599936142787966,
 'learning_rate_Hydroxylation-P': 0.0051448610826102725,
 'log_base': 2.9536158485494077,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2353999311,
 'sample_weights': [3.2135316211205316, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.077309593606924,
 'weight_decay_Hydroxylation-K': 3.035405378767277,
 'weight_decay_Hydroxylation-P': 6.436395116636958}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.418
[2,     1] loss: 1231.065
[3,     1] loss: 1235.316
[4,     1] loss: 1234.697
[5,     1] loss: 1230.887
[6,     1] loss: 1229.676
[7,     1] loss: 1227.232
[8,     1] loss: 1222.200
[9,     1] loss: 1219.781
[10,     1] loss: 1204.429
[11,     1] loss: 1185.942
[12,     1] loss: 1172.299
[13,     1] loss: 1131.079
[14,     1] loss: 1108.859
[15,     1] loss: 1083.927
[16,     1] loss: 1062.585
[17,     1] loss: 1044.412
[18,     1] loss: 1021.382
[19,     1] loss: 1048.365
[20,     1] loss: 1055.472
[21,     1] loss: 1016.663
[22,     1] loss: 1002.888
[23,     1] loss: 995.766
[24,     1] loss: 1032.320
[25,     1] loss: 995.029
[26,     1] loss: 990.937
[27,     1] loss: 986.547
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004254872232253025,
 'learning_rate_Hydroxylation-K': 0.003047401858538668,
 'learning_rate_Hydroxylation-P': 0.006325038746274476,
 'log_base': 2.888248167338951,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 332318359,
 'sample_weights': [1.5414558648018664, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.718913383368303,
 'weight_decay_Hydroxylation-K': 4.014125549550938,
 'weight_decay_Hydroxylation-P': 2.899812414258693}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.460
[2,     1] loss: 1240.776
[3,     1] loss: 1238.322
[4,     1] loss: 1237.543
[5,     1] loss: 1236.490
[6,     1] loss: 1227.962
[7,     1] loss: 1228.827
[8,     1] loss: 1198.368
[9,     1] loss: 1165.307
[10,     1] loss: 1137.129
[11,     1] loss: 1097.995
[12,     1] loss: 1091.486
[13,     1] loss: 1051.363
[14,     1] loss: 1014.892
[15,     1] loss: 1021.247
[16,     1] loss: 1015.123
[17,     1] loss: 1008.272
[18,     1] loss: 1005.334
[19,     1] loss: 978.347
[20,     1] loss: 985.205
[21,     1] loss: 1002.625
[22,     1] loss: 962.590
[23,     1] loss: 952.626
[24,     1] loss: 943.751
[25,     1] loss: 930.307
[26,     1] loss: 965.297
[27,     1] loss: 910.742
[28,     1] loss: 903.891
[29,     1] loss: 912.753
[30,     1] loss: 917.359
[31,     1] loss: 887.413
[32,     1] loss: 857.813
[33,     1] loss: 863.861
[34,     1] loss: 862.505
[35,     1] loss: 799.410
[36,     1] loss: 907.898
[37,     1] loss: 1004.390
[38,     1] loss: 847.899
[39,     1] loss: 820.115
[40,     1] loss: 863.855
[41,     1] loss: 832.440
[42,     1] loss: 793.136
[43,     1] loss: 841.030
[44,     1] loss: 743.219
[45,     1] loss: 799.255
[46,     1] loss: 783.293
[47,     1] loss: 787.366
[48,     1] loss: 792.840
[49,     1] loss: 697.764
[50,     1] loss: 798.219
[51,     1] loss: 771.710
[52,     1] loss: 707.082
[53,     1] loss: 759.536
[54,     1] loss: 687.721
[55,     1] loss: 779.236
[56,     1] loss: 678.870
[57,     1] loss: 680.000
[58,     1] loss: 716.588
[59,     1] loss: 652.390
[60,     1] loss: 583.418
[61,     1] loss: 789.743
[62,     1] loss: 880.504
[63,     1] loss: 583.496
[64,     1] loss: 855.891
[65,     1] loss: 671.841
[66,     1] loss: 762.197
[67,     1] loss: 742.458
[68,     1] loss: 609.119
[69,     1] loss: 699.714
[70,     1] loss: 563.605
[71,     1] loss: 647.572
[72,     1] loss: 549.434
[73,     1] loss: 626.377
[74,     1] loss: 775.162
[75,     1] loss: 546.606
[76,     1] loss: 630.930
[77,     1] loss: 637.041
[78,     1] loss: 568.022
[79,     1] loss: 619.384
[80,     1] loss: 548.349
[81,     1] loss: 608.022
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026457896594273087,
 'learning_rate_Hydroxylation-K': 0.002429391084885547,
 'learning_rate_Hydroxylation-P': 0.0017282834184854266,
 'log_base': 2.6881321745443745,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4180645586,
 'sample_weights': [1.5739809689325623, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.326749600646306,
 'weight_decay_Hydroxylation-K': 6.557869737097288,
 'weight_decay_Hydroxylation-P': 3.3480885212621594}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.119
[2,     1] loss: 1268.320
[3,     1] loss: 1265.657
[4,     1] loss: 1261.006
[5,     1] loss: 1260.128
[6,     1] loss: 1254.664
[7,     1] loss: 1245.976
[8,     1] loss: 1224.993
[9,     1] loss: 1193.131
[10,     1] loss: 1170.875
[11,     1] loss: 1145.860
[12,     1] loss: 1123.806
[13,     1] loss: 1099.002
[14,     1] loss: 1106.176
[15,     1] loss: 1037.222
[16,     1] loss: 1119.510
[17,     1] loss: 1040.075
[18,     1] loss: 1026.739
[19,     1] loss: 1026.584
[20,     1] loss: 1042.549
[21,     1] loss: 1006.835
[22,     1] loss: 1013.425
[23,     1] loss: 1027.427
[24,     1] loss: 1027.214
[25,     1] loss: 954.967
[26,     1] loss: 982.699
[27,     1] loss: 988.268
[28,     1] loss: 921.928
[29,     1] loss: 942.283
[30,     1] loss: 930.036
[31,     1] loss: 899.208
[32,     1] loss: 927.965
[33,     1] loss: 925.803
[34,     1] loss: 879.107
[35,     1] loss: 880.823
[36,     1] loss: 812.018
[37,     1] loss: 888.333
[38,     1] loss: 888.451
[39,     1] loss: 944.463
[40,     1] loss: 785.939
[41,     1] loss: 844.108
[42,     1] loss: 807.796
[43,     1] loss: 871.671
[44,     1] loss: 843.973
[45,     1] loss: 834.302
[46,     1] loss: 825.799
[47,     1] loss: 770.569
[48,     1] loss: 809.102
[49,     1] loss: 747.295
[50,     1] loss: 748.725
[51,     1] loss: 711.077
[52,     1] loss: 780.021
[53,     1] loss: 752.478
[54,     1] loss: 710.536
[55,     1] loss: 688.971
[56,     1] loss: 685.941
[57,     1] loss: 728.913
[58,     1] loss: 680.948
[59,     1] loss: 663.619
[60,     1] loss: 665.586
[61,     1] loss: 644.835
[62,     1] loss: 613.611
[63,     1] loss: 648.733
[64,     1] loss: 666.387
[65,     1] loss: 590.742
[66,     1] loss: 611.544
[67,     1] loss: 630.339
[68,     1] loss: 633.163
[69,     1] loss: 801.552
[70,     1] loss: 715.725
[71,     1] loss: 660.097
[72,     1] loss: 706.983
[73,     1] loss: 601.506
[74,     1] loss: 666.281
[75,     1] loss: 615.106
[76,     1] loss: 655.505
[77,     1] loss: 518.588
[78,     1] loss: 654.333
[79,     1] loss: 572.330
[80,     1] loss: 560.156
[81,     1] loss: 617.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004643880339382527,
 'learning_rate_Hydroxylation-K': 0.0013725582935685504,
 'learning_rate_Hydroxylation-P': 0.002991132479214716,
 'log_base': 2.959168462098909,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2932482014,
 'sample_weights': [1.6882731440322656, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.886420572037439,
 'weight_decay_Hydroxylation-K': 2.9691911829718984,
 'weight_decay_Hydroxylation-P': 3.169257900683408}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.413
[2,     1] loss: 1238.968
[3,     1] loss: 1234.512
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017028766606865143,
 'learning_rate_Hydroxylation-K': 0.003345961005895231,
 'learning_rate_Hydroxylation-P': 0.0021014944474705465,
 'log_base': 2.4089365094626563,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3596231545,
 'sample_weights': [1.5387873255448785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.469232431199907,
 'weight_decay_Hydroxylation-K': 6.704431342965735,
 'weight_decay_Hydroxylation-P': 2.556693902530628}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1308.245
[2,     1] loss: 1309.394
[3,     1] loss: 1305.807
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033788148126239004,
 'learning_rate_Hydroxylation-K': 0.004748777532404989,
 'learning_rate_Hydroxylation-P': 0.008252152075510466,
 'log_base': 2.9114074886104566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 128137965,
 'sample_weights': [1.898852288383148, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.607743205559853,
 'weight_decay_Hydroxylation-K': 5.869476494989945,
 'weight_decay_Hydroxylation-P': 0.952017983052559}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.378
[2,     1] loss: 1248.641
[3,     1] loss: 1240.375
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012210456172847691,
 'learning_rate_Hydroxylation-K': 0.005612783832769999,
 'learning_rate_Hydroxylation-P': 0.007857799427086769,
 'log_base': 2.577595475352359,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 838439378,
 'sample_weights': [1.5622177727509705, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.208994336864938,
 'weight_decay_Hydroxylation-K': 0.7279773190474903,
 'weight_decay_Hydroxylation-P': 6.479084028262471}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.135
[2,     1] loss: 1279.526
[3,     1] loss: 1282.372
[4,     1] loss: 1280.577
[5,     1] loss: 1278.832
[6,     1] loss: 1276.304
[7,     1] loss: 1278.895
[8,     1] loss: 1272.144
[9,     1] loss: 1268.455
[10,     1] loss: 1265.538
[11,     1] loss: 1254.034
[12,     1] loss: 1245.528
[13,     1] loss: 1235.531
[14,     1] loss: 1213.704
[15,     1] loss: 1197.104
[16,     1] loss: 1173.088
[17,     1] loss: 1154.127
[18,     1] loss: 1132.348
[19,     1] loss: 1121.853
[20,     1] loss: 1103.998
[21,     1] loss: 1099.802
[22,     1] loss: 1120.619
[23,     1] loss: 1086.297
[24,     1] loss: 1047.528
[25,     1] loss: 1073.490
[26,     1] loss: 1090.139
[27,     1] loss: 1070.792
[28,     1] loss: 1092.762
[29,     1] loss: 1075.792
[30,     1] loss: 1035.083
[31,     1] loss: 1018.319
[32,     1] loss: 1042.213
[33,     1] loss: 1037.457
[34,     1] loss: 1029.323
[35,     1] loss: 1035.777
[36,     1] loss: 1038.021
[37,     1] loss: 1017.811
[38,     1] loss: 981.735
[39,     1] loss: 992.548
[40,     1] loss: 939.548
[41,     1] loss: 1002.946
[42,     1] loss: 916.998
[43,     1] loss: 942.258
[44,     1] loss: 980.981
[45,     1] loss: 901.110
[46,     1] loss: 953.395
[47,     1] loss: 934.331
[48,     1] loss: 932.746
[49,     1] loss: 952.194
[50,     1] loss: 919.855
[51,     1] loss: 955.954
[52,     1] loss: 897.493
[53,     1] loss: 916.086
[54,     1] loss: 864.174
[55,     1] loss: 905.215
[56,     1] loss: 863.809
[57,     1] loss: 875.488
[58,     1] loss: 907.759
[59,     1] loss: 904.703
[60,     1] loss: 804.458
[61,     1] loss: 862.388
[62,     1] loss: 810.528
[63,     1] loss: 878.534
[64,     1] loss: 830.360
[65,     1] loss: 807.516
[66,     1] loss: 815.440
[67,     1] loss: 808.631
[68,     1] loss: 822.332
[69,     1] loss: 790.086
[70,     1] loss: 776.709
[71,     1] loss: 757.365
[72,     1] loss: 864.188
[73,     1] loss: 730.501
[74,     1] loss: 782.569
[75,     1] loss: 755.704
[76,     1] loss: 698.644
[77,     1] loss: 748.682
[78,     1] loss: 755.025
[79,     1] loss: 701.309
[80,     1] loss: 723.479
[81,     1] loss: 692.728
[82,     1] loss: 696.258
[83,     1] loss: 679.688
[84,     1] loss: 688.279
[85,     1] loss: 613.702
[86,     1] loss: 720.182
[87,     1] loss: 645.528
[88,     1] loss: 649.718
[89,     1] loss: 689.652
[90,     1] loss: 636.132
[91,     1] loss: 581.694
[92,     1] loss: 653.415
[93,     1] loss: 598.829
[94,     1] loss: 630.222
[95,     1] loss: 571.446
[96,     1] loss: 607.199
[97,     1] loss: 585.255
[98,     1] loss: 614.249
[99,     1] loss: 620.070
[100,     1] loss: 593.244
[101,     1] loss: 587.395
[102,     1] loss: 604.305
[103,     1] loss: 591.865
[104,     1] loss: 578.874
[105,     1] loss: 568.834
[106,     1] loss: 587.244
[107,     1] loss: 544.991
[108,     1] loss: 559.615
Early stopping applied (best metric=0.3455255329608917)
Finished Training
Total time taken: 16.907731533050537
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.907
[2,     1] loss: 1280.764
[3,     1] loss: 1280.185
[4,     1] loss: 1282.362
[5,     1] loss: 1277.775
[6,     1] loss: 1277.681
[7,     1] loss: 1277.116
[8,     1] loss: 1274.818
[9,     1] loss: 1274.249
[10,     1] loss: 1269.945
[11,     1] loss: 1270.080
[12,     1] loss: 1254.220
[13,     1] loss: 1247.859
[14,     1] loss: 1230.890
[15,     1] loss: 1213.190
[16,     1] loss: 1201.733
[17,     1] loss: 1176.680
[18,     1] loss: 1164.439
[19,     1] loss: 1141.475
[20,     1] loss: 1127.341
[21,     1] loss: 1123.728
[22,     1] loss: 1104.943
[23,     1] loss: 1113.405
[24,     1] loss: 1073.917
[25,     1] loss: 1090.557
[26,     1] loss: 1060.153
[27,     1] loss: 1059.411
[28,     1] loss: 1087.003
[29,     1] loss: 1055.384
[30,     1] loss: 1076.753
[31,     1] loss: 1037.757
[32,     1] loss: 1029.182
[33,     1] loss: 1011.342
[34,     1] loss: 1007.916
[35,     1] loss: 1015.779
[36,     1] loss: 979.253
[37,     1] loss: 1005.636
[38,     1] loss: 1009.136
[39,     1] loss: 996.177
[40,     1] loss: 1005.854
[41,     1] loss: 996.001
[42,     1] loss: 966.395
[43,     1] loss: 941.466
[44,     1] loss: 989.020
[45,     1] loss: 985.248
[46,     1] loss: 976.099
[47,     1] loss: 955.058
[48,     1] loss: 957.622
[49,     1] loss: 931.128
[50,     1] loss: 913.304
[51,     1] loss: 882.371
[52,     1] loss: 932.582
[53,     1] loss: 925.237
[54,     1] loss: 868.747
[55,     1] loss: 875.470
[56,     1] loss: 827.880
[57,     1] loss: 855.048
[58,     1] loss: 909.757
[59,     1] loss: 863.216
[60,     1] loss: 823.929
[61,     1] loss: 809.645
[62,     1] loss: 879.565
[63,     1] loss: 832.836
[64,     1] loss: 847.456
[65,     1] loss: 809.179
[66,     1] loss: 816.661
[67,     1] loss: 848.949
[68,     1] loss: 794.913
[69,     1] loss: 769.558
[70,     1] loss: 751.382
[71,     1] loss: 751.460
[72,     1] loss: 732.324
[73,     1] loss: 770.265
[74,     1] loss: 745.449
[75,     1] loss: 746.584
[76,     1] loss: 690.308
[77,     1] loss: 725.159
[78,     1] loss: 747.997
[79,     1] loss: 670.356
[80,     1] loss: 704.758
[81,     1] loss: 737.869
[82,     1] loss: 741.573
[83,     1] loss: 738.667
[84,     1] loss: 629.099
[85,     1] loss: 714.435
[86,     1] loss: 702.372
[87,     1] loss: 642.349
[88,     1] loss: 666.887
[89,     1] loss: 652.737
[90,     1] loss: 663.163
[91,     1] loss: 661.483
[92,     1] loss: 662.793
[93,     1] loss: 644.527
[94,     1] loss: 682.217
[95,     1] loss: 620.435
[96,     1] loss: 627.907
[97,     1] loss: 589.450
[98,     1] loss: 573.877
[99,     1] loss: 605.624
[100,     1] loss: 557.109
[101,     1] loss: 646.812
[102,     1] loss: 546.334
[103,     1] loss: 568.539
[104,     1] loss: 608.787
[105,     1] loss: 552.759
[106,     1] loss: 568.652
[107,     1] loss: 553.565
[108,     1] loss: 576.604
Early stopping applied (best metric=0.3715691566467285)
Finished Training
Total time taken: 16.83466410636902
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.320
[2,     1] loss: 1280.307
[3,     1] loss: 1278.320
[4,     1] loss: 1281.099
[5,     1] loss: 1278.687
[6,     1] loss: 1279.871
[7,     1] loss: 1277.989
[8,     1] loss: 1278.157
[9,     1] loss: 1276.409
[10,     1] loss: 1279.472
[11,     1] loss: 1274.022
[12,     1] loss: 1275.153
[13,     1] loss: 1269.090
[14,     1] loss: 1268.335
[15,     1] loss: 1262.155
[16,     1] loss: 1252.154
[17,     1] loss: 1239.722
[18,     1] loss: 1229.581
[19,     1] loss: 1220.093
[20,     1] loss: 1204.314
[21,     1] loss: 1191.357
[22,     1] loss: 1162.386
[23,     1] loss: 1138.632
[24,     1] loss: 1154.554
[25,     1] loss: 1106.498
[26,     1] loss: 1108.549
[27,     1] loss: 1101.523
[28,     1] loss: 1057.299
[29,     1] loss: 1091.397
[30,     1] loss: 1074.465
[31,     1] loss: 1061.054
[32,     1] loss: 1080.302
[33,     1] loss: 1085.794
[34,     1] loss: 1034.089
[35,     1] loss: 1000.693
[36,     1] loss: 1036.540
[37,     1] loss: 1050.837
[38,     1] loss: 1020.178
[39,     1] loss: 1042.676
[40,     1] loss: 990.128
[41,     1] loss: 1029.403
[42,     1] loss: 984.563
[43,     1] loss: 969.225
[44,     1] loss: 966.334
[45,     1] loss: 971.353
[46,     1] loss: 958.489
[47,     1] loss: 973.142
[48,     1] loss: 981.375
[49,     1] loss: 968.361
[50,     1] loss: 947.876
[51,     1] loss: 973.831
[52,     1] loss: 898.890
[53,     1] loss: 898.344
[54,     1] loss: 906.944
[55,     1] loss: 935.068
[56,     1] loss: 877.402
[57,     1] loss: 873.714
[58,     1] loss: 880.624
[59,     1] loss: 854.196
[60,     1] loss: 830.744
[61,     1] loss: 816.778
[62,     1] loss: 917.538
[63,     1] loss: 828.322
[64,     1] loss: 810.488
[65,     1] loss: 817.883
[66,     1] loss: 816.699
[67,     1] loss: 769.427
[68,     1] loss: 806.164
[69,     1] loss: 800.314
[70,     1] loss: 805.820
[71,     1] loss: 742.548
[72,     1] loss: 743.281
[73,     1] loss: 727.044
[74,     1] loss: 766.819
[75,     1] loss: 701.383
[76,     1] loss: 691.557
[77,     1] loss: 720.922
[78,     1] loss: 778.234
[79,     1] loss: 682.847
[80,     1] loss: 686.971
[81,     1] loss: 677.593
[82,     1] loss: 675.985
[83,     1] loss: 685.986
[84,     1] loss: 738.023
[85,     1] loss: 714.687
[86,     1] loss: 707.306
[87,     1] loss: 725.371
[88,     1] loss: 678.963
[89,     1] loss: 677.509
[90,     1] loss: 673.648
[91,     1] loss: 647.962
[92,     1] loss: 647.288
[93,     1] loss: 663.148
[94,     1] loss: 599.305
[95,     1] loss: 709.472
[96,     1] loss: 615.475
Early stopping applied (best metric=0.38357996940612793)
Finished Training
Total time taken: 12.983577966690063
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.053
[2,     1] loss: 1280.929
[3,     1] loss: 1278.854
[4,     1] loss: 1276.373
[5,     1] loss: 1275.453
[6,     1] loss: 1277.465
[7,     1] loss: 1273.568
[8,     1] loss: 1268.497
[9,     1] loss: 1262.527
[10,     1] loss: 1252.756
[11,     1] loss: 1242.713
[12,     1] loss: 1221.558
[13,     1] loss: 1198.363
[14,     1] loss: 1185.208
[15,     1] loss: 1150.832
[16,     1] loss: 1130.523
[17,     1] loss: 1108.115
[18,     1] loss: 1093.908
[19,     1] loss: 1082.001
[20,     1] loss: 1054.854
[21,     1] loss: 1042.895
[22,     1] loss: 1027.112
[23,     1] loss: 1019.617
[24,     1] loss: 1036.958
[25,     1] loss: 1045.678
[26,     1] loss: 1049.847
[27,     1] loss: 1013.432
[28,     1] loss: 954.032
[29,     1] loss: 1017.548
[30,     1] loss: 1017.427
[31,     1] loss: 1019.069
[32,     1] loss: 1002.488
[33,     1] loss: 965.940
[34,     1] loss: 1001.571
[35,     1] loss: 1005.133
[36,     1] loss: 985.874
[37,     1] loss: 995.094
[38,     1] loss: 991.267
[39,     1] loss: 962.636
[40,     1] loss: 996.134
[41,     1] loss: 961.147
[42,     1] loss: 917.709
[43,     1] loss: 953.684
[44,     1] loss: 942.009
[45,     1] loss: 930.344
[46,     1] loss: 985.626
[47,     1] loss: 929.340
[48,     1] loss: 928.173
[49,     1] loss: 882.843
[50,     1] loss: 881.632
[51,     1] loss: 881.536
[52,     1] loss: 882.545
[53,     1] loss: 908.127
[54,     1] loss: 877.099
[55,     1] loss: 847.687
[56,     1] loss: 847.588
[57,     1] loss: 872.541
[58,     1] loss: 836.707
[59,     1] loss: 817.003
[60,     1] loss: 885.391
[61,     1] loss: 830.913
[62,     1] loss: 770.813
[63,     1] loss: 839.127
[64,     1] loss: 811.860
[65,     1] loss: 780.161
[66,     1] loss: 851.694
[67,     1] loss: 814.275
[68,     1] loss: 834.716
[69,     1] loss: 787.608
[70,     1] loss: 772.190
[71,     1] loss: 771.923
[72,     1] loss: 725.753
[73,     1] loss: 750.019
[74,     1] loss: 799.318
[75,     1] loss: 724.989
[76,     1] loss: 764.297
[77,     1] loss: 703.522
[78,     1] loss: 769.620
[79,     1] loss: 757.650
[80,     1] loss: 691.611
[81,     1] loss: 758.598
[82,     1] loss: 687.456
[83,     1] loss: 773.887
[84,     1] loss: 711.375
[85,     1] loss: 744.734
[86,     1] loss: 691.037
[87,     1] loss: 685.818
[88,     1] loss: 692.729
[89,     1] loss: 681.977
[90,     1] loss: 659.677
[91,     1] loss: 645.380
[92,     1] loss: 610.426
[93,     1] loss: 648.586
[94,     1] loss: 649.299
[95,     1] loss: 638.681
[96,     1] loss: 596.176
[97,     1] loss: 606.860
[98,     1] loss: 602.067
[99,     1] loss: 631.757
[100,     1] loss: 582.000
[101,     1] loss: 583.088
[102,     1] loss: 556.167
[103,     1] loss: 528.204
[104,     1] loss: 596.637
[105,     1] loss: 573.934
[106,     1] loss: 523.092
[107,     1] loss: 530.621
[108,     1] loss: 518.839
[109,     1] loss: 533.206
[110,     1] loss: 587.212
[111,     1] loss: 506.159
[112,     1] loss: 525.782
[113,     1] loss: 590.353
[114,     1] loss: 511.645
[115,     1] loss: 495.696
[116,     1] loss: 510.561
[117,     1] loss: 542.318
[118,     1] loss: 509.500
[119,     1] loss: 476.930
[120,     1] loss: 543.486
[121,     1] loss: 510.743
[122,     1] loss: 536.201
[123,     1] loss: 536.130
[124,     1] loss: 510.728
[125,     1] loss: 495.234
Early stopping applied (best metric=0.37785887718200684)
Finished Training
Total time taken: 16.91119122505188
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1283.319
[2,     1] loss: 1283.014
[3,     1] loss: 1281.206
[4,     1] loss: 1283.825
[5,     1] loss: 1283.839
[6,     1] loss: 1281.167
[7,     1] loss: 1283.066
[8,     1] loss: 1280.622
[9,     1] loss: 1276.645
[10,     1] loss: 1273.840
[11,     1] loss: 1277.274
[12,     1] loss: 1270.604
[13,     1] loss: 1271.561
[14,     1] loss: 1262.354
[15,     1] loss: 1254.466
[16,     1] loss: 1244.214
[17,     1] loss: 1230.066
[18,     1] loss: 1209.298
[19,     1] loss: 1195.043
[20,     1] loss: 1173.148
[21,     1] loss: 1165.113
[22,     1] loss: 1128.280
[23,     1] loss: 1116.815
[24,     1] loss: 1108.461
[25,     1] loss: 1088.359
[26,     1] loss: 1070.856
[27,     1] loss: 1096.217
[28,     1] loss: 1041.788
[29,     1] loss: 985.878
[30,     1] loss: 1047.583
[31,     1] loss: 1059.259
[32,     1] loss: 1039.375
[33,     1] loss: 1039.281
[34,     1] loss: 997.977
[35,     1] loss: 1046.253
[36,     1] loss: 984.348
[37,     1] loss: 985.208
[38,     1] loss: 1040.336
[39,     1] loss: 1026.828
[40,     1] loss: 966.051
[41,     1] loss: 974.072
[42,     1] loss: 978.371
[43,     1] loss: 956.401
[44,     1] loss: 946.030
[45,     1] loss: 918.201
[46,     1] loss: 949.798
[47,     1] loss: 935.218
[48,     1] loss: 914.028
[49,     1] loss: 880.258
[50,     1] loss: 930.325
[51,     1] loss: 901.834
[52,     1] loss: 867.041
[53,     1] loss: 912.117
[54,     1] loss: 883.268
[55,     1] loss: 899.427
[56,     1] loss: 821.469
[57,     1] loss: 876.572
[58,     1] loss: 827.987
[59,     1] loss: 785.076
[60,     1] loss: 797.335
[61,     1] loss: 800.883
[62,     1] loss: 845.576
[63,     1] loss: 782.667
[64,     1] loss: 761.237
[65,     1] loss: 763.331
[66,     1] loss: 809.115
[67,     1] loss: 777.765
[68,     1] loss: 746.180
[69,     1] loss: 719.843
[70,     1] loss: 756.469
[71,     1] loss: 746.348
[72,     1] loss: 756.899
[73,     1] loss: 686.684
[74,     1] loss: 711.441
[75,     1] loss: 667.383
[76,     1] loss: 790.401
[77,     1] loss: 628.547
[78,     1] loss: 772.254
[79,     1] loss: 641.087
[80,     1] loss: 751.151
[81,     1] loss: 688.058
[82,     1] loss: 678.214
[83,     1] loss: 693.136
[84,     1] loss: 661.366
[85,     1] loss: 668.302
[86,     1] loss: 612.136
[87,     1] loss: 637.691
[88,     1] loss: 697.175
[89,     1] loss: 628.678
[90,     1] loss: 619.831
[91,     1] loss: 574.940
[92,     1] loss: 599.622
[93,     1] loss: 609.673
[94,     1] loss: 607.322
[95,     1] loss: 613.864
[96,     1] loss: 576.626
[97,     1] loss: 571.856
[98,     1] loss: 549.517
[99,     1] loss: 611.752
Early stopping applied (best metric=0.3993510901927948)
Finished Training
Total time taken: 13.389066696166992
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.376
[2,     1] loss: 1282.810
[3,     1] loss: 1277.128
[4,     1] loss: 1280.659
[5,     1] loss: 1279.430
[6,     1] loss: 1277.583
[7,     1] loss: 1273.489
[8,     1] loss: 1278.277
[9,     1] loss: 1275.727
[10,     1] loss: 1276.151
[11,     1] loss: 1276.805
[12,     1] loss: 1274.728
[13,     1] loss: 1269.942
[14,     1] loss: 1271.717
[15,     1] loss: 1263.079
[16,     1] loss: 1254.635
[17,     1] loss: 1244.954
[18,     1] loss: 1232.294
[19,     1] loss: 1215.817
[20,     1] loss: 1203.106
[21,     1] loss: 1166.733
[22,     1] loss: 1163.444
[23,     1] loss: 1132.605
[24,     1] loss: 1158.511
[25,     1] loss: 1130.066
[26,     1] loss: 1075.063
[27,     1] loss: 1079.154
[28,     1] loss: 1087.564
[29,     1] loss: 1090.721
[30,     1] loss: 1077.216
[31,     1] loss: 1035.025
[32,     1] loss: 1077.841
[33,     1] loss: 1038.812
[34,     1] loss: 1089.657
[35,     1] loss: 1005.123
[36,     1] loss: 1026.859
[37,     1] loss: 1007.698
[38,     1] loss: 1030.006
[39,     1] loss: 1023.801
[40,     1] loss: 1025.479
[41,     1] loss: 1020.727
[42,     1] loss: 1006.832
[43,     1] loss: 986.290
[44,     1] loss: 973.371
[45,     1] loss: 982.513
[46,     1] loss: 988.305
[47,     1] loss: 997.128
[48,     1] loss: 959.859
[49,     1] loss: 958.729
[50,     1] loss: 994.142
[51,     1] loss: 937.030
[52,     1] loss: 956.097
[53,     1] loss: 924.060
[54,     1] loss: 916.603
[55,     1] loss: 890.456
[56,     1] loss: 924.226
[57,     1] loss: 929.195
[58,     1] loss: 902.136
[59,     1] loss: 887.197
[60,     1] loss: 933.985
[61,     1] loss: 891.376
[62,     1] loss: 831.218
[63,     1] loss: 849.403
[64,     1] loss: 870.487
[65,     1] loss: 808.061
[66,     1] loss: 836.900
[67,     1] loss: 825.427
[68,     1] loss: 868.016
[69,     1] loss: 867.126
[70,     1] loss: 865.734
[71,     1] loss: 765.508
[72,     1] loss: 763.524
[73,     1] loss: 813.231
[74,     1] loss: 757.575
[75,     1] loss: 731.555
[76,     1] loss: 742.592
[77,     1] loss: 748.276
[78,     1] loss: 753.132
[79,     1] loss: 693.602
[80,     1] loss: 751.023
[81,     1] loss: 702.728
[82,     1] loss: 741.151
Early stopping applied (best metric=0.3414683938026428)
Finished Training
Total time taken: 11.10411810874939
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.729
[2,     1] loss: 1279.221
[3,     1] loss: 1279.187
[4,     1] loss: 1278.113
[5,     1] loss: 1280.264
[6,     1] loss: 1277.248
[7,     1] loss: 1276.039
[8,     1] loss: 1276.826
[9,     1] loss: 1271.325
[10,     1] loss: 1273.067
[11,     1] loss: 1265.106
[12,     1] loss: 1260.898
[13,     1] loss: 1263.422
[14,     1] loss: 1245.958
[15,     1] loss: 1231.012
[16,     1] loss: 1216.413
[17,     1] loss: 1192.714
[18,     1] loss: 1176.663
[19,     1] loss: 1170.161
[20,     1] loss: 1149.525
[21,     1] loss: 1107.114
[22,     1] loss: 1138.816
[23,     1] loss: 1100.784
[24,     1] loss: 1068.129
[25,     1] loss: 1081.140
[26,     1] loss: 1052.090
[27,     1] loss: 1005.489
[28,     1] loss: 1061.411
[29,     1] loss: 1054.506
[30,     1] loss: 1047.972
[31,     1] loss: 1066.351
[32,     1] loss: 1020.123
[33,     1] loss: 1004.521
[34,     1] loss: 1033.214
[35,     1] loss: 1000.192
[36,     1] loss: 1033.377
[37,     1] loss: 965.081
[38,     1] loss: 1009.186
[39,     1] loss: 987.760
[40,     1] loss: 1006.638
[41,     1] loss: 965.497
[42,     1] loss: 978.236
[43,     1] loss: 951.482
[44,     1] loss: 917.537
[45,     1] loss: 918.008
[46,     1] loss: 933.988
[47,     1] loss: 961.074
[48,     1] loss: 899.381
[49,     1] loss: 914.760
[50,     1] loss: 914.718
[51,     1] loss: 867.840
[52,     1] loss: 834.476
[53,     1] loss: 874.155
[54,     1] loss: 841.512
[55,     1] loss: 833.720
[56,     1] loss: 912.076
[57,     1] loss: 833.740
[58,     1] loss: 846.752
[59,     1] loss: 826.635
[60,     1] loss: 858.887
[61,     1] loss: 853.027
[62,     1] loss: 787.324
[63,     1] loss: 779.571
[64,     1] loss: 804.348
[65,     1] loss: 812.034
[66,     1] loss: 765.294
[67,     1] loss: 757.303
[68,     1] loss: 815.544
[69,     1] loss: 849.218
[70,     1] loss: 732.173
[71,     1] loss: 879.408
[72,     1] loss: 723.345
[73,     1] loss: 818.900
[74,     1] loss: 762.475
[75,     1] loss: 840.586
[76,     1] loss: 738.549
[77,     1] loss: 678.913
[78,     1] loss: 754.675
[79,     1] loss: 772.234
[80,     1] loss: 680.375
[81,     1] loss: 692.745
[82,     1] loss: 689.838
Early stopping applied (best metric=0.4213614761829376)
Finished Training
Total time taken: 11.128118515014648
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.204
[2,     1] loss: 1283.330
[3,     1] loss: 1278.699
[4,     1] loss: 1280.078
[5,     1] loss: 1276.152
[6,     1] loss: 1276.788
[7,     1] loss: 1276.574
[8,     1] loss: 1277.688
[9,     1] loss: 1273.237
[10,     1] loss: 1270.329
[11,     1] loss: 1262.237
[12,     1] loss: 1253.822
[13,     1] loss: 1238.894
[14,     1] loss: 1216.987
[15,     1] loss: 1196.093
[16,     1] loss: 1183.288
[17,     1] loss: 1145.865
[18,     1] loss: 1134.951
[19,     1] loss: 1124.357
[20,     1] loss: 1112.191
[21,     1] loss: 1096.001
[22,     1] loss: 1111.203
[23,     1] loss: 1073.712
[24,     1] loss: 1041.430
[25,     1] loss: 1030.038
[26,     1] loss: 1011.026
[27,     1] loss: 1044.634
[28,     1] loss: 1059.843
[29,     1] loss: 1030.185
[30,     1] loss: 1006.608
[31,     1] loss: 1002.930
[32,     1] loss: 1007.292
[33,     1] loss: 1004.646
[34,     1] loss: 1045.439
[35,     1] loss: 1022.836
[36,     1] loss: 1014.093
[37,     1] loss: 965.748
[38,     1] loss: 950.767
[39,     1] loss: 993.147
[40,     1] loss: 948.542
[41,     1] loss: 906.918
[42,     1] loss: 974.341
[43,     1] loss: 1017.136
[44,     1] loss: 916.755
[45,     1] loss: 933.477
[46,     1] loss: 940.965
[47,     1] loss: 928.404
[48,     1] loss: 976.610
[49,     1] loss: 927.492
[50,     1] loss: 951.503
[51,     1] loss: 936.052
[52,     1] loss: 919.721
[53,     1] loss: 930.282
[54,     1] loss: 892.484
[55,     1] loss: 914.346
[56,     1] loss: 911.088
[57,     1] loss: 861.613
[58,     1] loss: 872.339
[59,     1] loss: 882.607
[60,     1] loss: 858.352
[61,     1] loss: 800.022
[62,     1] loss: 823.488
[63,     1] loss: 795.481
[64,     1] loss: 802.329
[65,     1] loss: 833.204
[66,     1] loss: 862.775
[67,     1] loss: 810.439
[68,     1] loss: 768.716
[69,     1] loss: 822.281
[70,     1] loss: 789.408
[71,     1] loss: 819.396
[72,     1] loss: 772.007
[73,     1] loss: 778.486
[74,     1] loss: 755.626
[75,     1] loss: 770.753
[76,     1] loss: 734.024
[77,     1] loss: 714.543
[78,     1] loss: 741.544
[79,     1] loss: 696.731
[80,     1] loss: 745.044
[81,     1] loss: 712.462
[82,     1] loss: 791.527
[83,     1] loss: 708.949
[84,     1] loss: 696.343
[85,     1] loss: 771.822
[86,     1] loss: 663.591
[87,     1] loss: 671.909
[88,     1] loss: 666.528
[89,     1] loss: 691.885
[90,     1] loss: 678.138
[91,     1] loss: 601.600
[92,     1] loss: 633.682
[93,     1] loss: 624.169
[94,     1] loss: 743.531
[95,     1] loss: 609.080
[96,     1] loss: 663.259
[97,     1] loss: 596.722
[98,     1] loss: 638.847
[99,     1] loss: 594.708
[100,     1] loss: 623.731
[101,     1] loss: 597.694
[102,     1] loss: 610.776
[103,     1] loss: 540.990
[104,     1] loss: 621.361
[105,     1] loss: 579.025
[106,     1] loss: 609.653
[107,     1] loss: 571.081
[108,     1] loss: 571.244
[109,     1] loss: 565.495
[110,     1] loss: 570.590
[111,     1] loss: 538.037
[112,     1] loss: 564.983
[113,     1] loss: 513.739
[114,     1] loss: 557.422
[115,     1] loss: 541.689
[116,     1] loss: 461.122
[117,     1] loss: 503.692
Early stopping applied (best metric=0.3572826683521271)
Finished Training
Total time taken: 15.852076292037964
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.352
[2,     1] loss: 1276.584
[3,     1] loss: 1277.660
[4,     1] loss: 1278.838
[5,     1] loss: 1272.219
[6,     1] loss: 1279.501
[7,     1] loss: 1277.192
[8,     1] loss: 1276.168
[9,     1] loss: 1276.731
[10,     1] loss: 1267.345
[11,     1] loss: 1266.867
[12,     1] loss: 1257.012
[13,     1] loss: 1243.270
[14,     1] loss: 1232.889
[15,     1] loss: 1207.870
[16,     1] loss: 1209.449
[17,     1] loss: 1182.081
[18,     1] loss: 1146.904
[19,     1] loss: 1148.346
[20,     1] loss: 1118.478
[21,     1] loss: 1092.598
[22,     1] loss: 1087.585
[23,     1] loss: 1051.327
[24,     1] loss: 1026.175
[25,     1] loss: 1049.807
[26,     1] loss: 1033.267
[27,     1] loss: 1075.801
[28,     1] loss: 1083.341
[29,     1] loss: 1057.476
[30,     1] loss: 1026.505
[31,     1] loss: 1033.985
[32,     1] loss: 993.814
[33,     1] loss: 1018.848
[34,     1] loss: 978.380
[35,     1] loss: 1014.930
[36,     1] loss: 1000.959
[37,     1] loss: 970.098
[38,     1] loss: 998.385
[39,     1] loss: 972.987
[40,     1] loss: 945.863
[41,     1] loss: 933.624
[42,     1] loss: 943.913
[43,     1] loss: 954.961
[44,     1] loss: 939.789
[45,     1] loss: 920.800
[46,     1] loss: 918.808
[47,     1] loss: 936.271
[48,     1] loss: 931.618
[49,     1] loss: 845.441
[50,     1] loss: 921.871
[51,     1] loss: 893.734
[52,     1] loss: 905.778
[53,     1] loss: 882.989
[54,     1] loss: 888.194
[55,     1] loss: 875.667
[56,     1] loss: 859.658
[57,     1] loss: 819.565
[58,     1] loss: 844.821
[59,     1] loss: 846.937
[60,     1] loss: 871.247
[61,     1] loss: 855.006
[62,     1] loss: 884.633
[63,     1] loss: 825.873
[64,     1] loss: 751.690
[65,     1] loss: 822.108
[66,     1] loss: 827.034
[67,     1] loss: 829.223
[68,     1] loss: 820.516
[69,     1] loss: 754.177
[70,     1] loss: 861.262
[71,     1] loss: 802.408
[72,     1] loss: 774.315
[73,     1] loss: 720.345
[74,     1] loss: 830.682
[75,     1] loss: 738.345
[76,     1] loss: 801.219
[77,     1] loss: 756.195
[78,     1] loss: 762.027
[79,     1] loss: 709.000
[80,     1] loss: 690.602
[81,     1] loss: 651.774
[82,     1] loss: 683.768
[83,     1] loss: 697.534
[84,     1] loss: 708.172
[85,     1] loss: 669.497
[86,     1] loss: 685.789
[87,     1] loss: 640.630
[88,     1] loss: 659.620
[89,     1] loss: 669.658
[90,     1] loss: 671.388
[91,     1] loss: 668.120
[92,     1] loss: 643.759
[93,     1] loss: 604.365
[94,     1] loss: 608.680
[95,     1] loss: 663.844
[96,     1] loss: 609.714
[97,     1] loss: 630.552
[98,     1] loss: 647.424
[99,     1] loss: 599.279
[100,     1] loss: 600.445
[101,     1] loss: 603.610
[102,     1] loss: 684.744
[103,     1] loss: 599.254
[104,     1] loss: 571.693
[105,     1] loss: 625.952
[106,     1] loss: 570.093
[107,     1] loss: 582.901
[108,     1] loss: 611.956
[109,     1] loss: 534.524
[110,     1] loss: 548.341
[111,     1] loss: 511.046
[112,     1] loss: 575.385
[113,     1] loss: 554.959
Early stopping applied (best metric=0.4018717110157013)
Finished Training
Total time taken: 15.297622203826904
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1285.350
[2,     1] loss: 1283.030
[3,     1] loss: 1281.510
[4,     1] loss: 1283.375
[5,     1] loss: 1279.320
[6,     1] loss: 1284.059
[7,     1] loss: 1282.054
[8,     1] loss: 1280.832
[9,     1] loss: 1279.636
[10,     1] loss: 1277.329
[11,     1] loss: 1280.586
[12,     1] loss: 1280.074
[13,     1] loss: 1277.088
[14,     1] loss: 1275.802
[15,     1] loss: 1275.615
[16,     1] loss: 1270.081
[17,     1] loss: 1264.805
[18,     1] loss: 1256.608
[19,     1] loss: 1246.942
[20,     1] loss: 1226.903
[21,     1] loss: 1218.324
[22,     1] loss: 1197.166
[23,     1] loss: 1177.342
[24,     1] loss: 1170.221
[25,     1] loss: 1140.532
[26,     1] loss: 1121.292
[27,     1] loss: 1111.235
[28,     1] loss: 1063.133
[29,     1] loss: 1075.721
[30,     1] loss: 1068.886
[31,     1] loss: 1072.172
[32,     1] loss: 1029.347
[33,     1] loss: 1078.757
[34,     1] loss: 1065.461
[35,     1] loss: 1058.954
[36,     1] loss: 1041.605
[37,     1] loss: 1010.998
[38,     1] loss: 1043.650
[39,     1] loss: 1036.609
[40,     1] loss: 1005.991
[41,     1] loss: 995.647
[42,     1] loss: 962.750
[43,     1] loss: 1012.790
[44,     1] loss: 980.345
[45,     1] loss: 1026.824
[46,     1] loss: 1007.011
[47,     1] loss: 982.887
[48,     1] loss: 1041.482
[49,     1] loss: 963.927
[50,     1] loss: 973.477
[51,     1] loss: 972.364
[52,     1] loss: 974.167
[53,     1] loss: 959.136
[54,     1] loss: 903.089
[55,     1] loss: 943.791
[56,     1] loss: 916.920
[57,     1] loss: 889.623
[58,     1] loss: 930.907
[59,     1] loss: 940.863
[60,     1] loss: 860.005
[61,     1] loss: 838.684
[62,     1] loss: 876.863
[63,     1] loss: 853.435
[64,     1] loss: 929.471
[65,     1] loss: 854.916
[66,     1] loss: 861.886
[67,     1] loss: 826.657
[68,     1] loss: 891.395
[69,     1] loss: 793.088
[70,     1] loss: 838.533
[71,     1] loss: 849.872
[72,     1] loss: 754.455
[73,     1] loss: 838.913
[74,     1] loss: 770.026
[75,     1] loss: 841.076
[76,     1] loss: 797.056
[77,     1] loss: 796.575
[78,     1] loss: 790.074
[79,     1] loss: 718.419
[80,     1] loss: 743.321
[81,     1] loss: 737.291
[82,     1] loss: 787.299
[83,     1] loss: 748.153
[84,     1] loss: 767.475
[85,     1] loss: 754.105
[86,     1] loss: 652.194
[87,     1] loss: 711.210
[88,     1] loss: 672.867
[89,     1] loss: 676.011
[90,     1] loss: 655.561
[91,     1] loss: 690.351
[92,     1] loss: 614.429
[93,     1] loss: 703.679
[94,     1] loss: 612.655
[95,     1] loss: 624.726
[96,     1] loss: 615.965
[97,     1] loss: 628.785
[98,     1] loss: 555.984
[99,     1] loss: 677.046
[100,     1] loss: 612.274
[101,     1] loss: 618.717
[102,     1] loss: 592.029
[103,     1] loss: 650.845
[104,     1] loss: 574.889
[105,     1] loss: 624.828
[106,     1] loss: 549.618
[107,     1] loss: 551.277
[108,     1] loss: 554.261
[109,     1] loss: 546.330
[110,     1] loss: 553.953
Early stopping applied (best metric=0.37519267201423645)
Finished Training
Total time taken: 16.427173376083374
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.447
[2,     1] loss: 1278.907
[3,     1] loss: 1280.069
[4,     1] loss: 1279.187
[5,     1] loss: 1277.167
[6,     1] loss: 1274.889
[7,     1] loss: 1275.739
[8,     1] loss: 1271.571
[9,     1] loss: 1269.383
[10,     1] loss: 1261.016
[11,     1] loss: 1253.609
[12,     1] loss: 1240.107
[13,     1] loss: 1231.569
[14,     1] loss: 1202.433
[15,     1] loss: 1180.435
[16,     1] loss: 1155.542
[17,     1] loss: 1132.181
[18,     1] loss: 1117.349
[19,     1] loss: 1084.935
[20,     1] loss: 1052.773
[21,     1] loss: 1078.389
[22,     1] loss: 1051.396
[23,     1] loss: 1082.588
[24,     1] loss: 1043.292
[25,     1] loss: 1075.565
[26,     1] loss: 1014.418
[27,     1] loss: 1048.854
[28,     1] loss: 1039.713
[29,     1] loss: 1026.596
[30,     1] loss: 991.928
[31,     1] loss: 1059.246
[32,     1] loss: 1006.822
[33,     1] loss: 1072.876
[34,     1] loss: 1005.268
[35,     1] loss: 1009.574
[36,     1] loss: 1067.829
[37,     1] loss: 990.453
[38,     1] loss: 1001.570
[39,     1] loss: 1018.737
[40,     1] loss: 982.133
[41,     1] loss: 1016.924
[42,     1] loss: 995.703
[43,     1] loss: 967.428
[44,     1] loss: 981.924
[45,     1] loss: 932.636
[46,     1] loss: 945.681
[47,     1] loss: 897.745
[48,     1] loss: 938.536
[49,     1] loss: 993.427
[50,     1] loss: 941.324
[51,     1] loss: 932.234
[52,     1] loss: 917.947
[53,     1] loss: 888.215
[54,     1] loss: 898.413
[55,     1] loss: 921.471
[56,     1] loss: 864.175
[57,     1] loss: 864.049
[58,     1] loss: 895.317
[59,     1] loss: 889.892
[60,     1] loss: 811.174
[61,     1] loss: 852.646
[62,     1] loss: 866.654
[63,     1] loss: 830.073
[64,     1] loss: 860.088
[65,     1] loss: 876.011
[66,     1] loss: 830.459
[67,     1] loss: 786.108
[68,     1] loss: 750.466
[69,     1] loss: 763.195
[70,     1] loss: 769.313
[71,     1] loss: 776.883
[72,     1] loss: 739.228
[73,     1] loss: 758.821
[74,     1] loss: 702.005
[75,     1] loss: 758.760
[76,     1] loss: 813.784
[77,     1] loss: 722.871
[78,     1] loss: 652.445
[79,     1] loss: 741.929
[80,     1] loss: 706.205
[81,     1] loss: 727.847
[82,     1] loss: 684.696
[83,     1] loss: 739.539
[84,     1] loss: 663.353
[85,     1] loss: 743.943
[86,     1] loss: 621.457
[87,     1] loss: 722.329
[88,     1] loss: 689.515
[89,     1] loss: 756.736
[90,     1] loss: 632.719
[91,     1] loss: 671.405
[92,     1] loss: 611.855
[93,     1] loss: 587.869
[94,     1] loss: 592.271
[95,     1] loss: 611.902
[96,     1] loss: 559.474
[97,     1] loss: 593.982
[98,     1] loss: 537.777
[99,     1] loss: 582.484
[100,     1] loss: 573.104
[101,     1] loss: 558.570
[102,     1] loss: 594.035
[103,     1] loss: 608.514
[104,     1] loss: 554.948
[105,     1] loss: 567.502
[106,     1] loss: 565.763
[107,     1] loss: 609.795
[108,     1] loss: 534.990
[109,     1] loss: 502.342
[110,     1] loss: 554.251
Early stopping applied (best metric=0.3568825125694275)
Finished Training
Total time taken: 18.388088703155518
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.932
[2,     1] loss: 1283.293
[3,     1] loss: 1283.332
[4,     1] loss: 1280.118
[5,     1] loss: 1279.078
[6,     1] loss: 1278.341
[7,     1] loss: 1274.341
[8,     1] loss: 1272.587
[9,     1] loss: 1274.979
[10,     1] loss: 1270.235
[11,     1] loss: 1261.792
[12,     1] loss: 1255.897
[13,     1] loss: 1244.830
[14,     1] loss: 1228.438
[15,     1] loss: 1213.269
[16,     1] loss: 1185.213
[17,     1] loss: 1163.908
[18,     1] loss: 1150.118
[19,     1] loss: 1127.625
[20,     1] loss: 1095.280
[21,     1] loss: 1067.242
[22,     1] loss: 1062.357
[23,     1] loss: 1025.106
[24,     1] loss: 1032.160
[25,     1] loss: 989.291
[26,     1] loss: 1014.980
[27,     1] loss: 1065.476
[28,     1] loss: 1045.491
[29,     1] loss: 979.049
[30,     1] loss: 987.313
[31,     1] loss: 1032.925
[32,     1] loss: 935.871
[33,     1] loss: 993.527
[34,     1] loss: 986.772
[35,     1] loss: 941.935
[36,     1] loss: 1004.902
[37,     1] loss: 982.756
[38,     1] loss: 943.066
[39,     1] loss: 931.031
[40,     1] loss: 925.151
[41,     1] loss: 980.935
[42,     1] loss: 885.605
[43,     1] loss: 868.485
[44,     1] loss: 943.841
[45,     1] loss: 888.442
[46,     1] loss: 877.242
[47,     1] loss: 888.855
[48,     1] loss: 845.846
[49,     1] loss: 856.806
[50,     1] loss: 823.173
[51,     1] loss: 876.128
[52,     1] loss: 841.834
[53,     1] loss: 896.502
[54,     1] loss: 857.328
[55,     1] loss: 835.821
[56,     1] loss: 831.750
[57,     1] loss: 768.250
[58,     1] loss: 794.834
[59,     1] loss: 778.640
[60,     1] loss: 815.922
[61,     1] loss: 793.748
[62,     1] loss: 783.029
[63,     1] loss: 794.387
[64,     1] loss: 773.634
[65,     1] loss: 764.027
[66,     1] loss: 765.460
[67,     1] loss: 791.342
[68,     1] loss: 761.724
[69,     1] loss: 734.334
[70,     1] loss: 732.277
[71,     1] loss: 725.948
[72,     1] loss: 714.723
[73,     1] loss: 719.993
[74,     1] loss: 717.287
[75,     1] loss: 691.647
[76,     1] loss: 713.299
[77,     1] loss: 694.958
[78,     1] loss: 646.452
[79,     1] loss: 683.786
[80,     1] loss: 689.611
[81,     1] loss: 682.062
[82,     1] loss: 705.205
[83,     1] loss: 681.194
[84,     1] loss: 707.277
[85,     1] loss: 671.951
[86,     1] loss: 703.747
[87,     1] loss: 599.938
[88,     1] loss: 640.395
[89,     1] loss: 649.818
[90,     1] loss: 670.618
[91,     1] loss: 660.770
[92,     1] loss: 594.422
[93,     1] loss: 664.874
[94,     1] loss: 624.277
[95,     1] loss: 658.415
[96,     1] loss: 559.521
[97,     1] loss: 592.226
[98,     1] loss: 557.380
[99,     1] loss: 553.949
[100,     1] loss: 601.513
[101,     1] loss: 586.187
[102,     1] loss: 642.377
Early stopping applied (best metric=0.40790948271751404)
Finished Training
Total time taken: 15.204396724700928
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.273
[2,     1] loss: 1280.595
[3,     1] loss: 1277.638
[4,     1] loss: 1283.622
[5,     1] loss: 1281.939
[6,     1] loss: 1279.297
[7,     1] loss: 1278.288
[8,     1] loss: 1276.714
[9,     1] loss: 1275.009
[10,     1] loss: 1276.316
[11,     1] loss: 1274.725
[12,     1] loss: 1271.926
[13,     1] loss: 1268.809
[14,     1] loss: 1265.959
[15,     1] loss: 1256.854
[16,     1] loss: 1243.422
[17,     1] loss: 1233.286
[18,     1] loss: 1220.853
[19,     1] loss: 1196.860
[20,     1] loss: 1165.313
[21,     1] loss: 1153.887
[22,     1] loss: 1149.772
[23,     1] loss: 1124.286
[24,     1] loss: 1121.073
[25,     1] loss: 1107.996
[26,     1] loss: 1080.403
[27,     1] loss: 1084.683
[28,     1] loss: 1062.753
[29,     1] loss: 1094.132
[30,     1] loss: 1029.160
[31,     1] loss: 1023.549
[32,     1] loss: 1077.216
[33,     1] loss: 1048.063
[34,     1] loss: 1049.904
[35,     1] loss: 1032.772
[36,     1] loss: 1006.111
[37,     1] loss: 1017.488
[38,     1] loss: 1032.852
[39,     1] loss: 995.530
[40,     1] loss: 980.049
[41,     1] loss: 979.038
[42,     1] loss: 958.021
[43,     1] loss: 964.025
[44,     1] loss: 955.489
[45,     1] loss: 941.404
[46,     1] loss: 930.187
[47,     1] loss: 870.772
[48,     1] loss: 951.616
[49,     1] loss: 886.501
[50,     1] loss: 907.820
[51,     1] loss: 900.053
[52,     1] loss: 888.653
[53,     1] loss: 883.727
[54,     1] loss: 871.029
[55,     1] loss: 882.167
[56,     1] loss: 854.689
[57,     1] loss: 861.514
[58,     1] loss: 813.982
[59,     1] loss: 895.921
[60,     1] loss: 842.984
[61,     1] loss: 829.176
[62,     1] loss: 793.238
[63,     1] loss: 832.407
[64,     1] loss: 773.230
[65,     1] loss: 817.146
[66,     1] loss: 797.881
[67,     1] loss: 797.221
[68,     1] loss: 764.457
[69,     1] loss: 793.387
[70,     1] loss: 750.115
[71,     1] loss: 741.884
[72,     1] loss: 733.157
[73,     1] loss: 703.366
[74,     1] loss: 751.987
[75,     1] loss: 748.149
[76,     1] loss: 699.240
[77,     1] loss: 706.132
[78,     1] loss: 655.371
[79,     1] loss: 692.917
[80,     1] loss: 783.087
[81,     1] loss: 691.169
[82,     1] loss: 793.865
[83,     1] loss: 667.484
[84,     1] loss: 753.817
[85,     1] loss: 644.599
[86,     1] loss: 727.057
[87,     1] loss: 590.399
[88,     1] loss: 718.061
[89,     1] loss: 638.349
[90,     1] loss: 676.677
[91,     1] loss: 585.704
[92,     1] loss: 631.885
[93,     1] loss: 597.156
[94,     1] loss: 583.978
Early stopping applied (best metric=0.36816728115081787)
Finished Training
Total time taken: 14.322649717330933
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.846
[2,     1] loss: 1286.113
[3,     1] loss: 1281.916
[4,     1] loss: 1277.637
[5,     1] loss: 1279.822
[6,     1] loss: 1279.677
[7,     1] loss: 1277.875
[8,     1] loss: 1280.767
[9,     1] loss: 1276.714
[10,     1] loss: 1277.908
[11,     1] loss: 1275.854
[12,     1] loss: 1273.822
[13,     1] loss: 1269.400
[14,     1] loss: 1266.909
[15,     1] loss: 1258.964
[16,     1] loss: 1256.111
[17,     1] loss: 1243.588
[18,     1] loss: 1237.779
[19,     1] loss: 1211.214
[20,     1] loss: 1190.013
[21,     1] loss: 1186.927
[22,     1] loss: 1171.962
[23,     1] loss: 1173.838
[24,     1] loss: 1122.545
[25,     1] loss: 1115.554
[26,     1] loss: 1087.059
[27,     1] loss: 1105.476
[28,     1] loss: 1081.952
[29,     1] loss: 1064.203
[30,     1] loss: 1091.810
[31,     1] loss: 1097.797
[32,     1] loss: 1028.243
[33,     1] loss: 1051.319
[34,     1] loss: 1054.787
[35,     1] loss: 1066.126
[36,     1] loss: 1048.209
[37,     1] loss: 1002.482
[38,     1] loss: 1031.808
[39,     1] loss: 1030.855
[40,     1] loss: 1013.514
[41,     1] loss: 1019.551
[42,     1] loss: 989.448
[43,     1] loss: 990.510
[44,     1] loss: 978.619
[45,     1] loss: 998.371
[46,     1] loss: 991.374
[47,     1] loss: 993.574
[48,     1] loss: 973.187
[49,     1] loss: 982.818
[50,     1] loss: 945.186
[51,     1] loss: 945.091
[52,     1] loss: 901.128
[53,     1] loss: 934.879
[54,     1] loss: 963.873
[55,     1] loss: 944.632
[56,     1] loss: 901.470
[57,     1] loss: 860.929
[58,     1] loss: 901.319
[59,     1] loss: 892.088
[60,     1] loss: 824.034
[61,     1] loss: 816.834
[62,     1] loss: 876.234
[63,     1] loss: 865.620
[64,     1] loss: 809.097
[65,     1] loss: 841.187
[66,     1] loss: 835.226
[67,     1] loss: 781.985
[68,     1] loss: 801.366
[69,     1] loss: 754.399
[70,     1] loss: 774.931
[71,     1] loss: 774.604
[72,     1] loss: 763.009
[73,     1] loss: 737.646
[74,     1] loss: 781.332
[75,     1] loss: 745.266
[76,     1] loss: 698.917
[77,     1] loss: 720.738
[78,     1] loss: 771.299
[79,     1] loss: 713.740
[80,     1] loss: 728.826
[81,     1] loss: 759.572
[82,     1] loss: 752.182
[83,     1] loss: 708.675
[84,     1] loss: 773.885
[85,     1] loss: 649.588
[86,     1] loss: 789.353
[87,     1] loss: 712.459
[88,     1] loss: 730.361
[89,     1] loss: 653.063
[90,     1] loss: 714.828
[91,     1] loss: 642.842
[92,     1] loss: 664.470
[93,     1] loss: 603.409
[94,     1] loss: 604.414
[95,     1] loss: 615.609
[96,     1] loss: 616.738
[97,     1] loss: 605.483
[98,     1] loss: 605.474
Early stopping applied (best metric=0.2948519289493561)
Finished Training
Total time taken: 16.325551986694336
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1281.947
[2,     1] loss: 1281.031
[3,     1] loss: 1280.702
[4,     1] loss: 1281.359
[5,     1] loss: 1281.967
[6,     1] loss: 1278.758
[7,     1] loss: 1280.129
[8,     1] loss: 1276.214
[9,     1] loss: 1272.225
[10,     1] loss: 1271.969
[11,     1] loss: 1263.952
[12,     1] loss: 1252.986
[13,     1] loss: 1238.210
[14,     1] loss: 1225.956
[15,     1] loss: 1201.107
[16,     1] loss: 1188.543
[17,     1] loss: 1171.403
[18,     1] loss: 1153.118
[19,     1] loss: 1130.992
[20,     1] loss: 1104.390
[21,     1] loss: 1125.918
[22,     1] loss: 1102.813
[23,     1] loss: 1109.737
[24,     1] loss: 1068.890
[25,     1] loss: 1085.217
[26,     1] loss: 1082.352
[27,     1] loss: 1091.237
[28,     1] loss: 1046.823
[29,     1] loss: 1057.444
[30,     1] loss: 1061.317
[31,     1] loss: 1038.872
[32,     1] loss: 1053.091
[33,     1] loss: 1034.341
[34,     1] loss: 976.185
[35,     1] loss: 974.446
[36,     1] loss: 1029.372
[37,     1] loss: 982.548
[38,     1] loss: 946.098
[39,     1] loss: 977.294
[40,     1] loss: 995.277
[41,     1] loss: 970.180
[42,     1] loss: 1001.143
[43,     1] loss: 974.060
[44,     1] loss: 988.401
[45,     1] loss: 964.074
[46,     1] loss: 975.656
[47,     1] loss: 948.707
[48,     1] loss: 931.748
[49,     1] loss: 944.889
[50,     1] loss: 909.023
[51,     1] loss: 881.672
[52,     1] loss: 904.376
[53,     1] loss: 926.398
[54,     1] loss: 916.999
[55,     1] loss: 882.378
[56,     1] loss: 882.788
[57,     1] loss: 906.631
[58,     1] loss: 844.272
[59,     1] loss: 879.256
[60,     1] loss: 840.612
[61,     1] loss: 876.504
[62,     1] loss: 866.471
[63,     1] loss: 865.066
[64,     1] loss: 816.815
[65,     1] loss: 850.523
[66,     1] loss: 810.656
[67,     1] loss: 842.215
[68,     1] loss: 780.445
[69,     1] loss: 817.562
[70,     1] loss: 712.606
[71,     1] loss: 816.073
[72,     1] loss: 752.703
[73,     1] loss: 732.970
[74,     1] loss: 737.725
[75,     1] loss: 743.606
[76,     1] loss: 737.884
[77,     1] loss: 730.266
[78,     1] loss: 761.240
[79,     1] loss: 698.648
[80,     1] loss: 701.828
[81,     1] loss: 633.306
[82,     1] loss: 720.293
[83,     1] loss: 696.726
[84,     1] loss: 682.034
[85,     1] loss: 710.518
[86,     1] loss: 676.393
[87,     1] loss: 638.146
[88,     1] loss: 699.137
[89,     1] loss: 594.618
[90,     1] loss: 640.918
[91,     1] loss: 659.873
[92,     1] loss: 633.903
[93,     1] loss: 602.968
[94,     1] loss: 619.002
[95,     1] loss: 638.662
[96,     1] loss: 615.914
[97,     1] loss: 602.105
[98,     1] loss: 577.312
[99,     1] loss: 585.793
[100,     1] loss: 575.401
[101,     1] loss: 572.363
[102,     1] loss: 606.989
[103,     1] loss: 553.081
[104,     1] loss: 568.036
[105,     1] loss: 560.037
[106,     1] loss: 564.698
[107,     1] loss: 621.817
[108,     1] loss: 582.097
[109,     1] loss: 623.997
[110,     1] loss: 562.213
[111,     1] loss: 592.370
[112,     1] loss: 546.838
[113,     1] loss: 503.465
[114,     1] loss: 596.726
[115,     1] loss: 640.862
[116,     1] loss: 534.870
[117,     1] loss: 496.278
[118,     1] loss: 523.358
[119,     1] loss: 503.183
[120,     1] loss: 551.001
[121,     1] loss: 489.431
Early stopping applied (best metric=0.3319019675254822)
Finished Training
Total time taken: 18.812373399734497
{'Hydroxylation-K Validation Accuracy': 0.7659278959810875, 'Hydroxylation-K Validation Sensitivity': 0.6577777777777778, 'Hydroxylation-K Validation Specificity': 0.7929824561403509, 'Hydroxylation-K Validation Precision': 0.44873703437944923, 'Hydroxylation-K AUC ROC': 0.7899805068226121, 'Hydroxylation-K AUC PR': 0.5819046344419984, 'Hydroxylation-K MCC': 0.3979510017723125, 'Hydroxylation-K F1': 0.5286744918139221, 'Validation Loss (Hydroxylation-K)': 0.4606122155984243, 'Hydroxylation-P Validation Accuracy': 0.7782190582542341, 'Hydroxylation-P Validation Sensitivity': 0.8047619047619048, 'Hydroxylation-P Validation Specificity': 0.7725123447553494, 'Hydroxylation-P Validation Precision': 0.4356484364987282, 'Hydroxylation-P AUC ROC': 0.851674457103905, 'Hydroxylation-P AUC PR': 0.5830189330485066, 'Hydroxylation-P MCC': 0.47098133468153774, 'Hydroxylation-P F1': 0.5633409278842189, 'Validation Loss (Hydroxylation-P)': 0.3689849813779195, 'Validation Loss (total)': 0.8295971989631653, 'TimeToTrain': 15.325893370310466}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008584123522743353,
 'learning_rate_Hydroxylation-K': 0.0065086742544610285,
 'learning_rate_Hydroxylation-P': 0.0019554030244508394,
 'log_base': 1.7264110472809588,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1114734373,
 'sample_weights': [1.7644497282852298, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.657002951006112,
 'weight_decay_Hydroxylation-K': 7.399610529294527,
 'weight_decay_Hydroxylation-P': 7.5363316501440805}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1552.178
[2,     1] loss: 1551.470
[3,     1] loss: 1551.655
[4,     1] loss: 1551.301
[5,     1] loss: 1551.497
[6,     1] loss: 1550.307
[7,     1] loss: 1556.801
[8,     1] loss: 1549.915
[9,     1] loss: 1545.266
[10,     1] loss: 1546.391
[11,     1] loss: 1537.223
[12,     1] loss: 1532.294
[13,     1] loss: 1510.985
[14,     1] loss: 1485.714
[15,     1] loss: 1443.127
[16,     1] loss: 1414.047
[17,     1] loss: 1386.375
[18,     1] loss: 1385.978
[19,     1] loss: 1331.835
[20,     1] loss: 1300.131
[21,     1] loss: 1230.196
[22,     1] loss: 1340.816
[23,     1] loss: 1308.626
[24,     1] loss: 1280.831
[25,     1] loss: 1333.171
[26,     1] loss: 1255.529
[27,     1] loss: 1295.794
[28,     1] loss: 1262.214
[29,     1] loss: 1247.515
[30,     1] loss: 1225.553
[31,     1] loss: 1235.092
[32,     1] loss: 1216.950
[33,     1] loss: 1237.690
[34,     1] loss: 1165.659
[35,     1] loss: 1155.775
[36,     1] loss: 1182.794
[37,     1] loss: 1183.966
[38,     1] loss: 1176.660
[39,     1] loss: 1199.461
[40,     1] loss: 1137.726
[41,     1] loss: 1174.095
[42,     1] loss: 1175.782
[43,     1] loss: 1121.478
[44,     1] loss: 1089.156
[45,     1] loss: 1045.838
[46,     1] loss: 1062.239
[47,     1] loss: 1080.705
[48,     1] loss: 1037.226
[49,     1] loss: 1159.394
[50,     1] loss: 1220.542
[51,     1] loss: 978.979
[52,     1] loss: 1171.246
[53,     1] loss: 1051.593
[54,     1] loss: 1126.314
[55,     1] loss: 1012.876
[56,     1] loss: 1057.135
[57,     1] loss: 935.672
[58,     1] loss: 963.481
[59,     1] loss: 888.893
[60,     1] loss: 987.932
[61,     1] loss: 1061.104
[62,     1] loss: 789.957
[63,     1] loss: 1120.909
[64,     1] loss: 1222.219
[65,     1] loss: 1111.037
[66,     1] loss: 885.851
[67,     1] loss: 1214.602
[68,     1] loss: 991.428
[69,     1] loss: 1083.593
[70,     1] loss: 1183.162
[71,     1] loss: 965.583
[72,     1] loss: 976.511
[73,     1] loss: 1053.849
[74,     1] loss: 802.238
[75,     1] loss: 895.591
[76,     1] loss: 860.327
[77,     1] loss: 869.949
[78,     1] loss: 772.337
[79,     1] loss: 832.338
[80,     1] loss: 761.062
[81,     1] loss: 928.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004683060755961748,
 'learning_rate_Hydroxylation-K': 0.002225776301417442,
 'learning_rate_Hydroxylation-P': 0.003043002872111467,
 'log_base': 2.2053524077471316,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3455143826,
 'sample_weights': [3.0573378022065856, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.4306829789154,
 'weight_decay_Hydroxylation-K': 5.273303161572471,
 'weight_decay_Hydroxylation-P': 3.7077039818398907}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1359.705
[2,     1] loss: 1352.567
[3,     1] loss: 1355.726
[4,     1] loss: 1357.741
[5,     1] loss: 1356.271
[6,     1] loss: 1349.529
[7,     1] loss: 1349.628
[8,     1] loss: 1345.449
[9,     1] loss: 1340.607
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0040979941596494,
 'learning_rate_Hydroxylation-K': 0.00021500620468323247,
 'learning_rate_Hydroxylation-P': 0.008474437812184968,
 'log_base': 2.400502330947999,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1280068270,
 'sample_weights': [2.110848295237395, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.863252872233526,
 'weight_decay_Hydroxylation-K': 5.830168806705858,
 'weight_decay_Hydroxylation-P': 1.1178833319759993}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.372
[2,     1] loss: 1313.395
[3,     1] loss: 1310.393
[4,     1] loss: 1309.908
[5,     1] loss: 1307.045
[6,     1] loss: 1292.009
[7,     1] loss: 1277.945
[8,     1] loss: 1247.858
[9,     1] loss: 1197.659
[10,     1] loss: 1174.953
[11,     1] loss: 1144.307
[12,     1] loss: 1106.307
[13,     1] loss: 1100.077
[14,     1] loss: 1078.854
[15,     1] loss: 1026.639
[16,     1] loss: 1012.900
[17,     1] loss: 1020.965
[18,     1] loss: 1019.577
[19,     1] loss: 1024.648
[20,     1] loss: 1011.046
[21,     1] loss: 974.447
[22,     1] loss: 970.602
[23,     1] loss: 968.858
[24,     1] loss: 1003.316
[25,     1] loss: 915.940
[26,     1] loss: 947.073
[27,     1] loss: 980.176
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004720408188410084,
 'learning_rate_Hydroxylation-K': 0.00016596525160216232,
 'learning_rate_Hydroxylation-P': 0.009311086223124326,
 'log_base': 2.4600007771635046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 42656669,
 'sample_weights': [1.9064577496146091, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.377535271201882,
 'weight_decay_Hydroxylation-K': 4.185700037353625,
 'weight_decay_Hydroxylation-P': 1.5819453946622009}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.494
[2,     1] loss: 1302.271
[3,     1] loss: 1300.836
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002433218252029957,
 'learning_rate_Hydroxylation-K': 0.003089768998026943,
 'learning_rate_Hydroxylation-P': 0.009690451153782167,
 'log_base': 1.6078602735794245,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3703677866,
 'sample_weights': [1.85460369035006, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.543547766713741,
 'weight_decay_Hydroxylation-K': 9.751752698753249,
 'weight_decay_Hydroxylation-P': 3.4858658036676453}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1651.918
[2,     1] loss: 1653.756
[3,     1] loss: 1642.626
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034334308907481128,
 'learning_rate_Hydroxylation-K': 0.0007476059358875387,
 'learning_rate_Hydroxylation-P': 0.009722429016320604,
 'log_base': 2.8589122412505135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2117693176,
 'sample_weights': [3.5153256020261283, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.978245992840739,
 'weight_decay_Hydroxylation-K': 8.89879185347288,
 'weight_decay_Hydroxylation-P': 1.4046052870434325}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.100
[2,     1] loss: 1248.774
[3,     1] loss: 1245.376
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027370595641415217,
 'learning_rate_Hydroxylation-K': 0.0012237126529128562,
 'learning_rate_Hydroxylation-P': 0.006858033464550482,
 'log_base': 1.7556149314207816,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1751139986,
 'sample_weights': [1.5892780290743656, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.903991152106152,
 'weight_decay_Hydroxylation-K': 5.355891296075661,
 'weight_decay_Hydroxylation-P': 1.3328966849178323}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1535.227
[2,     1] loss: 1540.921
[3,     1] loss: 1534.039
[4,     1] loss: 1534.936
[5,     1] loss: 1527.401
[6,     1] loss: 1527.054
[7,     1] loss: 1524.738
[8,     1] loss: 1513.486
[9,     1] loss: 1500.699
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003327416690829197,
 'learning_rate_Hydroxylation-K': 0.0008011103821143326,
 'learning_rate_Hydroxylation-P': 0.008668955977675155,
 'log_base': 2.1021765513435478,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2431605500,
 'sample_weights': [2.966215785489584, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.277553658565473,
 'weight_decay_Hydroxylation-K': 8.763260460219179,
 'weight_decay_Hydroxylation-P': 0.5239131592914248}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.188
[2,     1] loss: 1386.825
[3,     1] loss: 1378.372
[4,     1] loss: 1383.046
[5,     1] loss: 1378.851
[6,     1] loss: 1374.875
[7,     1] loss: 1368.939
[8,     1] loss: 1362.592
[9,     1] loss: 1344.913
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008925314451040042,
 'learning_rate_Hydroxylation-K': 0.002232643444727746,
 'learning_rate_Hydroxylation-P': 0.002216835098113972,
 'log_base': 2.5628921329104655,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3745629648,
 'sample_weights': [2.2469760825157756, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.758264533671712,
 'weight_decay_Hydroxylation-K': 5.7365014183695315,
 'weight_decay_Hydroxylation-P': 6.074905373152973}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.381
[2,     1] loss: 1281.905
[3,     1] loss: 1280.666
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024309053177593997,
 'learning_rate_Hydroxylation-K': 0.0005552965429043975,
 'learning_rate_Hydroxylation-P': 0.0014236608409076126,
 'log_base': 2.5650813349962336,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1472285067,
 'sample_weights': [1.7738589411295382, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.211196946186129,
 'weight_decay_Hydroxylation-K': 5.788539407418973,
 'weight_decay_Hydroxylation-P': 3.582276164091826}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.248
[2,     1] loss: 1281.163
[3,     1] loss: 1280.992
[4,     1] loss: 1280.068
[5,     1] loss: 1279.688
[6,     1] loss: 1268.943
[7,     1] loss: 1262.146
[8,     1] loss: 1250.864
[9,     1] loss: 1227.520
[10,     1] loss: 1199.709
[11,     1] loss: 1151.836
[12,     1] loss: 1136.698
[13,     1] loss: 1105.190
[14,     1] loss: 1101.906
[15,     1] loss: 1081.499
[16,     1] loss: 1054.243
[17,     1] loss: 1052.654
[18,     1] loss: 1008.411
[19,     1] loss: 1054.902
[20,     1] loss: 1040.976
[21,     1] loss: 1025.236
[22,     1] loss: 1054.707
[23,     1] loss: 1012.471
[24,     1] loss: 983.590
[25,     1] loss: 952.232
[26,     1] loss: 957.022
[27,     1] loss: 951.992
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0042546459516021595,
 'learning_rate_Hydroxylation-K': 0.0024727489518421505,
 'learning_rate_Hydroxylation-P': 0.0019578633558809087,
 'log_base': 2.661448239137663,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1165248888,
 'sample_weights': [1.772251101205938, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.546782535202121,
 'weight_decay_Hydroxylation-K': 6.71901158513498,
 'weight_decay_Hydroxylation-P': 3.496223620373681}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.975
[2,     1] loss: 1265.372
[3,     1] loss: 1266.377
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019714791346239257,
 'learning_rate_Hydroxylation-K': 0.0019453208816343744,
 'learning_rate_Hydroxylation-P': 0.009493976980217827,
 'log_base': 2.2920915915859363,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1876895882,
 'sample_weights': [1.7054791972872454, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.977085723909324,
 'weight_decay_Hydroxylation-K': 5.606516690075549,
 'weight_decay_Hydroxylation-P': 0.5735681365030126}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.358
[2,     1] loss: 1333.235
[3,     1] loss: 1334.817
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004761076411551156,
 'learning_rate_Hydroxylation-K': 0.002065301180404723,
 'learning_rate_Hydroxylation-P': 0.009513080967962583,
 'log_base': 2.753993352818541,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1150471340,
 'sample_weights': [2.012675196077415, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.636306119844072,
 'weight_decay_Hydroxylation-K': 5.793573428538964,
 'weight_decay_Hydroxylation-P': 0.5957035846307263}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.992
[2,     1] loss: 1260.082
[3,     1] loss: 1254.823
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005001345113629802,
 'learning_rate_Hydroxylation-K': 0.005031757169876704,
 'learning_rate_Hydroxylation-P': 0.009099367406545922,
 'log_base': 1.5385601390879025,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1719037736,
 'sample_weights': [1.647934330529309, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.344487627103533,
 'weight_decay_Hydroxylation-K': 0.7730679802511266,
 'weight_decay_Hydroxylation-P': 7.871468537383484}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1726.953
[2,     1] loss: 1727.332
[3,     1] loss: 1730.771
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006516982383168423,
 'learning_rate_Hydroxylation-K': 0.0024474421973417148,
 'learning_rate_Hydroxylation-P': 0.008688053252712168,
 'log_base': 2.3006971726527814,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1881427254,
 'sample_weights': [3.8747934422121375, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.40738962897961,
 'weight_decay_Hydroxylation-K': 7.247555397501044,
 'weight_decay_Hydroxylation-P': 0.22793961697100262}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.080
[2,     1] loss: 1330.650
[3,     1] loss: 1327.807
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006694077266770344,
 'learning_rate_Hydroxylation-K': 1.811124596241831e-05,
 'learning_rate_Hydroxylation-P': 0.0031595819392959508,
 'log_base': 2.598739563117554,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 280859772,
 'sample_weights': [2.0036230342552663, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.397214406813187,
 'weight_decay_Hydroxylation-K': 6.0104868207619315,
 'weight_decay_Hydroxylation-P': 0.30443569537983084}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.218
[2,     1] loss: 1282.456
[3,     1] loss: 1277.671
[4,     1] loss: 1280.852
[5,     1] loss: 1275.478
[6,     1] loss: 1272.175
[7,     1] loss: 1271.087
[8,     1] loss: 1273.980
[9,     1] loss: 1264.527
[10,     1] loss: 1254.738
[11,     1] loss: 1239.442
[12,     1] loss: 1214.521
[13,     1] loss: 1171.419
[14,     1] loss: 1146.808
[15,     1] loss: 1117.662
[16,     1] loss: 1088.114
[17,     1] loss: 1148.145
[18,     1] loss: 1132.530
[19,     1] loss: 1067.112
[20,     1] loss: 1068.662
[21,     1] loss: 1090.501
[22,     1] loss: 1008.557
[23,     1] loss: 1052.289
[24,     1] loss: 1042.209
[25,     1] loss: 1032.194
[26,     1] loss: 1027.357
[27,     1] loss: 1010.509
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0044040521469276474,
 'learning_rate_Hydroxylation-K': 0.004649036519443095,
 'learning_rate_Hydroxylation-P': 0.00998014180423234,
 'log_base': 1.1817058178157769,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2729666810,
 'sample_weights': [1.7480594207535767, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6257084640380803,
 'weight_decay_Hydroxylation-K': 8.033193200182902,
 'weight_decay_Hydroxylation-P': 3.1850625537390997}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3249.847
[2,     1] loss: 3246.793
[3,     1] loss: 3240.044
[4,     1] loss: 3252.357
[5,     1] loss: 3250.492
[6,     1] loss: 3247.744
[7,     1] loss: 3234.352
[8,     1] loss: 3230.508
[9,     1] loss: 3229.133
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006646320282740495,
 'learning_rate_Hydroxylation-K': 0.0024905136925696568,
 'learning_rate_Hydroxylation-P': 0.0022503318324155917,
 'log_base': 2.2338525497221835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 435412162,
 'sample_weights': [9.999120253019475, 1.2499381824105473],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.83072819683992,
 'weight_decay_Hydroxylation-K': 2.1392440614800146,
 'weight_decay_Hydroxylation-P': 1.3802112607051642}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.677
[2,     1] loss: 1344.548
[3,     1] loss: 1356.846
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032800941440235066,
 'learning_rate_Hydroxylation-K': 0.0013901800634364355,
 'learning_rate_Hydroxylation-P': 0.005056643063575041,
 'log_base': 2.5756137287479213,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2374079710,
 'sample_weights': [2.0771253204801776, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.872058359513833,
 'weight_decay_Hydroxylation-K': 8.151935379325003,
 'weight_decay_Hydroxylation-P': 2.373532096415832}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.850
[2,     1] loss: 1278.523
[3,     1] loss: 1277.418
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004956411296775504,
 'learning_rate_Hydroxylation-K': 0.003528283599479172,
 'learning_rate_Hydroxylation-P': 0.008078520313746463,
 'log_base': 2.843843741545868,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2753306421,
 'sample_weights': [1.7645751953750979, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.684639156679134,
 'weight_decay_Hydroxylation-K': 2.7202232434004587,
 'weight_decay_Hydroxylation-P': 4.270467207251928}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.617
[2,     1] loss: 1247.434
[3,     1] loss: 1245.689
[4,     1] loss: 1242.825
[5,     1] loss: 1241.665
[6,     1] loss: 1243.729
[7,     1] loss: 1238.584
[8,     1] loss: 1231.205
[9,     1] loss: 1215.896
[10,     1] loss: 1194.773
[11,     1] loss: 1168.356
[12,     1] loss: 1111.996
[13,     1] loss: 1098.848
[14,     1] loss: 1059.372
[15,     1] loss: 1093.077
[16,     1] loss: 1087.680
[17,     1] loss: 1010.802
[18,     1] loss: 1083.359
[19,     1] loss: 1037.126
[20,     1] loss: 1052.256
[21,     1] loss: 1010.353
[22,     1] loss: 1008.558
[23,     1] loss: 993.698
[24,     1] loss: 953.233
[25,     1] loss: 967.390
[26,     1] loss: 989.158
[27,     1] loss: 914.903
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004378120859606938,
 'learning_rate_Hydroxylation-K': 0.0029901973110030675,
 'learning_rate_Hydroxylation-P': 0.006871064930749201,
 'log_base': 2.9492879761661954,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1277939496,
 'sample_weights': [1.5973139333965938, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.332282745428554,
 'weight_decay_Hydroxylation-K': 3.173943769018921,
 'weight_decay_Hydroxylation-P': 4.254323719508026}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.137
[2,     1] loss: 1237.561
[3,     1] loss: 1234.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004442882148606376,
 'learning_rate_Hydroxylation-K': 0.0006408625810314656,
 'learning_rate_Hydroxylation-P': 0.007181588730933834,
 'log_base': 2.56404264504175,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4075870472,
 'sample_weights': [1.5435457275893156, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.542896912600835,
 'weight_decay_Hydroxylation-K': 4.498246417230148,
 'weight_decay_Hydroxylation-P': 1.5201564440594602}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.224
[2,     1] loss: 1284.799
[3,     1] loss: 1287.478
[4,     1] loss: 1277.318
[5,     1] loss: 1276.066
[6,     1] loss: 1267.333
[7,     1] loss: 1256.213
[8,     1] loss: 1210.807
[9,     1] loss: 1175.901
[10,     1] loss: 1134.004
[11,     1] loss: 1089.076
[12,     1] loss: 1068.566
[13,     1] loss: 1137.989
[14,     1] loss: 1113.867
[15,     1] loss: 1075.080
[16,     1] loss: 1092.524
[17,     1] loss: 1063.359
[18,     1] loss: 1020.910
[19,     1] loss: 1072.449
[20,     1] loss: 1014.183
[21,     1] loss: 1002.918
[22,     1] loss: 960.890
[23,     1] loss: 1041.249
[24,     1] loss: 926.885
[25,     1] loss: 965.605
[26,     1] loss: 913.533
[27,     1] loss: 901.409
[28,     1] loss: 918.440
[29,     1] loss: 922.363
[30,     1] loss: 862.007
[31,     1] loss: 917.026
[32,     1] loss: 866.958
[33,     1] loss: 928.776
[34,     1] loss: 900.312
[35,     1] loss: 824.393
[36,     1] loss: 903.247
[37,     1] loss: 967.243
[38,     1] loss: 907.825
[39,     1] loss: 825.961
[40,     1] loss: 900.121
[41,     1] loss: 779.478
[42,     1] loss: 824.415
[43,     1] loss: 718.195
[44,     1] loss: 833.169
[45,     1] loss: 798.741
[46,     1] loss: 737.903
[47,     1] loss: 778.372
[48,     1] loss: 826.688
[49,     1] loss: 681.050
[50,     1] loss: 850.568
[51,     1] loss: 837.563
[52,     1] loss: 723.380
[53,     1] loss: 810.010
[54,     1] loss: 753.867
[55,     1] loss: 758.360
[56,     1] loss: 663.452
[57,     1] loss: 786.109
[58,     1] loss: 651.543
[59,     1] loss: 671.564
[60,     1] loss: 627.723
[61,     1] loss: 660.959
[62,     1] loss: 839.856
[63,     1] loss: 738.317
[64,     1] loss: 644.728
[65,     1] loss: 653.958
[66,     1] loss: 567.825
[67,     1] loss: 626.167
[68,     1] loss: 547.684
[69,     1] loss: 537.977
[70,     1] loss: 470.401
[71,     1] loss: 507.703
[72,     1] loss: 808.155
[73,     1] loss: 2274.473
[74,     1] loss: 866.213
[75,     1] loss: 1204.919
[76,     1] loss: 1171.895
[77,     1] loss: 1178.186
[78,     1] loss: 1202.003
Early stopping applied (best metric=0.36909765005111694)
Finished Training
Total time taken: 10.585577249526978
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.269
[2,     1] loss: 1283.880
[3,     1] loss: 1283.206
[4,     1] loss: 1280.904
[5,     1] loss: 1278.233
[6,     1] loss: 1281.185
[7,     1] loss: 1273.592
[8,     1] loss: 1259.433
[9,     1] loss: 1227.807
[10,     1] loss: 1195.528
[11,     1] loss: 1162.946
[12,     1] loss: 1126.616
[13,     1] loss: 1160.116
[14,     1] loss: 1119.776
[15,     1] loss: 1094.602
[16,     1] loss: 1077.626
[17,     1] loss: 1010.569
[18,     1] loss: 1025.789
[19,     1] loss: 1052.751
[20,     1] loss: 1020.013
[21,     1] loss: 1028.678
[22,     1] loss: 949.147
[23,     1] loss: 974.122
[24,     1] loss: 944.003
[25,     1] loss: 990.673
[26,     1] loss: 930.304
[27,     1] loss: 975.055
[28,     1] loss: 938.533
[29,     1] loss: 1035.449
[30,     1] loss: 964.921
[31,     1] loss: 908.442
[32,     1] loss: 932.413
[33,     1] loss: 937.853
[34,     1] loss: 929.949
[35,     1] loss: 887.704
[36,     1] loss: 852.811
[37,     1] loss: 900.830
[38,     1] loss: 869.509
[39,     1] loss: 841.403
[40,     1] loss: 940.775
[41,     1] loss: 842.157
[42,     1] loss: 851.791
[43,     1] loss: 845.715
[44,     1] loss: 811.878
[45,     1] loss: 807.605
[46,     1] loss: 797.252
[47,     1] loss: 780.749
[48,     1] loss: 825.426
[49,     1] loss: 796.853
[50,     1] loss: 738.968
[51,     1] loss: 716.660
[52,     1] loss: 652.105
[53,     1] loss: 659.560
[54,     1] loss: 918.305
[55,     1] loss: 2275.296
[56,     1] loss: 987.270
[57,     1] loss: 985.528
[58,     1] loss: 1142.513
[59,     1] loss: 1160.995
[60,     1] loss: 1134.123
[61,     1] loss: 1162.403
[62,     1] loss: 1139.819
[63,     1] loss: 1130.534
[64,     1] loss: 1115.961
[65,     1] loss: 1126.378
[66,     1] loss: 1073.346
[67,     1] loss: 1077.044
[68,     1] loss: 1061.799
[69,     1] loss: 1061.802
[70,     1] loss: 1070.479
[71,     1] loss: 1027.899
[72,     1] loss: 993.346
[73,     1] loss: 979.448
[74,     1] loss: 943.292
[75,     1] loss: 921.211
[76,     1] loss: 917.005
[77,     1] loss: 905.275
[78,     1] loss: 860.320
[79,     1] loss: 921.345
[80,     1] loss: 888.851
[81,     1] loss: 844.545
[82,     1] loss: 853.893
[83,     1] loss: 794.997
[84,     1] loss: 772.597
[85,     1] loss: 927.365
[86,     1] loss: 804.654
[87,     1] loss: 755.754
[88,     1] loss: 802.921
Early stopping applied (best metric=0.413332998752594)
Finished Training
Total time taken: 13.024646043777466
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.158
[2,     1] loss: 1281.467
[3,     1] loss: 1281.490
[4,     1] loss: 1276.169
[5,     1] loss: 1270.199
[6,     1] loss: 1255.282
[7,     1] loss: 1220.507
[8,     1] loss: 1164.194
[9,     1] loss: 1110.843
[10,     1] loss: 1099.198
[11,     1] loss: 1136.995
[12,     1] loss: 1097.987
[13,     1] loss: 1201.135
[14,     1] loss: 1075.765
[15,     1] loss: 1056.186
[16,     1] loss: 1120.113
[17,     1] loss: 1053.815
[18,     1] loss: 1052.032
[19,     1] loss: 1094.811
[20,     1] loss: 1100.451
[21,     1] loss: 1031.216
[22,     1] loss: 1010.582
[23,     1] loss: 1040.033
[24,     1] loss: 967.548
[25,     1] loss: 1034.128
[26,     1] loss: 1020.202
[27,     1] loss: 955.177
[28,     1] loss: 960.033
[29,     1] loss: 970.524
[30,     1] loss: 916.998
[31,     1] loss: 963.768
[32,     1] loss: 945.242
[33,     1] loss: 899.780
[34,     1] loss: 880.582
[35,     1] loss: 868.937
[36,     1] loss: 915.417
[37,     1] loss: 904.765
[38,     1] loss: 918.848
[39,     1] loss: 880.202
[40,     1] loss: 854.703
[41,     1] loss: 875.168
[42,     1] loss: 777.816
[43,     1] loss: 843.130
[44,     1] loss: 900.252
[45,     1] loss: 872.951
[46,     1] loss: 828.808
[47,     1] loss: 829.077
[48,     1] loss: 806.971
[49,     1] loss: 774.358
[50,     1] loss: 735.319
[51,     1] loss: 741.352
[52,     1] loss: 943.438
[53,     1] loss: 1613.243
[54,     1] loss: 785.826
[55,     1] loss: 984.679
[56,     1] loss: 1016.623
[57,     1] loss: 1001.520
[58,     1] loss: 1026.542
[59,     1] loss: 1041.595
[60,     1] loss: 1045.533
[61,     1] loss: 978.458
[62,     1] loss: 930.746
[63,     1] loss: 892.617
[64,     1] loss: 919.905
[65,     1] loss: 939.481
[66,     1] loss: 909.687
[67,     1] loss: 919.808
[68,     1] loss: 841.550
[69,     1] loss: 889.734
[70,     1] loss: 810.515
[71,     1] loss: 835.728
[72,     1] loss: 843.635
[73,     1] loss: 813.951
[74,     1] loss: 813.223
[75,     1] loss: 743.659
[76,     1] loss: 751.294
[77,     1] loss: 738.200
[78,     1] loss: 920.263
[79,     1] loss: 1211.202
[80,     1] loss: 943.433
[81,     1] loss: 883.226
[82,     1] loss: 1057.725
[83,     1] loss: 960.037
[84,     1] loss: 974.604
[85,     1] loss: 963.930
[86,     1] loss: 879.900
[87,     1] loss: 840.406
[88,     1] loss: 925.143
[89,     1] loss: 830.219
[90,     1] loss: 882.470
[91,     1] loss: 802.080
[92,     1] loss: 771.047
[93,     1] loss: 745.610
[94,     1] loss: 737.854
[95,     1] loss: 728.447
[96,     1] loss: 718.159
[97,     1] loss: 691.214
[98,     1] loss: 761.693
[99,     1] loss: 1240.539
[100,     1] loss: 765.005
Early stopping applied (best metric=0.3392678201198578)
Finished Training
Total time taken: 16.694653749465942
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.799
[2,     1] loss: 1280.986
[3,     1] loss: 1288.649
[4,     1] loss: 1277.373
[5,     1] loss: 1277.662
[6,     1] loss: 1279.538
[7,     1] loss: 1267.361
[8,     1] loss: 1256.275
[9,     1] loss: 1206.950
[10,     1] loss: 1165.427
[11,     1] loss: 1136.335
[12,     1] loss: 1136.478
[13,     1] loss: 1053.979
[14,     1] loss: 1042.783
[15,     1] loss: 1000.849
[16,     1] loss: 979.486
[17,     1] loss: 1026.734
[18,     1] loss: 1032.989
[19,     1] loss: 1016.119
[20,     1] loss: 982.444
[21,     1] loss: 947.253
[22,     1] loss: 1002.381
[23,     1] loss: 970.124
[24,     1] loss: 954.696
[25,     1] loss: 932.136
[26,     1] loss: 938.340
[27,     1] loss: 932.860
[28,     1] loss: 884.792
[29,     1] loss: 893.321
[30,     1] loss: 950.601
[31,     1] loss: 1096.439
[32,     1] loss: 862.949
[33,     1] loss: 946.644
[34,     1] loss: 880.651
[35,     1] loss: 896.568
[36,     1] loss: 855.815
[37,     1] loss: 856.559
[38,     1] loss: 890.671
[39,     1] loss: 795.470
[40,     1] loss: 829.724
[41,     1] loss: 843.374
[42,     1] loss: 816.438
[43,     1] loss: 941.121
[44,     1] loss: 759.578
[45,     1] loss: 907.244
[46,     1] loss: 771.401
[47,     1] loss: 889.288
[48,     1] loss: 768.420
[49,     1] loss: 876.489
[50,     1] loss: 710.227
[51,     1] loss: 771.530
[52,     1] loss: 704.678
[53,     1] loss: 769.655
[54,     1] loss: 742.613
[55,     1] loss: 668.088
[56,     1] loss: 763.599
[57,     1] loss: 727.995
[58,     1] loss: 634.512
[59,     1] loss: 653.448
[60,     1] loss: 630.582
[61,     1] loss: 570.107
[62,     1] loss: 582.516
[63,     1] loss: 1342.835
[64,     1] loss: 2557.963
[65,     1] loss: 1772.435
[66,     1] loss: 1177.606
[67,     1] loss: 1231.930
[68,     1] loss: 1263.663
[69,     1] loss: 1278.039
[70,     1] loss: 1280.134
[71,     1] loss: 1281.665
[72,     1] loss: 1281.229
[73,     1] loss: 1282.344
[74,     1] loss: 1282.758
[75,     1] loss: 1282.964
[76,     1] loss: 1282.407
[77,     1] loss: 1282.641
[78,     1] loss: 1282.065
[79,     1] loss: 1283.165
[80,     1] loss: 1283.138
[81,     1] loss: 1281.493
[82,     1] loss: 1279.806
[83,     1] loss: 1282.877
[84,     1] loss: 1281.988
[85,     1] loss: 1280.331
[86,     1] loss: 1281.856
Early stopping applied (best metric=0.40376994013786316)
Finished Training
Total time taken: 14.474149465560913
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1281.143
[2,     1] loss: 1283.069
[3,     1] loss: 1286.631
[4,     1] loss: 1285.826
[5,     1] loss: 1281.021
[6,     1] loss: 1277.763
[7,     1] loss: 1270.538
[8,     1] loss: 1255.921
[9,     1] loss: 1238.958
[10,     1] loss: 1203.863
[11,     1] loss: 1175.024
[12,     1] loss: 1155.929
[13,     1] loss: 1077.342
[14,     1] loss: 1096.697
[15,     1] loss: 1147.080
[16,     1] loss: 1096.925
[17,     1] loss: 1060.039
[18,     1] loss: 1084.588
[19,     1] loss: 1079.886
[20,     1] loss: 1039.008
[21,     1] loss: 1028.283
[22,     1] loss: 1037.142
[23,     1] loss: 964.589
[24,     1] loss: 942.870
[25,     1] loss: 990.712
[26,     1] loss: 960.189
[27,     1] loss: 931.598
[28,     1] loss: 1070.688
[29,     1] loss: 1070.951
[30,     1] loss: 974.936
[31,     1] loss: 1017.220
[32,     1] loss: 956.823
[33,     1] loss: 935.204
[34,     1] loss: 903.452
[35,     1] loss: 959.904
[36,     1] loss: 891.820
[37,     1] loss: 941.729
[38,     1] loss: 901.850
[39,     1] loss: 882.143
[40,     1] loss: 884.717
[41,     1] loss: 847.647
[42,     1] loss: 831.205
[43,     1] loss: 919.644
[44,     1] loss: 1184.404
[45,     1] loss: 796.323
[46,     1] loss: 977.026
[47,     1] loss: 877.207
[48,     1] loss: 938.779
[49,     1] loss: 944.599
[50,     1] loss: 844.410
[51,     1] loss: 938.311
[52,     1] loss: 871.076
[53,     1] loss: 883.399
[54,     1] loss: 838.576
[55,     1] loss: 776.875
[56,     1] loss: 844.715
[57,     1] loss: 810.100
[58,     1] loss: 805.523
[59,     1] loss: 784.729
[60,     1] loss: 683.702
[61,     1] loss: 807.633
[62,     1] loss: 828.809
[63,     1] loss: 760.003
[64,     1] loss: 720.435
[65,     1] loss: 775.213
[66,     1] loss: 692.156
[67,     1] loss: 713.004
[68,     1] loss: 671.696
[69,     1] loss: 688.990
[70,     1] loss: 874.514
[71,     1] loss: 888.092
[72,     1] loss: 787.554
[73,     1] loss: 846.265
[74,     1] loss: 695.679
[75,     1] loss: 827.468
[76,     1] loss: 741.529
[77,     1] loss: 741.246
[78,     1] loss: 744.814
[79,     1] loss: 692.256
[80,     1] loss: 686.282
[81,     1] loss: 716.317
[82,     1] loss: 666.582
[83,     1] loss: 697.972
[84,     1] loss: 580.903
[85,     1] loss: 658.509
[86,     1] loss: 638.882
[87,     1] loss: 551.268
[88,     1] loss: 522.678
[89,     1] loss: 570.087
[90,     1] loss: 870.941
[91,     1] loss: 1152.361
[92,     1] loss: 931.343
[93,     1] loss: 1092.362
[94,     1] loss: 1040.804
[95,     1] loss: 1001.947
[96,     1] loss: 945.680
[97,     1] loss: 852.945
[98,     1] loss: 862.872
[99,     1] loss: 922.541
[100,     1] loss: 856.364
[101,     1] loss: 825.585
[102,     1] loss: 836.684
[103,     1] loss: 840.639
[104,     1] loss: 822.965
[105,     1] loss: 782.762
[106,     1] loss: 720.779
[107,     1] loss: 710.454
[108,     1] loss: 755.720
[109,     1] loss: 1239.953
[110,     1] loss: 774.432
[111,     1] loss: 828.383
[112,     1] loss: 830.858
[113,     1] loss: 896.509
[114,     1] loss: 813.878
[115,     1] loss: 802.613
[116,     1] loss: 781.485
[117,     1] loss: 854.269
[118,     1] loss: 647.437
[119,     1] loss: 853.106
[120,     1] loss: 956.946
[121,     1] loss: 872.532
[122,     1] loss: 742.358
[123,     1] loss: 843.945
[124,     1] loss: 710.072
[125,     1] loss: 865.667
[126,     1] loss: 652.367
[127,     1] loss: 790.105
[128,     1] loss: 781.813
[129,     1] loss: 739.508
[130,     1] loss: 667.387
[131,     1] loss: 738.136
[132,     1] loss: 670.540
[133,     1] loss: 664.888
[134,     1] loss: 618.573
[135,     1] loss: 597.986
[136,     1] loss: 621.413
[137,     1] loss: 550.677
[138,     1] loss: 596.409
[139,     1] loss: 758.365
[140,     1] loss: 546.054
[141,     1] loss: 594.131
[142,     1] loss: 705.595
[143,     1] loss: 589.620
[144,     1] loss: 1061.566
[145,     1] loss: 1141.653
[146,     1] loss: 1147.457
[147,     1] loss: 1067.668
[148,     1] loss: 1150.416
[149,     1] loss: 1129.030
[150,     1] loss: 983.140
[151,     1] loss: 846.149
[152,     1] loss: 875.674
[153,     1] loss: 917.847
[154,     1] loss: 947.996
[155,     1] loss: 854.865
[156,     1] loss: 823.198
[157,     1] loss: 833.113
[158,     1] loss: 805.212
[159,     1] loss: 806.569
[160,     1] loss: 748.808
[161,     1] loss: 707.874
[162,     1] loss: 751.071
[163,     1] loss: 833.354
[164,     1] loss: 758.886
[165,     1] loss: 672.716
[166,     1] loss: 642.948
[167,     1] loss: 742.770
[168,     1] loss: 859.707
[169,     1] loss: 693.057
[170,     1] loss: 872.866
[171,     1] loss: 678.035
[172,     1] loss: 764.155
[173,     1] loss: 604.843
[174,     1] loss: 684.515
[175,     1] loss: 615.273
[176,     1] loss: 667.246
[177,     1] loss: 740.878
[178,     1] loss: 889.224
[179,     1] loss: 625.208
[180,     1] loss: 768.378
[181,     1] loss: 658.574
[182,     1] loss: 670.549
[183,     1] loss: 634.167
[184,     1] loss: 723.928
[185,     1] loss: 576.700
[186,     1] loss: 535.083
[187,     1] loss: 596.820
[188,     1] loss: 670.278
[189,     1] loss: 724.926
[190,     1] loss: 960.830
[191,     1] loss: 645.147
[192,     1] loss: 968.472
[193,     1] loss: 756.753
[194,     1] loss: 868.582
[195,     1] loss: 839.708
[196,     1] loss: 702.571
[197,     1] loss: 757.331
[198,     1] loss: 674.344
[199,     1] loss: 667.520
[200,     1] loss: 621.059
Finished Training
Total time taken: 30.97721767425537
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.469
[2,     1] loss: 1288.632
[3,     1] loss: 1282.356
[4,     1] loss: 1282.942
[5,     1] loss: 1282.102
[6,     1] loss: 1280.360
[7,     1] loss: 1277.034
[8,     1] loss: 1272.505
[9,     1] loss: 1265.743
[10,     1] loss: 1233.169
[11,     1] loss: 1198.317
[12,     1] loss: 1155.477
[13,     1] loss: 1115.318
[14,     1] loss: 1087.909
[15,     1] loss: 1126.342
[16,     1] loss: 1042.136
[17,     1] loss: 1075.858
[18,     1] loss: 1010.424
[19,     1] loss: 1038.844
[20,     1] loss: 1003.377
[21,     1] loss: 1062.306
[22,     1] loss: 996.011
[23,     1] loss: 1025.767
[24,     1] loss: 967.877
[25,     1] loss: 954.512
[26,     1] loss: 927.204
[27,     1] loss: 952.083
[28,     1] loss: 943.886
[29,     1] loss: 939.197
[30,     1] loss: 881.349
[31,     1] loss: 907.152
[32,     1] loss: 889.375
[33,     1] loss: 859.327
[34,     1] loss: 886.825
[35,     1] loss: 855.275
[36,     1] loss: 868.111
[37,     1] loss: 844.797
[38,     1] loss: 817.565
[39,     1] loss: 781.163
[40,     1] loss: 853.363
[41,     1] loss: 820.273
[42,     1] loss: 855.073
[43,     1] loss: 757.873
[44,     1] loss: 798.488
[45,     1] loss: 820.616
[46,     1] loss: 760.036
[47,     1] loss: 949.276
[48,     1] loss: 798.161
[49,     1] loss: 843.365
[50,     1] loss: 800.896
[51,     1] loss: 870.607
[52,     1] loss: 716.868
[53,     1] loss: 841.404
[54,     1] loss: 646.840
[55,     1] loss: 828.611
[56,     1] loss: 686.626
[57,     1] loss: 756.977
[58,     1] loss: 699.888
[59,     1] loss: 713.429
[60,     1] loss: 743.606
[61,     1] loss: 620.054
[62,     1] loss: 666.189
[63,     1] loss: 605.042
[64,     1] loss: 571.632
[65,     1] loss: 571.696
[66,     1] loss: 567.238
[67,     1] loss: 981.308
[68,     1] loss: 1744.921
[69,     1] loss: 797.045
[70,     1] loss: 1043.223
[71,     1] loss: 1109.756
[72,     1] loss: 1129.351
[73,     1] loss: 1136.657
[74,     1] loss: 1113.809
[75,     1] loss: 1088.523
[76,     1] loss: 1085.134
[77,     1] loss: 1052.058
[78,     1] loss: 1030.026
[79,     1] loss: 1008.083
[80,     1] loss: 993.557
[81,     1] loss: 961.206
[82,     1] loss: 936.515
[83,     1] loss: 915.896
[84,     1] loss: 946.109
[85,     1] loss: 898.837
[86,     1] loss: 917.218
[87,     1] loss: 908.349
[88,     1] loss: 917.151
[89,     1] loss: 889.376
[90,     1] loss: 915.620
[91,     1] loss: 851.237
[92,     1] loss: 832.317
[93,     1] loss: 856.866
[94,     1] loss: 839.272
[95,     1] loss: 775.121
[96,     1] loss: 792.072
[97,     1] loss: 866.137
[98,     1] loss: 846.222
[99,     1] loss: 949.396
[100,     1] loss: 829.851
[101,     1] loss: 902.288
[102,     1] loss: 855.523
[103,     1] loss: 887.707
[104,     1] loss: 812.539
[105,     1] loss: 860.758
[106,     1] loss: 803.864
[107,     1] loss: 807.144
[108,     1] loss: 829.301
[109,     1] loss: 718.478
[110,     1] loss: 828.233
[111,     1] loss: 802.706
[112,     1] loss: 709.935
[113,     1] loss: 790.266
[114,     1] loss: 744.022
[115,     1] loss: 677.921
[116,     1] loss: 800.172
[117,     1] loss: 1040.035
[118,     1] loss: 767.517
[119,     1] loss: 839.013
[120,     1] loss: 771.360
[121,     1] loss: 848.436
[122,     1] loss: 720.478
[123,     1] loss: 734.766
[124,     1] loss: 901.602
[125,     1] loss: 675.150
[126,     1] loss: 880.204
[127,     1] loss: 788.410
[128,     1] loss: 846.784
[129,     1] loss: 789.427
[130,     1] loss: 788.413
[131,     1] loss: 744.614
[132,     1] loss: 798.660
[133,     1] loss: 666.255
Early stopping applied (best metric=0.39375877380371094)
Finished Training
Total time taken: 21.17125177383423
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.169
[2,     1] loss: 1284.696
[3,     1] loss: 1278.418
[4,     1] loss: 1276.476
[5,     1] loss: 1274.318
[6,     1] loss: 1260.142
[7,     1] loss: 1233.571
[8,     1] loss: 1202.601
[9,     1] loss: 1151.697
[10,     1] loss: 1121.231
[11,     1] loss: 1134.952
[12,     1] loss: 1057.229
[13,     1] loss: 1049.570
[14,     1] loss: 1081.729
[15,     1] loss: 1041.769
[16,     1] loss: 1065.781
[17,     1] loss: 1026.410
[18,     1] loss: 1018.020
[19,     1] loss: 1013.418
[20,     1] loss: 988.268
[21,     1] loss: 995.412
[22,     1] loss: 974.041
[23,     1] loss: 937.792
[24,     1] loss: 921.649
[25,     1] loss: 937.449
[26,     1] loss: 972.609
[27,     1] loss: 927.771
[28,     1] loss: 948.562
[29,     1] loss: 939.338
[30,     1] loss: 918.620
[31,     1] loss: 915.672
[32,     1] loss: 908.625
[33,     1] loss: 910.796
[34,     1] loss: 801.438
[35,     1] loss: 868.608
[36,     1] loss: 844.342
[37,     1] loss: 805.849
[38,     1] loss: 833.325
[39,     1] loss: 873.341
[40,     1] loss: 832.222
[41,     1] loss: 771.918
[42,     1] loss: 849.525
[43,     1] loss: 755.105
[44,     1] loss: 796.207
[45,     1] loss: 693.048
[46,     1] loss: 712.253
[47,     1] loss: 706.926
[48,     1] loss: 776.578
[49,     1] loss: 1135.652
[50,     1] loss: 798.912
[51,     1] loss: 933.088
[52,     1] loss: 874.576
[53,     1] loss: 886.705
[54,     1] loss: 875.469
[55,     1] loss: 839.582
[56,     1] loss: 794.059
[57,     1] loss: 818.285
[58,     1] loss: 732.699
[59,     1] loss: 761.182
[60,     1] loss: 726.863
[61,     1] loss: 746.785
[62,     1] loss: 705.967
[63,     1] loss: 722.288
[64,     1] loss: 748.157
[65,     1] loss: 691.915
[66,     1] loss: 647.180
[67,     1] loss: 721.631
[68,     1] loss: 623.881
[69,     1] loss: 601.275
[70,     1] loss: 626.980
[71,     1] loss: 550.881
[72,     1] loss: 578.144
[73,     1] loss: 521.236
Early stopping applied (best metric=0.36296671628952026)
Finished Training
Total time taken: 11.892689943313599
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.417
[2,     1] loss: 1284.861
[3,     1] loss: 1276.808
[4,     1] loss: 1280.067
[5,     1] loss: 1276.444
[6,     1] loss: 1260.854
[7,     1] loss: 1219.592
[8,     1] loss: 1164.766
[9,     1] loss: 1107.209
[10,     1] loss: 1089.401
[11,     1] loss: 1097.036
[12,     1] loss: 1133.282
[13,     1] loss: 1099.522
[14,     1] loss: 1036.680
[15,     1] loss: 1023.890
[16,     1] loss: 1062.119
[17,     1] loss: 1023.744
[18,     1] loss: 994.063
[19,     1] loss: 988.038
[20,     1] loss: 970.953
[21,     1] loss: 986.832
[22,     1] loss: 951.523
[23,     1] loss: 970.477
[24,     1] loss: 938.864
[25,     1] loss: 933.871
[26,     1] loss: 937.346
[27,     1] loss: 959.480
[28,     1] loss: 910.264
[29,     1] loss: 925.131
[30,     1] loss: 854.780
[31,     1] loss: 870.288
[32,     1] loss: 957.584
[33,     1] loss: 1012.301
[34,     1] loss: 922.417
[35,     1] loss: 966.716
[36,     1] loss: 886.134
[37,     1] loss: 958.172
[38,     1] loss: 841.640
[39,     1] loss: 892.456
[40,     1] loss: 831.858
[41,     1] loss: 850.406
[42,     1] loss: 836.778
[43,     1] loss: 778.880
[44,     1] loss: 789.550
[45,     1] loss: 781.473
[46,     1] loss: 750.808
[47,     1] loss: 766.015
[48,     1] loss: 696.418
[49,     1] loss: 690.518
[50,     1] loss: 738.599
[51,     1] loss: 1693.125
[52,     1] loss: 1053.592
[53,     1] loss: 921.237
[54,     1] loss: 956.905
[55,     1] loss: 1012.529
[56,     1] loss: 1028.180
[57,     1] loss: 1017.911
[58,     1] loss: 985.121
[59,     1] loss: 956.113
[60,     1] loss: 923.143
[61,     1] loss: 892.924
[62,     1] loss: 976.983
[63,     1] loss: 898.751
[64,     1] loss: 902.464
[65,     1] loss: 892.646
[66,     1] loss: 872.170
[67,     1] loss: 880.906
[68,     1] loss: 890.238
[69,     1] loss: 847.259
[70,     1] loss: 830.161
[71,     1] loss: 841.195
[72,     1] loss: 838.672
[73,     1] loss: 765.071
[74,     1] loss: 800.463
[75,     1] loss: 752.993
[76,     1] loss: 761.538
[77,     1] loss: 728.690
[78,     1] loss: 715.989
[79,     1] loss: 685.047
[80,     1] loss: 660.193
[81,     1] loss: 611.744
[82,     1] loss: 613.026
[83,     1] loss: 735.789
[84,     1] loss: 1761.740
[85,     1] loss: 704.205
[86,     1] loss: 1283.246
[87,     1] loss: 962.960
[88,     1] loss: 1085.191
[89,     1] loss: 1116.441
[90,     1] loss: 1079.087
[91,     1] loss: 977.141
[92,     1] loss: 908.076
[93,     1] loss: 1021.637
[94,     1] loss: 929.212
[95,     1] loss: 1015.256
[96,     1] loss: 1007.121
[97,     1] loss: 911.696
[98,     1] loss: 968.733
[99,     1] loss: 976.373
[100,     1] loss: 890.361
[101,     1] loss: 864.663
[102,     1] loss: 895.793
[103,     1] loss: 871.712
[104,     1] loss: 834.294
[105,     1] loss: 837.666
[106,     1] loss: 822.123
[107,     1] loss: 823.728
[108,     1] loss: 802.261
[109,     1] loss: 775.491
[110,     1] loss: 788.488
[111,     1] loss: 767.067
[112,     1] loss: 706.119
[113,     1] loss: 860.806
[114,     1] loss: 1103.977
[115,     1] loss: 790.444
[116,     1] loss: 948.586
[117,     1] loss: 892.546
[118,     1] loss: 835.870
[119,     1] loss: 859.641
[120,     1] loss: 794.344
[121,     1] loss: 909.411
[122,     1] loss: 793.859
[123,     1] loss: 785.786
Early stopping applied (best metric=0.34470900893211365)
Finished Training
Total time taken: 18.08814287185669
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.580
[2,     1] loss: 1288.337
[3,     1] loss: 1283.350
[4,     1] loss: 1286.534
[5,     1] loss: 1280.116
[6,     1] loss: 1277.382
[7,     1] loss: 1276.843
[8,     1] loss: 1275.340
[9,     1] loss: 1267.341
[10,     1] loss: 1260.851
[11,     1] loss: 1233.094
[12,     1] loss: 1202.669
[13,     1] loss: 1169.179
[14,     1] loss: 1128.521
[15,     1] loss: 1127.386
[16,     1] loss: 1057.386
[17,     1] loss: 1060.759
[18,     1] loss: 1054.855
[19,     1] loss: 1087.498
[20,     1] loss: 1137.087
[21,     1] loss: 1013.493
[22,     1] loss: 1078.146
[23,     1] loss: 1009.300
[24,     1] loss: 1051.802
[25,     1] loss: 1095.664
[26,     1] loss: 1015.202
[27,     1] loss: 1031.137
[28,     1] loss: 1016.905
[29,     1] loss: 996.685
[30,     1] loss: 983.764
[31,     1] loss: 942.723
[32,     1] loss: 971.779
[33,     1] loss: 978.946
[34,     1] loss: 964.684
[35,     1] loss: 939.900
[36,     1] loss: 985.960
[37,     1] loss: 910.923
[38,     1] loss: 910.455
[39,     1] loss: 906.987
[40,     1] loss: 883.842
[41,     1] loss: 815.501
[42,     1] loss: 864.589
[43,     1] loss: 881.643
[44,     1] loss: 910.554
[45,     1] loss: 863.778
[46,     1] loss: 867.610
[47,     1] loss: 862.600
[48,     1] loss: 856.917
[49,     1] loss: 814.604
[50,     1] loss: 817.605
[51,     1] loss: 751.234
[52,     1] loss: 758.220
[53,     1] loss: 803.260
[54,     1] loss: 811.691
[55,     1] loss: 701.428
[56,     1] loss: 799.663
[57,     1] loss: 864.460
[58,     1] loss: 666.637
[59,     1] loss: 815.756
[60,     1] loss: 791.423
[61,     1] loss: 749.795
[62,     1] loss: 751.558
[63,     1] loss: 702.558
[64,     1] loss: 800.826
[65,     1] loss: 708.337
[66,     1] loss: 762.779
[67,     1] loss: 630.354
[68,     1] loss: 741.094
[69,     1] loss: 611.526
[70,     1] loss: 675.780
[71,     1] loss: 698.610
[72,     1] loss: 586.773
[73,     1] loss: 712.035
[74,     1] loss: 694.472
[75,     1] loss: 602.934
[76,     1] loss: 676.638
[77,     1] loss: 551.907
[78,     1] loss: 578.979
[79,     1] loss: 569.402
[80,     1] loss: 482.901
[81,     1] loss: 497.527
[82,     1] loss: 598.190
[83,     1] loss: 1168.558
[84,     1] loss: 1390.942
[85,     1] loss: 869.531
[86,     1] loss: 1140.609
[87,     1] loss: 1150.998
[88,     1] loss: 1155.784
[89,     1] loss: 1139.227
[90,     1] loss: 1104.996
[91,     1] loss: 1097.750
[92,     1] loss: 1105.856
[93,     1] loss: 1041.892
[94,     1] loss: 1045.663
[95,     1] loss: 1022.457
[96,     1] loss: 1000.973
[97,     1] loss: 976.103
[98,     1] loss: 978.435
[99,     1] loss: 939.790
[100,     1] loss: 933.405
Early stopping applied (best metric=0.3820157051086426)
Finished Training
Total time taken: 13.55120325088501
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1287.668
[2,     1] loss: 1288.342
[3,     1] loss: 1283.272
[4,     1] loss: 1285.419
[5,     1] loss: 1282.178
[6,     1] loss: 1279.722
[7,     1] loss: 1280.581
[8,     1] loss: 1276.623
[9,     1] loss: 1264.642
[10,     1] loss: 1253.214
[11,     1] loss: 1224.524
[12,     1] loss: 1185.534
[13,     1] loss: 1132.250
[14,     1] loss: 1102.594
[15,     1] loss: 1060.408
[16,     1] loss: 1105.839
[17,     1] loss: 1079.481
[18,     1] loss: 1059.338
[19,     1] loss: 1060.250
[20,     1] loss: 1006.662
[21,     1] loss: 1033.086
[22,     1] loss: 1046.255
[23,     1] loss: 1017.334
[24,     1] loss: 1004.857
[25,     1] loss: 1001.633
[26,     1] loss: 1008.103
[27,     1] loss: 996.986
[28,     1] loss: 964.497
[29,     1] loss: 912.632
[30,     1] loss: 962.202
[31,     1] loss: 902.359
[32,     1] loss: 905.601
[33,     1] loss: 897.213
[34,     1] loss: 911.025
[35,     1] loss: 891.256
[36,     1] loss: 917.797
[37,     1] loss: 871.240
[38,     1] loss: 881.690
[39,     1] loss: 851.405
[40,     1] loss: 905.620
[41,     1] loss: 978.118
[42,     1] loss: 819.966
[43,     1] loss: 865.308
[44,     1] loss: 831.883
[45,     1] loss: 811.142
[46,     1] loss: 885.382
[47,     1] loss: 796.566
[48,     1] loss: 804.331
[49,     1] loss: 847.885
[50,     1] loss: 730.529
[51,     1] loss: 781.779
[52,     1] loss: 780.056
[53,     1] loss: 740.789
[54,     1] loss: 794.979
[55,     1] loss: 1003.038
[56,     1] loss: 1297.958
[57,     1] loss: 897.052
[58,     1] loss: 910.581
[59,     1] loss: 1055.499
[60,     1] loss: 1030.313
[61,     1] loss: 1018.674
[62,     1] loss: 995.593
[63,     1] loss: 958.981
[64,     1] loss: 960.676
[65,     1] loss: 939.109
[66,     1] loss: 908.435
[67,     1] loss: 927.320
[68,     1] loss: 962.485
[69,     1] loss: 877.225
[70,     1] loss: 904.053
[71,     1] loss: 837.246
[72,     1] loss: 852.748
[73,     1] loss: 835.783
[74,     1] loss: 830.414
[75,     1] loss: 810.641
[76,     1] loss: 783.360
[77,     1] loss: 728.743
[78,     1] loss: 755.681
[79,     1] loss: 744.211
[80,     1] loss: 806.922
[81,     1] loss: 692.564
[82,     1] loss: 740.495
[83,     1] loss: 726.701
[84,     1] loss: 657.685
[85,     1] loss: 709.510
[86,     1] loss: 1251.578
[87,     1] loss: 632.391
[88,     1] loss: 1045.032
[89,     1] loss: 827.588
[90,     1] loss: 1012.241
[91,     1] loss: 978.438
[92,     1] loss: 845.994
[93,     1] loss: 908.951
[94,     1] loss: 781.384
[95,     1] loss: 785.318
[96,     1] loss: 784.999
[97,     1] loss: 731.904
[98,     1] loss: 758.264
[99,     1] loss: 733.478
[100,     1] loss: 754.437
[101,     1] loss: 748.782
[102,     1] loss: 651.437
[103,     1] loss: 697.758
[104,     1] loss: 726.605
[105,     1] loss: 622.118
[106,     1] loss: 644.716
[107,     1] loss: 780.952
[108,     1] loss: 943.536
[109,     1] loss: 638.227
[110,     1] loss: 857.079
[111,     1] loss: 695.929
[112,     1] loss: 754.375
[113,     1] loss: 612.464
[114,     1] loss: 780.829
[115,     1] loss: 716.357
[116,     1] loss: 696.168
[117,     1] loss: 701.564
[118,     1] loss: 643.350
[119,     1] loss: 679.211
[120,     1] loss: 569.548
[121,     1] loss: 818.758
[122,     1] loss: 911.371
[123,     1] loss: 727.432
[124,     1] loss: 806.975
[125,     1] loss: 784.521
[126,     1] loss: 703.609
[127,     1] loss: 672.898
[128,     1] loss: 639.092
[129,     1] loss: 707.282
[130,     1] loss: 655.796
[131,     1] loss: 641.976
[132,     1] loss: 603.458
[133,     1] loss: 573.404
[134,     1] loss: 572.339
[135,     1] loss: 747.419
[136,     1] loss: 1237.428
[137,     1] loss: 1488.032
[138,     1] loss: 1200.295
[139,     1] loss: 1176.894
[140,     1] loss: 1222.993
[141,     1] loss: 1250.804
[142,     1] loss: 1268.538
[143,     1] loss: 1268.496
[144,     1] loss: 1265.410
[145,     1] loss: 1260.569
[146,     1] loss: 1256.241
[147,     1] loss: 1245.801
[148,     1] loss: 1236.277
[149,     1] loss: 1218.291
[150,     1] loss: 1206.935
[151,     1] loss: 1177.255
[152,     1] loss: 1145.819
[153,     1] loss: 1122.542
[154,     1] loss: 1107.847
Early stopping applied (best metric=0.34128543734550476)
Finished Training
Total time taken: 25.58706498146057
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.615
[2,     1] loss: 1288.595
[3,     1] loss: 1278.825
[4,     1] loss: 1280.314
[5,     1] loss: 1278.497
[6,     1] loss: 1277.583
[7,     1] loss: 1273.168
[8,     1] loss: 1260.513
[9,     1] loss: 1234.055
[10,     1] loss: 1184.111
[11,     1] loss: 1154.160
[12,     1] loss: 1113.979
[13,     1] loss: 1101.410
[14,     1] loss: 1088.877
[15,     1] loss: 1105.801
[16,     1] loss: 1099.150
[17,     1] loss: 1141.546
[18,     1] loss: 1060.374
[19,     1] loss: 1114.011
[20,     1] loss: 1029.356
[21,     1] loss: 1052.125
[22,     1] loss: 1033.993
[23,     1] loss: 1048.387
[24,     1] loss: 957.509
[25,     1] loss: 1007.372
[26,     1] loss: 960.716
[27,     1] loss: 943.379
[28,     1] loss: 940.158
[29,     1] loss: 944.597
[30,     1] loss: 924.884
[31,     1] loss: 905.108
[32,     1] loss: 933.090
[33,     1] loss: 968.311
[34,     1] loss: 898.401
[35,     1] loss: 867.218
[36,     1] loss: 858.951
[37,     1] loss: 884.746
[38,     1] loss: 862.854
[39,     1] loss: 845.163
[40,     1] loss: 906.257
[41,     1] loss: 1372.292
[42,     1] loss: 898.354
[43,     1] loss: 1117.537
[44,     1] loss: 1049.379
[45,     1] loss: 1024.421
[46,     1] loss: 1034.992
[47,     1] loss: 1012.932
[48,     1] loss: 972.203
[49,     1] loss: 973.024
[50,     1] loss: 925.323
[51,     1] loss: 885.430
[52,     1] loss: 876.602
[53,     1] loss: 887.129
[54,     1] loss: 824.990
[55,     1] loss: 876.971
[56,     1] loss: 821.873
[57,     1] loss: 826.680
[58,     1] loss: 829.474
[59,     1] loss: 822.639
[60,     1] loss: 758.161
[61,     1] loss: 732.127
[62,     1] loss: 775.673
[63,     1] loss: 900.601
[64,     1] loss: 870.461
[65,     1] loss: 725.016
[66,     1] loss: 841.743
[67,     1] loss: 742.982
[68,     1] loss: 735.255
[69,     1] loss: 746.809
[70,     1] loss: 617.838
[71,     1] loss: 623.687
[72,     1] loss: 780.345
[73,     1] loss: 1429.512
[74,     1] loss: 1360.042
[75,     1] loss: 1228.009
[76,     1] loss: 1141.399
[77,     1] loss: 1195.384
[78,     1] loss: 1231.676
[79,     1] loss: 1248.544
[80,     1] loss: 1257.092
[81,     1] loss: 1261.239
[82,     1] loss: 1265.695
[83,     1] loss: 1265.552
[84,     1] loss: 1254.157
[85,     1] loss: 1245.255
[86,     1] loss: 1236.740
[87,     1] loss: 1249.248
[88,     1] loss: 1208.709
[89,     1] loss: 1192.554
[90,     1] loss: 1160.993
[91,     1] loss: 1152.538
[92,     1] loss: 1113.880
[93,     1] loss: 1122.447
[94,     1] loss: 1103.868
[95,     1] loss: 1089.143
[96,     1] loss: 1057.944
[97,     1] loss: 1006.768
[98,     1] loss: 1049.814
[99,     1] loss: 959.886
[100,     1] loss: 1038.167
[101,     1] loss: 1020.271
[102,     1] loss: 998.903
[103,     1] loss: 953.969
[104,     1] loss: 975.884
[105,     1] loss: 1016.472
[106,     1] loss: 958.836
[107,     1] loss: 966.744
[108,     1] loss: 931.948
[109,     1] loss: 970.016
[110,     1] loss: 918.346
[111,     1] loss: 929.360
[112,     1] loss: 867.695
[113,     1] loss: 930.489
[114,     1] loss: 939.688
[115,     1] loss: 886.458
[116,     1] loss: 925.675
[117,     1] loss: 910.884
[118,     1] loss: 916.918
Early stopping applied (best metric=0.3166227638721466)
Finished Training
Total time taken: 19.68752098083496
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.598
[2,     1] loss: 1282.464
[3,     1] loss: 1283.705
[4,     1] loss: 1281.571
[5,     1] loss: 1277.719
[6,     1] loss: 1274.805
[7,     1] loss: 1269.795
[8,     1] loss: 1238.861
[9,     1] loss: 1197.152
[10,     1] loss: 1146.882
[11,     1] loss: 1104.428
[12,     1] loss: 1105.044
[13,     1] loss: 1138.953
[14,     1] loss: 1069.666
[15,     1] loss: 1076.919
[16,     1] loss: 1037.186
[17,     1] loss: 1105.600
[18,     1] loss: 1075.136
[19,     1] loss: 1054.438
[20,     1] loss: 1024.138
[21,     1] loss: 1049.784
[22,     1] loss: 1024.050
[23,     1] loss: 1015.042
[24,     1] loss: 1003.318
[25,     1] loss: 946.759
[26,     1] loss: 990.162
[27,     1] loss: 981.840
[28,     1] loss: 1008.628
[29,     1] loss: 966.963
[30,     1] loss: 933.664
[31,     1] loss: 929.549
[32,     1] loss: 924.049
[33,     1] loss: 905.803
[34,     1] loss: 898.188
[35,     1] loss: 911.822
[36,     1] loss: 882.305
[37,     1] loss: 875.181
[38,     1] loss: 993.053
[39,     1] loss: 1057.492
[40,     1] loss: 907.468
[41,     1] loss: 907.969
[42,     1] loss: 925.493
[43,     1] loss: 900.989
[44,     1] loss: 902.740
[45,     1] loss: 884.372
[46,     1] loss: 847.351
[47,     1] loss: 816.087
[48,     1] loss: 854.035
[49,     1] loss: 816.650
[50,     1] loss: 888.887
[51,     1] loss: 844.974
[52,     1] loss: 798.768
[53,     1] loss: 822.977
[54,     1] loss: 692.774
[55,     1] loss: 790.110
[56,     1] loss: 803.474
[57,     1] loss: 796.702
[58,     1] loss: 682.276
[59,     1] loss: 770.810
[60,     1] loss: 738.541
[61,     1] loss: 692.495
[62,     1] loss: 704.042
[63,     1] loss: 893.712
[64,     1] loss: 860.221
[65,     1] loss: 758.956
[66,     1] loss: 835.664
[67,     1] loss: 696.082
[68,     1] loss: 801.626
[69,     1] loss: 697.952
[70,     1] loss: 727.739
[71,     1] loss: 639.799
[72,     1] loss: 732.277
[73,     1] loss: 621.916
[74,     1] loss: 665.747
[75,     1] loss: 655.932
[76,     1] loss: 637.265
[77,     1] loss: 570.554
[78,     1] loss: 584.843
[79,     1] loss: 475.252
[80,     1] loss: 538.596
[81,     1] loss: 604.576
[82,     1] loss: 2023.326
[83,     1] loss: 1410.153
[84,     1] loss: 1215.098
[85,     1] loss: 1206.682
[86,     1] loss: 1243.173
[87,     1] loss: 1258.923
[88,     1] loss: 1259.886
[89,     1] loss: 1258.993
[90,     1] loss: 1254.073
[91,     1] loss: 1249.656
[92,     1] loss: 1245.369
[93,     1] loss: 1247.855
[94,     1] loss: 1235.660
[95,     1] loss: 1233.333
[96,     1] loss: 1227.263
[97,     1] loss: 1226.566
[98,     1] loss: 1210.375
[99,     1] loss: 1211.698
[100,     1] loss: 1205.651
[101,     1] loss: 1177.489
[102,     1] loss: 1164.474
[103,     1] loss: 1167.355
[104,     1] loss: 1144.341
[105,     1] loss: 1139.573
[106,     1] loss: 1128.586
Early stopping applied (best metric=0.3650836944580078)
Finished Training
Total time taken: 17.51371431350708
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.813
[2,     1] loss: 1282.226
[3,     1] loss: 1286.620
[4,     1] loss: 1282.362
[5,     1] loss: 1281.712
[6,     1] loss: 1279.129
[7,     1] loss: 1277.545
[8,     1] loss: 1273.559
[9,     1] loss: 1267.092
[10,     1] loss: 1250.782
[11,     1] loss: 1214.093
[12,     1] loss: 1165.625
[13,     1] loss: 1145.133
[14,     1] loss: 1223.659
[15,     1] loss: 1092.953
[16,     1] loss: 1138.550
[17,     1] loss: 1099.584
[18,     1] loss: 1053.582
[19,     1] loss: 1099.581
[20,     1] loss: 1061.982
[21,     1] loss: 1050.485
[22,     1] loss: 1019.422
[23,     1] loss: 1003.594
[24,     1] loss: 1036.835
[25,     1] loss: 971.155
[26,     1] loss: 955.651
[27,     1] loss: 932.619
[28,     1] loss: 936.034
[29,     1] loss: 939.467
[30,     1] loss: 919.038
[31,     1] loss: 899.071
[32,     1] loss: 898.143
[33,     1] loss: 891.509
[34,     1] loss: 960.290
[35,     1] loss: 962.812
[36,     1] loss: 850.853
[37,     1] loss: 863.427
[38,     1] loss: 850.533
[39,     1] loss: 855.869
[40,     1] loss: 814.834
[41,     1] loss: 862.450
[42,     1] loss: 754.601
[43,     1] loss: 819.400
[44,     1] loss: 797.838
[45,     1] loss: 819.464
[46,     1] loss: 757.106
[47,     1] loss: 884.924
[48,     1] loss: 1098.448
[49,     1] loss: 754.179
[50,     1] loss: 893.644
[51,     1] loss: 789.984
[52,     1] loss: 839.548
[53,     1] loss: 830.417
[54,     1] loss: 775.596
[55,     1] loss: 738.707
[56,     1] loss: 754.501
[57,     1] loss: 770.484
[58,     1] loss: 673.229
[59,     1] loss: 698.479
[60,     1] loss: 660.057
[61,     1] loss: 715.876
[62,     1] loss: 735.829
[63,     1] loss: 648.520
[64,     1] loss: 688.658
[65,     1] loss: 586.375
[66,     1] loss: 685.718
[67,     1] loss: 619.958
[68,     1] loss: 532.992
[69,     1] loss: 601.372
[70,     1] loss: 628.598
[71,     1] loss: 570.661
[72,     1] loss: 501.767
[73,     1] loss: 573.866
[74,     1] loss: 638.553
[75,     1] loss: 667.997
[76,     1] loss: 548.359
[77,     1] loss: 715.357
Early stopping applied (best metric=0.3988911509513855)
Finished Training
Total time taken: 12.964712619781494
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.418
[2,     1] loss: 1285.685
[3,     1] loss: 1282.871
[4,     1] loss: 1282.792
[5,     1] loss: 1280.878
[6,     1] loss: 1276.773
[7,     1] loss: 1272.907
[8,     1] loss: 1270.778
[9,     1] loss: 1261.078
[10,     1] loss: 1249.242
[11,     1] loss: 1200.592
[12,     1] loss: 1146.607
[13,     1] loss: 1094.833
[14,     1] loss: 1098.576
[15,     1] loss: 1086.917
[16,     1] loss: 1106.209
[17,     1] loss: 1142.110
[18,     1] loss: 1041.053
[19,     1] loss: 1083.816
[20,     1] loss: 1035.113
[21,     1] loss: 1050.658
[22,     1] loss: 1069.126
[23,     1] loss: 1027.834
[24,     1] loss: 1005.241
[25,     1] loss: 983.459
[26,     1] loss: 992.698
[27,     1] loss: 930.214
[28,     1] loss: 956.502
[29,     1] loss: 941.297
[30,     1] loss: 1006.378
[31,     1] loss: 929.940
[32,     1] loss: 899.213
[33,     1] loss: 967.907
[34,     1] loss: 900.488
[35,     1] loss: 967.465
[36,     1] loss: 879.971
[37,     1] loss: 915.318
[38,     1] loss: 890.409
[39,     1] loss: 787.382
[40,     1] loss: 830.514
[41,     1] loss: 770.851
[42,     1] loss: 896.673
[43,     1] loss: 1164.282
[44,     1] loss: 1223.153
[45,     1] loss: 1038.920
[46,     1] loss: 1030.088
[47,     1] loss: 1035.059
[48,     1] loss: 1037.462
[49,     1] loss: 1103.631
[50,     1] loss: 1070.648
[51,     1] loss: 1057.274
[52,     1] loss: 1005.898
[53,     1] loss: 983.831
[54,     1] loss: 980.698
[55,     1] loss: 966.581
[56,     1] loss: 922.403
[57,     1] loss: 918.888
[58,     1] loss: 914.706
[59,     1] loss: 839.789
[60,     1] loss: 881.180
[61,     1] loss: 873.717
[62,     1] loss: 831.547
[63,     1] loss: 855.602
[64,     1] loss: 825.091
[65,     1] loss: 810.965
[66,     1] loss: 842.174
[67,     1] loss: 794.012
[68,     1] loss: 751.368
[69,     1] loss: 774.505
[70,     1] loss: 691.470
[71,     1] loss: 734.653
[72,     1] loss: 812.279
[73,     1] loss: 718.667
[74,     1] loss: 812.411
[75,     1] loss: 979.925
[76,     1] loss: 840.979
[77,     1] loss: 896.725
[78,     1] loss: 763.983
[79,     1] loss: 761.468
[80,     1] loss: 762.654
[81,     1] loss: 737.509
[82,     1] loss: 679.825
[83,     1] loss: 701.177
[84,     1] loss: 644.132
[85,     1] loss: 629.157
[86,     1] loss: 619.501
[87,     1] loss: 688.417
[88,     1] loss: 1012.715
[89,     1] loss: 731.917
[90,     1] loss: 729.451
Early stopping applied (best metric=0.3470142185688019)
Finished Training
Total time taken: 13.793755769729614
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1287.446
[2,     1] loss: 1283.212
[3,     1] loss: 1281.169
[4,     1] loss: 1279.690
[5,     1] loss: 1280.127
[6,     1] loss: 1276.887
[7,     1] loss: 1267.400
[8,     1] loss: 1246.685
[9,     1] loss: 1212.351
[10,     1] loss: 1187.111
[11,     1] loss: 1127.155
[12,     1] loss: 1151.818
[13,     1] loss: 1084.064
[14,     1] loss: 1062.892
[15,     1] loss: 1037.639
[16,     1] loss: 1091.694
[17,     1] loss: 1058.867
[18,     1] loss: 1036.847
[19,     1] loss: 1012.066
[20,     1] loss: 1003.574
[21,     1] loss: 1004.681
[22,     1] loss: 992.617
[23,     1] loss: 1002.757
[24,     1] loss: 951.070
[25,     1] loss: 1012.869
[26,     1] loss: 937.924
[27,     1] loss: 984.475
[28,     1] loss: 1088.705
[29,     1] loss: 933.157
[30,     1] loss: 992.011
[31,     1] loss: 978.556
[32,     1] loss: 951.544
[33,     1] loss: 974.011
[34,     1] loss: 884.666
[35,     1] loss: 906.676
[36,     1] loss: 863.743
[37,     1] loss: 918.724
[38,     1] loss: 854.478
[39,     1] loss: 817.108
[40,     1] loss: 821.389
[41,     1] loss: 791.919
[42,     1] loss: 867.138
[43,     1] loss: 802.351
[44,     1] loss: 949.847
[45,     1] loss: 1027.292
[46,     1] loss: 816.823
[47,     1] loss: 915.111
[48,     1] loss: 847.815
[49,     1] loss: 868.664
[50,     1] loss: 877.259
[51,     1] loss: 743.142
[52,     1] loss: 835.588
[53,     1] loss: 744.621
[54,     1] loss: 758.907
[55,     1] loss: 745.938
[56,     1] loss: 734.053
[57,     1] loss: 790.007
[58,     1] loss: 747.144
[59,     1] loss: 715.669
[60,     1] loss: 676.425
[61,     1] loss: 688.719
[62,     1] loss: 659.269
[63,     1] loss: 654.348
[64,     1] loss: 779.284
[65,     1] loss: 1376.472
[66,     1] loss: 727.767
[67,     1] loss: 987.205
[68,     1] loss: 923.768
[69,     1] loss: 962.868
[70,     1] loss: 956.589
[71,     1] loss: 923.761
[72,     1] loss: 813.325
[73,     1] loss: 887.428
[74,     1] loss: 863.453
[75,     1] loss: 878.347
[76,     1] loss: 765.094
[77,     1] loss: 792.678
[78,     1] loss: 776.706
[79,     1] loss: 784.747
[80,     1] loss: 808.236
[81,     1] loss: 677.676
[82,     1] loss: 623.105
[83,     1] loss: 711.088
[84,     1] loss: 586.162
[85,     1] loss: 785.176
[86,     1] loss: 1116.254
[87,     1] loss: 772.513
[88,     1] loss: 755.392
[89,     1] loss: 938.567
[90,     1] loss: 749.964
[91,     1] loss: 786.460
[92,     1] loss: 723.879
[93,     1] loss: 710.521
[94,     1] loss: 643.629
[95,     1] loss: 704.985
[96,     1] loss: 584.827
[97,     1] loss: 699.300
[98,     1] loss: 947.069
[99,     1] loss: 610.604
[100,     1] loss: 857.706
[101,     1] loss: 639.283
[102,     1] loss: 734.737
[103,     1] loss: 630.498
[104,     1] loss: 837.209
[105,     1] loss: 667.652
[106,     1] loss: 694.113
[107,     1] loss: 602.841
Early stopping applied (best metric=0.3651592433452606)
Finished Training
Total time taken: 15.244141578674316
{'Hydroxylation-K Validation Accuracy': 0.7184988179669031, 'Hydroxylation-K Validation Sensitivity': 0.6696296296296296, 'Hydroxylation-K Validation Specificity': 0.731578947368421, 'Hydroxylation-K Validation Precision': 0.3852979041369134, 'Hydroxylation-K AUC ROC': 0.7744639376218324, 'Hydroxylation-K AUC PR': 0.5924402267749508, 'Hydroxylation-K MCC': 0.33779287966037913, 'Hydroxylation-K F1': 0.4834477769771176, 'Validation Loss (Hydroxylation-K)': 0.46631311376889545, 'Hydroxylation-P Validation Accuracy': 0.7866327597583879, 'Hydroxylation-P Validation Sensitivity': 0.8047619047619048, 'Hydroxylation-P Validation Specificity': 0.7826549952616091, 'Hydroxylation-P Validation Precision': 0.4452986053554138, 'Hydroxylation-P AUC ROC': 0.8539127510614867, 'Hydroxylation-P AUC PR': 0.6181462733519486, 'Hydroxylation-P MCC': 0.48139105979551633, 'Hydroxylation-P F1': 0.572312464854298, 'Validation Loss (Hydroxylation-P)': 0.3630028128623962, 'Validation Loss (total)': 0.8293159246444702, 'TimeToTrain': 17.016696151097616}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009386059782564182,
 'learning_rate_Hydroxylation-K': 0.0008646659946451177,
 'learning_rate_Hydroxylation-P': 0.006408158391511884,
 'log_base': 1.1472991023772834,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1353844186,
 'sample_weights': [1.774328641594215, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.48250937437161,
 'weight_decay_Hydroxylation-K': 5.237030691590181,
 'weight_decay_Hydroxylation-P': 8.92444630556451}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3967.132
[2,     1] loss: 3947.290
[3,     1] loss: 3946.260
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007588736886504657,
 'learning_rate_Hydroxylation-K': 0.004362328258984759,
 'learning_rate_Hydroxylation-P': 0.0075384862421955376,
 'log_base': 2.0205080522816994,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2241978693,
 'sample_weights': [12.149306313030673, 1.5187217941371198],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.751851204449613,
 'weight_decay_Hydroxylation-K': 0.4144778710208724,
 'weight_decay_Hydroxylation-P': 6.557985333067849}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.831
[2,     1] loss: 1407.355
[3,     1] loss: 1408.095
[4,     1] loss: 1407.732
[5,     1] loss: 1405.060
[6,     1] loss: 1397.951
[7,     1] loss: 1381.509
[8,     1] loss: 1351.460
[9,     1] loss: 1302.051
[10,     1] loss: 1294.786
[11,     1] loss: 1312.506
[12,     1] loss: 1212.065
[13,     1] loss: 1219.344
[14,     1] loss: 1216.916
[15,     1] loss: 1146.129
[16,     1] loss: 1124.806
[17,     1] loss: 1148.691
[18,     1] loss: 1091.834
[19,     1] loss: 1108.646
[20,     1] loss: 1110.544
[21,     1] loss: 1087.722
[22,     1] loss: 1080.550
[23,     1] loss: 1026.294
[24,     1] loss: 1055.055
[25,     1] loss: 1031.306
[26,     1] loss: 989.260
[27,     1] loss: 1052.935
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023048644431564787,
 'learning_rate_Hydroxylation-K': 0.0005579996300200894,
 'learning_rate_Hydroxylation-P': 0.009976433279246926,
 'log_base': 2.2954770415698813,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2699168725,
 'sample_weights': [2.373563009562588, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.599481187145194,
 'weight_decay_Hydroxylation-K': 6.003513758948298,
 'weight_decay_Hydroxylation-P': 0.6187706734552598}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.511
[2,     1] loss: 1328.890
[3,     1] loss: 1336.511
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004143770371548034,
 'learning_rate_Hydroxylation-K': 0.0076631561688042076,
 'learning_rate_Hydroxylation-P': 0.007038955816696389,
 'log_base': 1.0551956532978124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2447760940,
 'sample_weights': [2.00910026582066, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.84359470361226,
 'weight_decay_Hydroxylation-K': 0.5491542237427476,
 'weight_decay_Hydroxylation-P': 0.45486252428043716}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10099.133
[2,     1] loss: 10064.554
[3,     1] loss: 10089.248
[4,     1] loss: 10083.940
[5,     1] loss: 10069.536
[6,     1] loss: 10037.969
[7,     1] loss: 10052.059
[8,     1] loss: 10047.741
[9,     1] loss: 10043.430
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002260026395289154,
 'learning_rate_Hydroxylation-K': 0.0014549924769678597,
 'learning_rate_Hydroxylation-P': 0.006679604125951475,
 'log_base': 2.888524387497164,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2300027191,
 'sample_weights': [31.073164511997827, 3.8842951968841413],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.084673617549612,
 'weight_decay_Hydroxylation-K': 4.893066720021094,
 'weight_decay_Hydroxylation-P': 0.5018161251660418}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.885
[2,     1] loss: 1237.196
[3,     1] loss: 1242.364
[4,     1] loss: 1236.592
[5,     1] loss: 1231.083
[6,     1] loss: 1220.901
[7,     1] loss: 1190.979
[8,     1] loss: 1154.690
[9,     1] loss: 1125.976
[10,     1] loss: 1089.780
[11,     1] loss: 1079.382
[12,     1] loss: 1071.457
[13,     1] loss: 1028.098
[14,     1] loss: 1011.612
[15,     1] loss: 961.288
[16,     1] loss: 991.170
[17,     1] loss: 949.667
[18,     1] loss: 970.529
[19,     1] loss: 980.587
[20,     1] loss: 971.506
[21,     1] loss: 956.441
[22,     1] loss: 981.883
[23,     1] loss: 927.208
[24,     1] loss: 927.954
[25,     1] loss: 978.162
[26,     1] loss: 920.637
[27,     1] loss: 977.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005944544887015217,
 'learning_rate_Hydroxylation-K': 0.0007589431496233285,
 'learning_rate_Hydroxylation-P': 0.008198602977992672,
 'log_base': 2.8301229529486274,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1072044158,
 'sample_weights': [1.5738390670158482, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.978486379889874,
 'weight_decay_Hydroxylation-K': 5.6052949563696535,
 'weight_decay_Hydroxylation-P': 0.5402435815752977}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.773
[2,     1] loss: 1243.063
[3,     1] loss: 1252.785
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037727167082635127,
 'learning_rate_Hydroxylation-K': 0.0006116401706897099,
 'learning_rate_Hydroxylation-P': 0.0026859069503027775,
 'log_base': 1.4561355539823642,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2277655222,
 'sample_weights': [1.604739787283559, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.433520909740206,
 'weight_decay_Hydroxylation-K': 8.885431878354847,
 'weight_decay_Hydroxylation-P': 2.366354146237437}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1848.970
[2,     1] loss: 1841.982
[3,     1] loss: 1842.399
[4,     1] loss: 1846.480
[5,     1] loss: 1842.060
[6,     1] loss: 1838.303
[7,     1] loss: 1843.418
[8,     1] loss: 1831.993
[9,     1] loss: 1817.035
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00565401832344068,
 'learning_rate_Hydroxylation-K': 0.0012139261017543321,
 'learning_rate_Hydroxylation-P': 0.0070654947109803036,
 'log_base': 2.8477124704106225,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2626725009,
 'sample_weights': [4.44253629561508, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.442604041474643,
 'weight_decay_Hydroxylation-K': 5.1407060129687165,
 'weight_decay_Hydroxylation-P': 0.5699670980199029}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.565
[2,     1] loss: 1244.383
[3,     1] loss: 1260.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007462371056573676,
 'learning_rate_Hydroxylation-K': 0.0024151267977673254,
 'learning_rate_Hydroxylation-P': 0.005819403323849994,
 'log_base': 2.462532724102717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3088689918,
 'sample_weights': [1.5952389638752666, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9006065517532633,
 'weight_decay_Hydroxylation-K': 0.7264515506039051,
 'weight_decay_Hydroxylation-P': 2.076349039074234}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.228
[2,     1] loss: 1317.396
[3,     1] loss: 1301.111
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003287625524513953,
 'learning_rate_Hydroxylation-K': 0.0022416776765035677,
 'learning_rate_Hydroxylation-P': 0.005675614982586025,
 'log_base': 2.9452837550698567,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2446742627,
 'sample_weights': [1.8524866432284808, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.108781783410141,
 'weight_decay_Hydroxylation-K': 5.321230966393392,
 'weight_decay_Hydroxylation-P': 1.5745181614516488}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.185
[2,     1] loss: 1235.726
[3,     1] loss: 1233.721
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00017723779860398265,
 'learning_rate_Hydroxylation-K': 0.0024034041739766122,
 'learning_rate_Hydroxylation-P': 0.0046529787689143605,
 'log_base': 2.874072124699017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 907611433,
 'sample_weights': [1.5454871011277374, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2583608932866293,
 'weight_decay_Hydroxylation-K': 3.4194819933615515,
 'weight_decay_Hydroxylation-P': 8.11231719568185}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.000
[2,     1] loss: 1239.965
[3,     1] loss: 1242.802
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007077214214392968,
 'learning_rate_Hydroxylation-K': 0.003027341828097846,
 'learning_rate_Hydroxylation-P': 0.00041721138893255696,
 'log_base': 2.7374066831515442,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4034770397,
 'sample_weights': [1.5813165606357562, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.004928298864733,
 'weight_decay_Hydroxylation-K': 8.921385590921739,
 'weight_decay_Hydroxylation-P': 9.429946300561928}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.630
[2,     1] loss: 1260.705
[3,     1] loss: 1256.739
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004958085711471284,
 'learning_rate_Hydroxylation-K': 0.009015821154932517,
 'learning_rate_Hydroxylation-P': 0.003475133143384973,
 'log_base': 1.5363097177820024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1337601923,
 'sample_weights': [1.6578201599784945, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.872988024775926,
 'weight_decay_Hydroxylation-K': 8.244006471772899,
 'weight_decay_Hydroxylation-P': 7.958676722187285}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1732.815
[2,     1] loss: 1731.510
[3,     1] loss: 1720.077
[4,     1] loss: 1743.591
[5,     1] loss: 1731.614
[6,     1] loss: 1725.904
[7,     1] loss: 1723.418
[8,     1] loss: 1737.193
[9,     1] loss: 1729.124
[10,     1] loss: 1727.711
[11,     1] loss: 1728.441
[12,     1] loss: 1724.857
[13,     1] loss: 1727.288
[14,     1] loss: 1724.079
[15,     1] loss: 1724.823
[16,     1] loss: 1723.542
[17,     1] loss: 1722.995
[18,     1] loss: 1725.869
[19,     1] loss: 1719.644
[20,     1] loss: 1719.576
[21,     1] loss: 1714.069
[22,     1] loss: 1703.896
[23,     1] loss: 1696.920
[24,     1] loss: 1661.020
[25,     1] loss: 1645.275
[26,     1] loss: 1624.289
[27,     1] loss: 1587.169
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004201825958723433,
 'learning_rate_Hydroxylation-K': 0.0015122890759002186,
 'learning_rate_Hydroxylation-P': 0.007204049352891791,
 'log_base': 2.240982719017817,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2342722921,
 'sample_weights': [3.8880024628453236, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.02195931343705,
 'weight_decay_Hydroxylation-K': 2.5888978378394536,
 'weight_decay_Hydroxylation-P': 2.7074776875567754}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.586
[2,     1] loss: 1343.663
[3,     1] loss: 1345.586
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009987379360743464,
 'learning_rate_Hydroxylation-K': 0.0020810176173368744,
 'learning_rate_Hydroxylation-P': 0.0067689487517330355,
 'log_base': 2.1890368846062196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1304950005,
 'sample_weights': [2.0689220252524456, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.547434545497551,
 'weight_decay_Hydroxylation-K': 7.552108114527009,
 'weight_decay_Hydroxylation-P': 2.357519086388866}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1357.942
[2,     1] loss: 1350.384
[3,     1] loss: 1361.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008510276552647628,
 'learning_rate_Hydroxylation-K': 0.006717918469110179,
 'learning_rate_Hydroxylation-P': 0.005590576405032517,
 'log_base': 2.745860720719411,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1891319952,
 'sample_weights': [2.1308549158228582, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.832454566277597,
 'weight_decay_Hydroxylation-K': 2.6481545273904,
 'weight_decay_Hydroxylation-P': 3.5100363886225168}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.642
[2,     1] loss: 1256.690
[3,     1] loss: 1258.663
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00943745916283385,
 'learning_rate_Hydroxylation-K': 0.009649609924769233,
 'learning_rate_Hydroxylation-P': 0.00915685912598425,
 'log_base': 2.9224553966977505,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3387108949,
 'sample_weights': [1.6527592284568988, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.861053285871388,
 'weight_decay_Hydroxylation-K': 5.448856583835529,
 'weight_decay_Hydroxylation-P': 1.5482647547735668}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.488
[2,     1] loss: 1240.207
[3,     1] loss: 1236.988
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035981311610621684,
 'learning_rate_Hydroxylation-K': 0.00044940360695270486,
 'learning_rate_Hydroxylation-P': 0.008976929976364553,
 'log_base': 2.9991978527693153,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4130463318,
 'sample_weights': [1.5567004379581795, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.328292642350252,
 'weight_decay_Hydroxylation-K': 8.40907571594606,
 'weight_decay_Hydroxylation-P': 1.8551787388059846}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1226.659
[2,     1] loss: 1231.229
[3,     1] loss: 1228.500
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033930975733739736,
 'learning_rate_Hydroxylation-K': 0.003940372897225624,
 'learning_rate_Hydroxylation-P': 0.003188987890132764,
 'log_base': 2.8550420659305216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 81559844,
 'sample_weights': [1.5199626203528835, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.210997476151747,
 'weight_decay_Hydroxylation-K': 4.468592676659588,
 'weight_decay_Hydroxylation-P': 5.048954303110377}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.161
[2,     1] loss: 1247.824
[3,     1] loss: 1242.433
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005746484193015658,
 'learning_rate_Hydroxylation-K': 0.001432506343020765,
 'learning_rate_Hydroxylation-P': 0.00807428122103212,
 'log_base': 2.138493330802298,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3578310334,
 'sample_weights': [1.591330195070864, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.757673162898957,
 'weight_decay_Hydroxylation-K': 1.224794391058011,
 'weight_decay_Hydroxylation-P': 9.066158692995172}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1377.723
[2,     1] loss: 1372.666
[3,     1] loss: 1370.090
[4,     1] loss: 1373.159
[5,     1] loss: 1374.464
[6,     1] loss: 1370.469
[7,     1] loss: 1366.593
[8,     1] loss: 1361.846
[9,     1] loss: 1358.669
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00971549767176607,
 'learning_rate_Hydroxylation-K': 0.005773224732845854,
 'learning_rate_Hydroxylation-P': 0.0037273429694489124,
 'log_base': 2.13262135905849,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 222100574,
 'sample_weights': [2.19634230616742, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.505818992716353,
 'weight_decay_Hydroxylation-K': 9.594661824122461,
 'weight_decay_Hydroxylation-P': 8.339867180005427}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1377.608
[2,     1] loss: 1380.611
[3,     1] loss: 1373.200
[4,     1] loss: 1373.028
[5,     1] loss: 1374.128
[6,     1] loss: 1372.578
[7,     1] loss: 1369.414
[8,     1] loss: 1372.780
[9,     1] loss: 1373.185
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006157776005988498,
 'learning_rate_Hydroxylation-K': 0.009202366531852088,
 'learning_rate_Hydroxylation-P': 0.004105705522181178,
 'log_base': 1.2252647472503537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1141103713,
 'sample_weights': [2.2043162889954977, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.066255137634412,
 'weight_decay_Hydroxylation-K': 1.2150489913962608,
 'weight_decay_Hydroxylation-P': 0.6413125292002433}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2672.942
[2,     1] loss: 2662.635
[3,     1] loss: 2673.592
[4,     1] loss: 2662.885
[5,     1] loss: 2679.368
[6,     1] loss: 2657.228
[7,     1] loss: 2661.898
[8,     1] loss: 2647.988
[9,     1] loss: 2639.883
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004131409999182193,
 'learning_rate_Hydroxylation-K': 0.0004619164257609011,
 'learning_rate_Hydroxylation-P': 0.007874410514887393,
 'log_base': 2.3859695329809263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4270365806,
 'sample_weights': [8.217504853317172, 1.0272276780753216],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.377483823015192,
 'weight_decay_Hydroxylation-K': 5.156071707078446,
 'weight_decay_Hydroxylation-P': 1.5283700821632413}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.403
[2,     1] loss: 1314.149
[3,     1] loss: 1312.913
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035019735940080587,
 'learning_rate_Hydroxylation-K': 0.0038337217719929227,
 'learning_rate_Hydroxylation-P': 0.00044591265857322733,
 'log_base': 2.5495104473012717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1179784911,
 'sample_weights': [1.919770565807422, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.09585636532856,
 'weight_decay_Hydroxylation-K': 9.801383663685602,
 'weight_decay_Hydroxylation-P': 3.2108401306664542}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.143
[2,     1] loss: 1288.920
[3,     1] loss: 1283.046
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031623350009238357,
 'learning_rate_Hydroxylation-K': 0.002131939241712257,
 'learning_rate_Hydroxylation-P': 0.0032345878739118853,
 'log_base': 2.7353166198840144,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 518986110,
 'sample_weights': [1.7837810906609237, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.390363926517095,
 'weight_decay_Hydroxylation-K': 3.981475718173277,
 'weight_decay_Hydroxylation-P': 1.1678561676673727}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.004
[2,     1] loss: 1258.332
[3,     1] loss: 1255.301
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003598656848880882,
 'learning_rate_Hydroxylation-K': 0.0017048983524522775,
 'learning_rate_Hydroxylation-P': 0.009173498273188325,
 'log_base': 1.0745987026136077,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1552211297,
 'sample_weights': [1.6590785599279188, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.8479713704898277,
 'weight_decay_Hydroxylation-K': 9.423061649673496,
 'weight_decay_Hydroxylation-P': 6.533679127725229}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7481.706
[2,     1] loss: 7552.509
[3,     1] loss: 7529.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004338664108207509,
 'learning_rate_Hydroxylation-K': 0.004894568575616993,
 'learning_rate_Hydroxylation-P': 0.0011379870313910792,
 'log_base': 1.720767755756957,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4123392211,
 'sample_weights': [23.20369677874155, 2.900573834789863],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6859649492215305,
 'weight_decay_Hydroxylation-K': 1.3153302870371921,
 'weight_decay_Hydroxylation-P': 8.14321039460684}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1559.114
[2,     1] loss: 1558.922
[3,     1] loss: 1561.980
[4,     1] loss: 1555.721
[5,     1] loss: 1554.737
[6,     1] loss: 1551.509
[7,     1] loss: 1551.055
[8,     1] loss: 1544.207
[9,     1] loss: 1548.028
[10,     1] loss: 1536.417
[11,     1] loss: 1520.861
[12,     1] loss: 1504.626
[13,     1] loss: 1487.202
[14,     1] loss: 1448.107
[15,     1] loss: 1399.027
[16,     1] loss: 1381.459
[17,     1] loss: 1354.293
[18,     1] loss: 1370.456
[19,     1] loss: 1315.630
[20,     1] loss: 1349.311
[21,     1] loss: 1254.802
[22,     1] loss: 1324.185
[23,     1] loss: 1284.050
[24,     1] loss: 1261.074
[25,     1] loss: 1302.005
[26,     1] loss: 1245.522
[27,     1] loss: 1254.697
[28,     1] loss: 1264.173
[29,     1] loss: 1227.602
[30,     1] loss: 1265.167
[31,     1] loss: 1223.559
[32,     1] loss: 1191.929
[33,     1] loss: 1222.258
[34,     1] loss: 1217.316
[35,     1] loss: 1213.290
[36,     1] loss: 1201.704
[37,     1] loss: 1149.293
[38,     1] loss: 1092.490
[39,     1] loss: 1164.447
[40,     1] loss: 1129.040
[41,     1] loss: 1097.070
[42,     1] loss: 1111.078
[43,     1] loss: 1024.854
[44,     1] loss: 1156.289
[45,     1] loss: 1051.564
[46,     1] loss: 1100.142
[47,     1] loss: 1013.006
[48,     1] loss: 1048.498
[49,     1] loss: 1018.783
[50,     1] loss: 1062.188
[51,     1] loss: 990.409
[52,     1] loss: 1043.948
[53,     1] loss: 1156.244
[54,     1] loss: 939.922
[55,     1] loss: 1044.132
[56,     1] loss: 979.335
[57,     1] loss: 947.441
[58,     1] loss: 1024.502
[59,     1] loss: 946.368
[60,     1] loss: 993.591
[61,     1] loss: 952.848
[62,     1] loss: 889.155
[63,     1] loss: 890.901
[64,     1] loss: 844.570
[65,     1] loss: 908.438
[66,     1] loss: 928.954
[67,     1] loss: 884.602
[68,     1] loss: 880.806
[69,     1] loss: 848.135
[70,     1] loss: 845.875
[71,     1] loss: 878.001
[72,     1] loss: 816.515
[73,     1] loss: 857.958
[74,     1] loss: 796.236
[75,     1] loss: 820.966
[76,     1] loss: 902.884
[77,     1] loss: 740.017
[78,     1] loss: 774.033
[79,     1] loss: 674.527
[80,     1] loss: 803.579
[81,     1] loss: 777.365
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028461201658172633,
 'learning_rate_Hydroxylation-K': 0.004400311930570043,
 'learning_rate_Hydroxylation-P': 0.0005329308495867021,
 'log_base': 1.1653775923242686,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 356553725,
 'sample_weights': [3.075780574362757, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.510791232655946,
 'weight_decay_Hydroxylation-K': 0.9499977981709703,
 'weight_decay_Hydroxylation-P': 5.4737435511880035}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3547.884
[2,     1] loss: 3544.267
[3,     1] loss: 3544.500
[4,     1] loss: 3539.035
[5,     1] loss: 3540.792
[6,     1] loss: 3536.370
[7,     1] loss: 3533.433
[8,     1] loss: 3525.193
[9,     1] loss: 3541.047
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006337553112178592,
 'learning_rate_Hydroxylation-K': 0.009662053769971209,
 'learning_rate_Hydroxylation-P': 0.00012280766322241466,
 'log_base': 1.3158753863130856,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3433761393,
 'sample_weights': [10.908174275944745, 1.3635743128276294],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.990716911184304,
 'weight_decay_Hydroxylation-K': 7.953315155928186,
 'weight_decay_Hydroxylation-P': 8.38618134873524}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2189.889
[2,     1] loss: 2189.793
[3,     1] loss: 2190.097
[4,     1] loss: 2191.593
[5,     1] loss: 2196.640
[6,     1] loss: 2198.404
[7,     1] loss: 2186.884
[8,     1] loss: 2190.310
[9,     1] loss: 2185.854
[10,     1] loss: 2190.601
[11,     1] loss: 2185.055
[12,     1] loss: 2191.022
[13,     1] loss: 2188.070
[14,     1] loss: 2178.795
[15,     1] loss: 2182.927
[16,     1] loss: 2187.566
[17,     1] loss: 2188.002
[18,     1] loss: 2180.847
[19,     1] loss: 2181.553
[20,     1] loss: 2187.149
[21,     1] loss: 2180.161
[22,     1] loss: 2169.095
[23,     1] loss: 2168.311
[24,     1] loss: 2168.881
[25,     1] loss: 2156.133
[26,     1] loss: 2162.158
[27,     1] loss: 2148.377
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003960537708364655,
 'learning_rate_Hydroxylation-K': 0.0049743230943826286,
 'learning_rate_Hydroxylation-P': 0.0006004441185844787,
 'log_base': 1.8503584689199444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4267240254,
 'sample_weights': [6.081712749656676, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3096502137075139,
 'weight_decay_Hydroxylation-K': 0.8267690804041494,
 'weight_decay_Hydroxylation-P': 8.341782087081087}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1481.180
[2,     1] loss: 1482.912
[3,     1] loss: 1483.026
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004616504858612486,
 'learning_rate_Hydroxylation-K': 0.0030557686299889208,
 'learning_rate_Hydroxylation-P': 0.0014126901974677,
 'log_base': 1.437947009187561,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 804362324,
 'sample_weights': [2.7128681620921324, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9610902999616684,
 'weight_decay_Hydroxylation-K': 0.028053102266556218,
 'weight_decay_Hydroxylation-P': 7.53828697498266}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1878.661
[2,     1] loss: 1873.635
[3,     1] loss: 1879.857
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004263051477986674,
 'learning_rate_Hydroxylation-K': 0.004107787028083212,
 'learning_rate_Hydroxylation-P': 0.009894187927738742,
 'log_base': 1.0825819633303746,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3186007657,
 'sample_weights': [4.596276791032288, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.286072036264474,
 'weight_decay_Hydroxylation-K': 3.3242258716216093,
 'weight_decay_Hydroxylation-P': 3.4846648245329996}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6878.947
[2,     1] loss: 6833.391
[3,     1] loss: 6810.666
[4,     1] loss: 6845.479
[5,     1] loss: 6815.338
[6,     1] loss: 6807.298
[7,     1] loss: 6842.339
[8,     1] loss: 6823.449
[9,     1] loss: 6812.868
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005182815974468784,
 'learning_rate_Hydroxylation-K': 0.0006122517153245928,
 'learning_rate_Hydroxylation-P': 0.0020791301394628902,
 'log_base': 2.227400470593251,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1958901925,
 'sample_weights': [21.03927412085996, 2.630010579773127],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.1533195884953917,
 'weight_decay_Hydroxylation-K': 9.447302481748086,
 'weight_decay_Hydroxylation-P': 2.436211605555944}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.895
[2,     1] loss: 1353.745
[3,     1] loss: 1354.814
[4,     1] loss: 1349.067
[5,     1] loss: 1345.691
[6,     1] loss: 1344.674
[7,     1] loss: 1341.510
[8,     1] loss: 1337.237
[9,     1] loss: 1334.221
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0042267873407506856,
 'learning_rate_Hydroxylation-K': 0.006120753434965543,
 'learning_rate_Hydroxylation-P': 0.0005604235241503393,
 'log_base': 1.4562396314474362,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3017517892,
 'sample_weights': [2.084627590237295, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6787754933821026,
 'weight_decay_Hydroxylation-K': 3.0922189683017347,
 'weight_decay_Hydroxylation-P': 6.551107394118105}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1845.987
[2,     1] loss: 1853.991
[3,     1] loss: 1839.670
[4,     1] loss: 1840.519
[5,     1] loss: 1846.427
[6,     1] loss: 1842.849
[7,     1] loss: 1846.439
[8,     1] loss: 1840.893
[9,     1] loss: 1830.004
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004040777924906898,
 'learning_rate_Hydroxylation-K': 0.0011584130194317568,
 'learning_rate_Hydroxylation-P': 0.003611777146233046,
 'log_base': 2.8058666081111325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2350894292,
 'sample_weights': [4.441691508808672, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.768260431595499,
 'weight_decay_Hydroxylation-K': 5.351186525027181,
 'weight_decay_Hydroxylation-P': 2.2181285710468988}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.499
[2,     1] loss: 1248.596
[3,     1] loss: 1253.869
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009383874374493143,
 'learning_rate_Hydroxylation-K': 0.003642861800151777,
 'learning_rate_Hydroxylation-P': 0.0098540437304173,
 'log_base': 1.0170383916673118,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2095825040,
 'sample_weights': [1.618128345382165, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.319785159065134,
 'weight_decay_Hydroxylation-K': 7.663919835251352,
 'weight_decay_Hydroxylation-P': 0.6310476108290035}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32006.193
Exploding loss, terminate run (best metric=0.5324130058288574)
Finished Training
Total time taken: 0.2260127067565918
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32126.594
Exploding loss, terminate run (best metric=0.5284178256988525)
Finished Training
Total time taken: 0.2080371379852295
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32139.027
Exploding loss, terminate run (best metric=0.5346052050590515)
Finished Training
Total time taken: 0.2239990234375
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32016.252
Exploding loss, terminate run (best metric=0.5277491211891174)
Finished Training
Total time taken: 0.22700095176696777
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31936.996
Exploding loss, terminate run (best metric=0.5277712941169739)
Finished Training
Total time taken: 0.2180004119873047
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31980.086
Exploding loss, terminate run (best metric=0.5316070914268494)
Finished Training
Total time taken: 0.21200299263000488
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32262.918
Exploding loss, terminate run (best metric=0.5278904438018799)
Finished Training
Total time taken: 0.22100019454956055
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32103.578
Exploding loss, terminate run (best metric=0.530550479888916)
Finished Training
Total time taken: 0.2330024242401123
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32003.754
Exploding loss, terminate run (best metric=0.5290871858596802)
Finished Training
Total time taken: 0.22801733016967773
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32331.852
Exploding loss, terminate run (best metric=0.5279725790023804)
Finished Training
Total time taken: 0.2030010223388672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31950.320
Exploding loss, terminate run (best metric=0.5342269539833069)
Finished Training
Total time taken: 0.21799707412719727
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32089.205
Exploding loss, terminate run (best metric=0.5307266712188721)
Finished Training
Total time taken: 0.2395188808441162
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31997.613
Exploding loss, terminate run (best metric=0.5302273631095886)
Finished Training
Total time taken: 0.21799850463867188
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32173.457
Exploding loss, terminate run (best metric=0.5332021117210388)
Finished Training
Total time taken: 0.22500061988830566
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32452.441
Exploding loss, terminate run (best metric=0.5322315692901611)
Finished Training
Total time taken: 0.22000432014465332
{'Hydroxylation-K Validation Accuracy': 0.5151891252955083, 'Hydroxylation-K Validation Sensitivity': 0.4888888888888889, 'Hydroxylation-K Validation Specificity': 0.5228070175438596, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.617504873294347, 'Hydroxylation-K AUC PR': 0.34612146829794443, 'Hydroxylation-K MCC': 0.011695906432748537, 'Hydroxylation-K F1': 0.17846195949644225, 'Validation Loss (Hydroxylation-K)': 0.5564912160237631, 'Hydroxylation-P Validation Accuracy': 0.5206019322200227, 'Hydroxylation-P Validation Sensitivity': 0.4780952380952381, 'Hydroxylation-P Validation Specificity': 0.5292682926829269, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5788214370823229, 'Hydroxylation-P AUC PR': 0.24377142616856634, 'Hydroxylation-P MCC': 0.010309948987183147, 'Hydroxylation-P F1': 0.15625052504713952, 'Validation Loss (Hydroxylation-P)': 0.530578593413035, 'Validation Loss (total)': 1.0870698054631551, 'TimeToTrain': 0.22137290636698406}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00013195011342191935,
 'learning_rate_Hydroxylation-K': 0.003918010708426538,
 'learning_rate_Hydroxylation-P': 0.009449116462824689,
 'log_base': 1.4104605085130553,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4194190047,
 'sample_weights': [98.88693468877365, 12.335183138732994],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.388208847714266,
 'weight_decay_Hydroxylation-K': 6.855573472807545,
 'weight_decay_Hydroxylation-P': 0.38454662792022054}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1928.978
[2,     1] loss: 1942.150
[3,     1] loss: 1944.108
[4,     1] loss: 1925.038
[5,     1] loss: 1926.465
[6,     1] loss: 1933.734
[7,     1] loss: 1925.806
[8,     1] loss: 1933.263
[9,     1] loss: 1932.058
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002818962794485481,
 'learning_rate_Hydroxylation-K': 0.0001800147525804333,
 'learning_rate_Hydroxylation-P': 0.009956910166820682,
 'log_base': 2.6777725509986374,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4145042928,
 'sample_weights': [4.854214168684793, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.538709698675305,
 'weight_decay_Hydroxylation-K': 7.78861228582963,
 'weight_decay_Hydroxylation-P': 0.01371805694766981}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.714
[2,     1] loss: 1267.526
[3,     1] loss: 1263.125
[4,     1] loss: 1264.881
[5,     1] loss: 1263.479
[6,     1] loss: 1256.864
[7,     1] loss: 1247.947
[8,     1] loss: 1223.516
[9,     1] loss: 1201.095
[10,     1] loss: 1154.340
[11,     1] loss: 1111.073
[12,     1] loss: 1120.152
[13,     1] loss: 1055.035
[14,     1] loss: 1074.899
[15,     1] loss: 1062.387
[16,     1] loss: 1118.239
[17,     1] loss: 1070.693
[18,     1] loss: 1083.112
[19,     1] loss: 988.126
[20,     1] loss: 1022.195
[21,     1] loss: 1018.491
[22,     1] loss: 987.631
[23,     1] loss: 961.000
[24,     1] loss: 1023.975
[25,     1] loss: 929.297
[26,     1] loss: 970.516
[27,     1] loss: 980.854
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0090420345674838,
 'learning_rate_Hydroxylation-K': 0.002830561641000781,
 'learning_rate_Hydroxylation-P': 7.212253682519886e-05,
 'log_base': 1.65295900000176,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2408860563,
 'sample_weights': [1.6948914144783624, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.636203982701019,
 'weight_decay_Hydroxylation-K': 5.7381536437735585,
 'weight_decay_Hydroxylation-P': 9.735056857015921}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1618.799
[2,     1] loss: 1607.140
[3,     1] loss: 1604.064
[4,     1] loss: 1610.961
[5,     1] loss: 1611.662
[6,     1] loss: 1610.119
[7,     1] loss: 1610.686
[8,     1] loss: 1606.406
[9,     1] loss: 1604.991
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007013695249148903,
 'learning_rate_Hydroxylation-K': 0.004256636910245677,
 'learning_rate_Hydroxylation-P': 0.0010344411496950184,
 'log_base': 1.6389762344160188,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3763887628,
 'sample_weights': [3.3218319092100757, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.511535541828591,
 'weight_decay_Hydroxylation-K': 2.0274288811559904,
 'weight_decay_Hydroxylation-P': 9.256681506972763}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1622.299
[2,     1] loss: 1625.968
[3,     1] loss: 1616.826
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00994288952224411,
 'learning_rate_Hydroxylation-K': 0.00028441160961877737,
 'learning_rate_Hydroxylation-P': 0.004994179164997611,
 'log_base': 2.9434498687674027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2629989998,
 'sample_weights': [3.378948462060076, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.8618536985990486,
 'weight_decay_Hydroxylation-K': 1.9843797819123588,
 'weight_decay_Hydroxylation-P': 4.698651832629471}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.449
[2,     1] loss: 1234.649
[3,     1] loss: 1237.037
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003441290997003494,
 'learning_rate_Hydroxylation-K': 0.000459449574646038,
 'learning_rate_Hydroxylation-P': 0.0038213877150292926,
 'log_base': 2.453611523426125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1995389620,
 'sample_weights': [1.546378742329913, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.050803127728965,
 'weight_decay_Hydroxylation-K': 9.172747163760047,
 'weight_decay_Hydroxylation-P': 4.735467231278278}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.906
[2,     1] loss: 1299.255
[3,     1] loss: 1301.283
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031816978508334725,
 'learning_rate_Hydroxylation-K': 0.0012324813133267113,
 'learning_rate_Hydroxylation-P': 0.002606459861508016,
 'log_base': 2.6764483443490237,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1075047929,
 'sample_weights': [1.8599773062583171, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.752571078981537,
 'weight_decay_Hydroxylation-K': 0.33439700476422307,
 'weight_decay_Hydroxylation-P': 5.680200641698045}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.618
[2,     1] loss: 1264.614
[3,     1] loss: 1266.413
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004271705799725741,
 'learning_rate_Hydroxylation-K': 0.006892253007228387,
 'learning_rate_Hydroxylation-P': 0.003805861076299406,
 'log_base': 2.9858028640414984,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 767736682,
 'sample_weights': [1.6957429833130575, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2280774653416087,
 'weight_decay_Hydroxylation-K': 8.478794615086269,
 'weight_decay_Hydroxylation-P': 4.122260547117164}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.911
[2,     1] loss: 1230.204
[3,     1] loss: 1232.954
[4,     1] loss: 1228.929
[5,     1] loss: 1227.938
[6,     1] loss: 1227.714
[7,     1] loss: 1225.768
[8,     1] loss: 1221.694
[9,     1] loss: 1212.664
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0046602339032392566,
 'learning_rate_Hydroxylation-K': 0.004346472784699179,
 'learning_rate_Hydroxylation-P': 0.009103186677261912,
 'log_base': 2.731272559472147,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 635700688,
 'sample_weights': [1.5261824227560747, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.80708809354933,
 'weight_decay_Hydroxylation-K': 6.568535099210773,
 'weight_decay_Hydroxylation-P': 0.04261569520897239}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.465
[2,     1] loss: 1256.157
[3,     1] loss: 1264.272
[4,     1] loss: 1257.548
[5,     1] loss: 1261.390
[6,     1] loss: 1258.051
[7,     1] loss: 1254.885
[8,     1] loss: 1251.571
[9,     1] loss: 1242.398
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005679204148586592,
 'learning_rate_Hydroxylation-K': 0.001184653084982155,
 'learning_rate_Hydroxylation-P': 0.0028681845486061737,
 'log_base': 2.781722504041764,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1442630064,
 'sample_weights': [1.6615216112127058, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.241653398681048,
 'weight_decay_Hydroxylation-K': 1.6462087535745311,
 'weight_decay_Hydroxylation-P': 4.894583641093217}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.935
[2,     1] loss: 1252.861
[3,     1] loss: 1249.888
[4,     1] loss: 1267.954
[5,     1] loss: 1253.029
[6,     1] loss: 1249.441
[7,     1] loss: 1250.501
[8,     1] loss: 1243.298
[9,     1] loss: 1242.819
[10,     1] loss: 1236.468
[11,     1] loss: 1223.562
[12,     1] loss: 1205.307
[13,     1] loss: 1182.167
[14,     1] loss: 1164.884
[15,     1] loss: 1143.738
[16,     1] loss: 1101.621
[17,     1] loss: 1061.822
[18,     1] loss: 1113.143
[19,     1] loss: 1043.526
[20,     1] loss: 1059.401
[21,     1] loss: 1053.615
[22,     1] loss: 1078.972
[23,     1] loss: 1089.752
[24,     1] loss: 978.866
[25,     1] loss: 1040.814
[26,     1] loss: 1015.763
[27,     1] loss: 992.664
[28,     1] loss: 1015.194
[29,     1] loss: 1051.103
[30,     1] loss: 987.000
[31,     1] loss: 1027.895
[32,     1] loss: 991.036
[33,     1] loss: 1007.835
[34,     1] loss: 939.278
[35,     1] loss: 976.616
[36,     1] loss: 959.367
[37,     1] loss: 968.275
[38,     1] loss: 927.376
[39,     1] loss: 955.662
[40,     1] loss: 914.174
[41,     1] loss: 923.324
[42,     1] loss: 896.309
[43,     1] loss: 910.159
[44,     1] loss: 872.626
[45,     1] loss: 877.026
[46,     1] loss: 850.170
[47,     1] loss: 879.330
[48,     1] loss: 840.554
[49,     1] loss: 848.943
[50,     1] loss: 901.384
[51,     1] loss: 843.949
[52,     1] loss: 881.116
[53,     1] loss: 837.279
[54,     1] loss: 820.342
[55,     1] loss: 820.806
[56,     1] loss: 806.105
[57,     1] loss: 857.310
[58,     1] loss: 772.043
[59,     1] loss: 781.284
[60,     1] loss: 796.471
[61,     1] loss: 829.748
[62,     1] loss: 791.666
[63,     1] loss: 809.832
[64,     1] loss: 781.133
[65,     1] loss: 707.471
[66,     1] loss: 728.067
[67,     1] loss: 720.724
[68,     1] loss: 673.121
[69,     1] loss: 687.023
[70,     1] loss: 718.754
[71,     1] loss: 695.067
[72,     1] loss: 679.025
[73,     1] loss: 661.381
[74,     1] loss: 657.531
[75,     1] loss: 610.889
[76,     1] loss: 606.720
[77,     1] loss: 662.014
[78,     1] loss: 704.317
[79,     1] loss: 642.921
[80,     1] loss: 602.070
[81,     1] loss: 663.069
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008177695336944764,
 'learning_rate_Hydroxylation-K': 0.004332292766641077,
 'learning_rate_Hydroxylation-P': 0.00663545619858013,
 'log_base': 2.8253958131365753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3021333343,
 'sample_weights': [1.6317970324683686, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.299155059158166,
 'weight_decay_Hydroxylation-K': 8.413301771828204,
 'weight_decay_Hydroxylation-P': 3.69200841809929}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.602
[2,     1] loss: 1247.925
[3,     1] loss: 1247.004
[4,     1] loss: 1246.465
[5,     1] loss: 1243.115
[6,     1] loss: 1242.409
[7,     1] loss: 1250.333
[8,     1] loss: 1243.415
[9,     1] loss: 1240.283
[10,     1] loss: 1241.088
[11,     1] loss: 1236.039
[12,     1] loss: 1225.655
[13,     1] loss: 1222.643
[14,     1] loss: 1184.146
[15,     1] loss: 1144.530
[16,     1] loss: 1098.751
[17,     1] loss: 1085.631
[18,     1] loss: 1083.106
[19,     1] loss: 1155.736
[20,     1] loss: 1004.992
[21,     1] loss: 1037.765
[22,     1] loss: 1024.070
[23,     1] loss: 1004.969
[24,     1] loss: 991.697
[25,     1] loss: 1003.983
[26,     1] loss: 983.249
[27,     1] loss: 991.390
[28,     1] loss: 971.877
[29,     1] loss: 922.702
[30,     1] loss: 961.674
[31,     1] loss: 884.359
[32,     1] loss: 890.579
[33,     1] loss: 889.822
[34,     1] loss: 874.178
[35,     1] loss: 907.975
[36,     1] loss: 876.159
[37,     1] loss: 922.007
[38,     1] loss: 966.981
[39,     1] loss: 900.714
[40,     1] loss: 822.629
[41,     1] loss: 846.044
[42,     1] loss: 821.203
[43,     1] loss: 819.321
[44,     1] loss: 762.020
[45,     1] loss: 843.341
[46,     1] loss: 939.825
[47,     1] loss: 961.978
[48,     1] loss: 799.155
[49,     1] loss: 870.653
[50,     1] loss: 802.668
[51,     1] loss: 799.284
[52,     1] loss: 788.207
[53,     1] loss: 788.697
[54,     1] loss: 760.227
[55,     1] loss: 769.470
[56,     1] loss: 730.691
[57,     1] loss: 753.467
[58,     1] loss: 746.404
[59,     1] loss: 742.517
[60,     1] loss: 696.396
[61,     1] loss: 686.070
[62,     1] loss: 765.997
[63,     1] loss: 656.567
[64,     1] loss: 616.796
[65,     1] loss: 709.991
[66,     1] loss: 635.443
[67,     1] loss: 578.549
[68,     1] loss: 636.389
[69,     1] loss: 1011.795
[70,     1] loss: 1094.083
[71,     1] loss: 628.786
[72,     1] loss: 807.485
[73,     1] loss: 816.729
[74,     1] loss: 731.552
[75,     1] loss: 839.081
[76,     1] loss: 784.715
[77,     1] loss: 750.338
[78,     1] loss: 844.251
[79,     1] loss: 745.478
[80,     1] loss: 670.471
[81,     1] loss: 707.470
[82,     1] loss: 658.311
[83,     1] loss: 678.085
[84,     1] loss: 560.658
[85,     1] loss: 647.129
[86,     1] loss: 523.587
[87,     1] loss: 512.577
[88,     1] loss: 601.461
[89,     1] loss: 724.692
[90,     1] loss: 497.940
[91,     1] loss: 507.914
[92,     1] loss: 557.679
[93,     1] loss: 465.236
[94,     1] loss: 468.303
[95,     1] loss: 478.919
[96,     1] loss: 543.513
[97,     1] loss: 910.385
[98,     1] loss: 698.252
[99,     1] loss: 527.779
[100,     1] loss: 642.938
[101,     1] loss: 495.396
[102,     1] loss: 638.721
[103,     1] loss: 665.586
[104,     1] loss: 513.686
[105,     1] loss: 728.461
[106,     1] loss: 580.755
[107,     1] loss: 633.507
[108,     1] loss: 580.842
[109,     1] loss: 590.932
[110,     1] loss: 547.070
[111,     1] loss: 527.882
[112,     1] loss: 574.031
[113,     1] loss: 393.995
[114,     1] loss: 450.468
[115,     1] loss: 479.138
[116,     1] loss: 369.227
Early stopping applied (best metric=0.36401641368865967)
Finished Training
Total time taken: 16.969183921813965
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.136
[2,     1] loss: 1253.574
[3,     1] loss: 1249.956
[4,     1] loss: 1247.384
[5,     1] loss: 1245.437
[6,     1] loss: 1245.788
[7,     1] loss: 1244.533
[8,     1] loss: 1248.303
[9,     1] loss: 1246.528
[10,     1] loss: 1245.751
[11,     1] loss: 1243.939
[12,     1] loss: 1240.471
[13,     1] loss: 1238.700
[14,     1] loss: 1230.860
[15,     1] loss: 1221.111
[16,     1] loss: 1197.159
[17,     1] loss: 1168.590
[18,     1] loss: 1135.579
[19,     1] loss: 1086.486
[20,     1] loss: 1056.036
[21,     1] loss: 1064.574
[22,     1] loss: 1074.085
[23,     1] loss: 1064.458
[24,     1] loss: 1057.096
[25,     1] loss: 1006.945
[26,     1] loss: 1042.775
[27,     1] loss: 1017.963
[28,     1] loss: 992.643
[29,     1] loss: 973.091
[30,     1] loss: 976.535
[31,     1] loss: 983.651
[32,     1] loss: 937.877
[33,     1] loss: 906.887
[34,     1] loss: 935.275
[35,     1] loss: 913.231
[36,     1] loss: 920.502
[37,     1] loss: 885.351
[38,     1] loss: 882.069
[39,     1] loss: 980.815
[40,     1] loss: 1048.949
[41,     1] loss: 849.783
[42,     1] loss: 1004.571
[43,     1] loss: 894.686
[44,     1] loss: 884.711
[45,     1] loss: 941.456
[46,     1] loss: 901.282
[47,     1] loss: 843.503
[48,     1] loss: 843.638
[49,     1] loss: 804.671
[50,     1] loss: 833.918
[51,     1] loss: 827.318
[52,     1] loss: 813.760
[53,     1] loss: 788.760
[54,     1] loss: 757.407
[55,     1] loss: 755.235
[56,     1] loss: 814.117
[57,     1] loss: 966.389
[58,     1] loss: 749.105
[59,     1] loss: 845.267
[60,     1] loss: 783.697
[61,     1] loss: 799.181
[62,     1] loss: 743.119
[63,     1] loss: 773.131
[64,     1] loss: 771.387
[65,     1] loss: 688.505
[66,     1] loss: 703.696
[67,     1] loss: 675.098
[68,     1] loss: 669.584
[69,     1] loss: 685.968
[70,     1] loss: 585.169
[71,     1] loss: 597.280
[72,     1] loss: 575.392
[73,     1] loss: 578.482
[74,     1] loss: 1348.957
[75,     1] loss: 1211.663
[76,     1] loss: 664.892
[77,     1] loss: 908.732
[78,     1] loss: 951.126
[79,     1] loss: 872.544
[80,     1] loss: 862.559
[81,     1] loss: 885.434
[82,     1] loss: 858.593
[83,     1] loss: 812.003
[84,     1] loss: 872.324
[85,     1] loss: 800.733
[86,     1] loss: 754.195
[87,     1] loss: 828.607
[88,     1] loss: 704.790
[89,     1] loss: 731.196
[90,     1] loss: 688.798
[91,     1] loss: 667.307
[92,     1] loss: 610.445
[93,     1] loss: 601.932
[94,     1] loss: 587.624
[95,     1] loss: 535.939
[96,     1] loss: 518.314
[97,     1] loss: 570.440
[98,     1] loss: 529.989
[99,     1] loss: 795.086
[100,     1] loss: 1863.512
[101,     1] loss: 1234.132
[102,     1] loss: 1063.792
[103,     1] loss: 943.759
[104,     1] loss: 1021.901
[105,     1] loss: 1110.986
[106,     1] loss: 1069.133
[107,     1] loss: 1055.009
[108,     1] loss: 1020.188
[109,     1] loss: 1021.740
[110,     1] loss: 1003.486
[111,     1] loss: 1012.921
Early stopping applied (best metric=0.3832744061946869)
Finished Training
Total time taken: 18.45101547241211
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.747
[2,     1] loss: 1251.817
[3,     1] loss: 1253.861
[4,     1] loss: 1248.758
[5,     1] loss: 1246.753
[6,     1] loss: 1250.170
[7,     1] loss: 1249.367
[8,     1] loss: 1249.433
[9,     1] loss: 1247.450
[10,     1] loss: 1245.196
[11,     1] loss: 1247.849
[12,     1] loss: 1247.614
[13,     1] loss: 1245.809
[14,     1] loss: 1245.294
[15,     1] loss: 1246.532
[16,     1] loss: 1244.584
[17,     1] loss: 1245.024
[18,     1] loss: 1243.509
[19,     1] loss: 1238.856
[20,     1] loss: 1239.575
[21,     1] loss: 1223.485
[22,     1] loss: 1219.817
[23,     1] loss: 1190.658
[24,     1] loss: 1159.711
[25,     1] loss: 1120.993
[26,     1] loss: 1117.648
[27,     1] loss: 1098.602
[28,     1] loss: 1070.980
[29,     1] loss: 1099.018
[30,     1] loss: 1053.161
[31,     1] loss: 1050.991
[32,     1] loss: 1047.412
[33,     1] loss: 1028.554
[34,     1] loss: 1012.587
[35,     1] loss: 1023.828
[36,     1] loss: 1005.639
[37,     1] loss: 1033.636
[38,     1] loss: 961.714
[39,     1] loss: 1006.535
[40,     1] loss: 977.727
[41,     1] loss: 942.803
[42,     1] loss: 948.738
[43,     1] loss: 949.491
[44,     1] loss: 946.842
[45,     1] loss: 940.294
[46,     1] loss: 926.221
[47,     1] loss: 910.861
[48,     1] loss: 881.421
[49,     1] loss: 981.725
[50,     1] loss: 1045.245
[51,     1] loss: 912.164
[52,     1] loss: 956.539
[53,     1] loss: 915.749
[54,     1] loss: 931.884
[55,     1] loss: 894.677
[56,     1] loss: 929.804
[57,     1] loss: 862.742
[58,     1] loss: 920.828
[59,     1] loss: 796.596
[60,     1] loss: 876.670
[61,     1] loss: 901.620
[62,     1] loss: 784.772
[63,     1] loss: 924.590
[64,     1] loss: 830.086
[65,     1] loss: 772.669
[66,     1] loss: 836.849
[67,     1] loss: 730.427
[68,     1] loss: 806.314
[69,     1] loss: 830.080
[70,     1] loss: 722.151
[71,     1] loss: 768.780
[72,     1] loss: 756.310
[73,     1] loss: 698.615
[74,     1] loss: 654.225
[75,     1] loss: 799.401
[76,     1] loss: 952.189
[77,     1] loss: 638.910
[78,     1] loss: 827.492
[79,     1] loss: 709.259
[80,     1] loss: 797.857
[81,     1] loss: 656.752
[82,     1] loss: 875.754
[83,     1] loss: 651.154
[84,     1] loss: 854.365
[85,     1] loss: 608.502
[86,     1] loss: 788.434
[87,     1] loss: 614.135
[88,     1] loss: 772.840
[89,     1] loss: 613.450
[90,     1] loss: 775.441
[91,     1] loss: 597.219
[92,     1] loss: 665.359
[93,     1] loss: 620.840
[94,     1] loss: 574.689
[95,     1] loss: 646.850
[96,     1] loss: 561.642
[97,     1] loss: 725.423
[98,     1] loss: 680.512
[99,     1] loss: 588.426
[100,     1] loss: 681.887
[101,     1] loss: 535.416
[102,     1] loss: 661.681
[103,     1] loss: 501.913
[104,     1] loss: 547.067
[105,     1] loss: 489.165
[106,     1] loss: 523.357
[107,     1] loss: 523.614
[108,     1] loss: 669.577
[109,     1] loss: 622.400
[110,     1] loss: 482.363
[111,     1] loss: 562.301
[112,     1] loss: 559.296
[113,     1] loss: 486.185
[114,     1] loss: 453.563
[115,     1] loss: 783.432
[116,     1] loss: 1050.136
[117,     1] loss: 574.618
[118,     1] loss: 712.211
[119,     1] loss: 679.711
Early stopping applied (best metric=0.39154982566833496)
Finished Training
Total time taken: 19.836370706558228
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.153
[2,     1] loss: 1245.929
[3,     1] loss: 1253.424
[4,     1] loss: 1243.318
[5,     1] loss: 1241.965
[6,     1] loss: 1231.654
[7,     1] loss: 1208.540
[8,     1] loss: 1180.992
[9,     1] loss: 1135.651
[10,     1] loss: 1109.424
[11,     1] loss: 1091.633
[12,     1] loss: 1057.639
[13,     1] loss: 1043.358
[14,     1] loss: 1049.722
[15,     1] loss: 1012.329
[16,     1] loss: 984.704
[17,     1] loss: 994.654
[18,     1] loss: 983.294
[19,     1] loss: 911.083
[20,     1] loss: 958.224
[21,     1] loss: 994.899
[22,     1] loss: 928.121
[23,     1] loss: 902.302
[24,     1] loss: 944.953
[25,     1] loss: 909.762
[26,     1] loss: 938.203
[27,     1] loss: 913.357
[28,     1] loss: 945.650
[29,     1] loss: 917.807
[30,     1] loss: 907.815
[31,     1] loss: 878.615
[32,     1] loss: 850.726
[33,     1] loss: 862.288
[34,     1] loss: 877.442
[35,     1] loss: 968.517
[36,     1] loss: 956.407
[37,     1] loss: 806.657
[38,     1] loss: 864.367
[39,     1] loss: 829.957
[40,     1] loss: 834.538
[41,     1] loss: 869.309
[42,     1] loss: 811.203
[43,     1] loss: 847.278
[44,     1] loss: 774.349
[45,     1] loss: 749.297
[46,     1] loss: 794.252
[47,     1] loss: 925.682
[48,     1] loss: 1035.968
[49,     1] loss: 748.443
[50,     1] loss: 889.838
[51,     1] loss: 805.838
[52,     1] loss: 831.323
[53,     1] loss: 835.868
[54,     1] loss: 782.404
[55,     1] loss: 790.171
[56,     1] loss: 727.380
[57,     1] loss: 713.245
[58,     1] loss: 718.805
[59,     1] loss: 759.535
[60,     1] loss: 698.001
[61,     1] loss: 751.500
[62,     1] loss: 711.456
[63,     1] loss: 655.530
[64,     1] loss: 805.473
[65,     1] loss: 638.320
[66,     1] loss: 657.515
[67,     1] loss: 711.445
[68,     1] loss: 590.985
[69,     1] loss: 735.416
[70,     1] loss: 545.871
[71,     1] loss: 713.025
[72,     1] loss: 660.602
[73,     1] loss: 555.914
[74,     1] loss: 743.331
[75,     1] loss: 557.314
[76,     1] loss: 583.692
[77,     1] loss: 518.686
[78,     1] loss: 450.842
[79,     1] loss: 563.186
[80,     1] loss: 662.000
[81,     1] loss: 1081.762
[82,     1] loss: 520.651
[83,     1] loss: 717.113
[84,     1] loss: 691.055
[85,     1] loss: 734.997
[86,     1] loss: 614.886
[87,     1] loss: 766.408
[88,     1] loss: 617.191
[89,     1] loss: 652.120
[90,     1] loss: 515.541
[91,     1] loss: 660.240
[92,     1] loss: 576.729
[93,     1] loss: 574.014
[94,     1] loss: 631.207
[95,     1] loss: 459.302
[96,     1] loss: 508.671
[97,     1] loss: 439.252
[98,     1] loss: 468.085
[99,     1] loss: 437.256
[100,     1] loss: 380.534
[101,     1] loss: 444.600
[102,     1] loss: 788.964
[103,     1] loss: 1138.709
[104,     1] loss: 1093.446
[105,     1] loss: 1041.009
[106,     1] loss: 1101.974
[107,     1] loss: 1164.041
[108,     1] loss: 1163.246
Early stopping applied (best metric=0.3610260784626007)
Finished Training
Total time taken: 18.51020097732544
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1248.671
[2,     1] loss: 1250.452
[3,     1] loss: 1240.903
[4,     1] loss: 1241.640
[5,     1] loss: 1237.167
[6,     1] loss: 1211.617
[7,     1] loss: 1200.796
[8,     1] loss: 1212.453
[9,     1] loss: 1101.514
[10,     1] loss: 1134.229
[11,     1] loss: 1029.648
[12,     1] loss: 1089.457
[13,     1] loss: 1062.269
[14,     1] loss: 1013.653
[15,     1] loss: 1063.551
[16,     1] loss: 968.528
[17,     1] loss: 976.115
[18,     1] loss: 1072.440
[19,     1] loss: 976.853
[20,     1] loss: 989.904
[21,     1] loss: 949.777
[22,     1] loss: 949.447
[23,     1] loss: 965.992
[24,     1] loss: 891.891
[25,     1] loss: 963.335
[26,     1] loss: 877.393
[27,     1] loss: 904.113
[28,     1] loss: 881.222
[29,     1] loss: 896.341
[30,     1] loss: 825.645
[31,     1] loss: 887.410
[32,     1] loss: 1009.661
[33,     1] loss: 813.465
[34,     1] loss: 891.756
[35,     1] loss: 830.445
[36,     1] loss: 884.283
[37,     1] loss: 800.029
[38,     1] loss: 874.292
[39,     1] loss: 775.950
[40,     1] loss: 830.265
[41,     1] loss: 827.551
[42,     1] loss: 851.829
[43,     1] loss: 740.920
[44,     1] loss: 792.950
[45,     1] loss: 835.217
[46,     1] loss: 706.254
[47,     1] loss: 746.933
[48,     1] loss: 766.317
[49,     1] loss: 818.290
[50,     1] loss: 897.220
[51,     1] loss: 722.191
[52,     1] loss: 811.314
[53,     1] loss: 742.573
[54,     1] loss: 813.563
[55,     1] loss: 685.733
[56,     1] loss: 749.025
[57,     1] loss: 642.105
[58,     1] loss: 720.665
[59,     1] loss: 656.291
[60,     1] loss: 587.780
[61,     1] loss: 593.456
[62,     1] loss: 524.573
[63,     1] loss: 590.587
[64,     1] loss: 1011.044
[65,     1] loss: 897.020
[66,     1] loss: 595.371
[67,     1] loss: 743.197
[68,     1] loss: 752.041
[69,     1] loss: 668.902
[70,     1] loss: 723.564
[71,     1] loss: 546.349
[72,     1] loss: 667.966
[73,     1] loss: 606.308
[74,     1] loss: 653.517
[75,     1] loss: 548.484
[76,     1] loss: 599.576
[77,     1] loss: 510.663
[78,     1] loss: 516.301
[79,     1] loss: 493.844
[80,     1] loss: 485.801
Early stopping applied (best metric=0.3770492672920227)
Finished Training
Total time taken: 13.811044692993164
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.631
[2,     1] loss: 1255.701
[3,     1] loss: 1252.971
[4,     1] loss: 1249.898
[5,     1] loss: 1247.595
[6,     1] loss: 1244.697
[7,     1] loss: 1252.558
[8,     1] loss: 1249.122
[9,     1] loss: 1244.547
[10,     1] loss: 1247.142
[11,     1] loss: 1244.495
[12,     1] loss: 1244.833
[13,     1] loss: 1245.116
[14,     1] loss: 1247.696
[15,     1] loss: 1244.712
[16,     1] loss: 1243.680
[17,     1] loss: 1238.981
[18,     1] loss: 1239.463
[19,     1] loss: 1231.174
[20,     1] loss: 1211.730
[21,     1] loss: 1197.681
[22,     1] loss: 1156.781
[23,     1] loss: 1119.812
[24,     1] loss: 1058.928
[25,     1] loss: 1119.089
[26,     1] loss: 993.024
[27,     1] loss: 1069.840
[28,     1] loss: 1047.456
[29,     1] loss: 1006.928
[30,     1] loss: 1012.375
[31,     1] loss: 1003.798
[32,     1] loss: 974.058
[33,     1] loss: 953.968
[34,     1] loss: 997.938
[35,     1] loss: 958.873
[36,     1] loss: 954.769
[37,     1] loss: 951.938
[38,     1] loss: 909.078
[39,     1] loss: 890.844
[40,     1] loss: 946.002
[41,     1] loss: 905.395
[42,     1] loss: 883.598
[43,     1] loss: 872.088
[44,     1] loss: 846.490
[45,     1] loss: 841.387
[46,     1] loss: 878.236
[47,     1] loss: 903.044
[48,     1] loss: 842.214
[49,     1] loss: 817.636
[50,     1] loss: 846.997
[51,     1] loss: 926.626
[52,     1] loss: 815.384
[53,     1] loss: 800.416
[54,     1] loss: 818.521
[55,     1] loss: 766.108
[56,     1] loss: 780.712
[57,     1] loss: 723.063
[58,     1] loss: 761.395
[59,     1] loss: 750.605
[60,     1] loss: 890.760
[61,     1] loss: 1181.061
[62,     1] loss: 695.397
[63,     1] loss: 867.760
[64,     1] loss: 754.829
[65,     1] loss: 804.845
[66,     1] loss: 870.951
[67,     1] loss: 758.094
[68,     1] loss: 815.703
[69,     1] loss: 735.696
[70,     1] loss: 674.343
[71,     1] loss: 712.649
[72,     1] loss: 680.848
[73,     1] loss: 685.773
[74,     1] loss: 730.943
[75,     1] loss: 573.231
[76,     1] loss: 593.972
[77,     1] loss: 725.072
[78,     1] loss: 598.720
[79,     1] loss: 565.393
[80,     1] loss: 631.817
[81,     1] loss: 552.849
[82,     1] loss: 533.589
[83,     1] loss: 769.109
[84,     1] loss: 770.269
[85,     1] loss: 683.562
[86,     1] loss: 574.677
[87,     1] loss: 684.025
[88,     1] loss: 582.077
[89,     1] loss: 634.929
[90,     1] loss: 590.784
[91,     1] loss: 685.520
[92,     1] loss: 524.332
[93,     1] loss: 558.342
[94,     1] loss: 555.616
[95,     1] loss: 483.206
[96,     1] loss: 593.158
[97,     1] loss: 528.713
[98,     1] loss: 437.863
[99,     1] loss: 455.589
[100,     1] loss: 530.175
[101,     1] loss: 607.067
[102,     1] loss: 586.295
[103,     1] loss: 530.720
[104,     1] loss: 429.000
Early stopping applied (best metric=0.3730129897594452)
Finished Training
Total time taken: 16.610646724700928
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.482
[2,     1] loss: 1251.491
[3,     1] loss: 1248.686
[4,     1] loss: 1245.035
[5,     1] loss: 1247.474
[6,     1] loss: 1247.569
[7,     1] loss: 1246.148
[8,     1] loss: 1245.975
[9,     1] loss: 1246.064
[10,     1] loss: 1244.363
[11,     1] loss: 1243.408
[12,     1] loss: 1241.541
[13,     1] loss: 1238.304
[14,     1] loss: 1235.215
[15,     1] loss: 1221.306
[16,     1] loss: 1199.465
[17,     1] loss: 1176.036
[18,     1] loss: 1143.805
[19,     1] loss: 1117.676
[20,     1] loss: 1090.872
[21,     1] loss: 1083.340
[22,     1] loss: 1084.120
[23,     1] loss: 1078.112
[24,     1] loss: 1008.184
[25,     1] loss: 1065.036
[26,     1] loss: 1052.357
[27,     1] loss: 1062.970
[28,     1] loss: 1077.359
[29,     1] loss: 1034.711
[30,     1] loss: 1052.895
[31,     1] loss: 1013.224
[32,     1] loss: 983.256
[33,     1] loss: 967.346
[34,     1] loss: 953.176
[35,     1] loss: 925.556
[36,     1] loss: 923.227
[37,     1] loss: 959.696
[38,     1] loss: 947.899
[39,     1] loss: 943.612
[40,     1] loss: 962.692
[41,     1] loss: 926.943
[42,     1] loss: 967.320
[43,     1] loss: 938.651
[44,     1] loss: 891.390
[45,     1] loss: 895.156
[46,     1] loss: 876.530
[47,     1] loss: 864.449
[48,     1] loss: 867.141
[49,     1] loss: 835.429
[50,     1] loss: 778.784
[51,     1] loss: 883.256
[52,     1] loss: 978.268
[53,     1] loss: 815.399
[54,     1] loss: 814.274
[55,     1] loss: 842.193
[56,     1] loss: 821.511
[57,     1] loss: 741.476
[58,     1] loss: 764.408
[59,     1] loss: 787.026
[60,     1] loss: 711.254
[61,     1] loss: 815.262
[62,     1] loss: 1152.896
[63,     1] loss: 717.635
[64,     1] loss: 926.031
[65,     1] loss: 791.807
[66,     1] loss: 850.302
[67,     1] loss: 885.175
[68,     1] loss: 770.700
[69,     1] loss: 816.088
[70,     1] loss: 785.031
[71,     1] loss: 759.909
[72,     1] loss: 773.708
[73,     1] loss: 714.006
[74,     1] loss: 734.449
[75,     1] loss: 645.175
[76,     1] loss: 725.085
[77,     1] loss: 644.733
[78,     1] loss: 600.019
[79,     1] loss: 674.331
[80,     1] loss: 737.829
[81,     1] loss: 846.007
[82,     1] loss: 581.069
[83,     1] loss: 817.503
[84,     1] loss: 672.348
[85,     1] loss: 787.201
[86,     1] loss: 624.971
[87,     1] loss: 752.729
[88,     1] loss: 599.390
[89,     1] loss: 758.227
[90,     1] loss: 594.826
[91,     1] loss: 749.338
[92,     1] loss: 595.235
[93,     1] loss: 626.819
[94,     1] loss: 531.854
[95,     1] loss: 582.175
[96,     1] loss: 574.793
[97,     1] loss: 485.624
[98,     1] loss: 592.074
[99,     1] loss: 587.859
[100,     1] loss: 448.825
[101,     1] loss: 509.332
[102,     1] loss: 714.985
[103,     1] loss: 637.658
[104,     1] loss: 513.629
[105,     1] loss: 524.017
[106,     1] loss: 609.831
[107,     1] loss: 604.041
[108,     1] loss: 458.104
[109,     1] loss: 561.072
[110,     1] loss: 707.658
[111,     1] loss: 524.137
[112,     1] loss: 640.625
[113,     1] loss: 537.786
[114,     1] loss: 535.492
[115,     1] loss: 621.537
[116,     1] loss: 681.139
[117,     1] loss: 456.566
[118,     1] loss: 720.849
[119,     1] loss: 596.006
[120,     1] loss: 618.215
[121,     1] loss: 557.508
[122,     1] loss: 479.750
[123,     1] loss: 510.214
[124,     1] loss: 461.064
[125,     1] loss: 527.947
[126,     1] loss: 674.786
[127,     1] loss: 436.886
[128,     1] loss: 609.299
[129,     1] loss: 648.175
[130,     1] loss: 459.708
[131,     1] loss: 678.822
[132,     1] loss: 499.820
[133,     1] loss: 506.611
[134,     1] loss: 455.884
[135,     1] loss: 476.747
[136,     1] loss: 514.548
[137,     1] loss: 371.267
[138,     1] loss: 517.801
[139,     1] loss: 582.075
[140,     1] loss: 403.233
[141,     1] loss: 550.584
[142,     1] loss: 391.724
[143,     1] loss: 382.541
[144,     1] loss: 503.882
[145,     1] loss: 478.655
[146,     1] loss: 409.970
[147,     1] loss: 371.376
[148,     1] loss: 463.785
[149,     1] loss: 517.903
[150,     1] loss: 743.260
[151,     1] loss: 574.983
[152,     1] loss: 450.367
[153,     1] loss: 549.521
[154,     1] loss: 410.291
[155,     1] loss: 633.460
[156,     1] loss: 563.732
[157,     1] loss: 429.428
[158,     1] loss: 575.089
[159,     1] loss: 505.868
[160,     1] loss: 377.394
[161,     1] loss: 648.176
[162,     1] loss: 708.144
[163,     1] loss: 408.637
[164,     1] loss: 648.686
[165,     1] loss: 472.536
[166,     1] loss: 624.403
[167,     1] loss: 468.670
[168,     1] loss: 416.088
[169,     1] loss: 475.197
[170,     1] loss: 356.813
[171,     1] loss: 411.625
[172,     1] loss: 540.736
[173,     1] loss: 352.103
[174,     1] loss: 414.629
[175,     1] loss: 495.226
[176,     1] loss: 324.259
[177,     1] loss: 390.080
[178,     1] loss: 620.942
[179,     1] loss: 657.364
[180,     1] loss: 615.311
[181,     1] loss: 504.094
[182,     1] loss: 528.462
[183,     1] loss: 462.498
[184,     1] loss: 521.770
[185,     1] loss: 470.888
[186,     1] loss: 409.037
[187,     1] loss: 448.090
[188,     1] loss: 533.631
[189,     1] loss: 595.213
[190,     1] loss: 378.881
[191,     1] loss: 560.523
[192,     1] loss: 628.791
[193,     1] loss: 395.551
[194,     1] loss: 564.132
[195,     1] loss: 463.111
[196,     1] loss: 369.860
[197,     1] loss: 462.154
[198,     1] loss: 532.595
[199,     1] loss: 337.619
[200,     1] loss: 449.080
Finished Training
Total time taken: 30.761385679244995
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.245
[2,     1] loss: 1257.639
[3,     1] loss: 1253.320
[4,     1] loss: 1246.466
[5,     1] loss: 1248.857
[6,     1] loss: 1247.399
[7,     1] loss: 1245.267
[8,     1] loss: 1248.667
[9,     1] loss: 1248.820
[10,     1] loss: 1245.392
[11,     1] loss: 1245.269
[12,     1] loss: 1246.528
[13,     1] loss: 1244.965
[14,     1] loss: 1243.292
[15,     1] loss: 1239.802
[16,     1] loss: 1240.073
[17,     1] loss: 1235.122
[18,     1] loss: 1219.955
[19,     1] loss: 1207.389
[20,     1] loss: 1172.332
[21,     1] loss: 1169.559
[22,     1] loss: 1122.112
[23,     1] loss: 1081.630
[24,     1] loss: 1050.461
[25,     1] loss: 1023.770
[26,     1] loss: 1062.642
[27,     1] loss: 1005.748
[28,     1] loss: 973.393
[29,     1] loss: 1043.461
[30,     1] loss: 1035.582
[31,     1] loss: 958.730
[32,     1] loss: 984.669
[33,     1] loss: 951.371
[34,     1] loss: 997.354
[35,     1] loss: 958.489
[36,     1] loss: 942.293
[37,     1] loss: 933.130
[38,     1] loss: 863.094
[39,     1] loss: 858.823
[40,     1] loss: 936.304
[41,     1] loss: 1001.596
[42,     1] loss: 903.139
[43,     1] loss: 834.930
[44,     1] loss: 894.435
[45,     1] loss: 887.929
[46,     1] loss: 827.012
[47,     1] loss: 879.989
[48,     1] loss: 821.755
[49,     1] loss: 819.918
[50,     1] loss: 819.227
[51,     1] loss: 735.769
[52,     1] loss: 811.540
[53,     1] loss: 885.281
[54,     1] loss: 931.250
[55,     1] loss: 739.893
[56,     1] loss: 800.755
[57,     1] loss: 722.229
[58,     1] loss: 784.232
[59,     1] loss: 751.907
[60,     1] loss: 724.525
[61,     1] loss: 695.701
[62,     1] loss: 695.918
[63,     1] loss: 654.064
[64,     1] loss: 658.378
[65,     1] loss: 694.729
[66,     1] loss: 1038.777
[67,     1] loss: 973.891
[68,     1] loss: 699.827
[69,     1] loss: 836.914
[70,     1] loss: 864.160
[71,     1] loss: 747.769
[72,     1] loss: 815.729
[73,     1] loss: 802.974
[74,     1] loss: 745.360
[75,     1] loss: 743.486
[76,     1] loss: 683.335
[77,     1] loss: 721.680
[78,     1] loss: 637.172
[79,     1] loss: 696.256
[80,     1] loss: 623.726
[81,     1] loss: 646.029
[82,     1] loss: 655.555
[83,     1] loss: 581.277
[84,     1] loss: 658.011
[85,     1] loss: 659.932
[86,     1] loss: 544.286
[87,     1] loss: 589.991
[88,     1] loss: 848.821
[89,     1] loss: 823.170
[90,     1] loss: 629.588
[91,     1] loss: 732.722
[92,     1] loss: 646.854
[93,     1] loss: 670.682
[94,     1] loss: 618.638
[95,     1] loss: 724.923
[96,     1] loss: 555.559
[97,     1] loss: 662.838
[98,     1] loss: 595.282
[99,     1] loss: 580.519
[100,     1] loss: 634.190
[101,     1] loss: 493.838
[102,     1] loss: 505.284
[103,     1] loss: 483.916
[104,     1] loss: 429.214
[105,     1] loss: 582.548
[106,     1] loss: 588.246
[107,     1] loss: 418.335
[108,     1] loss: 462.819
[109,     1] loss: 884.849
[110,     1] loss: 1454.901
[111,     1] loss: 667.949
[112,     1] loss: 791.569
[113,     1] loss: 911.106
Early stopping applied (best metric=0.38017526268959045)
Finished Training
Total time taken: 16.78524684906006
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.308
[2,     1] loss: 1246.758
[3,     1] loss: 1260.929
[4,     1] loss: 1247.926
[5,     1] loss: 1247.345
[6,     1] loss: 1247.432
[7,     1] loss: 1243.303
[8,     1] loss: 1245.275
[9,     1] loss: 1243.186
[10,     1] loss: 1243.558
[11,     1] loss: 1246.836
[12,     1] loss: 1244.056
[13,     1] loss: 1243.154
[14,     1] loss: 1239.094
[15,     1] loss: 1236.729
[16,     1] loss: 1228.675
[17,     1] loss: 1217.958
[18,     1] loss: 1197.162
[19,     1] loss: 1176.776
[20,     1] loss: 1132.446
[21,     1] loss: 1104.394
[22,     1] loss: 1101.513
[23,     1] loss: 1072.484
[24,     1] loss: 1053.296
[25,     1] loss: 1045.005
[26,     1] loss: 1035.621
[27,     1] loss: 1031.066
[28,     1] loss: 1026.773
[29,     1] loss: 1008.439
[30,     1] loss: 1043.346
[31,     1] loss: 1000.352
[32,     1] loss: 1027.636
[33,     1] loss: 986.264
[34,     1] loss: 911.947
[35,     1] loss: 963.315
[36,     1] loss: 947.285
[37,     1] loss: 926.139
[38,     1] loss: 971.393
[39,     1] loss: 874.915
[40,     1] loss: 855.286
[41,     1] loss: 890.358
[42,     1] loss: 835.944
[43,     1] loss: 871.682
[44,     1] loss: 888.448
[45,     1] loss: 814.693
[46,     1] loss: 840.716
[47,     1] loss: 795.177
[48,     1] loss: 749.710
[49,     1] loss: 762.531
[50,     1] loss: 735.771
[51,     1] loss: 794.326
[52,     1] loss: 1257.965
[53,     1] loss: 1511.012
[54,     1] loss: 832.184
[55,     1] loss: 908.148
[56,     1] loss: 1031.625
[57,     1] loss: 1040.076
[58,     1] loss: 1017.321
[59,     1] loss: 987.060
[60,     1] loss: 978.650
[61,     1] loss: 969.440
[62,     1] loss: 980.169
[63,     1] loss: 945.442
[64,     1] loss: 900.044
[65,     1] loss: 879.388
[66,     1] loss: 913.105
[67,     1] loss: 913.886
[68,     1] loss: 868.885
[69,     1] loss: 836.969
[70,     1] loss: 821.598
[71,     1] loss: 830.712
[72,     1] loss: 789.232
[73,     1] loss: 736.312
[74,     1] loss: 783.046
[75,     1] loss: 842.544
[76,     1] loss: 761.605
[77,     1] loss: 731.282
[78,     1] loss: 770.211
[79,     1] loss: 763.443
[80,     1] loss: 678.021
[81,     1] loss: 656.445
[82,     1] loss: 611.245
[83,     1] loss: 627.160
[84,     1] loss: 878.018
[85,     1] loss: 1868.997
[86,     1] loss: 732.689
[87,     1] loss: 988.201
[88,     1] loss: 939.020
[89,     1] loss: 935.079
[90,     1] loss: 951.167
[91,     1] loss: 946.691
[92,     1] loss: 874.219
[93,     1] loss: 904.592
[94,     1] loss: 918.036
[95,     1] loss: 883.808
[96,     1] loss: 920.177
[97,     1] loss: 876.333
[98,     1] loss: 887.644
[99,     1] loss: 832.717
[100,     1] loss: 837.481
[101,     1] loss: 766.408
[102,     1] loss: 800.314
[103,     1] loss: 727.989
[104,     1] loss: 732.187
[105,     1] loss: 667.013
[106,     1] loss: 678.157
[107,     1] loss: 696.236
[108,     1] loss: 590.068
[109,     1] loss: 649.392
[110,     1] loss: 887.007
[111,     1] loss: 811.666
[112,     1] loss: 705.478
[113,     1] loss: 744.793
[114,     1] loss: 625.223
[115,     1] loss: 658.341
[116,     1] loss: 777.274
[117,     1] loss: 599.037
[118,     1] loss: 881.229
[119,     1] loss: 1097.242
[120,     1] loss: 929.553
[121,     1] loss: 848.738
[122,     1] loss: 944.514
[123,     1] loss: 904.445
[124,     1] loss: 777.575
[125,     1] loss: 810.197
[126,     1] loss: 744.315
[127,     1] loss: 737.109
[128,     1] loss: 747.406
[129,     1] loss: 738.423
[130,     1] loss: 671.530
[131,     1] loss: 646.925
[132,     1] loss: 631.162
[133,     1] loss: 634.408
[134,     1] loss: 605.269
[135,     1] loss: 584.505
[136,     1] loss: 586.746
[137,     1] loss: 555.810
[138,     1] loss: 699.791
[139,     1] loss: 1038.927
[140,     1] loss: 604.616
[141,     1] loss: 864.245
[142,     1] loss: 757.089
[143,     1] loss: 833.818
[144,     1] loss: 661.783
[145,     1] loss: 820.269
[146,     1] loss: 657.086
[147,     1] loss: 805.267
[148,     1] loss: 656.748
[149,     1] loss: 652.809
[150,     1] loss: 631.000
[151,     1] loss: 703.563
[152,     1] loss: 575.571
[153,     1] loss: 662.866
[154,     1] loss: 603.599
[155,     1] loss: 595.065
[156,     1] loss: 568.217
[157,     1] loss: 481.393
[158,     1] loss: 528.150
[159,     1] loss: 437.869
[160,     1] loss: 614.757
[161,     1] loss: 1107.759
[162,     1] loss: 605.146
[163,     1] loss: 784.958
[164,     1] loss: 643.302
[165,     1] loss: 590.677
[166,     1] loss: 616.222
[167,     1] loss: 674.300
[168,     1] loss: 610.669
[169,     1] loss: 555.369
[170,     1] loss: 643.473
[171,     1] loss: 702.666
[172,     1] loss: 531.292
[173,     1] loss: 676.707
[174,     1] loss: 763.955
[175,     1] loss: 717.380
[176,     1] loss: 606.480
[177,     1] loss: 637.950
[178,     1] loss: 554.625
[179,     1] loss: 590.146
[180,     1] loss: 486.408
[181,     1] loss: 534.930
[182,     1] loss: 714.653
[183,     1] loss: 438.970
[184,     1] loss: 1044.885
[185,     1] loss: 1206.499
[186,     1] loss: 1163.505
[187,     1] loss: 1059.941
[188,     1] loss: 1138.861
[189,     1] loss: 1157.210
[190,     1] loss: 1149.166
[191,     1] loss: 1110.651
[192,     1] loss: 1082.847
[193,     1] loss: 1043.268
[194,     1] loss: 1008.416
[195,     1] loss: 971.793
[196,     1] loss: 945.271
[197,     1] loss: 944.325
[198,     1] loss: 931.580
[199,     1] loss: 914.608
[200,     1] loss: 942.059
Finished Training
Total time taken: 29.362667560577393
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1259.954
[2,     1] loss: 1251.688
[3,     1] loss: 1252.801
[4,     1] loss: 1249.404
[5,     1] loss: 1251.895
[6,     1] loss: 1249.697
[7,     1] loss: 1248.421
[8,     1] loss: 1246.909
[9,     1] loss: 1247.908
[10,     1] loss: 1249.467
[11,     1] loss: 1249.201
[12,     1] loss: 1245.528
[13,     1] loss: 1246.561
[14,     1] loss: 1248.059
[15,     1] loss: 1247.902
[16,     1] loss: 1242.988
[17,     1] loss: 1242.068
[18,     1] loss: 1237.273
[19,     1] loss: 1224.791
[20,     1] loss: 1222.984
[21,     1] loss: 1190.111
[22,     1] loss: 1158.991
[23,     1] loss: 1125.496
[24,     1] loss: 1082.680
[25,     1] loss: 1068.036
[26,     1] loss: 1047.847
[27,     1] loss: 963.923
[28,     1] loss: 1016.072
[29,     1] loss: 1162.243
[30,     1] loss: 939.858
[31,     1] loss: 1068.464
[32,     1] loss: 984.086
[33,     1] loss: 1002.043
[34,     1] loss: 1000.980
[35,     1] loss: 969.575
[36,     1] loss: 951.381
[37,     1] loss: 962.990
[38,     1] loss: 946.644
[39,     1] loss: 950.346
[40,     1] loss: 917.932
[41,     1] loss: 927.613
[42,     1] loss: 945.088
[43,     1] loss: 875.426
[44,     1] loss: 896.981
[45,     1] loss: 877.895
[46,     1] loss: 934.642
[47,     1] loss: 842.871
[48,     1] loss: 950.720
[49,     1] loss: 853.381
[50,     1] loss: 956.090
[51,     1] loss: 816.846
[52,     1] loss: 856.715
[53,     1] loss: 856.668
[54,     1] loss: 846.896
[55,     1] loss: 754.180
[56,     1] loss: 887.437
[57,     1] loss: 842.352
[58,     1] loss: 738.924
[59,     1] loss: 809.187
[60,     1] loss: 724.494
[61,     1] loss: 739.485
[62,     1] loss: 794.561
[63,     1] loss: 789.181
[64,     1] loss: 746.090
[65,     1] loss: 662.383
[66,     1] loss: 788.325
[67,     1] loss: 880.403
[68,     1] loss: 745.773
[69,     1] loss: 662.740
[70,     1] loss: 734.010
[71,     1] loss: 668.889
[72,     1] loss: 650.614
[73,     1] loss: 827.370
[74,     1] loss: 1230.041
[75,     1] loss: 1022.674
[76,     1] loss: 948.948
[77,     1] loss: 777.362
[78,     1] loss: 936.972
[79,     1] loss: 936.992
[80,     1] loss: 878.989
[81,     1] loss: 866.140
[82,     1] loss: 888.675
[83,     1] loss: 884.097
[84,     1] loss: 762.804
[85,     1] loss: 833.788
[86,     1] loss: 869.579
[87,     1] loss: 766.283
[88,     1] loss: 781.413
[89,     1] loss: 736.911
[90,     1] loss: 703.408
[91,     1] loss: 716.999
[92,     1] loss: 688.622
[93,     1] loss: 662.128
[94,     1] loss: 674.083
[95,     1] loss: 617.405
[96,     1] loss: 708.470
[97,     1] loss: 647.725
[98,     1] loss: 594.390
[99,     1] loss: 603.409
[100,     1] loss: 529.770
[101,     1] loss: 566.563
[102,     1] loss: 802.464
[103,     1] loss: 2100.400
[104,     1] loss: 1247.852
[105,     1] loss: 766.525
[106,     1] loss: 1238.757
[107,     1] loss: 1010.158
[108,     1] loss: 1100.134
[109,     1] loss: 1125.048
[110,     1] loss: 1092.562
[111,     1] loss: 1040.229
[112,     1] loss: 1003.814
[113,     1] loss: 1034.627
[114,     1] loss: 1038.617
[115,     1] loss: 1024.115
[116,     1] loss: 1047.640
[117,     1] loss: 1023.620
[118,     1] loss: 1025.563
[119,     1] loss: 981.762
[120,     1] loss: 975.110
[121,     1] loss: 965.946
[122,     1] loss: 937.073
[123,     1] loss: 930.485
[124,     1] loss: 874.366
[125,     1] loss: 893.428
[126,     1] loss: 825.073
[127,     1] loss: 852.636
[128,     1] loss: 769.799
[129,     1] loss: 780.036
[130,     1] loss: 738.739
[131,     1] loss: 694.521
[132,     1] loss: 691.753
[133,     1] loss: 678.473
[134,     1] loss: 653.729
[135,     1] loss: 679.287
[136,     1] loss: 1388.455
Early stopping applied (best metric=0.3633459806442261)
Finished Training
Total time taken: 21.25317668914795
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.359
[2,     1] loss: 1275.526
[3,     1] loss: 1251.649
[4,     1] loss: 1248.121
[5,     1] loss: 1248.873
[6,     1] loss: 1247.466
[7,     1] loss: 1246.646
[8,     1] loss: 1246.951
[9,     1] loss: 1247.025
[10,     1] loss: 1246.141
[11,     1] loss: 1246.351
[12,     1] loss: 1246.269
[13,     1] loss: 1246.349
[14,     1] loss: 1244.886
[15,     1] loss: 1247.419
[16,     1] loss: 1245.850
[17,     1] loss: 1245.657
[18,     1] loss: 1247.462
[19,     1] loss: 1247.511
[20,     1] loss: 1246.854
[21,     1] loss: 1249.182
[22,     1] loss: 1245.311
[23,     1] loss: 1245.741
[24,     1] loss: 1244.971
[25,     1] loss: 1243.904
[26,     1] loss: 1241.392
[27,     1] loss: 1240.468
[28,     1] loss: 1234.308
[29,     1] loss: 1223.148
[30,     1] loss: 1210.584
[31,     1] loss: 1190.526
[32,     1] loss: 1154.114
[33,     1] loss: 1141.004
[34,     1] loss: 1119.702
[35,     1] loss: 1079.527
[36,     1] loss: 1093.105
[37,     1] loss: 1092.901
[38,     1] loss: 1025.343
[39,     1] loss: 1069.406
[40,     1] loss: 981.950
[41,     1] loss: 979.447
[42,     1] loss: 959.160
[43,     1] loss: 951.781
[44,     1] loss: 973.278
[45,     1] loss: 945.617
[46,     1] loss: 947.123
[47,     1] loss: 950.840
[48,     1] loss: 921.598
[49,     1] loss: 938.733
[50,     1] loss: 916.340
[51,     1] loss: 883.829
[52,     1] loss: 860.422
[53,     1] loss: 902.856
[54,     1] loss: 885.664
[55,     1] loss: 996.633
[56,     1] loss: 1183.203
[57,     1] loss: 904.595
[58,     1] loss: 957.095
[59,     1] loss: 1009.918
[60,     1] loss: 905.504
[61,     1] loss: 928.890
[62,     1] loss: 965.282
[63,     1] loss: 913.207
[64,     1] loss: 859.480
[65,     1] loss: 911.026
[66,     1] loss: 864.803
[67,     1] loss: 894.612
[68,     1] loss: 837.346
[69,     1] loss: 817.519
[70,     1] loss: 830.999
[71,     1] loss: 811.085
[72,     1] loss: 829.623
[73,     1] loss: 821.282
[74,     1] loss: 803.633
[75,     1] loss: 827.438
[76,     1] loss: 762.788
[77,     1] loss: 742.488
[78,     1] loss: 727.860
[79,     1] loss: 799.112
[80,     1] loss: 687.527
[81,     1] loss: 666.998
[82,     1] loss: 684.474
[83,     1] loss: 714.677
[84,     1] loss: 666.718
[85,     1] loss: 676.419
[86,     1] loss: 699.280
[87,     1] loss: 1342.181
[88,     1] loss: 1552.139
[89,     1] loss: 974.089
[90,     1] loss: 863.187
[91,     1] loss: 1044.711
[92,     1] loss: 1106.296
[93,     1] loss: 1082.564
[94,     1] loss: 1065.660
[95,     1] loss: 1072.943
[96,     1] loss: 1035.997
[97,     1] loss: 1012.241
[98,     1] loss: 1022.367
[99,     1] loss: 972.841
[100,     1] loss: 954.373
[101,     1] loss: 950.677
[102,     1] loss: 955.592
[103,     1] loss: 959.033
[104,     1] loss: 952.798
[105,     1] loss: 867.796
[106,     1] loss: 883.008
[107,     1] loss: 877.903
[108,     1] loss: 887.650
[109,     1] loss: 844.623
[110,     1] loss: 842.247
[111,     1] loss: 855.756
[112,     1] loss: 804.254
[113,     1] loss: 819.001
[114,     1] loss: 798.129
[115,     1] loss: 762.413
[116,     1] loss: 745.245
[117,     1] loss: 759.851
[118,     1] loss: 828.577
[119,     1] loss: 1052.878
[120,     1] loss: 928.727
[121,     1] loss: 905.974
[122,     1] loss: 882.391
[123,     1] loss: 947.485
[124,     1] loss: 920.069
[125,     1] loss: 846.804
[126,     1] loss: 896.907
[127,     1] loss: 818.602
[128,     1] loss: 817.603
[129,     1] loss: 832.289
[130,     1] loss: 781.432
[131,     1] loss: 789.945
[132,     1] loss: 752.455
[133,     1] loss: 792.474
[134,     1] loss: 692.352
[135,     1] loss: 738.613
[136,     1] loss: 723.859
[137,     1] loss: 762.548
[138,     1] loss: 720.195
[139,     1] loss: 702.444
[140,     1] loss: 609.587
[141,     1] loss: 715.975
[142,     1] loss: 791.758
[143,     1] loss: 928.754
[144,     1] loss: 909.028
[145,     1] loss: 716.752
[146,     1] loss: 800.994
[147,     1] loss: 672.890
[148,     1] loss: 824.597
[149,     1] loss: 708.656
[150,     1] loss: 755.469
[151,     1] loss: 799.372
[152,     1] loss: 672.939
[153,     1] loss: 782.370
[154,     1] loss: 658.997
[155,     1] loss: 710.124
[156,     1] loss: 675.637
[157,     1] loss: 647.642
[158,     1] loss: 667.032
[159,     1] loss: 690.900
[160,     1] loss: 624.499
[161,     1] loss: 542.917
[162,     1] loss: 592.060
[163,     1] loss: 690.802
[164,     1] loss: 1245.236
[165,     1] loss: 642.511
[166,     1] loss: 869.666
[167,     1] loss: 737.706
Early stopping applied (best metric=0.3739308714866638)
Finished Training
Total time taken: 26.146171808242798
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.110
[2,     1] loss: 1253.510
[3,     1] loss: 1247.206
[4,     1] loss: 1249.338
[5,     1] loss: 1247.198
[6,     1] loss: 1245.016
[7,     1] loss: 1242.848
[8,     1] loss: 1241.007
[9,     1] loss: 1233.397
[10,     1] loss: 1217.988
[11,     1] loss: 1190.817
[12,     1] loss: 1150.444
[13,     1] loss: 1130.272
[14,     1] loss: 1095.650
[15,     1] loss: 1054.306
[16,     1] loss: 1030.867
[17,     1] loss: 1013.957
[18,     1] loss: 970.062
[19,     1] loss: 1074.514
[20,     1] loss: 1018.994
[21,     1] loss: 1015.777
[22,     1] loss: 953.003
[23,     1] loss: 966.266
[24,     1] loss: 979.337
[25,     1] loss: 963.442
[26,     1] loss: 959.528
[27,     1] loss: 961.152
[28,     1] loss: 935.100
[29,     1] loss: 945.677
[30,     1] loss: 873.461
[31,     1] loss: 903.920
[32,     1] loss: 921.105
[33,     1] loss: 831.822
[34,     1] loss: 914.292
[35,     1] loss: 832.540
[36,     1] loss: 864.722
[37,     1] loss: 856.049
[38,     1] loss: 827.026
[39,     1] loss: 972.059
[40,     1] loss: 746.874
[41,     1] loss: 940.201
[42,     1] loss: 818.722
[43,     1] loss: 867.894
[44,     1] loss: 766.196
[45,     1] loss: 796.495
[46,     1] loss: 760.616
[47,     1] loss: 793.516
[48,     1] loss: 735.305
[49,     1] loss: 719.768
[50,     1] loss: 680.673
[51,     1] loss: 739.949
[52,     1] loss: 657.378
[53,     1] loss: 647.821
[54,     1] loss: 728.479
[55,     1] loss: 845.542
[56,     1] loss: 629.768
[57,     1] loss: 664.644
[58,     1] loss: 725.223
[59,     1] loss: 601.587
[60,     1] loss: 750.531
[61,     1] loss: 720.731
[62,     1] loss: 586.191
[63,     1] loss: 790.668
[64,     1] loss: 723.564
[65,     1] loss: 643.146
[66,     1] loss: 664.421
[67,     1] loss: 629.709
[68,     1] loss: 608.478
[69,     1] loss: 622.447
[70,     1] loss: 575.058
[71,     1] loss: 525.248
[72,     1] loss: 629.210
[73,     1] loss: 542.840
[74,     1] loss: 483.407
[75,     1] loss: 417.578
[76,     1] loss: 484.413
[77,     1] loss: 647.427
[78,     1] loss: 560.861
[79,     1] loss: 452.588
[80,     1] loss: 421.935
[81,     1] loss: 546.788
[82,     1] loss: 552.053
[83,     1] loss: 424.893
[84,     1] loss: 602.755
[85,     1] loss: 743.933
[86,     1] loss: 474.415
[87,     1] loss: 657.520
[88,     1] loss: 435.558
[89,     1] loss: 547.951
[90,     1] loss: 445.317
[91,     1] loss: 446.773
[92,     1] loss: 439.187
[93,     1] loss: 384.297
[94,     1] loss: 393.508
[95,     1] loss: 488.752
[96,     1] loss: 548.681
[97,     1] loss: 583.872
[98,     1] loss: 416.365
[99,     1] loss: 429.359
[100,     1] loss: 608.183
[101,     1] loss: 368.426
[102,     1] loss: 634.962
[103,     1] loss: 863.998
[104,     1] loss: 524.542
[105,     1] loss: 724.811
[106,     1] loss: 571.479
[107,     1] loss: 511.772
[108,     1] loss: 508.611
[109,     1] loss: 539.313
[110,     1] loss: 500.192
[111,     1] loss: 504.364
[112,     1] loss: 535.162
[113,     1] loss: 563.631
[114,     1] loss: 474.221
Early stopping applied (best metric=0.41726627945899963)
Finished Training
Total time taken: 18.654654502868652
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.824
[2,     1] loss: 1263.489
[3,     1] loss: 1245.157
[4,     1] loss: 1248.289
[5,     1] loss: 1248.566
[6,     1] loss: 1247.870
[7,     1] loss: 1247.863
[8,     1] loss: 1247.180
[9,     1] loss: 1246.060
[10,     1] loss: 1246.002
[11,     1] loss: 1246.172
[12,     1] loss: 1244.981
[13,     1] loss: 1244.850
[14,     1] loss: 1242.260
[15,     1] loss: 1242.965
[16,     1] loss: 1243.163
[17,     1] loss: 1235.280
[18,     1] loss: 1230.520
[19,     1] loss: 1210.593
[20,     1] loss: 1200.724
[21,     1] loss: 1174.281
[22,     1] loss: 1171.874
[23,     1] loss: 1132.476
[24,     1] loss: 1091.452
[25,     1] loss: 1103.200
[26,     1] loss: 1046.483
[27,     1] loss: 1083.333
[28,     1] loss: 1059.762
[29,     1] loss: 1044.374
[30,     1] loss: 1029.709
[31,     1] loss: 1042.616
[32,     1] loss: 1016.729
[33,     1] loss: 980.767
[34,     1] loss: 987.599
[35,     1] loss: 975.339
[36,     1] loss: 940.226
[37,     1] loss: 921.007
[38,     1] loss: 954.170
[39,     1] loss: 1002.123
[40,     1] loss: 894.510
[41,     1] loss: 953.599
[42,     1] loss: 921.877
[43,     1] loss: 912.858
[44,     1] loss: 913.328
[45,     1] loss: 887.154
[46,     1] loss: 879.494
[47,     1] loss: 877.820
[48,     1] loss: 896.229
[49,     1] loss: 882.106
[50,     1] loss: 823.800
[51,     1] loss: 882.887
[52,     1] loss: 826.878
[53,     1] loss: 852.757
[54,     1] loss: 812.792
[55,     1] loss: 981.438
[56,     1] loss: 1145.364
[57,     1] loss: 864.547
[58,     1] loss: 912.736
[59,     1] loss: 976.902
[60,     1] loss: 878.396
[61,     1] loss: 904.996
[62,     1] loss: 953.753
[63,     1] loss: 860.149
[64,     1] loss: 855.344
[65,     1] loss: 870.553
[66,     1] loss: 831.885
[67,     1] loss: 835.147
[68,     1] loss: 835.894
[69,     1] loss: 768.851
[70,     1] loss: 795.963
[71,     1] loss: 757.802
[72,     1] loss: 746.157
[73,     1] loss: 761.219
[74,     1] loss: 736.285
[75,     1] loss: 801.961
[76,     1] loss: 773.569
[77,     1] loss: 699.126
[78,     1] loss: 682.458
[79,     1] loss: 669.909
[80,     1] loss: 646.090
[81,     1] loss: 726.427
[82,     1] loss: 648.809
[83,     1] loss: 625.145
[84,     1] loss: 572.450
[85,     1] loss: 587.275
[86,     1] loss: 956.012
[87,     1] loss: 2184.329
[88,     1] loss: 787.543
[89,     1] loss: 1021.934
[90,     1] loss: 1037.795
[91,     1] loss: 1000.020
[92,     1] loss: 1042.372
[93,     1] loss: 1023.135
[94,     1] loss: 995.080
[95,     1] loss: 990.311
[96,     1] loss: 959.151
Early stopping applied (best metric=0.39262834191322327)
Finished Training
Total time taken: 13.157628774642944
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.334
[2,     1] loss: 1258.830
[3,     1] loss: 1253.697
[4,     1] loss: 1244.611
[5,     1] loss: 1245.844
[6,     1] loss: 1245.660
[7,     1] loss: 1245.497
[8,     1] loss: 1241.306
[9,     1] loss: 1241.934
[10,     1] loss: 1235.401
[11,     1] loss: 1227.136
[12,     1] loss: 1208.979
[13,     1] loss: 1178.659
[14,     1] loss: 1155.095
[15,     1] loss: 1110.932
[16,     1] loss: 1070.011
[17,     1] loss: 1067.310
[18,     1] loss: 1073.396
[19,     1] loss: 1046.123
[20,     1] loss: 985.879
[21,     1] loss: 1031.644
[22,     1] loss: 1002.147
[23,     1] loss: 1034.870
[24,     1] loss: 983.839
[25,     1] loss: 1008.604
[26,     1] loss: 957.049
[27,     1] loss: 1036.293
[28,     1] loss: 955.883
[29,     1] loss: 930.630
[30,     1] loss: 915.472
[31,     1] loss: 949.723
[32,     1] loss: 947.773
[33,     1] loss: 899.489
[34,     1] loss: 932.225
[35,     1] loss: 875.877
[36,     1] loss: 891.218
[37,     1] loss: 892.298
[38,     1] loss: 913.913
[39,     1] loss: 863.952
[40,     1] loss: 825.925
[41,     1] loss: 865.851
[42,     1] loss: 819.619
[43,     1] loss: 803.554
[44,     1] loss: 804.245
[45,     1] loss: 825.046
[46,     1] loss: 926.686
[47,     1] loss: 1208.988
[48,     1] loss: 902.146
[49,     1] loss: 1069.073
[50,     1] loss: 919.081
[51,     1] loss: 936.331
[52,     1] loss: 970.930
[53,     1] loss: 941.554
[54,     1] loss: 919.422
[55,     1] loss: 873.924
[56,     1] loss: 909.589
[57,     1] loss: 874.169
[58,     1] loss: 860.374
[59,     1] loss: 871.373
[60,     1] loss: 901.756
[61,     1] loss: 812.927
[62,     1] loss: 823.519
[63,     1] loss: 812.182
[64,     1] loss: 784.220
[65,     1] loss: 843.174
[66,     1] loss: 809.765
[67,     1] loss: 812.825
[68,     1] loss: 765.137
[69,     1] loss: 730.479
[70,     1] loss: 757.239
[71,     1] loss: 768.002
[72,     1] loss: 678.502
[73,     1] loss: 685.678
[74,     1] loss: 792.411
[75,     1] loss: 770.101
[76,     1] loss: 642.205
[77,     1] loss: 686.784
[78,     1] loss: 731.208
[79,     1] loss: 801.573
[80,     1] loss: 605.482
[81,     1] loss: 789.473
[82,     1] loss: 874.648
[83,     1] loss: 649.405
[84,     1] loss: 815.170
[85,     1] loss: 658.843
[86,     1] loss: 690.521
[87,     1] loss: 647.738
[88,     1] loss: 722.805
[89,     1] loss: 601.152
[90,     1] loss: 690.848
[91,     1] loss: 652.163
[92,     1] loss: 613.118
[93,     1] loss: 543.829
[94,     1] loss: 547.808
[95,     1] loss: 606.859
[96,     1] loss: 621.108
[97,     1] loss: 888.527
[98,     1] loss: 689.477
[99,     1] loss: 549.245
[100,     1] loss: 686.431
[101,     1] loss: 573.190
[102,     1] loss: 679.497
[103,     1] loss: 725.677
[104,     1] loss: 598.360
[105,     1] loss: 710.088
[106,     1] loss: 585.388
[107,     1] loss: 626.521
[108,     1] loss: 596.603
[109,     1] loss: 461.874
[110,     1] loss: 497.365
[111,     1] loss: 448.028
[112,     1] loss: 499.303
[113,     1] loss: 647.134
[114,     1] loss: 1757.458
[115,     1] loss: 654.359
[116,     1] loss: 1002.142
[117,     1] loss: 865.310
[118,     1] loss: 868.098
[119,     1] loss: 807.744
[120,     1] loss: 822.749
[121,     1] loss: 816.830
[122,     1] loss: 769.268
[123,     1] loss: 789.706
[124,     1] loss: 811.561
[125,     1] loss: 736.675
[126,     1] loss: 709.432
[127,     1] loss: 711.963
[128,     1] loss: 776.752
[129,     1] loss: 642.833
[130,     1] loss: 647.910
[131,     1] loss: 669.351
[132,     1] loss: 596.604
[133,     1] loss: 629.337
[134,     1] loss: 673.680
[135,     1] loss: 542.402
[136,     1] loss: 515.540
[137,     1] loss: 579.081
[138,     1] loss: 507.376
[139,     1] loss: 581.340
[140,     1] loss: 755.842
[141,     1] loss: 1645.941
Early stopping applied (best metric=0.3553197681903839)
Finished Training
Total time taken: 21.542789220809937
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1251.776
[2,     1] loss: 1254.896
[3,     1] loss: 1245.377
[4,     1] loss: 1251.158
[5,     1] loss: 1250.031
[6,     1] loss: 1249.428
[7,     1] loss: 1248.461
[8,     1] loss: 1247.816
[9,     1] loss: 1245.988
[10,     1] loss: 1241.490
[11,     1] loss: 1240.743
[12,     1] loss: 1232.849
[13,     1] loss: 1223.949
[14,     1] loss: 1205.115
[15,     1] loss: 1169.667
[16,     1] loss: 1154.761
[17,     1] loss: 1131.117
[18,     1] loss: 1085.114
[19,     1] loss: 1047.818
[20,     1] loss: 1048.718
[21,     1] loss: 1059.167
[22,     1] loss: 1061.176
[23,     1] loss: 1047.175
[24,     1] loss: 993.908
[25,     1] loss: 1057.021
[26,     1] loss: 949.450
[27,     1] loss: 1024.944
[28,     1] loss: 998.002
[29,     1] loss: 965.739
[30,     1] loss: 929.975
[31,     1] loss: 945.591
[32,     1] loss: 958.220
[33,     1] loss: 921.533
[34,     1] loss: 891.701
[35,     1] loss: 892.960
[36,     1] loss: 880.544
[37,     1] loss: 861.485
[38,     1] loss: 1033.554
[39,     1] loss: 1037.172
[40,     1] loss: 864.832
[41,     1] loss: 935.085
[42,     1] loss: 913.084
[43,     1] loss: 916.849
[44,     1] loss: 894.878
[45,     1] loss: 848.474
[46,     1] loss: 852.301
[47,     1] loss: 840.697
[48,     1] loss: 819.065
[49,     1] loss: 858.589
[50,     1] loss: 775.194
[51,     1] loss: 817.922
[52,     1] loss: 820.581
[53,     1] loss: 840.934
[54,     1] loss: 798.258
[55,     1] loss: 805.767
[56,     1] loss: 773.020
[57,     1] loss: 775.949
[58,     1] loss: 694.393
[59,     1] loss: 711.341
[60,     1] loss: 831.492
[61,     1] loss: 760.630
[62,     1] loss: 677.135
[63,     1] loss: 658.659
[64,     1] loss: 635.995
[65,     1] loss: 675.134
[66,     1] loss: 576.493
[67,     1] loss: 660.461
[68,     1] loss: 1192.987
[69,     1] loss: 1166.563
[70,     1] loss: 732.746
[71,     1] loss: 897.068
[72,     1] loss: 879.092
[73,     1] loss: 843.369
[74,     1] loss: 835.366
[75,     1] loss: 862.009
[76,     1] loss: 753.113
[77,     1] loss: 742.024
[78,     1] loss: 835.870
[79,     1] loss: 727.195
[80,     1] loss: 702.571
[81,     1] loss: 647.734
[82,     1] loss: 647.871
[83,     1] loss: 674.049
[84,     1] loss: 736.082
[85,     1] loss: 599.135
[86,     1] loss: 642.902
[87,     1] loss: 622.669
[88,     1] loss: 582.989
[89,     1] loss: 624.557
[90,     1] loss: 564.624
[91,     1] loss: 484.372
[92,     1] loss: 460.185
[93,     1] loss: 510.086
[94,     1] loss: 1237.054
[95,     1] loss: 2921.319
[96,     1] loss: 2202.951
[97,     1] loss: 1360.420
[98,     1] loss: 1181.056
[99,     1] loss: 1238.814
[100,     1] loss: 1247.157
[101,     1] loss: 1248.442
[102,     1] loss: 1248.396
[103,     1] loss: 1249.268
[104,     1] loss: 1246.658
[105,     1] loss: 1250.922
[106,     1] loss: 1250.324
[107,     1] loss: 1248.902
[108,     1] loss: 1247.211
[109,     1] loss: 1248.746
[110,     1] loss: 1247.854
[111,     1] loss: 1250.183
[112,     1] loss: 1247.787
[113,     1] loss: 1248.638
[114,     1] loss: 1246.879
[115,     1] loss: 1248.071
[116,     1] loss: 1250.124
[117,     1] loss: 1249.828
[118,     1] loss: 1248.501
[119,     1] loss: 1247.618
[120,     1] loss: 1248.889
[121,     1] loss: 1243.468
[122,     1] loss: 1245.821
[123,     1] loss: 1239.290
[124,     1] loss: 1248.882
[125,     1] loss: 1231.604
[126,     1] loss: 1232.446
[127,     1] loss: 1224.286
[128,     1] loss: 1216.655
[129,     1] loss: 1221.625
[130,     1] loss: 1194.377
[131,     1] loss: 1190.113
[132,     1] loss: 1191.799
[133,     1] loss: 1186.434
[134,     1] loss: 1190.444
[135,     1] loss: 1159.774
[136,     1] loss: 1132.220
Early stopping applied (best metric=0.35335779190063477)
Finished Training
Total time taken: 19.35276174545288
{'Hydroxylation-K Validation Accuracy': 0.714066193853428, 'Hydroxylation-K Validation Sensitivity': 0.6037037037037037, 'Hydroxylation-K Validation Specificity': 0.7421052631578947, 'Hydroxylation-K Validation Precision': 0.3801412696923533, 'Hydroxylation-K AUC ROC': 0.7485769980506822, 'Hydroxylation-K AUC PR': 0.5259044571850544, 'Hydroxylation-K MCC': 0.3003310955758236, 'Hydroxylation-K F1': 0.461477004680403, 'Validation Loss (Hydroxylation-K)': 0.5061308324337006, 'Hydroxylation-P Validation Accuracy': 0.7849243016428946, 'Hydroxylation-P Validation Sensitivity': 0.802962962962963, 'Hydroxylation-P Validation Specificity': 0.7810663873509901, 'Hydroxylation-P Validation Precision': 0.4470621710544443, 'Hydroxylation-P AUC ROC': 0.8495602420894206, 'Hydroxylation-P AUC PR': 0.562232705949381, 'Hydroxylation-P MCC': 0.48060955656650384, 'Hydroxylation-P F1': 0.5719321235199065, 'Validation Loss (Hydroxylation-P)': 0.36867360870043436, 'Validation Loss (total)': 0.8748044490814209, 'TimeToTrain': 20.080329688390098}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009173028899365821,
 'learning_rate_Hydroxylation-K': 0.009692972039750445,
 'learning_rate_Hydroxylation-P': 0.002117038022922093,
 'log_base': 1.0152336682588006,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4148955179,
 'sample_weights': [1.6085149046686391, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.699610996972447,
 'weight_decay_Hydroxylation-K': 2.9063787115062585,
 'weight_decay_Hydroxylation-P': 4.39630491063042}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35996.742
Exploding loss, terminate run (best metric=0.539465069770813)
Finished Training
Total time taken: 0.20000052452087402
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35807.531
Exploding loss, terminate run (best metric=0.5327631235122681)
Finished Training
Total time taken: 0.20999956130981445
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35938.691
Exploding loss, terminate run (best metric=0.5416786670684814)
Finished Training
Total time taken: 0.2240002155303955
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35896.062
Exploding loss, terminate run (best metric=0.5271875262260437)
Finished Training
Total time taken: 0.20600080490112305
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36001.820
Exploding loss, terminate run (best metric=0.5313782691955566)
Finished Training
Total time taken: 0.23200273513793945
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35908.652
Exploding loss, terminate run (best metric=0.5354370474815369)
Finished Training
Total time taken: 0.22700071334838867
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35836.477
Exploding loss, terminate run (best metric=0.5328633189201355)
Finished Training
Total time taken: 0.2370007038116455
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35817.250
Exploding loss, terminate run (best metric=0.552924394607544)
Finished Training
Total time taken: 0.22300052642822266
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35776.512
Exploding loss, terminate run (best metric=0.5277055501937866)
Finished Training
Total time taken: 0.20499968528747559
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35830.707
Exploding loss, terminate run (best metric=0.5557174682617188)
Finished Training
Total time taken: 0.22400140762329102
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35868.059
Exploding loss, terminate run (best metric=0.5380041003227234)
Finished Training
Total time taken: 0.2350006103515625
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36015.176
Exploding loss, terminate run (best metric=0.5472906827926636)
Finished Training
Total time taken: 0.2239985466003418
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35723.055
Exploding loss, terminate run (best metric=0.5664907693862915)
Finished Training
Total time taken: 0.21000242233276367
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36099.094
Exploding loss, terminate run (best metric=0.5449025630950928)
Finished Training
Total time taken: 0.23099899291992188
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35975.633
Exploding loss, terminate run (best metric=0.5595353841781616)
Finished Training
Total time taken: 0.22500300407409668
{'Hydroxylation-K Validation Accuracy': 0.5994680851063829, 'Hydroxylation-K Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-K Validation Specificity': 0.6666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6046783625730994, 'Hydroxylation-K AUC PR': 0.32332545343038127, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.11182266009852218, 'Validation Loss (Hydroxylation-K)': 0.5678360939025879, 'Hydroxylation-P Validation Accuracy': 0.607587838180803, 'Hydroxylation-P Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-P Validation Specificity': 0.6666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5690945625677408, 'Hydroxylation-P AUC PR': 0.24947328485445922, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.10011106282292724, 'Validation Loss (Hydroxylation-P)': 0.5422229290008544, 'Validation Loss (total)': 1.1100590149561564, 'TimeToTrain': 0.2208673636118571}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006326401880934147,
 'learning_rate_Hydroxylation-K': 0.008941667617176187,
 'learning_rate_Hydroxylation-P': 0.00967768964687182,
 'log_base': 1.6030558564505117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3629761020,
 'sample_weights': [110.5035732779859, 13.784245797064878],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5736272264886852,
 'weight_decay_Hydroxylation-K': 1.3303044417779115,
 'weight_decay_Hydroxylation-P': 2.437937404257698}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1653.991
[2,     1] loss: 1651.862
[3,     1] loss: 1656.446
[4,     1] loss: 1648.012
[5,     1] loss: 1646.313
[6,     1] loss: 1647.424
[7,     1] loss: 1627.436
[8,     1] loss: 1621.574
[9,     1] loss: 1603.181
[10,     1] loss: 1554.318
[11,     1] loss: 1533.856
[12,     1] loss: 1461.210
[13,     1] loss: 1440.485
[14,     1] loss: 1422.183
[15,     1] loss: 1438.774
[16,     1] loss: 1420.102
[17,     1] loss: 1400.360
[18,     1] loss: 1389.973
[19,     1] loss: 1335.901
[20,     1] loss: 1369.253
[21,     1] loss: 1322.384
[22,     1] loss: 1322.361
[23,     1] loss: 1328.867
[24,     1] loss: 1354.172
[25,     1] loss: 1330.220
[26,     1] loss: 1301.873
[27,     1] loss: 1293.494
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006832719595293981,
 'learning_rate_Hydroxylation-K': 0.005692474294476003,
 'learning_rate_Hydroxylation-P': 0.00306316311774805,
 'log_base': 2.8234900871503066,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3391096050,
 'sample_weights': [3.537617490435266, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.219389419492774,
 'weight_decay_Hydroxylation-K': 2.695710235622186,
 'weight_decay_Hydroxylation-P': 6.301817212020428}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.304
[2,     1] loss: 1248.381
[3,     1] loss: 1242.126
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004406101636860801,
 'learning_rate_Hydroxylation-K': 0.0008061978000511861,
 'learning_rate_Hydroxylation-P': 0.004430210700361021,
 'log_base': 2.0835096202961076,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3867208382,
 'sample_weights': [1.6083674222531859, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.761975700913862,
 'weight_decay_Hydroxylation-K': 5.1155072413686815,
 'weight_decay_Hydroxylation-P': 1.0964894795734652}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.020
[2,     1] loss: 1389.411
[3,     1] loss: 1386.510
[4,     1] loss: 1385.892
[5,     1] loss: 1385.269
[6,     1] loss: 1376.900
[7,     1] loss: 1363.132
[8,     1] loss: 1332.005
[9,     1] loss: 1292.722
[10,     1] loss: 1227.495
[11,     1] loss: 1220.889
[12,     1] loss: 1253.555
[13,     1] loss: 1222.199
[14,     1] loss: 1233.408
[15,     1] loss: 1144.574
[16,     1] loss: 1154.943
[17,     1] loss: 1097.125
[18,     1] loss: 1158.229
[19,     1] loss: 1104.954
[20,     1] loss: 1151.616
[21,     1] loss: 1152.279
[22,     1] loss: 1088.904
[23,     1] loss: 1112.750
[24,     1] loss: 1065.723
[25,     1] loss: 1022.343
[26,     1] loss: 1071.368
[27,     1] loss: 993.003
[28,     1] loss: 1041.163
[29,     1] loss: 1053.101
[30,     1] loss: 954.499
[31,     1] loss: 1021.294
[32,     1] loss: 942.634
[33,     1] loss: 1008.828
[34,     1] loss: 977.849
[35,     1] loss: 935.206
[36,     1] loss: 926.445
[37,     1] loss: 871.900
[38,     1] loss: 853.505
[39,     1] loss: 863.859
[40,     1] loss: 837.608
[41,     1] loss: 880.498
[42,     1] loss: 1428.128
[43,     1] loss: 863.380
[44,     1] loss: 1170.318
[45,     1] loss: 995.884
[46,     1] loss: 1045.050
[47,     1] loss: 1096.827
[48,     1] loss: 1079.839
[49,     1] loss: 996.313
[50,     1] loss: 957.975
[51,     1] loss: 1042.943
[52,     1] loss: 925.475
[53,     1] loss: 916.898
[54,     1] loss: 888.192
[55,     1] loss: 859.743
[56,     1] loss: 847.350
[57,     1] loss: 797.348
[58,     1] loss: 836.661
[59,     1] loss: 775.142
[60,     1] loss: 816.639
[61,     1] loss: 716.532
[62,     1] loss: 715.502
[63,     1] loss: 844.904
[64,     1] loss: 971.771
[65,     1] loss: 732.680
[66,     1] loss: 875.319
[67,     1] loss: 726.471
[68,     1] loss: 804.608
[69,     1] loss: 654.588
[70,     1] loss: 732.472
[71,     1] loss: 806.349
[72,     1] loss: 609.506
[73,     1] loss: 760.343
[74,     1] loss: 1012.763
[75,     1] loss: 679.382
[76,     1] loss: 935.729
[77,     1] loss: 766.573
[78,     1] loss: 789.776
[79,     1] loss: 782.701
[80,     1] loss: 676.085
[81,     1] loss: 700.803
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007764319674512267,
 'learning_rate_Hydroxylation-K': 0.008182365179722593,
 'learning_rate_Hydroxylation-P': 0.004039776836487463,
 'log_base': 1.030112621765917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2331556059,
 'sample_weights': [2.274279040431827, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.693148092843711,
 'weight_decay_Hydroxylation-K': 2.864230915458485,
 'weight_decay_Hydroxylation-P': 2.627886415136448}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18243.869
Exploding loss, terminate run (best metric=0.5319374203681946)
Finished Training
Total time taken: 0.2019972801208496
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18278.346
Exploding loss, terminate run (best metric=0.5409501194953918)
Finished Training
Total time taken: 0.22800254821777344
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18261.947
Exploding loss, terminate run (best metric=0.5364393591880798)
Finished Training
Total time taken: 0.2440013885498047
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18232.016
Exploding loss, terminate run (best metric=0.5344372987747192)
Finished Training
Total time taken: 0.23199963569641113
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18252.475
Exploding loss, terminate run (best metric=0.5283674597740173)
Finished Training
Total time taken: 0.20800089836120605
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18281.279
Exploding loss, terminate run (best metric=0.5322827100753784)
Finished Training
Total time taken: 0.22700047492980957
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18231.691
Exploding loss, terminate run (best metric=0.5424133539199829)
Finished Training
Total time taken: 0.23599910736083984
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18263.863
Exploding loss, terminate run (best metric=0.5295647978782654)
Finished Training
Total time taken: 0.22400331497192383
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18265.535
Exploding loss, terminate run (best metric=0.5349610447883606)
Finished Training
Total time taken: 0.2050032615661621
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18238.926
Exploding loss, terminate run (best metric=0.5315119028091431)
Finished Training
Total time taken: 0.21900081634521484
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18196.838
Exploding loss, terminate run (best metric=0.5319690108299255)
Finished Training
Total time taken: 0.23800134658813477
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18288.832
Exploding loss, terminate run (best metric=0.5272180438041687)
Finished Training
Total time taken: 0.2220003604888916
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18257.564
Exploding loss, terminate run (best metric=0.5276283621788025)
Finished Training
Total time taken: 0.20300054550170898
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18362.143
Exploding loss, terminate run (best metric=0.5322406888008118)
Finished Training
Total time taken: 0.22499942779541016
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18211.469
Exploding loss, terminate run (best metric=0.5296447277069092)
Finished Training
Total time taken: 0.23300385475158691
{'Hydroxylation-K Validation Accuracy': 0.5216903073286052, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6099610136452241, 'Hydroxylation-K AUC PR': 0.2788141544900801, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.15779967159277505, 'Validation Loss (Hydroxylation-K)': 0.5578534364700317, 'Hydroxylation-P Validation Accuracy': 0.5224913794562036, 'Hydroxylation-P Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-P Validation Specificity': 0.5333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5563145491957403, 'Hydroxylation-P AUC PR': 0.2594589614276595, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.1409602136451631, 'Validation Loss (Hydroxylation-P)': 0.53277108669281, 'Validation Loss (total)': 1.0906245231628418, 'TimeToTrain': 0.22306761741638184}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012425213517600763,
 'learning_rate_Hydroxylation-K': 0.004026907969649395,
 'learning_rate_Hydroxylation-P': 0.008972657641470799,
 'log_base': 1.4328873906748703,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3581995880,
 'sample_weights': [56.31231558279499, 7.024413567536364],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.14302690988792,
 'weight_decay_Hydroxylation-K': 8.523530672463274,
 'weight_decay_Hydroxylation-P': 1.1738966522441716}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1891.035
[2,     1] loss: 1891.678
[3,     1] loss: 1883.884
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008101548079472857,
 'learning_rate_Hydroxylation-K': 0.000459720242804538,
 'learning_rate_Hydroxylation-P': 0.00457185720119884,
 'log_base': 1.638247912850593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2526716398,
 'sample_weights': [4.641318618243394, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.235578670934178,
 'weight_decay_Hydroxylation-K': 3.467183975767587,
 'weight_decay_Hydroxylation-P': 1.0501083642694042}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1622.143
[2,     1] loss: 1630.755
[3,     1] loss: 1613.486
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00806658030150611,
 'learning_rate_Hydroxylation-K': 0.006972370973961435,
 'learning_rate_Hydroxylation-P': 0.0029031433022747447,
 'log_base': 1.040795243355982,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 925974185,
 'sample_weights': [3.3819909537563126, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.181922290156438,
 'weight_decay_Hydroxylation-K': 0.2045051124040791,
 'weight_decay_Hydroxylation-P': 3.774231445429811}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13547.219
[2,     1] loss: 13498.244
[3,     1] loss: 13612.030
[4,     1] loss: 14033.619
[5,     1] loss: 13560.844
[6,     1] loss: 13478.486
[7,     1] loss: 13617.346
[8,     1] loss: 13723.783
[9,     1] loss: 13647.411
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004056470099625989,
 'learning_rate_Hydroxylation-K': 0.0022207810371827944,
 'learning_rate_Hydroxylation-P': 0.004923969591340188,
 'log_base': 1.9321016588354916,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2138527735,
 'sample_weights': [41.751654113993766, 5.219157819421174],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.286870421195756,
 'weight_decay_Hydroxylation-K': 6.254527063168706,
 'weight_decay_Hydroxylation-P': 0.42598848126920763}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1439.493
[2,     1] loss: 1452.026
[3,     1] loss: 1446.689
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028301046358866957,
 'learning_rate_Hydroxylation-K': 0.0009475050677880612,
 'learning_rate_Hydroxylation-P': 0.0017925378263990203,
 'log_base': 1.9442162052574499,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3674642686,
 'sample_weights': [2.5348040913802716, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9169659467728302,
 'weight_decay_Hydroxylation-K': 1.8749676473913923,
 'weight_decay_Hydroxylation-P': 8.820448197231253}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1441.230
[2,     1] loss: 1437.784
[3,     1] loss: 1435.525
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00726922796557605,
 'learning_rate_Hydroxylation-K': 0.007975939761550668,
 'learning_rate_Hydroxylation-P': 0.0032801172839804094,
 'log_base': 1.1030915238259262,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1266528968,
 'sample_weights': [2.5109735401995614, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.851709005089567,
 'weight_decay_Hydroxylation-K': 5.4813460751937075,
 'weight_decay_Hydroxylation-P': 4.575105866628142}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5514.267
[2,     1] loss: 5514.372
[3,     1] loss: 5537.004
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074659946562114585,
 'learning_rate_Hydroxylation-K': 0.004516481589871276,
 'learning_rate_Hydroxylation-P': 0.0011629209660040964,
 'log_base': 2.9418189275036855,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 738253731,
 'sample_weights': [17.01487014054646, 2.1269407027087097],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.738583904771177,
 'weight_decay_Hydroxylation-K': 2.5547390995026142,
 'weight_decay_Hydroxylation-P': 5.083934152577291}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.958
[2,     1] loss: 1240.400
[3,     1] loss: 1239.369
[4,     1] loss: 1234.215
[5,     1] loss: 1237.806
[6,     1] loss: 1236.714
[7,     1] loss: 1234.513
[8,     1] loss: 1233.136
[9,     1] loss: 1233.774
[10,     1] loss: 1232.239
[11,     1] loss: 1230.550
[12,     1] loss: 1230.540
[13,     1] loss: 1228.076
[14,     1] loss: 1220.983
[15,     1] loss: 1214.467
[16,     1] loss: 1202.887
[17,     1] loss: 1175.612
[18,     1] loss: 1142.950
[19,     1] loss: 1116.116
[20,     1] loss: 1105.259
[21,     1] loss: 1016.415
[22,     1] loss: 1009.386
[23,     1] loss: 1000.629
[24,     1] loss: 995.239
[25,     1] loss: 996.884
[26,     1] loss: 1035.964
[27,     1] loss: 965.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015218270174913017,
 'learning_rate_Hydroxylation-K': 0.0013905711711447713,
 'learning_rate_Hydroxylation-P': 0.008127007604645149,
 'log_base': 1.039820946899676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1845346712,
 'sample_weights': [1.547173043407313, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.355065183361535,
 'weight_decay_Hydroxylation-K': 7.314074017819177,
 'weight_decay_Hydroxylation-P': 0.15051162258009798}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13855.467
[2,     1] loss: 13852.512
[3,     1] loss: 13806.964
[4,     1] loss: 13956.535
[5,     1] loss: 13868.944
[6,     1] loss: 13828.467
[7,     1] loss: 13825.785
[8,     1] loss: 13865.613
[9,     1] loss: 13811.143
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003055073749841124,
 'learning_rate_Hydroxylation-K': 0.0019096671737854295,
 'learning_rate_Hydroxylation-P': 0.005145826826203856,
 'log_base': 2.220776520321471,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 643134217,
 'sample_weights': [42.75303235979228, 5.344334922284887],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.103036760854923,
 'weight_decay_Hydroxylation-K': 3.034345800276039,
 'weight_decay_Hydroxylation-P': 9.27640985150895}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.523
[2,     1] loss: 1353.127
[3,     1] loss: 1349.667
[4,     1] loss: 1350.022
[5,     1] loss: 1350.201
[6,     1] loss: 1348.594
[7,     1] loss: 1345.083
[8,     1] loss: 1347.085
[9,     1] loss: 1345.267
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003078048486122611,
 'learning_rate_Hydroxylation-K': 0.00617774348232691,
 'learning_rate_Hydroxylation-P': 0.00782963722543284,
 'log_base': 2.469125458387984,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2657299941,
 'sample_weights': [2.092409188157317, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.412753870559525,
 'weight_decay_Hydroxylation-K': 0.9787232806003551,
 'weight_decay_Hydroxylation-P': 6.248822252434977}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1310.970
[2,     1] loss: 1297.139
[3,     1] loss: 1299.332
[4,     1] loss: 1295.886
[5,     1] loss: 1297.667
[6,     1] loss: 1302.884
[7,     1] loss: 1296.073
[8,     1] loss: 1295.757
[9,     1] loss: 1293.481
[10,     1] loss: 1289.804
[11,     1] loss: 1289.563
[12,     1] loss: 1281.981
[13,     1] loss: 1274.531
[14,     1] loss: 1259.730
[15,     1] loss: 1257.098
[16,     1] loss: 1227.752
[17,     1] loss: 1209.731
[18,     1] loss: 1210.619
[19,     1] loss: 1171.135
[20,     1] loss: 1145.331
[21,     1] loss: 1146.548
[22,     1] loss: 1161.285
[23,     1] loss: 1127.965
[24,     1] loss: 1125.128
[25,     1] loss: 1102.592
[26,     1] loss: 1063.679
[27,     1] loss: 1080.400
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006397683281345601,
 'learning_rate_Hydroxylation-K': 0.007437461607301592,
 'learning_rate_Hydroxylation-P': 0.004579983259774313,
 'log_base': 1.1199642578494504,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2522708446,
 'sample_weights': [1.8470069676856682, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.818605068479845,
 'weight_decay_Hydroxylation-K': 3.100502824155334,
 'weight_decay_Hydroxylation-P': 1.1001927401285407}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4795.855
[2,     1] loss: 4778.269
[3,     1] loss: 4745.818
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017457795649666237,
 'learning_rate_Hydroxylation-K': 0.0031503728349547896,
 'learning_rate_Hydroxylation-P': 0.004883951366723527,
 'log_base': 2.7396211720031216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3147232352,
 'sample_weights': [14.735134245629416, 1.8419627377714238],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8006627039283467,
 'weight_decay_Hydroxylation-K': 0.9559057488563878,
 'weight_decay_Hydroxylation-P': 9.080320940796453}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.634
[2,     1] loss: 1261.502
[3,     1] loss: 1255.028
[4,     1] loss: 1253.821
[5,     1] loss: 1254.654
[6,     1] loss: 1248.516
[7,     1] loss: 1245.167
[8,     1] loss: 1233.106
[9,     1] loss: 1224.970
[10,     1] loss: 1200.806
[11,     1] loss: 1187.834
[12,     1] loss: 1153.916
[13,     1] loss: 1140.070
[14,     1] loss: 1114.792
[15,     1] loss: 1077.343
[16,     1] loss: 1083.837
[17,     1] loss: 1054.118
[18,     1] loss: 1030.983
[19,     1] loss: 1069.666
[20,     1] loss: 1004.098
[21,     1] loss: 1009.942
[22,     1] loss: 1022.509
[23,     1] loss: 1004.101
[24,     1] loss: 1016.076
[25,     1] loss: 969.974
[26,     1] loss: 955.659
[27,     1] loss: 949.273
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004729072754425764,
 'learning_rate_Hydroxylation-K': 0.0024924138650092003,
 'learning_rate_Hydroxylation-P': 0.005538472169090943,
 'log_base': 2.731817507495698,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 740686231,
 'sample_weights': [1.656489971561824, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.000544630925054,
 'weight_decay_Hydroxylation-K': 3.5248579395628132,
 'weight_decay_Hydroxylation-P': 3.849738850660091}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.806
[2,     1] loss: 1264.708
[3,     1] loss: 1263.509
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008305747509030554,
 'learning_rate_Hydroxylation-K': 0.008717363693941552,
 'learning_rate_Hydroxylation-P': 0.005540781397530116,
 'log_base': 1.0540318160053652,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2440577453,
 'sample_weights': [1.6611917730895087, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.323166951347599,
 'weight_decay_Hydroxylation-K': 1.2725369819807302,
 'weight_decay_Hydroxylation-P': 1.963009666198145}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10253.436
[2,     1] loss: 10452.455
[3,     1] loss: 10467.104
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001249238038159604,
 'learning_rate_Hydroxylation-K': 0.007481089366596358,
 'learning_rate_Hydroxylation-P': 0.008286430691988988,
 'log_base': 1.0930268500168168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3771890446,
 'sample_weights': [31.72481057884112, 3.965754093242461],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.370269133534903,
 'weight_decay_Hydroxylation-K': 9.304806973887652,
 'weight_decay_Hydroxylation-P': 1.1571647623515267}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6088.003
[2,     1] loss: 6108.449
[3,     1] loss: 6101.450
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009184195445320373,
 'learning_rate_Hydroxylation-K': 0.0012443212761554991,
 'learning_rate_Hydroxylation-P': 0.0031798289323454183,
 'log_base': 2.8448915190162434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2459810064,
 'sample_weights': [18.768168800153024, 2.346111478172651],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.395682660588243,
 'weight_decay_Hydroxylation-K': 0.2816802212146788,
 'weight_decay_Hydroxylation-P': 3.8294405771861637}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.357
[2,     1] loss: 1251.298
[3,     1] loss: 1255.583
[4,     1] loss: 1243.918
[5,     1] loss: 1242.504
[6,     1] loss: 1245.423
[7,     1] loss: 1242.555
[8,     1] loss: 1241.899
[9,     1] loss: 1239.396
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005177789256356743,
 'learning_rate_Hydroxylation-K': 0.004274889433428214,
 'learning_rate_Hydroxylation-P': 0.0047505821131866854,
 'log_base': 2.3216376458756205,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 934173205,
 'sample_weights': [1.5967511526675415, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9542540074982853,
 'weight_decay_Hydroxylation-K': 3.7618100655537985,
 'weight_decay_Hydroxylation-P': 8.174327336647288}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.233
[2,     1] loss: 1334.813
[3,     1] loss: 1328.300
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018076525896501842,
 'learning_rate_Hydroxylation-K': 0.008219035557817587,
 'learning_rate_Hydroxylation-P': 0.002578028249624102,
 'log_base': 2.018818833067826,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1662896971,
 'sample_weights': [1.9820693613137697, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.82809636047699,
 'weight_decay_Hydroxylation-K': 5.4040832897856586,
 'weight_decay_Hydroxylation-P': 5.447600706523659}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1414.904
[2,     1] loss: 1411.216
[3,     1] loss: 1411.355
[4,     1] loss: 1408.941
[5,     1] loss: 1408.187
[6,     1] loss: 1407.549
[7,     1] loss: 1404.757
[8,     1] loss: 1405.525
[9,     1] loss: 1395.068
[10,     1] loss: 1390.411
[11,     1] loss: 1374.489
[12,     1] loss: 1363.388
[13,     1] loss: 1341.452
[14,     1] loss: 1323.659
[15,     1] loss: 1299.722
[16,     1] loss: 1262.530
[17,     1] loss: 1251.074
[18,     1] loss: 1229.517
[19,     1] loss: 1187.460
[20,     1] loss: 1174.258
[21,     1] loss: 1184.143
[22,     1] loss: 1171.706
[23,     1] loss: 1125.335
[24,     1] loss: 1223.596
[25,     1] loss: 1147.850
[26,     1] loss: 1147.865
[27,     1] loss: 1129.335
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022955342168056924,
 'learning_rate_Hydroxylation-K': 0.004243658433296389,
 'learning_rate_Hydroxylation-P': 0.009929471111474845,
 'log_base': 2.1835175536897515,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 772445459,
 'sample_weights': [2.376388889316733, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.905429153413433,
 'weight_decay_Hydroxylation-K': 5.169005904921283,
 'weight_decay_Hydroxylation-P': 0.1663041249190886}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1357.895
[2,     1] loss: 1358.203
[3,     1] loss: 1355.981
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004200142827898289,
 'learning_rate_Hydroxylation-K': 0.005946940315795795,
 'learning_rate_Hydroxylation-P': 0.002407250455330392,
 'log_base': 1.7603120739837945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1683105872,
 'sample_weights': [2.137743329835366, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9917365184159155,
 'weight_decay_Hydroxylation-K': 3.6713907092476887,
 'weight_decay_Hydroxylation-P': 8.446847396734244}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1533.091
[2,     1] loss: 1529.046
[3,     1] loss: 1528.264
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008287879812686817,
 'learning_rate_Hydroxylation-K': 0.009221387308515162,
 'learning_rate_Hydroxylation-P': 0.001767169060952358,
 'log_base': 1.0868103231850628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 49145579,
 'sample_weights': [2.95220052716142, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.661753592069222,
 'weight_decay_Hydroxylation-K': 0.9879075017853507,
 'weight_decay_Hydroxylation-P': 2.3461256307472027}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6531.256
[2,     1] loss: 6452.358
[3,     1] loss: 6629.877
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005974560215596251,
 'learning_rate_Hydroxylation-K': 0.009305616375660042,
 'learning_rate_Hydroxylation-P': 0.003960440181588931,
 'log_base': 1.6528284543195313,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3202967755,
 'sample_weights': [20.054070389401605, 2.506855369090339],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.7704121938401824,
 'weight_decay_Hydroxylation-K': 1.2336250281158507,
 'weight_decay_Hydroxylation-P': 6.318248472593741}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1614.184
[2,     1] loss: 1612.697
[3,     1] loss: 1609.777
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007704575262386428,
 'learning_rate_Hydroxylation-K': 0.00738716703988324,
 'learning_rate_Hydroxylation-P': 0.006350877630896092,
 'log_base': 1.0132050474543628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 902398204,
 'sample_weights': [3.3223540282231676, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.89072272314247,
 'weight_decay_Hydroxylation-K': 2.4368010784431844,
 'weight_decay_Hydroxylation-P': 4.463520575314236}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41335.523
Exploding loss, terminate run (best metric=0.5507950782775879)
Finished Training
Total time taken: 0.20104026794433594
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41343.547
Exploding loss, terminate run (best metric=0.5264940857887268)
Finished Training
Total time taken: 0.22500109672546387
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41187.434
Exploding loss, terminate run (best metric=0.5444784760475159)
Finished Training
Total time taken: 0.23399877548217773
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41214.543
Exploding loss, terminate run (best metric=0.5309218764305115)
Finished Training
Total time taken: 0.23503351211547852
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41564.906
Exploding loss, terminate run (best metric=0.5359954237937927)
Finished Training
Total time taken: 0.2030012607574463
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41410.703
Exploding loss, terminate run (best metric=0.5534987449645996)
Finished Training
Total time taken: 0.20600223541259766
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41517.062
Exploding loss, terminate run (best metric=0.5284698605537415)
Finished Training
Total time taken: 0.22600030899047852
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41247.883
Exploding loss, terminate run (best metric=0.5410351753234863)
Finished Training
Total time taken: 0.24100303649902344
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41351.023
Exploding loss, terminate run (best metric=0.5398090481758118)
Finished Training
Total time taken: 0.2259988784790039
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41647.777
Exploding loss, terminate run (best metric=0.5282201766967773)
Finished Training
Total time taken: 0.2050004005432129
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41230.664
Exploding loss, terminate run (best metric=0.5319617986679077)
Finished Training
Total time taken: 0.22600102424621582
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41336.500
Exploding loss, terminate run (best metric=0.5285382270812988)
Finished Training
Total time taken: 0.22900080680847168
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41460.688
Exploding loss, terminate run (best metric=0.5278046727180481)
Finished Training
Total time taken: 0.21500086784362793
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41322.180
Exploding loss, terminate run (best metric=0.5513084530830383)
Finished Training
Total time taken: 0.2050008773803711
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41269.809
Exploding loss, terminate run (best metric=0.5290395617485046)
Finished Training
Total time taken: 0.22600317001342773
{'Hydroxylation-K Validation Accuracy': 0.4828014184397163, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6351851851851852, 'Hydroxylation-K AUC PR': 0.34798621021740184, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1807881773399015, 'Validation Loss (Hydroxylation-K)': 0.5627873500188192, 'Hydroxylation-P Validation Accuracy': 0.47817647158350673, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6294235531677549, 'Hydroxylation-P AUC PR': 0.2853021300156879, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16011130737252802, 'Validation Loss (Hydroxylation-P)': 0.5365580439567565, 'Validation Loss (total)': 1.0993453900019328, 'TimeToTrain': 0.2202057679494222}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052435211681804384,
 'learning_rate_Hydroxylation-K': 0.0025919819745900906,
 'learning_rate_Hydroxylation-P': 0.005272277902162612,
 'log_base': 2.932961070570448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3171020210,
 'sample_weights': [127.3519190361786, 15.88591303112463],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.401859261357133,
 'weight_decay_Hydroxylation-K': 1.1865577456190382,
 'weight_decay_Hydroxylation-P': 4.162268034508009}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.044
[2,     1] loss: 1233.441
[3,     1] loss: 1238.460
[4,     1] loss: 1230.864
[5,     1] loss: 1227.551
[6,     1] loss: 1219.673
[7,     1] loss: 1205.925
[8,     1] loss: 1172.022
[9,     1] loss: 1132.672
[10,     1] loss: 1082.915
[11,     1] loss: 1028.696
[12,     1] loss: 1034.079
[13,     1] loss: 1029.583
[14,     1] loss: 1064.780
[15,     1] loss: 1015.664
[16,     1] loss: 1024.786
[17,     1] loss: 966.369
[18,     1] loss: 985.929
[19,     1] loss: 948.968
[20,     1] loss: 921.216
[21,     1] loss: 905.874
[22,     1] loss: 933.941
[23,     1] loss: 983.481
[24,     1] loss: 858.303
[25,     1] loss: 981.949
[26,     1] loss: 893.242
[27,     1] loss: 930.750
[28,     1] loss: 896.945
[29,     1] loss: 927.001
[30,     1] loss: 852.795
[31,     1] loss: 929.666
[32,     1] loss: 817.864
[33,     1] loss: 878.540
[34,     1] loss: 768.874
[35,     1] loss: 847.304
[36,     1] loss: 864.220
[37,     1] loss: 822.944
[38,     1] loss: 766.078
[39,     1] loss: 794.866
[40,     1] loss: 795.447
[41,     1] loss: 769.156
[42,     1] loss: 786.146
[43,     1] loss: 751.967
[44,     1] loss: 817.685
[45,     1] loss: 800.379
[46,     1] loss: 722.156
[47,     1] loss: 742.142
[48,     1] loss: 701.640
[49,     1] loss: 670.265
[50,     1] loss: 793.209
[51,     1] loss: 781.376
[52,     1] loss: 639.031
[53,     1] loss: 674.722
[54,     1] loss: 647.488
[55,     1] loss: 642.651
[56,     1] loss: 692.352
[57,     1] loss: 681.724
[58,     1] loss: 717.837
[59,     1] loss: 579.267
[60,     1] loss: 564.493
[61,     1] loss: 646.884
[62,     1] loss: 635.302
[63,     1] loss: 616.915
[64,     1] loss: 582.090
[65,     1] loss: 650.533
[66,     1] loss: 737.248
[67,     1] loss: 719.328
[68,     1] loss: 569.406
[69,     1] loss: 757.800
[70,     1] loss: 587.158
[71,     1] loss: 630.550
[72,     1] loss: 607.872
[73,     1] loss: 607.515
Early stopping applied (best metric=0.3518199920654297)
Finished Training
Total time taken: 10.80175232887268
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.976
[2,     1] loss: 1241.559
[3,     1] loss: 1236.079
[4,     1] loss: 1230.192
[5,     1] loss: 1233.280
[6,     1] loss: 1230.179
[7,     1] loss: 1234.091
[8,     1] loss: 1235.832
[9,     1] loss: 1225.833
[10,     1] loss: 1217.903
[11,     1] loss: 1204.179
[12,     1] loss: 1178.406
[13,     1] loss: 1134.162
[14,     1] loss: 1114.358
[15,     1] loss: 1081.468
[16,     1] loss: 1055.148
[17,     1] loss: 1023.974
[18,     1] loss: 1064.483
[19,     1] loss: 1023.347
[20,     1] loss: 1003.170
[21,     1] loss: 1043.386
[22,     1] loss: 983.737
[23,     1] loss: 978.704
[24,     1] loss: 987.347
[25,     1] loss: 967.196
[26,     1] loss: 948.908
[27,     1] loss: 969.984
[28,     1] loss: 900.663
[29,     1] loss: 906.207
[30,     1] loss: 943.972
[31,     1] loss: 897.972
[32,     1] loss: 865.695
[33,     1] loss: 883.199
[34,     1] loss: 827.280
[35,     1] loss: 809.464
[36,     1] loss: 830.179
[37,     1] loss: 903.860
[38,     1] loss: 924.320
[39,     1] loss: 854.433
[40,     1] loss: 912.519
[41,     1] loss: 833.585
[42,     1] loss: 887.508
[43,     1] loss: 795.669
[44,     1] loss: 796.181
[45,     1] loss: 780.053
[46,     1] loss: 809.613
[47,     1] loss: 777.021
[48,     1] loss: 786.453
[49,     1] loss: 748.069
[50,     1] loss: 812.702
[51,     1] loss: 715.424
[52,     1] loss: 796.439
[53,     1] loss: 733.154
[54,     1] loss: 845.758
[55,     1] loss: 1152.818
[56,     1] loss: 815.338
[57,     1] loss: 897.335
[58,     1] loss: 805.325
[59,     1] loss: 848.093
[60,     1] loss: 835.330
[61,     1] loss: 776.305
[62,     1] loss: 807.769
[63,     1] loss: 804.055
[64,     1] loss: 737.754
[65,     1] loss: 733.070
[66,     1] loss: 653.777
[67,     1] loss: 765.774
[68,     1] loss: 613.986
[69,     1] loss: 689.559
[70,     1] loss: 661.076
[71,     1] loss: 641.402
[72,     1] loss: 614.694
[73,     1] loss: 566.306
[74,     1] loss: 581.685
[75,     1] loss: 598.985
[76,     1] loss: 506.207
[77,     1] loss: 499.146
[78,     1] loss: 627.190
[79,     1] loss: 1780.186
[80,     1] loss: 1576.542
[81,     1] loss: 1233.553
[82,     1] loss: 1064.936
[83,     1] loss: 1156.452
[84,     1] loss: 1197.715
[85,     1] loss: 1213.070
[86,     1] loss: 1222.217
[87,     1] loss: 1223.368
[88,     1] loss: 1224.897
[89,     1] loss: 1225.913
[90,     1] loss: 1223.855
[91,     1] loss: 1222.513
[92,     1] loss: 1199.346
[93,     1] loss: 1197.826
[94,     1] loss: 1208.834
[95,     1] loss: 1199.190
[96,     1] loss: 1169.961
[97,     1] loss: 1152.582
[98,     1] loss: 1160.178
[99,     1] loss: 1140.608
[100,     1] loss: 1124.944
[101,     1] loss: 1119.891
[102,     1] loss: 1151.206
[103,     1] loss: 1127.645
[104,     1] loss: 1125.065
[105,     1] loss: 1099.153
[106,     1] loss: 1087.827
[107,     1] loss: 1106.241
[108,     1] loss: 1084.126
[109,     1] loss: 1063.350
[110,     1] loss: 1043.219
[111,     1] loss: 1027.039
[112,     1] loss: 1020.758
Early stopping applied (best metric=0.3506118059158325)
Finished Training
Total time taken: 16.138636589050293
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.938
[2,     1] loss: 1236.522
[3,     1] loss: 1235.805
[4,     1] loss: 1234.238
[5,     1] loss: 1235.037
[6,     1] loss: 1233.275
[7,     1] loss: 1227.293
[8,     1] loss: 1214.301
[9,     1] loss: 1198.530
[10,     1] loss: 1181.890
[11,     1] loss: 1141.438
[12,     1] loss: 1123.831
[13,     1] loss: 1103.825
[14,     1] loss: 1107.369
[15,     1] loss: 1033.855
[16,     1] loss: 1029.074
[17,     1] loss: 1045.581
[18,     1] loss: 998.906
[19,     1] loss: 1044.081
[20,     1] loss: 1076.571
[21,     1] loss: 989.241
[22,     1] loss: 1035.736
[23,     1] loss: 973.113
[24,     1] loss: 999.157
[25,     1] loss: 1012.511
[26,     1] loss: 985.564
[27,     1] loss: 967.481
[28,     1] loss: 938.684
[29,     1] loss: 954.907
[30,     1] loss: 931.431
[31,     1] loss: 906.022
[32,     1] loss: 920.268
[33,     1] loss: 889.662
[34,     1] loss: 958.039
[35,     1] loss: 956.822
[36,     1] loss: 924.025
[37,     1] loss: 898.451
[38,     1] loss: 886.084
[39,     1] loss: 879.601
[40,     1] loss: 841.320
[41,     1] loss: 798.863
[42,     1] loss: 816.106
[43,     1] loss: 866.390
[44,     1] loss: 781.138
[45,     1] loss: 775.477
[46,     1] loss: 897.441
[47,     1] loss: 1152.943
[48,     1] loss: 785.514
[49,     1] loss: 916.945
[50,     1] loss: 786.627
[51,     1] loss: 836.784
[52,     1] loss: 864.785
[53,     1] loss: 809.145
[54,     1] loss: 813.870
[55,     1] loss: 779.053
[56,     1] loss: 782.367
[57,     1] loss: 762.861
[58,     1] loss: 733.521
[59,     1] loss: 768.337
[60,     1] loss: 780.635
[61,     1] loss: 694.421
[62,     1] loss: 858.128
[63,     1] loss: 733.240
[64,     1] loss: 714.817
[65,     1] loss: 708.733
[66,     1] loss: 661.150
[67,     1] loss: 689.863
[68,     1] loss: 597.283
[69,     1] loss: 597.215
[70,     1] loss: 811.149
[71,     1] loss: 1393.263
[72,     1] loss: 1026.424
[73,     1] loss: 1034.305
[74,     1] loss: 931.605
[75,     1] loss: 965.826
[76,     1] loss: 991.233
[77,     1] loss: 981.241
[78,     1] loss: 1083.440
[79,     1] loss: 1032.895
[80,     1] loss: 990.488
[81,     1] loss: 966.501
[82,     1] loss: 960.634
[83,     1] loss: 957.669
[84,     1] loss: 953.036
[85,     1] loss: 850.160
[86,     1] loss: 874.582
[87,     1] loss: 907.245
[88,     1] loss: 868.133
[89,     1] loss: 848.053
[90,     1] loss: 773.716
[91,     1] loss: 816.256
[92,     1] loss: 816.778
Early stopping applied (best metric=0.2957158386707306)
Finished Training
Total time taken: 14.120123624801636
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.048
[2,     1] loss: 1245.767
[3,     1] loss: 1241.724
[4,     1] loss: 1234.608
[5,     1] loss: 1235.546
[6,     1] loss: 1230.211
[7,     1] loss: 1237.308
[8,     1] loss: 1240.732
[9,     1] loss: 1232.072
[10,     1] loss: 1235.809
[11,     1] loss: 1235.076
[12,     1] loss: 1230.435
[13,     1] loss: 1228.143
[14,     1] loss: 1220.443
[15,     1] loss: 1208.625
[16,     1] loss: 1193.048
[17,     1] loss: 1174.947
[18,     1] loss: 1139.729
[19,     1] loss: 1094.101
[20,     1] loss: 1063.676
[21,     1] loss: 1023.407
[22,     1] loss: 1005.607
[23,     1] loss: 1024.266
[24,     1] loss: 1074.008
[25,     1] loss: 969.648
[26,     1] loss: 1000.728
[27,     1] loss: 969.165
[28,     1] loss: 953.921
[29,     1] loss: 977.049
[30,     1] loss: 951.789
[31,     1] loss: 905.893
[32,     1] loss: 904.140
[33,     1] loss: 930.002
[34,     1] loss: 901.462
[35,     1] loss: 873.502
[36,     1] loss: 866.085
[37,     1] loss: 853.109
[38,     1] loss: 918.521
[39,     1] loss: 1007.921
[40,     1] loss: 856.767
[41,     1] loss: 948.976
[42,     1] loss: 883.052
[43,     1] loss: 929.711
[44,     1] loss: 915.421
[45,     1] loss: 859.718
[46,     1] loss: 884.203
[47,     1] loss: 851.757
[48,     1] loss: 833.091
[49,     1] loss: 874.065
[50,     1] loss: 831.753
[51,     1] loss: 813.818
[52,     1] loss: 766.379
[53,     1] loss: 866.350
[54,     1] loss: 768.327
[55,     1] loss: 878.470
[56,     1] loss: 771.260
[57,     1] loss: 800.122
[58,     1] loss: 730.456
[59,     1] loss: 739.667
[60,     1] loss: 708.495
[61,     1] loss: 698.538
[62,     1] loss: 800.066
[63,     1] loss: 936.048
[64,     1] loss: 673.807
[65,     1] loss: 784.856
[66,     1] loss: 744.921
[67,     1] loss: 781.369
[68,     1] loss: 708.009
[69,     1] loss: 758.438
[70,     1] loss: 672.644
[71,     1] loss: 720.914
[72,     1] loss: 669.536
[73,     1] loss: 646.975
[74,     1] loss: 735.487
[75,     1] loss: 575.400
[76,     1] loss: 605.525
[77,     1] loss: 733.654
[78,     1] loss: 630.444
[79,     1] loss: 535.353
[80,     1] loss: 582.814
[81,     1] loss: 637.130
[82,     1] loss: 727.635
[83,     1] loss: 534.870
[84,     1] loss: 706.726
[85,     1] loss: 840.937
[86,     1] loss: 664.835
[87,     1] loss: 673.633
[88,     1] loss: 681.765
[89,     1] loss: 596.679
[90,     1] loss: 637.836
[91,     1] loss: 575.159
[92,     1] loss: 735.415
[93,     1] loss: 543.613
[94,     1] loss: 614.053
[95,     1] loss: 543.534
[96,     1] loss: 585.752
[97,     1] loss: 594.368
[98,     1] loss: 450.503
Early stopping applied (best metric=0.3854624629020691)
Finished Training
Total time taken: 15.293676853179932
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1243.059
[2,     1] loss: 1236.019
[3,     1] loss: 1237.475
[4,     1] loss: 1231.784
[5,     1] loss: 1235.219
[6,     1] loss: 1220.217
[7,     1] loss: 1206.684
[8,     1] loss: 1169.994
[9,     1] loss: 1121.225
[10,     1] loss: 1093.028
[11,     1] loss: 1097.281
[12,     1] loss: 1067.711
[13,     1] loss: 963.599
[14,     1] loss: 1065.625
[15,     1] loss: 977.189
[16,     1] loss: 958.510
[17,     1] loss: 988.330
[18,     1] loss: 974.115
[19,     1] loss: 989.279
[20,     1] loss: 993.384
[21,     1] loss: 965.115
[22,     1] loss: 1005.631
[23,     1] loss: 939.052
[24,     1] loss: 949.938
[25,     1] loss: 935.599
[26,     1] loss: 920.106
[27,     1] loss: 944.098
[28,     1] loss: 889.661
[29,     1] loss: 918.709
[30,     1] loss: 895.114
[31,     1] loss: 880.517
[32,     1] loss: 875.643
[33,     1] loss: 810.071
[34,     1] loss: 840.281
[35,     1] loss: 851.774
[36,     1] loss: 815.181
[37,     1] loss: 871.441
[38,     1] loss: 807.687
[39,     1] loss: 837.736
[40,     1] loss: 752.500
[41,     1] loss: 766.732
[42,     1] loss: 746.616
[43,     1] loss: 729.277
[44,     1] loss: 700.939
[45,     1] loss: 786.632
[46,     1] loss: 1196.299
[47,     1] loss: 868.295
[48,     1] loss: 840.383
[49,     1] loss: 816.582
[50,     1] loss: 919.170
[51,     1] loss: 893.882
[52,     1] loss: 868.660
[53,     1] loss: 812.771
[54,     1] loss: 821.060
[55,     1] loss: 864.075
[56,     1] loss: 820.340
[57,     1] loss: 780.970
[58,     1] loss: 801.926
[59,     1] loss: 753.321
[60,     1] loss: 748.647
[61,     1] loss: 786.541
[62,     1] loss: 702.856
[63,     1] loss: 735.177
[64,     1] loss: 750.873
[65,     1] loss: 703.310
[66,     1] loss: 698.108
[67,     1] loss: 661.812
[68,     1] loss: 640.644
[69,     1] loss: 624.981
[70,     1] loss: 605.841
[71,     1] loss: 558.448
[72,     1] loss: 567.399
[73,     1] loss: 604.266
[74,     1] loss: 654.520
[75,     1] loss: 1661.300
[76,     1] loss: 1137.219
[77,     1] loss: 1048.188
[78,     1] loss: 1014.240
[79,     1] loss: 1057.641
[80,     1] loss: 1051.678
[81,     1] loss: 1048.825
[82,     1] loss: 1030.022
[83,     1] loss: 1016.009
[84,     1] loss: 994.213
[85,     1] loss: 911.823
[86,     1] loss: 909.802
[87,     1] loss: 970.874
[88,     1] loss: 936.999
[89,     1] loss: 915.462
[90,     1] loss: 915.059
[91,     1] loss: 882.700
Early stopping applied (best metric=0.3592812120914459)
Finished Training
Total time taken: 12.559096813201904
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.037
[2,     1] loss: 1245.283
[3,     1] loss: 1237.092
[4,     1] loss: 1236.131
[5,     1] loss: 1233.691
[6,     1] loss: 1233.924
[7,     1] loss: 1236.019
[8,     1] loss: 1233.790
[9,     1] loss: 1233.111
[10,     1] loss: 1234.236
[11,     1] loss: 1234.195
[12,     1] loss: 1230.721
[13,     1] loss: 1227.612
[14,     1] loss: 1220.613
[15,     1] loss: 1212.419
[16,     1] loss: 1177.751
[17,     1] loss: 1143.429
[18,     1] loss: 1125.505
[19,     1] loss: 1087.314
[20,     1] loss: 1037.847
[21,     1] loss: 1024.463
[22,     1] loss: 1063.172
[23,     1] loss: 1033.787
[24,     1] loss: 1004.352
[25,     1] loss: 1010.460
[26,     1] loss: 948.994
[27,     1] loss: 1006.530
[28,     1] loss: 1018.097
[29,     1] loss: 972.337
[30,     1] loss: 950.590
[31,     1] loss: 958.073
[32,     1] loss: 959.313
[33,     1] loss: 875.779
[34,     1] loss: 893.588
[35,     1] loss: 938.632
[36,     1] loss: 952.870
[37,     1] loss: 868.024
[38,     1] loss: 847.333
[39,     1] loss: 808.241
[40,     1] loss: 870.307
[41,     1] loss: 851.328
[42,     1] loss: 939.883
[43,     1] loss: 883.701
[44,     1] loss: 821.601
[45,     1] loss: 902.764
[46,     1] loss: 774.429
[47,     1] loss: 781.762
[48,     1] loss: 778.277
[49,     1] loss: 826.358
[50,     1] loss: 748.786
[51,     1] loss: 722.034
[52,     1] loss: 782.865
[53,     1] loss: 740.753
[54,     1] loss: 741.832
[55,     1] loss: 702.458
[56,     1] loss: 697.292
[57,     1] loss: 646.100
[58,     1] loss: 634.287
[59,     1] loss: 731.225
[60,     1] loss: 1206.704
[61,     1] loss: 1081.602
[62,     1] loss: 985.428
[63,     1] loss: 802.917
[64,     1] loss: 878.028
[65,     1] loss: 961.991
[66,     1] loss: 945.352
[67,     1] loss: 935.035
[68,     1] loss: 877.345
[69,     1] loss: 849.224
[70,     1] loss: 869.889
[71,     1] loss: 824.624
[72,     1] loss: 808.652
[73,     1] loss: 739.681
[74,     1] loss: 765.071
[75,     1] loss: 744.000
[76,     1] loss: 703.477
[77,     1] loss: 700.124
[78,     1] loss: 707.455
[79,     1] loss: 663.679
[80,     1] loss: 617.929
[81,     1] loss: 607.819
[82,     1] loss: 644.250
[83,     1] loss: 625.025
[84,     1] loss: 636.823
[85,     1] loss: 663.287
[86,     1] loss: 640.184
[87,     1] loss: 581.229
[88,     1] loss: 482.288
[89,     1] loss: 573.450
[90,     1] loss: 527.496
[91,     1] loss: 553.887
[92,     1] loss: 513.708
[93,     1] loss: 584.406
Early stopping applied (best metric=0.38275375962257385)
Finished Training
Total time taken: 15.025654792785645
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.348
[2,     1] loss: 1237.476
[3,     1] loss: 1238.959
[4,     1] loss: 1237.123
[5,     1] loss: 1232.234
[6,     1] loss: 1236.074
[7,     1] loss: 1235.385
[8,     1] loss: 1235.415
[9,     1] loss: 1236.215
[10,     1] loss: 1236.535
[11,     1] loss: 1231.263
[12,     1] loss: 1230.394
[13,     1] loss: 1224.596
[14,     1] loss: 1216.741
[15,     1] loss: 1204.758
[16,     1] loss: 1181.846
[17,     1] loss: 1155.271
[18,     1] loss: 1116.797
[19,     1] loss: 1093.148
[20,     1] loss: 1038.548
[21,     1] loss: 1026.042
[22,     1] loss: 1021.263
[23,     1] loss: 1024.680
[24,     1] loss: 1001.702
[25,     1] loss: 990.301
[26,     1] loss: 998.493
[27,     1] loss: 941.505
[28,     1] loss: 959.256
[29,     1] loss: 934.280
[30,     1] loss: 918.805
[31,     1] loss: 950.162
[32,     1] loss: 930.331
[33,     1] loss: 961.940
[34,     1] loss: 920.349
[35,     1] loss: 866.771
[36,     1] loss: 908.427
[37,     1] loss: 863.979
[38,     1] loss: 870.327
[39,     1] loss: 873.834
[40,     1] loss: 879.251
[41,     1] loss: 832.878
[42,     1] loss: 847.041
[43,     1] loss: 818.592
[44,     1] loss: 813.201
[45,     1] loss: 786.540
[46,     1] loss: 807.788
[47,     1] loss: 862.513
[48,     1] loss: 784.532
[49,     1] loss: 759.922
[50,     1] loss: 774.164
[51,     1] loss: 710.479
[52,     1] loss: 865.503
[53,     1] loss: 885.198
[54,     1] loss: 785.568
[55,     1] loss: 799.454
[56,     1] loss: 713.289
[57,     1] loss: 759.388
[58,     1] loss: 748.476
[59,     1] loss: 692.867
[60,     1] loss: 679.899
[61,     1] loss: 683.143
[62,     1] loss: 674.238
[63,     1] loss: 582.296
[64,     1] loss: 593.388
[65,     1] loss: 607.515
[66,     1] loss: 664.971
[67,     1] loss: 737.704
[68,     1] loss: 689.137
[69,     1] loss: 553.697
[70,     1] loss: 606.214
[71,     1] loss: 549.205
[72,     1] loss: 500.173
[73,     1] loss: 540.297
[74,     1] loss: 697.134
[75,     1] loss: 1685.756
[76,     1] loss: 809.942
[77,     1] loss: 992.454
[78,     1] loss: 967.497
[79,     1] loss: 1021.467
[80,     1] loss: 1053.839
[81,     1] loss: 1030.206
[82,     1] loss: 995.422
[83,     1] loss: 903.965
[84,     1] loss: 836.077
[85,     1] loss: 846.619
[86,     1] loss: 841.091
[87,     1] loss: 828.483
[88,     1] loss: 772.830
[89,     1] loss: 767.641
[90,     1] loss: 781.203
[91,     1] loss: 763.945
[92,     1] loss: 811.127
[93,     1] loss: 767.204
[94,     1] loss: 753.503
[95,     1] loss: 749.090
[96,     1] loss: 732.796
[97,     1] loss: 617.173
[98,     1] loss: 681.419
[99,     1] loss: 657.264
[100,     1] loss: 648.024
[101,     1] loss: 564.939
[102,     1] loss: 644.914
[103,     1] loss: 867.430
[104,     1] loss: 708.600
[105,     1] loss: 623.681
[106,     1] loss: 714.694
[107,     1] loss: 553.058
[108,     1] loss: 646.814
[109,     1] loss: 512.868
[110,     1] loss: 561.151
Early stopping applied (best metric=0.32039567828178406)
Finished Training
Total time taken: 15.206177473068237
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.545
[2,     1] loss: 1245.374
[3,     1] loss: 1241.574
[4,     1] loss: 1236.062
[5,     1] loss: 1237.972
[6,     1] loss: 1233.936
[7,     1] loss: 1235.378
[8,     1] loss: 1234.340
[9,     1] loss: 1233.144
[10,     1] loss: 1230.273
[11,     1] loss: 1229.672
[12,     1] loss: 1222.244
[13,     1] loss: 1217.565
[14,     1] loss: 1204.803
[15,     1] loss: 1175.943
[16,     1] loss: 1144.247
[17,     1] loss: 1124.442
[18,     1] loss: 1088.215
[19,     1] loss: 1051.708
[20,     1] loss: 1079.125
[21,     1] loss: 1052.617
[22,     1] loss: 1021.881
[23,     1] loss: 1032.787
[24,     1] loss: 990.300
[25,     1] loss: 1020.024
[26,     1] loss: 1025.714
[27,     1] loss: 992.291
[28,     1] loss: 973.859
[29,     1] loss: 946.672
[30,     1] loss: 915.227
[31,     1] loss: 966.466
[32,     1] loss: 909.101
[33,     1] loss: 990.360
[34,     1] loss: 1004.548
[35,     1] loss: 915.197
[36,     1] loss: 916.979
[37,     1] loss: 871.938
[38,     1] loss: 889.482
[39,     1] loss: 848.623
[40,     1] loss: 906.296
[41,     1] loss: 851.206
[42,     1] loss: 846.099
[43,     1] loss: 854.338
[44,     1] loss: 832.191
[45,     1] loss: 885.463
[46,     1] loss: 842.689
[47,     1] loss: 785.286
[48,     1] loss: 839.041
[49,     1] loss: 789.789
[50,     1] loss: 774.063
[51,     1] loss: 774.008
[52,     1] loss: 732.189
[53,     1] loss: 726.564
[54,     1] loss: 712.781
[55,     1] loss: 801.920
[56,     1] loss: 1621.811
[57,     1] loss: 738.277
[58,     1] loss: 1040.726
[59,     1] loss: 872.193
[60,     1] loss: 894.134
[61,     1] loss: 964.182
[62,     1] loss: 992.843
[63,     1] loss: 964.263
[64,     1] loss: 879.315
[65,     1] loss: 859.026
[66,     1] loss: 884.624
[67,     1] loss: 868.475
[68,     1] loss: 793.312
[69,     1] loss: 869.211
[70,     1] loss: 797.632
[71,     1] loss: 794.788
[72,     1] loss: 792.497
[73,     1] loss: 750.824
[74,     1] loss: 741.927
[75,     1] loss: 773.938
[76,     1] loss: 682.937
[77,     1] loss: 623.477
[78,     1] loss: 657.269
[79,     1] loss: 646.517
[80,     1] loss: 649.211
[81,     1] loss: 661.210
[82,     1] loss: 941.337
[83,     1] loss: 645.375
[84,     1] loss: 768.658
[85,     1] loss: 822.017
[86,     1] loss: 832.801
[87,     1] loss: 773.906
[88,     1] loss: 693.196
[89,     1] loss: 644.996
[90,     1] loss: 710.350
[91,     1] loss: 623.682
[92,     1] loss: 678.417
[93,     1] loss: 545.701
Early stopping applied (best metric=0.3377874493598938)
Finished Training
Total time taken: 13.67864727973938
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.068
[2,     1] loss: 1232.878
[3,     1] loss: 1247.176
[4,     1] loss: 1233.988
[5,     1] loss: 1232.143
[6,     1] loss: 1232.329
[7,     1] loss: 1219.625
[8,     1] loss: 1205.244
[9,     1] loss: 1185.606
[10,     1] loss: 1148.844
[11,     1] loss: 1105.990
[12,     1] loss: 1077.491
[13,     1] loss: 1027.232
[14,     1] loss: 1120.572
[15,     1] loss: 976.307
[16,     1] loss: 1032.498
[17,     1] loss: 977.692
[18,     1] loss: 1034.892
[19,     1] loss: 1027.917
[20,     1] loss: 977.302
[21,     1] loss: 969.055
[22,     1] loss: 977.416
[23,     1] loss: 965.008
[24,     1] loss: 947.449
[25,     1] loss: 897.248
[26,     1] loss: 948.023
[27,     1] loss: 999.738
[28,     1] loss: 906.110
[29,     1] loss: 899.136
[30,     1] loss: 887.864
[31,     1] loss: 915.892
[32,     1] loss: 845.244
[33,     1] loss: 855.478
[34,     1] loss: 879.460
[35,     1] loss: 883.720
[36,     1] loss: 807.431
[37,     1] loss: 821.557
[38,     1] loss: 787.812
[39,     1] loss: 852.733
[40,     1] loss: 756.405
[41,     1] loss: 810.455
[42,     1] loss: 817.726
[43,     1] loss: 713.439
[44,     1] loss: 747.852
[45,     1] loss: 768.301
[46,     1] loss: 706.816
[47,     1] loss: 705.733
[48,     1] loss: 919.221
[49,     1] loss: 928.555
[50,     1] loss: 735.709
[51,     1] loss: 797.909
[52,     1] loss: 773.136
[53,     1] loss: 803.115
[54,     1] loss: 744.023
[55,     1] loss: 734.325
[56,     1] loss: 737.695
[57,     1] loss: 674.961
[58,     1] loss: 684.307
[59,     1] loss: 767.144
[60,     1] loss: 625.455
[61,     1] loss: 711.339
[62,     1] loss: 746.191
[63,     1] loss: 673.791
[64,     1] loss: 783.479
[65,     1] loss: 673.517
[66,     1] loss: 645.357
[67,     1] loss: 683.892
[68,     1] loss: 592.118
[69,     1] loss: 565.493
[70,     1] loss: 613.733
[71,     1] loss: 883.550
[72,     1] loss: 965.922
[73,     1] loss: 676.003
[74,     1] loss: 837.726
[75,     1] loss: 729.054
[76,     1] loss: 763.375
[77,     1] loss: 798.803
[78,     1] loss: 688.076
[79,     1] loss: 680.391
[80,     1] loss: 757.049
[81,     1] loss: 607.519
[82,     1] loss: 647.440
[83,     1] loss: 638.180
[84,     1] loss: 638.716
[85,     1] loss: 611.805
[86,     1] loss: 552.818
[87,     1] loss: 591.304
[88,     1] loss: 485.754
[89,     1] loss: 506.973
[90,     1] loss: 632.565
[91,     1] loss: 649.894
[92,     1] loss: 690.813
[93,     1] loss: 507.439
[94,     1] loss: 639.845
[95,     1] loss: 485.174
[96,     1] loss: 591.447
[97,     1] loss: 488.688
[98,     1] loss: 464.081
[99,     1] loss: 638.735
[100,     1] loss: 438.460
[101,     1] loss: 405.904
[102,     1] loss: 437.707
[103,     1] loss: 403.271
[104,     1] loss: 371.554
Early stopping applied (best metric=0.3337802588939667)
Finished Training
Total time taken: 16.05878782272339
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1238.673
[2,     1] loss: 1239.908
[3,     1] loss: 1233.531
[4,     1] loss: 1233.073
[5,     1] loss: 1233.783
[6,     1] loss: 1227.207
[7,     1] loss: 1218.641
[8,     1] loss: 1198.178
[9,     1] loss: 1187.555
[10,     1] loss: 1145.302
[11,     1] loss: 1092.443
[12,     1] loss: 1057.119
[13,     1] loss: 1016.619
[14,     1] loss: 1038.579
[15,     1] loss: 1073.797
[16,     1] loss: 1035.479
[17,     1] loss: 952.673
[18,     1] loss: 961.701
[19,     1] loss: 989.326
[20,     1] loss: 1026.389
[21,     1] loss: 947.659
[22,     1] loss: 917.078
[23,     1] loss: 921.988
[24,     1] loss: 904.257
[25,     1] loss: 930.488
[26,     1] loss: 915.335
[27,     1] loss: 885.032
[28,     1] loss: 917.456
[29,     1] loss: 867.311
[30,     1] loss: 900.390
[31,     1] loss: 893.436
[32,     1] loss: 865.200
[33,     1] loss: 885.757
[34,     1] loss: 851.701
[35,     1] loss: 821.016
[36,     1] loss: 887.263
[37,     1] loss: 812.729
[38,     1] loss: 830.616
[39,     1] loss: 835.884
[40,     1] loss: 849.852
[41,     1] loss: 775.215
[42,     1] loss: 813.487
[43,     1] loss: 743.411
[44,     1] loss: 746.311
[45,     1] loss: 786.967
[46,     1] loss: 893.854
[47,     1] loss: 841.568
[48,     1] loss: 692.850
[49,     1] loss: 793.159
[50,     1] loss: 748.394
[51,     1] loss: 765.551
[52,     1] loss: 697.122
[53,     1] loss: 671.097
[54,     1] loss: 731.585
[55,     1] loss: 714.039
[56,     1] loss: 666.635
[57,     1] loss: 690.716
[58,     1] loss: 758.813
[59,     1] loss: 679.994
[60,     1] loss: 612.402
[61,     1] loss: 640.165
[62,     1] loss: 586.486
[63,     1] loss: 632.689
[64,     1] loss: 687.587
[65,     1] loss: 953.287
[66,     1] loss: 607.677
[67,     1] loss: 722.705
[68,     1] loss: 650.692
[69,     1] loss: 711.089
[70,     1] loss: 586.426
[71,     1] loss: 629.988
[72,     1] loss: 555.194
[73,     1] loss: 560.327
[74,     1] loss: 504.734
[75,     1] loss: 496.934
[76,     1] loss: 555.401
[77,     1] loss: 671.024
[78,     1] loss: 882.415
[79,     1] loss: 516.095
[80,     1] loss: 739.933
[81,     1] loss: 590.488
[82,     1] loss: 647.241
[83,     1] loss: 546.762
[84,     1] loss: 609.836
[85,     1] loss: 493.714
[86,     1] loss: 513.961
[87,     1] loss: 486.271
[88,     1] loss: 593.802
[89,     1] loss: 905.086
[90,     1] loss: 444.458
[91,     1] loss: 759.189
[92,     1] loss: 533.298
[93,     1] loss: 762.871
[94,     1] loss: 577.041
[95,     1] loss: 593.716
[96,     1] loss: 533.247
[97,     1] loss: 489.079
[98,     1] loss: 519.759
[99,     1] loss: 412.607
[100,     1] loss: 516.208
[101,     1] loss: 627.793
[102,     1] loss: 440.793
[103,     1] loss: 689.832
[104,     1] loss: 491.645
Early stopping applied (best metric=0.3678344190120697)
Finished Training
Total time taken: 16.4505672454834
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.431
[2,     1] loss: 1238.738
[3,     1] loss: 1236.616
[4,     1] loss: 1235.027
[5,     1] loss: 1231.280
[6,     1] loss: 1232.537
[7,     1] loss: 1235.368
[8,     1] loss: 1231.779
[9,     1] loss: 1229.761
[10,     1] loss: 1223.541
[11,     1] loss: 1216.047
[12,     1] loss: 1198.408
[13,     1] loss: 1160.967
[14,     1] loss: 1141.462
[15,     1] loss: 1109.849
[16,     1] loss: 1092.612
[17,     1] loss: 1043.882
[18,     1] loss: 1068.784
[19,     1] loss: 1015.233
[20,     1] loss: 1014.635
[21,     1] loss: 1004.725
[22,     1] loss: 976.651
[23,     1] loss: 955.119
[24,     1] loss: 959.701
[25,     1] loss: 972.076
[26,     1] loss: 938.206
[27,     1] loss: 945.512
[28,     1] loss: 932.051
[29,     1] loss: 953.824
[30,     1] loss: 872.159
[31,     1] loss: 866.859
[32,     1] loss: 832.665
[33,     1] loss: 925.182
[34,     1] loss: 944.730
[35,     1] loss: 897.916
[36,     1] loss: 933.812
[37,     1] loss: 854.585
[38,     1] loss: 909.167
[39,     1] loss: 828.082
[40,     1] loss: 883.056
[41,     1] loss: 822.201
[42,     1] loss: 929.882
[43,     1] loss: 822.834
[44,     1] loss: 882.129
[45,     1] loss: 768.539
[46,     1] loss: 822.571
[47,     1] loss: 734.745
[48,     1] loss: 817.471
[49,     1] loss: 786.899
[50,     1] loss: 790.712
[51,     1] loss: 737.679
[52,     1] loss: 734.909
[53,     1] loss: 762.032
[54,     1] loss: 638.683
[55,     1] loss: 707.052
[56,     1] loss: 717.149
[57,     1] loss: 821.262
[58,     1] loss: 830.696
[59,     1] loss: 688.169
[60,     1] loss: 792.012
[61,     1] loss: 655.538
[62,     1] loss: 758.912
[63,     1] loss: 680.426
[64,     1] loss: 777.870
[65,     1] loss: 662.116
[66,     1] loss: 721.179
[67,     1] loss: 581.817
[68,     1] loss: 737.957
[69,     1] loss: 610.067
[70,     1] loss: 612.281
[71,     1] loss: 631.942
[72,     1] loss: 534.464
[73,     1] loss: 752.154
[74,     1] loss: 949.519
[75,     1] loss: 587.841
[76,     1] loss: 760.965
[77,     1] loss: 676.845
[78,     1] loss: 653.356
[79,     1] loss: 676.469
[80,     1] loss: 580.372
[81,     1] loss: 667.892
[82,     1] loss: 533.879
[83,     1] loss: 542.378
[84,     1] loss: 519.145
[85,     1] loss: 485.325
[86,     1] loss: 502.085
[87,     1] loss: 612.454
[88,     1] loss: 829.169
[89,     1] loss: 458.724
[90,     1] loss: 586.043
[91,     1] loss: 602.886
[92,     1] loss: 601.852
[93,     1] loss: 623.402
[94,     1] loss: 494.913
[95,     1] loss: 596.610
[96,     1] loss: 423.266
[97,     1] loss: 485.867
[98,     1] loss: 537.629
[99,     1] loss: 440.509
[100,     1] loss: 478.940
[101,     1] loss: 637.196
[102,     1] loss: 399.117
[103,     1] loss: 634.052
[104,     1] loss: 648.959
[105,     1] loss: 547.939
[106,     1] loss: 548.499
[107,     1] loss: 451.648
[108,     1] loss: 477.194
[109,     1] loss: 385.322
[110,     1] loss: 484.274
[111,     1] loss: 689.543
[112,     1] loss: 467.850
[113,     1] loss: 501.295
[114,     1] loss: 483.365
[115,     1] loss: 408.611
[116,     1] loss: 526.205
[117,     1] loss: 729.207
[118,     1] loss: 393.238
[119,     1] loss: 510.280
[120,     1] loss: 560.058
[121,     1] loss: 491.602
[122,     1] loss: 459.403
[123,     1] loss: 391.592
[124,     1] loss: 527.030
[125,     1] loss: 434.521
[126,     1] loss: 339.313
[127,     1] loss: 457.966
Early stopping applied (best metric=0.36153003573417664)
Finished Training
Total time taken: 18.621354579925537
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.650
[2,     1] loss: 1231.586
[3,     1] loss: 1233.547
[4,     1] loss: 1235.888
[5,     1] loss: 1235.291
[6,     1] loss: 1228.693
[7,     1] loss: 1215.821
[8,     1] loss: 1197.608
[9,     1] loss: 1177.431
[10,     1] loss: 1156.393
[11,     1] loss: 1097.762
[12,     1] loss: 1064.373
[13,     1] loss: 1044.911
[14,     1] loss: 1003.455
[15,     1] loss: 1041.025
[16,     1] loss: 1039.793
[17,     1] loss: 1016.608
[18,     1] loss: 969.295
[19,     1] loss: 985.438
[20,     1] loss: 1007.394
[21,     1] loss: 946.198
[22,     1] loss: 959.210
[23,     1] loss: 947.579
[24,     1] loss: 989.526
[25,     1] loss: 962.733
[26,     1] loss: 935.844
[27,     1] loss: 926.907
[28,     1] loss: 868.411
[29,     1] loss: 903.821
[30,     1] loss: 880.430
[31,     1] loss: 872.372
[32,     1] loss: 908.140
[33,     1] loss: 848.831
[34,     1] loss: 951.871
[35,     1] loss: 902.554
[36,     1] loss: 826.773
[37,     1] loss: 831.827
[38,     1] loss: 851.589
[39,     1] loss: 941.058
[40,     1] loss: 819.264
[41,     1] loss: 839.186
[42,     1] loss: 781.496
[43,     1] loss: 754.270
[44,     1] loss: 744.854
[45,     1] loss: 759.110
[46,     1] loss: 782.263
[47,     1] loss: 712.674
[48,     1] loss: 762.337
[49,     1] loss: 966.383
[50,     1] loss: 670.169
[51,     1] loss: 894.060
[52,     1] loss: 705.094
[53,     1] loss: 780.746
[54,     1] loss: 738.505
[55,     1] loss: 759.623
[56,     1] loss: 684.324
[57,     1] loss: 657.714
[58,     1] loss: 639.979
[59,     1] loss: 657.886
[60,     1] loss: 723.941
[61,     1] loss: 705.878
[62,     1] loss: 670.672
[63,     1] loss: 620.860
[64,     1] loss: 793.635
[65,     1] loss: 698.631
[66,     1] loss: 674.005
[67,     1] loss: 616.624
[68,     1] loss: 600.242
[69,     1] loss: 577.556
[70,     1] loss: 534.234
[71,     1] loss: 572.385
[72,     1] loss: 695.480
[73,     1] loss: 826.637
[74,     1] loss: 797.208
[75,     1] loss: 807.719
[76,     1] loss: 671.325
[77,     1] loss: 788.559
[78,     1] loss: 732.562
[79,     1] loss: 685.602
[80,     1] loss: 693.979
[81,     1] loss: 639.031
[82,     1] loss: 625.027
[83,     1] loss: 596.149
[84,     1] loss: 585.383
[85,     1] loss: 571.710
[86,     1] loss: 631.087
[87,     1] loss: 560.516
[88,     1] loss: 451.765
[89,     1] loss: 407.175
[90,     1] loss: 431.744
[91,     1] loss: 505.855
[92,     1] loss: 1098.447
[93,     1] loss: 750.806
[94,     1] loss: 624.120
[95,     1] loss: 678.922
[96,     1] loss: 611.592
[97,     1] loss: 583.728
[98,     1] loss: 622.568
[99,     1] loss: 604.952
Early stopping applied (best metric=0.33848363161087036)
Finished Training
Total time taken: 15.555725574493408
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.747
[2,     1] loss: 1234.591
[3,     1] loss: 1234.539
[4,     1] loss: 1230.511
[5,     1] loss: 1236.122
[6,     1] loss: 1227.239
[7,     1] loss: 1223.498
[8,     1] loss: 1214.808
[9,     1] loss: 1206.240
[10,     1] loss: 1156.630
[11,     1] loss: 1118.608
[12,     1] loss: 1097.900
[13,     1] loss: 1092.456
[14,     1] loss: 1036.142
[15,     1] loss: 1076.176
[16,     1] loss: 1019.827
[17,     1] loss: 979.618
[18,     1] loss: 1064.132
[19,     1] loss: 1019.358
[20,     1] loss: 984.232
[21,     1] loss: 950.169
[22,     1] loss: 980.119
[23,     1] loss: 913.696
[24,     1] loss: 913.556
[25,     1] loss: 929.554
[26,     1] loss: 959.897
[27,     1] loss: 924.518
[28,     1] loss: 848.895
[29,     1] loss: 889.529
[30,     1] loss: 859.431
[31,     1] loss: 854.730
[32,     1] loss: 815.372
[33,     1] loss: 853.800
[34,     1] loss: 899.627
[35,     1] loss: 993.274
[36,     1] loss: 828.142
[37,     1] loss: 881.222
[38,     1] loss: 823.120
[39,     1] loss: 842.635
[40,     1] loss: 822.273
[41,     1] loss: 798.723
[42,     1] loss: 743.425
[43,     1] loss: 736.261
[44,     1] loss: 728.421
[45,     1] loss: 828.371
[46,     1] loss: 705.489
[47,     1] loss: 744.831
[48,     1] loss: 766.589
[49,     1] loss: 691.163
[50,     1] loss: 692.582
[51,     1] loss: 754.086
[52,     1] loss: 717.858
[53,     1] loss: 663.215
[54,     1] loss: 728.725
[55,     1] loss: 954.227
[56,     1] loss: 632.227
[57,     1] loss: 721.224
[58,     1] loss: 669.583
[59,     1] loss: 717.524
[60,     1] loss: 593.665
[61,     1] loss: 629.068
[62,     1] loss: 661.938
[63,     1] loss: 639.161
[64,     1] loss: 810.464
[65,     1] loss: 900.291
[66,     1] loss: 690.365
[67,     1] loss: 804.803
[68,     1] loss: 720.058
[69,     1] loss: 700.716
[70,     1] loss: 744.821
[71,     1] loss: 700.060
[72,     1] loss: 648.251
[73,     1] loss: 669.749
[74,     1] loss: 577.247
[75,     1] loss: 666.625
[76,     1] loss: 651.017
[77,     1] loss: 552.995
[78,     1] loss: 586.721
[79,     1] loss: 503.670
[80,     1] loss: 535.826
[81,     1] loss: 530.561
[82,     1] loss: 442.017
Early stopping applied (best metric=0.32831960916519165)
Finished Training
Total time taken: 11.58959674835205
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.789
[2,     1] loss: 1237.460
[3,     1] loss: 1233.946
[4,     1] loss: 1236.485
[5,     1] loss: 1232.614
[6,     1] loss: 1227.858
[7,     1] loss: 1226.157
[8,     1] loss: 1217.557
[9,     1] loss: 1191.387
[10,     1] loss: 1162.967
[11,     1] loss: 1121.331
[12,     1] loss: 1095.759
[13,     1] loss: 1078.345
[14,     1] loss: 983.630
[15,     1] loss: 1034.799
[16,     1] loss: 1030.174
[17,     1] loss: 1031.562
[18,     1] loss: 1024.108
[19,     1] loss: 1010.385
[20,     1] loss: 994.754
[21,     1] loss: 1027.326
[22,     1] loss: 1010.260
[23,     1] loss: 981.480
[24,     1] loss: 950.085
[25,     1] loss: 932.843
[26,     1] loss: 913.554
[27,     1] loss: 888.448
[28,     1] loss: 923.611
[29,     1] loss: 870.233
[30,     1] loss: 902.163
[31,     1] loss: 933.353
[32,     1] loss: 849.943
[33,     1] loss: 899.953
[34,     1] loss: 870.248
[35,     1] loss: 837.405
[36,     1] loss: 864.104
[37,     1] loss: 881.712
[38,     1] loss: 743.224
[39,     1] loss: 851.459
[40,     1] loss: 794.694
[41,     1] loss: 782.366
[42,     1] loss: 876.484
[43,     1] loss: 783.332
[44,     1] loss: 771.035
[45,     1] loss: 859.440
[46,     1] loss: 735.993
[47,     1] loss: 700.926
[48,     1] loss: 801.487
[49,     1] loss: 734.714
[50,     1] loss: 682.368
[51,     1] loss: 736.671
[52,     1] loss: 692.681
[53,     1] loss: 676.354
[54,     1] loss: 643.645
[55,     1] loss: 661.875
[56,     1] loss: 721.083
[57,     1] loss: 857.949
[58,     1] loss: 681.947
[59,     1] loss: 724.289
[60,     1] loss: 702.820
[61,     1] loss: 761.715
[62,     1] loss: 671.836
[63,     1] loss: 622.636
[64,     1] loss: 674.774
[65,     1] loss: 554.181
[66,     1] loss: 590.502
[67,     1] loss: 952.689
[68,     1] loss: 565.670
[69,     1] loss: 783.090
[70,     1] loss: 695.532
[71,     1] loss: 842.166
[72,     1] loss: 682.021
[73,     1] loss: 729.635
[74,     1] loss: 701.479
[75,     1] loss: 624.135
[76,     1] loss: 701.893
[77,     1] loss: 536.347
[78,     1] loss: 706.250
[79,     1] loss: 648.476
[80,     1] loss: 543.861
[81,     1] loss: 571.371
[82,     1] loss: 473.817
[83,     1] loss: 524.087
[84,     1] loss: 458.379
[85,     1] loss: 541.031
[86,     1] loss: 889.201
[87,     1] loss: 468.522
[88,     1] loss: 636.127
[89,     1] loss: 732.039
[90,     1] loss: 681.722
[91,     1] loss: 543.493
[92,     1] loss: 648.276
[93,     1] loss: 522.845
[94,     1] loss: 660.786
[95,     1] loss: 496.016
[96,     1] loss: 580.454
[97,     1] loss: 568.215
[98,     1] loss: 550.611
[99,     1] loss: 544.924
[100,     1] loss: 426.793
[101,     1] loss: 621.830
[102,     1] loss: 441.970
[103,     1] loss: 398.515
[104,     1] loss: 388.281
[105,     1] loss: 428.788
[106,     1] loss: 372.059
[107,     1] loss: 329.222
[108,     1] loss: 343.552
[109,     1] loss: 434.761
[110,     1] loss: 622.164
[111,     1] loss: 1887.444
[112,     1] loss: 668.910
[113,     1] loss: 1050.795
[114,     1] loss: 1018.489
[115,     1] loss: 993.453
[116,     1] loss: 948.463
[117,     1] loss: 903.225
[118,     1] loss: 834.121
[119,     1] loss: 835.942
[120,     1] loss: 864.456
[121,     1] loss: 824.570
[122,     1] loss: 802.100
[123,     1] loss: 771.570
[124,     1] loss: 768.005
[125,     1] loss: 714.148
[126,     1] loss: 762.712
[127,     1] loss: 715.750
[128,     1] loss: 702.214
[129,     1] loss: 653.949
[130,     1] loss: 882.668
[131,     1] loss: 1405.954
[132,     1] loss: 1128.507
[133,     1] loss: 880.246
[134,     1] loss: 1005.042
[135,     1] loss: 1055.587
[136,     1] loss: 936.975
[137,     1] loss: 924.927
[138,     1] loss: 880.271
[139,     1] loss: 878.667
[140,     1] loss: 881.000
[141,     1] loss: 903.378
[142,     1] loss: 847.804
[143,     1] loss: 812.130
[144,     1] loss: 800.599
[145,     1] loss: 802.467
[146,     1] loss: 792.864
[147,     1] loss: 726.748
[148,     1] loss: 758.478
[149,     1] loss: 674.922
[150,     1] loss: 615.192
Early stopping applied (best metric=0.3096727430820465)
Finished Training
Total time taken: 23.840638399124146
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1234.805
[2,     1] loss: 1241.266
[3,     1] loss: 1236.849
[4,     1] loss: 1234.572
[5,     1] loss: 1237.599
[6,     1] loss: 1231.997
[7,     1] loss: 1233.308
[8,     1] loss: 1224.504
[9,     1] loss: 1210.996
[10,     1] loss: 1199.527
[11,     1] loss: 1142.101
[12,     1] loss: 1117.550
[13,     1] loss: 1104.001
[14,     1] loss: 1061.818
[15,     1] loss: 1150.239
[16,     1] loss: 1025.375
[17,     1] loss: 1053.778
[18,     1] loss: 992.655
[19,     1] loss: 1008.442
[20,     1] loss: 1029.809
[21,     1] loss: 961.491
[22,     1] loss: 982.323
[23,     1] loss: 1011.010
[24,     1] loss: 1025.515
[25,     1] loss: 940.715
[26,     1] loss: 928.736
[27,     1] loss: 959.679
[28,     1] loss: 932.869
[29,     1] loss: 946.578
[30,     1] loss: 889.139
[31,     1] loss: 876.856
[32,     1] loss: 917.260
[33,     1] loss: 872.478
[34,     1] loss: 935.500
[35,     1] loss: 863.428
[36,     1] loss: 914.995
[37,     1] loss: 823.949
[38,     1] loss: 878.851
[39,     1] loss: 814.382
[40,     1] loss: 843.081
[41,     1] loss: 796.531
[42,     1] loss: 808.386
[43,     1] loss: 899.612
[44,     1] loss: 815.129
[45,     1] loss: 817.122
[46,     1] loss: 788.507
[47,     1] loss: 765.010
[48,     1] loss: 761.105
[49,     1] loss: 713.355
[50,     1] loss: 798.377
[51,     1] loss: 769.576
[52,     1] loss: 716.676
[53,     1] loss: 783.648
[54,     1] loss: 774.039
[55,     1] loss: 704.081
[56,     1] loss: 768.882
[57,     1] loss: 640.436
[58,     1] loss: 664.244
[59,     1] loss: 639.527
[60,     1] loss: 593.731
[61,     1] loss: 567.526
[62,     1] loss: 651.094
[63,     1] loss: 873.968
[64,     1] loss: 1953.582
[65,     1] loss: 857.260
[66,     1] loss: 839.318
[67,     1] loss: 1078.161
[68,     1] loss: 1003.518
[69,     1] loss: 985.999
[70,     1] loss: 1035.466
[71,     1] loss: 1049.236
[72,     1] loss: 1021.762
[73,     1] loss: 977.355
[74,     1] loss: 912.004
[75,     1] loss: 918.084
[76,     1] loss: 943.681
[77,     1] loss: 890.793
[78,     1] loss: 928.857
[79,     1] loss: 842.456
[80,     1] loss: 832.010
[81,     1] loss: 771.153
[82,     1] loss: 802.373
Early stopping applied (best metric=0.3786059319972992)
Finished Training
Total time taken: 12.02406096458435
{'Hydroxylation-K Validation Accuracy': 0.7650709219858156, 'Hydroxylation-K Validation Sensitivity': 0.6237037037037036, 'Hydroxylation-K Validation Specificity': 0.8, 'Hydroxylation-K Validation Precision': 0.4630180766945473, 'Hydroxylation-K AUC ROC': 0.7812475633528265, 'Hydroxylation-K AUC PR': 0.5678235903721202, 'Hydroxylation-K MCC': 0.38847433210392074, 'Hydroxylation-K F1': 0.5204176694922645, 'Validation Loss (Hydroxylation-K)': 0.47358145713806155, 'Hydroxylation-P Validation Accuracy': 0.7879409674635806, 'Hydroxylation-P Validation Sensitivity': 0.8223280423280424, 'Hydroxylation-P Validation Specificity': 0.7806399321661929, 'Hydroxylation-P Validation Precision': 0.4491055081461165, 'Hydroxylation-P AUC ROC': 0.8710397383553152, 'Hydroxylation-P AUC PR': 0.6291050135934028, 'Hydroxylation-P MCC': 0.49260940905921624, 'Hydroxylation-P F1': 0.5792561954319904, 'Validation Loss (Hydroxylation-P)': 0.34680365522702533, 'Validation Loss (total)': 0.8203851143519084, 'TimeToTrain': 15.130966472625733}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00686200079035891,
 'learning_rate_Hydroxylation-K': 0.009220624930311883,
 'learning_rate_Hydroxylation-P': 0.005049685679770028,
 'log_base': 1.403891230550707,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2791538141,
 'sample_weights': [1.5526599470505313, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.881187020211499,
 'weight_decay_Hydroxylation-K': 5.7571624594117266,
 'weight_decay_Hydroxylation-P': 4.000403826771251}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1945.748
[2,     1] loss: 1949.333
[3,     1] loss: 1943.156
[4,     1] loss: 1948.646
[5,     1] loss: 1945.217
[6,     1] loss: 1943.220
[7,     1] loss: 1945.796
[8,     1] loss: 1931.744
[9,     1] loss: 1926.909
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005256450743802203,
 'learning_rate_Hydroxylation-K': 0.006004281399845401,
 'learning_rate_Hydroxylation-P': 0.007037618734930071,
 'log_base': 1.084159129095028,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4048122260,
 'sample_weights': [4.921013467106684, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.088245030268619,
 'weight_decay_Hydroxylation-K': 2.8397378427588444,
 'weight_decay_Hydroxylation-P': 5.40563134731977}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6717.627
[2,     1] loss: 6757.113
[3,     1] loss: 6719.153
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005366404157105675,
 'learning_rate_Hydroxylation-K': 0.00810434478724775,
 'learning_rate_Hydroxylation-P': 0.003344201254276806,
 'log_base': 1.2087719620178885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 674892814,
 'sample_weights': [20.66022580110197, 2.582627714495454],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.112601986508071,
 'weight_decay_Hydroxylation-K': 0.033138808281176324,
 'weight_decay_Hydroxylation-P': 0.21986342575720852}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2859.213
[2,     1] loss: 2865.919
[3,     1] loss: 2864.225
[4,     1] loss: 2856.571
[5,     1] loss: 2860.201
[6,     1] loss: 2846.086
[7,     1] loss: 2833.123
[8,     1] loss: 2811.580
[9,     1] loss: 2773.601
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003909516191261265,
 'learning_rate_Hydroxylation-K': 0.0058569102630420455,
 'learning_rate_Hydroxylation-P': 0.001995028843995323,
 'log_base': 1.4179993855368294,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1198929993,
 'sample_weights': [8.804850630154458, 1.1006487285447182],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.292950750007028,
 'weight_decay_Hydroxylation-K': 2.1347579989678787,
 'weight_decay_Hydroxylation-P': 8.077374154021017}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1925.337
[2,     1] loss: 1914.052
[3,     1] loss: 1914.702
[4,     1] loss: 1923.663
[5,     1] loss: 1917.475
[6,     1] loss: 1915.145
[7,     1] loss: 1918.175
[8,     1] loss: 1918.428
[9,     1] loss: 1913.738
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005965278297527219,
 'learning_rate_Hydroxylation-K': 0.0012932567943793006,
 'learning_rate_Hydroxylation-P': 0.0065647884073361615,
 'log_base': 2.8420654174097093,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2812435084,
 'sample_weights': [4.780121725829606, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.451835385137024,
 'weight_decay_Hydroxylation-K': 2.3426393919317965,
 'weight_decay_Hydroxylation-P': 4.537031465171822}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.389
[2,     1] loss: 1249.874
[3,     1] loss: 1245.983
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006749415506508767,
 'learning_rate_Hydroxylation-K': 0.002725201173002149,
 'learning_rate_Hydroxylation-P': 0.0020199511561854516,
 'log_base': 2.3137987300442457,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3163905032,
 'sample_weights': [1.5982704883885628, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.902418558176926,
 'weight_decay_Hydroxylation-K': 5.426158260522621,
 'weight_decay_Hydroxylation-P': 4.839314065668681}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.646
[2,     1] loss: 1328.072
[3,     1] loss: 1328.822
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007564195633846222,
 'learning_rate_Hydroxylation-K': 0.007240978063847491,
 'learning_rate_Hydroxylation-P': 0.007148220303469733,
 'log_base': 1.4861638997429198,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3753215967,
 'sample_weights': [1.9900605113186696, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.778801018116505,
 'weight_decay_Hydroxylation-K': 1.8903422280598454,
 'weight_decay_Hydroxylation-P': 3.569031530825634}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1804.628
[2,     1] loss: 1787.859
[3,     1] loss: 1846.460
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009729028426557753,
 'learning_rate_Hydroxylation-K': 0.0001406723246427594,
 'learning_rate_Hydroxylation-P': 0.0076868047012131555,
 'log_base': 1.2434323057693735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 234691487,
 'sample_weights': [4.213656183075595, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.70972132408822,
 'weight_decay_Hydroxylation-K': 0.7107592053747807,
 'weight_decay_Hydroxylation-P': 0.06307148887803216}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2525.307
[2,     1] loss: 2528.138
[3,     1] loss: 2520.081
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00509255723452217,
 'learning_rate_Hydroxylation-K': 0.0037217724217555623,
 'learning_rate_Hydroxylation-P': 0.003350838663669519,
 'log_base': 2.6404439278510417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 514172779,
 'sample_weights': [7.6623705183276085, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.713908005990203,
 'weight_decay_Hydroxylation-K': 7.312946092343457,
 'weight_decay_Hydroxylation-P': 3.2525103784764}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.794
[2,     1] loss: 1284.038
[3,     1] loss: 1269.863
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008977379354152213,
 'learning_rate_Hydroxylation-K': 0.009959772146306634,
 'learning_rate_Hydroxylation-P': 0.0037209412294586268,
 'log_base': 1.3528875290163103,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1977044016,
 'sample_weights': [1.7193966802921952, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.81250841414792,
 'weight_decay_Hydroxylation-K': 3.461124238408245,
 'weight_decay_Hydroxylation-P': 6.9587926784729435}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2065.860
[2,     1] loss: 2065.855
[3,     1] loss: 2096.045
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008133256867786211,
 'learning_rate_Hydroxylation-K': 0.007957395791581839,
 'learning_rate_Hydroxylation-P': 0.0015710759783550668,
 'log_base': 1.188676249754727,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2489591714,
 'sample_weights': [5.523545580722367, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.18610890774177,
 'weight_decay_Hydroxylation-K': 2.9301234418049926,
 'weight_decay_Hydroxylation-P': 4.672594787776804}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3143.731
[2,     1] loss: 3157.342
[3,     1] loss: 3130.766
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008285838798981662,
 'learning_rate_Hydroxylation-K': 0.0052234126572390125,
 'learning_rate_Hydroxylation-P': 0.006237853916640343,
 'log_base': 1.185492379646355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3633463865,
 'sample_weights': [9.658877108391147, 1.2074061508904788],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.348152227923262,
 'weight_decay_Hydroxylation-K': 2.0251147083034833,
 'weight_decay_Hydroxylation-P': 4.437280706405362}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3210.009
[2,     1] loss: 3214.562
[3,     1] loss: 3188.884
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007063847835861291,
 'learning_rate_Hydroxylation-K': 0.0032961469379914807,
 'learning_rate_Hydroxylation-P': 0.008091881203854769,
 'log_base': 1.055010470016196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3161317795,
 'sample_weights': [9.811123776463603, 1.2264377175436632],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.3873593662418235,
 'weight_decay_Hydroxylation-K': 8.150171092716494,
 'weight_decay_Hydroxylation-P': 4.152913748994274}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10201.123
[2,     1] loss: 10157.920
[3,     1] loss: 10115.097
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005175430725306097,
 'learning_rate_Hydroxylation-K': 0.0026526784914248113,
 'learning_rate_Hydroxylation-P': 0.004479444128468657,
 'log_base': 2.9258088131291053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 431477795,
 'sample_weights': [31.17500660084839, 3.8970259484112395],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.282771733812792,
 'weight_decay_Hydroxylation-K': 1.69066034751315,
 'weight_decay_Hydroxylation-P': 3.8316187117141163}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.753
[2,     1] loss: 1234.273
[3,     1] loss: 1239.623
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009531944914813838,
 'learning_rate_Hydroxylation-K': 0.009871679140651178,
 'learning_rate_Hydroxylation-P': 0.0005796390828863099,
 'log_base': 1.014350451089606,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1444656089,
 'sample_weights': [1.5550375429145429, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.270399356147011,
 'weight_decay_Hydroxylation-K': 7.202561487405003,
 'weight_decay_Hydroxylation-P': 2.0905819293009893}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37973.340
Exploding loss, terminate run (best metric=0.5311239957809448)
Finished Training
Total time taken: 0.22202706336975098
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37902.695
Exploding loss, terminate run (best metric=0.5422968864440918)
Finished Training
Total time taken: 0.20300030708312988
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38188.328
Exploding loss, terminate run (best metric=0.5387477874755859)
Finished Training
Total time taken: 0.22800111770629883
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38369.105
Exploding loss, terminate run (best metric=0.5287234783172607)
Finished Training
Total time taken: 0.23400068283081055
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 38110.387
Exploding loss, terminate run (best metric=0.5339576601982117)
Finished Training
Total time taken: 0.23700308799743652
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38035.562
Exploding loss, terminate run (best metric=0.5322791934013367)
Finished Training
Total time taken: 0.22500061988830566
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37868.316
Exploding loss, terminate run (best metric=0.5350753664970398)
Finished Training
Total time taken: 0.2030029296875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38126.137
Exploding loss, terminate run (best metric=0.5284392237663269)
Finished Training
Total time taken: 0.22700285911560059
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38103.109
Exploding loss, terminate run (best metric=0.5286283493041992)
Finished Training
Total time taken: 0.2310025691986084
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 38347.258
Exploding loss, terminate run (best metric=0.5289966464042664)
Finished Training
Total time taken: 0.199324369430542
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37960.742
Exploding loss, terminate run (best metric=0.5349902510643005)
Finished Training
Total time taken: 0.2050027847290039
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37995.516
Exploding loss, terminate run (best metric=0.5270115733146667)
Finished Training
Total time taken: 0.21699905395507812
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 38144.371
Exploding loss, terminate run (best metric=0.5305915474891663)
Finished Training
Total time taken: 0.22900128364562988
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 37910.922
Exploding loss, terminate run (best metric=0.5267310738563538)
Finished Training
Total time taken: 0.20600056648254395
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 37921.688
Exploding loss, terminate run (best metric=0.5283945202827454)
Finished Training
Total time taken: 0.22100067138671875
{'Hydroxylation-K Validation Accuracy': 0.39828605200945627, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5904093567251463, 'Hydroxylation-K AUC PR': 0.3248674621057812, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22208538587848933, 'Validation Loss (Hydroxylation-K)': 0.5590131481488546, 'Hydroxylation-P Validation Accuracy': 0.39229372451483, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5339258241910494, 'Hydroxylation-P AUC PR': 0.23987190709417905, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20039331036639543, 'Validation Loss (Hydroxylation-P)': 0.5317325035730998, 'Validation Loss (total)': 1.0907456636428834, 'TimeToTrain': 0.21915799776713055}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003553222169119276,
 'learning_rate_Hydroxylation-K': 0.0011765864356152672,
 'learning_rate_Hydroxylation-P': 0.00635054881483411,
 'log_base': 2.600264603132437,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 215650337,
 'sample_weights': [117.25349794506235, 14.626233236589504],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.972977513020085,
 'weight_decay_Hydroxylation-K': 1.9657086626971547,
 'weight_decay_Hydroxylation-P': 1.4694373070984215}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.820
[2,     1] loss: 1278.221
[3,     1] loss: 1274.434
[4,     1] loss: 1281.795
[5,     1] loss: 1274.426
[6,     1] loss: 1271.468
[7,     1] loss: 1272.114
[8,     1] loss: 1264.054
[9,     1] loss: 1254.806
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033793889484143415,
 'learning_rate_Hydroxylation-K': 0.0014860744247132802,
 'learning_rate_Hydroxylation-P': 0.008253785328012297,
 'log_base': 2.969308539572481,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4043425960,
 'sample_weights': [1.746986259143753, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.290294113603091,
 'weight_decay_Hydroxylation-K': 1.1028281177803332,
 'weight_decay_Hydroxylation-P': 7.862532038603307}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.910
[2,     1] loss: 1239.103
[3,     1] loss: 1230.387
[4,     1] loss: 1227.829
[5,     1] loss: 1228.473
[6,     1] loss: 1224.403
[7,     1] loss: 1217.049
[8,     1] loss: 1209.054
[9,     1] loss: 1194.102
[10,     1] loss: 1164.892
[11,     1] loss: 1159.566
[12,     1] loss: 1092.402
[13,     1] loss: 1089.363
[14,     1] loss: 1017.914
[15,     1] loss: 1064.998
[16,     1] loss: 1056.179
[17,     1] loss: 1011.154
[18,     1] loss: 1021.887
[19,     1] loss: 1019.003
[20,     1] loss: 994.856
[21,     1] loss: 982.633
[22,     1] loss: 961.557
[23,     1] loss: 992.825
[24,     1] loss: 996.368
[25,     1] loss: 986.183
[26,     1] loss: 973.959
[27,     1] loss: 947.317
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00843468622370647,
 'learning_rate_Hydroxylation-K': 0.009959407881352439,
 'learning_rate_Hydroxylation-P': 0.0008093586281046717,
 'log_base': 1.092019104572338,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 275490345,
 'sample_weights': [1.5339506505278024, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.531970338020928,
 'weight_decay_Hydroxylation-K': 5.94185778694245,
 'weight_decay_Hydroxylation-P': 1.7999963015753264}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6151.084
[2,     1] loss: 6159.503
[3,     1] loss: 6160.243
[4,     1] loss: 6147.644
[5,     1] loss: 6159.989
[6,     1] loss: 6177.495
[7,     1] loss: 6162.804
[8,     1] loss: 6141.585
[9,     1] loss: 6156.179
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028639532046276254,
 'learning_rate_Hydroxylation-K': 0.0003198622233315421,
 'learning_rate_Hydroxylation-P': 0.006627198104306513,
 'log_base': 2.548215120805046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2552313388,
 'sample_weights': [18.96483038041599, 2.370695122729746],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.796382792703113,
 'weight_decay_Hydroxylation-K': 6.060334845068228,
 'weight_decay_Hydroxylation-P': 1.902496443162455}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.405
[2,     1] loss: 1290.990
[3,     1] loss: 1281.152
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009076471933289233,
 'learning_rate_Hydroxylation-K': 0.007258166832091661,
 'learning_rate_Hydroxylation-P': 0.00312706212910892,
 'log_base': 1.0187474885266328,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3115195861,
 'sample_weights': [1.7847502164880236, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.344189332440738,
 'weight_decay_Hydroxylation-K': 6.528740649365372,
 'weight_decay_Hydroxylation-P': 0.5191129749619234}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29458.205
Exploding loss, terminate run (best metric=0.5342456698417664)
Finished Training
Total time taken: 0.22499966621398926
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29218.590
Exploding loss, terminate run (best metric=0.5309754014015198)
Finished Training
Total time taken: 0.22499871253967285
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29290.730
Exploding loss, terminate run (best metric=0.5279091000556946)
Finished Training
Total time taken: 0.22700071334838867
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29235.477
Exploding loss, terminate run (best metric=0.5418129563331604)
Finished Training
Total time taken: 0.2049999237060547
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29237.133
Exploding loss, terminate run (best metric=0.5320829749107361)
Finished Training
Total time taken: 0.22900080680847168
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29178.129
Exploding loss, terminate run (best metric=0.535207211971283)
Finished Training
Total time taken: 0.20200085639953613
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29255.791
Exploding loss, terminate run (best metric=0.5297690629959106)
Finished Training
Total time taken: 0.22699856758117676
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29224.199
Exploding loss, terminate run (best metric=0.5272533893585205)
Finished Training
Total time taken: 0.22300076484680176
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29211.801
Exploding loss, terminate run (best metric=0.5312052965164185)
Finished Training
Total time taken: 0.20800042152404785
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29267.145
Exploding loss, terminate run (best metric=0.5296614766120911)
Finished Training
Total time taken: 0.22100043296813965
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29099.312
Exploding loss, terminate run (best metric=0.5326261520385742)
Finished Training
Total time taken: 0.2220015525817871
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29635.199
Exploding loss, terminate run (best metric=0.5281615853309631)
Finished Training
Total time taken: 0.220001220703125
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29117.785
Exploding loss, terminate run (best metric=0.5286189913749695)
Finished Training
Total time taken: 0.207000732421875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29078.705
Exploding loss, terminate run (best metric=0.5350185036659241)
Finished Training
Total time taken: 0.22899866104125977
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29279.934
Exploding loss, terminate run (best metric=0.6099411249160767)
Finished Training
Total time taken: 0.2310011386871338
{'Hydroxylation-K Validation Accuracy': 0.4050236406619385, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6083430799220273, 'Hydroxylation-K AUC PR': 0.3470483533554564, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22676518883415436, 'Validation Loss (Hydroxylation-K)': 0.5667306184768677, 'Hydroxylation-P Validation Accuracy': 0.39272470094580647, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5727310856259308, 'Hydroxylation-P AUC PR': 0.26766217894795996, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20070368111395248, 'Validation Loss (Hydroxylation-P)': 0.5369659264882406, 'Validation Loss (total)': 1.1036965449651082, 'TimeToTrain': 0.22006694475809732}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008725813447288339,
 'learning_rate_Hydroxylation-K': 0.009360823220008302,
 'learning_rate_Hydroxylation-P': 0.004448912783065216,
 'log_base': 1.3205371940229516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3264645375,
 'sample_weights': [89.94770672505544, 11.220101410308605],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.748193216514294,
 'weight_decay_Hydroxylation-K': 6.3412137528245465,
 'weight_decay_Hydroxylation-P': 2.254188176929209}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2185.734
[2,     1] loss: 2167.232
[3,     1] loss: 2209.977
[4,     1] loss: 2187.960
[5,     1] loss: 2179.730
[6,     1] loss: 2181.655
[7,     1] loss: 2173.596
[8,     1] loss: 2174.840
[9,     1] loss: 2171.725
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007329075042895336,
 'learning_rate_Hydroxylation-K': 0.003259995285439331,
 'learning_rate_Hydroxylation-P': 0.008461335554435703,
 'log_base': 2.5265129949141993,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3281627832,
 'sample_weights': [6.004357069360401, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7458285990854385,
 'weight_decay_Hydroxylation-K': 9.737897965713994,
 'weight_decay_Hydroxylation-P': 6.846084759454287}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.685
[2,     1] loss: 1288.724
[3,     1] loss: 1284.072
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007160501056470096,
 'learning_rate_Hydroxylation-K': 0.007979135608145642,
 'learning_rate_Hydroxylation-P': 0.0024680209666931878,
 'log_base': 1.206347421539166,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 45740864,
 'sample_weights': [1.8012202610889472, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.126289896735104,
 'weight_decay_Hydroxylation-K': 2.0337769083876482,
 'weight_decay_Hydroxylation-P': 2.7449100438434324}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2898.022
[2,     1] loss: 2901.659
[3,     1] loss: 2900.702
[4,     1] loss: 2911.335
[5,     1] loss: 2889.096
[6,     1] loss: 2899.192
[7,     1] loss: 2886.068
[8,     1] loss: 2883.076
[9,     1] loss: 2875.441
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007548626551876915,
 'learning_rate_Hydroxylation-K': 0.009533177274580286,
 'learning_rate_Hydroxylation-P': 0.0015330237585487031,
 'log_base': 1.148172573379385,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2384696784,
 'sample_weights': [8.899086614196198, 1.1124286803435008],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.647800560883165,
 'weight_decay_Hydroxylation-K': 8.358067494764406,
 'weight_decay_Hydroxylation-P': 3.941820601256102}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3922.227
[2,     1] loss: 3932.987
[3,     1] loss: 4024.263
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0072592077944971896,
 'learning_rate_Hydroxylation-K': 0.006590929201421505,
 'learning_rate_Hydroxylation-P': 0.006045847321619839,
 'log_base': 1.161663280634569,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2758240708,
 'sample_weights': [12.082388886945848, 1.5103567936355207],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.349735122518164,
 'weight_decay_Hydroxylation-K': 3.402746292033658,
 'weight_decay_Hydroxylation-P': 5.488122276000133}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3633.051
[2,     1] loss: 3639.638
[3,     1] loss: 3623.861
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000567927876796691,
 'learning_rate_Hydroxylation-K': 0.004661643780226938,
 'learning_rate_Hydroxylation-P': 0.009778290705072912,
 'log_base': 1.2432342740115885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 814685731,
 'sample_weights': [11.140550551038604, 1.3926224662227686],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.176307850520877,
 'weight_decay_Hydroxylation-K': 7.3909549775106225,
 'weight_decay_Hydroxylation-P': 2.288375496966167}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2525.234
[2,     1] loss: 2519.344
[3,     1] loss: 2533.402
[4,     1] loss: 2535.265
[5,     1] loss: 2532.874
[6,     1] loss: 2520.999
[7,     1] loss: 2527.764
[8,     1] loss: 2524.262
[9,     1] loss: 2528.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00640547662962445,
 'learning_rate_Hydroxylation-K': 0.009269584953332008,
 'learning_rate_Hydroxylation-P': 0.0029300428075716037,
 'log_base': 1.01147944896534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3808142570,
 'sample_weights': [7.66797608530492, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.211807651056434,
 'weight_decay_Hydroxylation-K': 0.006088149636382434,
 'weight_decay_Hydroxylation-P': 0.10781834388443645}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47340.539
Exploding loss, terminate run (best metric=0.5326226949691772)
Finished Training
Total time taken: 0.2019972801208496
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47366.656
Exploding loss, terminate run (best metric=0.5429071187973022)
Finished Training
Total time taken: 0.22799992561340332
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47877.062
Exploding loss, terminate run (best metric=0.5268433690071106)
Finished Training
Total time taken: 0.23000097274780273
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47267.734
Exploding loss, terminate run (best metric=0.5382344722747803)
Finished Training
Total time taken: 0.2070019245147705
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47400.219
Exploding loss, terminate run (best metric=0.5326824188232422)
Finished Training
Total time taken: 0.23000121116638184
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47671.406
Exploding loss, terminate run (best metric=0.5721710920333862)
Finished Training
Total time taken: 0.23400235176086426
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47386.875
Exploding loss, terminate run (best metric=0.5371394753456116)
Finished Training
Total time taken: 0.2220015525817871
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47550.559
Exploding loss, terminate run (best metric=0.5444247722625732)
Finished Training
Total time taken: 0.2090005874633789
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47310.258
Exploding loss, terminate run (best metric=0.5252878069877625)
Finished Training
Total time taken: 0.2259979248046875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47589.516
Exploding loss, terminate run (best metric=0.5504816770553589)
Finished Training
Total time taken: 0.22499918937683105
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47459.270
Exploding loss, terminate run (best metric=0.5320455431938171)
Finished Training
Total time taken: 0.22900056838989258
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47450.477
Exploding loss, terminate run (best metric=0.5704197883605957)
Finished Training
Total time taken: 0.20600271224975586
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47713.766
Exploding loss, terminate run (best metric=0.5360433459281921)
Finished Training
Total time taken: 0.22800087928771973
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47767.781
Exploding loss, terminate run (best metric=0.5418155789375305)
Finished Training
Total time taken: 0.22800064086914062
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47925.766
Exploding loss, terminate run (best metric=0.5278559327125549)
Finished Training
Total time taken: 0.20199823379516602
{'Hydroxylation-K Validation Accuracy': 0.5994680851063829, 'Hydroxylation-K Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-K Validation Specificity': 0.6666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5889278752436647, 'Hydroxylation-K AUC PR': 0.32770621924735543, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.11182266009852218, 'Validation Loss (Hydroxylation-K)': 0.5619158466657003, 'Hydroxylation-P Validation Accuracy': 0.6082556892205133, 'Hydroxylation-P Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-P Validation Specificity': 0.6666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5520566160525759, 'Hydroxylation-P AUC PR': 0.24564756836045457, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.10059261829102523, 'Validation Loss (Hydroxylation-P)': 0.540731672445933, 'Validation Loss (total)': 1.1026475111643472, 'TimeToTrain': 0.2204003969828288}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005653592439775685,
 'learning_rate_Hydroxylation-K': 0.009849533757655779,
 'learning_rate_Hydroxylation-P': 0.003740599209264889,
 'log_base': 1.0286673998107616,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1529735183,
 'sample_weights': [146.37048803152632, 18.25829450227237],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.167634419783468,
 'weight_decay_Hydroxylation-K': 6.521985991575159,
 'weight_decay_Hydroxylation-P': 1.0810412980608959}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19317.438
Exploding loss, terminate run (best metric=0.532141387462616)
Finished Training
Total time taken: 0.22200298309326172
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19120.789
Exploding loss, terminate run (best metric=0.5307298898696899)
Finished Training
Total time taken: 0.2350006103515625
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19300.689
Exploding loss, terminate run (best metric=0.5345289707183838)
Finished Training
Total time taken: 0.21899867057800293
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19266.840
Exploding loss, terminate run (best metric=0.5301173329353333)
Finished Training
Total time taken: 0.20900249481201172
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19152.834
Exploding loss, terminate run (best metric=0.5335493087768555)
Finished Training
Total time taken: 0.23199844360351562
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19215.713
Exploding loss, terminate run (best metric=0.532624363899231)
Finished Training
Total time taken: 0.21400046348571777
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19155.227
Exploding loss, terminate run (best metric=0.5293341279029846)
Finished Training
Total time taken: 0.23199868202209473
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19201.781
Exploding loss, terminate run (best metric=0.5308822989463806)
Finished Training
Total time taken: 0.2409038543701172
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19233.746
Exploding loss, terminate run (best metric=0.5272501707077026)
Finished Training
Total time taken: 0.2417125701904297
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19196.588
Exploding loss, terminate run (best metric=0.5283467173576355)
Finished Training
Total time taken: 0.21397829055786133
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19134.869
Exploding loss, terminate run (best metric=0.5314993262290955)
Finished Training
Total time taken: 0.20599842071533203
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19301.373
Exploding loss, terminate run (best metric=0.5304622650146484)
Finished Training
Total time taken: 0.23000144958496094
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19056.234
Exploding loss, terminate run (best metric=0.528046727180481)
Finished Training
Total time taken: 0.23803377151489258
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19260.771
Exploding loss, terminate run (best metric=0.5297561287879944)
Finished Training
Total time taken: 0.2099621295928955
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19194.742
Exploding loss, terminate run (best metric=0.5273634791374207)
Finished Training
Total time taken: 0.2050027847290039
{'Hydroxylation-K Validation Accuracy': 0.4894208037825059, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4807017543859649, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6745614035087719, 'Hydroxylation-K AUC PR': 0.4075383003485762, 'Hydroxylation-K MCC': 0.027799715875866927, 'Hydroxylation-K F1': 0.19912424740010948, 'Validation Loss (Hydroxylation-K)': 0.5576874852180481, 'Hydroxylation-P Validation Accuracy': 0.4879436407627362, 'Hydroxylation-P Validation Sensitivity': 0.5334391534391535, 'Hydroxylation-P Validation Specificity': 0.4772357723577236, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5967063205068218, 'Hydroxylation-P AUC PR': 0.27703352625705857, 'Hydroxylation-P MCC': 0.018566998876449102, 'Hydroxylation-P F1': 0.16948195542758052, 'Validation Loss (Hydroxylation-P)': 0.5304421663284302, 'Validation Loss (total)': 1.0881296555201212, 'TimeToTrain': 0.22323970794677733}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008513651780082829,
 'learning_rate_Hydroxylation-K': 0.00848567014258868,
 'learning_rate_Hydroxylation-P': 0.008689978403882298,
 'log_base': 1.0798411028736559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2335458154,
 'sample_weights': [59.10950385772092, 7.373335594023701],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.696018884929349,
 'weight_decay_Hydroxylation-K': 1.7986774360925337,
 'weight_decay_Hydroxylation-P': 4.526394148889785}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7047.894
[2,     1] loss: 7074.228
[3,     1] loss: 7174.734
[4,     1] loss: 7094.274
[5,     1] loss: 7027.736
[6,     1] loss: 7091.661
[7,     1] loss: 7050.998
[8,     1] loss: 7067.287
[9,     1] loss: 7055.717
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005839097094677228,
 'learning_rate_Hydroxylation-K': 0.007967044615879484,
 'learning_rate_Hydroxylation-P': 0.0035755187205108955,
 'log_base': 1.2368317623257814,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1489649648,
 'sample_weights': [21.733606479647403, 2.7168054681803877],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.432017022194048,
 'weight_decay_Hydroxylation-K': 6.180337307976626,
 'weight_decay_Hydroxylation-P': 1.032965356677613}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2569.319
[2,     1] loss: 2556.568
[3,     1] loss: 2559.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003953373465818135,
 'learning_rate_Hydroxylation-K': 0.006036305026114488,
 'learning_rate_Hydroxylation-P': 0.006741589457511284,
 'log_base': 2.322192158805111,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2446720239,
 'sample_weights': [7.854241165411899, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.688298896719479,
 'weight_decay_Hydroxylation-K': 1.4805612147047982,
 'weight_decay_Hydroxylation-P': 8.548579872253898}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1332.764
[2,     1] loss: 1331.522
[3,     1] loss: 1332.548
[4,     1] loss: 1321.635
[5,     1] loss: 1321.464
[6,     1] loss: 1321.088
[7,     1] loss: 1314.125
[8,     1] loss: 1305.982
[9,     1] loss: 1274.100
[10,     1] loss: 1248.801
[11,     1] loss: 1210.002
[12,     1] loss: 1166.857
[13,     1] loss: 1111.219
[14,     1] loss: 1091.676
[15,     1] loss: 1066.143
[16,     1] loss: 1034.205
[17,     1] loss: 1118.954
[18,     1] loss: 1094.956
[19,     1] loss: 984.355
[20,     1] loss: 1076.512
[21,     1] loss: 1054.457
[22,     1] loss: 1053.178
[23,     1] loss: 1038.605
[24,     1] loss: 983.069
[25,     1] loss: 1036.833
[26,     1] loss: 975.001
[27,     1] loss: 976.813
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009300366910809956,
 'learning_rate_Hydroxylation-K': 0.008747813532754244,
 'learning_rate_Hydroxylation-P': 0.003352400705441508,
 'log_base': 1.0621311136238394,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 349302534,
 'sample_weights': [1.9815075269211186, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.04696963415274,
 'weight_decay_Hydroxylation-K': 4.68414176993066,
 'weight_decay_Hydroxylation-P': 2.0920981426721204}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8971.918
[2,     1] loss: 9031.092
[3,     1] loss: 8990.562
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007509137545464462,
 'learning_rate_Hydroxylation-K': 0.0075000934259423875,
 'learning_rate_Hydroxylation-P': 0.00311223097585372,
 'log_base': 1.0175280926805887,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3440232471,
 'sample_weights': [27.69601637886739, 3.462135417579404],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.162553546361191,
 'weight_decay_Hydroxylation-K': 0.14243196481681394,
 'weight_decay_Hydroxylation-P': 1.1899766651264923}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31250.236
Exploding loss, terminate run (best metric=0.5604239106178284)
Finished Training
Total time taken: 0.22100090980529785
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31150.518
Exploding loss, terminate run (best metric=0.5295345187187195)
Finished Training
Total time taken: 0.21000003814697266
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31247.012
Exploding loss, terminate run (best metric=0.5262424349784851)
Finished Training
Total time taken: 0.23800110816955566
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31556.146
Exploding loss, terminate run (best metric=0.5270040035247803)
Finished Training
Total time taken: 0.2350008487701416
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31151.713
Exploding loss, terminate run (best metric=0.5283187031745911)
Finished Training
Total time taken: 0.23702764511108398
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31213.336
Exploding loss, terminate run (best metric=0.5547428727149963)
Finished Training
Total time taken: 0.2390003204345703
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31517.461
Exploding loss, terminate run (best metric=0.5487414598464966)
Finished Training
Total time taken: 0.23800134658813477
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31208.020
Exploding loss, terminate run (best metric=0.5267050862312317)
Finished Training
Total time taken: 0.21700263023376465
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31062.812
Exploding loss, terminate run (best metric=0.5425103902816772)
Finished Training
Total time taken: 0.20703339576721191
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31259.691
Exploding loss, terminate run (best metric=0.5282739996910095)
Finished Training
Total time taken: 0.20799851417541504
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31218.727
Exploding loss, terminate run (best metric=0.531693160533905)
Finished Training
Total time taken: 0.22700262069702148
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31345.977
Exploding loss, terminate run (best metric=0.5276313424110413)
Finished Training
Total time taken: 0.23000121116638184
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31156.822
Exploding loss, terminate run (best metric=0.5269175171852112)
Finished Training
Total time taken: 0.22002959251403809
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31231.117
Exploding loss, terminate run (best metric=0.5278211236000061)
Finished Training
Total time taken: 0.20400047302246094
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31452.031
Exploding loss, terminate run (best metric=0.530022144317627)
Finished Training
Total time taken: 0.22902417182922363
{'Hydroxylation-K Validation Accuracy': 0.39411938534278956, 'Hydroxylation-K Validation Sensitivity': 0.6866666666666666, 'Hydroxylation-K Validation Specificity': 0.32280701754385965, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.615925925925926, 'Hydroxylation-K AUC PR': 0.3189194470972555, 'Hydroxylation-K MCC': 0.009857281161802884, 'Hydroxylation-K F1': 0.2431380174574367, 'Validation Loss (Hydroxylation-K)': 0.5600133657455444, 'Hydroxylation-P Validation Accuracy': 0.3905227145830161, 'Hydroxylation-P Validation Sensitivity': 0.6780952380952381, 'Hydroxylation-P Validation Specificity': 0.32926829268292684, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5984346050588756, 'Hydroxylation-P AUC PR': 0.2807194706171895, 'Hydroxylation-P MCC': 0.010309948987183147, 'Hydroxylation-P F1': 0.2157692141286423, 'Validation Loss (Hydroxylation-P)': 0.5344388445218404, 'Validation Loss (total)': 1.0944522142410278, 'TimeToTrain': 0.22400832176208496}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006676387230067675,
 'learning_rate_Hydroxylation-K': 0.00021968856517075154,
 'learning_rate_Hydroxylation-P': 0.0038604964308468956,
 'log_base': 2.769958546297561,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 150618676,
 'sample_weights': [96.14743003591117, 11.9934565829567],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3169965533406147,
 'weight_decay_Hydroxylation-K': 8.274380809355282,
 'weight_decay_Hydroxylation-P': 6.873151438365245}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.105
[2,     1] loss: 1257.170
[3,     1] loss: 1256.142
[4,     1] loss: 1252.155
[5,     1] loss: 1252.500
[6,     1] loss: 1251.929
[7,     1] loss: 1248.350
[8,     1] loss: 1247.702
[9,     1] loss: 1244.268
[10,     1] loss: 1240.967
[11,     1] loss: 1239.212
[12,     1] loss: 1227.910
[13,     1] loss: 1211.844
[14,     1] loss: 1185.041
[15,     1] loss: 1161.278
[16,     1] loss: 1131.376
[17,     1] loss: 1098.010
[18,     1] loss: 1088.687
[19,     1] loss: 1066.418
[20,     1] loss: 1059.119
[21,     1] loss: 1026.907
[22,     1] loss: 1080.795
[23,     1] loss: 1002.003
[24,     1] loss: 1018.254
[25,     1] loss: 1020.986
[26,     1] loss: 1021.467
[27,     1] loss: 1051.458
[28,     1] loss: 989.218
[29,     1] loss: 981.576
[30,     1] loss: 1023.894
[31,     1] loss: 967.403
[32,     1] loss: 978.675
[33,     1] loss: 951.201
[34,     1] loss: 952.780
[35,     1] loss: 911.952
[36,     1] loss: 977.030
[37,     1] loss: 933.528
[38,     1] loss: 931.656
[39,     1] loss: 928.276
[40,     1] loss: 950.571
[41,     1] loss: 891.643
[42,     1] loss: 909.349
[43,     1] loss: 907.683
[44,     1] loss: 918.895
[45,     1] loss: 904.819
[46,     1] loss: 888.645
[47,     1] loss: 901.465
[48,     1] loss: 881.547
[49,     1] loss: 860.956
[50,     1] loss: 857.667
[51,     1] loss: 864.315
[52,     1] loss: 853.032
[53,     1] loss: 888.675
[54,     1] loss: 881.129
[55,     1] loss: 818.298
[56,     1] loss: 867.229
[57,     1] loss: 816.105
[58,     1] loss: 851.214
[59,     1] loss: 797.314
[60,     1] loss: 780.478
[61,     1] loss: 831.070
[62,     1] loss: 796.782
[63,     1] loss: 771.806
[64,     1] loss: 793.230
[65,     1] loss: 800.438
[66,     1] loss: 747.852
[67,     1] loss: 756.789
[68,     1] loss: 774.603
[69,     1] loss: 799.636
[70,     1] loss: 735.203
[71,     1] loss: 777.140
[72,     1] loss: 740.072
[73,     1] loss: 721.644
[74,     1] loss: 680.292
[75,     1] loss: 749.920
[76,     1] loss: 720.549
[77,     1] loss: 662.569
[78,     1] loss: 752.452
[79,     1] loss: 817.670
[80,     1] loss: 677.393
[81,     1] loss: 690.615
[82,     1] loss: 662.126
[83,     1] loss: 705.346
[84,     1] loss: 641.060
[85,     1] loss: 666.352
[86,     1] loss: 650.675
[87,     1] loss: 603.051
[88,     1] loss: 592.531
[89,     1] loss: 604.905
[90,     1] loss: 633.084
[91,     1] loss: 553.731
[92,     1] loss: 594.811
[93,     1] loss: 628.015
[94,     1] loss: 638.749
[95,     1] loss: 536.992
[96,     1] loss: 650.812
[97,     1] loss: 661.910
[98,     1] loss: 658.208
[99,     1] loss: 624.706
[100,     1] loss: 606.618
[101,     1] loss: 614.700
[102,     1] loss: 535.513
[103,     1] loss: 547.654
[104,     1] loss: 493.723
[105,     1] loss: 545.735
[106,     1] loss: 523.138
Early stopping applied (best metric=0.3332831561565399)
Finished Training
Total time taken: 16.94915270805359
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.027
[2,     1] loss: 1253.891
[3,     1] loss: 1255.082
[4,     1] loss: 1253.119
[5,     1] loss: 1249.789
[6,     1] loss: 1250.107
[7,     1] loss: 1246.716
[8,     1] loss: 1244.359
[9,     1] loss: 1228.643
[10,     1] loss: 1226.694
[11,     1] loss: 1217.365
[12,     1] loss: 1188.677
[13,     1] loss: 1164.974
[14,     1] loss: 1137.179
[15,     1] loss: 1069.976
[16,     1] loss: 1103.306
[17,     1] loss: 1025.865
[18,     1] loss: 1100.769
[19,     1] loss: 1056.744
[20,     1] loss: 1048.192
[21,     1] loss: 1032.279
[22,     1] loss: 1018.569
[23,     1] loss: 1031.608
[24,     1] loss: 1064.008
[25,     1] loss: 1018.920
[26,     1] loss: 987.052
[27,     1] loss: 1014.063
[28,     1] loss: 987.337
[29,     1] loss: 980.714
[30,     1] loss: 933.331
[31,     1] loss: 949.579
[32,     1] loss: 940.189
[33,     1] loss: 920.789
[34,     1] loss: 995.565
[35,     1] loss: 939.370
[36,     1] loss: 915.084
[37,     1] loss: 929.054
[38,     1] loss: 916.199
[39,     1] loss: 960.312
[40,     1] loss: 925.495
[41,     1] loss: 931.067
[42,     1] loss: 896.137
[43,     1] loss: 922.922
[44,     1] loss: 923.660
[45,     1] loss: 881.454
[46,     1] loss: 907.936
[47,     1] loss: 846.602
[48,     1] loss: 867.216
[49,     1] loss: 870.107
[50,     1] loss: 814.226
[51,     1] loss: 890.423
[52,     1] loss: 807.441
[53,     1] loss: 818.624
[54,     1] loss: 859.434
[55,     1] loss: 834.566
[56,     1] loss: 815.212
[57,     1] loss: 854.614
[58,     1] loss: 822.609
[59,     1] loss: 795.070
[60,     1] loss: 755.327
[61,     1] loss: 793.857
[62,     1] loss: 757.467
[63,     1] loss: 742.084
[64,     1] loss: 790.137
[65,     1] loss: 774.616
[66,     1] loss: 684.373
[67,     1] loss: 716.176
[68,     1] loss: 696.976
[69,     1] loss: 694.896
[70,     1] loss: 641.827
[71,     1] loss: 670.989
[72,     1] loss: 709.258
[73,     1] loss: 706.653
[74,     1] loss: 812.327
[75,     1] loss: 797.407
[76,     1] loss: 629.465
[77,     1] loss: 711.259
[78,     1] loss: 681.109
[79,     1] loss: 744.814
[80,     1] loss: 658.887
[81,     1] loss: 685.287
[82,     1] loss: 597.805
[83,     1] loss: 609.271
[84,     1] loss: 607.732
[85,     1] loss: 604.234
[86,     1] loss: 648.460
[87,     1] loss: 608.363
[88,     1] loss: 530.958
[89,     1] loss: 581.357
[90,     1] loss: 569.140
[91,     1] loss: 672.451
[92,     1] loss: 676.391
[93,     1] loss: 572.415
[94,     1] loss: 661.459
[95,     1] loss: 523.746
[96,     1] loss: 614.472
[97,     1] loss: 578.842
[98,     1] loss: 570.952
[99,     1] loss: 559.727
[100,     1] loss: 594.889
[101,     1] loss: 507.042
[102,     1] loss: 565.833
[103,     1] loss: 505.199
[104,     1] loss: 577.566
[105,     1] loss: 491.290
[106,     1] loss: 461.607
[107,     1] loss: 489.354
[108,     1] loss: 457.522
[109,     1] loss: 474.811
[110,     1] loss: 480.617
[111,     1] loss: 444.327
Early stopping applied (best metric=0.3217770755290985)
Finished Training
Total time taken: 16.55510663986206
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.018
[2,     1] loss: 1256.023
[3,     1] loss: 1252.332
[4,     1] loss: 1251.073
[5,     1] loss: 1250.653
[6,     1] loss: 1243.850
[7,     1] loss: 1235.892
[8,     1] loss: 1210.642
[9,     1] loss: 1188.819
[10,     1] loss: 1158.016
[11,     1] loss: 1085.750
[12,     1] loss: 1044.333
[13,     1] loss: 980.319
[14,     1] loss: 1042.973
[15,     1] loss: 1075.321
[16,     1] loss: 972.677
[17,     1] loss: 945.846
[18,     1] loss: 976.506
[19,     1] loss: 976.261
[20,     1] loss: 967.122
[21,     1] loss: 961.915
[22,     1] loss: 915.910
[23,     1] loss: 954.803
[24,     1] loss: 934.496
[25,     1] loss: 949.878
[26,     1] loss: 900.701
[27,     1] loss: 919.324
[28,     1] loss: 941.790
[29,     1] loss: 893.562
[30,     1] loss: 940.850
[31,     1] loss: 923.467
[32,     1] loss: 906.923
[33,     1] loss: 880.323
[34,     1] loss: 838.455
[35,     1] loss: 838.114
[36,     1] loss: 852.623
[37,     1] loss: 842.557
[38,     1] loss: 848.791
[39,     1] loss: 849.524
[40,     1] loss: 914.138
[41,     1] loss: 764.442
[42,     1] loss: 842.365
[43,     1] loss: 804.092
[44,     1] loss: 785.510
[45,     1] loss: 795.249
[46,     1] loss: 770.939
[47,     1] loss: 788.007
[48,     1] loss: 733.075
[49,     1] loss: 765.893
[50,     1] loss: 756.063
[51,     1] loss: 673.651
[52,     1] loss: 736.050
[53,     1] loss: 715.623
[54,     1] loss: 720.424
[55,     1] loss: 722.268
[56,     1] loss: 712.190
[57,     1] loss: 725.231
[58,     1] loss: 672.385
[59,     1] loss: 689.088
[60,     1] loss: 693.628
[61,     1] loss: 685.488
[62,     1] loss: 684.069
[63,     1] loss: 690.957
[64,     1] loss: 634.985
[65,     1] loss: 668.649
[66,     1] loss: 640.107
[67,     1] loss: 658.318
[68,     1] loss: 717.024
[69,     1] loss: 643.652
[70,     1] loss: 703.515
[71,     1] loss: 658.291
[72,     1] loss: 597.631
[73,     1] loss: 616.749
[74,     1] loss: 607.834
[75,     1] loss: 553.770
[76,     1] loss: 658.447
[77,     1] loss: 562.281
[78,     1] loss: 556.384
[79,     1] loss: 526.928
[80,     1] loss: 605.960
[81,     1] loss: 534.035
[82,     1] loss: 554.960
[83,     1] loss: 591.836
[84,     1] loss: 538.324
[85,     1] loss: 583.285
[86,     1] loss: 521.569
[87,     1] loss: 550.614
[88,     1] loss: 508.178
[89,     1] loss: 510.999
[90,     1] loss: 532.920
[91,     1] loss: 492.460
[92,     1] loss: 509.416
[93,     1] loss: 451.122
[94,     1] loss: 501.037
[95,     1] loss: 487.892
[96,     1] loss: 527.405
[97,     1] loss: 470.865
[98,     1] loss: 504.499
[99,     1] loss: 468.363
[100,     1] loss: 530.712
[101,     1] loss: 517.398
[102,     1] loss: 528.515
[103,     1] loss: 493.046
[104,     1] loss: 446.652
[105,     1] loss: 489.189
[106,     1] loss: 455.721
[107,     1] loss: 463.147
[108,     1] loss: 426.960
[109,     1] loss: 510.939
[110,     1] loss: 440.789
[111,     1] loss: 452.293
[112,     1] loss: 464.321
[113,     1] loss: 399.168
[114,     1] loss: 418.566
[115,     1] loss: 371.406
Early stopping applied (best metric=0.4373359680175781)
Finished Training
Total time taken: 17.364132165908813
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.849
[2,     1] loss: 1254.546
[3,     1] loss: 1259.146
[4,     1] loss: 1254.196
[5,     1] loss: 1252.073
[6,     1] loss: 1250.951
[7,     1] loss: 1245.522
[8,     1] loss: 1236.921
[9,     1] loss: 1226.736
[10,     1] loss: 1203.332
[11,     1] loss: 1171.395
[12,     1] loss: 1131.530
[13,     1] loss: 1110.967
[14,     1] loss: 1088.922
[15,     1] loss: 1054.198
[16,     1] loss: 1053.231
[17,     1] loss: 1078.981
[18,     1] loss: 976.357
[19,     1] loss: 1036.451
[20,     1] loss: 1015.872
[21,     1] loss: 1019.105
[22,     1] loss: 988.948
[23,     1] loss: 1031.463
[24,     1] loss: 1023.260
[25,     1] loss: 995.504
[26,     1] loss: 979.668
[27,     1] loss: 974.035
[28,     1] loss: 944.975
[29,     1] loss: 955.008
[30,     1] loss: 921.593
[31,     1] loss: 935.696
[32,     1] loss: 926.742
[33,     1] loss: 950.273
[34,     1] loss: 934.207
[35,     1] loss: 923.333
[36,     1] loss: 889.648
[37,     1] loss: 905.060
[38,     1] loss: 911.143
[39,     1] loss: 920.696
[40,     1] loss: 902.670
[41,     1] loss: 876.302
[42,     1] loss: 916.972
[43,     1] loss: 857.637
[44,     1] loss: 887.617
[45,     1] loss: 844.729
[46,     1] loss: 868.504
[47,     1] loss: 894.984
[48,     1] loss: 866.901
[49,     1] loss: 838.113
[50,     1] loss: 826.599
[51,     1] loss: 783.993
[52,     1] loss: 854.554
[53,     1] loss: 756.973
[54,     1] loss: 830.347
[55,     1] loss: 791.879
[56,     1] loss: 790.582
[57,     1] loss: 795.012
[58,     1] loss: 782.788
[59,     1] loss: 801.734
[60,     1] loss: 821.609
[61,     1] loss: 771.890
[62,     1] loss: 765.843
[63,     1] loss: 736.305
[64,     1] loss: 691.623
[65,     1] loss: 707.823
[66,     1] loss: 734.226
[67,     1] loss: 710.721
[68,     1] loss: 669.891
[69,     1] loss: 719.597
[70,     1] loss: 745.578
[71,     1] loss: 745.174
[72,     1] loss: 714.537
[73,     1] loss: 722.623
[74,     1] loss: 703.448
[75,     1] loss: 667.803
[76,     1] loss: 715.510
[77,     1] loss: 711.822
[78,     1] loss: 682.186
[79,     1] loss: 717.154
[80,     1] loss: 633.868
[81,     1] loss: 632.589
[82,     1] loss: 730.870
[83,     1] loss: 614.132
[84,     1] loss: 682.314
[85,     1] loss: 645.777
[86,     1] loss: 632.902
[87,     1] loss: 643.707
[88,     1] loss: 555.019
[89,     1] loss: 569.174
[90,     1] loss: 552.820
[91,     1] loss: 583.831
[92,     1] loss: 566.784
[93,     1] loss: 542.750
[94,     1] loss: 533.944
[95,     1] loss: 528.101
[96,     1] loss: 520.943
[97,     1] loss: 573.105
[98,     1] loss: 569.806
[99,     1] loss: 531.016
[100,     1] loss: 553.638
[101,     1] loss: 507.013
[102,     1] loss: 508.705
[103,     1] loss: 547.874
[104,     1] loss: 523.881
[105,     1] loss: 512.508
[106,     1] loss: 494.285
[107,     1] loss: 444.713
[108,     1] loss: 495.299
[109,     1] loss: 558.419
[110,     1] loss: 555.232
[111,     1] loss: 440.714
[112,     1] loss: 521.468
[113,     1] loss: 525.027
[114,     1] loss: 477.387
[115,     1] loss: 503.352
[116,     1] loss: 516.969
[117,     1] loss: 457.982
[118,     1] loss: 488.762
[119,     1] loss: 432.281
[120,     1] loss: 452.839
[121,     1] loss: 412.467
[122,     1] loss: 385.087
[123,     1] loss: 451.469
[124,     1] loss: 408.647
[125,     1] loss: 444.890
[126,     1] loss: 459.719
[127,     1] loss: 413.337
[128,     1] loss: 413.001
[129,     1] loss: 412.014
[130,     1] loss: 360.204
[131,     1] loss: 411.729
[132,     1] loss: 387.273
[133,     1] loss: 365.121
[134,     1] loss: 358.009
[135,     1] loss: 397.126
[136,     1] loss: 370.938
[137,     1] loss: 364.309
[138,     1] loss: 402.432
[139,     1] loss: 374.030
[140,     1] loss: 373.444
[141,     1] loss: 316.509
[142,     1] loss: 381.367
[143,     1] loss: 362.505
[144,     1] loss: 379.474
[145,     1] loss: 385.756
[146,     1] loss: 389.265
[147,     1] loss: 340.136
Early stopping applied (best metric=0.380390465259552)
Finished Training
Total time taken: 23.421710729599
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1264.038
[2,     1] loss: 1259.424
[3,     1] loss: 1254.762
[4,     1] loss: 1252.567
[5,     1] loss: 1251.863
[6,     1] loss: 1254.947
[7,     1] loss: 1252.176
[8,     1] loss: 1255.394
[9,     1] loss: 1251.741
[10,     1] loss: 1249.492
[11,     1] loss: 1241.462
[12,     1] loss: 1234.891
[13,     1] loss: 1227.211
[14,     1] loss: 1206.729
[15,     1] loss: 1179.551
[16,     1] loss: 1162.440
[17,     1] loss: 1148.496
[18,     1] loss: 1099.638
[19,     1] loss: 1071.940
[20,     1] loss: 1046.678
[21,     1] loss: 1034.833
[22,     1] loss: 975.242
[23,     1] loss: 1019.301
[24,     1] loss: 1037.503
[25,     1] loss: 1049.118
[26,     1] loss: 1027.076
[27,     1] loss: 1011.156
[28,     1] loss: 1024.128
[29,     1] loss: 1031.030
[30,     1] loss: 1042.268
[31,     1] loss: 982.805
[32,     1] loss: 973.376
[33,     1] loss: 965.454
[34,     1] loss: 975.265
[35,     1] loss: 957.791
[36,     1] loss: 930.726
[37,     1] loss: 963.049
[38,     1] loss: 928.435
[39,     1] loss: 933.736
[40,     1] loss: 902.945
[41,     1] loss: 905.514
[42,     1] loss: 907.840
[43,     1] loss: 900.729
[44,     1] loss: 888.859
[45,     1] loss: 868.766
[46,     1] loss: 890.012
[47,     1] loss: 896.654
[48,     1] loss: 853.028
[49,     1] loss: 848.150
[50,     1] loss: 809.500
[51,     1] loss: 844.795
[52,     1] loss: 813.419
[53,     1] loss: 883.209
[54,     1] loss: 811.278
[55,     1] loss: 844.060
[56,     1] loss: 827.969
[57,     1] loss: 802.557
[58,     1] loss: 800.025
[59,     1] loss: 832.415
[60,     1] loss: 807.801
[61,     1] loss: 726.268
[62,     1] loss: 764.243
[63,     1] loss: 732.531
[64,     1] loss: 707.180
[65,     1] loss: 724.318
[66,     1] loss: 736.029
[67,     1] loss: 767.812
[68,     1] loss: 756.089
[69,     1] loss: 711.885
[70,     1] loss: 704.429
[71,     1] loss: 685.783
[72,     1] loss: 679.460
[73,     1] loss: 676.212
[74,     1] loss: 693.342
[75,     1] loss: 712.346
[76,     1] loss: 649.431
[77,     1] loss: 654.052
[78,     1] loss: 660.435
[79,     1] loss: 666.837
[80,     1] loss: 709.800
[81,     1] loss: 694.257
[82,     1] loss: 609.829
[83,     1] loss: 759.371
[84,     1] loss: 681.452
[85,     1] loss: 661.031
[86,     1] loss: 675.556
[87,     1] loss: 645.380
[88,     1] loss: 641.366
[89,     1] loss: 603.179
[90,     1] loss: 607.525
[91,     1] loss: 556.623
[92,     1] loss: 618.625
[93,     1] loss: 596.918
[94,     1] loss: 567.491
[95,     1] loss: 540.498
[96,     1] loss: 585.427
[97,     1] loss: 485.070
[98,     1] loss: 537.382
[99,     1] loss: 545.227
[100,     1] loss: 529.869
[101,     1] loss: 492.524
[102,     1] loss: 529.443
[103,     1] loss: 532.803
[104,     1] loss: 510.161
[105,     1] loss: 513.053
[106,     1] loss: 509.551
[107,     1] loss: 519.781
[108,     1] loss: 514.989
[109,     1] loss: 478.828
Early stopping applied (best metric=0.3857286870479584)
Finished Training
Total time taken: 17.935316801071167
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.700
[2,     1] loss: 1254.274
[3,     1] loss: 1248.439
[4,     1] loss: 1255.743
[5,     1] loss: 1252.983
[6,     1] loss: 1254.119
[7,     1] loss: 1250.772
[8,     1] loss: 1250.739
[9,     1] loss: 1251.496
[10,     1] loss: 1248.258
[11,     1] loss: 1248.611
[12,     1] loss: 1241.558
[13,     1] loss: 1230.842
[14,     1] loss: 1224.124
[15,     1] loss: 1205.638
[16,     1] loss: 1180.324
[17,     1] loss: 1146.473
[18,     1] loss: 1124.261
[19,     1] loss: 1079.496
[20,     1] loss: 1066.252
[21,     1] loss: 1087.928
[22,     1] loss: 1008.257
[23,     1] loss: 1021.462
[24,     1] loss: 996.566
[25,     1] loss: 1024.625
[26,     1] loss: 1030.077
[27,     1] loss: 992.069
[28,     1] loss: 1016.023
[29,     1] loss: 1015.273
[30,     1] loss: 982.971
[31,     1] loss: 1014.128
[32,     1] loss: 980.019
[33,     1] loss: 986.113
[34,     1] loss: 948.099
[35,     1] loss: 985.256
[36,     1] loss: 959.851
[37,     1] loss: 962.945
[38,     1] loss: 952.823
[39,     1] loss: 924.085
[40,     1] loss: 943.289
[41,     1] loss: 902.916
[42,     1] loss: 983.707
[43,     1] loss: 916.980
[44,     1] loss: 888.389
[45,     1] loss: 931.279
[46,     1] loss: 945.943
[47,     1] loss: 885.208
[48,     1] loss: 870.489
[49,     1] loss: 906.508
[50,     1] loss: 853.266
[51,     1] loss: 834.184
[52,     1] loss: 837.173
[53,     1] loss: 878.070
[54,     1] loss: 832.084
[55,     1] loss: 899.490
[56,     1] loss: 854.549
[57,     1] loss: 859.918
[58,     1] loss: 817.236
[59,     1] loss: 829.562
[60,     1] loss: 844.621
[61,     1] loss: 784.771
[62,     1] loss: 818.418
[63,     1] loss: 792.247
[64,     1] loss: 789.845
[65,     1] loss: 764.787
[66,     1] loss: 783.071
[67,     1] loss: 773.193
[68,     1] loss: 760.881
[69,     1] loss: 803.864
[70,     1] loss: 732.730
[71,     1] loss: 800.106
[72,     1] loss: 800.139
[73,     1] loss: 744.730
[74,     1] loss: 782.283
[75,     1] loss: 741.860
[76,     1] loss: 766.223
[77,     1] loss: 708.518
[78,     1] loss: 779.749
[79,     1] loss: 716.051
[80,     1] loss: 780.214
[81,     1] loss: 709.307
[82,     1] loss: 706.470
[83,     1] loss: 735.023
[84,     1] loss: 728.666
[85,     1] loss: 656.976
[86,     1] loss: 660.156
[87,     1] loss: 667.246
[88,     1] loss: 723.225
[89,     1] loss: 658.367
[90,     1] loss: 708.907
[91,     1] loss: 713.336
[92,     1] loss: 642.724
[93,     1] loss: 663.656
[94,     1] loss: 620.257
[95,     1] loss: 595.060
[96,     1] loss: 557.580
[97,     1] loss: 615.789
[98,     1] loss: 579.791
[99,     1] loss: 581.026
[100,     1] loss: 539.703
[101,     1] loss: 532.833
[102,     1] loss: 562.759
[103,     1] loss: 562.680
[104,     1] loss: 539.558
[105,     1] loss: 529.183
[106,     1] loss: 545.049
[107,     1] loss: 540.308
[108,     1] loss: 552.034
[109,     1] loss: 543.467
[110,     1] loss: 564.678
[111,     1] loss: 503.138
[112,     1] loss: 497.151
[113,     1] loss: 493.073
[114,     1] loss: 536.288
[115,     1] loss: 544.028
[116,     1] loss: 479.217
[117,     1] loss: 586.849
[118,     1] loss: 495.718
Early stopping applied (best metric=0.32624873518943787)
Finished Training
Total time taken: 18.782347202301025
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.280
[2,     1] loss: 1253.905
[3,     1] loss: 1253.472
[4,     1] loss: 1256.332
[5,     1] loss: 1248.273
[6,     1] loss: 1239.776
[7,     1] loss: 1229.357
[8,     1] loss: 1202.688
[9,     1] loss: 1161.051
[10,     1] loss: 1125.171
[11,     1] loss: 1077.671
[12,     1] loss: 1021.942
[13,     1] loss: 1037.850
[14,     1] loss: 1025.156
[15,     1] loss: 1020.009
[16,     1] loss: 974.410
[17,     1] loss: 984.389
[18,     1] loss: 946.486
[19,     1] loss: 959.534
[20,     1] loss: 942.683
[21,     1] loss: 932.941
[22,     1] loss: 923.536
[23,     1] loss: 946.456
[24,     1] loss: 919.327
[25,     1] loss: 885.867
[26,     1] loss: 896.791
[27,     1] loss: 918.208
[28,     1] loss: 869.909
[29,     1] loss: 865.884
[30,     1] loss: 907.584
[31,     1] loss: 890.873
[32,     1] loss: 884.125
[33,     1] loss: 913.718
[34,     1] loss: 845.680
[35,     1] loss: 822.413
[36,     1] loss: 872.505
[37,     1] loss: 823.937
[38,     1] loss: 861.159
[39,     1] loss: 792.088
[40,     1] loss: 802.230
[41,     1] loss: 806.887
[42,     1] loss: 859.083
[43,     1] loss: 796.517
[44,     1] loss: 752.185
[45,     1] loss: 782.390
[46,     1] loss: 800.005
[47,     1] loss: 743.929
[48,     1] loss: 841.775
[49,     1] loss: 695.279
[50,     1] loss: 749.256
[51,     1] loss: 748.169
[52,     1] loss: 769.889
[53,     1] loss: 712.779
[54,     1] loss: 738.015
[55,     1] loss: 699.200
[56,     1] loss: 728.859
[57,     1] loss: 742.404
[58,     1] loss: 663.117
[59,     1] loss: 635.837
[60,     1] loss: 658.183
[61,     1] loss: 608.531
[62,     1] loss: 665.408
[63,     1] loss: 704.315
[64,     1] loss: 611.175
[65,     1] loss: 601.805
[66,     1] loss: 654.762
[67,     1] loss: 629.547
[68,     1] loss: 661.768
[69,     1] loss: 628.732
[70,     1] loss: 585.655
Early stopping applied (best metric=0.42778337001800537)
Finished Training
Total time taken: 11.623650789260864
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.413
[2,     1] loss: 1253.107
[3,     1] loss: 1254.854
[4,     1] loss: 1251.259
[5,     1] loss: 1248.281
[6,     1] loss: 1235.495
[7,     1] loss: 1220.120
[8,     1] loss: 1196.291
[9,     1] loss: 1162.375
[10,     1] loss: 1128.488
[11,     1] loss: 1102.501
[12,     1] loss: 1057.850
[13,     1] loss: 1055.116
[14,     1] loss: 1033.434
[15,     1] loss: 1045.281
[16,     1] loss: 1031.314
[17,     1] loss: 1067.246
[18,     1] loss: 1021.169
[19,     1] loss: 1016.873
[20,     1] loss: 999.179
[21,     1] loss: 1018.707
[22,     1] loss: 983.067
[23,     1] loss: 983.632
[24,     1] loss: 964.725
[25,     1] loss: 971.899
[26,     1] loss: 984.912
[27,     1] loss: 943.987
[28,     1] loss: 1004.388
[29,     1] loss: 988.654
[30,     1] loss: 928.700
[31,     1] loss: 925.066
[32,     1] loss: 934.733
[33,     1] loss: 913.349
[34,     1] loss: 921.336
[35,     1] loss: 935.898
[36,     1] loss: 952.229
[37,     1] loss: 868.998
[38,     1] loss: 895.370
[39,     1] loss: 895.474
[40,     1] loss: 893.087
[41,     1] loss: 894.351
[42,     1] loss: 881.219
[43,     1] loss: 829.414
[44,     1] loss: 846.825
[45,     1] loss: 839.913
[46,     1] loss: 802.952
[47,     1] loss: 782.281
[48,     1] loss: 809.688
[49,     1] loss: 823.124
[50,     1] loss: 785.373
[51,     1] loss: 741.174
[52,     1] loss: 756.852
[53,     1] loss: 849.658
[54,     1] loss: 745.513
[55,     1] loss: 767.343
[56,     1] loss: 762.099
[57,     1] loss: 724.127
[58,     1] loss: 750.643
[59,     1] loss: 757.882
[60,     1] loss: 766.745
[61,     1] loss: 793.882
[62,     1] loss: 703.691
[63,     1] loss: 792.537
[64,     1] loss: 724.517
[65,     1] loss: 734.724
[66,     1] loss: 674.601
[67,     1] loss: 715.832
[68,     1] loss: 651.297
[69,     1] loss: 685.736
[70,     1] loss: 651.889
[71,     1] loss: 641.893
[72,     1] loss: 630.337
[73,     1] loss: 614.822
[74,     1] loss: 652.816
[75,     1] loss: 625.130
[76,     1] loss: 615.963
[77,     1] loss: 576.041
[78,     1] loss: 615.369
[79,     1] loss: 652.679
[80,     1] loss: 572.690
[81,     1] loss: 575.550
[82,     1] loss: 569.171
[83,     1] loss: 609.917
[84,     1] loss: 577.839
[85,     1] loss: 524.077
Early stopping applied (best metric=0.35641154646873474)
Finished Training
Total time taken: 14.10159945487976
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.172
[2,     1] loss: 1251.219
[3,     1] loss: 1272.549
[4,     1] loss: 1252.790
[5,     1] loss: 1251.168
[6,     1] loss: 1255.781
[7,     1] loss: 1260.266
[8,     1] loss: 1253.964
[9,     1] loss: 1253.504
[10,     1] loss: 1255.362
[11,     1] loss: 1253.255
[12,     1] loss: 1255.896
[13,     1] loss: 1253.547
[14,     1] loss: 1252.954
[15,     1] loss: 1251.364
[16,     1] loss: 1253.275
[17,     1] loss: 1252.280
[18,     1] loss: 1251.404
[19,     1] loss: 1252.951
[20,     1] loss: 1251.949
[21,     1] loss: 1250.358
[22,     1] loss: 1252.162
[23,     1] loss: 1250.159
[24,     1] loss: 1247.628
[25,     1] loss: 1245.120
[26,     1] loss: 1240.218
[27,     1] loss: 1234.456
[28,     1] loss: 1229.191
[29,     1] loss: 1213.350
[30,     1] loss: 1196.121
[31,     1] loss: 1172.315
[32,     1] loss: 1138.841
[33,     1] loss: 1124.972
[34,     1] loss: 1075.082
[35,     1] loss: 1046.637
[36,     1] loss: 1061.293
[37,     1] loss: 1086.998
[38,     1] loss: 1053.938
[39,     1] loss: 1015.574
[40,     1] loss: 1026.009
[41,     1] loss: 1017.830
[42,     1] loss: 1038.277
[43,     1] loss: 1027.621
[44,     1] loss: 985.496
[45,     1] loss: 998.337
[46,     1] loss: 1019.179
[47,     1] loss: 959.893
[48,     1] loss: 1012.509
[49,     1] loss: 947.714
[50,     1] loss: 980.866
[51,     1] loss: 944.670
[52,     1] loss: 922.263
[53,     1] loss: 957.206
[54,     1] loss: 963.813
[55,     1] loss: 949.267
[56,     1] loss: 905.527
[57,     1] loss: 935.287
[58,     1] loss: 903.795
[59,     1] loss: 946.892
[60,     1] loss: 900.299
[61,     1] loss: 978.409
[62,     1] loss: 915.154
[63,     1] loss: 894.144
[64,     1] loss: 919.607
[65,     1] loss: 902.615
[66,     1] loss: 880.460
[67,     1] loss: 880.043
[68,     1] loss: 861.280
[69,     1] loss: 853.327
[70,     1] loss: 851.527
[71,     1] loss: 829.588
[72,     1] loss: 862.794
[73,     1] loss: 860.367
[74,     1] loss: 863.857
[75,     1] loss: 834.533
[76,     1] loss: 817.480
[77,     1] loss: 780.950
[78,     1] loss: 768.350
[79,     1] loss: 790.332
[80,     1] loss: 805.827
[81,     1] loss: 829.145
[82,     1] loss: 794.725
[83,     1] loss: 706.999
[84,     1] loss: 831.073
[85,     1] loss: 786.608
[86,     1] loss: 787.984
[87,     1] loss: 747.471
[88,     1] loss: 699.026
[89,     1] loss: 709.019
[90,     1] loss: 693.699
[91,     1] loss: 702.272
[92,     1] loss: 752.973
[93,     1] loss: 671.251
[94,     1] loss: 662.915
[95,     1] loss: 762.380
[96,     1] loss: 703.270
[97,     1] loss: 645.966
[98,     1] loss: 724.334
[99,     1] loss: 770.669
[100,     1] loss: 629.935
[101,     1] loss: 728.853
[102,     1] loss: 631.328
[103,     1] loss: 710.698
[104,     1] loss: 642.603
[105,     1] loss: 653.239
[106,     1] loss: 566.059
[107,     1] loss: 662.891
[108,     1] loss: 615.528
[109,     1] loss: 601.289
[110,     1] loss: 548.154
[111,     1] loss: 580.461
[112,     1] loss: 577.491
[113,     1] loss: 553.432
[114,     1] loss: 625.314
[115,     1] loss: 492.956
[116,     1] loss: 546.758
[117,     1] loss: 540.335
[118,     1] loss: 486.462
[119,     1] loss: 564.714
[120,     1] loss: 482.306
[121,     1] loss: 505.893
[122,     1] loss: 500.179
[123,     1] loss: 484.435
[124,     1] loss: 454.787
[125,     1] loss: 489.825
[126,     1] loss: 483.565
[127,     1] loss: 503.431
[128,     1] loss: 477.068
Early stopping applied (best metric=0.29910093545913696)
Finished Training
Total time taken: 20.137755632400513
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1263.598
[2,     1] loss: 1269.854
[3,     1] loss: 1257.189
[4,     1] loss: 1259.067
[5,     1] loss: 1256.073
[6,     1] loss: 1254.914
[7,     1] loss: 1255.645
[8,     1] loss: 1254.607
[9,     1] loss: 1254.743
[10,     1] loss: 1253.681
[11,     1] loss: 1254.702
[12,     1] loss: 1253.397
[13,     1] loss: 1253.131
[14,     1] loss: 1253.001
[15,     1] loss: 1251.950
[16,     1] loss: 1252.396
[17,     1] loss: 1250.230
[18,     1] loss: 1248.414
[19,     1] loss: 1245.688
[20,     1] loss: 1242.930
[21,     1] loss: 1239.786
[22,     1] loss: 1229.911
[23,     1] loss: 1217.693
[24,     1] loss: 1199.454
[25,     1] loss: 1184.145
[26,     1] loss: 1165.475
[27,     1] loss: 1136.740
[28,     1] loss: 1101.694
[29,     1] loss: 1074.809
[30,     1] loss: 1056.292
[31,     1] loss: 1060.279
[32,     1] loss: 1044.707
[33,     1] loss: 995.494
[34,     1] loss: 1006.202
[35,     1] loss: 1020.146
[36,     1] loss: 1013.762
[37,     1] loss: 983.182
[38,     1] loss: 1010.841
[39,     1] loss: 960.421
[40,     1] loss: 1015.115
[41,     1] loss: 997.318
[42,     1] loss: 1007.470
[43,     1] loss: 974.223
[44,     1] loss: 968.474
[45,     1] loss: 947.449
[46,     1] loss: 966.374
[47,     1] loss: 929.032
[48,     1] loss: 924.922
[49,     1] loss: 946.734
[50,     1] loss: 931.806
[51,     1] loss: 919.816
[52,     1] loss: 886.990
[53,     1] loss: 893.326
[54,     1] loss: 878.165
[55,     1] loss: 882.220
[56,     1] loss: 879.222
[57,     1] loss: 817.063
[58,     1] loss: 831.514
[59,     1] loss: 904.996
[60,     1] loss: 799.991
[61,     1] loss: 837.071
[62,     1] loss: 862.921
[63,     1] loss: 825.298
[64,     1] loss: 843.836
[65,     1] loss: 815.167
[66,     1] loss: 827.374
[67,     1] loss: 836.612
[68,     1] loss: 789.020
[69,     1] loss: 765.878
[70,     1] loss: 787.088
[71,     1] loss: 813.766
[72,     1] loss: 797.466
[73,     1] loss: 809.717
[74,     1] loss: 804.953
[75,     1] loss: 715.024
[76,     1] loss: 788.907
[77,     1] loss: 776.255
[78,     1] loss: 730.100
[79,     1] loss: 825.292
[80,     1] loss: 757.806
[81,     1] loss: 777.123
[82,     1] loss: 767.893
[83,     1] loss: 748.048
[84,     1] loss: 743.840
[85,     1] loss: 711.469
[86,     1] loss: 723.218
[87,     1] loss: 702.286
[88,     1] loss: 715.588
[89,     1] loss: 710.322
[90,     1] loss: 709.704
[91,     1] loss: 672.244
[92,     1] loss: 657.059
[93,     1] loss: 658.249
[94,     1] loss: 691.192
[95,     1] loss: 658.832
[96,     1] loss: 645.666
[97,     1] loss: 659.105
[98,     1] loss: 672.401
[99,     1] loss: 627.349
[100,     1] loss: 623.975
[101,     1] loss: 604.253
[102,     1] loss: 578.679
[103,     1] loss: 672.066
[104,     1] loss: 569.926
[105,     1] loss: 631.834
[106,     1] loss: 589.903
[107,     1] loss: 617.569
[108,     1] loss: 689.121
[109,     1] loss: 664.110
[110,     1] loss: 591.474
[111,     1] loss: 737.976
[112,     1] loss: 564.079
[113,     1] loss: 610.936
[114,     1] loss: 583.719
[115,     1] loss: 603.609
[116,     1] loss: 522.015
[117,     1] loss: 536.650
[118,     1] loss: 537.408
[119,     1] loss: 505.211
[120,     1] loss: 494.196
[121,     1] loss: 511.515
[122,     1] loss: 484.901
[123,     1] loss: 522.722
[124,     1] loss: 481.660
[125,     1] loss: 449.222
[126,     1] loss: 453.332
[127,     1] loss: 444.179
[128,     1] loss: 465.078
[129,     1] loss: 442.038
[130,     1] loss: 459.584
[131,     1] loss: 447.324
[132,     1] loss: 465.920
[133,     1] loss: 440.902
[134,     1] loss: 476.856
[135,     1] loss: 444.923
[136,     1] loss: 432.077
[137,     1] loss: 458.110
[138,     1] loss: 431.463
[139,     1] loss: 390.948
[140,     1] loss: 392.031
[141,     1] loss: 384.131
[142,     1] loss: 394.989
[143,     1] loss: 392.686
Early stopping applied (best metric=0.39416739344596863)
Finished Training
Total time taken: 22.016894578933716
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.821
[2,     1] loss: 1258.290
[3,     1] loss: 1254.868
[4,     1] loss: 1253.905
[5,     1] loss: 1251.861
[6,     1] loss: 1251.086
[7,     1] loss: 1252.334
[8,     1] loss: 1250.284
[9,     1] loss: 1247.490
[10,     1] loss: 1243.457
[11,     1] loss: 1237.226
[12,     1] loss: 1227.087
[13,     1] loss: 1203.786
[14,     1] loss: 1189.765
[15,     1] loss: 1160.803
[16,     1] loss: 1125.524
[17,     1] loss: 1092.753
[18,     1] loss: 1036.903
[19,     1] loss: 1026.779
[20,     1] loss: 1024.405
[21,     1] loss: 1015.198
[22,     1] loss: 1013.707
[23,     1] loss: 956.746
[24,     1] loss: 968.286
[25,     1] loss: 956.158
[26,     1] loss: 977.027
[27,     1] loss: 946.637
[28,     1] loss: 954.472
[29,     1] loss: 985.867
[30,     1] loss: 896.927
[31,     1] loss: 932.198
[32,     1] loss: 900.973
[33,     1] loss: 901.730
[34,     1] loss: 888.071
[35,     1] loss: 930.929
[36,     1] loss: 911.220
[37,     1] loss: 883.666
[38,     1] loss: 905.722
[39,     1] loss: 862.311
[40,     1] loss: 861.643
[41,     1] loss: 841.885
[42,     1] loss: 899.480
[43,     1] loss: 807.987
[44,     1] loss: 821.716
[45,     1] loss: 851.229
[46,     1] loss: 895.672
[47,     1] loss: 753.528
[48,     1] loss: 859.836
[49,     1] loss: 778.708
[50,     1] loss: 788.893
[51,     1] loss: 761.621
[52,     1] loss: 772.968
[53,     1] loss: 732.049
[54,     1] loss: 781.443
[55,     1] loss: 790.269
[56,     1] loss: 774.362
[57,     1] loss: 800.692
[58,     1] loss: 734.988
[59,     1] loss: 699.017
[60,     1] loss: 740.237
[61,     1] loss: 816.784
[62,     1] loss: 785.511
[63,     1] loss: 722.557
[64,     1] loss: 717.891
[65,     1] loss: 686.376
[66,     1] loss: 689.765
[67,     1] loss: 688.812
[68,     1] loss: 658.233
[69,     1] loss: 639.517
[70,     1] loss: 647.235
[71,     1] loss: 619.837
[72,     1] loss: 642.500
[73,     1] loss: 651.337
[74,     1] loss: 608.822
[75,     1] loss: 721.548
[76,     1] loss: 678.917
[77,     1] loss: 673.494
Early stopping applied (best metric=0.42521318793296814)
Finished Training
Total time taken: 12.178156614303589
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.479
[2,     1] loss: 1251.277
[3,     1] loss: 1252.913
[4,     1] loss: 1251.716
[5,     1] loss: 1251.115
[6,     1] loss: 1251.473
[7,     1] loss: 1245.236
[8,     1] loss: 1244.149
[9,     1] loss: 1240.351
[10,     1] loss: 1228.009
[11,     1] loss: 1221.389
[12,     1] loss: 1197.461
[13,     1] loss: 1182.723
[14,     1] loss: 1124.983
[15,     1] loss: 1123.377
[16,     1] loss: 1080.144
[17,     1] loss: 1096.218
[18,     1] loss: 1049.852
[19,     1] loss: 1044.706
[20,     1] loss: 1058.624
[21,     1] loss: 1073.476
[22,     1] loss: 1043.849
[23,     1] loss: 1042.775
[24,     1] loss: 1013.413
[25,     1] loss: 1019.007
[26,     1] loss: 1033.890
[27,     1] loss: 1028.628
[28,     1] loss: 952.293
[29,     1] loss: 988.137
[30,     1] loss: 949.625
[31,     1] loss: 945.017
[32,     1] loss: 999.344
[33,     1] loss: 982.527
[34,     1] loss: 944.456
[35,     1] loss: 971.553
[36,     1] loss: 948.646
[37,     1] loss: 913.714
[38,     1] loss: 957.548
[39,     1] loss: 933.714
[40,     1] loss: 905.543
[41,     1] loss: 920.682
[42,     1] loss: 860.924
[43,     1] loss: 901.263
[44,     1] loss: 877.141
[45,     1] loss: 875.491
[46,     1] loss: 863.756
[47,     1] loss: 914.854
[48,     1] loss: 894.765
[49,     1] loss: 847.451
[50,     1] loss: 857.203
[51,     1] loss: 898.262
[52,     1] loss: 854.108
[53,     1] loss: 843.271
[54,     1] loss: 820.534
[55,     1] loss: 778.749
[56,     1] loss: 799.189
[57,     1] loss: 799.902
[58,     1] loss: 782.889
[59,     1] loss: 795.171
[60,     1] loss: 796.571
[61,     1] loss: 781.386
[62,     1] loss: 818.421
[63,     1] loss: 813.211
[64,     1] loss: 784.386
[65,     1] loss: 757.630
[66,     1] loss: 743.731
[67,     1] loss: 707.031
[68,     1] loss: 788.288
[69,     1] loss: 802.971
[70,     1] loss: 740.249
[71,     1] loss: 753.823
[72,     1] loss: 688.347
[73,     1] loss: 674.457
[74,     1] loss: 691.952
[75,     1] loss: 750.919
[76,     1] loss: 666.686
[77,     1] loss: 686.580
[78,     1] loss: 670.764
[79,     1] loss: 666.238
[80,     1] loss: 640.473
[81,     1] loss: 649.423
[82,     1] loss: 677.495
[83,     1] loss: 683.009
[84,     1] loss: 609.713
[85,     1] loss: 613.288
[86,     1] loss: 662.141
[87,     1] loss: 609.506
[88,     1] loss: 652.341
[89,     1] loss: 605.480
[90,     1] loss: 631.018
[91,     1] loss: 563.383
[92,     1] loss: 554.543
[93,     1] loss: 632.643
[94,     1] loss: 586.734
[95,     1] loss: 517.216
[96,     1] loss: 590.938
[97,     1] loss: 549.378
[98,     1] loss: 542.040
[99,     1] loss: 602.773
[100,     1] loss: 520.091
[101,     1] loss: 621.213
[102,     1] loss: 596.427
[103,     1] loss: 496.836
[104,     1] loss: 584.882
[105,     1] loss: 520.304
[106,     1] loss: 550.943
[107,     1] loss: 486.989
[108,     1] loss: 501.254
[109,     1] loss: 512.485
[110,     1] loss: 472.147
[111,     1] loss: 463.123
[112,     1] loss: 436.427
[113,     1] loss: 447.003
[114,     1] loss: 480.459
[115,     1] loss: 424.748
[116,     1] loss: 443.708
[117,     1] loss: 433.871
[118,     1] loss: 499.582
[119,     1] loss: 475.365
[120,     1] loss: 459.810
[121,     1] loss: 447.532
[122,     1] loss: 465.758
[123,     1] loss: 419.598
Early stopping applied (best metric=0.2936558425426483)
Finished Training
Total time taken: 20.42821455001831
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.972
[2,     1] loss: 1254.324
[3,     1] loss: 1254.311
[4,     1] loss: 1246.150
[5,     1] loss: 1251.660
[6,     1] loss: 1237.437
[7,     1] loss: 1221.841
[8,     1] loss: 1191.003
[9,     1] loss: 1158.149
[10,     1] loss: 1120.762
[11,     1] loss: 1086.902
[12,     1] loss: 1068.099
[13,     1] loss: 1066.131
[14,     1] loss: 1039.237
[15,     1] loss: 1006.011
[16,     1] loss: 971.310
[17,     1] loss: 1000.672
[18,     1] loss: 1016.759
[19,     1] loss: 995.280
[20,     1] loss: 1002.897
[21,     1] loss: 970.168
[22,     1] loss: 966.144
[23,     1] loss: 937.686
[24,     1] loss: 953.000
[25,     1] loss: 943.033
[26,     1] loss: 985.681
[27,     1] loss: 936.151
[28,     1] loss: 920.910
[29,     1] loss: 927.864
[30,     1] loss: 944.003
[31,     1] loss: 937.317
[32,     1] loss: 866.288
[33,     1] loss: 890.324
[34,     1] loss: 931.363
[35,     1] loss: 914.094
[36,     1] loss: 897.993
[37,     1] loss: 911.389
[38,     1] loss: 898.867
[39,     1] loss: 852.598
[40,     1] loss: 861.906
[41,     1] loss: 864.314
[42,     1] loss: 879.982
[43,     1] loss: 856.421
[44,     1] loss: 816.026
[45,     1] loss: 804.998
[46,     1] loss: 857.171
[47,     1] loss: 837.588
[48,     1] loss: 767.982
[49,     1] loss: 830.739
[50,     1] loss: 796.707
[51,     1] loss: 837.880
[52,     1] loss: 794.174
[53,     1] loss: 806.840
[54,     1] loss: 746.217
[55,     1] loss: 811.884
[56,     1] loss: 767.084
[57,     1] loss: 754.082
[58,     1] loss: 758.076
[59,     1] loss: 721.702
[60,     1] loss: 720.192
[61,     1] loss: 720.199
[62,     1] loss: 708.481
[63,     1] loss: 712.117
[64,     1] loss: 714.174
[65,     1] loss: 710.693
[66,     1] loss: 698.424
[67,     1] loss: 738.802
[68,     1] loss: 685.116
[69,     1] loss: 686.933
[70,     1] loss: 727.124
[71,     1] loss: 660.196
[72,     1] loss: 667.365
[73,     1] loss: 668.172
[74,     1] loss: 661.779
[75,     1] loss: 632.290
[76,     1] loss: 573.361
[77,     1] loss: 607.365
[78,     1] loss: 575.499
[79,     1] loss: 568.676
[80,     1] loss: 587.188
[81,     1] loss: 600.768
[82,     1] loss: 571.407
[83,     1] loss: 801.833
[84,     1] loss: 627.300
[85,     1] loss: 615.322
[86,     1] loss: 571.812
[87,     1] loss: 579.802
[88,     1] loss: 524.559
[89,     1] loss: 543.693
[90,     1] loss: 556.709
[91,     1] loss: 497.794
[92,     1] loss: 543.039
[93,     1] loss: 521.512
[94,     1] loss: 517.115
[95,     1] loss: 537.415
[96,     1] loss: 558.831
[97,     1] loss: 457.522
[98,     1] loss: 500.897
[99,     1] loss: 494.686
[100,     1] loss: 525.314
[101,     1] loss: 537.363
[102,     1] loss: 529.658
Early stopping applied (best metric=0.3838869333267212)
Finished Training
Total time taken: 14.067138433456421
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.268
[2,     1] loss: 1255.237
[3,     1] loss: 1262.417
[4,     1] loss: 1253.987
[5,     1] loss: 1252.524
[6,     1] loss: 1254.213
[7,     1] loss: 1251.141
[8,     1] loss: 1252.775
[9,     1] loss: 1252.755
[10,     1] loss: 1250.577
[11,     1] loss: 1251.013
[12,     1] loss: 1249.244
[13,     1] loss: 1251.244
[14,     1] loss: 1248.403
[15,     1] loss: 1245.599
[16,     1] loss: 1241.577
[17,     1] loss: 1234.792
[18,     1] loss: 1230.277
[19,     1] loss: 1214.277
[20,     1] loss: 1214.685
[21,     1] loss: 1181.881
[22,     1] loss: 1154.404
[23,     1] loss: 1142.053
[24,     1] loss: 1118.446
[25,     1] loss: 1105.026
[26,     1] loss: 1074.531
[27,     1] loss: 1097.640
[28,     1] loss: 1030.883
[29,     1] loss: 1004.045
[30,     1] loss: 1080.610
[31,     1] loss: 1001.217
[32,     1] loss: 1048.281
[33,     1] loss: 1002.346
[34,     1] loss: 1017.729
[35,     1] loss: 1005.264
[36,     1] loss: 991.942
[37,     1] loss: 998.106
[38,     1] loss: 965.806
[39,     1] loss: 977.250
[40,     1] loss: 968.324
[41,     1] loss: 943.200
[42,     1] loss: 935.748
[43,     1] loss: 972.005
[44,     1] loss: 865.766
[45,     1] loss: 937.389
[46,     1] loss: 963.526
[47,     1] loss: 913.828
[48,     1] loss: 941.898
[49,     1] loss: 893.360
[50,     1] loss: 913.036
[51,     1] loss: 922.907
[52,     1] loss: 870.216
[53,     1] loss: 881.744
[54,     1] loss: 867.638
[55,     1] loss: 878.347
[56,     1] loss: 934.212
[57,     1] loss: 839.445
[58,     1] loss: 927.295
[59,     1] loss: 853.422
[60,     1] loss: 853.281
[61,     1] loss: 854.552
[62,     1] loss: 833.656
[63,     1] loss: 836.000
[64,     1] loss: 837.073
[65,     1] loss: 784.688
[66,     1] loss: 833.211
[67,     1] loss: 790.943
[68,     1] loss: 806.752
[69,     1] loss: 800.933
[70,     1] loss: 761.486
[71,     1] loss: 767.987
[72,     1] loss: 758.118
[73,     1] loss: 731.142
[74,     1] loss: 711.013
[75,     1] loss: 746.747
[76,     1] loss: 745.703
[77,     1] loss: 724.722
[78,     1] loss: 717.285
[79,     1] loss: 734.968
[80,     1] loss: 695.565
[81,     1] loss: 691.208
Early stopping applied (best metric=0.35753607749938965)
Finished Training
Total time taken: 13.515097618103027
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1256.982
[2,     1] loss: 1259.827
[3,     1] loss: 1256.520
[4,     1] loss: 1256.213
[5,     1] loss: 1258.086
[6,     1] loss: 1256.183
[7,     1] loss: 1253.837
[8,     1] loss: 1254.589
[9,     1] loss: 1252.470
[10,     1] loss: 1250.071
[11,     1] loss: 1249.726
[12,     1] loss: 1247.501
[13,     1] loss: 1240.350
[14,     1] loss: 1234.641
[15,     1] loss: 1223.816
[16,     1] loss: 1203.473
[17,     1] loss: 1192.210
[18,     1] loss: 1159.320
[19,     1] loss: 1144.286
[20,     1] loss: 1103.254
[21,     1] loss: 1089.121
[22,     1] loss: 1067.208
[23,     1] loss: 1053.464
[24,     1] loss: 1046.052
[25,     1] loss: 1027.834
[26,     1] loss: 1030.207
[27,     1] loss: 1000.858
[28,     1] loss: 998.318
[29,     1] loss: 1022.017
[30,     1] loss: 969.185
[31,     1] loss: 973.328
[32,     1] loss: 977.600
[33,     1] loss: 988.817
[34,     1] loss: 983.818
[35,     1] loss: 968.370
[36,     1] loss: 940.407
[37,     1] loss: 936.260
[38,     1] loss: 980.012
[39,     1] loss: 993.283
[40,     1] loss: 962.368
[41,     1] loss: 949.665
[42,     1] loss: 917.360
[43,     1] loss: 913.652
[44,     1] loss: 913.983
[45,     1] loss: 918.164
[46,     1] loss: 920.874
[47,     1] loss: 869.108
[48,     1] loss: 866.058
[49,     1] loss: 885.994
[50,     1] loss: 850.988
[51,     1] loss: 856.120
[52,     1] loss: 888.439
[53,     1] loss: 867.501
[54,     1] loss: 857.000
[55,     1] loss: 836.353
[56,     1] loss: 816.502
[57,     1] loss: 808.850
[58,     1] loss: 805.991
[59,     1] loss: 833.445
[60,     1] loss: 833.538
[61,     1] loss: 785.583
[62,     1] loss: 801.575
[63,     1] loss: 844.761
[64,     1] loss: 793.585
[65,     1] loss: 770.336
[66,     1] loss: 778.146
[67,     1] loss: 774.737
[68,     1] loss: 754.567
[69,     1] loss: 784.933
[70,     1] loss: 733.882
[71,     1] loss: 683.050
[72,     1] loss: 706.793
[73,     1] loss: 689.993
[74,     1] loss: 723.080
[75,     1] loss: 632.477
[76,     1] loss: 692.704
[77,     1] loss: 632.743
[78,     1] loss: 728.156
[79,     1] loss: 736.736
[80,     1] loss: 775.573
[81,     1] loss: 665.669
[82,     1] loss: 707.155
[83,     1] loss: 649.994
[84,     1] loss: 660.176
[85,     1] loss: 653.412
[86,     1] loss: 618.516
[87,     1] loss: 598.089
[88,     1] loss: 631.931
[89,     1] loss: 649.115
[90,     1] loss: 646.133
[91,     1] loss: 621.371
[92,     1] loss: 590.298
[93,     1] loss: 584.679
[94,     1] loss: 576.476
[95,     1] loss: 574.371
[96,     1] loss: 607.120
[97,     1] loss: 548.941
[98,     1] loss: 597.296
[99,     1] loss: 547.940
[100,     1] loss: 560.324
[101,     1] loss: 566.943
[102,     1] loss: 563.417
[103,     1] loss: 597.914
[104,     1] loss: 563.178
[105,     1] loss: 574.076
[106,     1] loss: 576.427
[107,     1] loss: 464.429
[108,     1] loss: 565.172
[109,     1] loss: 526.112
[110,     1] loss: 563.661
[111,     1] loss: 526.314
[112,     1] loss: 516.458
[113,     1] loss: 573.331
[114,     1] loss: 481.679
[115,     1] loss: 547.113
[116,     1] loss: 497.195
[117,     1] loss: 497.111
[118,     1] loss: 498.142
[119,     1] loss: 497.711
[120,     1] loss: 464.429
[121,     1] loss: 455.915
[122,     1] loss: 469.638
[123,     1] loss: 505.552
[124,     1] loss: 489.011
[125,     1] loss: 518.117
[126,     1] loss: 496.872
[127,     1] loss: 445.362
[128,     1] loss: 446.651
[129,     1] loss: 587.228
[130,     1] loss: 458.056
[131,     1] loss: 550.301
[132,     1] loss: 454.614
[133,     1] loss: 468.115
Early stopping applied (best metric=0.3713766634464264)
Finished Training
Total time taken: 18.9966082572937
{'Hydroxylation-K Validation Accuracy': 0.735195035460993, 'Hydroxylation-K Validation Sensitivity': 0.5481481481481482, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.3894278433984316, 'Hydroxylation-K AUC ROC': 0.7275633528265107, 'Hydroxylation-K AUC PR': 0.5318219404811583, 'Hydroxylation-K MCC': 0.29450342678774233, 'Hydroxylation-K F1': 0.4502002610442508, 'Validation Loss (Hydroxylation-K)': 0.5356539587179819, 'Hydroxylation-P Validation Accuracy': 0.8107033991506353, 'Hydroxylation-P Validation Sensitivity': 0.7746560846560847, 'Hydroxylation-P Validation Specificity': 0.8185046635742431, 'Hydroxylation-P Validation Precision': 0.4873542959700315, 'Hydroxylation-P AUC ROC': 0.8405857672511869, 'Hydroxylation-P AUC PR': 0.6001468286249232, 'Hydroxylation-P MCC': 0.5054960650897353, 'Hydroxylation-P F1': 0.5946324003801791, 'Validation Loss (Hydroxylation-P)': 0.3662597358226776, 'Validation Loss (total)': 0.9019136985143026, 'TimeToTrain': 17.204858811696372}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009970045171241212,
 'learning_rate_Hydroxylation-K': 0.0077845319772151655,
 'learning_rate_Hydroxylation-P': 0.0020413675524943997,
 'log_base': 1.451804535214802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2236215383,
 'sample_weights': [1.6398002377122713, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.365196312355891,
 'weight_decay_Hydroxylation-K': 7.743108910343077,
 'weight_decay_Hydroxylation-P': 1.9087950015566402}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1849.590
[2,     1] loss: 1876.904
[3,     1] loss: 1854.438
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005307298598809368,
 'learning_rate_Hydroxylation-K': 0.001949286457588489,
 'learning_rate_Hydroxylation-P': 0.0033480198480949014,
 'log_base': 2.0381188796115226,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1308759131,
 'sample_weights': [4.47803246732841, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.999566370293069,
 'weight_decay_Hydroxylation-K': 4.86919293004248,
 'weight_decay_Hydroxylation-P': 1.925240859276756}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.776
[2,     1] loss: 1402.644
[3,     1] loss: 1402.305
[4,     1] loss: 1403.427
[5,     1] loss: 1400.053
[6,     1] loss: 1397.996
[7,     1] loss: 1389.785
[8,     1] loss: 1385.068
[9,     1] loss: 1368.279
[10,     1] loss: 1331.630
[11,     1] loss: 1282.969
[12,     1] loss: 1245.453
[13,     1] loss: 1237.277
[14,     1] loss: 1156.333
[15,     1] loss: 1154.728
[16,     1] loss: 1172.638
[17,     1] loss: 1105.761
[18,     1] loss: 1150.324
[19,     1] loss: 1288.274
[20,     1] loss: 1101.909
[21,     1] loss: 1197.849
[22,     1] loss: 1085.577
[23,     1] loss: 1111.055
[24,     1] loss: 1083.259
[25,     1] loss: 990.096
[26,     1] loss: 1099.754
[27,     1] loss: 1007.160
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008298003696365283,
 'learning_rate_Hydroxylation-K': 0.008281329591802994,
 'learning_rate_Hydroxylation-P': 0.003719988283317279,
 'log_base': 1.1796982024882956,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 568108345,
 'sample_weights': [2.34463373863418, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.786524706258108,
 'weight_decay_Hydroxylation-K': 8.335573398390714,
 'weight_decay_Hydroxylation-P': 0.5525509501329509}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3273.173
[2,     1] loss: 3276.131
[3,     1] loss: 3272.919
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005948634682938203,
 'learning_rate_Hydroxylation-K': 0.008323436659481478,
 'learning_rate_Hydroxylation-P': 0.0032976961092797278,
 'log_base': 1.1128913111209333,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2124901132,
 'sample_weights': [10.102001900027707, 1.2627988837133486],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.193615505623674,
 'weight_decay_Hydroxylation-K': 2.231897739869565,
 'weight_decay_Hydroxylation-P': 1.1795439518815454}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5080.988
[2,     1] loss: 5115.070
[3,     1] loss: 5052.569
[4,     1] loss: 5105.373
[5,     1] loss: 5054.135
[6,     1] loss: 5088.067
[7,     1] loss: 5048.929
[8,     1] loss: 5054.478
[9,     1] loss: 5045.938
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068629977763537495,
 'learning_rate_Hydroxylation-K': 0.006757309956633876,
 'learning_rate_Hydroxylation-P': 0.0031147163408337688,
 'log_base': 1.1501102570725688,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3618285043,
 'sample_weights': [15.607900943761317, 1.9510627778476513],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.8976727461873235,
 'weight_decay_Hydroxylation-K': 0.4974021178120681,
 'weight_decay_Hydroxylation-P': 1.703186189133203}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3888.244
[2,     1] loss: 3855.937
[3,     1] loss: 3883.160
[4,     1] loss: 3851.238
[5,     1] loss: 3942.891
[6,     1] loss: 3905.038
[7,     1] loss: 3866.102
[8,     1] loss: 3848.093
[9,     1] loss: 3852.488
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009841180195506647,
 'learning_rate_Hydroxylation-K': 0.009486518440890112,
 'learning_rate_Hydroxylation-P': 0.00046416502527363117,
 'log_base': 1.3550961271187874,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2533433795,
 'sample_weights': [11.936717053620285, 1.4921471129868271],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.671476063964326,
 'weight_decay_Hydroxylation-K': 9.005493380228899,
 'weight_decay_Hydroxylation-P': 4.108391290670828}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2063.356
[2,     1] loss: 2063.077
[3,     1] loss: 2058.745
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005442003424098412,
 'learning_rate_Hydroxylation-K': 0.0074188154554727355,
 'learning_rate_Hydroxylation-P': 0.005402951368871211,
 'log_base': 1.0796712261601524,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4219818745,
 'sample_weights': [5.493895393354647, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.150101153236116,
 'weight_decay_Hydroxylation-K': 1.441137504140745,
 'weight_decay_Hydroxylation-P': 3.517621898382675}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7111.026
[2,     1] loss: 7104.190
[3,     1] loss: 7062.880
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026098731758717178,
 'learning_rate_Hydroxylation-K': 0.009948055864742022,
 'learning_rate_Hydroxylation-P': 0.001148597900679687,
 'log_base': 1.5060566750796347,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 683341335,
 'sample_weights': [21.778212199205562, 2.722381397924087],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.213405820871231,
 'weight_decay_Hydroxylation-K': 7.197845476603191,
 'weight_decay_Hydroxylation-P': 2.4351929303833764}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1764.515
[2,     1] loss: 1761.538
[3,     1] loss: 1762.507
[4,     1] loss: 1767.750
[5,     1] loss: 1760.738
[6,     1] loss: 1765.611
[7,     1] loss: 1760.327
[8,     1] loss: 1766.573
[9,     1] loss: 1754.745
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027435321632715394,
 'learning_rate_Hydroxylation-K': 0.0016931865454976986,
 'learning_rate_Hydroxylation-P': 0.009782800386968818,
 'log_base': 1.2119231785718763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3752433386,
 'sample_weights': [4.0768363951912345, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2839639740426416,
 'weight_decay_Hydroxylation-K': 9.754551277124742,
 'weight_decay_Hydroxylation-P': 2.359931467157377}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2823.308
[2,     1] loss: 2800.930
[3,     1] loss: 2826.692
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003609078202598,
 'learning_rate_Hydroxylation-K': 0.009927934036799556,
 'learning_rate_Hydroxylation-P': 0.00569490353667184,
 'log_base': 1.0788541327657557,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 777038839,
 'sample_weights': [8.685584317751527, 1.0857398651671915],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2191346812689803,
 'weight_decay_Hydroxylation-K': 0.2020077216726138,
 'weight_decay_Hydroxylation-P': 1.1647883551429539}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7140.126
[2,     1] loss: 7188.696
[3,     1] loss: 7157.621
[4,     1] loss: 7130.034
[5,     1] loss: 7129.896
[6,     1] loss: 7127.186
[7,     1] loss: 7125.028
[8,     1] loss: 7119.165
[9,     1] loss: 7116.445
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006073651083884361,
 'learning_rate_Hydroxylation-K': 0.0009360404667253059,
 'learning_rate_Hydroxylation-P': 0.005920620059680154,
 'log_base': 2.668437513206251,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 122954924,
 'sample_weights': [21.99544626459275, 2.749536702188552],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.008940923312123,
 'weight_decay_Hydroxylation-K': 3.5724676760366374,
 'weight_decay_Hydroxylation-P': 3.692772108569003}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.061
[2,     1] loss: 1265.258
[3,     1] loss: 1266.096
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008050309900978122,
 'learning_rate_Hydroxylation-K': 0.008892719552900134,
 'learning_rate_Hydroxylation-P': 0.0033654872965185045,
 'log_base': 1.019049329533452,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2247742848,
 'sample_weights': [1.7009219395496218, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.072795830580247,
 'weight_decay_Hydroxylation-K': 0.2709560924996565,
 'weight_decay_Hydroxylation-P': 0.20641748539120763}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28646.383
Exploding loss, terminate run (best metric=0.5392585396766663)
Finished Training
Total time taken: 0.2069997787475586
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28724.029
Exploding loss, terminate run (best metric=0.5266323089599609)
Finished Training
Total time taken: 0.22300267219543457
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28674.160
Exploding loss, terminate run (best metric=0.527140200138092)
Finished Training
Total time taken: 0.22100067138671875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28747.510
Exploding loss, terminate run (best metric=0.526800274848938)
Finished Training
Total time taken: 0.20600080490112305
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28910.039
Exploding loss, terminate run (best metric=0.5468927025794983)
Finished Training
Total time taken: 0.22600102424621582
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28765.633
Exploding loss, terminate run (best metric=0.5364468097686768)
Finished Training
Total time taken: 0.2240312099456787
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28752.418
Exploding loss, terminate run (best metric=0.5361636877059937)
Finished Training
Total time taken: 0.2480010986328125
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28775.473
Exploding loss, terminate run (best metric=0.5314928293228149)
Finished Training
Total time taken: 0.2440028190612793
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28858.289
Exploding loss, terminate run (best metric=0.5273056626319885)
Finished Training
Total time taken: 0.2200000286102295
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28698.156
Exploding loss, terminate run (best metric=0.5376754999160767)
Finished Training
Total time taken: 0.20600008964538574
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28700.637
Exploding loss, terminate run (best metric=0.5314465761184692)
Finished Training
Total time taken: 0.22600078582763672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28794.623
Exploding loss, terminate run (best metric=0.5452147126197815)
Finished Training
Total time taken: 0.22700071334838867
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28665.699
Exploding loss, terminate run (best metric=0.5338087677955627)
Finished Training
Total time taken: 0.23699951171875
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28731.988
Exploding loss, terminate run (best metric=0.536661684513092)
Finished Training
Total time taken: 0.2220010757446289
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28776.484
Exploding loss, terminate run (best metric=0.5283995866775513)
Finished Training
Total time taken: 0.21200251579284668
{'Hydroxylation-K Validation Accuracy': 0.4805555555555555, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5612865497076023, 'Hydroxylation-K AUC PR': 0.25564459638904213, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1792282430213465, 'Validation Loss (Hydroxylation-K)': 0.5606459061304728, 'Hydroxylation-P Validation Accuracy': 0.4791568617498266, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5650796975980277, 'Hydroxylation-P AUC PR': 0.25176928980267776, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.1608176412279126, 'Validation Loss (Hydroxylation-P)': 0.5340893228848775, 'Validation Loss (total)': 1.0947352488835653, 'TimeToTrain': 0.2232696533203125}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003785894299869476,
 'learning_rate_Hydroxylation-K': 0.006133811824445466,
 'learning_rate_Hydroxylation-P': 0.0027546707221740236,
 'log_base': 2.0893149252387575,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1094390502,
 'sample_weights': [88.53561857770755, 11.043957150600372],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.1251030576682375,
 'weight_decay_Hydroxylation-K': 0.22066131878168882,
 'weight_decay_Hydroxylation-P': 8.379967166774389}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.135
[2,     1] loss: 1384.143
[3,     1] loss: 1387.049
[4,     1] loss: 1387.002
[5,     1] loss: 1386.792
[6,     1] loss: 1385.900
[7,     1] loss: 1386.185
[8,     1] loss: 1383.028
[9,     1] loss: 1383.877
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006118462855089671,
 'learning_rate_Hydroxylation-K': 0.009368577466260011,
 'learning_rate_Hydroxylation-P': 0.0064575329541609276,
 'log_base': 1.030570241432488,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4273558793,
 'sample_weights': [2.265690923107043, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.8097721693426205,
 'weight_decay_Hydroxylation-K': 5.006182096168861,
 'weight_decay_Hydroxylation-P': 3.022000145820329}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17995.543
Exploding loss, terminate run (best metric=0.5353139042854309)
Finished Training
Total time taken: 0.20600199699401855
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17968.102
Exploding loss, terminate run (best metric=0.5309897661209106)
Finished Training
Total time taken: 0.23700284957885742
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17957.852
Exploding loss, terminate run (best metric=0.529283881187439)
Finished Training
Total time taken: 0.23999905586242676
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18012.248
Exploding loss, terminate run (best metric=0.5446165204048157)
Finished Training
Total time taken: 0.2239999771118164
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18019.830
Exploding loss, terminate run (best metric=0.528251051902771)
Finished Training
Total time taken: 0.2065136432647705
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18082.926
Exploding loss, terminate run (best metric=0.5506950616836548)
Finished Training
Total time taken: 0.22499871253967285
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18089.252
Exploding loss, terminate run (best metric=0.5312368869781494)
Finished Training
Total time taken: 0.23200130462646484
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17943.340
Exploding loss, terminate run (best metric=0.5347699522972107)
Finished Training
Total time taken: 0.23003053665161133
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17964.434
Exploding loss, terminate run (best metric=0.5303435325622559)
Finished Training
Total time taken: 0.21000051498413086
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18023.336
Exploding loss, terminate run (best metric=0.5287343263626099)
Finished Training
Total time taken: 0.22500061988830566
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17967.273
Exploding loss, terminate run (best metric=0.5344784259796143)
Finished Training
Total time taken: 0.2310025691986084
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17978.182
Exploding loss, terminate run (best metric=0.5344474911689758)
Finished Training
Total time taken: 0.22101449966430664
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18041.244
Exploding loss, terminate run (best metric=0.5278146862983704)
Finished Training
Total time taken: 0.20600032806396484
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18025.449
Exploding loss, terminate run (best metric=0.5297088623046875)
Finished Training
Total time taken: 0.22600078582763672
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18008.180
Exploding loss, terminate run (best metric=0.528570830821991)
Finished Training
Total time taken: 0.23199868202209473
{'Hydroxylation-K Validation Accuracy': 0.5944148936170213, 'Hydroxylation-K Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-K Validation Specificity': 0.6631578947368421, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6257309941520468, 'Hydroxylation-K AUC PR': 0.34163325629990593, 'Hydroxylation-K MCC': -0.00977212618581356, 'Hydroxylation-K F1': 0.11026272577996717, 'Validation Loss (Hydroxylation-K)': 0.5592751344045003, 'Hydroxylation-P Validation Accuracy': 0.6066031165930663, 'Hydroxylation-P Validation Sensitivity': 0.34285714285714286, 'Hydroxylation-P Validation Specificity': 0.6630081300813008, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6082559514902774, 'Hydroxylation-P AUC PR': 0.27087707716669646, 'Hydroxylation-P MCC': 0.015051361883571491, 'Hydroxylation-P F1': 0.1154190613379388, 'Validation Loss (Hydroxylation-P)': 0.5332836786905925, 'Validation Loss (total)': 1.0925588130950927, 'TimeToTrain': 0.2234377384185791}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004835252776427935,
 'learning_rate_Hydroxylation-K': 0.00785052831573446,
 'learning_rate_Hydroxylation-P': 0.005377922220717223,
 'log_base': 1.0290125348625574,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1125382431,
 'sample_weights': [55.481732181063, 6.920806368014999],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.048308512234514,
 'weight_decay_Hydroxylation-K': 6.498111025992084,
 'weight_decay_Hydroxylation-P': 3.534823881071037}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18898.076
Exploding loss, terminate run (best metric=0.5357622504234314)
Finished Training
Total time taken: 0.20380163192749023
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19027.680
Exploding loss, terminate run (best metric=0.5293008089065552)
Finished Training
Total time taken: 0.23500323295593262
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19081.799
Exploding loss, terminate run (best metric=0.5353565812110901)
Finished Training
Total time taken: 0.24000096321105957
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18935.301
Exploding loss, terminate run (best metric=0.5270792245864868)
Finished Training
Total time taken: 0.22400212287902832
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18950.914
Exploding loss, terminate run (best metric=0.534139096736908)
Finished Training
Total time taken: 0.21399903297424316
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18892.920
Exploding loss, terminate run (best metric=0.5343484878540039)
Finished Training
Total time taken: 0.22300028800964355
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18919.117
Exploding loss, terminate run (best metric=0.5266121625900269)
Finished Training
Total time taken: 0.2180020809173584
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19062.869
Exploding loss, terminate run (best metric=0.5305570960044861)
Finished Training
Total time taken: 0.22100114822387695
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18916.957
Exploding loss, terminate run (best metric=0.5270503163337708)
Finished Training
Total time taken: 0.2090005874633789
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18927.975
Exploding loss, terminate run (best metric=0.5275119543075562)
Finished Training
Total time taken: 0.22700285911560059
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19023.545
Exploding loss, terminate run (best metric=0.5313558578491211)
Finished Training
Total time taken: 0.22702741622924805
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18902.879
Exploding loss, terminate run (best metric=0.5319024920463562)
Finished Training
Total time taken: 0.20602822303771973
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18994.262
Exploding loss, terminate run (best metric=0.5314117074012756)
Finished Training
Total time taken: 0.2070009708404541
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18978.059
Exploding loss, terminate run (best metric=0.5378990769386292)
Finished Training
Total time taken: 0.22899842262268066
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18940.102
Exploding loss, terminate run (best metric=0.5277286171913147)
Finished Training
Total time taken: 0.24100136756896973
{'Hydroxylation-K Validation Accuracy': 0.5574763593380614, 'Hydroxylation-K Validation Sensitivity': 0.41333333333333333, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6309356725146199, 'Hydroxylation-K AUC PR': 0.35415132451197734, 'Hydroxylation-K MCC': 0.01942647457028731, 'Hydroxylation-K F1': 0.1522041177213591, 'Validation Loss (Hydroxylation-K)': 0.5566479921340942, 'Hydroxylation-P Validation Accuracy': 0.5659486658883643, 'Hydroxylation-P Validation Sensitivity': 0.4114285714285714, 'Hydroxylation-P Validation Specificity': 0.5991869918699188, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6232907749030747, 'Hydroxylation-P AUC PR': 0.31375848173198184, 'Hydroxylation-P MCC': 0.020574880114986635, 'Hydroxylation-P F1': 0.13882991864927882, 'Validation Loss (Hydroxylation-P)': 0.5312010486920674, 'Validation Loss (total)': 1.0878490447998046, 'TimeToTrain': 0.22165802319844563}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009204915426040742,
 'learning_rate_Hydroxylation-K': 0.005988415911641048,
 'learning_rate_Hydroxylation-P': 0.005655908760691089,
 'log_base': 2.8383454730804956,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2718542368,
 'sample_weights': [58.41617700993803, 7.286849814387402],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.234828833912845,
 'weight_decay_Hydroxylation-K': 0.952525249140397,
 'weight_decay_Hydroxylation-P': 8.119953231653753}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.254
[2,     1] loss: 1245.500
[3,     1] loss: 1248.189
[4,     1] loss: 1247.448
[5,     1] loss: 1248.169
[6,     1] loss: 1241.510
[7,     1] loss: 1240.166
[8,     1] loss: 1240.051
[9,     1] loss: 1235.097
[10,     1] loss: 1224.606
[11,     1] loss: 1215.067
[12,     1] loss: 1175.033
[13,     1] loss: 1153.106
[14,     1] loss: 1135.844
[15,     1] loss: 1075.930
[16,     1] loss: 1054.411
[17,     1] loss: 1065.591
[18,     1] loss: 995.940
[19,     1] loss: 1125.275
[20,     1] loss: 1031.645
[21,     1] loss: 1040.804
[22,     1] loss: 1023.369
[23,     1] loss: 1028.661
[24,     1] loss: 1018.405
[25,     1] loss: 1014.381
[26,     1] loss: 949.843
[27,     1] loss: 959.746
[28,     1] loss: 985.302
[29,     1] loss: 948.470
[30,     1] loss: 978.747
[31,     1] loss: 937.768
[32,     1] loss: 926.619
[33,     1] loss: 952.869
[34,     1] loss: 917.853
[35,     1] loss: 889.005
[36,     1] loss: 867.233
[37,     1] loss: 874.394
[38,     1] loss: 1084.516
[39,     1] loss: 940.789
[40,     1] loss: 903.579
[41,     1] loss: 882.203
[42,     1] loss: 941.332
[43,     1] loss: 847.050
[44,     1] loss: 938.169
[45,     1] loss: 851.577
[46,     1] loss: 967.293
[47,     1] loss: 883.158
[48,     1] loss: 890.061
[49,     1] loss: 833.840
[50,     1] loss: 838.338
[51,     1] loss: 815.381
[52,     1] loss: 801.939
[53,     1] loss: 847.983
[54,     1] loss: 896.003
[55,     1] loss: 773.253
[56,     1] loss: 749.768
[57,     1] loss: 776.993
[58,     1] loss: 708.327
[59,     1] loss: 757.270
[60,     1] loss: 706.694
[61,     1] loss: 708.489
[62,     1] loss: 625.627
[63,     1] loss: 703.989
[64,     1] loss: 911.057
[65,     1] loss: 1485.476
[66,     1] loss: 852.145
[67,     1] loss: 1084.483
[68,     1] loss: 927.855
[69,     1] loss: 957.984
[70,     1] loss: 987.446
[71,     1] loss: 1013.267
[72,     1] loss: 957.500
[73,     1] loss: 901.077
[74,     1] loss: 929.729
[75,     1] loss: 896.680
[76,     1] loss: 861.236
[77,     1] loss: 860.801
[78,     1] loss: 932.095
[79,     1] loss: 746.293
[80,     1] loss: 820.762
[81,     1] loss: 734.695
[82,     1] loss: 737.377
[83,     1] loss: 696.299
[84,     1] loss: 701.985
[85,     1] loss: 664.183
Early stopping applied (best metric=0.3390859067440033)
Finished Training
Total time taken: 11.899197578430176
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.771
[2,     1] loss: 1249.390
[3,     1] loss: 1246.668
[4,     1] loss: 1248.496
[5,     1] loss: 1246.329
[6,     1] loss: 1245.882
[7,     1] loss: 1242.273
[8,     1] loss: 1244.513
[9,     1] loss: 1243.041
[10,     1] loss: 1243.942
[11,     1] loss: 1241.989
[12,     1] loss: 1242.194
[13,     1] loss: 1247.064
[14,     1] loss: 1227.094
[15,     1] loss: 1228.569
[16,     1] loss: 1216.367
[17,     1] loss: 1184.560
[18,     1] loss: 1171.505
[19,     1] loss: 1150.669
[20,     1] loss: 1115.554
[21,     1] loss: 1099.364
[22,     1] loss: 1038.151
[23,     1] loss: 1030.501
[24,     1] loss: 1018.069
[25,     1] loss: 1058.517
[26,     1] loss: 1000.014
[27,     1] loss: 1000.141
[28,     1] loss: 999.562
[29,     1] loss: 984.498
[30,     1] loss: 934.037
[31,     1] loss: 955.343
[32,     1] loss: 963.396
[33,     1] loss: 924.338
[34,     1] loss: 942.594
[35,     1] loss: 926.502
[36,     1] loss: 922.672
[37,     1] loss: 911.119
[38,     1] loss: 978.740
[39,     1] loss: 923.656
[40,     1] loss: 879.485
[41,     1] loss: 922.407
[42,     1] loss: 868.408
[43,     1] loss: 905.798
[44,     1] loss: 886.096
[45,     1] loss: 813.771
[46,     1] loss: 881.949
[47,     1] loss: 792.262
[48,     1] loss: 863.000
[49,     1] loss: 837.909
[50,     1] loss: 760.623
[51,     1] loss: 751.703
[52,     1] loss: 772.019
[53,     1] loss: 776.119
[54,     1] loss: 796.146
[55,     1] loss: 729.611
[56,     1] loss: 703.032
[57,     1] loss: 732.935
[58,     1] loss: 1064.473
[59,     1] loss: 1575.762
[60,     1] loss: 819.550
[61,     1] loss: 958.540
[62,     1] loss: 1105.799
[63,     1] loss: 1016.036
[64,     1] loss: 1034.637
[65,     1] loss: 1020.134
[66,     1] loss: 1027.662
[67,     1] loss: 1040.314
[68,     1] loss: 985.746
[69,     1] loss: 942.453
[70,     1] loss: 963.105
[71,     1] loss: 949.748
[72,     1] loss: 910.260
[73,     1] loss: 892.800
[74,     1] loss: 874.091
[75,     1] loss: 877.486
[76,     1] loss: 856.086
[77,     1] loss: 868.788
[78,     1] loss: 868.604
[79,     1] loss: 799.504
[80,     1] loss: 831.340
[81,     1] loss: 792.606
[82,     1] loss: 755.260
[83,     1] loss: 784.939
[84,     1] loss: 709.076
[85,     1] loss: 674.061
[86,     1] loss: 838.306
[87,     1] loss: 1247.932
[88,     1] loss: 924.237
[89,     1] loss: 870.440
[90,     1] loss: 872.114
[91,     1] loss: 934.268
[92,     1] loss: 880.744
[93,     1] loss: 829.282
[94,     1] loss: 848.769
[95,     1] loss: 841.673
[96,     1] loss: 855.334
[97,     1] loss: 808.861
[98,     1] loss: 799.769
[99,     1] loss: 806.825
[100,     1] loss: 753.241
[101,     1] loss: 736.437
[102,     1] loss: 778.625
[103,     1] loss: 707.120
[104,     1] loss: 718.841
[105,     1] loss: 740.338
[106,     1] loss: 733.493
[107,     1] loss: 651.021
[108,     1] loss: 619.273
[109,     1] loss: 614.140
[110,     1] loss: 772.424
[111,     1] loss: 2153.938
[112,     1] loss: 906.526
[113,     1] loss: 1027.979
[114,     1] loss: 979.554
[115,     1] loss: 1066.397
[116,     1] loss: 1101.433
[117,     1] loss: 1032.261
[118,     1] loss: 1001.645
[119,     1] loss: 983.769
[120,     1] loss: 1047.708
[121,     1] loss: 995.486
[122,     1] loss: 973.970
[123,     1] loss: 965.516
[124,     1] loss: 946.729
[125,     1] loss: 917.656
[126,     1] loss: 931.099
[127,     1] loss: 901.464
[128,     1] loss: 895.895
[129,     1] loss: 884.763
[130,     1] loss: 833.130
[131,     1] loss: 873.199
[132,     1] loss: 820.444
[133,     1] loss: 838.845
[134,     1] loss: 856.999
[135,     1] loss: 859.304
[136,     1] loss: 815.936
[137,     1] loss: 814.587
[138,     1] loss: 805.682
[139,     1] loss: 786.992
[140,     1] loss: 756.708
[141,     1] loss: 765.107
[142,     1] loss: 823.703
[143,     1] loss: 884.727
[144,     1] loss: 834.483
[145,     1] loss: 731.224
[146,     1] loss: 849.826
[147,     1] loss: 760.351
[148,     1] loss: 851.095
[149,     1] loss: 731.583
[150,     1] loss: 706.880
[151,     1] loss: 724.100
[152,     1] loss: 679.197
[153,     1] loss: 728.638
[154,     1] loss: 1182.878
[155,     1] loss: 1041.715
[156,     1] loss: 814.758
[157,     1] loss: 869.715
[158,     1] loss: 879.590
[159,     1] loss: 855.534
[160,     1] loss: 893.647
[161,     1] loss: 833.074
[162,     1] loss: 831.193
[163,     1] loss: 751.836
[164,     1] loss: 810.771
[165,     1] loss: 746.051
[166,     1] loss: 792.738
[167,     1] loss: 738.843
[168,     1] loss: 695.751
[169,     1] loss: 723.567
[170,     1] loss: 655.832
[171,     1] loss: 645.431
[172,     1] loss: 621.743
[173,     1] loss: 606.664
[174,     1] loss: 663.783
[175,     1] loss: 763.564
[176,     1] loss: 1707.311
[177,     1] loss: 1507.196
[178,     1] loss: 1153.653
[179,     1] loss: 1156.625
[180,     1] loss: 1223.641
[181,     1] loss: 1232.654
[182,     1] loss: 1233.234
[183,     1] loss: 1233.799
[184,     1] loss: 1230.668
[185,     1] loss: 1234.053
[186,     1] loss: 1233.842
[187,     1] loss: 1238.105
[188,     1] loss: 1225.760
[189,     1] loss: 1223.245
[190,     1] loss: 1218.653
[191,     1] loss: 1211.079
[192,     1] loss: 1197.901
[193,     1] loss: 1180.189
[194,     1] loss: 1160.081
[195,     1] loss: 1105.708
[196,     1] loss: 1074.561
[197,     1] loss: 1094.006
[198,     1] loss: 1097.097
[199,     1] loss: 1242.988
[200,     1] loss: 1075.581
Finished Training
Total time taken: 31.588500499725342
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.939
[2,     1] loss: 1254.591
[3,     1] loss: 1247.677
[4,     1] loss: 1251.394
[5,     1] loss: 1247.835
[6,     1] loss: 1245.426
[7,     1] loss: 1249.911
[8,     1] loss: 1245.466
[9,     1] loss: 1249.798
[10,     1] loss: 1244.767
[11,     1] loss: 1245.656
[12,     1] loss: 1245.282
[13,     1] loss: 1246.587
[14,     1] loss: 1244.161
[15,     1] loss: 1245.012
[16,     1] loss: 1244.324
[17,     1] loss: 1244.637
[18,     1] loss: 1247.329
[19,     1] loss: 1244.936
[20,     1] loss: 1244.469
[21,     1] loss: 1243.820
[22,     1] loss: 1243.622
[23,     1] loss: 1243.222
[24,     1] loss: 1240.911
[25,     1] loss: 1237.507
[26,     1] loss: 1234.075
[27,     1] loss: 1226.472
[28,     1] loss: 1213.289
[29,     1] loss: 1198.806
[30,     1] loss: 1183.438
[31,     1] loss: 1161.337
[32,     1] loss: 1124.809
[33,     1] loss: 1159.568
[34,     1] loss: 1121.698
[35,     1] loss: 1118.882
[36,     1] loss: 1110.177
[37,     1] loss: 1107.733
[38,     1] loss: 1095.343
[39,     1] loss: 1099.924
[40,     1] loss: 1053.228
[41,     1] loss: 1037.217
[42,     1] loss: 1029.474
[43,     1] loss: 974.920
[44,     1] loss: 1023.285
[45,     1] loss: 981.553
[46,     1] loss: 994.497
[47,     1] loss: 1026.803
[48,     1] loss: 1143.573
[49,     1] loss: 973.217
[50,     1] loss: 1016.881
[51,     1] loss: 986.261
[52,     1] loss: 1007.273
[53,     1] loss: 996.749
[54,     1] loss: 971.048
[55,     1] loss: 988.196
[56,     1] loss: 970.824
[57,     1] loss: 967.919
[58,     1] loss: 972.250
[59,     1] loss: 909.831
[60,     1] loss: 927.707
[61,     1] loss: 947.877
[62,     1] loss: 937.681
[63,     1] loss: 864.033
[64,     1] loss: 900.643
[65,     1] loss: 886.275
[66,     1] loss: 831.027
[67,     1] loss: 815.812
[68,     1] loss: 910.321
[69,     1] loss: 1097.988
[70,     1] loss: 936.381
[71,     1] loss: 939.544
[72,     1] loss: 879.158
[73,     1] loss: 905.988
[74,     1] loss: 839.561
[75,     1] loss: 875.500
[76,     1] loss: 836.998
[77,     1] loss: 838.562
[78,     1] loss: 791.701
[79,     1] loss: 782.799
[80,     1] loss: 804.586
[81,     1] loss: 758.753
[82,     1] loss: 741.617
[83,     1] loss: 710.987
[84,     1] loss: 751.555
[85,     1] loss: 1657.823
[86,     1] loss: 1643.527
[87,     1] loss: 1363.991
[88,     1] loss: 1124.827
[89,     1] loss: 1111.673
[90,     1] loss: 1152.546
[91,     1] loss: 1189.846
[92,     1] loss: 1244.716
[93,     1] loss: 1186.791
[94,     1] loss: 1183.786
[95,     1] loss: 1198.726
[96,     1] loss: 1196.310
[97,     1] loss: 1190.786
[98,     1] loss: 1164.102
[99,     1] loss: 1142.001
[100,     1] loss: 1128.603
[101,     1] loss: 1132.928
[102,     1] loss: 1104.944
[103,     1] loss: 1086.547
[104,     1] loss: 1096.696
[105,     1] loss: 1047.747
[106,     1] loss: 1041.944
[107,     1] loss: 997.424
[108,     1] loss: 1009.648
[109,     1] loss: 1012.454
[110,     1] loss: 981.677
[111,     1] loss: 991.323
[112,     1] loss: 967.556
[113,     1] loss: 1003.404
[114,     1] loss: 984.195
[115,     1] loss: 940.401
[116,     1] loss: 957.777
[117,     1] loss: 934.632
[118,     1] loss: 926.316
[119,     1] loss: 929.598
[120,     1] loss: 898.319
[121,     1] loss: 951.221
[122,     1] loss: 1060.050
[123,     1] loss: 958.631
[124,     1] loss: 968.356
Early stopping applied (best metric=0.2851259410381317)
Finished Training
Total time taken: 18.95862865447998
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.251
[2,     1] loss: 1249.600
[3,     1] loss: 1245.944
[4,     1] loss: 1249.009
[5,     1] loss: 1246.997
[6,     1] loss: 1244.147
[7,     1] loss: 1245.408
[8,     1] loss: 1242.126
[9,     1] loss: 1241.814
[10,     1] loss: 1241.012
[11,     1] loss: 1238.411
[12,     1] loss: 1234.396
[13,     1] loss: 1227.117
[14,     1] loss: 1213.777
[15,     1] loss: 1195.982
[16,     1] loss: 1143.987
[17,     1] loss: 1134.822
[18,     1] loss: 1048.309
[19,     1] loss: 1059.605
[20,     1] loss: 1061.799
[21,     1] loss: 1021.925
[22,     1] loss: 1044.249
[23,     1] loss: 1029.964
[24,     1] loss: 1002.014
[25,     1] loss: 988.804
[26,     1] loss: 977.387
[27,     1] loss: 955.524
[28,     1] loss: 946.741
[29,     1] loss: 957.158
[30,     1] loss: 945.204
[31,     1] loss: 911.088
[32,     1] loss: 907.662
[33,     1] loss: 910.488
[34,     1] loss: 879.538
[35,     1] loss: 906.034
[36,     1] loss: 971.879
[37,     1] loss: 895.201
[38,     1] loss: 829.895
[39,     1] loss: 861.604
[40,     1] loss: 792.417
[41,     1] loss: 802.205
[42,     1] loss: 827.229
[43,     1] loss: 831.077
[44,     1] loss: 852.099
[45,     1] loss: 970.804
[46,     1] loss: 937.000
[47,     1] loss: 806.903
[48,     1] loss: 884.196
[49,     1] loss: 890.078
[50,     1] loss: 851.278
[51,     1] loss: 842.889
[52,     1] loss: 840.497
[53,     1] loss: 801.487
[54,     1] loss: 772.856
[55,     1] loss: 741.162
[56,     1] loss: 756.944
[57,     1] loss: 806.046
[58,     1] loss: 767.358
[59,     1] loss: 695.787
[60,     1] loss: 680.138
[61,     1] loss: 751.971
[62,     1] loss: 686.823
[63,     1] loss: 643.995
[64,     1] loss: 620.145
[65,     1] loss: 560.098
[66,     1] loss: 605.761
[67,     1] loss: 2199.479
[68,     1] loss: 1078.517
[69,     1] loss: 717.120
[70,     1] loss: 871.146
[71,     1] loss: 995.656
[72,     1] loss: 985.382
[73,     1] loss: 951.226
[74,     1] loss: 975.644
[75,     1] loss: 971.893
[76,     1] loss: 992.326
[77,     1] loss: 917.997
[78,     1] loss: 882.672
[79,     1] loss: 910.710
[80,     1] loss: 900.508
[81,     1] loss: 864.332
Early stopping applied (best metric=0.4037325978279114)
Finished Training
Total time taken: 13.56704568862915
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1253.494
[2,     1] loss: 1256.596
[3,     1] loss: 1250.203
[4,     1] loss: 1250.833
[5,     1] loss: 1249.090
[6,     1] loss: 1245.954
[7,     1] loss: 1252.309
[8,     1] loss: 1248.234
[9,     1] loss: 1245.852
[10,     1] loss: 1247.125
[11,     1] loss: 1245.331
[12,     1] loss: 1246.905
[13,     1] loss: 1248.735
[14,     1] loss: 1245.238
[15,     1] loss: 1247.412
[16,     1] loss: 1245.443
[17,     1] loss: 1243.802
[18,     1] loss: 1249.518
[19,     1] loss: 1249.035
[20,     1] loss: 1246.281
[21,     1] loss: 1245.921
[22,     1] loss: 1246.452
[23,     1] loss: 1247.142
[24,     1] loss: 1248.527
[25,     1] loss: 1245.918
[26,     1] loss: 1244.161
[27,     1] loss: 1248.366
[28,     1] loss: 1246.125
[29,     1] loss: 1244.824
[30,     1] loss: 1242.129
[31,     1] loss: 1243.312
[32,     1] loss: 1239.275
[33,     1] loss: 1235.892
[34,     1] loss: 1216.240
[35,     1] loss: 1210.267
[36,     1] loss: 1200.981
[37,     1] loss: 1193.473
[38,     1] loss: 1133.104
[39,     1] loss: 1097.052
[40,     1] loss: 1119.142
[41,     1] loss: 1067.020
[42,     1] loss: 1061.574
[43,     1] loss: 1031.957
[44,     1] loss: 1035.976
[45,     1] loss: 1199.356
[46,     1] loss: 1035.168
[47,     1] loss: 1124.709
[48,     1] loss: 1030.804
[49,     1] loss: 1026.953
[50,     1] loss: 1068.459
[51,     1] loss: 1025.152
[52,     1] loss: 954.105
[53,     1] loss: 1009.456
[54,     1] loss: 972.175
[55,     1] loss: 974.936
[56,     1] loss: 948.531
[57,     1] loss: 919.058
[58,     1] loss: 972.038
[59,     1] loss: 894.889
[60,     1] loss: 907.924
[61,     1] loss: 912.427
[62,     1] loss: 893.584
[63,     1] loss: 907.334
[64,     1] loss: 911.910
[65,     1] loss: 833.976
[66,     1] loss: 851.790
[67,     1] loss: 842.501
[68,     1] loss: 840.937
[69,     1] loss: 871.393
[70,     1] loss: 850.435
[71,     1] loss: 805.374
[72,     1] loss: 770.839
[73,     1] loss: 775.526
[74,     1] loss: 757.166
[75,     1] loss: 773.118
[76,     1] loss: 1093.829
[77,     1] loss: 1607.350
[78,     1] loss: 820.788
[79,     1] loss: 1035.989
[80,     1] loss: 1074.593
[81,     1] loss: 988.481
[82,     1] loss: 1017.533
[83,     1] loss: 1045.400
[84,     1] loss: 1030.592
[85,     1] loss: 986.991
[86,     1] loss: 947.642
[87,     1] loss: 965.734
[88,     1] loss: 914.679
[89,     1] loss: 927.187
[90,     1] loss: 908.527
[91,     1] loss: 954.474
[92,     1] loss: 888.754
[93,     1] loss: 886.129
[94,     1] loss: 824.235
[95,     1] loss: 864.919
[96,     1] loss: 837.160
[97,     1] loss: 886.383
[98,     1] loss: 802.038
[99,     1] loss: 819.136
[100,     1] loss: 794.233
[101,     1] loss: 779.047
[102,     1] loss: 901.259
[103,     1] loss: 960.102
[104,     1] loss: 759.586
[105,     1] loss: 872.457
[106,     1] loss: 815.747
[107,     1] loss: 849.112
[108,     1] loss: 744.420
[109,     1] loss: 786.978
[110,     1] loss: 817.907
[111,     1] loss: 699.274
[112,     1] loss: 725.570
[113,     1] loss: 681.528
[114,     1] loss: 682.717
Early stopping applied (best metric=0.3094543218612671)
Finished Training
Total time taken: 19.105192184448242
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.907
[2,     1] loss: 1245.231
[3,     1] loss: 1245.913
[4,     1] loss: 1244.839
[5,     1] loss: 1244.781
[6,     1] loss: 1240.620
[7,     1] loss: 1242.949
[8,     1] loss: 1245.082
[9,     1] loss: 1242.910
[10,     1] loss: 1231.711
[11,     1] loss: 1218.868
[12,     1] loss: 1206.556
[13,     1] loss: 1179.984
[14,     1] loss: 1142.816
[15,     1] loss: 1113.315
[16,     1] loss: 1107.470
[17,     1] loss: 1083.848
[18,     1] loss: 1074.449
[19,     1] loss: 1044.985
[20,     1] loss: 1021.390
[21,     1] loss: 1000.792
[22,     1] loss: 1010.937
[23,     1] loss: 1030.781
[24,     1] loss: 1051.103
[25,     1] loss: 1003.453
[26,     1] loss: 957.373
[27,     1] loss: 952.356
[28,     1] loss: 950.245
[29,     1] loss: 921.624
[30,     1] loss: 901.483
[31,     1] loss: 925.547
[32,     1] loss: 898.298
[33,     1] loss: 876.495
[34,     1] loss: 912.250
[35,     1] loss: 897.707
[36,     1] loss: 893.762
[37,     1] loss: 965.683
[38,     1] loss: 831.718
[39,     1] loss: 915.635
[40,     1] loss: 887.326
[41,     1] loss: 812.550
[42,     1] loss: 797.619
[43,     1] loss: 832.380
[44,     1] loss: 787.325
[45,     1] loss: 784.945
[46,     1] loss: 783.980
[47,     1] loss: 1391.311
[48,     1] loss: 828.776
[49,     1] loss: 964.191
[50,     1] loss: 902.140
[51,     1] loss: 991.631
[52,     1] loss: 990.639
[53,     1] loss: 860.289
[54,     1] loss: 867.800
[55,     1] loss: 895.035
[56,     1] loss: 835.325
[57,     1] loss: 816.574
[58,     1] loss: 808.107
[59,     1] loss: 759.611
[60,     1] loss: 860.468
[61,     1] loss: 779.276
[62,     1] loss: 739.459
[63,     1] loss: 756.485
[64,     1] loss: 709.516
[65,     1] loss: 744.314
[66,     1] loss: 670.582
[67,     1] loss: 733.950
[68,     1] loss: 765.137
[69,     1] loss: 626.135
[70,     1] loss: 827.703
[71,     1] loss: 780.808
[72,     1] loss: 661.346
[73,     1] loss: 732.549
[74,     1] loss: 658.603
[75,     1] loss: 731.778
[76,     1] loss: 657.893
[77,     1] loss: 648.019
[78,     1] loss: 606.265
[79,     1] loss: 710.963
[80,     1] loss: 615.069
[81,     1] loss: 677.203
[82,     1] loss: 567.389
[83,     1] loss: 541.206
[84,     1] loss: 538.548
[85,     1] loss: 508.695
[86,     1] loss: 496.268
[87,     1] loss: 1062.602
[88,     1] loss: 2653.145
[89,     1] loss: 1904.538
[90,     1] loss: 1176.701
[91,     1] loss: 1187.958
[92,     1] loss: 1237.327
[93,     1] loss: 1241.854
[94,     1] loss: 1237.424
Early stopping applied (best metric=0.3180240988731384)
Finished Training
Total time taken: 14.363708972930908
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.580
[2,     1] loss: 1249.052
[3,     1] loss: 1252.220
[4,     1] loss: 1244.999
[5,     1] loss: 1246.202
[6,     1] loss: 1246.585
[7,     1] loss: 1242.505
[8,     1] loss: 1242.796
[9,     1] loss: 1244.843
[10,     1] loss: 1242.256
[11,     1] loss: 1239.826
[12,     1] loss: 1237.355
[13,     1] loss: 1231.036
[14,     1] loss: 1223.382
[15,     1] loss: 1200.020
[16,     1] loss: 1180.512
[17,     1] loss: 1136.322
[18,     1] loss: 1111.036
[19,     1] loss: 1076.781
[20,     1] loss: 1016.636
[21,     1] loss: 1031.338
[22,     1] loss: 982.286
[23,     1] loss: 1058.086
[24,     1] loss: 1012.604
[25,     1] loss: 1018.205
[26,     1] loss: 1020.703
[27,     1] loss: 982.981
[28,     1] loss: 958.867
[29,     1] loss: 977.623
[30,     1] loss: 974.920
[31,     1] loss: 912.306
[32,     1] loss: 955.446
[33,     1] loss: 910.827
[34,     1] loss: 899.079
[35,     1] loss: 899.169
[36,     1] loss: 911.179
[37,     1] loss: 840.264
[38,     1] loss: 932.430
[39,     1] loss: 993.018
[40,     1] loss: 1018.793
[41,     1] loss: 850.178
[42,     1] loss: 896.984
[43,     1] loss: 871.571
[44,     1] loss: 880.381
[45,     1] loss: 869.278
[46,     1] loss: 873.054
[47,     1] loss: 782.064
[48,     1] loss: 823.107
[49,     1] loss: 805.417
[50,     1] loss: 806.488
[51,     1] loss: 755.201
[52,     1] loss: 770.797
[53,     1] loss: 1039.875
[54,     1] loss: 1290.811
[55,     1] loss: 894.797
[56,     1] loss: 899.930
[57,     1] loss: 1055.655
[58,     1] loss: 994.758
[59,     1] loss: 948.257
[60,     1] loss: 960.072
[61,     1] loss: 963.184
[62,     1] loss: 968.903
[63,     1] loss: 902.585
[64,     1] loss: 881.698
[65,     1] loss: 959.200
[66,     1] loss: 825.782
[67,     1] loss: 853.155
[68,     1] loss: 826.344
[69,     1] loss: 828.520
[70,     1] loss: 842.960
[71,     1] loss: 765.942
[72,     1] loss: 753.479
[73,     1] loss: 778.346
[74,     1] loss: 746.782
[75,     1] loss: 804.510
[76,     1] loss: 672.055
[77,     1] loss: 678.257
[78,     1] loss: 762.131
[79,     1] loss: 909.255
[80,     1] loss: 649.208
[81,     1] loss: 732.466
[82,     1] loss: 844.031
[83,     1] loss: 722.897
[84,     1] loss: 940.280
[85,     1] loss: 713.502
[86,     1] loss: 826.310
[87,     1] loss: 703.662
[88,     1] loss: 784.463
[89,     1] loss: 657.702
[90,     1] loss: 729.928
[91,     1] loss: 675.977
[92,     1] loss: 622.413
[93,     1] loss: 711.909
[94,     1] loss: 616.797
[95,     1] loss: 549.386
[96,     1] loss: 508.203
[97,     1] loss: 635.680
[98,     1] loss: 993.069
[99,     1] loss: 742.232
[100,     1] loss: 604.633
[101,     1] loss: 646.933
Early stopping applied (best metric=0.37896597385406494)
Finished Training
Total time taken: 13.583857297897339
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.753
[2,     1] loss: 1248.652
[3,     1] loss: 1250.620
[4,     1] loss: 1245.238
[5,     1] loss: 1253.334
[6,     1] loss: 1247.431
[7,     1] loss: 1245.293
[8,     1] loss: 1245.907
[9,     1] loss: 1246.301
[10,     1] loss: 1245.954
[11,     1] loss: 1247.057
[12,     1] loss: 1246.708
[13,     1] loss: 1244.888
[14,     1] loss: 1244.546
[15,     1] loss: 1245.499
[16,     1] loss: 1245.031
[17,     1] loss: 1243.834
[18,     1] loss: 1242.629
[19,     1] loss: 1240.819
[20,     1] loss: 1238.600
[21,     1] loss: 1237.919
[22,     1] loss: 1231.933
[23,     1] loss: 1224.972
[24,     1] loss: 1206.476
[25,     1] loss: 1195.967
[26,     1] loss: 1145.946
[27,     1] loss: 1117.071
[28,     1] loss: 1113.045
[29,     1] loss: 1054.529
[30,     1] loss: 1062.551
[31,     1] loss: 1050.184
[32,     1] loss: 1031.295
[33,     1] loss: 1016.868
[34,     1] loss: 1017.752
[35,     1] loss: 1030.723
[36,     1] loss: 988.371
[37,     1] loss: 955.560
[38,     1] loss: 997.450
[39,     1] loss: 972.263
[40,     1] loss: 961.211
[41,     1] loss: 951.833
[42,     1] loss: 959.782
[43,     1] loss: 902.223
[44,     1] loss: 949.643
[45,     1] loss: 871.531
[46,     1] loss: 914.587
[47,     1] loss: 928.767
[48,     1] loss: 910.533
[49,     1] loss: 925.256
[50,     1] loss: 866.570
[51,     1] loss: 829.150
[52,     1] loss: 894.296
[53,     1] loss: 842.086
[54,     1] loss: 822.699
[55,     1] loss: 988.908
[56,     1] loss: 1048.403
[57,     1] loss: 824.456
[58,     1] loss: 980.740
[59,     1] loss: 922.802
[60,     1] loss: 894.225
[61,     1] loss: 962.130
[62,     1] loss: 886.305
[63,     1] loss: 909.922
[64,     1] loss: 878.428
[65,     1] loss: 820.598
[66,     1] loss: 915.178
[67,     1] loss: 786.669
[68,     1] loss: 830.923
[69,     1] loss: 806.303
[70,     1] loss: 880.479
[71,     1] loss: 778.518
[72,     1] loss: 768.815
[73,     1] loss: 833.264
[74,     1] loss: 713.742
[75,     1] loss: 839.824
[76,     1] loss: 913.116
[77,     1] loss: 690.798
[78,     1] loss: 832.668
[79,     1] loss: 766.592
[80,     1] loss: 761.673
[81,     1] loss: 800.997
[82,     1] loss: 669.422
[83,     1] loss: 776.359
[84,     1] loss: 654.483
[85,     1] loss: 617.366
[86,     1] loss: 620.213
[87,     1] loss: 679.500
[88,     1] loss: 778.032
[89,     1] loss: 1008.987
[90,     1] loss: 1140.158
[91,     1] loss: 981.510
[92,     1] loss: 1013.535
[93,     1] loss: 966.692
[94,     1] loss: 984.524
[95,     1] loss: 877.001
[96,     1] loss: 898.168
[97,     1] loss: 955.695
[98,     1] loss: 837.823
[99,     1] loss: 892.661
[100,     1] loss: 791.327
[101,     1] loss: 800.207
[102,     1] loss: 776.111
[103,     1] loss: 715.850
[104,     1] loss: 726.999
[105,     1] loss: 739.383
[106,     1] loss: 652.821
[107,     1] loss: 635.666
[108,     1] loss: 800.026
[109,     1] loss: 629.170
[110,     1] loss: 599.395
[111,     1] loss: 733.008
[112,     1] loss: 540.928
[113,     1] loss: 605.315
[114,     1] loss: 956.520
[115,     1] loss: 589.854
[116,     1] loss: 1326.740
[117,     1] loss: 1109.303
[118,     1] loss: 1223.153
[119,     1] loss: 1107.130
[120,     1] loss: 1177.344
[121,     1] loss: 1194.041
[122,     1] loss: 1195.114
[123,     1] loss: 1216.059
[124,     1] loss: 1190.327
[125,     1] loss: 1183.134
[126,     1] loss: 1175.911
[127,     1] loss: 1167.500
[128,     1] loss: 1150.474
[129,     1] loss: 1166.125
[130,     1] loss: 1136.020
[131,     1] loss: 1119.207
[132,     1] loss: 1093.948
[133,     1] loss: 1049.160
[134,     1] loss: 1038.872
[135,     1] loss: 1013.251
[136,     1] loss: 1061.251
[137,     1] loss: 1011.377
[138,     1] loss: 997.003
[139,     1] loss: 941.951
[140,     1] loss: 921.323
[141,     1] loss: 921.601
[142,     1] loss: 1188.404
[143,     1] loss: 1275.857
[144,     1] loss: 1070.457
[145,     1] loss: 899.569
[146,     1] loss: 905.305
[147,     1] loss: 932.249
[148,     1] loss: 949.275
[149,     1] loss: 902.869
[150,     1] loss: 933.745
[151,     1] loss: 910.428
[152,     1] loss: 904.134
[153,     1] loss: 973.970
[154,     1] loss: 982.395
[155,     1] loss: 836.700
[156,     1] loss: 842.236
[157,     1] loss: 803.308
[158,     1] loss: 896.608
[159,     1] loss: 843.484
[160,     1] loss: 792.530
Early stopping applied (best metric=0.3201863169670105)
Finished Training
Total time taken: 23.321308851242065
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.825
[2,     1] loss: 1245.157
[3,     1] loss: 1247.285
[4,     1] loss: 1242.221
[5,     1] loss: 1239.833
[6,     1] loss: 1237.795
[7,     1] loss: 1229.365
[8,     1] loss: 1209.846
[9,     1] loss: 1171.463
[10,     1] loss: 1164.145
[11,     1] loss: 1110.163
[12,     1] loss: 1074.830
[13,     1] loss: 1045.759
[14,     1] loss: 1033.865
[15,     1] loss: 1037.136
[16,     1] loss: 999.678
[17,     1] loss: 991.280
[18,     1] loss: 999.517
[19,     1] loss: 1023.801
[20,     1] loss: 1055.430
[21,     1] loss: 984.425
[22,     1] loss: 957.524
[23,     1] loss: 969.741
[24,     1] loss: 971.746
[25,     1] loss: 974.465
[26,     1] loss: 963.241
[27,     1] loss: 942.492
[28,     1] loss: 950.667
[29,     1] loss: 899.220
[30,     1] loss: 984.493
[31,     1] loss: 924.722
[32,     1] loss: 960.319
[33,     1] loss: 918.308
[34,     1] loss: 910.852
[35,     1] loss: 892.788
[36,     1] loss: 863.540
[37,     1] loss: 847.698
[38,     1] loss: 857.690
[39,     1] loss: 840.964
[40,     1] loss: 915.316
[41,     1] loss: 968.393
[42,     1] loss: 854.126
[43,     1] loss: 908.939
[44,     1] loss: 892.470
[45,     1] loss: 825.833
[46,     1] loss: 819.796
[47,     1] loss: 784.123
[48,     1] loss: 786.859
[49,     1] loss: 744.543
[50,     1] loss: 916.480
[51,     1] loss: 953.819
[52,     1] loss: 757.558
[53,     1] loss: 789.468
[54,     1] loss: 811.735
[55,     1] loss: 740.853
[56,     1] loss: 777.813
[57,     1] loss: 748.168
[58,     1] loss: 677.287
[59,     1] loss: 666.246
[60,     1] loss: 657.086
[61,     1] loss: 630.895
[62,     1] loss: 875.092
[63,     1] loss: 2234.759
[64,     1] loss: 807.137
[65,     1] loss: 1303.183
[66,     1] loss: 1025.570
[67,     1] loss: 997.967
[68,     1] loss: 1064.274
[69,     1] loss: 1108.015
[70,     1] loss: 1114.730
[71,     1] loss: 1131.872
[72,     1] loss: 1118.351
[73,     1] loss: 1077.541
[74,     1] loss: 1062.646
[75,     1] loss: 1024.371
[76,     1] loss: 1027.031
[77,     1] loss: 958.037
[78,     1] loss: 977.061
[79,     1] loss: 969.499
[80,     1] loss: 954.506
[81,     1] loss: 949.049
[82,     1] loss: 903.438
[83,     1] loss: 865.821
[84,     1] loss: 802.076
[85,     1] loss: 857.039
[86,     1] loss: 859.409
[87,     1] loss: 798.980
[88,     1] loss: 783.240
[89,     1] loss: 778.703
[90,     1] loss: 822.144
[91,     1] loss: 930.994
[92,     1] loss: 830.454
[93,     1] loss: 726.429
[94,     1] loss: 815.179
[95,     1] loss: 735.397
[96,     1] loss: 730.798
[97,     1] loss: 712.551
[98,     1] loss: 645.348
[99,     1] loss: 809.253
[100,     1] loss: 1250.078
[101,     1] loss: 945.143
[102,     1] loss: 724.578
[103,     1] loss: 883.528
[104,     1] loss: 840.120
[105,     1] loss: 808.943
[106,     1] loss: 876.458
Early stopping applied (best metric=0.344293475151062)
Finished Training
Total time taken: 15.130266427993774
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1245.990
[2,     1] loss: 1254.049
[3,     1] loss: 1246.964
[4,     1] loss: 1245.166
[5,     1] loss: 1241.744
[6,     1] loss: 1232.272
[7,     1] loss: 1217.135
[8,     1] loss: 1192.244
[9,     1] loss: 1136.756
[10,     1] loss: 1074.758
[11,     1] loss: 1056.606
[12,     1] loss: 1049.526
[13,     1] loss: 1049.695
[14,     1] loss: 1020.428
[15,     1] loss: 1009.256
[16,     1] loss: 962.368
[17,     1] loss: 982.798
[18,     1] loss: 1003.580
[19,     1] loss: 1002.385
[20,     1] loss: 993.358
[21,     1] loss: 974.684
[22,     1] loss: 958.117
[23,     1] loss: 920.205
[24,     1] loss: 876.509
[25,     1] loss: 853.200
[26,     1] loss: 869.921
[27,     1] loss: 903.853
[28,     1] loss: 857.489
[29,     1] loss: 884.828
[30,     1] loss: 1002.525
[31,     1] loss: 1033.802
[32,     1] loss: 864.031
[33,     1] loss: 957.650
[34,     1] loss: 845.415
[35,     1] loss: 846.346
[36,     1] loss: 864.698
[37,     1] loss: 792.210
[38,     1] loss: 822.447
[39,     1] loss: 805.130
[40,     1] loss: 810.123
[41,     1] loss: 821.019
[42,     1] loss: 775.591
[43,     1] loss: 768.332
[44,     1] loss: 739.699
[45,     1] loss: 698.155
[46,     1] loss: 719.665
[47,     1] loss: 708.191
[48,     1] loss: 1026.284
[49,     1] loss: 930.351
[50,     1] loss: 725.910
[51,     1] loss: 890.006
[52,     1] loss: 790.748
[53,     1] loss: 778.583
[54,     1] loss: 840.289
[55,     1] loss: 763.392
[56,     1] loss: 699.702
[57,     1] loss: 770.648
[58,     1] loss: 632.737
[59,     1] loss: 745.356
[60,     1] loss: 716.521
[61,     1] loss: 684.030
[62,     1] loss: 796.303
[63,     1] loss: 607.179
[64,     1] loss: 753.112
[65,     1] loss: 652.072
[66,     1] loss: 765.578
[67,     1] loss: 641.352
[68,     1] loss: 667.675
[69,     1] loss: 600.960
[70,     1] loss: 578.509
[71,     1] loss: 569.511
[72,     1] loss: 592.961
[73,     1] loss: 510.797
[74,     1] loss: 602.588
[75,     1] loss: 922.732
[76,     1] loss: 981.666
[77,     1] loss: 781.852
[78,     1] loss: 945.363
[79,     1] loss: 817.872
[80,     1] loss: 790.739
[81,     1] loss: 887.126
[82,     1] loss: 744.750
[83,     1] loss: 836.319
[84,     1] loss: 796.759
[85,     1] loss: 686.957
[86,     1] loss: 744.187
[87,     1] loss: 646.033
[88,     1] loss: 714.290
[89,     1] loss: 635.555
[90,     1] loss: 662.198
[91,     1] loss: 551.787
Early stopping applied (best metric=0.3850700259208679)
Finished Training
Total time taken: 14.069223403930664
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.849
[2,     1] loss: 1252.488
[3,     1] loss: 1250.882
[4,     1] loss: 1248.173
[5,     1] loss: 1245.767
[6,     1] loss: 1245.474
[7,     1] loss: 1248.054
[8,     1] loss: 1243.939
[9,     1] loss: 1244.235
[10,     1] loss: 1246.178
[11,     1] loss: 1246.556
[12,     1] loss: 1245.685
[13,     1] loss: 1247.730
[14,     1] loss: 1244.592
[15,     1] loss: 1246.343
[16,     1] loss: 1245.546
[17,     1] loss: 1243.472
[18,     1] loss: 1243.720
[19,     1] loss: 1244.265
[20,     1] loss: 1244.653
[21,     1] loss: 1242.726
[22,     1] loss: 1239.638
[23,     1] loss: 1240.605
[24,     1] loss: 1240.850
[25,     1] loss: 1232.714
[26,     1] loss: 1235.428
[27,     1] loss: 1214.465
[28,     1] loss: 1201.380
[29,     1] loss: 1180.212
[30,     1] loss: 1155.551
[31,     1] loss: 1151.195
[32,     1] loss: 1102.216
[33,     1] loss: 1095.170
[34,     1] loss: 1028.415
[35,     1] loss: 1063.920
[36,     1] loss: 1073.534
[37,     1] loss: 1000.058
[38,     1] loss: 982.576
[39,     1] loss: 967.114
[40,     1] loss: 962.601
[41,     1] loss: 961.310
[42,     1] loss: 969.560
[43,     1] loss: 902.208
[44,     1] loss: 961.089
[45,     1] loss: 939.612
[46,     1] loss: 869.163
[47,     1] loss: 930.167
[48,     1] loss: 858.725
[49,     1] loss: 899.263
[50,     1] loss: 916.039
[51,     1] loss: 868.004
[52,     1] loss: 858.229
[53,     1] loss: 865.163
[54,     1] loss: 857.559
[55,     1] loss: 814.274
[56,     1] loss: 804.248
[57,     1] loss: 826.602
[58,     1] loss: 786.108
[59,     1] loss: 789.488
[60,     1] loss: 816.341
[61,     1] loss: 1103.278
[62,     1] loss: 1020.907
[63,     1] loss: 832.062
[64,     1] loss: 877.563
[65,     1] loss: 935.207
[66,     1] loss: 887.462
[67,     1] loss: 825.387
[68,     1] loss: 904.294
[69,     1] loss: 832.878
[70,     1] loss: 834.021
[71,     1] loss: 827.630
[72,     1] loss: 811.461
[73,     1] loss: 778.645
[74,     1] loss: 794.624
[75,     1] loss: 775.864
[76,     1] loss: 767.430
[77,     1] loss: 709.946
[78,     1] loss: 665.104
[79,     1] loss: 761.458
[80,     1] loss: 714.599
[81,     1] loss: 852.987
[82,     1] loss: 852.674
[83,     1] loss: 671.839
[84,     1] loss: 719.357
[85,     1] loss: 761.206
[86,     1] loss: 668.201
[87,     1] loss: 739.002
[88,     1] loss: 684.708
[89,     1] loss: 703.150
[90,     1] loss: 721.029
[91,     1] loss: 678.040
[92,     1] loss: 599.341
[93,     1] loss: 585.660
[94,     1] loss: 603.554
[95,     1] loss: 986.400
[96,     1] loss: 2028.136
[97,     1] loss: 877.016
[98,     1] loss: 973.260
[99,     1] loss: 1132.263
[100,     1] loss: 1076.570
[101,     1] loss: 1030.600
[102,     1] loss: 1046.755
[103,     1] loss: 1070.308
[104,     1] loss: 1031.846
[105,     1] loss: 1031.548
[106,     1] loss: 957.370
[107,     1] loss: 956.801
[108,     1] loss: 967.504
[109,     1] loss: 921.856
[110,     1] loss: 861.291
[111,     1] loss: 876.043
[112,     1] loss: 779.130
[113,     1] loss: 807.989
[114,     1] loss: 766.703
[115,     1] loss: 911.226
Early stopping applied (best metric=0.39264318346977234)
Finished Training
Total time taken: 19.359274864196777
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.616
[2,     1] loss: 1253.027
[3,     1] loss: 1247.465
[4,     1] loss: 1243.045
[5,     1] loss: 1248.894
[6,     1] loss: 1250.196
[7,     1] loss: 1243.994
[8,     1] loss: 1243.948
[9,     1] loss: 1242.145
[10,     1] loss: 1244.291
[11,     1] loss: 1244.068
[12,     1] loss: 1240.588
[13,     1] loss: 1235.248
[14,     1] loss: 1232.891
[15,     1] loss: 1225.732
[16,     1] loss: 1204.851
[17,     1] loss: 1177.286
[18,     1] loss: 1135.372
[19,     1] loss: 1102.741
[20,     1] loss: 1082.129
[21,     1] loss: 1083.740
[22,     1] loss: 1123.200
[23,     1] loss: 1010.154
[24,     1] loss: 1055.520
[25,     1] loss: 1053.304
[26,     1] loss: 1006.701
[27,     1] loss: 1039.243
[28,     1] loss: 976.000
[29,     1] loss: 1012.030
[30,     1] loss: 1020.778
[31,     1] loss: 965.424
[32,     1] loss: 995.998
[33,     1] loss: 935.113
[34,     1] loss: 897.513
[35,     1] loss: 1009.379
[36,     1] loss: 932.524
[37,     1] loss: 975.211
[38,     1] loss: 910.177
[39,     1] loss: 947.798
[40,     1] loss: 862.851
[41,     1] loss: 880.295
[42,     1] loss: 882.951
[43,     1] loss: 853.892
[44,     1] loss: 870.596
[45,     1] loss: 800.654
[46,     1] loss: 915.172
[47,     1] loss: 817.733
[48,     1] loss: 809.257
[49,     1] loss: 795.762
[50,     1] loss: 797.074
[51,     1] loss: 739.805
[52,     1] loss: 771.475
[53,     1] loss: 780.601
[54,     1] loss: 841.925
[55,     1] loss: 1232.848
[56,     1] loss: 778.185
[57,     1] loss: 917.010
[58,     1] loss: 817.573
[59,     1] loss: 915.376
[60,     1] loss: 917.513
[61,     1] loss: 856.234
[62,     1] loss: 878.880
[63,     1] loss: 861.481
[64,     1] loss: 771.301
[65,     1] loss: 834.948
[66,     1] loss: 773.712
[67,     1] loss: 876.656
[68,     1] loss: 719.110
[69,     1] loss: 845.219
[70,     1] loss: 717.859
[71,     1] loss: 860.953
[72,     1] loss: 704.924
[73,     1] loss: 764.451
[74,     1] loss: 735.166
[75,     1] loss: 823.375
[76,     1] loss: 660.051
[77,     1] loss: 869.138
[78,     1] loss: 650.544
[79,     1] loss: 691.756
[80,     1] loss: 771.121
[81,     1] loss: 659.443
[82,     1] loss: 866.702
[83,     1] loss: 700.518
[84,     1] loss: 790.896
[85,     1] loss: 728.680
[86,     1] loss: 743.068
[87,     1] loss: 626.524
[88,     1] loss: 794.809
[89,     1] loss: 624.432
[90,     1] loss: 671.221
[91,     1] loss: 762.844
[92,     1] loss: 554.001
[93,     1] loss: 699.308
[94,     1] loss: 787.899
[95,     1] loss: 548.720
[96,     1] loss: 656.051
[97,     1] loss: 726.699
Early stopping applied (best metric=0.38199836015701294)
Finished Training
Total time taken: 15.721196413040161
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.766
[2,     1] loss: 1245.536
[3,     1] loss: 1247.099
[4,     1] loss: 1243.673
[5,     1] loss: 1242.518
[6,     1] loss: 1243.536
[7,     1] loss: 1239.229
[8,     1] loss: 1235.424
[9,     1] loss: 1215.794
[10,     1] loss: 1209.846
[11,     1] loss: 1173.242
[12,     1] loss: 1139.662
[13,     1] loss: 1115.617
[14,     1] loss: 1094.233
[15,     1] loss: 1084.737
[16,     1] loss: 1105.105
[17,     1] loss: 1052.360
[18,     1] loss: 1044.436
[19,     1] loss: 998.540
[20,     1] loss: 1022.013
[21,     1] loss: 1006.456
[22,     1] loss: 1000.391
[23,     1] loss: 954.244
[24,     1] loss: 970.761
[25,     1] loss: 960.693
[26,     1] loss: 1038.134
[27,     1] loss: 941.533
[28,     1] loss: 922.683
[29,     1] loss: 919.375
[30,     1] loss: 939.819
[31,     1] loss: 926.888
[32,     1] loss: 915.774
[33,     1] loss: 903.054
[34,     1] loss: 931.561
[35,     1] loss: 916.460
[36,     1] loss: 954.263
[37,     1] loss: 979.974
[38,     1] loss: 867.544
[39,     1] loss: 932.031
[40,     1] loss: 900.565
[41,     1] loss: 957.138
[42,     1] loss: 854.016
[43,     1] loss: 932.189
[44,     1] loss: 910.414
[45,     1] loss: 833.739
[46,     1] loss: 865.404
[47,     1] loss: 871.435
[48,     1] loss: 924.224
[49,     1] loss: 823.203
[50,     1] loss: 938.267
[51,     1] loss: 807.927
[52,     1] loss: 866.699
[53,     1] loss: 800.303
[54,     1] loss: 801.166
[55,     1] loss: 752.456
[56,     1] loss: 818.625
[57,     1] loss: 750.907
[58,     1] loss: 734.140
[59,     1] loss: 805.572
[60,     1] loss: 1012.793
[61,     1] loss: 1804.256
[62,     1] loss: 879.238
[63,     1] loss: 994.764
[64,     1] loss: 1060.009
[65,     1] loss: 1084.795
[66,     1] loss: 1051.935
[67,     1] loss: 1059.206
[68,     1] loss: 1044.543
[69,     1] loss: 1046.135
[70,     1] loss: 989.389
[71,     1] loss: 985.165
[72,     1] loss: 992.270
[73,     1] loss: 948.312
[74,     1] loss: 948.339
[75,     1] loss: 956.824
[76,     1] loss: 920.542
[77,     1] loss: 887.848
[78,     1] loss: 881.179
[79,     1] loss: 866.283
[80,     1] loss: 871.167
[81,     1] loss: 868.953
[82,     1] loss: 841.120
[83,     1] loss: 824.139
[84,     1] loss: 809.229
[85,     1] loss: 822.939
[86,     1] loss: 902.099
[87,     1] loss: 1211.528
[88,     1] loss: 965.223
[89,     1] loss: 951.264
[90,     1] loss: 932.244
[91,     1] loss: 932.318
[92,     1] loss: 930.215
[93,     1] loss: 867.828
[94,     1] loss: 857.454
[95,     1] loss: 927.311
[96,     1] loss: 889.200
[97,     1] loss: 933.774
[98,     1] loss: 817.871
[99,     1] loss: 902.042
[100,     1] loss: 847.031
[101,     1] loss: 867.604
[102,     1] loss: 781.900
[103,     1] loss: 819.682
[104,     1] loss: 826.543
[105,     1] loss: 742.575
[106,     1] loss: 791.381
[107,     1] loss: 780.750
[108,     1] loss: 759.951
[109,     1] loss: 746.922
[110,     1] loss: 724.548
[111,     1] loss: 721.462
[112,     1] loss: 737.463
[113,     1] loss: 787.810
[114,     1] loss: 1035.268
[115,     1] loss: 1092.441
[116,     1] loss: 756.507
[117,     1] loss: 939.639
[118,     1] loss: 845.905
[119,     1] loss: 826.536
[120,     1] loss: 860.759
[121,     1] loss: 735.768
[122,     1] loss: 870.920
[123,     1] loss: 784.206
[124,     1] loss: 835.397
[125,     1] loss: 746.797
[126,     1] loss: 762.604
[127,     1] loss: 715.685
[128,     1] loss: 733.880
[129,     1] loss: 711.415
[130,     1] loss: 747.830
[131,     1] loss: 701.321
[132,     1] loss: 656.936
[133,     1] loss: 614.073
[134,     1] loss: 742.481
[135,     1] loss: 1205.095
[136,     1] loss: 665.057
[137,     1] loss: 1087.094
[138,     1] loss: 762.428
[139,     1] loss: 927.848
[140,     1] loss: 802.249
[141,     1] loss: 760.127
[142,     1] loss: 892.967
[143,     1] loss: 714.837
[144,     1] loss: 819.762
[145,     1] loss: 696.946
[146,     1] loss: 741.996
[147,     1] loss: 602.933
[148,     1] loss: 673.785
[149,     1] loss: 575.708
[150,     1] loss: 603.759
[151,     1] loss: 599.252
[152,     1] loss: 555.809
[153,     1] loss: 511.964
[154,     1] loss: 563.401
[155,     1] loss: 1145.969
[156,     1] loss: 1795.358
[157,     1] loss: 963.962
[158,     1] loss: 1089.086
[159,     1] loss: 1107.399
[160,     1] loss: 1153.276
[161,     1] loss: 1147.847
[162,     1] loss: 1120.502
[163,     1] loss: 1135.292
[164,     1] loss: 1151.872
[165,     1] loss: 1106.208
[166,     1] loss: 1096.813
[167,     1] loss: 1115.103
[168,     1] loss: 1091.714
[169,     1] loss: 1069.106
[170,     1] loss: 1051.042
[171,     1] loss: 1049.589
[172,     1] loss: 1026.558
[173,     1] loss: 1011.023
[174,     1] loss: 1007.759
[175,     1] loss: 1005.603
[176,     1] loss: 950.782
[177,     1] loss: 918.162
[178,     1] loss: 931.003
[179,     1] loss: 919.066
[180,     1] loss: 900.247
[181,     1] loss: 861.426
[182,     1] loss: 908.919
[183,     1] loss: 972.434
[184,     1] loss: 1029.636
[185,     1] loss: 911.545
[186,     1] loss: 963.094
[187,     1] loss: 890.521
[188,     1] loss: 875.779
[189,     1] loss: 839.152
[190,     1] loss: 898.525
[191,     1] loss: 854.604
[192,     1] loss: 815.994
[193,     1] loss: 818.836
[194,     1] loss: 770.225
[195,     1] loss: 827.927
[196,     1] loss: 722.321
[197,     1] loss: 836.285
[198,     1] loss: 769.620
Early stopping applied (best metric=0.2971045672893524)
Finished Training
Total time taken: 32.64393329620361
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.011
[2,     1] loss: 1244.289
[3,     1] loss: 1248.855
[4,     1] loss: 1239.859
[5,     1] loss: 1244.925
[6,     1] loss: 1240.385
[7,     1] loss: 1231.154
[8,     1] loss: 1216.330
[9,     1] loss: 1197.335
[10,     1] loss: 1167.407
[11,     1] loss: 1132.236
[12,     1] loss: 1115.655
[13,     1] loss: 1100.984
[14,     1] loss: 1074.816
[15,     1] loss: 1040.497
[16,     1] loss: 981.812
[17,     1] loss: 1027.685
[18,     1] loss: 1034.138
[19,     1] loss: 1000.923
[20,     1] loss: 1021.046
[21,     1] loss: 981.106
[22,     1] loss: 999.390
[23,     1] loss: 974.293
[24,     1] loss: 966.721
[25,     1] loss: 986.117
[26,     1] loss: 973.353
[27,     1] loss: 940.807
[28,     1] loss: 938.374
[29,     1] loss: 899.698
[30,     1] loss: 940.367
[31,     1] loss: 922.959
[32,     1] loss: 855.304
[33,     1] loss: 879.194
[34,     1] loss: 918.311
[35,     1] loss: 860.627
[36,     1] loss: 815.782
[37,     1] loss: 870.688
[38,     1] loss: 934.731
[39,     1] loss: 837.622
[40,     1] loss: 856.100
[41,     1] loss: 917.031
[42,     1] loss: 803.792
[43,     1] loss: 812.609
[44,     1] loss: 836.370
[45,     1] loss: 842.481
[46,     1] loss: 724.078
[47,     1] loss: 738.001
[48,     1] loss: 747.485
[49,     1] loss: 810.334
[50,     1] loss: 1114.998
[51,     1] loss: 1202.531
[52,     1] loss: 954.925
[53,     1] loss: 852.449
[54,     1] loss: 1022.430
[55,     1] loss: 980.314
[56,     1] loss: 920.477
[57,     1] loss: 925.757
[58,     1] loss: 956.096
[59,     1] loss: 976.857
[60,     1] loss: 880.615
[61,     1] loss: 835.774
[62,     1] loss: 888.279
[63,     1] loss: 775.880
[64,     1] loss: 792.215
[65,     1] loss: 803.630
[66,     1] loss: 794.421
[67,     1] loss: 805.408
[68,     1] loss: 721.590
[69,     1] loss: 790.187
[70,     1] loss: 697.076
[71,     1] loss: 768.061
[72,     1] loss: 690.374
[73,     1] loss: 714.547
[74,     1] loss: 799.179
[75,     1] loss: 735.711
[76,     1] loss: 624.568
[77,     1] loss: 759.827
[78,     1] loss: 765.495
[79,     1] loss: 621.843
[80,     1] loss: 653.393
[81,     1] loss: 679.862
[82,     1] loss: 582.144
[83,     1] loss: 588.127
[84,     1] loss: 607.087
[85,     1] loss: 646.037
[86,     1] loss: 1496.147
[87,     1] loss: 1725.012
[88,     1] loss: 1488.715
[89,     1] loss: 1106.978
[90,     1] loss: 1197.609
[91,     1] loss: 1189.665
[92,     1] loss: 1158.580
[93,     1] loss: 1167.295
[94,     1] loss: 1189.071
[95,     1] loss: 1171.484
Early stopping applied (best metric=0.3887152373790741)
Finished Training
Total time taken: 15.836660623550415
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1252.848
[2,     1] loss: 1255.445
[3,     1] loss: 1245.417
[4,     1] loss: 1248.335
[5,     1] loss: 1246.866
[6,     1] loss: 1246.159
[7,     1] loss: 1247.912
[8,     1] loss: 1246.887
[9,     1] loss: 1247.423
[10,     1] loss: 1246.226
[11,     1] loss: 1248.952
[12,     1] loss: 1248.653
[13,     1] loss: 1247.948
[14,     1] loss: 1248.242
[15,     1] loss: 1245.097
[16,     1] loss: 1247.907
[17,     1] loss: 1245.880
[18,     1] loss: 1243.436
[19,     1] loss: 1246.192
[20,     1] loss: 1246.927
[21,     1] loss: 1246.199
[22,     1] loss: 1243.510
[23,     1] loss: 1242.373
[24,     1] loss: 1241.524
[25,     1] loss: 1239.320
[26,     1] loss: 1229.214
[27,     1] loss: 1225.336
[28,     1] loss: 1201.723
[29,     1] loss: 1174.038
[30,     1] loss: 1153.622
[31,     1] loss: 1100.456
[32,     1] loss: 1071.099
[33,     1] loss: 1055.723
[34,     1] loss: 1029.658
[35,     1] loss: 1108.470
[36,     1] loss: 991.038
[37,     1] loss: 1011.536
[38,     1] loss: 975.609
[39,     1] loss: 989.809
[40,     1] loss: 978.278
[41,     1] loss: 973.104
[42,     1] loss: 978.308
[43,     1] loss: 927.176
[44,     1] loss: 948.536
[45,     1] loss: 890.597
[46,     1] loss: 943.264
[47,     1] loss: 874.726
[48,     1] loss: 892.232
[49,     1] loss: 921.674
[50,     1] loss: 866.374
[51,     1] loss: 892.132
[52,     1] loss: 827.049
[53,     1] loss: 863.218
[54,     1] loss: 844.616
[55,     1] loss: 778.007
[56,     1] loss: 874.567
[57,     1] loss: 965.399
[58,     1] loss: 769.573
[59,     1] loss: 826.911
[60,     1] loss: 868.041
[61,     1] loss: 795.617
[62,     1] loss: 808.100
[63,     1] loss: 714.189
[64,     1] loss: 749.015
[65,     1] loss: 812.384
[66,     1] loss: 779.968
[67,     1] loss: 730.110
[68,     1] loss: 679.479
[69,     1] loss: 751.085
[70,     1] loss: 852.423
[71,     1] loss: 966.723
[72,     1] loss: 720.730
[73,     1] loss: 798.826
[74,     1] loss: 710.899
[75,     1] loss: 799.238
[76,     1] loss: 674.207
[77,     1] loss: 705.776
[78,     1] loss: 693.003
[79,     1] loss: 712.263
[80,     1] loss: 765.412
[81,     1] loss: 686.296
[82,     1] loss: 636.731
[83,     1] loss: 839.091
[84,     1] loss: 988.743
[85,     1] loss: 757.934
[86,     1] loss: 911.165
[87,     1] loss: 776.780
[88,     1] loss: 758.972
[89,     1] loss: 798.660
[90,     1] loss: 641.742
[91,     1] loss: 870.071
[92,     1] loss: 688.442
[93,     1] loss: 864.117
[94,     1] loss: 646.337
[95,     1] loss: 802.969
[96,     1] loss: 681.592
[97,     1] loss: 813.562
[98,     1] loss: 600.371
[99,     1] loss: 788.353
[100,     1] loss: 616.398
[101,     1] loss: 666.055
[102,     1] loss: 545.121
[103,     1] loss: 659.680
[104,     1] loss: 656.676
[105,     1] loss: 559.103
[106,     1] loss: 692.114
[107,     1] loss: 728.350
[108,     1] loss: 525.090
[109,     1] loss: 808.316
[110,     1] loss: 893.897
[111,     1] loss: 601.231
[112,     1] loss: 772.704
Early stopping applied (best metric=0.37739336490631104)
Finished Training
Total time taken: 17.84825611114502
{'Hydroxylation-K Validation Accuracy': 0.7211879432624113, 'Hydroxylation-K Validation Sensitivity': 0.6155555555555555, 'Hydroxylation-K Validation Specificity': 0.7473684210526316, 'Hydroxylation-K Validation Precision': 0.39359540127341985, 'Hydroxylation-K AUC ROC': 0.7598245614035087, 'Hydroxylation-K AUC PR': 0.5004929832575588, 'Hydroxylation-K MCC': 0.3178622390209795, 'Hydroxylation-K F1': 0.47313179931238136, 'Validation Loss (Hydroxylation-K)': 0.5222622056802114, 'Hydroxylation-P Validation Accuracy': 0.8023685092127303, 'Hydroxylation-P Validation Sensitivity': 0.8048677248677248, 'Hydroxylation-P Validation Specificity': 0.8018504663574243, 'Hydroxylation-P Validation Precision': 0.4781530175123007, 'Hydroxylation-P AUC ROC': 0.8628558127824923, 'Hydroxylation-P AUC PR': 0.6285658542505912, 'Hydroxylation-P MCC': 0.5094640143051892, 'Hydroxylation-P F1': 0.595389982501113, 'Validation Loss (Hydroxylation-P)': 0.3459307571252187, 'Validation Loss (total)': 0.8681929628054301, 'TimeToTrain': 18.466416724522908}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': False,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004948855406258137,
 'learning_rate_Hydroxylation-K': 0.0012899699863845916,
 'learning_rate_Hydroxylation-P': 0.005001178808747827,
 'log_base': 2.9987863126777072,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2738634974,
 'sample_weights': [1.6014641703166086, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.967659352813328,
 'weight_decay_Hydroxylation-K': 1.3884101414855463,
 'weight_decay_Hydroxylation-P': 4.378285509462673}
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.834
[2,     1] loss: 1230.095
[3,     1] loss: 1232.584
[4,     1] loss: 1229.930
[5,     1] loss: 1227.694
[6,     1] loss: 1223.485
[7,     1] loss: 1224.285
[8,     1] loss: 1217.095
[9,     1] loss: 1209.960
[10,     1] loss: 1188.288
[11,     1] loss: 1171.758
[12,     1] loss: 1117.192
[13,     1] loss: 1084.143
[14,     1] loss: 1083.925
[15,     1] loss: 1079.959
[16,     1] loss: 1026.678
[17,     1] loss: 1004.671
[18,     1] loss: 1010.051
[19,     1] loss: 983.572
[20,     1] loss: 1012.283
[21,     1] loss: 957.387
[22,     1] loss: 979.726
[23,     1] loss: 964.106
[24,     1] loss: 943.971
[25,     1] loss: 936.720
[26,     1] loss: 899.859
[27,     1] loss: 867.751
[28,     1] loss: 933.362
[29,     1] loss: 852.630
[30,     1] loss: 868.385
[31,     1] loss: 908.461
[32,     1] loss: 837.195
[33,     1] loss: 891.243
[34,     1] loss: 939.174
[35,     1] loss: 901.856
[36,     1] loss: 842.291
[37,     1] loss: 865.345
[38,     1] loss: 809.896
[39,     1] loss: 821.035
[40,     1] loss: 820.552
[41,     1] loss: 791.471
[42,     1] loss: 771.120
[43,     1] loss: 749.438
[44,     1] loss: 739.368
[45,     1] loss: 894.904
[46,     1] loss: 923.158
[47,     1] loss: 734.826
[48,     1] loss: 839.197
[49,     1] loss: 754.928
[50,     1] loss: 856.868
[51,     1] loss: 780.630
[52,     1] loss: 821.499
[53,     1] loss: 712.939
[54,     1] loss: 809.258
[55,     1] loss: 678.865
[56,     1] loss: 739.795
[57,     1] loss: 690.589
[58,     1] loss: 651.103
[59,     1] loss: 695.863
[60,     1] loss: 686.096
[61,     1] loss: 628.828
[62,     1] loss: 748.369
[63,     1] loss: 789.736
[64,     1] loss: 615.112
[65,     1] loss: 769.751
[66,     1] loss: 711.703
[67,     1] loss: 622.918
[68,     1] loss: 622.203
[69,     1] loss: 564.732
[70,     1] loss: 616.895
[71,     1] loss: 793.815
[72,     1] loss: 886.397
[73,     1] loss: 684.721
[74,     1] loss: 744.313
[75,     1] loss: 647.171
[76,     1] loss: 727.831
[77,     1] loss: 721.128
[78,     1] loss: 643.234
[79,     1] loss: 612.278
[80,     1] loss: 619.966
[81,     1] loss: 580.318
[82,     1] loss: 525.478
[83,     1] loss: 532.294
[84,     1] loss: 633.596
[85,     1] loss: 524.402
[86,     1] loss: 474.380
[87,     1] loss: 616.474
[88,     1] loss: 604.630
[89,     1] loss: 500.011
[90,     1] loss: 481.202
[91,     1] loss: 580.347
[92,     1] loss: 537.617
[93,     1] loss: 445.323
[94,     1] loss: 481.759
[95,     1] loss: 945.366
[96,     1] loss: 1464.985
[97,     1] loss: 684.912
[98,     1] loss: 824.243
[99,     1] loss: 992.917
[100,     1] loss: 958.439
[101,     1] loss: 912.754
[102,     1] loss: 859.530
[103,     1] loss: 801.392
[104,     1] loss: 757.310
[105,     1] loss: 743.142
[106,     1] loss: 826.205
[107,     1] loss: 775.731
[108,     1] loss: 740.451
[109,     1] loss: 698.629
[110,     1] loss: 705.057
[111,     1] loss: 695.558
[112,     1] loss: 658.760
[113,     1] loss: 632.594
[114,     1] loss: 593.786
[115,     1] loss: 576.166
[116,     1] loss: 669.764
[117,     1] loss: 714.637
[118,     1] loss: 518.999
[119,     1] loss: 572.520
[120,     1] loss: 612.636
[121,     1] loss: 501.683
[122,     1] loss: 558.795
[123,     1] loss: 582.493
[124,     1] loss: 473.726
[125,     1] loss: 512.743
[126,     1] loss: 814.452
[127,     1] loss: 669.369
[128,     1] loss: 535.483
[129,     1] loss: 633.210
[130,     1] loss: 509.015
[131,     1] loss: 577.043
[132,     1] loss: 606.179
[133,     1] loss: 445.257
[134,     1] loss: 506.597
[135,     1] loss: 541.468
[136,     1] loss: 443.403
[137,     1] loss: 798.455
[138,     1] loss: 1131.760
[139,     1] loss: 731.488
[140,     1] loss: 720.415
[141,     1] loss: 1016.789
[142,     1] loss: 795.382
[143,     1] loss: 780.483
Early stopping applied (best metric=0.3211349844932556)
Finished Training
Total time taken: 21.532209157943726
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.188
[2,     1] loss: 1229.318
[3,     1] loss: 1228.282
[4,     1] loss: 1224.212
[5,     1] loss: 1220.015
[6,     1] loss: 1207.179
[7,     1] loss: 1176.331
[8,     1] loss: 1131.603
[9,     1] loss: 1148.084
[10,     1] loss: 1172.935
[11,     1] loss: 1103.764
[12,     1] loss: 1054.035
[13,     1] loss: 1089.932
[14,     1] loss: 1029.039
[15,     1] loss: 1059.849
[16,     1] loss: 1068.921
[17,     1] loss: 1027.513
[18,     1] loss: 1005.743
[19,     1] loss: 1030.627
[20,     1] loss: 1004.695
[21,     1] loss: 975.628
[22,     1] loss: 1006.142
[23,     1] loss: 990.883
[24,     1] loss: 978.962
[25,     1] loss: 1006.701
[26,     1] loss: 958.352
[27,     1] loss: 937.854
[28,     1] loss: 987.833
[29,     1] loss: 908.563
[30,     1] loss: 906.661
[31,     1] loss: 938.410
[32,     1] loss: 909.367
[33,     1] loss: 873.507
[34,     1] loss: 890.906
[35,     1] loss: 827.669
[36,     1] loss: 834.917
[37,     1] loss: 836.127
[38,     1] loss: 823.312
[39,     1] loss: 829.687
[40,     1] loss: 826.320
[41,     1] loss: 829.623
[42,     1] loss: 863.011
[43,     1] loss: 765.583
[44,     1] loss: 808.915
[45,     1] loss: 890.322
[46,     1] loss: 905.086
[47,     1] loss: 776.252
[48,     1] loss: 801.750
[49,     1] loss: 745.139
[50,     1] loss: 784.327
[51,     1] loss: 704.448
[52,     1] loss: 802.289
[53,     1] loss: 730.807
[54,     1] loss: 686.200
[55,     1] loss: 779.183
[56,     1] loss: 679.778
[57,     1] loss: 695.952
[58,     1] loss: 718.586
[59,     1] loss: 651.988
[60,     1] loss: 688.151
[61,     1] loss: 630.372
[62,     1] loss: 591.020
[63,     1] loss: 566.318
[64,     1] loss: 618.104
[65,     1] loss: 936.529
[66,     1] loss: 1242.527
[67,     1] loss: 704.270
[68,     1] loss: 949.610
[69,     1] loss: 958.750
[70,     1] loss: 974.011
[71,     1] loss: 938.676
[72,     1] loss: 918.277
[73,     1] loss: 877.349
[74,     1] loss: 803.896
[75,     1] loss: 780.671
[76,     1] loss: 857.759
[77,     1] loss: 787.280
[78,     1] loss: 810.747
[79,     1] loss: 776.707
[80,     1] loss: 800.021
[81,     1] loss: 781.480
[82,     1] loss: 766.554
[83,     1] loss: 757.720
[84,     1] loss: 726.609
[85,     1] loss: 744.711
[86,     1] loss: 710.752
[87,     1] loss: 694.159
[88,     1] loss: 631.375
[89,     1] loss: 645.872
[90,     1] loss: 725.717
[91,     1] loss: 791.935
[92,     1] loss: 738.116
[93,     1] loss: 646.401
[94,     1] loss: 662.562
[95,     1] loss: 613.400
[96,     1] loss: 648.329
[97,     1] loss: 621.974
[98,     1] loss: 504.552
[99,     1] loss: 651.422
[100,     1] loss: 759.362
[101,     1] loss: 563.565
[102,     1] loss: 610.788
[103,     1] loss: 662.397
[104,     1] loss: 504.585
[105,     1] loss: 1088.473
[106,     1] loss: 1267.931
[107,     1] loss: 927.513
[108,     1] loss: 932.610
[109,     1] loss: 1018.053
[110,     1] loss: 1041.720
[111,     1] loss: 1045.973
[112,     1] loss: 998.628
[113,     1] loss: 882.305
[114,     1] loss: 843.241
[115,     1] loss: 899.393
[116,     1] loss: 907.491
[117,     1] loss: 860.963
[118,     1] loss: 819.306
[119,     1] loss: 810.146
[120,     1] loss: 856.275
[121,     1] loss: 793.684
[122,     1] loss: 835.785
[123,     1] loss: 786.283
[124,     1] loss: 796.000
[125,     1] loss: 784.083
[126,     1] loss: 745.198
[127,     1] loss: 768.054
[128,     1] loss: 660.415
[129,     1] loss: 733.736
[130,     1] loss: 645.396
[131,     1] loss: 704.704
[132,     1] loss: 694.634
[133,     1] loss: 584.841
[134,     1] loss: 617.339
[135,     1] loss: 673.227
[136,     1] loss: 717.255
[137,     1] loss: 551.597
[138,     1] loss: 547.867
[139,     1] loss: 722.091
[140,     1] loss: 599.800
[141,     1] loss: 618.652
[142,     1] loss: 519.315
[143,     1] loss: 625.836
[144,     1] loss: 802.394
[145,     1] loss: 507.072
[146,     1] loss: 830.923
[147,     1] loss: 960.453
[148,     1] loss: 947.977
[149,     1] loss: 774.913
[150,     1] loss: 973.877
[151,     1] loss: 845.046
[152,     1] loss: 782.514
[153,     1] loss: 775.324
[154,     1] loss: 782.953
[155,     1] loss: 642.391
[156,     1] loss: 738.883
[157,     1] loss: 623.236
[158,     1] loss: 649.628
[159,     1] loss: 612.094
[160,     1] loss: 631.946
[161,     1] loss: 578.742
[162,     1] loss: 520.855
[163,     1] loss: 613.760
[164,     1] loss: 1069.562
[165,     1] loss: 729.603
[166,     1] loss: 738.846
[167,     1] loss: 767.532
[168,     1] loss: 786.786
[169,     1] loss: 702.950
[170,     1] loss: 648.057
[171,     1] loss: 734.372
[172,     1] loss: 757.514
[173,     1] loss: 528.572
[174,     1] loss: 908.396
[175,     1] loss: 847.598
[176,     1] loss: 920.322
[177,     1] loss: 666.587
[178,     1] loss: 824.525
[179,     1] loss: 775.173
[180,     1] loss: 663.627
[181,     1] loss: 781.919
[182,     1] loss: 625.166
[183,     1] loss: 671.939
[184,     1] loss: 609.349
[185,     1] loss: 540.527
[186,     1] loss: 572.215
[187,     1] loss: 530.363
[188,     1] loss: 538.459
Early stopping applied (best metric=0.3798361122608185)
Finished Training
Total time taken: 28.24092173576355
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.133
[2,     1] loss: 1226.256
[3,     1] loss: 1233.890
[4,     1] loss: 1229.313
[5,     1] loss: 1225.420
[6,     1] loss: 1214.031
[7,     1] loss: 1200.992
[8,     1] loss: 1167.615
[9,     1] loss: 1138.776
[10,     1] loss: 1114.657
[11,     1] loss: 1080.420
[12,     1] loss: 1009.895
[13,     1] loss: 1031.122
[14,     1] loss: 1043.268
[15,     1] loss: 983.899
[16,     1] loss: 979.157
[17,     1] loss: 955.019
[18,     1] loss: 958.200
[19,     1] loss: 1015.697
[20,     1] loss: 1008.319
[21,     1] loss: 951.990
[22,     1] loss: 948.691
[23,     1] loss: 956.932
[24,     1] loss: 906.476
[25,     1] loss: 966.570
[26,     1] loss: 881.918
[27,     1] loss: 890.963
[28,     1] loss: 865.181
[29,     1] loss: 903.248
[30,     1] loss: 875.826
[31,     1] loss: 843.800
[32,     1] loss: 812.710
[33,     1] loss: 808.703
[34,     1] loss: 786.923
[35,     1] loss: 818.415
[36,     1] loss: 779.129
[37,     1] loss: 773.237
[38,     1] loss: 856.206
[39,     1] loss: 880.891
[40,     1] loss: 774.750
[41,     1] loss: 906.018
[42,     1] loss: 800.452
[43,     1] loss: 865.917
[44,     1] loss: 711.214
[45,     1] loss: 841.330
[46,     1] loss: 720.622
[47,     1] loss: 782.666
[48,     1] loss: 735.669
[49,     1] loss: 722.500
[50,     1] loss: 690.089
[51,     1] loss: 628.435
[52,     1] loss: 714.394
[53,     1] loss: 809.439
[54,     1] loss: 753.435
[55,     1] loss: 638.278
[56,     1] loss: 758.682
[57,     1] loss: 667.369
[58,     1] loss: 746.731
[59,     1] loss: 761.841
[60,     1] loss: 612.240
[61,     1] loss: 754.418
[62,     1] loss: 600.537
[63,     1] loss: 618.950
[64,     1] loss: 598.702
[65,     1] loss: 597.978
[66,     1] loss: 662.276
[67,     1] loss: 578.213
[68,     1] loss: 558.564
[69,     1] loss: 638.494
[70,     1] loss: 661.567
[71,     1] loss: 680.345
[72,     1] loss: 521.206
[73,     1] loss: 617.215
[74,     1] loss: 633.917
[75,     1] loss: 476.159
[76,     1] loss: 537.523
[77,     1] loss: 686.450
[78,     1] loss: 488.035
[79,     1] loss: 474.084
[80,     1] loss: 725.801
[81,     1] loss: 683.615
[82,     1] loss: 497.184
[83,     1] loss: 632.465
[84,     1] loss: 509.218
[85,     1] loss: 558.008
[86,     1] loss: 464.701
[87,     1] loss: 566.876
[88,     1] loss: 508.064
[89,     1] loss: 383.447
[90,     1] loss: 452.707
[91,     1] loss: 509.388
[92,     1] loss: 531.912
[93,     1] loss: 457.873
[94,     1] loss: 420.955
[95,     1] loss: 400.195
[96,     1] loss: 404.311
[97,     1] loss: 391.351
[98,     1] loss: 373.610
[99,     1] loss: 499.634
[100,     1] loss: 1324.277
[101,     1] loss: 955.158
[102,     1] loss: 708.925
[103,     1] loss: 884.781
[104,     1] loss: 919.282
[105,     1] loss: 829.395
[106,     1] loss: 687.672
[107,     1] loss: 794.373
[108,     1] loss: 733.856
[109,     1] loss: 724.903
[110,     1] loss: 740.759
[111,     1] loss: 661.071
[112,     1] loss: 685.675
[113,     1] loss: 573.714
[114,     1] loss: 696.702
[115,     1] loss: 508.421
[116,     1] loss: 595.661
[117,     1] loss: 517.365
[118,     1] loss: 505.427
Early stopping applied (best metric=0.3726622462272644)
Finished Training
Total time taken: 17.682647228240967
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.169
[2,     1] loss: 1230.132
[3,     1] loss: 1227.461
[4,     1] loss: 1232.552
[5,     1] loss: 1230.973
[6,     1] loss: 1230.717
[7,     1] loss: 1223.160
[8,     1] loss: 1226.658
[9,     1] loss: 1227.619
[10,     1] loss: 1225.289
[11,     1] loss: 1220.294
[12,     1] loss: 1214.236
[13,     1] loss: 1202.314
[14,     1] loss: 1174.998
[15,     1] loss: 1146.962
[16,     1] loss: 1100.712
[17,     1] loss: 1078.695
[18,     1] loss: 1048.322
[19,     1] loss: 1033.278
[20,     1] loss: 1030.638
[21,     1] loss: 1048.882
[22,     1] loss: 986.080
[23,     1] loss: 980.953
[24,     1] loss: 996.163
[25,     1] loss: 985.357
[26,     1] loss: 979.919
[27,     1] loss: 948.959
[28,     1] loss: 981.372
[29,     1] loss: 958.025
[30,     1] loss: 891.189
[31,     1] loss: 917.903
[32,     1] loss: 955.453
[33,     1] loss: 908.915
[34,     1] loss: 921.273
[35,     1] loss: 943.234
[36,     1] loss: 857.946
[37,     1] loss: 900.626
[38,     1] loss: 881.845
[39,     1] loss: 920.582
[40,     1] loss: 869.734
[41,     1] loss: 838.150
[42,     1] loss: 816.062
[43,     1] loss: 817.772
[44,     1] loss: 839.387
[45,     1] loss: 782.253
[46,     1] loss: 822.203
[47,     1] loss: 751.547
[48,     1] loss: 788.057
[49,     1] loss: 769.637
[50,     1] loss: 791.867
[51,     1] loss: 802.085
[52,     1] loss: 698.412
[53,     1] loss: 775.581
[54,     1] loss: 727.932
[55,     1] loss: 689.185
[56,     1] loss: 668.887
[57,     1] loss: 688.484
[58,     1] loss: 810.403
[59,     1] loss: 765.138
[60,     1] loss: 663.622
[61,     1] loss: 723.023
[62,     1] loss: 674.153
[63,     1] loss: 637.421
[64,     1] loss: 613.341
[65,     1] loss: 611.845
[66,     1] loss: 853.456
[67,     1] loss: 1363.587
[68,     1] loss: 677.373
[69,     1] loss: 820.363
[70,     1] loss: 914.117
[71,     1] loss: 817.725
[72,     1] loss: 831.000
[73,     1] loss: 834.661
[74,     1] loss: 816.729
[75,     1] loss: 768.584
[76,     1] loss: 799.309
[77,     1] loss: 791.050
[78,     1] loss: 652.252
[79,     1] loss: 692.980
[80,     1] loss: 612.046
[81,     1] loss: 689.810
[82,     1] loss: 660.567
[83,     1] loss: 617.955
[84,     1] loss: 536.435
[85,     1] loss: 596.959
[86,     1] loss: 550.277
[87,     1] loss: 543.191
[88,     1] loss: 488.257
[89,     1] loss: 596.281
[90,     1] loss: 1023.011
[91,     1] loss: 1337.628
[92,     1] loss: 1121.070
[93,     1] loss: 996.359
[94,     1] loss: 1070.672
[95,     1] loss: 1076.273
[96,     1] loss: 1122.318
[97,     1] loss: 1133.968
[98,     1] loss: 1049.577
[99,     1] loss: 1022.354
[100,     1] loss: 1029.379
[101,     1] loss: 979.807
[102,     1] loss: 969.124
[103,     1] loss: 942.639
Early stopping applied (best metric=0.37094536423683167)
Finished Training
Total time taken: 15.631166696548462
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1231.173
[2,     1] loss: 1230.031
[3,     1] loss: 1229.689
[4,     1] loss: 1229.133
[5,     1] loss: 1224.853
[6,     1] loss: 1222.829
[7,     1] loss: 1210.203
[8,     1] loss: 1183.698
[9,     1] loss: 1146.222
[10,     1] loss: 1095.268
[11,     1] loss: 1086.791
[12,     1] loss: 1049.290
[13,     1] loss: 1054.352
[14,     1] loss: 981.260
[15,     1] loss: 1000.900
[16,     1] loss: 997.211
[17,     1] loss: 956.081
[18,     1] loss: 957.697
[19,     1] loss: 963.933
[20,     1] loss: 967.337
[21,     1] loss: 921.986
[22,     1] loss: 915.582
[23,     1] loss: 983.888
[24,     1] loss: 920.901
[25,     1] loss: 864.164
[26,     1] loss: 976.510
[27,     1] loss: 917.110
[28,     1] loss: 989.421
[29,     1] loss: 852.098
[30,     1] loss: 915.627
[31,     1] loss: 911.531
[32,     1] loss: 891.469
[33,     1] loss: 842.211
[34,     1] loss: 853.228
[35,     1] loss: 875.484
[36,     1] loss: 805.643
[37,     1] loss: 788.941
[38,     1] loss: 838.285
[39,     1] loss: 804.484
[40,     1] loss: 795.782
[41,     1] loss: 764.006
[42,     1] loss: 788.932
[43,     1] loss: 817.598
[44,     1] loss: 807.256
[45,     1] loss: 732.498
[46,     1] loss: 744.609
[47,     1] loss: 764.655
[48,     1] loss: 742.862
[49,     1] loss: 694.982
[50,     1] loss: 693.938
[51,     1] loss: 739.661
[52,     1] loss: 778.398
[53,     1] loss: 1455.625
[54,     1] loss: 873.165
[55,     1] loss: 938.089
[56,     1] loss: 899.706
[57,     1] loss: 920.306
[58,     1] loss: 936.340
[59,     1] loss: 929.238
[60,     1] loss: 914.596
[61,     1] loss: 844.931
[62,     1] loss: 838.777
[63,     1] loss: 862.041
[64,     1] loss: 834.239
[65,     1] loss: 815.659
[66,     1] loss: 830.774
[67,     1] loss: 801.237
[68,     1] loss: 773.161
[69,     1] loss: 816.613
[70,     1] loss: 774.730
[71,     1] loss: 781.774
[72,     1] loss: 755.487
[73,     1] loss: 787.054
[74,     1] loss: 735.031
[75,     1] loss: 743.097
[76,     1] loss: 664.244
[77,     1] loss: 710.882
[78,     1] loss: 666.044
[79,     1] loss: 675.716
[80,     1] loss: 630.599
[81,     1] loss: 665.069
[82,     1] loss: 621.834
[83,     1] loss: 697.221
[84,     1] loss: 979.746
[85,     1] loss: 701.467
[86,     1] loss: 727.267
[87,     1] loss: 683.168
[88,     1] loss: 784.066
[89,     1] loss: 603.520
[90,     1] loss: 740.378
[91,     1] loss: 577.668
[92,     1] loss: 632.655
[93,     1] loss: 528.982
[94,     1] loss: 606.118
[95,     1] loss: 685.616
[96,     1] loss: 562.383
[97,     1] loss: 653.164
Early stopping applied (best metric=0.32506442070007324)
Finished Training
Total time taken: 15.424147605895996
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.823
[2,     1] loss: 1223.894
[3,     1] loss: 1223.928
[4,     1] loss: 1220.257
[5,     1] loss: 1198.786
[6,     1] loss: 1183.059
[7,     1] loss: 1151.745
[8,     1] loss: 1091.156
[9,     1] loss: 1056.457
[10,     1] loss: 1084.705
[11,     1] loss: 1029.082
[12,     1] loss: 1022.412
[13,     1] loss: 1037.092
[14,     1] loss: 950.469
[15,     1] loss: 953.767
[16,     1] loss: 1005.241
[17,     1] loss: 930.771
[18,     1] loss: 964.306
[19,     1] loss: 996.470
[20,     1] loss: 922.798
[21,     1] loss: 941.444
[22,     1] loss: 912.048
[23,     1] loss: 911.367
[24,     1] loss: 926.192
[25,     1] loss: 911.952
[26,     1] loss: 820.661
[27,     1] loss: 907.533
[28,     1] loss: 916.360
[29,     1] loss: 821.429
[30,     1] loss: 901.227
[31,     1] loss: 829.901
[32,     1] loss: 900.789
[33,     1] loss: 813.836
[34,     1] loss: 863.577
[35,     1] loss: 803.505
[36,     1] loss: 827.306
[37,     1] loss: 816.633
[38,     1] loss: 729.579
[39,     1] loss: 714.531
[40,     1] loss: 738.340
[41,     1] loss: 741.225
[42,     1] loss: 757.383
[43,     1] loss: 768.342
[44,     1] loss: 695.090
[45,     1] loss: 676.883
[46,     1] loss: 773.145
[47,     1] loss: 756.662
[48,     1] loss: 645.360
[49,     1] loss: 626.661
[50,     1] loss: 594.072
[51,     1] loss: 630.271
[52,     1] loss: 696.870
[53,     1] loss: 1451.971
[54,     1] loss: 666.362
[55,     1] loss: 977.932
[56,     1] loss: 778.408
[57,     1] loss: 860.755
[58,     1] loss: 899.327
[59,     1] loss: 900.131
[60,     1] loss: 849.479
[61,     1] loss: 786.897
[62,     1] loss: 813.551
[63,     1] loss: 838.239
[64,     1] loss: 772.103
[65,     1] loss: 782.758
[66,     1] loss: 754.975
[67,     1] loss: 717.754
[68,     1] loss: 730.589
[69,     1] loss: 694.971
[70,     1] loss: 668.848
[71,     1] loss: 644.740
[72,     1] loss: 599.058
[73,     1] loss: 549.566
[74,     1] loss: 616.176
[75,     1] loss: 556.433
[76,     1] loss: 536.444
[77,     1] loss: 520.396
[78,     1] loss: 632.743
[79,     1] loss: 1069.693
[80,     1] loss: 608.388
[81,     1] loss: 976.646
[82,     1] loss: 704.926
[83,     1] loss: 849.922
[84,     1] loss: 825.316
[85,     1] loss: 710.100
[86,     1] loss: 827.887
[87,     1] loss: 648.323
[88,     1] loss: 612.216
[89,     1] loss: 740.887
[90,     1] loss: 561.653
[91,     1] loss: 724.004
[92,     1] loss: 583.473
[93,     1] loss: 637.853
[94,     1] loss: 567.873
[95,     1] loss: 549.414
[96,     1] loss: 693.427
[97,     1] loss: 485.146
[98,     1] loss: 776.355
[99,     1] loss: 558.281
[100,     1] loss: 673.871
[101,     1] loss: 527.520
[102,     1] loss: 699.564
[103,     1] loss: 541.057
[104,     1] loss: 567.897
[105,     1] loss: 449.791
[106,     1] loss: 633.272
[107,     1] loss: 795.016
[108,     1] loss: 433.980
[109,     1] loss: 590.352
[110,     1] loss: 510.981
[111,     1] loss: 522.202
[112,     1] loss: 432.080
[113,     1] loss: 536.880
[114,     1] loss: 446.647
[115,     1] loss: 416.666
[116,     1] loss: 537.554
[117,     1] loss: 779.223
[118,     1] loss: 422.923
[119,     1] loss: 758.023
[120,     1] loss: 563.775
[121,     1] loss: 692.732
[122,     1] loss: 504.089
[123,     1] loss: 695.109
[124,     1] loss: 450.097
[125,     1] loss: 601.790
[126,     1] loss: 779.060
[127,     1] loss: 476.530
[128,     1] loss: 655.024
[129,     1] loss: 488.277
[130,     1] loss: 719.881
[131,     1] loss: 493.206
[132,     1] loss: 607.111
[133,     1] loss: 456.247
[134,     1] loss: 505.770
[135,     1] loss: 383.726
[136,     1] loss: 453.709
[137,     1] loss: 441.026
[138,     1] loss: 364.034
[139,     1] loss: 409.205
[140,     1] loss: 389.076
[141,     1] loss: 337.544
[142,     1] loss: 376.087
[143,     1] loss: 443.446
[144,     1] loss: 1003.074
[145,     1] loss: 1012.379
[146,     1] loss: 649.462
[147,     1] loss: 873.656
[148,     1] loss: 722.414
[149,     1] loss: 581.196
[150,     1] loss: 766.151
[151,     1] loss: 669.434
[152,     1] loss: 611.826
[153,     1] loss: 636.513
[154,     1] loss: 621.037
[155,     1] loss: 638.281
[156,     1] loss: 535.439
[157,     1] loss: 580.824
[158,     1] loss: 462.129
[159,     1] loss: 560.422
[160,     1] loss: 444.351
Early stopping applied (best metric=0.3608151972293854)
Finished Training
Total time taken: 25.727388858795166
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.125
[2,     1] loss: 1230.759
[3,     1] loss: 1227.627
[4,     1] loss: 1225.495
[5,     1] loss: 1222.217
[6,     1] loss: 1219.853
[7,     1] loss: 1199.827
[8,     1] loss: 1170.320
[9,     1] loss: 1100.726
[10,     1] loss: 1069.847
[11,     1] loss: 1016.720
[12,     1] loss: 1028.713
[13,     1] loss: 1063.950
[14,     1] loss: 1027.176
[15,     1] loss: 931.646
[16,     1] loss: 932.006
[17,     1] loss: 948.219
[18,     1] loss: 934.559
[19,     1] loss: 981.834
[20,     1] loss: 937.791
[21,     1] loss: 927.427
[22,     1] loss: 920.124
[23,     1] loss: 916.349
[24,     1] loss: 833.659
[25,     1] loss: 897.844
[26,     1] loss: 877.991
[27,     1] loss: 834.437
[28,     1] loss: 804.814
[29,     1] loss: 791.578
[30,     1] loss: 913.549
[31,     1] loss: 902.095
[32,     1] loss: 780.609
[33,     1] loss: 860.491
[34,     1] loss: 764.226
[35,     1] loss: 808.367
[36,     1] loss: 718.047
[37,     1] loss: 823.789
[38,     1] loss: 766.958
[39,     1] loss: 768.400
[40,     1] loss: 784.781
[41,     1] loss: 753.903
[42,     1] loss: 780.218
[43,     1] loss: 693.518
[44,     1] loss: 669.280
[45,     1] loss: 730.828
[46,     1] loss: 712.081
[47,     1] loss: 675.825
[48,     1] loss: 622.646
[49,     1] loss: 764.557
[50,     1] loss: 1003.667
[51,     1] loss: 693.001
[52,     1] loss: 819.533
[53,     1] loss: 675.640
[54,     1] loss: 766.876
[55,     1] loss: 777.302
[56,     1] loss: 676.777
[57,     1] loss: 751.169
[58,     1] loss: 637.284
[59,     1] loss: 650.527
[60,     1] loss: 660.692
[61,     1] loss: 651.438
[62,     1] loss: 549.305
[63,     1] loss: 601.544
[64,     1] loss: 552.769
[65,     1] loss: 651.804
[66,     1] loss: 784.690
[67,     1] loss: 544.518
[68,     1] loss: 734.154
[69,     1] loss: 648.843
[70,     1] loss: 712.565
Early stopping applied (best metric=0.41804787516593933)
Finished Training
Total time taken: 11.901798963546753
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.724
[2,     1] loss: 1229.871
[3,     1] loss: 1226.853
[4,     1] loss: 1230.459
[5,     1] loss: 1227.479
[6,     1] loss: 1224.041
[7,     1] loss: 1222.160
[8,     1] loss: 1214.737
[9,     1] loss: 1197.131
[10,     1] loss: 1172.158
[11,     1] loss: 1132.704
[12,     1] loss: 1092.483
[13,     1] loss: 1055.610
[14,     1] loss: 1090.267
[15,     1] loss: 1055.346
[16,     1] loss: 1079.654
[17,     1] loss: 1014.109
[18,     1] loss: 1034.888
[19,     1] loss: 986.988
[20,     1] loss: 1008.198
[21,     1] loss: 988.646
[22,     1] loss: 987.029
[23,     1] loss: 1002.386
[24,     1] loss: 956.147
[25,     1] loss: 946.526
[26,     1] loss: 972.964
[27,     1] loss: 904.032
[28,     1] loss: 881.679
[29,     1] loss: 942.242
[30,     1] loss: 921.672
[31,     1] loss: 886.413
[32,     1] loss: 844.522
[33,     1] loss: 984.603
[34,     1] loss: 895.921
[35,     1] loss: 923.620
[36,     1] loss: 957.960
[37,     1] loss: 986.795
[38,     1] loss: 876.824
[39,     1] loss: 899.997
[40,     1] loss: 859.033
[41,     1] loss: 873.717
[42,     1] loss: 850.116
[43,     1] loss: 806.067
[44,     1] loss: 863.217
[45,     1] loss: 852.975
[46,     1] loss: 788.677
[47,     1] loss: 881.995
[48,     1] loss: 781.501
[49,     1] loss: 825.404
[50,     1] loss: 779.722
[51,     1] loss: 832.670
[52,     1] loss: 760.305
[53,     1] loss: 816.603
[54,     1] loss: 724.544
[55,     1] loss: 693.854
[56,     1] loss: 726.384
[57,     1] loss: 717.119
[58,     1] loss: 686.468
[59,     1] loss: 661.340
[60,     1] loss: 679.553
[61,     1] loss: 688.225
[62,     1] loss: 669.679
[63,     1] loss: 945.000
[64,     1] loss: 1323.573
[65,     1] loss: 801.155
[66,     1] loss: 1005.135
[67,     1] loss: 994.791
[68,     1] loss: 994.255
[69,     1] loss: 1004.300
[70,     1] loss: 988.447
[71,     1] loss: 949.115
[72,     1] loss: 898.686
[73,     1] loss: 877.380
[74,     1] loss: 911.002
[75,     1] loss: 856.442
[76,     1] loss: 865.413
[77,     1] loss: 802.383
[78,     1] loss: 839.978
[79,     1] loss: 794.265
[80,     1] loss: 806.051
[81,     1] loss: 800.001
[82,     1] loss: 729.234
[83,     1] loss: 762.492
[84,     1] loss: 693.001
[85,     1] loss: 700.405
[86,     1] loss: 689.565
[87,     1] loss: 676.473
[88,     1] loss: 617.537
[89,     1] loss: 774.472
[90,     1] loss: 834.659
[91,     1] loss: 628.997
[92,     1] loss: 722.729
[93,     1] loss: 598.477
[94,     1] loss: 763.987
[95,     1] loss: 687.335
[96,     1] loss: 567.645
[97,     1] loss: 647.235
[98,     1] loss: 579.291
[99,     1] loss: 543.311
[100,     1] loss: 481.727
[101,     1] loss: 515.568
[102,     1] loss: 586.502
[103,     1] loss: 964.004
[104,     1] loss: 1354.149
[105,     1] loss: 726.796
[106,     1] loss: 954.304
[107,     1] loss: 1001.031
[108,     1] loss: 966.016
[109,     1] loss: 985.899
[110,     1] loss: 964.562
[111,     1] loss: 914.349
[112,     1] loss: 862.339
[113,     1] loss: 891.276
[114,     1] loss: 850.044
[115,     1] loss: 863.998
[116,     1] loss: 816.764
[117,     1] loss: 822.209
[118,     1] loss: 806.599
[119,     1] loss: 801.868
[120,     1] loss: 782.561
[121,     1] loss: 750.037
[122,     1] loss: 788.408
[123,     1] loss: 753.974
[124,     1] loss: 709.717
[125,     1] loss: 736.972
Early stopping applied (best metric=0.301689475774765)
Finished Training
Total time taken: 21.890368461608887
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.092
[2,     1] loss: 1236.413
[3,     1] loss: 1229.862
[4,     1] loss: 1228.171
[5,     1] loss: 1232.604
[6,     1] loss: 1227.039
[7,     1] loss: 1226.150
[8,     1] loss: 1228.072
[9,     1] loss: 1221.672
[10,     1] loss: 1217.125
[11,     1] loss: 1210.662
[12,     1] loss: 1189.582
[13,     1] loss: 1175.701
[14,     1] loss: 1158.510
[15,     1] loss: 1104.391
[16,     1] loss: 1045.520
[17,     1] loss: 1032.707
[18,     1] loss: 1038.464
[19,     1] loss: 1002.443
[20,     1] loss: 960.100
[21,     1] loss: 1067.685
[22,     1] loss: 1018.557
[23,     1] loss: 968.592
[24,     1] loss: 956.596
[25,     1] loss: 995.566
[26,     1] loss: 933.174
[27,     1] loss: 941.461
[28,     1] loss: 976.561
[29,     1] loss: 862.960
[30,     1] loss: 894.938
[31,     1] loss: 873.100
[32,     1] loss: 907.577
[33,     1] loss: 873.758
[34,     1] loss: 856.886
[35,     1] loss: 837.970
[36,     1] loss: 910.176
[37,     1] loss: 990.822
[38,     1] loss: 830.049
[39,     1] loss: 854.209
[40,     1] loss: 797.808
[41,     1] loss: 870.810
[42,     1] loss: 764.081
[43,     1] loss: 831.814
[44,     1] loss: 781.971
[45,     1] loss: 782.934
[46,     1] loss: 760.327
[47,     1] loss: 722.260
[48,     1] loss: 752.652
[49,     1] loss: 704.752
[50,     1] loss: 741.153
[51,     1] loss: 735.376
[52,     1] loss: 682.497
[53,     1] loss: 669.561
[54,     1] loss: 694.974
[55,     1] loss: 824.413
[56,     1] loss: 929.081
[57,     1] loss: 687.669
[58,     1] loss: 809.033
[59,     1] loss: 762.206
[60,     1] loss: 730.994
[61,     1] loss: 639.173
[62,     1] loss: 712.631
[63,     1] loss: 701.421
[64,     1] loss: 656.675
[65,     1] loss: 643.517
[66,     1] loss: 586.189
[67,     1] loss: 584.108
[68,     1] loss: 583.157
[69,     1] loss: 574.927
[70,     1] loss: 618.526
[71,     1] loss: 596.685
[72,     1] loss: 560.994
[73,     1] loss: 519.241
[74,     1] loss: 503.253
[75,     1] loss: 509.544
[76,     1] loss: 449.545
[77,     1] loss: 618.966
[78,     1] loss: 1328.736
[79,     1] loss: 513.537
[80,     1] loss: 906.182
[81,     1] loss: 704.544
Early stopping applied (best metric=0.3864251375198364)
Finished Training
Total time taken: 13.862733840942383
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1230.906
[2,     1] loss: 1233.790
[3,     1] loss: 1233.264
[4,     1] loss: 1227.723
[5,     1] loss: 1229.092
[6,     1] loss: 1227.137
[7,     1] loss: 1223.795
[8,     1] loss: 1226.823
[9,     1] loss: 1226.264
[10,     1] loss: 1216.302
[11,     1] loss: 1207.106
[12,     1] loss: 1192.146
[13,     1] loss: 1172.603
[14,     1] loss: 1127.121
[15,     1] loss: 1095.138
[16,     1] loss: 1069.785
[17,     1] loss: 1111.090
[18,     1] loss: 1073.742
[19,     1] loss: 1047.775
[20,     1] loss: 1047.068
[21,     1] loss: 1057.126
[22,     1] loss: 1009.929
[23,     1] loss: 1030.071
[24,     1] loss: 978.710
[25,     1] loss: 1023.434
[26,     1] loss: 986.417
[27,     1] loss: 956.601
[28,     1] loss: 930.013
[29,     1] loss: 955.421
[30,     1] loss: 919.782
[31,     1] loss: 934.417
[32,     1] loss: 899.333
[33,     1] loss: 865.644
[34,     1] loss: 905.368
[35,     1] loss: 886.629
[36,     1] loss: 886.567
[37,     1] loss: 820.658
[38,     1] loss: 814.571
[39,     1] loss: 854.088
[40,     1] loss: 833.808
[41,     1] loss: 914.848
[42,     1] loss: 946.998
[43,     1] loss: 899.120
[44,     1] loss: 913.538
[45,     1] loss: 869.628
[46,     1] loss: 901.818
[47,     1] loss: 814.595
[48,     1] loss: 878.328
[49,     1] loss: 790.839
[50,     1] loss: 753.474
[51,     1] loss: 836.334
[52,     1] loss: 756.970
[53,     1] loss: 736.450
[54,     1] loss: 723.333
[55,     1] loss: 737.313
[56,     1] loss: 743.565
[57,     1] loss: 776.431
[58,     1] loss: 736.782
[59,     1] loss: 670.715
[60,     1] loss: 683.602
[61,     1] loss: 702.692
[62,     1] loss: 723.490
[63,     1] loss: 936.543
[64,     1] loss: 913.491
[65,     1] loss: 746.446
[66,     1] loss: 718.864
[67,     1] loss: 778.930
[68,     1] loss: 704.051
[69,     1] loss: 759.880
[70,     1] loss: 755.877
[71,     1] loss: 640.088
[72,     1] loss: 660.682
[73,     1] loss: 671.370
[74,     1] loss: 610.639
[75,     1] loss: 617.340
[76,     1] loss: 514.615
[77,     1] loss: 621.171
[78,     1] loss: 601.836
[79,     1] loss: 528.221
[80,     1] loss: 564.887
[81,     1] loss: 671.172
[82,     1] loss: 806.063
[83,     1] loss: 558.106
[84,     1] loss: 700.421
[85,     1] loss: 589.693
[86,     1] loss: 597.081
[87,     1] loss: 613.803
[88,     1] loss: 589.871
[89,     1] loss: 534.633
[90,     1] loss: 632.362
[91,     1] loss: 495.959
[92,     1] loss: 492.723
[93,     1] loss: 515.295
[94,     1] loss: 440.155
[95,     1] loss: 493.233
[96,     1] loss: 479.246
[97,     1] loss: 612.664
[98,     1] loss: 827.121
[99,     1] loss: 460.195
Early stopping applied (best metric=0.3327886462211609)
Finished Training
Total time taken: 17.07230234146118
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.140
[2,     1] loss: 1239.507
[3,     1] loss: 1231.220
[4,     1] loss: 1229.027
[5,     1] loss: 1226.289
[6,     1] loss: 1228.361
[7,     1] loss: 1230.605
[8,     1] loss: 1227.775
[9,     1] loss: 1228.170
[10,     1] loss: 1229.539
[11,     1] loss: 1228.494
[12,     1] loss: 1225.666
[13,     1] loss: 1223.369
[14,     1] loss: 1221.219
[15,     1] loss: 1218.229
[16,     1] loss: 1210.937
[17,     1] loss: 1190.303
[18,     1] loss: 1171.828
[19,     1] loss: 1144.040
[20,     1] loss: 1120.464
[21,     1] loss: 1087.898
[22,     1] loss: 1083.225
[23,     1] loss: 1065.099
[24,     1] loss: 1036.892
[25,     1] loss: 1003.468
[26,     1] loss: 1048.540
[27,     1] loss: 1004.501
[28,     1] loss: 1053.385
[29,     1] loss: 981.232
[30,     1] loss: 999.612
[31,     1] loss: 1010.415
[32,     1] loss: 947.461
[33,     1] loss: 950.177
[34,     1] loss: 952.930
[35,     1] loss: 963.083
[36,     1] loss: 898.366
[37,     1] loss: 891.736
[38,     1] loss: 918.793
[39,     1] loss: 859.481
[40,     1] loss: 851.935
[41,     1] loss: 872.842
[42,     1] loss: 917.778
[43,     1] loss: 885.296
[44,     1] loss: 879.758
[45,     1] loss: 816.941
[46,     1] loss: 847.335
[47,     1] loss: 832.245
[48,     1] loss: 966.822
[49,     1] loss: 945.588
[50,     1] loss: 768.993
[51,     1] loss: 847.425
[52,     1] loss: 802.227
[53,     1] loss: 807.348
[54,     1] loss: 805.099
[55,     1] loss: 799.635
[56,     1] loss: 825.780
[57,     1] loss: 798.294
[58,     1] loss: 713.756
[59,     1] loss: 746.315
[60,     1] loss: 756.849
[61,     1] loss: 691.419
[62,     1] loss: 744.339
[63,     1] loss: 749.053
[64,     1] loss: 658.278
[65,     1] loss: 677.411
[66,     1] loss: 656.792
[67,     1] loss: 656.339
[68,     1] loss: 660.515
[69,     1] loss: 831.273
[70,     1] loss: 1012.030
[71,     1] loss: 872.028
[72,     1] loss: 908.381
[73,     1] loss: 828.152
[74,     1] loss: 810.093
[75,     1] loss: 893.285
[76,     1] loss: 804.508
[77,     1] loss: 782.513
[78,     1] loss: 785.660
[79,     1] loss: 759.191
[80,     1] loss: 685.130
[81,     1] loss: 706.031
[82,     1] loss: 658.580
[83,     1] loss: 707.522
[84,     1] loss: 622.502
[85,     1] loss: 668.603
[86,     1] loss: 660.990
[87,     1] loss: 628.208
[88,     1] loss: 547.107
[89,     1] loss: 570.319
[90,     1] loss: 594.929
[91,     1] loss: 603.831
[92,     1] loss: 522.617
[93,     1] loss: 485.118
[94,     1] loss: 529.169
[95,     1] loss: 598.628
[96,     1] loss: 753.955
[97,     1] loss: 1083.673
[98,     1] loss: 729.072
[99,     1] loss: 757.851
[100,     1] loss: 941.444
[101,     1] loss: 740.658
[102,     1] loss: 734.866
[103,     1] loss: 744.055
[104,     1] loss: 676.479
[105,     1] loss: 724.799
[106,     1] loss: 593.968
[107,     1] loss: 636.788
[108,     1] loss: 601.122
[109,     1] loss: 589.079
[110,     1] loss: 578.870
[111,     1] loss: 533.754
[112,     1] loss: 486.865
[113,     1] loss: 461.437
[114,     1] loss: 517.184
[115,     1] loss: 661.963
[116,     1] loss: 550.547
[117,     1] loss: 575.630
[118,     1] loss: 482.792
[119,     1] loss: 629.501
[120,     1] loss: 742.727
[121,     1] loss: 441.367
[122,     1] loss: 786.675
[123,     1] loss: 1057.993
[124,     1] loss: 718.556
[125,     1] loss: 667.156
[126,     1] loss: 991.076
[127,     1] loss: 656.103
[128,     1] loss: 720.757
[129,     1] loss: 759.428
[130,     1] loss: 573.063
[131,     1] loss: 826.230
Early stopping applied (best metric=0.37055468559265137)
Finished Training
Total time taken: 22.1346914768219
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.985
[2,     1] loss: 1228.974
[3,     1] loss: 1232.224
[4,     1] loss: 1229.341
[5,     1] loss: 1231.297
[6,     1] loss: 1234.225
[7,     1] loss: 1227.195
[8,     1] loss: 1228.568
[9,     1] loss: 1228.540
[10,     1] loss: 1225.743
[11,     1] loss: 1223.415
[12,     1] loss: 1218.038
[13,     1] loss: 1210.474
[14,     1] loss: 1201.413
[15,     1] loss: 1168.447
[16,     1] loss: 1137.978
[17,     1] loss: 1103.510
[18,     1] loss: 1059.713
[19,     1] loss: 1041.118
[20,     1] loss: 1046.349
[21,     1] loss: 1080.388
[22,     1] loss: 1006.380
[23,     1] loss: 1074.189
[24,     1] loss: 1026.891
[25,     1] loss: 1034.793
[26,     1] loss: 1013.142
[27,     1] loss: 971.467
[28,     1] loss: 957.173
[29,     1] loss: 993.763
[30,     1] loss: 967.841
[31,     1] loss: 983.003
[32,     1] loss: 930.564
[33,     1] loss: 955.347
[34,     1] loss: 992.112
[35,     1] loss: 911.277
[36,     1] loss: 1030.152
[37,     1] loss: 943.025
[38,     1] loss: 923.131
[39,     1] loss: 891.670
[40,     1] loss: 926.028
[41,     1] loss: 863.332
[42,     1] loss: 879.395
[43,     1] loss: 867.536
[44,     1] loss: 858.242
[45,     1] loss: 842.157
[46,     1] loss: 765.436
[47,     1] loss: 779.519
[48,     1] loss: 799.147
[49,     1] loss: 1089.302
[50,     1] loss: 1172.754
[51,     1] loss: 814.399
[52,     1] loss: 949.457
[53,     1] loss: 980.684
[54,     1] loss: 907.617
[55,     1] loss: 883.632
[56,     1] loss: 925.857
[57,     1] loss: 894.154
[58,     1] loss: 826.603
[59,     1] loss: 898.523
[60,     1] loss: 880.758
[61,     1] loss: 821.609
[62,     1] loss: 859.669
[63,     1] loss: 817.102
[64,     1] loss: 839.984
[65,     1] loss: 793.086
[66,     1] loss: 768.199
[67,     1] loss: 756.224
[68,     1] loss: 719.057
[69,     1] loss: 712.287
[70,     1] loss: 684.135
[71,     1] loss: 698.955
[72,     1] loss: 726.124
[73,     1] loss: 984.373
[74,     1] loss: 667.832
[75,     1] loss: 669.920
[76,     1] loss: 785.929
[77,     1] loss: 704.733
[78,     1] loss: 843.630
[79,     1] loss: 647.451
[80,     1] loss: 765.018
[81,     1] loss: 630.981
[82,     1] loss: 744.528
[83,     1] loss: 688.696
[84,     1] loss: 657.463
[85,     1] loss: 742.822
[86,     1] loss: 594.372
[87,     1] loss: 667.949
[88,     1] loss: 614.572
[89,     1] loss: 640.669
[90,     1] loss: 608.829
[91,     1] loss: 506.151
[92,     1] loss: 621.169
[93,     1] loss: 722.377
[94,     1] loss: 542.050
[95,     1] loss: 545.796
[96,     1] loss: 591.804
[97,     1] loss: 494.375
[98,     1] loss: 528.855
[99,     1] loss: 788.427
[100,     1] loss: 622.585
[101,     1] loss: 568.901
[102,     1] loss: 480.033
[103,     1] loss: 541.319
[104,     1] loss: 528.874
[105,     1] loss: 464.695
[106,     1] loss: 517.637
[107,     1] loss: 1308.435
[108,     1] loss: 534.491
[109,     1] loss: 676.412
[110,     1] loss: 713.097
[111,     1] loss: 727.474
[112,     1] loss: 589.615
[113,     1] loss: 716.305
[114,     1] loss: 674.567
[115,     1] loss: 664.434
[116,     1] loss: 555.786
[117,     1] loss: 583.960
[118,     1] loss: 615.553
[119,     1] loss: 555.852
[120,     1] loss: 611.525
[121,     1] loss: 445.268
[122,     1] loss: 546.426
[123,     1] loss: 515.969
[124,     1] loss: 491.841
[125,     1] loss: 620.769
[126,     1] loss: 447.436
[127,     1] loss: 640.852
[128,     1] loss: 975.319
[129,     1] loss: 504.247
[130,     1] loss: 925.420
[131,     1] loss: 566.133
[132,     1] loss: 727.920
[133,     1] loss: 647.400
[134,     1] loss: 586.674
[135,     1] loss: 663.889
[136,     1] loss: 695.339
[137,     1] loss: 602.283
[138,     1] loss: 593.564
[139,     1] loss: 623.955
[140,     1] loss: 565.448
[141,     1] loss: 559.143
[142,     1] loss: 533.097
[143,     1] loss: 443.660
[144,     1] loss: 552.839
[145,     1] loss: 455.478
[146,     1] loss: 493.322
[147,     1] loss: 438.900
[148,     1] loss: 428.008
[149,     1] loss: 482.122
[150,     1] loss: 657.710
[151,     1] loss: 496.716
[152,     1] loss: 490.982
[153,     1] loss: 520.621
Early stopping applied (best metric=0.3008531928062439)
Finished Training
Total time taken: 25.616782665252686
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1226.691
[2,     1] loss: 1244.933
[3,     1] loss: 1227.850
[4,     1] loss: 1224.950
[5,     1] loss: 1225.022
[6,     1] loss: 1220.704
[7,     1] loss: 1216.584
[8,     1] loss: 1206.498
[9,     1] loss: 1169.714
[10,     1] loss: 1135.106
[11,     1] loss: 1106.235
[12,     1] loss: 1063.780
[13,     1] loss: 1082.318
[14,     1] loss: 1030.277
[15,     1] loss: 1057.301
[16,     1] loss: 998.869
[17,     1] loss: 1050.390
[18,     1] loss: 989.292
[19,     1] loss: 1006.911
[20,     1] loss: 960.829
[21,     1] loss: 939.716
[22,     1] loss: 959.469
[23,     1] loss: 958.486
[24,     1] loss: 916.286
[25,     1] loss: 942.798
[26,     1] loss: 916.625
[27,     1] loss: 935.318
[28,     1] loss: 901.896
[29,     1] loss: 857.721
[30,     1] loss: 939.393
[31,     1] loss: 839.745
[32,     1] loss: 903.715
[33,     1] loss: 916.598
[34,     1] loss: 842.693
[35,     1] loss: 888.167
[36,     1] loss: 843.136
[37,     1] loss: 806.951
[38,     1] loss: 792.062
[39,     1] loss: 784.287
[40,     1] loss: 721.727
[41,     1] loss: 716.324
[42,     1] loss: 718.264
[43,     1] loss: 737.614
[44,     1] loss: 787.080
[45,     1] loss: 1623.336
[46,     1] loss: 807.654
[47,     1] loss: 1034.419
[48,     1] loss: 995.526
[49,     1] loss: 983.252
[50,     1] loss: 1003.729
[51,     1] loss: 994.381
[52,     1] loss: 967.544
[53,     1] loss: 958.884
[54,     1] loss: 912.040
[55,     1] loss: 882.049
[56,     1] loss: 916.656
[57,     1] loss: 899.458
[58,     1] loss: 896.062
[59,     1] loss: 820.148
[60,     1] loss: 829.520
[61,     1] loss: 857.395
[62,     1] loss: 871.847
[63,     1] loss: 807.350
[64,     1] loss: 823.174
[65,     1] loss: 784.573
[66,     1] loss: 775.784
[67,     1] loss: 818.010
[68,     1] loss: 774.076
[69,     1] loss: 764.130
[70,     1] loss: 739.437
[71,     1] loss: 771.715
[72,     1] loss: 760.934
[73,     1] loss: 703.980
[74,     1] loss: 733.324
[75,     1] loss: 663.941
[76,     1] loss: 691.999
[77,     1] loss: 693.731
[78,     1] loss: 638.336
[79,     1] loss: 682.853
[80,     1] loss: 828.781
[81,     1] loss: 884.055
[82,     1] loss: 635.187
[83,     1] loss: 773.846
[84,     1] loss: 653.930
[85,     1] loss: 706.032
[86,     1] loss: 643.692
[87,     1] loss: 677.023
[88,     1] loss: 626.621
[89,     1] loss: 604.400
[90,     1] loss: 567.947
[91,     1] loss: 586.779
Early stopping applied (best metric=0.3999653160572052)
Finished Training
Total time taken: 15.424526691436768
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.588
[2,     1] loss: 1232.082
[3,     1] loss: 1230.376
[4,     1] loss: 1228.337
[5,     1] loss: 1229.178
[6,     1] loss: 1227.770
[7,     1] loss: 1228.238
[8,     1] loss: 1226.811
[9,     1] loss: 1225.620
[10,     1] loss: 1224.357
[11,     1] loss: 1222.406
[12,     1] loss: 1215.481
[13,     1] loss: 1214.234
[14,     1] loss: 1198.369
[15,     1] loss: 1179.341
[16,     1] loss: 1131.502
[17,     1] loss: 1120.141
[18,     1] loss: 1097.808
[19,     1] loss: 1061.369
[20,     1] loss: 1049.794
[21,     1] loss: 1036.868
[22,     1] loss: 1036.606
[23,     1] loss: 978.857
[24,     1] loss: 962.963
[25,     1] loss: 1000.952
[26,     1] loss: 950.026
[27,     1] loss: 925.924
[28,     1] loss: 930.609
[29,     1] loss: 958.392
[30,     1] loss: 983.357
[31,     1] loss: 882.639
[32,     1] loss: 928.267
[33,     1] loss: 921.073
[34,     1] loss: 910.631
[35,     1] loss: 845.037
[36,     1] loss: 942.739
[37,     1] loss: 829.216
[38,     1] loss: 902.583
[39,     1] loss: 842.597
[40,     1] loss: 921.966
[41,     1] loss: 807.872
[42,     1] loss: 883.800
[43,     1] loss: 838.867
[44,     1] loss: 859.743
[45,     1] loss: 786.033
[46,     1] loss: 767.109
[47,     1] loss: 763.775
[48,     1] loss: 750.908
[49,     1] loss: 723.482
[50,     1] loss: 725.479
[51,     1] loss: 979.271
[52,     1] loss: 1385.067
[53,     1] loss: 861.997
[54,     1] loss: 833.937
[55,     1] loss: 980.970
[56,     1] loss: 1004.826
[57,     1] loss: 913.824
[58,     1] loss: 924.814
[59,     1] loss: 916.076
[60,     1] loss: 908.400
[61,     1] loss: 886.795
[62,     1] loss: 848.961
[63,     1] loss: 827.681
[64,     1] loss: 856.086
[65,     1] loss: 811.699
[66,     1] loss: 774.177
[67,     1] loss: 759.574
[68,     1] loss: 759.332
[69,     1] loss: 767.225
[70,     1] loss: 728.538
[71,     1] loss: 749.402
[72,     1] loss: 727.468
[73,     1] loss: 682.434
[74,     1] loss: 672.303
[75,     1] loss: 665.774
[76,     1] loss: 648.828
[77,     1] loss: 695.082
[78,     1] loss: 649.298
[79,     1] loss: 676.658
[80,     1] loss: 631.902
[81,     1] loss: 602.416
[82,     1] loss: 586.431
[83,     1] loss: 736.034
[84,     1] loss: 1047.846
[85,     1] loss: 1168.857
[86,     1] loss: 1061.069
[87,     1] loss: 919.035
[88,     1] loss: 923.992
Early stopping applied (best metric=0.3818483054637909)
Finished Training
Total time taken: 15.782730340957642
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1229.341
[2,     1] loss: 1231.562
[3,     1] loss: 1227.586
[4,     1] loss: 1227.543
[5,     1] loss: 1225.347
[6,     1] loss: 1220.225
[7,     1] loss: 1211.077
[8,     1] loss: 1190.546
[9,     1] loss: 1147.058
[10,     1] loss: 1091.976
[11,     1] loss: 1075.498
[12,     1] loss: 1126.211
[13,     1] loss: 1045.529
[14,     1] loss: 1038.645
[15,     1] loss: 1035.246
[16,     1] loss: 1035.464
[17,     1] loss: 968.908
[18,     1] loss: 1003.568
[19,     1] loss: 1005.586
[20,     1] loss: 990.112
[21,     1] loss: 993.150
[22,     1] loss: 960.645
[23,     1] loss: 925.503
[24,     1] loss: 929.237
[25,     1] loss: 932.387
[26,     1] loss: 953.368
[27,     1] loss: 929.158
[28,     1] loss: 917.735
[29,     1] loss: 930.112
[30,     1] loss: 889.497
[31,     1] loss: 883.034
[32,     1] loss: 870.286
[33,     1] loss: 865.097
[34,     1] loss: 876.410
[35,     1] loss: 822.735
[36,     1] loss: 818.919
[37,     1] loss: 885.420
[38,     1] loss: 790.378
[39,     1] loss: 822.455
[40,     1] loss: 815.631
[41,     1] loss: 853.253
[42,     1] loss: 917.169
[43,     1] loss: 801.384
[44,     1] loss: 880.124
[45,     1] loss: 799.329
[46,     1] loss: 826.753
[47,     1] loss: 722.252
[48,     1] loss: 784.302
[49,     1] loss: 738.823
[50,     1] loss: 742.824
[51,     1] loss: 798.130
[52,     1] loss: 771.924
[53,     1] loss: 728.663
[54,     1] loss: 793.150
[55,     1] loss: 729.326
[56,     1] loss: 727.308
[57,     1] loss: 676.936
[58,     1] loss: 687.763
[59,     1] loss: 669.329
[60,     1] loss: 625.686
[61,     1] loss: 731.325
[62,     1] loss: 1120.595
[63,     1] loss: 640.730
[64,     1] loss: 935.922
[65,     1] loss: 785.729
[66,     1] loss: 808.139
[67,     1] loss: 878.421
[68,     1] loss: 784.427
[69,     1] loss: 702.407
[70,     1] loss: 765.966
[71,     1] loss: 685.218
[72,     1] loss: 675.526
[73,     1] loss: 665.271
[74,     1] loss: 735.182
[75,     1] loss: 644.573
[76,     1] loss: 631.412
[77,     1] loss: 558.221
[78,     1] loss: 594.572
[79,     1] loss: 571.299
[80,     1] loss: 513.989
[81,     1] loss: 575.544
[82,     1] loss: 584.964
[83,     1] loss: 750.366
[84,     1] loss: 606.092
[85,     1] loss: 603.693
[86,     1] loss: 607.573
[87,     1] loss: 509.714
[88,     1] loss: 721.089
[89,     1] loss: 987.900
[90,     1] loss: 745.699
[91,     1] loss: 900.507
[92,     1] loss: 653.904
[93,     1] loss: 743.065
[94,     1] loss: 755.383
[95,     1] loss: 629.011
[96,     1] loss: 628.212
[97,     1] loss: 639.361
[98,     1] loss: 602.262
[99,     1] loss: 542.417
[100,     1] loss: 625.809
[101,     1] loss: 508.900
[102,     1] loss: 489.666
[103,     1] loss: 462.935
[104,     1] loss: 494.898
[105,     1] loss: 466.983
[106,     1] loss: 451.119
[107,     1] loss: 432.515
[108,     1] loss: 718.871
[109,     1] loss: 1084.228
[110,     1] loss: 1034.075
[111,     1] loss: 1001.061
[112,     1] loss: 955.352
[113,     1] loss: 988.306
[114,     1] loss: 979.555
[115,     1] loss: 1012.594
[116,     1] loss: 956.052
[117,     1] loss: 834.623
[118,     1] loss: 902.666
[119,     1] loss: 926.722
[120,     1] loss: 852.908
[121,     1] loss: 832.951
[122,     1] loss: 852.384
[123,     1] loss: 811.442
[124,     1] loss: 835.537
[125,     1] loss: 774.273
[126,     1] loss: 751.318
[127,     1] loss: 755.460
[128,     1] loss: 750.155
[129,     1] loss: 720.593
[130,     1] loss: 705.296
[131,     1] loss: 598.275
[132,     1] loss: 617.331
[133,     1] loss: 596.709
[134,     1] loss: 608.664
[135,     1] loss: 568.085
[136,     1] loss: 547.998
[137,     1] loss: 773.593
[138,     1] loss: 958.847
[139,     1] loss: 844.373
[140,     1] loss: 1003.293
[141,     1] loss: 911.385
[142,     1] loss: 892.830
[143,     1] loss: 821.997
[144,     1] loss: 768.867
[145,     1] loss: 732.150
[146,     1] loss: 747.035
[147,     1] loss: 680.039
Early stopping applied (best metric=0.3239428699016571)
Finished Training
Total time taken: 27.04237413406372
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.078
[2,     1] loss: 1229.262
[3,     1] loss: 1226.460
[4,     1] loss: 1229.938
[5,     1] loss: 1226.111
[6,     1] loss: 1224.330
[7,     1] loss: 1213.941
[8,     1] loss: 1202.340
[9,     1] loss: 1176.542
[10,     1] loss: 1130.023
[11,     1] loss: 1113.504
[12,     1] loss: 1123.677
[13,     1] loss: 1049.216
[14,     1] loss: 1137.115
[15,     1] loss: 1039.568
[16,     1] loss: 1099.286
[17,     1] loss: 1031.297
[18,     1] loss: 964.076
[19,     1] loss: 1025.006
[20,     1] loss: 1026.997
[21,     1] loss: 1023.748
[22,     1] loss: 1035.248
[23,     1] loss: 964.495
[24,     1] loss: 968.010
[25,     1] loss: 995.105
[26,     1] loss: 960.227
[27,     1] loss: 930.350
[28,     1] loss: 923.266
[29,     1] loss: 979.896
[30,     1] loss: 909.652
[31,     1] loss: 921.173
[32,     1] loss: 869.680
[33,     1] loss: 896.590
[34,     1] loss: 882.831
[35,     1] loss: 849.987
[36,     1] loss: 892.218
[37,     1] loss: 845.220
[38,     1] loss: 798.476
[39,     1] loss: 855.318
[40,     1] loss: 811.794
[41,     1] loss: 758.976
[42,     1] loss: 779.837
[43,     1] loss: 791.872
[44,     1] loss: 810.886
[45,     1] loss: 777.613
[46,     1] loss: 811.030
[47,     1] loss: 747.101
[48,     1] loss: 728.141
[49,     1] loss: 754.422
[50,     1] loss: 747.350
[51,     1] loss: 691.725
[52,     1] loss: 633.696
[53,     1] loss: 675.859
[54,     1] loss: 659.964
[55,     1] loss: 647.946
[56,     1] loss: 1191.032
[57,     1] loss: 1127.644
[58,     1] loss: 775.262
[59,     1] loss: 865.081
[60,     1] loss: 966.178
[61,     1] loss: 945.470
[62,     1] loss: 880.562
[63,     1] loss: 827.649
[64,     1] loss: 829.240
[65,     1] loss: 897.586
[66,     1] loss: 779.098
[67,     1] loss: 851.660
[68,     1] loss: 852.131
[69,     1] loss: 784.791
[70,     1] loss: 800.693
[71,     1] loss: 788.499
[72,     1] loss: 762.658
[73,     1] loss: 745.452
[74,     1] loss: 714.618
[75,     1] loss: 723.113
[76,     1] loss: 719.311
[77,     1] loss: 623.059
[78,     1] loss: 719.542
[79,     1] loss: 652.005
[80,     1] loss: 590.738
[81,     1] loss: 637.055
[82,     1] loss: 556.414
[83,     1] loss: 607.785
[84,     1] loss: 547.000
[85,     1] loss: 511.203
[86,     1] loss: 505.773
[87,     1] loss: 563.032
[88,     1] loss: 1186.038
[89,     1] loss: 2093.415
[90,     1] loss: 1391.848
[91,     1] loss: 1115.528
Early stopping applied (best metric=0.3789949417114258)
Finished Training
Total time taken: 15.939717054367065
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.228
[2,     1] loss: 1230.036
[3,     1] loss: 1227.922
[4,     1] loss: 1231.179
[5,     1] loss: 1230.047
[6,     1] loss: 1226.528
[7,     1] loss: 1222.408
[8,     1] loss: 1218.590
[9,     1] loss: 1200.281
[10,     1] loss: 1179.140
[11,     1] loss: 1150.336
[12,     1] loss: 1110.572
[13,     1] loss: 1077.074
[14,     1] loss: 1042.471
[15,     1] loss: 1069.389
[16,     1] loss: 1057.872
[17,     1] loss: 1099.125
[18,     1] loss: 1029.250
[19,     1] loss: 1110.018
[20,     1] loss: 1011.738
[21,     1] loss: 1013.509
[22,     1] loss: 1013.187
[23,     1] loss: 998.173
[24,     1] loss: 986.013
[25,     1] loss: 994.603
[26,     1] loss: 991.573
[27,     1] loss: 956.725
[28,     1] loss: 940.222
[29,     1] loss: 902.735
[30,     1] loss: 885.560
[31,     1] loss: 876.311
[32,     1] loss: 863.386
[33,     1] loss: 845.892
[34,     1] loss: 949.860
[35,     1] loss: 872.902
[36,     1] loss: 790.791
[37,     1] loss: 834.656
[38,     1] loss: 800.942
[39,     1] loss: 757.573
[40,     1] loss: 805.602
[41,     1] loss: 783.342
[42,     1] loss: 775.963
[43,     1] loss: 757.973
[44,     1] loss: 677.814
[45,     1] loss: 724.385
[46,     1] loss: 826.381
[47,     1] loss: 954.574
[48,     1] loss: 728.668
[49,     1] loss: 875.375
[50,     1] loss: 751.249
[51,     1] loss: 867.979
[52,     1] loss: 729.255
[53,     1] loss: 801.964
[54,     1] loss: 802.219
[55,     1] loss: 742.385
[56,     1] loss: 727.908
[57,     1] loss: 686.601
[58,     1] loss: 762.258
[59,     1] loss: 685.851
[60,     1] loss: 715.985
[61,     1] loss: 630.398
[62,     1] loss: 699.298
[63,     1] loss: 611.810
[64,     1] loss: 585.096
[65,     1] loss: 602.483
[66,     1] loss: 548.869
[67,     1] loss: 557.247
[68,     1] loss: 703.594
[69,     1] loss: 1162.102
[70,     1] loss: 962.506
[71,     1] loss: 913.747
[72,     1] loss: 804.439
[73,     1] loss: 874.500
[74,     1] loss: 967.826
[75,     1] loss: 960.914
[76,     1] loss: 867.162
[77,     1] loss: 832.573
[78,     1] loss: 811.952
[79,     1] loss: 825.198
[80,     1] loss: 841.589
[81,     1] loss: 746.891
[82,     1] loss: 737.622
[83,     1] loss: 776.313
[84,     1] loss: 731.860
[85,     1] loss: 729.116
[86,     1] loss: 701.471
[87,     1] loss: 688.479
[88,     1] loss: 714.947
[89,     1] loss: 639.269
[90,     1] loss: 663.373
[91,     1] loss: 627.153
[92,     1] loss: 612.568
[93,     1] loss: 596.552
[94,     1] loss: 635.869
[95,     1] loss: 590.388
[96,     1] loss: 578.432
[97,     1] loss: 765.398
[98,     1] loss: 643.172
[99,     1] loss: 502.226
[100,     1] loss: 582.228
[101,     1] loss: 523.103
[102,     1] loss: 491.846
Early stopping applied (best metric=0.38184839487075806)
Finished Training
Total time taken: 17.43675398826599
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.880
[2,     1] loss: 1230.615
[3,     1] loss: 1227.220
[4,     1] loss: 1228.147
[5,     1] loss: 1225.804
[6,     1] loss: 1223.276
[7,     1] loss: 1224.030
[8,     1] loss: 1218.510
[9,     1] loss: 1204.237
[10,     1] loss: 1185.716
[11,     1] loss: 1154.553
[12,     1] loss: 1120.368
[13,     1] loss: 1075.377
[14,     1] loss: 1062.261
[15,     1] loss: 1044.289
[16,     1] loss: 1037.794
[17,     1] loss: 1104.272
[18,     1] loss: 973.140
[19,     1] loss: 1010.286
[20,     1] loss: 986.235
[21,     1] loss: 1015.585
[22,     1] loss: 989.379
[23,     1] loss: 1030.169
[24,     1] loss: 982.635
[25,     1] loss: 962.046
[26,     1] loss: 966.050
[27,     1] loss: 924.789
[28,     1] loss: 884.711
[29,     1] loss: 884.674
[30,     1] loss: 864.029
[31,     1] loss: 829.007
[32,     1] loss: 867.699
[33,     1] loss: 832.094
[34,     1] loss: 859.812
[35,     1] loss: 912.217
[36,     1] loss: 808.852
[37,     1] loss: 835.367
[38,     1] loss: 867.523
[39,     1] loss: 826.565
[40,     1] loss: 797.695
[41,     1] loss: 804.394
[42,     1] loss: 789.861
[43,     1] loss: 733.021
[44,     1] loss: 720.309
[45,     1] loss: 754.977
[46,     1] loss: 845.045
[47,     1] loss: 1488.291
[48,     1] loss: 767.138
[49,     1] loss: 1030.297
[50,     1] loss: 942.756
[51,     1] loss: 929.377
[52,     1] loss: 933.172
[53,     1] loss: 900.587
[54,     1] loss: 948.504
[55,     1] loss: 874.870
[56,     1] loss: 805.473
[57,     1] loss: 889.798
[58,     1] loss: 814.224
[59,     1] loss: 798.958
[60,     1] loss: 836.043
[61,     1] loss: 771.018
[62,     1] loss: 830.793
[63,     1] loss: 755.804
[64,     1] loss: 757.324
[65,     1] loss: 750.841
[66,     1] loss: 675.860
[67,     1] loss: 736.748
[68,     1] loss: 663.214
[69,     1] loss: 687.520
[70,     1] loss: 617.670
[71,     1] loss: 640.723
[72,     1] loss: 631.492
[73,     1] loss: 691.642
[74,     1] loss: 750.902
[75,     1] loss: 593.140
[76,     1] loss: 558.126
[77,     1] loss: 633.642
[78,     1] loss: 534.445
[79,     1] loss: 646.482
[80,     1] loss: 736.268
Early stopping applied (best metric=0.3293604850769043)
Finished Training
Total time taken: 13.756073474884033
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.453
[2,     1] loss: 1226.884
[3,     1] loss: 1227.928
[4,     1] loss: 1230.365
[5,     1] loss: 1224.539
[6,     1] loss: 1219.444
[7,     1] loss: 1217.019
[8,     1] loss: 1211.673
[9,     1] loss: 1180.390
[10,     1] loss: 1134.572
[11,     1] loss: 1084.741
[12,     1] loss: 1054.435
[13,     1] loss: 1019.377
[14,     1] loss: 1030.384
[15,     1] loss: 971.880
[16,     1] loss: 995.162
[17,     1] loss: 977.761
[18,     1] loss: 987.671
[19,     1] loss: 959.775
[20,     1] loss: 957.002
[21,     1] loss: 1009.864
[22,     1] loss: 940.237
[23,     1] loss: 1001.936
[24,     1] loss: 895.682
[25,     1] loss: 942.330
[26,     1] loss: 905.487
[27,     1] loss: 887.899
[28,     1] loss: 952.325
[29,     1] loss: 876.774
[30,     1] loss: 904.092
[31,     1] loss: 840.725
[32,     1] loss: 940.538
[33,     1] loss: 848.713
[34,     1] loss: 885.292
[35,     1] loss: 850.072
[36,     1] loss: 868.073
[37,     1] loss: 860.447
[38,     1] loss: 845.756
[39,     1] loss: 823.876
[40,     1] loss: 809.112
[41,     1] loss: 836.193
[42,     1] loss: 757.856
[43,     1] loss: 817.040
[44,     1] loss: 799.914
[45,     1] loss: 783.495
[46,     1] loss: 773.617
[47,     1] loss: 815.691
[48,     1] loss: 810.531
[49,     1] loss: 719.488
[50,     1] loss: 904.461
[51,     1] loss: 801.293
[52,     1] loss: 769.745
[53,     1] loss: 788.784
[54,     1] loss: 750.322
[55,     1] loss: 738.366
[56,     1] loss: 723.351
[57,     1] loss: 722.388
[58,     1] loss: 600.252
[59,     1] loss: 718.059
[60,     1] loss: 709.500
[61,     1] loss: 653.216
[62,     1] loss: 590.304
[63,     1] loss: 650.036
[64,     1] loss: 796.902
[65,     1] loss: 1016.698
[66,     1] loss: 648.709
[67,     1] loss: 853.948
[68,     1] loss: 718.120
[69,     1] loss: 802.963
[70,     1] loss: 752.756
[71,     1] loss: 664.324
[72,     1] loss: 766.988
[73,     1] loss: 647.318
[74,     1] loss: 671.421
[75,     1] loss: 671.399
[76,     1] loss: 553.121
[77,     1] loss: 632.997
[78,     1] loss: 577.146
[79,     1] loss: 562.545
[80,     1] loss: 554.468
[81,     1] loss: 531.683
[82,     1] loss: 604.895
[83,     1] loss: 465.159
[84,     1] loss: 502.962
[85,     1] loss: 741.871
[86,     1] loss: 509.731
[87,     1] loss: 625.637
[88,     1] loss: 778.217
[89,     1] loss: 635.889
[90,     1] loss: 680.176
[91,     1] loss: 549.221
[92,     1] loss: 561.765
[93,     1] loss: 466.890
[94,     1] loss: 597.666
[95,     1] loss: 526.765
[96,     1] loss: 442.498
[97,     1] loss: 552.550
[98,     1] loss: 631.435
[99,     1] loss: 476.542
[100,     1] loss: 662.023
[101,     1] loss: 487.751
[102,     1] loss: 625.880
[103,     1] loss: 474.732
[104,     1] loss: 537.610
[105,     1] loss: 641.228
[106,     1] loss: 471.805
[107,     1] loss: 804.167
[108,     1] loss: 466.495
[109,     1] loss: 699.312
[110,     1] loss: 506.321
[111,     1] loss: 700.889
[112,     1] loss: 488.124
[113,     1] loss: 665.390
[114,     1] loss: 571.192
[115,     1] loss: 473.761
[116,     1] loss: 463.944
[117,     1] loss: 428.022
[118,     1] loss: 435.152
[119,     1] loss: 685.035
[120,     1] loss: 702.378
[121,     1] loss: 512.190
[122,     1] loss: 577.684
[123,     1] loss: 586.731
[124,     1] loss: 557.032
Early stopping applied (best metric=0.3844819962978363)
Finished Training
Total time taken: 24.425185680389404
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1233.918
[2,     1] loss: 1233.824
[3,     1] loss: 1230.884
[4,     1] loss: 1228.677
[5,     1] loss: 1229.390
[6,     1] loss: 1225.486
[7,     1] loss: 1220.346
[8,     1] loss: 1205.795
[9,     1] loss: 1173.719
[10,     1] loss: 1138.410
[11,     1] loss: 1066.478
[12,     1] loss: 1028.718
[13,     1] loss: 998.888
[14,     1] loss: 1008.110
[15,     1] loss: 1017.098
[16,     1] loss: 1012.501
[17,     1] loss: 992.912
[18,     1] loss: 997.886
[19,     1] loss: 976.782
[20,     1] loss: 989.699
[21,     1] loss: 949.076
[22,     1] loss: 954.089
[23,     1] loss: 960.881
[24,     1] loss: 923.446
[25,     1] loss: 888.339
[26,     1] loss: 904.775
[27,     1] loss: 852.341
[28,     1] loss: 896.311
[29,     1] loss: 891.706
[30,     1] loss: 929.558
[31,     1] loss: 885.139
[32,     1] loss: 846.112
[33,     1] loss: 914.003
[34,     1] loss: 830.500
[35,     1] loss: 899.485
[36,     1] loss: 842.797
[37,     1] loss: 935.928
[38,     1] loss: 824.778
[39,     1] loss: 833.005
[40,     1] loss: 895.700
[41,     1] loss: 792.298
[42,     1] loss: 856.347
[43,     1] loss: 821.768
[44,     1] loss: 836.645
[45,     1] loss: 774.745
[46,     1] loss: 805.990
[47,     1] loss: 767.083
[48,     1] loss: 733.208
[49,     1] loss: 794.671
[50,     1] loss: 712.376
[51,     1] loss: 713.167
[52,     1] loss: 653.441
[53,     1] loss: 753.560
[54,     1] loss: 707.199
[55,     1] loss: 658.272
[56,     1] loss: 657.905
[57,     1] loss: 603.538
[58,     1] loss: 672.352
[59,     1] loss: 974.490
[60,     1] loss: 1213.431
[61,     1] loss: 881.008
[62,     1] loss: 793.407
[63,     1] loss: 924.780
[64,     1] loss: 938.980
[65,     1] loss: 918.105
[66,     1] loss: 918.722
[67,     1] loss: 889.937
[68,     1] loss: 885.714
[69,     1] loss: 814.081
[70,     1] loss: 811.371
[71,     1] loss: 824.046
[72,     1] loss: 810.926
[73,     1] loss: 830.198
[74,     1] loss: 836.405
[75,     1] loss: 773.566
[76,     1] loss: 742.143
[77,     1] loss: 727.986
[78,     1] loss: 732.762
[79,     1] loss: 697.655
[80,     1] loss: 693.981
[81,     1] loss: 664.686
[82,     1] loss: 649.815
[83,     1] loss: 602.011
[84,     1] loss: 633.007
[85,     1] loss: 596.415
[86,     1] loss: 665.547
[87,     1] loss: 584.300
[88,     1] loss: 547.692
[89,     1] loss: 531.797
[90,     1] loss: 496.865
[91,     1] loss: 565.638
[92,     1] loss: 708.492
[93,     1] loss: 1302.642
[94,     1] loss: 752.735
[95,     1] loss: 761.974
[96,     1] loss: 941.462
[97,     1] loss: 792.129
[98,     1] loss: 823.138
[99,     1] loss: 858.384
[100,     1] loss: 761.338
[101,     1] loss: 712.038
[102,     1] loss: 823.135
Early stopping applied (best metric=0.410232275724411)
Finished Training
Total time taken: 18.834707021713257
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.090
[2,     1] loss: 1227.678
[3,     1] loss: 1229.728
[4,     1] loss: 1224.067
[5,     1] loss: 1222.241
[6,     1] loss: 1226.989
[7,     1] loss: 1216.973
[8,     1] loss: 1203.297
[9,     1] loss: 1177.827
[10,     1] loss: 1139.482
[11,     1] loss: 1081.033
[12,     1] loss: 1077.495
[13,     1] loss: 1126.321
[14,     1] loss: 1065.743
[15,     1] loss: 1062.648
[16,     1] loss: 1045.453
[17,     1] loss: 1042.653
[18,     1] loss: 1012.694
[19,     1] loss: 1026.724
[20,     1] loss: 994.874
[21,     1] loss: 992.165
[22,     1] loss: 970.764
[23,     1] loss: 977.461
[24,     1] loss: 918.808
[25,     1] loss: 970.489
[26,     1] loss: 913.829
[27,     1] loss: 900.153
[28,     1] loss: 927.567
[29,     1] loss: 925.293
[30,     1] loss: 881.463
[31,     1] loss: 876.804
[32,     1] loss: 914.109
[33,     1] loss: 904.699
[34,     1] loss: 883.479
[35,     1] loss: 892.536
[36,     1] loss: 830.470
[37,     1] loss: 873.917
[38,     1] loss: 849.926
[39,     1] loss: 860.298
[40,     1] loss: 898.968
[41,     1] loss: 852.353
[42,     1] loss: 807.980
[43,     1] loss: 832.349
[44,     1] loss: 776.258
[45,     1] loss: 775.213
[46,     1] loss: 796.375
[47,     1] loss: 789.508
[48,     1] loss: 1080.975
[49,     1] loss: 860.923
[50,     1] loss: 844.968
[51,     1] loss: 816.604
[52,     1] loss: 830.605
[53,     1] loss: 829.107
[54,     1] loss: 815.551
[55,     1] loss: 782.811
[56,     1] loss: 741.260
[57,     1] loss: 705.757
[58,     1] loss: 722.339
[59,     1] loss: 712.579
[60,     1] loss: 637.836
[61,     1] loss: 683.828
[62,     1] loss: 700.368
[63,     1] loss: 717.269
[64,     1] loss: 696.751
[65,     1] loss: 568.921
[66,     1] loss: 568.332
[67,     1] loss: 578.336
[68,     1] loss: 542.436
[69,     1] loss: 768.798
[70,     1] loss: 1509.749
[71,     1] loss: 594.515
[72,     1] loss: 1055.356
[73,     1] loss: 840.026
[74,     1] loss: 885.992
[75,     1] loss: 915.598
[76,     1] loss: 895.904
[77,     1] loss: 838.437
[78,     1] loss: 832.328
[79,     1] loss: 827.512
[80,     1] loss: 793.478
[81,     1] loss: 763.772
[82,     1] loss: 733.739
[83,     1] loss: 709.908
[84,     1] loss: 723.718
[85,     1] loss: 683.709
[86,     1] loss: 700.071
[87,     1] loss: 619.925
[88,     1] loss: 652.606
[89,     1] loss: 540.111
[90,     1] loss: 539.023
[91,     1] loss: 467.443
[92,     1] loss: 520.194
[93,     1] loss: 558.275
[94,     1] loss: 592.435
[95,     1] loss: 912.308
[96,     1] loss: 528.074
[97,     1] loss: 701.177
[98,     1] loss: 577.402
[99,     1] loss: 676.014
[100,     1] loss: 497.973
[101,     1] loss: 715.062
[102,     1] loss: 667.698
[103,     1] loss: 516.470
[104,     1] loss: 680.919
[105,     1] loss: 534.866
[106,     1] loss: 579.467
[107,     1] loss: 585.112
[108,     1] loss: 484.653
[109,     1] loss: 633.712
[110,     1] loss: 467.785
[111,     1] loss: 698.283
[112,     1] loss: 531.559
[113,     1] loss: 594.681
[114,     1] loss: 533.410
[115,     1] loss: 443.010
[116,     1] loss: 460.859
[117,     1] loss: 456.869
[118,     1] loss: 457.745
[119,     1] loss: 644.618
[120,     1] loss: 453.646
[121,     1] loss: 443.919
[122,     1] loss: 502.539
[123,     1] loss: 399.104
[124,     1] loss: 472.761
[125,     1] loss: 733.875
[126,     1] loss: 818.062
[127,     1] loss: 643.734
[128,     1] loss: 844.538
[129,     1] loss: 629.508
[130,     1] loss: 689.390
Early stopping applied (best metric=0.3068304657936096)
Finished Training
Total time taken: 23.642195224761963
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.955
[2,     1] loss: 1227.611
[3,     1] loss: 1230.491
[4,     1] loss: 1236.831
[5,     1] loss: 1228.165
[6,     1] loss: 1227.426
[7,     1] loss: 1232.944
[8,     1] loss: 1228.617
[9,     1] loss: 1227.116
[10,     1] loss: 1226.406
[11,     1] loss: 1222.723
[12,     1] loss: 1222.247
[13,     1] loss: 1209.459
[14,     1] loss: 1196.834
[15,     1] loss: 1168.246
[16,     1] loss: 1141.389
[17,     1] loss: 1110.546
[18,     1] loss: 1103.893
[19,     1] loss: 1029.765
[20,     1] loss: 1039.198
[21,     1] loss: 1047.075
[22,     1] loss: 1022.198
[23,     1] loss: 961.359
[24,     1] loss: 1022.472
[25,     1] loss: 980.767
[26,     1] loss: 988.049
[27,     1] loss: 952.929
[28,     1] loss: 938.740
[29,     1] loss: 928.854
[30,     1] loss: 887.741
[31,     1] loss: 923.842
[32,     1] loss: 958.343
[33,     1] loss: 878.609
[34,     1] loss: 866.752
[35,     1] loss: 842.181
[36,     1] loss: 916.219
[37,     1] loss: 994.574
[38,     1] loss: 822.992
[39,     1] loss: 876.177
[40,     1] loss: 854.486
[41,     1] loss: 852.215
[42,     1] loss: 825.847
[43,     1] loss: 853.505
[44,     1] loss: 810.792
[45,     1] loss: 847.911
[46,     1] loss: 803.008
[47,     1] loss: 813.833
[48,     1] loss: 796.597
[49,     1] loss: 753.962
[50,     1] loss: 808.600
[51,     1] loss: 726.344
[52,     1] loss: 761.493
[53,     1] loss: 741.295
[54,     1] loss: 773.939
[55,     1] loss: 813.013
[56,     1] loss: 700.917
[57,     1] loss: 736.412
[58,     1] loss: 740.106
[59,     1] loss: 673.819
[60,     1] loss: 711.309
[61,     1] loss: 648.185
[62,     1] loss: 714.893
[63,     1] loss: 749.649
[64,     1] loss: 644.357
[65,     1] loss: 636.063
[66,     1] loss: 650.387
[67,     1] loss: 603.390
[68,     1] loss: 584.282
[69,     1] loss: 574.558
[70,     1] loss: 642.756
[71,     1] loss: 1119.361
[72,     1] loss: 567.479
[73,     1] loss: 914.365
[74,     1] loss: 745.109
[75,     1] loss: 876.906
[76,     1] loss: 869.182
[77,     1] loss: 743.203
[78,     1] loss: 681.492
[79,     1] loss: 732.425
[80,     1] loss: 662.795
[81,     1] loss: 690.406
[82,     1] loss: 602.484
[83,     1] loss: 583.077
[84,     1] loss: 563.498
[85,     1] loss: 604.593
[86,     1] loss: 520.195
[87,     1] loss: 631.294
[88,     1] loss: 615.380
[89,     1] loss: 495.439
[90,     1] loss: 563.072
[91,     1] loss: 552.439
[92,     1] loss: 502.117
[93,     1] loss: 579.370
[94,     1] loss: 512.952
[95,     1] loss: 455.353
[96,     1] loss: 466.423
[97,     1] loss: 519.856
Early stopping applied (best metric=0.3478722870349884)
Finished Training
Total time taken: 17.637749910354614
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.721
[2,     1] loss: 1232.314
[3,     1] loss: 1226.325
[4,     1] loss: 1232.721
[5,     1] loss: 1228.708
[6,     1] loss: 1222.985
[7,     1] loss: 1222.130
[8,     1] loss: 1203.995
[9,     1] loss: 1182.573
[10,     1] loss: 1142.170
[11,     1] loss: 1108.243
[12,     1] loss: 1049.964
[13,     1] loss: 1008.258
[14,     1] loss: 1037.884
[15,     1] loss: 1003.815
[16,     1] loss: 1060.851
[17,     1] loss: 1065.659
[18,     1] loss: 974.641
[19,     1] loss: 1041.489
[20,     1] loss: 994.318
[21,     1] loss: 975.126
[22,     1] loss: 967.059
[23,     1] loss: 937.544
[24,     1] loss: 931.113
[25,     1] loss: 948.205
[26,     1] loss: 893.592
[27,     1] loss: 904.506
[28,     1] loss: 899.134
[29,     1] loss: 881.340
[30,     1] loss: 861.028
[31,     1] loss: 821.905
[32,     1] loss: 798.968
[33,     1] loss: 790.860
[34,     1] loss: 826.482
[35,     1] loss: 779.467
[36,     1] loss: 813.817
[37,     1] loss: 897.619
[38,     1] loss: 903.502
[39,     1] loss: 741.742
[40,     1] loss: 885.744
[41,     1] loss: 782.299
[42,     1] loss: 763.384
[43,     1] loss: 791.081
[44,     1] loss: 758.511
[45,     1] loss: 749.822
[46,     1] loss: 719.779
[47,     1] loss: 696.969
[48,     1] loss: 650.470
[49,     1] loss: 727.645
[50,     1] loss: 963.070
[51,     1] loss: 709.415
[52,     1] loss: 830.940
[53,     1] loss: 702.146
[54,     1] loss: 832.940
[55,     1] loss: 743.101
[56,     1] loss: 710.096
[57,     1] loss: 736.437
[58,     1] loss: 674.025
[59,     1] loss: 746.592
[60,     1] loss: 643.684
[61,     1] loss: 622.991
[62,     1] loss: 606.685
[63,     1] loss: 556.949
[64,     1] loss: 570.933
[65,     1] loss: 606.628
[66,     1] loss: 627.216
[67,     1] loss: 511.407
[68,     1] loss: 485.486
[69,     1] loss: 538.278
[70,     1] loss: 730.050
[71,     1] loss: 1341.491
[72,     1] loss: 1067.215
[73,     1] loss: 1105.438
[74,     1] loss: 798.980
[75,     1] loss: 882.646
[76,     1] loss: 951.852
[77,     1] loss: 1023.307
[78,     1] loss: 1028.193
[79,     1] loss: 994.402
[80,     1] loss: 969.951
[81,     1] loss: 935.547
[82,     1] loss: 927.452
[83,     1] loss: 867.004
[84,     1] loss: 806.148
[85,     1] loss: 866.584
[86,     1] loss: 869.388
[87,     1] loss: 806.370
[88,     1] loss: 797.686
Early stopping applied (best metric=0.40270447731018066)
Finished Training
Total time taken: 16.6002414226532
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.671
[2,     1] loss: 1228.450
[3,     1] loss: 1233.915
[4,     1] loss: 1225.085
[5,     1] loss: 1227.466
[6,     1] loss: 1215.966
[7,     1] loss: 1204.714
[8,     1] loss: 1173.976
[9,     1] loss: 1116.875
[10,     1] loss: 1060.410
[11,     1] loss: 1061.098
[12,     1] loss: 1014.480
[13,     1] loss: 1008.541
[14,     1] loss: 1008.392
[15,     1] loss: 1002.871
[16,     1] loss: 985.655
[17,     1] loss: 1022.714
[18,     1] loss: 965.218
[19,     1] loss: 988.443
[20,     1] loss: 945.203
[21,     1] loss: 934.578
[22,     1] loss: 947.711
[23,     1] loss: 901.810
[24,     1] loss: 886.748
[25,     1] loss: 931.452
[26,     1] loss: 917.689
[27,     1] loss: 883.536
[28,     1] loss: 877.158
[29,     1] loss: 868.762
[30,     1] loss: 843.601
[31,     1] loss: 901.958
[32,     1] loss: 915.586
[33,     1] loss: 772.675
[34,     1] loss: 841.410
[35,     1] loss: 783.977
[36,     1] loss: 840.059
[37,     1] loss: 830.962
[38,     1] loss: 762.874
[39,     1] loss: 846.253
[40,     1] loss: 785.385
[41,     1] loss: 776.457
[42,     1] loss: 777.254
[43,     1] loss: 680.885
[44,     1] loss: 707.862
[45,     1] loss: 708.928
[46,     1] loss: 702.520
[47,     1] loss: 680.410
[48,     1] loss: 773.174
[49,     1] loss: 1547.744
[50,     1] loss: 711.876
[51,     1] loss: 1018.683
[52,     1] loss: 948.714
[53,     1] loss: 903.888
[54,     1] loss: 934.911
[55,     1] loss: 977.065
[56,     1] loss: 943.785
[57,     1] loss: 901.700
[58,     1] loss: 879.256
[59,     1] loss: 830.691
[60,     1] loss: 910.738
[61,     1] loss: 860.973
[62,     1] loss: 811.150
[63,     1] loss: 857.420
[64,     1] loss: 791.768
[65,     1] loss: 773.740
[66,     1] loss: 825.256
[67,     1] loss: 794.271
[68,     1] loss: 757.408
[69,     1] loss: 749.953
[70,     1] loss: 724.119
[71,     1] loss: 736.093
[72,     1] loss: 737.574
[73,     1] loss: 729.409
[74,     1] loss: 687.430
[75,     1] loss: 695.891
[76,     1] loss: 653.595
[77,     1] loss: 656.191
[78,     1] loss: 685.379
[79,     1] loss: 600.618
[80,     1] loss: 645.680
[81,     1] loss: 835.474
[82,     1] loss: 1344.873
[83,     1] loss: 762.052
[84,     1] loss: 877.472
[85,     1] loss: 990.548
[86,     1] loss: 917.961
[87,     1] loss: 884.717
[88,     1] loss: 856.855
Early stopping applied (best metric=0.39023885130882263)
Finished Training
Total time taken: 16.282175302505493
(48,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176,)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1232.847
[2,     1] loss: 1235.738
[3,     1] loss: 1230.713
[4,     1] loss: 1229.187
[5,     1] loss: 1225.340
[6,     1] loss: 1228.132
[7,     1] loss: 1229.094
[8,     1] loss: 1221.559
[9,     1] loss: 1217.655
[10,     1] loss: 1199.861
[11,     1] loss: 1180.964
[12,     1] loss: 1144.742
[13,     1] loss: 1116.193
[14,     1] loss: 1070.652
[15,     1] loss: 1051.143
[16,     1] loss: 1034.568
[17,     1] loss: 1038.889
[18,     1] loss: 1012.787
[19,     1] loss: 1089.431
[20,     1] loss: 977.415
[21,     1] loss: 991.339
[22,     1] loss: 978.328
[23,     1] loss: 1033.276
[24,     1] loss: 976.613
[25,     1] loss: 991.321
[26,     1] loss: 950.455
[27,     1] loss: 956.244
[28,     1] loss: 904.461
[29,     1] loss: 917.327
[30,     1] loss: 899.867
[31,     1] loss: 963.719
[32,     1] loss: 858.557
[33,     1] loss: 946.578
[34,     1] loss: 910.714
[35,     1] loss: 911.089
[36,     1] loss: 862.806
[37,     1] loss: 902.347
[38,     1] loss: 857.038
[39,     1] loss: 873.227
[40,     1] loss: 803.832
[41,     1] loss: 853.928
[42,     1] loss: 808.123
[43,     1] loss: 828.212
[44,     1] loss: 812.704
[45,     1] loss: 745.859
[46,     1] loss: 783.500
[47,     1] loss: 865.690
[48,     1] loss: 805.331
[49,     1] loss: 759.831
[50,     1] loss: 751.780
[51,     1] loss: 762.034
[52,     1] loss: 769.189
[53,     1] loss: 809.829
[54,     1] loss: 737.482
[55,     1] loss: 645.305
[56,     1] loss: 612.736
[57,     1] loss: 695.380
[58,     1] loss: 772.174
[59,     1] loss: 950.013
[60,     1] loss: 927.615
[61,     1] loss: 776.345
[62,     1] loss: 813.616
[63,     1] loss: 843.855
[64,     1] loss: 774.296
[65,     1] loss: 776.468
[66,     1] loss: 760.066
[67,     1] loss: 795.390
[68,     1] loss: 761.310
[69,     1] loss: 731.441
[70,     1] loss: 698.012
[71,     1] loss: 656.275
[72,     1] loss: 609.974
[73,     1] loss: 640.313
[74,     1] loss: 652.319
[75,     1] loss: 642.665
[76,     1] loss: 593.436
[77,     1] loss: 507.990
[78,     1] loss: 591.773
[79,     1] loss: 670.646
[80,     1] loss: 740.109
[81,     1] loss: 635.770
[82,     1] loss: 623.914
[83,     1] loss: 592.037
[84,     1] loss: 545.912
[85,     1] loss: 753.162
[86,     1] loss: 832.384
[87,     1] loss: 553.989
[88,     1] loss: 775.151
[89,     1] loss: 590.514
[90,     1] loss: 726.003
[91,     1] loss: 525.002
[92,     1] loss: 657.773
[93,     1] loss: 511.710
[94,     1] loss: 486.551
[95,     1] loss: 539.253
[96,     1] loss: 431.011
[97,     1] loss: 463.265
[98,     1] loss: 489.066
[99,     1] loss: 555.683
[100,     1] loss: 756.831
[101,     1] loss: 1089.260
[102,     1] loss: 592.676
[103,     1] loss: 774.925
[104,     1] loss: 853.633
[105,     1] loss: 704.690
Early stopping applied (best metric=0.3550633192062378)
Finished Training
Total time taken: 18.80916166305542
results!
{'gpu_mode': True, 'epochs': 200, 'batch_size': 2048, 'learning_rate': 0.004948855406258137, 'test_data_ratio': 0.2, 'data_sample_mode': ['oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample'], 'crossValidation': True, 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'folds': 5, 'earlyStopping': True, 'ValidationMetric': 'Validation Loss (Hydroxylation-P)', 'earlyStoppingPatience': 50, 'CV_Repeats': 5, 'Experiment Name': 'Model architecture - added max, ranges, bceloss', 'CreateFigures': False, 'weight_decay': 6.967659352813328, 'embeddingType': 'adaptiveEmbedding', 'LSTM_layers': 1, 'LSTM_hidden_size': 64, 'LSTM_dropout': 0, 'MultiTask': False, 'MultiTask_sample_method': 'balanced', 'UseUncertaintyBasedLoss': False, 'useLrWeight': True, 'CNNType': 'Musite', 'FCType': 'Adapt', 'layerToSplitOn': 'FC', 'dontAverageLoss': False, 'useWeightDecayWeight': False, 'SeperateTuningLRandWD': True, 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'], 'n_trials': 500, 'FloatsToTune': {'learning_rate': [1e-05, 0.01], 'weight_decay': [0, 10], 'log_base': [1.01, 3], 'learning_rate_Hydroxylation-K': [1e-05, 0.01], 'learning_rate_Hydroxylation-P': [1e-05, 0.01], 'weight_decay_Hydroxylation-P': [0, 10], 'weight_decay_Hydroxylation-K': [0, 10]}, 'IntsToTune': {}, 'log_base': 2.9987863126777072, 'learning_rate_Hydroxylation-K': 0.0012899699863845916, 'learning_rate_Hydroxylation-P': 0.005001178808747827, 'weight_decay_Hydroxylation-P': 4.378285509462673, 'weight_decay_Hydroxylation-K': 1.3884101414855463, 'random_state': 2738634999, 'current_CV_Repeat': 5, 'sample_weights': [1.5212801934495765, 1], 'WeightDecayWeights': [], 'currentFold': 4}
{'Hydroxylation-K Validation Accuracy': 0.7229078014184397, 'Hydroxylation-K Validation Sensitivity': 0.6715555555555556, 'Hydroxylation-K Validation Specificity': 0.7357894736842105, 'Hydroxylation-K Validation Precision': 0.4012647631928154, 'Hydroxylation-K AUC ROC': 0.7844444444444444, 'Hydroxylation-K AUC PR': 0.5973386688700538, 'Hydroxylation-K MCC': 0.3498804703658171, 'Hydroxylation-K F1': 0.4971225623399455, 'Validation Loss (Hydroxylation-K)': 0.4702174508571625, 'Hydroxylation-P Validation Accuracy': 0.7690813664281001, 'Hydroxylation-P Validation Sensitivity': 0.8146666666666667, 'Hydroxylation-P Validation Specificity': 0.7592638036809816, 'Hydroxylation-P Validation Precision': 0.42670278882505525, 'Hydroxylation-P AUC ROC': 0.854375873158381, 'Hydroxylation-P AUC PR': 0.5838669981610717, 'Hydroxylation-P MCC': 0.46509920750341877, 'Hydroxylation-P F1': 0.5579223069295867, 'Validation Loss (Hydroxylation-P)': 0.36136805295944213, 'Validation Loss (total)': 0.831585500240326, 'TimeToTrain': 19.13323003768921}
{'Hydroxylation-K Validation Accuracy': 0.07071220173904426, 'Hydroxylation-K Validation Sensitivity': 0.14272768972991143, 'Hydroxylation-K Validation Specificity': 0.08509676204435736, 'Hydroxylation-K Validation Precision': 0.0886601258636906, 'Hydroxylation-K AUC ROC': 0.07098787549609212, 'Hydroxylation-K AUC PR': 0.10854520119770655, 'Hydroxylation-K MCC': 0.13667543491655143, 'Hydroxylation-K F1': 0.09345102895390281, 'Validation Loss (Hydroxylation-K)': 0.06673500541818977, 'Hydroxylation-P Validation Accuracy': 0.04310956382827103, 'Hydroxylation-P Validation Sensitivity': 0.062153600401199174, 'Hydroxylation-P Validation Specificity': 0.05421593820111131, 'Hydroxylation-P Validation Precision': 0.052562751508581695, 'Hydroxylation-P AUC ROC': 0.03171949600856535, 'Hydroxylation-P AUC PR': 0.06450804662643427, 'Hydroxylation-P MCC': 0.06438303646841584, 'Hydroxylation-P F1': 0.050162996459883966, 'Validation Loss (Hydroxylation-P)': 0.034873236266474227, 'Validation Loss (total)': 0.06878190427815496, 'TimeToTrain': 4.5557480055241975}
