{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00017728460421043144,
 'learning_rate_Hydroxylation-K': 0.009469166145197007,
 'learning_rate_Hydroxylation-P': 0.006810069236353667,
 'log_base': 2.3473102521803932,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1575256125,
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.306124920512895,
 'weight_decay_Hydroxylation-K': 3.8649035968006507,
 'weight_decay_Hydroxylation-P': 5.275797295447422}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.909
[2,     1] loss: 1322.547
[3,     1] loss: 1319.734
[4,     1] loss: 1321.534
[5,     1] loss: 1318.636
[6,     1] loss: 1322.979
[7,     1] loss: 1322.120
[8,     1] loss: 1321.802
[9,     1] loss: 1319.677
[10,     1] loss: 1319.087
[11,     1] loss: 1320.139
[12,     1] loss: 1321.095
[13,     1] loss: 1320.933
[14,     1] loss: 1320.590
[15,     1] loss: 1321.645
[16,     1] loss: 1319.619
[17,     1] loss: 1319.702
[18,     1] loss: 1319.411
[19,     1] loss: 1319.497
[20,     1] loss: 1322.227
[21,     1] loss: 1316.737
[22,     1] loss: 1318.350
[23,     1] loss: 1318.434
[24,     1] loss: 1317.771
[25,     1] loss: 1316.901
[26,     1] loss: 1317.391
[27,     1] loss: 1317.291
[28,     1] loss: 1318.815
[29,     1] loss: 1316.600
[30,     1] loss: 1316.229
[31,     1] loss: 1316.674
[32,     1] loss: 1318.898
[33,     1] loss: 1315.894
[34,     1] loss: 1315.351
[35,     1] loss: 1315.942
[36,     1] loss: 1317.217
[37,     1] loss: 1313.736
[38,     1] loss: 1312.045
[39,     1] loss: 1314.105
[40,     1] loss: 1312.756
[41,     1] loss: 1313.611
[42,     1] loss: 1311.635
[43,     1] loss: 1310.517
[44,     1] loss: 1309.356
[45,     1] loss: 1308.346
[46,     1] loss: 1308.525
[47,     1] loss: 1303.688
[48,     1] loss: 1299.637
[49,     1] loss: 1303.809
[50,     1] loss: 1298.488
[51,     1] loss: 1293.303
[52,     1] loss: 1291.980
[53,     1] loss: 1287.733
[54,     1] loss: 1285.506
[55,     1] loss: 1284.616
[56,     1] loss: 1273.785
[57,     1] loss: 1272.425
[58,     1] loss: 1267.453
[59,     1] loss: 1259.938
[60,     1] loss: 1259.289
[61,     1] loss: 1257.942
[62,     1] loss: 1247.970
[63,     1] loss: 1236.547
[64,     1] loss: 1244.673
[65,     1] loss: 1233.192
[66,     1] loss: 1225.354
[67,     1] loss: 1217.712
[68,     1] loss: 1210.486
[69,     1] loss: 1217.111
[70,     1] loss: 1223.214
[71,     1] loss: 1185.524
[72,     1] loss: 1188.991
[73,     1] loss: 1184.906
[74,     1] loss: 1183.960
[75,     1] loss: 1165.523
[76,     1] loss: 1166.551
[77,     1] loss: 1170.256
[78,     1] loss: 1123.840
[79,     1] loss: 1152.836
[80,     1] loss: 1143.078
[81,     1] loss: 1132.841
[82,     1] loss: 1140.621
[83,     1] loss: 1147.382
[84,     1] loss: 1133.292
[85,     1] loss: 1127.030
[86,     1] loss: 1133.015
[87,     1] loss: 1097.725
[88,     1] loss: 1114.525
[89,     1] loss: 1124.526
[90,     1] loss: 1111.020
[91,     1] loss: 1116.404
[92,     1] loss: 1112.302
[93,     1] loss: 1109.792
[94,     1] loss: 1127.495
[95,     1] loss: 1127.778
[96,     1] loss: 1065.657
[97,     1] loss: 1064.829
[98,     1] loss: 1105.423
[99,     1] loss: 1099.098
[100,     1] loss: 1113.841
[101,     1] loss: 1108.437
[102,     1] loss: 1104.113
[103,     1] loss: 1092.178
[104,     1] loss: 1098.072
[105,     1] loss: 1093.453
[106,     1] loss: 1063.209
[107,     1] loss: 1112.464
[108,     1] loss: 1095.873
[109,     1] loss: 1057.588
[110,     1] loss: 1091.661
[111,     1] loss: 1081.269
[112,     1] loss: 1055.799
[113,     1] loss: 1124.442
[114,     1] loss: 1069.689
[115,     1] loss: 1095.801
[116,     1] loss: 1105.452
[117,     1] loss: 1077.848
[118,     1] loss: 1074.982
[119,     1] loss: 1026.712
[120,     1] loss: 1106.297
[121,     1] loss: 1090.326
[122,     1] loss: 1056.985
[123,     1] loss: 1093.296
[124,     1] loss: 1076.702
[125,     1] loss: 1054.783
[126,     1] loss: 1060.038
[127,     1] loss: 1063.796
[128,     1] loss: 1063.150
[129,     1] loss: 1043.622
[130,     1] loss: 1051.675
[131,     1] loss: 1042.561
[132,     1] loss: 1081.213
[133,     1] loss: 1071.243
[134,     1] loss: 1047.366
[135,     1] loss: 1066.583
[136,     1] loss: 1054.052
[137,     1] loss: 1039.431
[138,     1] loss: 1055.638
[139,     1] loss: 1071.080
[140,     1] loss: 1013.048
[141,     1] loss: 1053.721
[142,     1] loss: 1039.871
[143,     1] loss: 1026.373
[144,     1] loss: 1034.446
[145,     1] loss: 1034.823
[146,     1] loss: 1025.632
[147,     1] loss: 995.759
[148,     1] loss: 1018.352
[149,     1] loss: 1031.855
[150,     1] loss: 1053.526
[151,     1] loss: 1062.124
[152,     1] loss: 992.035
[153,     1] loss: 1018.565
[154,     1] loss: 993.997
[155,     1] loss: 995.388
[156,     1] loss: 988.750
[157,     1] loss: 1041.131
[158,     1] loss: 1014.954
[159,     1] loss: 962.782
[160,     1] loss: 1019.444
[161,     1] loss: 1033.617
[162,     1] loss: 1006.754
[163,     1] loss: 1012.933
[164,     1] loss: 973.864
[165,     1] loss: 989.986
[166,     1] loss: 1015.191
[167,     1] loss: 1002.719
[168,     1] loss: 1007.471
[169,     1] loss: 982.534
[170,     1] loss: 967.615
[171,     1] loss: 998.066
[172,     1] loss: 968.417
[173,     1] loss: 996.305
[174,     1] loss: 952.391
[175,     1] loss: 999.769
[176,     1] loss: 970.589
[177,     1] loss: 968.679
[178,     1] loss: 936.340
[179,     1] loss: 936.507
[180,     1] loss: 969.620
[181,     1] loss: 979.736
[182,     1] loss: 921.494
[183,     1] loss: 948.221
[184,     1] loss: 970.493
[185,     1] loss: 990.332
[186,     1] loss: 960.443
[187,     1] loss: 948.625
[188,     1] loss: 979.063
[189,     1] loss: 943.806
[190,     1] loss: 985.167
[191,     1] loss: 961.040
[192,     1] loss: 931.871
[193,     1] loss: 937.919
[194,     1] loss: 958.282
[195,     1] loss: 942.420
[196,     1] loss: 891.183
[197,     1] loss: 969.615
[198,     1] loss: 953.276
[199,     1] loss: 923.518
[200,     1] loss: 907.988
Finished Training
Total time taken: 30.499094247817993
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.869
[2,     1] loss: 1318.960
[3,     1] loss: 1325.260
[4,     1] loss: 1316.919
[5,     1] loss: 1315.554
[6,     1] loss: 1322.624
[7,     1] loss: 1323.954
[8,     1] loss: 1318.251
[9,     1] loss: 1317.333
[10,     1] loss: 1322.537
[11,     1] loss: 1323.384
[12,     1] loss: 1322.587
[13,     1] loss: 1318.643
[14,     1] loss: 1320.350
[15,     1] loss: 1320.103
[16,     1] loss: 1317.916
[17,     1] loss: 1318.305
[18,     1] loss: 1317.539
[19,     1] loss: 1317.511
[20,     1] loss: 1318.923
[21,     1] loss: 1318.944
[22,     1] loss: 1315.695
[23,     1] loss: 1317.965
[24,     1] loss: 1320.490
[25,     1] loss: 1318.712
[26,     1] loss: 1316.269
[27,     1] loss: 1316.622
[28,     1] loss: 1316.826
[29,     1] loss: 1313.423
[30,     1] loss: 1314.300
[31,     1] loss: 1311.778
[32,     1] loss: 1312.703
[33,     1] loss: 1315.374
[34,     1] loss: 1313.639
[35,     1] loss: 1312.317
[36,     1] loss: 1310.571
[37,     1] loss: 1309.988
[38,     1] loss: 1306.916
[39,     1] loss: 1307.294
[40,     1] loss: 1306.174
[41,     1] loss: 1305.256
[42,     1] loss: 1302.203
[43,     1] loss: 1297.838
[44,     1] loss: 1299.416
[45,     1] loss: 1297.091
[46,     1] loss: 1290.001
[47,     1] loss: 1290.253
[48,     1] loss: 1279.641
[49,     1] loss: 1283.955
[50,     1] loss: 1277.715
[51,     1] loss: 1275.897
[52,     1] loss: 1271.351
[53,     1] loss: 1260.658
[54,     1] loss: 1265.040
[55,     1] loss: 1253.831
[56,     1] loss: 1257.518
[57,     1] loss: 1241.398
[58,     1] loss: 1242.342
[59,     1] loss: 1235.380
[60,     1] loss: 1225.512
[61,     1] loss: 1221.224
[62,     1] loss: 1230.520
[63,     1] loss: 1217.583
[64,     1] loss: 1204.631
[65,     1] loss: 1205.656
[66,     1] loss: 1196.659
[67,     1] loss: 1194.600
[68,     1] loss: 1180.096
[69,     1] loss: 1175.178
[70,     1] loss: 1173.595
[71,     1] loss: 1150.791
[72,     1] loss: 1164.558
[73,     1] loss: 1155.943
[74,     1] loss: 1139.611
[75,     1] loss: 1126.286
[76,     1] loss: 1139.015
[77,     1] loss: 1149.720
[78,     1] loss: 1132.652
[79,     1] loss: 1126.201
[80,     1] loss: 1118.999
[81,     1] loss: 1105.480
[82,     1] loss: 1094.530
[83,     1] loss: 1092.189
[84,     1] loss: 1118.842
[85,     1] loss: 1116.786
[86,     1] loss: 1102.049
[87,     1] loss: 1130.393
[88,     1] loss: 1106.254
[89,     1] loss: 1103.616
[90,     1] loss: 1096.659
[91,     1] loss: 1060.054
[92,     1] loss: 1093.382
[93,     1] loss: 1095.328
[94,     1] loss: 1108.365
[95,     1] loss: 1077.687
[96,     1] loss: 1076.049
[97,     1] loss: 1042.113
[98,     1] loss: 1052.568
[99,     1] loss: 1091.102
[100,     1] loss: 1098.845
[101,     1] loss: 1059.705
[102,     1] loss: 1056.670
[103,     1] loss: 1091.740
[104,     1] loss: 1095.844
[105,     1] loss: 1060.547
[106,     1] loss: 1082.513
[107,     1] loss: 1050.694
[108,     1] loss: 1056.348
[109,     1] loss: 1068.205
[110,     1] loss: 1036.906
[111,     1] loss: 1060.414
[112,     1] loss: 1062.501
[113,     1] loss: 1048.848
[114,     1] loss: 1047.934
[115,     1] loss: 1041.642
[116,     1] loss: 1039.339
[117,     1] loss: 1072.391
[118,     1] loss: 1017.296
[119,     1] loss: 1024.506
[120,     1] loss: 1062.101
[121,     1] loss: 1059.616
[122,     1] loss: 1027.811
[123,     1] loss: 1021.238
[124,     1] loss: 1066.089
[125,     1] loss: 1074.758
[126,     1] loss: 1036.903
[127,     1] loss: 1033.567
[128,     1] loss: 1031.837
[129,     1] loss: 1041.525
[130,     1] loss: 1047.441
[131,     1] loss: 1031.903
[132,     1] loss: 1069.467
[133,     1] loss: 1027.785
[134,     1] loss: 1029.901
[135,     1] loss: 1014.769
[136,     1] loss: 1021.349
[137,     1] loss: 1008.211
[138,     1] loss: 1003.119
[139,     1] loss: 995.289
[140,     1] loss: 1032.933
[141,     1] loss: 1016.149
[142,     1] loss: 1061.906
[143,     1] loss: 1020.993
[144,     1] loss: 995.721
[145,     1] loss: 986.599
[146,     1] loss: 1028.737
[147,     1] loss: 976.135
[148,     1] loss: 993.889
[149,     1] loss: 1003.698
[150,     1] loss: 966.150
[151,     1] loss: 1034.884
[152,     1] loss: 967.191
[153,     1] loss: 1011.556
[154,     1] loss: 991.047
[155,     1] loss: 981.957
[156,     1] loss: 1026.529
[157,     1] loss: 985.340
[158,     1] loss: 1012.636
[159,     1] loss: 982.363
[160,     1] loss: 942.225
[161,     1] loss: 963.426
[162,     1] loss: 1006.857
[163,     1] loss: 966.041
[164,     1] loss: 990.360
[165,     1] loss: 986.863
[166,     1] loss: 961.876
[167,     1] loss: 984.792
[168,     1] loss: 937.622
[169,     1] loss: 988.872
[170,     1] loss: 945.726
[171,     1] loss: 975.117
[172,     1] loss: 947.291
[173,     1] loss: 936.067
[174,     1] loss: 914.303
[175,     1] loss: 946.582
[176,     1] loss: 970.076
Early stopping applied (best metric=0.8233379125595093)
Finished Training
Total time taken: 28.44783353805542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.451
[2,     1] loss: 1322.068
[3,     1] loss: 1321.046
[4,     1] loss: 1323.392
[5,     1] loss: 1318.713
[6,     1] loss: 1325.198
[7,     1] loss: 1326.457
[8,     1] loss: 1324.403
[9,     1] loss: 1316.091
[10,     1] loss: 1321.098
[11,     1] loss: 1322.913
[12,     1] loss: 1319.409
[13,     1] loss: 1320.948
[14,     1] loss: 1321.795
[15,     1] loss: 1322.063
[16,     1] loss: 1318.520
[17,     1] loss: 1321.239
[18,     1] loss: 1319.363
[19,     1] loss: 1320.067
[20,     1] loss: 1320.362
[21,     1] loss: 1322.727
[22,     1] loss: 1321.141
[23,     1] loss: 1318.935
[24,     1] loss: 1321.699
[25,     1] loss: 1319.593
[26,     1] loss: 1320.930
[27,     1] loss: 1318.231
[28,     1] loss: 1319.931
[29,     1] loss: 1318.389
[30,     1] loss: 1321.789
[31,     1] loss: 1319.779
[32,     1] loss: 1319.207
[33,     1] loss: 1320.828
[34,     1] loss: 1317.032
[35,     1] loss: 1316.915
[36,     1] loss: 1318.464
[37,     1] loss: 1317.621
[38,     1] loss: 1320.798
[39,     1] loss: 1320.225
[40,     1] loss: 1319.636
[41,     1] loss: 1318.292
[42,     1] loss: 1320.280
[43,     1] loss: 1319.641
[44,     1] loss: 1319.657
[45,     1] loss: 1319.100
[46,     1] loss: 1319.135
[47,     1] loss: 1317.376
[48,     1] loss: 1317.282
[49,     1] loss: 1320.120
[50,     1] loss: 1316.547
[51,     1] loss: 1315.502
[52,     1] loss: 1315.331
[53,     1] loss: 1314.688
[54,     1] loss: 1316.133
[55,     1] loss: 1314.707
[56,     1] loss: 1315.967
[57,     1] loss: 1315.092
[58,     1] loss: 1314.808
[59,     1] loss: 1314.012
[60,     1] loss: 1312.924
[61,     1] loss: 1310.502
[62,     1] loss: 1311.526
[63,     1] loss: 1312.162
[64,     1] loss: 1310.411
[65,     1] loss: 1304.895
[66,     1] loss: 1309.123
[67,     1] loss: 1306.088
[68,     1] loss: 1305.663
[69,     1] loss: 1302.024
[70,     1] loss: 1303.105
[71,     1] loss: 1301.688
[72,     1] loss: 1296.058
[73,     1] loss: 1294.240
[74,     1] loss: 1292.941
[75,     1] loss: 1292.883
[76,     1] loss: 1285.722
[77,     1] loss: 1284.645
[78,     1] loss: 1281.232
[79,     1] loss: 1277.219
[80,     1] loss: 1268.595
[81,     1] loss: 1263.950
[82,     1] loss: 1267.362
[83,     1] loss: 1261.576
[84,     1] loss: 1257.911
[85,     1] loss: 1247.848
[86,     1] loss: 1246.799
[87,     1] loss: 1237.354
[88,     1] loss: 1235.887
[89,     1] loss: 1229.622
[90,     1] loss: 1216.230
[91,     1] loss: 1213.922
[92,     1] loss: 1214.258
[93,     1] loss: 1210.295
[94,     1] loss: 1193.421
[95,     1] loss: 1195.450
[96,     1] loss: 1193.459
[97,     1] loss: 1185.141
[98,     1] loss: 1157.580
[99,     1] loss: 1168.656
[100,     1] loss: 1173.345
[101,     1] loss: 1156.803
[102,     1] loss: 1174.473
[103,     1] loss: 1167.670
[104,     1] loss: 1178.127
[105,     1] loss: 1134.673
[106,     1] loss: 1152.618
[107,     1] loss: 1146.408
[108,     1] loss: 1141.788
[109,     1] loss: 1117.261
[110,     1] loss: 1111.505
[111,     1] loss: 1106.722
[112,     1] loss: 1138.365
[113,     1] loss: 1123.536
[114,     1] loss: 1126.396
[115,     1] loss: 1113.233
[116,     1] loss: 1127.807
[117,     1] loss: 1108.458
[118,     1] loss: 1114.049
[119,     1] loss: 1126.728
[120,     1] loss: 1080.935
[121,     1] loss: 1103.274
[122,     1] loss: 1121.303
[123,     1] loss: 1090.546
[124,     1] loss: 1098.082
[125,     1] loss: 1121.120
[126,     1] loss: 1103.654
[127,     1] loss: 1089.735
[128,     1] loss: 1082.140
[129,     1] loss: 1104.866
[130,     1] loss: 1105.531
[131,     1] loss: 1098.140
[132,     1] loss: 1064.433
[133,     1] loss: 1127.042
[134,     1] loss: 1063.961
[135,     1] loss: 1093.701
[136,     1] loss: 1066.327
[137,     1] loss: 1115.727
[138,     1] loss: 1108.408
[139,     1] loss: 1074.021
[140,     1] loss: 1045.399
[141,     1] loss: 1081.015
[142,     1] loss: 1070.397
[143,     1] loss: 1118.024
[144,     1] loss: 1081.166
[145,     1] loss: 1049.971
[146,     1] loss: 1039.406
[147,     1] loss: 1075.558
[148,     1] loss: 1061.239
[149,     1] loss: 1036.374
[150,     1] loss: 1066.472
[151,     1] loss: 1063.786
[152,     1] loss: 1063.705
[153,     1] loss: 1048.308
[154,     1] loss: 1066.217
[155,     1] loss: 1088.366
[156,     1] loss: 1096.380
[157,     1] loss: 1046.816
[158,     1] loss: 1028.327
[159,     1] loss: 1045.079
[160,     1] loss: 1049.587
[161,     1] loss: 1065.883
[162,     1] loss: 1050.862
[163,     1] loss: 1013.938
[164,     1] loss: 1021.948
[165,     1] loss: 1064.488
[166,     1] loss: 1013.892
[167,     1] loss: 1042.910
[168,     1] loss: 1048.403
[169,     1] loss: 990.862
[170,     1] loss: 1062.932
[171,     1] loss: 1011.238
[172,     1] loss: 1001.274
[173,     1] loss: 1041.087
[174,     1] loss: 995.274
[175,     1] loss: 966.887
[176,     1] loss: 1026.718
[177,     1] loss: 1059.505
[178,     1] loss: 1009.163
[179,     1] loss: 971.770
[180,     1] loss: 1011.949
[181,     1] loss: 1022.897
[182,     1] loss: 989.280
[183,     1] loss: 976.518
[184,     1] loss: 1060.836
[185,     1] loss: 987.802
[186,     1] loss: 986.668
[187,     1] loss: 991.223
[188,     1] loss: 960.130
[189,     1] loss: 966.462
[190,     1] loss: 957.315
[191,     1] loss: 963.001
[192,     1] loss: 987.268
[193,     1] loss: 940.847
[194,     1] loss: 943.495
[195,     1] loss: 953.830
[196,     1] loss: 952.259
[197,     1] loss: 932.026
[198,     1] loss: 916.002
[199,     1] loss: 906.556
[200,     1] loss: 986.084
Finished Training
Total time taken: 33.43578505516052
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.559
[2,     1] loss: 1325.731
[3,     1] loss: 1321.161
[4,     1] loss: 1322.107
[5,     1] loss: 1318.533
[6,     1] loss: 1325.318
[7,     1] loss: 1320.260
[8,     1] loss: 1319.591
[9,     1] loss: 1315.242
[10,     1] loss: 1320.100
[11,     1] loss: 1318.875
[12,     1] loss: 1320.716
[13,     1] loss: 1320.778
[14,     1] loss: 1320.502
[15,     1] loss: 1317.748
[16,     1] loss: 1319.867
[17,     1] loss: 1321.596
[18,     1] loss: 1320.734
[19,     1] loss: 1319.909
[20,     1] loss: 1319.207
[21,     1] loss: 1317.234
[22,     1] loss: 1320.043
[23,     1] loss: 1319.938
[24,     1] loss: 1317.258
[25,     1] loss: 1319.344
[26,     1] loss: 1317.464
[27,     1] loss: 1318.709
[28,     1] loss: 1319.647
[29,     1] loss: 1319.508
[30,     1] loss: 1319.231
[31,     1] loss: 1317.490
[32,     1] loss: 1317.976
[33,     1] loss: 1318.985
[34,     1] loss: 1318.907
[35,     1] loss: 1319.444
[36,     1] loss: 1318.203
[37,     1] loss: 1318.531
[38,     1] loss: 1318.696
[39,     1] loss: 1319.977
[40,     1] loss: 1315.774
[41,     1] loss: 1315.016
[42,     1] loss: 1316.181
[43,     1] loss: 1316.915
[44,     1] loss: 1315.348
[45,     1] loss: 1312.527
[46,     1] loss: 1315.939
[47,     1] loss: 1315.224
[48,     1] loss: 1316.605
[49,     1] loss: 1312.528
[50,     1] loss: 1310.526
[51,     1] loss: 1310.019
[52,     1] loss: 1305.356
[53,     1] loss: 1311.133
[54,     1] loss: 1307.724
[55,     1] loss: 1305.714
[56,     1] loss: 1305.232
[57,     1] loss: 1300.977
[58,     1] loss: 1300.494
[59,     1] loss: 1297.707
[60,     1] loss: 1296.705
[61,     1] loss: 1293.511
[62,     1] loss: 1289.590
[63,     1] loss: 1282.183
[64,     1] loss: 1285.599
[65,     1] loss: 1275.552
[66,     1] loss: 1269.920
[67,     1] loss: 1262.491
[68,     1] loss: 1252.023
[69,     1] loss: 1254.716
[70,     1] loss: 1249.424
[71,     1] loss: 1234.718
[72,     1] loss: 1225.104
[73,     1] loss: 1221.094
[74,     1] loss: 1225.205
[75,     1] loss: 1200.859
[76,     1] loss: 1214.349
[77,     1] loss: 1184.582
[78,     1] loss: 1197.789
[79,     1] loss: 1196.913
[80,     1] loss: 1174.625
[81,     1] loss: 1187.180
[82,     1] loss: 1167.501
[83,     1] loss: 1175.407
[84,     1] loss: 1170.271
[85,     1] loss: 1149.506
[86,     1] loss: 1132.836
[87,     1] loss: 1114.932
[88,     1] loss: 1131.249
[89,     1] loss: 1112.914
[90,     1] loss: 1126.938
[91,     1] loss: 1124.342
[92,     1] loss: 1118.463
[93,     1] loss: 1107.007
[94,     1] loss: 1128.348
[95,     1] loss: 1125.182
[96,     1] loss: 1085.557
[97,     1] loss: 1091.495
[98,     1] loss: 1100.788
[99,     1] loss: 1100.536
[100,     1] loss: 1107.933
[101,     1] loss: 1094.424
[102,     1] loss: 1099.639
[103,     1] loss: 1105.356
[104,     1] loss: 1075.245
[105,     1] loss: 1096.576
[106,     1] loss: 1055.201
[107,     1] loss: 1092.668
[108,     1] loss: 1085.232
[109,     1] loss: 1081.085
[110,     1] loss: 1085.856
[111,     1] loss: 1081.520
[112,     1] loss: 1102.972
[113,     1] loss: 1063.327
[114,     1] loss: 1104.396
[115,     1] loss: 1041.775
[116,     1] loss: 1071.546
[117,     1] loss: 1054.041
[118,     1] loss: 1054.370
[119,     1] loss: 1090.764
[120,     1] loss: 1089.924
[121,     1] loss: 1067.571
[122,     1] loss: 1040.390
[123,     1] loss: 1079.063
[124,     1] loss: 1057.310
[125,     1] loss: 1019.320
[126,     1] loss: 1051.670
[127,     1] loss: 1069.418
[128,     1] loss: 1059.157
[129,     1] loss: 1050.945
[130,     1] loss: 1062.121
[131,     1] loss: 1031.382
[132,     1] loss: 1048.139
[133,     1] loss: 1054.319
[134,     1] loss: 1016.203
[135,     1] loss: 1053.008
[136,     1] loss: 1039.814
[137,     1] loss: 1070.279
[138,     1] loss: 1062.491
[139,     1] loss: 1061.351
[140,     1] loss: 1033.224
[141,     1] loss: 1035.033
[142,     1] loss: 1029.816
[143,     1] loss: 1024.133
[144,     1] loss: 1071.702
[145,     1] loss: 1039.665
[146,     1] loss: 1019.215
[147,     1] loss: 990.110
[148,     1] loss: 1030.400
[149,     1] loss: 989.223
[150,     1] loss: 989.410
[151,     1] loss: 1018.519
[152,     1] loss: 994.616
[153,     1] loss: 1010.229
[154,     1] loss: 981.337
[155,     1] loss: 1011.440
[156,     1] loss: 996.297
[157,     1] loss: 994.463
[158,     1] loss: 1027.358
[159,     1] loss: 960.972
[160,     1] loss: 1009.012
[161,     1] loss: 985.145
[162,     1] loss: 988.029
[163,     1] loss: 1001.686
[164,     1] loss: 981.686
[165,     1] loss: 970.955
[166,     1] loss: 995.659
[167,     1] loss: 954.987
[168,     1] loss: 1002.151
[169,     1] loss: 987.665
[170,     1] loss: 975.432
[171,     1] loss: 993.128
[172,     1] loss: 988.675
[173,     1] loss: 958.045
[174,     1] loss: 963.807
[175,     1] loss: 957.938
[176,     1] loss: 916.395
[177,     1] loss: 945.095
[178,     1] loss: 965.652
[179,     1] loss: 950.823
[180,     1] loss: 992.243
[181,     1] loss: 1000.008
[182,     1] loss: 960.039
[183,     1] loss: 978.060
[184,     1] loss: 976.529
[185,     1] loss: 921.244
[186,     1] loss: 951.761
[187,     1] loss: 962.390
[188,     1] loss: 920.862
[189,     1] loss: 905.201
[190,     1] loss: 934.744
[191,     1] loss: 959.906
[192,     1] loss: 962.565
[193,     1] loss: 867.328
[194,     1] loss: 908.537
[195,     1] loss: 954.864
[196,     1] loss: 889.796
[197,     1] loss: 945.152
[198,     1] loss: 907.653
[199,     1] loss: 969.467
[200,     1] loss: 903.687
Finished Training
Total time taken: 26.817602157592773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1323.415
[2,     1] loss: 1322.604
[3,     1] loss: 1321.580
[4,     1] loss: 1321.341
[5,     1] loss: 1320.297
[6,     1] loss: 1322.978
[7,     1] loss: 1319.791
[8,     1] loss: 1320.772
[9,     1] loss: 1322.524
[10,     1] loss: 1324.976
[11,     1] loss: 1322.112
[12,     1] loss: 1319.862
[13,     1] loss: 1321.484
[14,     1] loss: 1324.684
[15,     1] loss: 1322.639
[16,     1] loss: 1323.867
[17,     1] loss: 1322.208
[18,     1] loss: 1319.272
[19,     1] loss: 1319.026
[20,     1] loss: 1322.465
[21,     1] loss: 1319.061
[22,     1] loss: 1321.040
[23,     1] loss: 1320.078
[24,     1] loss: 1319.358
[25,     1] loss: 1318.645
[26,     1] loss: 1319.612
[27,     1] loss: 1319.767
[28,     1] loss: 1318.589
[29,     1] loss: 1318.903
[30,     1] loss: 1317.326
[31,     1] loss: 1319.013
[32,     1] loss: 1318.952
[33,     1] loss: 1317.592
[34,     1] loss: 1317.046
[35,     1] loss: 1316.350
[36,     1] loss: 1315.097
[37,     1] loss: 1313.369
[38,     1] loss: 1313.997
[39,     1] loss: 1315.313
[40,     1] loss: 1313.065
[41,     1] loss: 1315.160
[42,     1] loss: 1308.918
[43,     1] loss: 1304.734
[44,     1] loss: 1307.038
[45,     1] loss: 1306.393
[46,     1] loss: 1304.127
[47,     1] loss: 1301.409
[48,     1] loss: 1299.813
[49,     1] loss: 1295.893
[50,     1] loss: 1294.944
[51,     1] loss: 1291.099
[52,     1] loss: 1286.129
[53,     1] loss: 1284.903
[54,     1] loss: 1283.931
[55,     1] loss: 1270.827
[56,     1] loss: 1270.861
[57,     1] loss: 1271.749
[58,     1] loss: 1266.008
[59,     1] loss: 1263.269
[60,     1] loss: 1259.557
[61,     1] loss: 1252.002
[62,     1] loss: 1248.789
[63,     1] loss: 1234.422
[64,     1] loss: 1230.516
[65,     1] loss: 1228.274
[66,     1] loss: 1209.369
[67,     1] loss: 1195.501
[68,     1] loss: 1212.523
[69,     1] loss: 1205.739
[70,     1] loss: 1204.000
[71,     1] loss: 1185.804
[72,     1] loss: 1186.781
[73,     1] loss: 1188.726
[74,     1] loss: 1193.021
[75,     1] loss: 1168.436
[76,     1] loss: 1160.056
[77,     1] loss: 1149.083
[78,     1] loss: 1161.817
[79,     1] loss: 1143.521
[80,     1] loss: 1150.992
[81,     1] loss: 1151.194
[82,     1] loss: 1163.334
[83,     1] loss: 1143.845
[84,     1] loss: 1092.776
[85,     1] loss: 1132.516
[86,     1] loss: 1098.730
[87,     1] loss: 1112.142
[88,     1] loss: 1065.544
[89,     1] loss: 1140.019
[90,     1] loss: 1111.618
[91,     1] loss: 1141.061
[92,     1] loss: 1108.229
[93,     1] loss: 1118.942
[94,     1] loss: 1160.462
[95,     1] loss: 1127.487
[96,     1] loss: 1103.053
[97,     1] loss: 1067.211
[98,     1] loss: 1080.004
[99,     1] loss: 1104.316
[100,     1] loss: 1137.560
[101,     1] loss: 1106.125
[102,     1] loss: 1066.364
[103,     1] loss: 1076.183
[104,     1] loss: 1106.386
[105,     1] loss: 1059.247
[106,     1] loss: 1094.893
[107,     1] loss: 1109.590
[108,     1] loss: 1119.614
[109,     1] loss: 1119.174
[110,     1] loss: 1060.462
[111,     1] loss: 1096.297
[112,     1] loss: 1099.391
[113,     1] loss: 1083.262
[114,     1] loss: 1074.665
[115,     1] loss: 1091.813
[116,     1] loss: 1074.655
[117,     1] loss: 1114.556
[118,     1] loss: 1103.409
[119,     1] loss: 1084.566
[120,     1] loss: 1089.078
[121,     1] loss: 1086.940
[122,     1] loss: 1040.724
[123,     1] loss: 1057.201
[124,     1] loss: 1085.388
[125,     1] loss: 1108.284
[126,     1] loss: 1061.670
[127,     1] loss: 1038.362
[128,     1] loss: 1074.391
[129,     1] loss: 1034.609
[130,     1] loss: 1096.624
[131,     1] loss: 1054.228
[132,     1] loss: 1075.114
[133,     1] loss: 1035.726
[134,     1] loss: 1099.740
[135,     1] loss: 1070.056
[136,     1] loss: 1052.344
[137,     1] loss: 1048.031
[138,     1] loss: 1086.404
[139,     1] loss: 1041.317
[140,     1] loss: 1014.458
[141,     1] loss: 1024.843
[142,     1] loss: 1052.521
[143,     1] loss: 1021.996
[144,     1] loss: 1068.002
[145,     1] loss: 1047.149
[146,     1] loss: 1046.656
[147,     1] loss: 1053.009
[148,     1] loss: 1049.582
[149,     1] loss: 1039.637
[150,     1] loss: 1030.050
[151,     1] loss: 1029.337
[152,     1] loss: 1070.971
[153,     1] loss: 1042.884
[154,     1] loss: 1027.833
[155,     1] loss: 1046.064
[156,     1] loss: 1064.878
[157,     1] loss: 1070.229
[158,     1] loss: 1062.296
[159,     1] loss: 1056.527
[160,     1] loss: 1063.948
[161,     1] loss: 1026.929
[162,     1] loss: 1039.486
[163,     1] loss: 1049.537
[164,     1] loss: 1010.025
[165,     1] loss: 1022.911
[166,     1] loss: 1013.337
[167,     1] loss: 1010.168
[168,     1] loss: 1023.837
[169,     1] loss: 1029.248
[170,     1] loss: 1006.337
[171,     1] loss: 960.899
[172,     1] loss: 990.445
[173,     1] loss: 988.842
[174,     1] loss: 1017.546
[175,     1] loss: 958.779
[176,     1] loss: 972.635
[177,     1] loss: 967.792
[178,     1] loss: 972.854
[179,     1] loss: 975.293
[180,     1] loss: 987.747
[181,     1] loss: 998.406
[182,     1] loss: 960.672
[183,     1] loss: 953.740
[184,     1] loss: 957.637
[185,     1] loss: 953.449
[186,     1] loss: 980.172
[187,     1] loss: 989.850
[188,     1] loss: 941.450
[189,     1] loss: 927.425
[190,     1] loss: 952.655
[191,     1] loss: 989.445
[192,     1] loss: 942.157
[193,     1] loss: 928.201
[194,     1] loss: 924.829
[195,     1] loss: 953.450
[196,     1] loss: 939.914
[197,     1] loss: 959.994
[198,     1] loss: 914.739
[199,     1] loss: 953.207
[200,     1] loss: 992.811
Finished Training
Total time taken: 29.201430320739746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1319.153
[2,     1] loss: 1326.976
[3,     1] loss: 1322.028
[4,     1] loss: 1319.250
[5,     1] loss: 1321.130
[6,     1] loss: 1321.381
[7,     1] loss: 1323.757
[8,     1] loss: 1324.325
[9,     1] loss: 1322.006
[10,     1] loss: 1323.064
[11,     1] loss: 1320.402
[12,     1] loss: 1317.857
[13,     1] loss: 1320.930
[14,     1] loss: 1319.309
[15,     1] loss: 1321.300
[16,     1] loss: 1323.418
[17,     1] loss: 1319.630
[18,     1] loss: 1319.601
[19,     1] loss: 1320.837
[20,     1] loss: 1319.354
[21,     1] loss: 1320.122
[22,     1] loss: 1318.528
[23,     1] loss: 1318.578
[24,     1] loss: 1317.895
[25,     1] loss: 1318.203
[26,     1] loss: 1318.263
[27,     1] loss: 1318.635
[28,     1] loss: 1320.369
[29,     1] loss: 1317.176
[30,     1] loss: 1317.759
[31,     1] loss: 1316.406
[32,     1] loss: 1315.769
[33,     1] loss: 1317.704
[34,     1] loss: 1316.492
[35,     1] loss: 1315.727
[36,     1] loss: 1316.371
[37,     1] loss: 1316.999
[38,     1] loss: 1314.038
[39,     1] loss: 1311.859
[40,     1] loss: 1314.217
[41,     1] loss: 1310.332
[42,     1] loss: 1311.557
[43,     1] loss: 1308.691
[44,     1] loss: 1308.452
[45,     1] loss: 1304.358
[46,     1] loss: 1307.930
[47,     1] loss: 1306.488
[48,     1] loss: 1299.725
[49,     1] loss: 1296.461
[50,     1] loss: 1293.821
[51,     1] loss: 1294.525
[52,     1] loss: 1292.812
[53,     1] loss: 1287.859
[54,     1] loss: 1282.785
[55,     1] loss: 1280.085
[56,     1] loss: 1276.365
[57,     1] loss: 1271.077
[58,     1] loss: 1268.805
[59,     1] loss: 1265.103
[60,     1] loss: 1267.020
[61,     1] loss: 1259.655
[62,     1] loss: 1249.985
[63,     1] loss: 1248.224
[64,     1] loss: 1243.358
[65,     1] loss: 1232.963
[66,     1] loss: 1236.721
[67,     1] loss: 1225.876
[68,     1] loss: 1218.584
[69,     1] loss: 1214.692
[70,     1] loss: 1207.542
[71,     1] loss: 1196.307
[72,     1] loss: 1191.708
[73,     1] loss: 1180.760
[74,     1] loss: 1182.058
[75,     1] loss: 1196.065
[76,     1] loss: 1187.124
[77,     1] loss: 1182.627
[78,     1] loss: 1152.948
[79,     1] loss: 1175.080
[80,     1] loss: 1148.720
[81,     1] loss: 1169.607
[82,     1] loss: 1119.177
[83,     1] loss: 1136.012
[84,     1] loss: 1126.993
[85,     1] loss: 1141.482
[86,     1] loss: 1146.697
[87,     1] loss: 1121.171
[88,     1] loss: 1102.781
[89,     1] loss: 1138.053
[90,     1] loss: 1136.850
[91,     1] loss: 1108.290
[92,     1] loss: 1144.236
[93,     1] loss: 1096.161
[94,     1] loss: 1083.313
[95,     1] loss: 1092.562
[96,     1] loss: 1097.055
[97,     1] loss: 1084.574
[98,     1] loss: 1063.572
[99,     1] loss: 1114.169
[100,     1] loss: 1112.011
[101,     1] loss: 1135.142
[102,     1] loss: 1100.590
[103,     1] loss: 1103.088
[104,     1] loss: 1059.582
[105,     1] loss: 1079.653
[106,     1] loss: 1096.825
[107,     1] loss: 1107.990
[108,     1] loss: 1093.589
[109,     1] loss: 1088.359
[110,     1] loss: 1071.487
[111,     1] loss: 1096.978
[112,     1] loss: 1121.255
[113,     1] loss: 1087.501
[114,     1] loss: 1077.589
[115,     1] loss: 1067.893
[116,     1] loss: 1081.098
[117,     1] loss: 1082.373
[118,     1] loss: 1039.023
[119,     1] loss: 1088.232
[120,     1] loss: 1047.505
[121,     1] loss: 1043.856
[122,     1] loss: 1088.625
[123,     1] loss: 1049.200
[124,     1] loss: 1043.264
[125,     1] loss: 1081.912
[126,     1] loss: 1063.068
[127,     1] loss: 1033.654
[128,     1] loss: 1049.525
[129,     1] loss: 1042.777
[130,     1] loss: 1068.080
[131,     1] loss: 1024.598
[132,     1] loss: 1024.555
[133,     1] loss: 1023.924
[134,     1] loss: 1021.240
[135,     1] loss: 1072.747
[136,     1] loss: 1057.483
[137,     1] loss: 1061.370
[138,     1] loss: 1026.250
[139,     1] loss: 1023.597
[140,     1] loss: 998.906
[141,     1] loss: 1027.967
[142,     1] loss: 1005.589
[143,     1] loss: 1046.428
[144,     1] loss: 1007.629
[145,     1] loss: 1032.267
[146,     1] loss: 986.504
[147,     1] loss: 1064.678
[148,     1] loss: 1001.276
[149,     1] loss: 1004.685
[150,     1] loss: 995.124
[151,     1] loss: 977.112
[152,     1] loss: 982.963
[153,     1] loss: 1004.521
[154,     1] loss: 1010.154
[155,     1] loss: 997.539
[156,     1] loss: 977.653
[157,     1] loss: 986.270
[158,     1] loss: 987.353
[159,     1] loss: 1032.338
[160,     1] loss: 993.058
[161,     1] loss: 952.815
[162,     1] loss: 949.636
[163,     1] loss: 982.547
[164,     1] loss: 988.770
[165,     1] loss: 979.598
[166,     1] loss: 991.911
[167,     1] loss: 987.361
[168,     1] loss: 997.710
[169,     1] loss: 967.886
[170,     1] loss: 942.690
[171,     1] loss: 925.124
[172,     1] loss: 930.434
[173,     1] loss: 953.561
[174,     1] loss: 969.933
[175,     1] loss: 968.470
[176,     1] loss: 945.637
[177,     1] loss: 903.170
[178,     1] loss: 962.908
[179,     1] loss: 943.560
[180,     1] loss: 972.834
[181,     1] loss: 925.963
[182,     1] loss: 942.118
[183,     1] loss: 878.697
[184,     1] loss: 959.351
[185,     1] loss: 932.847
[186,     1] loss: 870.564
[187,     1] loss: 902.903
[188,     1] loss: 914.745
[189,     1] loss: 882.241
[190,     1] loss: 911.242
[191,     1] loss: 917.651
[192,     1] loss: 935.463
[193,     1] loss: 924.686
[194,     1] loss: 892.698
[195,     1] loss: 893.930
[196,     1] loss: 913.984
[197,     1] loss: 855.424
[198,     1] loss: 908.679
[199,     1] loss: 906.419
[200,     1] loss: 883.699
Finished Training
Total time taken: 31.35878324508667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.640
[2,     1] loss: 1320.440
[3,     1] loss: 1325.182
[4,     1] loss: 1322.874
[5,     1] loss: 1323.191
[6,     1] loss: 1324.586
[7,     1] loss: 1325.930
[8,     1] loss: 1325.284
[9,     1] loss: 1318.894
[10,     1] loss: 1321.595
[11,     1] loss: 1323.564
[12,     1] loss: 1321.638
[13,     1] loss: 1322.073
[14,     1] loss: 1322.654
[15,     1] loss: 1325.479
[16,     1] loss: 1321.029
[17,     1] loss: 1323.071
[18,     1] loss: 1323.377
[19,     1] loss: 1319.967
[20,     1] loss: 1319.545
[21,     1] loss: 1321.617
[22,     1] loss: 1322.681
[23,     1] loss: 1319.471
[24,     1] loss: 1318.493
[25,     1] loss: 1320.032
[26,     1] loss: 1321.468
[27,     1] loss: 1316.732
[28,     1] loss: 1316.752
[29,     1] loss: 1320.264
[30,     1] loss: 1319.496
[31,     1] loss: 1319.561
[32,     1] loss: 1316.854
[33,     1] loss: 1316.647
[34,     1] loss: 1314.597
[35,     1] loss: 1318.496
[36,     1] loss: 1319.741
[37,     1] loss: 1317.138
[38,     1] loss: 1314.930
[39,     1] loss: 1314.628
[40,     1] loss: 1313.250
[41,     1] loss: 1312.503
[42,     1] loss: 1310.308
[43,     1] loss: 1316.452
[44,     1] loss: 1311.926
[45,     1] loss: 1310.619
[46,     1] loss: 1308.900
[47,     1] loss: 1308.649
[48,     1] loss: 1309.752
[49,     1] loss: 1305.775
[50,     1] loss: 1309.102
[51,     1] loss: 1307.398
[52,     1] loss: 1304.589
[53,     1] loss: 1298.408
[54,     1] loss: 1295.419
[55,     1] loss: 1295.396
[56,     1] loss: 1295.865
[57,     1] loss: 1291.394
[58,     1] loss: 1287.730
[59,     1] loss: 1285.384
[60,     1] loss: 1282.419
[61,     1] loss: 1279.026
[62,     1] loss: 1277.448
[63,     1] loss: 1275.998
[64,     1] loss: 1272.930
[65,     1] loss: 1260.336
[66,     1] loss: 1262.970
[67,     1] loss: 1251.954
[68,     1] loss: 1258.404
[69,     1] loss: 1253.869
[70,     1] loss: 1240.915
[71,     1] loss: 1243.091
[72,     1] loss: 1234.219
[73,     1] loss: 1242.554
[74,     1] loss: 1235.774
[75,     1] loss: 1228.249
[76,     1] loss: 1221.868
[77,     1] loss: 1227.404
[78,     1] loss: 1223.170
[79,     1] loss: 1222.598
[80,     1] loss: 1201.742
[81,     1] loss: 1220.756
[82,     1] loss: 1192.020
[83,     1] loss: 1200.637
[84,     1] loss: 1182.825
[85,     1] loss: 1193.249
[86,     1] loss: 1200.412
[87,     1] loss: 1201.519
[88,     1] loss: 1187.037
[89,     1] loss: 1179.109
[90,     1] loss: 1185.202
[91,     1] loss: 1165.260
[92,     1] loss: 1151.558
[93,     1] loss: 1129.772
[94,     1] loss: 1166.581
[95,     1] loss: 1166.382
[96,     1] loss: 1137.394
[97,     1] loss: 1153.823
[98,     1] loss: 1131.919
[99,     1] loss: 1136.182
[100,     1] loss: 1136.832
[101,     1] loss: 1133.781
[102,     1] loss: 1159.204
[103,     1] loss: 1135.947
[104,     1] loss: 1099.695
[105,     1] loss: 1153.912
[106,     1] loss: 1143.912
[107,     1] loss: 1135.257
[108,     1] loss: 1127.804
[109,     1] loss: 1120.620
[110,     1] loss: 1115.911
[111,     1] loss: 1123.581
[112,     1] loss: 1110.738
[113,     1] loss: 1100.798
[114,     1] loss: 1086.448
[115,     1] loss: 1098.614
[116,     1] loss: 1075.763
[117,     1] loss: 1113.192
[118,     1] loss: 1095.453
[119,     1] loss: 1099.153
[120,     1] loss: 1097.253
[121,     1] loss: 1116.264
[122,     1] loss: 1091.988
[123,     1] loss: 1096.398
[124,     1] loss: 1092.057
[125,     1] loss: 1084.287
[126,     1] loss: 1055.040
[127,     1] loss: 1074.434
[128,     1] loss: 1081.028
[129,     1] loss: 1037.100
[130,     1] loss: 1063.738
[131,     1] loss: 1102.233
[132,     1] loss: 1089.999
[133,     1] loss: 1074.379
[134,     1] loss: 1087.693
[135,     1] loss: 1062.006
[136,     1] loss: 1078.689
[137,     1] loss: 1026.098
[138,     1] loss: 1036.885
[139,     1] loss: 1037.127
[140,     1] loss: 1048.483
[141,     1] loss: 1063.310
[142,     1] loss: 1033.820
[143,     1] loss: 1062.659
[144,     1] loss: 1012.769
[145,     1] loss: 1059.252
[146,     1] loss: 1020.394
[147,     1] loss: 1025.254
[148,     1] loss: 1028.609
[149,     1] loss: 1017.255
[150,     1] loss: 1030.667
[151,     1] loss: 1024.464
[152,     1] loss: 1044.479
[153,     1] loss: 1007.579
[154,     1] loss: 1016.539
[155,     1] loss: 1003.807
[156,     1] loss: 1010.001
[157,     1] loss: 948.065
[158,     1] loss: 1029.370
[159,     1] loss: 1032.272
[160,     1] loss: 980.975
[161,     1] loss: 1025.116
[162,     1] loss: 978.581
[163,     1] loss: 996.346
[164,     1] loss: 953.569
[165,     1] loss: 981.010
[166,     1] loss: 988.066
[167,     1] loss: 965.310
[168,     1] loss: 987.563
[169,     1] loss: 1012.272
[170,     1] loss: 955.255
[171,     1] loss: 990.745
[172,     1] loss: 977.438
[173,     1] loss: 995.481
[174,     1] loss: 978.036
[175,     1] loss: 994.396
[176,     1] loss: 1019.601
[177,     1] loss: 976.967
[178,     1] loss: 956.300
[179,     1] loss: 963.297
Early stopping applied (best metric=0.7834174036979675)
Finished Training
Total time taken: 27.13962984085083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.493
[2,     1] loss: 1324.612
[3,     1] loss: 1323.781
[4,     1] loss: 1320.436
[5,     1] loss: 1323.771
[6,     1] loss: 1322.038
[7,     1] loss: 1326.999
[8,     1] loss: 1326.276
[9,     1] loss: 1318.825
[10,     1] loss: 1322.553
[11,     1] loss: 1322.602
[12,     1] loss: 1320.976
[13,     1] loss: 1319.864
[14,     1] loss: 1320.260
[15,     1] loss: 1319.583
[16,     1] loss: 1324.445
[17,     1] loss: 1318.868
[18,     1] loss: 1320.153
[19,     1] loss: 1322.302
[20,     1] loss: 1321.525
[21,     1] loss: 1320.189
[22,     1] loss: 1318.965
[23,     1] loss: 1317.483
[24,     1] loss: 1316.109
[25,     1] loss: 1316.975
[26,     1] loss: 1321.538
[27,     1] loss: 1317.693
[28,     1] loss: 1317.015
[29,     1] loss: 1318.750
[30,     1] loss: 1315.078
[31,     1] loss: 1314.546
[32,     1] loss: 1317.154
[33,     1] loss: 1315.240
[34,     1] loss: 1316.429
[35,     1] loss: 1315.525
[36,     1] loss: 1313.968
[37,     1] loss: 1312.454
[38,     1] loss: 1311.597
[39,     1] loss: 1313.477
[40,     1] loss: 1315.176
[41,     1] loss: 1310.206
[42,     1] loss: 1311.819
[43,     1] loss: 1310.058
[44,     1] loss: 1304.847
[45,     1] loss: 1306.025
[46,     1] loss: 1303.880
[47,     1] loss: 1302.065
[48,     1] loss: 1296.515
[49,     1] loss: 1299.000
[50,     1] loss: 1292.544
[51,     1] loss: 1288.079
[52,     1] loss: 1286.391
[53,     1] loss: 1283.196
[54,     1] loss: 1272.601
[55,     1] loss: 1270.268
[56,     1] loss: 1265.915
[57,     1] loss: 1262.098
[58,     1] loss: 1250.993
[59,     1] loss: 1243.720
[60,     1] loss: 1237.118
[61,     1] loss: 1229.407
[62,     1] loss: 1220.989
[63,     1] loss: 1216.520
[64,     1] loss: 1205.953
[65,     1] loss: 1205.662
[66,     1] loss: 1196.338
[67,     1] loss: 1190.968
[68,     1] loss: 1183.617
[69,     1] loss: 1176.818
[70,     1] loss: 1162.397
[71,     1] loss: 1163.121
[72,     1] loss: 1150.029
[73,     1] loss: 1138.692
[74,     1] loss: 1152.842
[75,     1] loss: 1139.349
[76,     1] loss: 1136.516
[77,     1] loss: 1116.744
[78,     1] loss: 1125.181
[79,     1] loss: 1110.083
[80,     1] loss: 1104.731
[81,     1] loss: 1093.678
[82,     1] loss: 1071.604
[83,     1] loss: 1092.593
[84,     1] loss: 1104.444
[85,     1] loss: 1098.327
[86,     1] loss: 1070.296
[87,     1] loss: 1059.585
[88,     1] loss: 1075.221
[89,     1] loss: 1055.936
[90,     1] loss: 1060.250
[91,     1] loss: 1066.524
[92,     1] loss: 1051.931
[93,     1] loss: 1086.629
[94,     1] loss: 1075.355
[95,     1] loss: 1029.346
[96,     1] loss: 1050.798
[97,     1] loss: 1051.077
[98,     1] loss: 1032.159
[99,     1] loss: 1029.371
[100,     1] loss: 1033.529
[101,     1] loss: 1022.380
[102,     1] loss: 1073.053
[103,     1] loss: 1012.177
[104,     1] loss: 1010.034
[105,     1] loss: 1060.252
[106,     1] loss: 1029.992
[107,     1] loss: 1011.954
[108,     1] loss: 1015.903
[109,     1] loss: 1040.631
[110,     1] loss: 1024.944
[111,     1] loss: 1044.176
[112,     1] loss: 1064.104
[113,     1] loss: 1002.932
[114,     1] loss: 1037.620
[115,     1] loss: 1022.578
[116,     1] loss: 1037.429
[117,     1] loss: 1009.249
[118,     1] loss: 1045.952
[119,     1] loss: 984.130
[120,     1] loss: 1015.119
[121,     1] loss: 971.954
[122,     1] loss: 997.188
[123,     1] loss: 974.779
[124,     1] loss: 986.640
[125,     1] loss: 1016.319
[126,     1] loss: 1015.862
[127,     1] loss: 990.977
[128,     1] loss: 987.420
[129,     1] loss: 999.201
[130,     1] loss: 1023.961
[131,     1] loss: 995.926
[132,     1] loss: 961.580
[133,     1] loss: 985.055
[134,     1] loss: 1022.025
[135,     1] loss: 979.827
[136,     1] loss: 987.144
[137,     1] loss: 957.154
[138,     1] loss: 992.979
[139,     1] loss: 965.293
[140,     1] loss: 1001.878
[141,     1] loss: 999.209
[142,     1] loss: 981.505
[143,     1] loss: 957.872
[144,     1] loss: 978.549
[145,     1] loss: 961.440
[146,     1] loss: 958.770
[147,     1] loss: 970.601
[148,     1] loss: 952.095
[149,     1] loss: 1018.702
[150,     1] loss: 957.235
[151,     1] loss: 952.351
[152,     1] loss: 963.017
[153,     1] loss: 899.919
[154,     1] loss: 938.988
[155,     1] loss: 931.256
[156,     1] loss: 936.893
[157,     1] loss: 962.692
[158,     1] loss: 947.828
[159,     1] loss: 982.235
[160,     1] loss: 934.294
[161,     1] loss: 940.043
[162,     1] loss: 982.375
[163,     1] loss: 932.194
[164,     1] loss: 953.948
[165,     1] loss: 988.071
[166,     1] loss: 901.229
[167,     1] loss: 923.508
[168,     1] loss: 895.985
[169,     1] loss: 909.336
[170,     1] loss: 961.378
[171,     1] loss: 915.946
[172,     1] loss: 933.336
[173,     1] loss: 926.379
[174,     1] loss: 966.136
[175,     1] loss: 894.282
[176,     1] loss: 904.897
[177,     1] loss: 930.284
[178,     1] loss: 884.057
[179,     1] loss: 931.387
[180,     1] loss: 893.799
[181,     1] loss: 923.507
[182,     1] loss: 909.426
[183,     1] loss: 933.525
[184,     1] loss: 946.816
[185,     1] loss: 930.129
[186,     1] loss: 902.044
[187,     1] loss: 879.052
[188,     1] loss: 969.021
[189,     1] loss: 901.563
[190,     1] loss: 854.639
[191,     1] loss: 918.149
[192,     1] loss: 859.733
[193,     1] loss: 906.944
[194,     1] loss: 923.014
[195,     1] loss: 892.832
[196,     1] loss: 886.210
[197,     1] loss: 877.058
[198,     1] loss: 865.524
[199,     1] loss: 900.029
[200,     1] loss: 927.427
Finished Training
Total time taken: 33.95753240585327
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.470
[2,     1] loss: 1324.319
[3,     1] loss: 1322.474
[4,     1] loss: 1324.550
[5,     1] loss: 1319.430
[6,     1] loss: 1320.863
[7,     1] loss: 1321.744
[8,     1] loss: 1320.062
[9,     1] loss: 1321.223
[10,     1] loss: 1322.066
[11,     1] loss: 1320.582
[12,     1] loss: 1317.442
[13,     1] loss: 1321.001
[14,     1] loss: 1318.823
[15,     1] loss: 1318.365
[16,     1] loss: 1320.025
[17,     1] loss: 1321.274
[18,     1] loss: 1319.717
[19,     1] loss: 1320.570
[20,     1] loss: 1317.310
[21,     1] loss: 1317.693
[22,     1] loss: 1317.750
[23,     1] loss: 1318.484
[24,     1] loss: 1320.001
[25,     1] loss: 1320.009
[26,     1] loss: 1317.184
[27,     1] loss: 1317.244
[28,     1] loss: 1318.019
[29,     1] loss: 1316.811
[30,     1] loss: 1312.800
[31,     1] loss: 1315.641
[32,     1] loss: 1319.152
[33,     1] loss: 1318.092
[34,     1] loss: 1318.517
[35,     1] loss: 1315.172
[36,     1] loss: 1315.441
[37,     1] loss: 1313.372
[38,     1] loss: 1312.578
[39,     1] loss: 1314.485
[40,     1] loss: 1313.647
[41,     1] loss: 1311.411
[42,     1] loss: 1311.598
[43,     1] loss: 1312.880
[44,     1] loss: 1308.702
[45,     1] loss: 1304.707
[46,     1] loss: 1307.647
[47,     1] loss: 1301.912
[48,     1] loss: 1301.628
[49,     1] loss: 1300.556
[50,     1] loss: 1301.522
[51,     1] loss: 1295.036
[52,     1] loss: 1293.031
[53,     1] loss: 1292.350
[54,     1] loss: 1289.928
[55,     1] loss: 1277.482
[56,     1] loss: 1280.479
[57,     1] loss: 1271.053
[58,     1] loss: 1275.320
[59,     1] loss: 1261.467
[60,     1] loss: 1261.784
[61,     1] loss: 1250.146
[62,     1] loss: 1254.030
[63,     1] loss: 1244.029
[64,     1] loss: 1240.482
[65,     1] loss: 1230.501
[66,     1] loss: 1228.275
[67,     1] loss: 1236.624
[68,     1] loss: 1206.167
[69,     1] loss: 1210.443
[70,     1] loss: 1189.779
[71,     1] loss: 1185.448
[72,     1] loss: 1182.734
[73,     1] loss: 1174.514
[74,     1] loss: 1170.087
[75,     1] loss: 1161.092
[76,     1] loss: 1147.515
[77,     1] loss: 1140.199
[78,     1] loss: 1130.698
[79,     1] loss: 1141.390
[80,     1] loss: 1133.371
[81,     1] loss: 1138.911
[82,     1] loss: 1121.282
[83,     1] loss: 1106.510
[84,     1] loss: 1131.312
[85,     1] loss: 1087.644
[86,     1] loss: 1092.761
[87,     1] loss: 1089.689
[88,     1] loss: 1091.844
[89,     1] loss: 1073.537
[90,     1] loss: 1106.539
[91,     1] loss: 1092.757
[92,     1] loss: 1117.023
[93,     1] loss: 1052.550
[94,     1] loss: 1079.168
[95,     1] loss: 1046.173
[96,     1] loss: 1075.735
[97,     1] loss: 1073.323
[98,     1] loss: 1072.553
[99,     1] loss: 1063.324
[100,     1] loss: 1038.077
[101,     1] loss: 1074.714
[102,     1] loss: 1107.566
[103,     1] loss: 1064.839
[104,     1] loss: 1090.128
[105,     1] loss: 1068.446
[106,     1] loss: 1035.472
[107,     1] loss: 1055.143
[108,     1] loss: 1061.692
[109,     1] loss: 1037.245
[110,     1] loss: 1059.922
[111,     1] loss: 1059.038
[112,     1] loss: 1052.740
[113,     1] loss: 1077.542
[114,     1] loss: 1040.113
[115,     1] loss: 1062.875
[116,     1] loss: 1057.243
[117,     1] loss: 1024.261
[118,     1] loss: 1006.288
[119,     1] loss: 1036.643
[120,     1] loss: 1045.391
[121,     1] loss: 1079.492
[122,     1] loss: 1031.293
[123,     1] loss: 1025.726
[124,     1] loss: 1001.112
[125,     1] loss: 1033.019
[126,     1] loss: 1052.212
[127,     1] loss: 1029.353
[128,     1] loss: 1009.338
[129,     1] loss: 985.385
Early stopping applied (best metric=0.9278177618980408)
Finished Training
Total time taken: 22.769338130950928
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1334.876
[2,     1] loss: 1322.394
[3,     1] loss: 1322.276
[4,     1] loss: 1324.590
[5,     1] loss: 1323.826
[6,     1] loss: 1320.728
[7,     1] loss: 1321.913
[8,     1] loss: 1327.702
[9,     1] loss: 1325.676
[10,     1] loss: 1324.924
[11,     1] loss: 1321.715
[12,     1] loss: 1319.156
[13,     1] loss: 1323.820
[14,     1] loss: 1323.679
[15,     1] loss: 1321.985
[16,     1] loss: 1320.066
[17,     1] loss: 1325.096
[18,     1] loss: 1325.024
[19,     1] loss: 1321.684
[20,     1] loss: 1322.246
[21,     1] loss: 1321.453
[22,     1] loss: 1323.164
Early stopping applied (best metric=1.0737075805664062)
Finished Training
Total time taken: 4.239121675491333
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.673
[2,     1] loss: 1322.128
[3,     1] loss: 1319.852
[4,     1] loss: 1320.926
[5,     1] loss: 1315.520
[6,     1] loss: 1321.921
[7,     1] loss: 1323.681
[8,     1] loss: 1319.249
[9,     1] loss: 1323.255
[10,     1] loss: 1319.649
[11,     1] loss: 1318.722
[12,     1] loss: 1319.996
[13,     1] loss: 1321.258
[14,     1] loss: 1322.144
[15,     1] loss: 1317.755
[16,     1] loss: 1321.974
[17,     1] loss: 1321.682
[18,     1] loss: 1319.039
[19,     1] loss: 1320.755
[20,     1] loss: 1316.610
[21,     1] loss: 1316.612
[22,     1] loss: 1319.670
[23,     1] loss: 1317.565
[24,     1] loss: 1318.876
[25,     1] loss: 1316.155
[26,     1] loss: 1318.179
[27,     1] loss: 1317.214
[28,     1] loss: 1315.697
[29,     1] loss: 1316.929
[30,     1] loss: 1315.214
[31,     1] loss: 1316.952
[32,     1] loss: 1315.151
[33,     1] loss: 1316.449
[34,     1] loss: 1314.509
[35,     1] loss: 1311.723
[36,     1] loss: 1313.008
[37,     1] loss: 1313.127
[38,     1] loss: 1312.045
[39,     1] loss: 1310.924
[40,     1] loss: 1308.821
[41,     1] loss: 1309.132
[42,     1] loss: 1308.183
[43,     1] loss: 1306.685
[44,     1] loss: 1306.469
[45,     1] loss: 1303.583
[46,     1] loss: 1297.180
[47,     1] loss: 1301.400
[48,     1] loss: 1296.424
[49,     1] loss: 1291.545
[50,     1] loss: 1292.336
[51,     1] loss: 1289.213
[52,     1] loss: 1283.479
[53,     1] loss: 1283.118
[54,     1] loss: 1278.351
[55,     1] loss: 1278.073
[56,     1] loss: 1271.082
[57,     1] loss: 1269.407
[58,     1] loss: 1270.372
[59,     1] loss: 1250.438
[60,     1] loss: 1247.692
[61,     1] loss: 1250.457
[62,     1] loss: 1246.509
[63,     1] loss: 1229.364
[64,     1] loss: 1241.312
[65,     1] loss: 1218.491
[66,     1] loss: 1218.014
[67,     1] loss: 1202.682
[68,     1] loss: 1206.921
[69,     1] loss: 1215.582
[70,     1] loss: 1202.306
[71,     1] loss: 1185.643
[72,     1] loss: 1182.406
[73,     1] loss: 1183.115
[74,     1] loss: 1194.134
[75,     1] loss: 1176.790
[76,     1] loss: 1168.897
[77,     1] loss: 1167.391
[78,     1] loss: 1159.613
[79,     1] loss: 1144.389
[80,     1] loss: 1160.372
[81,     1] loss: 1136.336
[82,     1] loss: 1134.351
[83,     1] loss: 1132.402
[84,     1] loss: 1122.295
[85,     1] loss: 1125.267
[86,     1] loss: 1107.603
[87,     1] loss: 1148.079
[88,     1] loss: 1116.381
[89,     1] loss: 1091.296
[90,     1] loss: 1129.425
[91,     1] loss: 1120.631
[92,     1] loss: 1088.638
[93,     1] loss: 1095.774
[94,     1] loss: 1089.958
[95,     1] loss: 1123.891
[96,     1] loss: 1097.968
[97,     1] loss: 1097.734
[98,     1] loss: 1108.182
[99,     1] loss: 1104.185
[100,     1] loss: 1089.834
[101,     1] loss: 1076.184
[102,     1] loss: 1105.745
[103,     1] loss: 1104.871
[104,     1] loss: 1069.654
[105,     1] loss: 1088.026
[106,     1] loss: 1087.323
[107,     1] loss: 1052.958
[108,     1] loss: 1117.139
[109,     1] loss: 1074.384
[110,     1] loss: 1039.219
[111,     1] loss: 1065.007
[112,     1] loss: 1058.857
[113,     1] loss: 1028.564
[114,     1] loss: 1113.089
[115,     1] loss: 1108.029
[116,     1] loss: 1068.176
[117,     1] loss: 1036.573
[118,     1] loss: 1023.949
[119,     1] loss: 1036.202
[120,     1] loss: 1055.771
[121,     1] loss: 1076.148
[122,     1] loss: 1020.674
[123,     1] loss: 1032.561
[124,     1] loss: 1037.342
[125,     1] loss: 1050.236
[126,     1] loss: 1072.338
[127,     1] loss: 1056.481
[128,     1] loss: 1017.386
[129,     1] loss: 1069.835
[130,     1] loss: 1060.034
[131,     1] loss: 1072.490
[132,     1] loss: 1045.018
[133,     1] loss: 1035.365
[134,     1] loss: 1031.174
[135,     1] loss: 1007.556
[136,     1] loss: 1008.179
[137,     1] loss: 1018.491
[138,     1] loss: 1055.333
[139,     1] loss: 1009.210
[140,     1] loss: 1029.064
[141,     1] loss: 1010.945
[142,     1] loss: 1010.959
[143,     1] loss: 1024.323
[144,     1] loss: 986.626
[145,     1] loss: 1025.756
[146,     1] loss: 1053.367
[147,     1] loss: 990.191
[148,     1] loss: 963.320
[149,     1] loss: 962.636
[150,     1] loss: 992.813
[151,     1] loss: 972.538
[152,     1] loss: 1007.750
[153,     1] loss: 960.989
[154,     1] loss: 992.847
[155,     1] loss: 981.055
[156,     1] loss: 943.096
[157,     1] loss: 958.455
[158,     1] loss: 987.845
[159,     1] loss: 973.715
[160,     1] loss: 952.824
[161,     1] loss: 980.172
[162,     1] loss: 979.265
[163,     1] loss: 1000.500
[164,     1] loss: 906.985
[165,     1] loss: 985.099
[166,     1] loss: 927.870
[167,     1] loss: 950.521
[168,     1] loss: 944.590
[169,     1] loss: 939.681
[170,     1] loss: 938.719
[171,     1] loss: 959.892
[172,     1] loss: 917.548
[173,     1] loss: 931.851
[174,     1] loss: 933.570
[175,     1] loss: 933.231
[176,     1] loss: 958.100
[177,     1] loss: 922.528
[178,     1] loss: 875.964
[179,     1] loss: 945.665
[180,     1] loss: 940.212
[181,     1] loss: 955.030
Early stopping applied (best metric=0.8355206251144409)
Finished Training
Total time taken: 32.70951962471008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.499
[2,     1] loss: 1329.614
[3,     1] loss: 1325.915
[4,     1] loss: 1319.843
[5,     1] loss: 1323.280
[6,     1] loss: 1322.141
[7,     1] loss: 1318.423
[8,     1] loss: 1321.600
[9,     1] loss: 1320.743
[10,     1] loss: 1321.705
[11,     1] loss: 1323.493
[12,     1] loss: 1319.574
[13,     1] loss: 1322.019
[14,     1] loss: 1318.789
[15,     1] loss: 1317.902
[16,     1] loss: 1317.257
[17,     1] loss: 1322.343
[18,     1] loss: 1319.722
[19,     1] loss: 1316.786
[20,     1] loss: 1321.980
[21,     1] loss: 1320.236
[22,     1] loss: 1316.252
[23,     1] loss: 1320.167
[24,     1] loss: 1320.266
[25,     1] loss: 1318.977
[26,     1] loss: 1320.868
[27,     1] loss: 1318.595
[28,     1] loss: 1314.696
[29,     1] loss: 1315.347
[30,     1] loss: 1315.944
[31,     1] loss: 1318.341
[32,     1] loss: 1317.287
[33,     1] loss: 1317.434
[34,     1] loss: 1313.556
[35,     1] loss: 1312.693
[36,     1] loss: 1316.689
[37,     1] loss: 1310.667
[38,     1] loss: 1315.594
[39,     1] loss: 1315.998
[40,     1] loss: 1312.649
[41,     1] loss: 1309.846
[42,     1] loss: 1309.962
[43,     1] loss: 1313.083
[44,     1] loss: 1306.742
[45,     1] loss: 1308.018
[46,     1] loss: 1306.326
[47,     1] loss: 1302.234
[48,     1] loss: 1303.266
[49,     1] loss: 1302.071
[50,     1] loss: 1297.685
[51,     1] loss: 1298.142
[52,     1] loss: 1294.747
[53,     1] loss: 1294.481
[54,     1] loss: 1290.305
[55,     1] loss: 1281.345
[56,     1] loss: 1277.268
[57,     1] loss: 1277.749
[58,     1] loss: 1274.725
[59,     1] loss: 1270.234
[60,     1] loss: 1271.998
[61,     1] loss: 1259.821
[62,     1] loss: 1250.068
[63,     1] loss: 1245.030
[64,     1] loss: 1239.496
[65,     1] loss: 1231.271
[66,     1] loss: 1239.001
[67,     1] loss: 1227.579
[68,     1] loss: 1212.277
[69,     1] loss: 1205.865
[70,     1] loss: 1197.215
[71,     1] loss: 1202.308
[72,     1] loss: 1175.846
[73,     1] loss: 1170.048
[74,     1] loss: 1191.236
[75,     1] loss: 1185.840
[76,     1] loss: 1163.761
[77,     1] loss: 1147.445
[78,     1] loss: 1167.706
[79,     1] loss: 1156.138
[80,     1] loss: 1143.924
[81,     1] loss: 1141.091
[82,     1] loss: 1153.396
[83,     1] loss: 1103.688
[84,     1] loss: 1136.876
[85,     1] loss: 1137.673
[86,     1] loss: 1105.682
[87,     1] loss: 1131.113
[88,     1] loss: 1119.040
[89,     1] loss: 1090.681
[90,     1] loss: 1125.187
[91,     1] loss: 1104.704
[92,     1] loss: 1130.224
[93,     1] loss: 1102.875
[94,     1] loss: 1104.751
[95,     1] loss: 1111.016
[96,     1] loss: 1125.071
[97,     1] loss: 1077.640
[98,     1] loss: 1063.817
[99,     1] loss: 1034.135
[100,     1] loss: 1115.845
[101,     1] loss: 1111.751
[102,     1] loss: 1035.719
[103,     1] loss: 1076.323
[104,     1] loss: 1097.017
[105,     1] loss: 1069.158
[106,     1] loss: 1044.125
[107,     1] loss: 1066.704
[108,     1] loss: 1068.698
[109,     1] loss: 1036.771
[110,     1] loss: 1014.163
[111,     1] loss: 1056.465
[112,     1] loss: 1046.291
[113,     1] loss: 1082.750
[114,     1] loss: 1079.852
[115,     1] loss: 1039.877
[116,     1] loss: 1053.060
[117,     1] loss: 1113.315
[118,     1] loss: 1072.737
[119,     1] loss: 1048.512
[120,     1] loss: 1063.236
[121,     1] loss: 1073.114
[122,     1] loss: 999.759
[123,     1] loss: 1049.621
[124,     1] loss: 1067.028
[125,     1] loss: 1018.190
[126,     1] loss: 1032.460
[127,     1] loss: 1058.450
[128,     1] loss: 1039.140
[129,     1] loss: 1042.880
[130,     1] loss: 1037.749
[131,     1] loss: 1048.194
[132,     1] loss: 1019.468
[133,     1] loss: 1057.533
[134,     1] loss: 995.681
[135,     1] loss: 1041.586
[136,     1] loss: 1046.001
[137,     1] loss: 1041.806
[138,     1] loss: 1017.495
[139,     1] loss: 1066.093
[140,     1] loss: 1028.255
[141,     1] loss: 1048.570
[142,     1] loss: 1050.058
[143,     1] loss: 982.041
[144,     1] loss: 1024.694
[145,     1] loss: 1034.855
[146,     1] loss: 990.118
[147,     1] loss: 1001.778
[148,     1] loss: 990.372
[149,     1] loss: 994.032
[150,     1] loss: 1011.850
[151,     1] loss: 1043.411
[152,     1] loss: 1001.432
[153,     1] loss: 1033.237
[154,     1] loss: 996.823
[155,     1] loss: 1055.327
[156,     1] loss: 996.572
[157,     1] loss: 980.656
[158,     1] loss: 982.274
[159,     1] loss: 1007.953
[160,     1] loss: 989.964
[161,     1] loss: 943.315
[162,     1] loss: 973.728
[163,     1] loss: 975.223
[164,     1] loss: 977.133
[165,     1] loss: 1003.745
[166,     1] loss: 946.563
[167,     1] loss: 943.962
[168,     1] loss: 963.466
[169,     1] loss: 1008.484
[170,     1] loss: 944.621
[171,     1] loss: 930.843
[172,     1] loss: 980.909
[173,     1] loss: 961.610
[174,     1] loss: 939.950
[175,     1] loss: 954.529
[176,     1] loss: 918.994
[177,     1] loss: 964.376
[178,     1] loss: 941.559
[179,     1] loss: 924.422
[180,     1] loss: 942.787
[181,     1] loss: 918.082
[182,     1] loss: 918.489
[183,     1] loss: 955.531
[184,     1] loss: 899.098
[185,     1] loss: 945.049
[186,     1] loss: 861.456
[187,     1] loss: 953.755
[188,     1] loss: 912.225
[189,     1] loss: 931.097
[190,     1] loss: 959.217
[191,     1] loss: 922.287
[192,     1] loss: 937.272
[193,     1] loss: 862.146
[194,     1] loss: 953.694
[195,     1] loss: 891.130
[196,     1] loss: 947.503
[197,     1] loss: 889.922
[198,     1] loss: 900.382
[199,     1] loss: 888.901
[200,     1] loss: 878.908
Finished Training
Total time taken: 30.83520770072937
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1321.481
[2,     1] loss: 1320.271
[3,     1] loss: 1319.630
[4,     1] loss: 1319.745
[5,     1] loss: 1322.575
[6,     1] loss: 1320.843
[7,     1] loss: 1325.847
[8,     1] loss: 1325.080
[9,     1] loss: 1319.955
[10,     1] loss: 1321.664
[11,     1] loss: 1319.291
[12,     1] loss: 1317.550
[13,     1] loss: 1321.897
[14,     1] loss: 1322.304
[15,     1] loss: 1321.133
[16,     1] loss: 1318.505
[17,     1] loss: 1319.266
[18,     1] loss: 1318.297
[19,     1] loss: 1319.430
[20,     1] loss: 1315.867
[21,     1] loss: 1320.879
[22,     1] loss: 1321.204
[23,     1] loss: 1319.102
[24,     1] loss: 1319.838
[25,     1] loss: 1320.868
[26,     1] loss: 1318.806
[27,     1] loss: 1318.388
[28,     1] loss: 1318.676
[29,     1] loss: 1317.363
[30,     1] loss: 1317.964
[31,     1] loss: 1318.187
[32,     1] loss: 1317.688
[33,     1] loss: 1317.916
[34,     1] loss: 1317.611
[35,     1] loss: 1317.077
[36,     1] loss: 1315.497
[37,     1] loss: 1317.462
[38,     1] loss: 1315.929
[39,     1] loss: 1314.288
[40,     1] loss: 1312.844
[41,     1] loss: 1314.178
[42,     1] loss: 1314.269
[43,     1] loss: 1310.953
[44,     1] loss: 1309.817
[45,     1] loss: 1310.255
[46,     1] loss: 1309.416
[47,     1] loss: 1305.671
[48,     1] loss: 1306.828
[49,     1] loss: 1306.151
[50,     1] loss: 1302.417
[51,     1] loss: 1298.580
[52,     1] loss: 1297.066
[53,     1] loss: 1294.983
[54,     1] loss: 1291.451
[55,     1] loss: 1290.027
[56,     1] loss: 1287.949
[57,     1] loss: 1279.204
[58,     1] loss: 1275.954
[59,     1] loss: 1275.538
[60,     1] loss: 1263.521
[61,     1] loss: 1266.859
[62,     1] loss: 1263.166
[63,     1] loss: 1252.990
[64,     1] loss: 1252.146
[65,     1] loss: 1246.711
[66,     1] loss: 1242.058
[67,     1] loss: 1231.888
[68,     1] loss: 1221.941
[69,     1] loss: 1211.161
[70,     1] loss: 1219.185
[71,     1] loss: 1211.481
[72,     1] loss: 1205.378
[73,     1] loss: 1194.274
[74,     1] loss: 1178.966
[75,     1] loss: 1169.953
[76,     1] loss: 1175.387
[77,     1] loss: 1171.390
[78,     1] loss: 1183.059
[79,     1] loss: 1167.426
[80,     1] loss: 1169.322
[81,     1] loss: 1167.808
[82,     1] loss: 1138.699
[83,     1] loss: 1137.374
[84,     1] loss: 1148.176
[85,     1] loss: 1123.712
[86,     1] loss: 1121.294
[87,     1] loss: 1122.095
[88,     1] loss: 1110.606
[89,     1] loss: 1119.366
[90,     1] loss: 1136.054
[91,     1] loss: 1102.537
[92,     1] loss: 1099.438
[93,     1] loss: 1137.104
[94,     1] loss: 1086.864
[95,     1] loss: 1107.078
[96,     1] loss: 1097.535
[97,     1] loss: 1071.615
[98,     1] loss: 1089.760
[99,     1] loss: 1074.151
[100,     1] loss: 1108.251
[101,     1] loss: 1126.776
[102,     1] loss: 1116.445
[103,     1] loss: 1110.113
[104,     1] loss: 1061.121
[105,     1] loss: 1092.150
[106,     1] loss: 1067.286
[107,     1] loss: 1107.521
[108,     1] loss: 1094.038
[109,     1] loss: 1099.419
[110,     1] loss: 1091.257
[111,     1] loss: 1107.105
[112,     1] loss: 1045.330
[113,     1] loss: 1071.956
[114,     1] loss: 1037.075
[115,     1] loss: 1027.017
[116,     1] loss: 1076.908
[117,     1] loss: 1050.051
[118,     1] loss: 1077.620
[119,     1] loss: 1068.869
[120,     1] loss: 1050.403
[121,     1] loss: 1071.586
[122,     1] loss: 1061.928
[123,     1] loss: 1046.095
[124,     1] loss: 1012.583
[125,     1] loss: 1087.812
[126,     1] loss: 1052.651
[127,     1] loss: 1080.218
[128,     1] loss: 1048.498
[129,     1] loss: 1050.439
[130,     1] loss: 1014.704
[131,     1] loss: 1029.395
[132,     1] loss: 1039.563
[133,     1] loss: 1055.699
[134,     1] loss: 1031.871
[135,     1] loss: 1021.609
[136,     1] loss: 1028.726
[137,     1] loss: 1041.455
[138,     1] loss: 1007.264
[139,     1] loss: 998.000
[140,     1] loss: 1005.545
[141,     1] loss: 1035.818
[142,     1] loss: 1050.114
[143,     1] loss: 1004.716
[144,     1] loss: 1024.018
[145,     1] loss: 1022.083
[146,     1] loss: 1012.802
[147,     1] loss: 1026.192
[148,     1] loss: 1012.192
[149,     1] loss: 989.755
[150,     1] loss: 1029.602
[151,     1] loss: 967.272
[152,     1] loss: 1025.194
[153,     1] loss: 958.575
[154,     1] loss: 953.530
[155,     1] loss: 974.523
[156,     1] loss: 974.126
[157,     1] loss: 1010.766
[158,     1] loss: 961.582
[159,     1] loss: 982.049
[160,     1] loss: 984.594
[161,     1] loss: 974.298
[162,     1] loss: 974.575
[163,     1] loss: 1014.882
[164,     1] loss: 963.595
[165,     1] loss: 983.651
[166,     1] loss: 953.536
[167,     1] loss: 971.070
[168,     1] loss: 938.688
[169,     1] loss: 997.290
[170,     1] loss: 924.535
[171,     1] loss: 950.052
[172,     1] loss: 949.076
[173,     1] loss: 924.130
[174,     1] loss: 938.953
[175,     1] loss: 936.308
[176,     1] loss: 961.422
[177,     1] loss: 979.813
[178,     1] loss: 950.499
[179,     1] loss: 972.449
[180,     1] loss: 948.961
[181,     1] loss: 957.666
[182,     1] loss: 917.740
[183,     1] loss: 898.011
Early stopping applied (best metric=0.9054145812988281)
Finished Training
Total time taken: 29.940946102142334
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.342
[2,     1] loss: 1322.260
[3,     1] loss: 1322.094
[4,     1] loss: 1321.195
[5,     1] loss: 1320.795
[6,     1] loss: 1321.577
[7,     1] loss: 1320.150
[8,     1] loss: 1320.240
[9,     1] loss: 1318.336
[10,     1] loss: 1320.785
[11,     1] loss: 1320.082
[12,     1] loss: 1320.603
[13,     1] loss: 1320.729
[14,     1] loss: 1317.981
[15,     1] loss: 1316.542
[16,     1] loss: 1320.580
[17,     1] loss: 1320.610
[18,     1] loss: 1317.063
[19,     1] loss: 1318.515
[20,     1] loss: 1317.300
[21,     1] loss: 1319.222
[22,     1] loss: 1320.170
[23,     1] loss: 1317.449
[24,     1] loss: 1318.103
[25,     1] loss: 1318.365
[26,     1] loss: 1319.725
[27,     1] loss: 1318.336
[28,     1] loss: 1317.990
[29,     1] loss: 1317.436
[30,     1] loss: 1316.471
[31,     1] loss: 1316.463
[32,     1] loss: 1316.892
[33,     1] loss: 1314.783
[34,     1] loss: 1316.380
[35,     1] loss: 1313.472
[36,     1] loss: 1314.898
[37,     1] loss: 1312.463
[38,     1] loss: 1312.481
[39,     1] loss: 1310.516
[40,     1] loss: 1311.822
[41,     1] loss: 1307.715
[42,     1] loss: 1306.312
[43,     1] loss: 1306.739
[44,     1] loss: 1306.064
[45,     1] loss: 1303.834
[46,     1] loss: 1302.410
[47,     1] loss: 1299.952
[48,     1] loss: 1297.108
[49,     1] loss: 1292.624
[50,     1] loss: 1288.199
[51,     1] loss: 1295.541
[52,     1] loss: 1291.415
[53,     1] loss: 1283.951
[54,     1] loss: 1287.517
[55,     1] loss: 1275.044
[56,     1] loss: 1274.933
[57,     1] loss: 1267.855
[58,     1] loss: 1262.973
[59,     1] loss: 1259.411
[60,     1] loss: 1257.542
[61,     1] loss: 1248.363
[62,     1] loss: 1245.350
[63,     1] loss: 1236.594
[64,     1] loss: 1225.988
[65,     1] loss: 1243.799
[66,     1] loss: 1228.197
[67,     1] loss: 1238.997
[68,     1] loss: 1218.629
[69,     1] loss: 1216.051
[70,     1] loss: 1207.506
[71,     1] loss: 1201.705
[72,     1] loss: 1203.835
[73,     1] loss: 1189.363
[74,     1] loss: 1191.192
[75,     1] loss: 1181.607
[76,     1] loss: 1170.214
[77,     1] loss: 1168.906
[78,     1] loss: 1163.699
[79,     1] loss: 1166.288
[80,     1] loss: 1151.069
[81,     1] loss: 1148.540
[82,     1] loss: 1163.559
[83,     1] loss: 1143.669
[84,     1] loss: 1146.026
[85,     1] loss: 1126.277
[86,     1] loss: 1147.782
[87,     1] loss: 1121.625
[88,     1] loss: 1137.159
[89,     1] loss: 1121.174
[90,     1] loss: 1114.126
[91,     1] loss: 1111.719
[92,     1] loss: 1117.544
[93,     1] loss: 1121.163
[94,     1] loss: 1116.906
[95,     1] loss: 1125.009
[96,     1] loss: 1123.199
[97,     1] loss: 1140.700
[98,     1] loss: 1104.356
[99,     1] loss: 1085.498
[100,     1] loss: 1104.242
[101,     1] loss: 1099.479
[102,     1] loss: 1102.569
[103,     1] loss: 1085.551
[104,     1] loss: 1113.393
[105,     1] loss: 1060.106
[106,     1] loss: 1098.438
[107,     1] loss: 1089.096
[108,     1] loss: 1112.413
[109,     1] loss: 1114.153
[110,     1] loss: 1077.610
[111,     1] loss: 1101.899
[112,     1] loss: 1089.427
[113,     1] loss: 1103.207
[114,     1] loss: 1074.621
[115,     1] loss: 1081.550
[116,     1] loss: 1077.487
[117,     1] loss: 1073.162
[118,     1] loss: 1124.808
[119,     1] loss: 1098.633
[120,     1] loss: 1075.939
[121,     1] loss: 1082.881
[122,     1] loss: 1092.268
[123,     1] loss: 1059.418
[124,     1] loss: 1084.815
[125,     1] loss: 1076.339
[126,     1] loss: 1096.641
[127,     1] loss: 1069.707
[128,     1] loss: 1071.548
[129,     1] loss: 1077.820
[130,     1] loss: 1099.054
[131,     1] loss: 1066.301
[132,     1] loss: 1093.428
[133,     1] loss: 1101.563
[134,     1] loss: 1079.496
[135,     1] loss: 1088.539
[136,     1] loss: 1078.361
[137,     1] loss: 1034.132
[138,     1] loss: 1059.927
[139,     1] loss: 1052.742
[140,     1] loss: 1061.395
[141,     1] loss: 1073.419
[142,     1] loss: 1044.359
[143,     1] loss: 1025.981
[144,     1] loss: 1078.981
[145,     1] loss: 1054.849
[146,     1] loss: 1038.854
[147,     1] loss: 1023.802
[148,     1] loss: 1035.562
[149,     1] loss: 1034.423
[150,     1] loss: 1054.444
[151,     1] loss: 1039.740
[152,     1] loss: 976.020
[153,     1] loss: 1044.783
[154,     1] loss: 995.795
[155,     1] loss: 1023.161
[156,     1] loss: 990.935
[157,     1] loss: 986.234
[158,     1] loss: 1038.851
[159,     1] loss: 996.472
[160,     1] loss: 970.518
[161,     1] loss: 1010.300
[162,     1] loss: 959.718
[163,     1] loss: 1020.300
[164,     1] loss: 1006.393
[165,     1] loss: 1005.425
[166,     1] loss: 981.815
[167,     1] loss: 1009.662
[168,     1] loss: 979.818
[169,     1] loss: 1023.536
[170,     1] loss: 1001.965
[171,     1] loss: 1027.208
[172,     1] loss: 985.272
[173,     1] loss: 995.191
[174,     1] loss: 970.147
[175,     1] loss: 975.409
[176,     1] loss: 971.725
[177,     1] loss: 989.313
[178,     1] loss: 1028.188
[179,     1] loss: 958.713
[180,     1] loss: 973.094
[181,     1] loss: 960.113
[182,     1] loss: 970.151
[183,     1] loss: 920.896
[184,     1] loss: 992.315
[185,     1] loss: 960.214
[186,     1] loss: 967.587
[187,     1] loss: 931.775
[188,     1] loss: 940.220
[189,     1] loss: 1023.883
[190,     1] loss: 964.358
[191,     1] loss: 993.471
[192,     1] loss: 931.725
[193,     1] loss: 921.357
[194,     1] loss: 959.198
[195,     1] loss: 924.916
[196,     1] loss: 959.418
[197,     1] loss: 950.784
[198,     1] loss: 966.526
[199,     1] loss: 950.987
[200,     1] loss: 959.945
Finished Training
Total time taken: 31.186680793762207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1328.991
[2,     1] loss: 1322.923
[3,     1] loss: 1324.039
[4,     1] loss: 1326.445
[5,     1] loss: 1323.034
[6,     1] loss: 1321.088
[7,     1] loss: 1321.883
[8,     1] loss: 1325.990
[9,     1] loss: 1322.558
[10,     1] loss: 1320.104
[11,     1] loss: 1324.140
[12,     1] loss: 1325.320
[13,     1] loss: 1322.549
[14,     1] loss: 1322.243
[15,     1] loss: 1321.832
[16,     1] loss: 1322.815
[17,     1] loss: 1323.148
[18,     1] loss: 1323.062
[19,     1] loss: 1321.863
[20,     1] loss: 1322.438
[21,     1] loss: 1322.928
[22,     1] loss: 1323.106
[23,     1] loss: 1323.726
[24,     1] loss: 1318.988
[25,     1] loss: 1319.067
[26,     1] loss: 1318.522
[27,     1] loss: 1318.902
[28,     1] loss: 1317.820
[29,     1] loss: 1317.419
[30,     1] loss: 1321.990
[31,     1] loss: 1320.717
[32,     1] loss: 1321.246
[33,     1] loss: 1314.839
[34,     1] loss: 1316.562
[35,     1] loss: 1316.309
[36,     1] loss: 1316.347
[37,     1] loss: 1313.887
[38,     1] loss: 1313.975
[39,     1] loss: 1314.997
[40,     1] loss: 1313.474
[41,     1] loss: 1313.173
[42,     1] loss: 1311.986
[43,     1] loss: 1309.889
[44,     1] loss: 1309.413
[45,     1] loss: 1307.619
[46,     1] loss: 1307.013
[47,     1] loss: 1302.376
[48,     1] loss: 1302.190
[49,     1] loss: 1300.580
[50,     1] loss: 1298.251
[51,     1] loss: 1296.298
[52,     1] loss: 1291.054
[53,     1] loss: 1286.978
[54,     1] loss: 1284.906
[55,     1] loss: 1277.624
[56,     1] loss: 1276.641
[57,     1] loss: 1274.524
[58,     1] loss: 1266.272
[59,     1] loss: 1262.131
[60,     1] loss: 1253.254
[61,     1] loss: 1245.002
[62,     1] loss: 1243.919
[63,     1] loss: 1223.317
[64,     1] loss: 1235.950
[65,     1] loss: 1220.770
[66,     1] loss: 1220.826
[67,     1] loss: 1218.232
[68,     1] loss: 1203.472
[69,     1] loss: 1195.750
[70,     1] loss: 1212.230
[71,     1] loss: 1188.839
[72,     1] loss: 1179.828
[73,     1] loss: 1170.891
[74,     1] loss: 1160.908
[75,     1] loss: 1187.025
[76,     1] loss: 1165.709
[77,     1] loss: 1176.377
[78,     1] loss: 1126.335
[79,     1] loss: 1177.111
[80,     1] loss: 1152.724
[81,     1] loss: 1147.797
[82,     1] loss: 1125.759
[83,     1] loss: 1126.719
[84,     1] loss: 1152.142
[85,     1] loss: 1145.185
[86,     1] loss: 1143.644
[87,     1] loss: 1141.133
[88,     1] loss: 1101.369
[89,     1] loss: 1109.401
[90,     1] loss: 1094.292
[91,     1] loss: 1125.478
[92,     1] loss: 1113.070
[93,     1] loss: 1118.322
[94,     1] loss: 1102.032
[95,     1] loss: 1082.415
[96,     1] loss: 1118.405
[97,     1] loss: 1128.980
[98,     1] loss: 1087.913
[99,     1] loss: 1088.934
[100,     1] loss: 1089.926
[101,     1] loss: 1078.013
[102,     1] loss: 1115.890
[103,     1] loss: 1099.533
[104,     1] loss: 1090.290
[105,     1] loss: 1093.903
[106,     1] loss: 1074.928
[107,     1] loss: 1107.054
[108,     1] loss: 1100.112
[109,     1] loss: 1067.180
[110,     1] loss: 1086.880
[111,     1] loss: 1098.541
[112,     1] loss: 1087.669
[113,     1] loss: 1092.637
[114,     1] loss: 1045.800
[115,     1] loss: 1109.631
[116,     1] loss: 1072.820
[117,     1] loss: 1076.564
[118,     1] loss: 1036.979
[119,     1] loss: 1044.538
[120,     1] loss: 1067.469
[121,     1] loss: 1057.389
[122,     1] loss: 1070.146
[123,     1] loss: 1053.532
[124,     1] loss: 1077.380
[125,     1] loss: 1073.177
[126,     1] loss: 1064.061
[127,     1] loss: 1063.286
[128,     1] loss: 1030.640
[129,     1] loss: 1045.727
[130,     1] loss: 1101.188
[131,     1] loss: 1049.473
[132,     1] loss: 1085.908
[133,     1] loss: 1059.341
[134,     1] loss: 1045.776
[135,     1] loss: 1057.173
[136,     1] loss: 1009.483
[137,     1] loss: 1026.031
[138,     1] loss: 1018.628
[139,     1] loss: 991.097
[140,     1] loss: 1041.129
[141,     1] loss: 998.070
[142,     1] loss: 1033.961
[143,     1] loss: 1012.809
[144,     1] loss: 1015.231
[145,     1] loss: 1014.117
[146,     1] loss: 1005.817
[147,     1] loss: 1038.711
[148,     1] loss: 1035.141
[149,     1] loss: 1015.950
[150,     1] loss: 1018.787
[151,     1] loss: 967.864
[152,     1] loss: 996.034
[153,     1] loss: 1039.590
[154,     1] loss: 999.957
[155,     1] loss: 976.345
[156,     1] loss: 999.303
[157,     1] loss: 1000.929
[158,     1] loss: 974.236
[159,     1] loss: 1023.563
[160,     1] loss: 1009.405
[161,     1] loss: 993.304
[162,     1] loss: 1001.409
[163,     1] loss: 973.898
[164,     1] loss: 992.282
[165,     1] loss: 951.248
[166,     1] loss: 974.710
[167,     1] loss: 988.812
[168,     1] loss: 973.726
[169,     1] loss: 974.693
[170,     1] loss: 984.374
[171,     1] loss: 948.179
[172,     1] loss: 989.594
[173,     1] loss: 958.246
[174,     1] loss: 942.419
[175,     1] loss: 960.700
[176,     1] loss: 983.913
[177,     1] loss: 978.571
[178,     1] loss: 946.061
[179,     1] loss: 955.844
[180,     1] loss: 965.384
[181,     1] loss: 970.072
[182,     1] loss: 945.367
[183,     1] loss: 933.400
[184,     1] loss: 991.036
[185,     1] loss: 1011.169
[186,     1] loss: 942.467
[187,     1] loss: 924.278
[188,     1] loss: 941.564
[189,     1] loss: 980.955
[190,     1] loss: 936.840
[191,     1] loss: 905.109
[192,     1] loss: 940.996
[193,     1] loss: 924.695
[194,     1] loss: 897.661
[195,     1] loss: 920.932
[196,     1] loss: 953.217
[197,     1] loss: 918.466
[198,     1] loss: 921.534
[199,     1] loss: 928.586
[200,     1] loss: 920.201
Finished Training
Total time taken: 29.487762928009033
{'Hydroxylation-K Validation Accuracy': 0.7335401891252955, 'Hydroxylation-K Validation Sensitivity': 0.7214814814814815, 'Hydroxylation-K Validation Specificity': 0.7368421052631579, 'Hydroxylation-K Validation Precision': 0.4515194052946836, 'Hydroxylation-K AUC ROC': 0.7929824561403509, 'Hydroxylation-K AUC PR': 0.5859142429630136, 'Hydroxylation-K MCC': 0.40412657099659893, 'Hydroxylation-K F1': 0.5422371507522462, 'Validation Loss (Hydroxylation-K)': 0.447064205010732, 'Hydroxylation-P Validation Accuracy': 0.7297780823308462, 'Hydroxylation-P Validation Sensitivity': 0.7881481481481482, 'Hydroxylation-P Validation Specificity': 0.7173100902788169, 'Hydroxylation-P Validation Precision': 0.40687630441560513, 'Hydroxylation-P AUC ROC': 0.8247657920634128, 'Hydroxylation-P AUC PR': 0.5651300259031247, 'Hydroxylation-P MCC': 0.41388412954036397, 'Hydroxylation-P F1': 0.5290504620409421, 'Validation Loss (Hydroxylation-P)': 0.4008706013361613, 'Validation Loss (total)': 0.8479348103205363, 'TimeToTrain': 28.135084517796834}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007277119877525575,
 'learning_rate_Hydroxylation-K': 0.0006439310460757958,
 'learning_rate_Hydroxylation-P': 0.004038002915830996,
 'log_base': 1.1829383894147039,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 869611390,
 'sample_weights': [1.9579750180559359, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.5941750712360205,
 'weight_decay_Hydroxylation-K': 6.567704121077962,
 'weight_decay_Hydroxylation-P': 4.712907306963519}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3235.741
[2,     1] loss: 3215.532
[3,     1] loss: 3226.439
[4,     1] loss: 3237.691
[5,     1] loss: 3220.635
[6,     1] loss: 3224.729
[7,     1] loss: 3210.777
[8,     1] loss: 3227.318
[9,     1] loss: 3212.990
[10,     1] loss: 3228.658
[11,     1] loss: 3204.201
[12,     1] loss: 3196.066
[13,     1] loss: 3190.308
[14,     1] loss: 3165.802
[15,     1] loss: 3128.443
[16,     1] loss: 3056.231
[17,     1] loss: 3036.655
[18,     1] loss: 2905.775
[19,     1] loss: 2912.556
[20,     1] loss: 2841.901
[21,     1] loss: 2753.205
[22,     1] loss: 2649.974
[23,     1] loss: 2519.020
[24,     1] loss: 2691.396
[25,     1] loss: 2675.057
[26,     1] loss: 2546.476
[27,     1] loss: 2614.004
[28,     1] loss: 2685.870
[29,     1] loss: 2520.777
[30,     1] loss: 2491.090
[31,     1] loss: 2540.685
[32,     1] loss: 2759.143
[33,     1] loss: 2253.175
[34,     1] loss: 2608.179
[35,     1] loss: 2459.235
[36,     1] loss: 2314.057
[37,     1] loss: 2188.540
[38,     1] loss: 2210.691
[39,     1] loss: 2054.206
[40,     1] loss: 2448.498
[41,     1] loss: 2201.112
[42,     1] loss: 1879.448
[43,     1] loss: 2080.281
[44,     1] loss: 1984.415
[45,     1] loss: 1968.070
[46,     1] loss: 1834.857
[47,     1] loss: 2096.977
[48,     1] loss: 1897.141
[49,     1] loss: 1730.614
[50,     1] loss: 1791.476
[51,     1] loss: 2135.934
[52,     1] loss: 2094.506
[53,     1] loss: 1787.436
[54,     1] loss: 1979.814
[55,     1] loss: 1752.981
[56,     1] loss: 1801.492
[57,     1] loss: 1729.907
[58,     1] loss: 1563.088
[59,     1] loss: 1837.828
[60,     1] loss: 2096.829
[61,     1] loss: 1608.549
Early stopping applied (best metric=0.835989236831665)
Finished Training
Total time taken: 11.720890045166016
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3214.758
[2,     1] loss: 3213.240
[3,     1] loss: 3217.991
[4,     1] loss: 3219.521
[5,     1] loss: 3230.585
[6,     1] loss: 3207.228
[7,     1] loss: 3229.875
[8,     1] loss: 3170.666
[9,     1] loss: 3169.920
[10,     1] loss: 3161.570
[11,     1] loss: 3109.423
[12,     1] loss: 3066.211
[13,     1] loss: 3021.309
[14,     1] loss: 2902.959
[15,     1] loss: 2980.356
[16,     1] loss: 2731.098
[17,     1] loss: 2772.955
[18,     1] loss: 2641.171
[19,     1] loss: 2644.005
[20,     1] loss: 2704.953
[21,     1] loss: 2616.670
[22,     1] loss: 2568.070
[23,     1] loss: 2547.205
[24,     1] loss: 2539.077
[25,     1] loss: 2415.274
[26,     1] loss: 2463.094
[27,     1] loss: 2460.404
[28,     1] loss: 2814.442
[29,     1] loss: 2331.676
[30,     1] loss: 2181.420
[31,     1] loss: 2454.079
[32,     1] loss: 2347.009
[33,     1] loss: 2350.698
[34,     1] loss: 2151.779
[35,     1] loss: 2143.246
[36,     1] loss: 2088.199
[37,     1] loss: 2108.703
[38,     1] loss: 2203.274
[39,     1] loss: 2330.252
[40,     1] loss: 1891.425
[41,     1] loss: 2468.643
[42,     1] loss: 1925.335
[43,     1] loss: 2457.754
[44,     1] loss: 2004.715
Early stopping applied (best metric=0.7878869771957397)
Finished Training
Total time taken: 7.566103458404541
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3261.739
[2,     1] loss: 3217.041
[3,     1] loss: 3203.372
[4,     1] loss: 3224.620
[5,     1] loss: 3239.938
[6,     1] loss: 3222.251
[7,     1] loss: 3248.268
[8,     1] loss: 3219.402
[9,     1] loss: 3229.640
[10,     1] loss: 3223.865
[11,     1] loss: 3215.943
[12,     1] loss: 3220.336
[13,     1] loss: 3214.185
[14,     1] loss: 3219.528
[15,     1] loss: 3215.568
[16,     1] loss: 3209.370
[17,     1] loss: 3234.125
[18,     1] loss: 3216.065
[19,     1] loss: 3220.464
[20,     1] loss: 3214.843
[21,     1] loss: 3215.980
[22,     1] loss: 3210.608
[23,     1] loss: 3205.626
[24,     1] loss: 3201.898
[25,     1] loss: 3176.485
[26,     1] loss: 3166.104
[27,     1] loss: 3132.999
[28,     1] loss: 3054.669
[29,     1] loss: 3021.958
[30,     1] loss: 2977.352
[31,     1] loss: 2936.630
[32,     1] loss: 2857.764
[33,     1] loss: 2768.788
[34,     1] loss: 2795.548
[35,     1] loss: 2712.577
[36,     1] loss: 2857.083
[37,     1] loss: 2605.910
[38,     1] loss: 2874.567
[39,     1] loss: 2649.335
[40,     1] loss: 2749.301
[41,     1] loss: 2598.245
[42,     1] loss: 2644.616
[43,     1] loss: 2414.312
[44,     1] loss: 2245.413
[45,     1] loss: 2576.602
[46,     1] loss: 2318.770
[47,     1] loss: 2767.100
[48,     1] loss: 2605.686
[49,     1] loss: 2452.075
[50,     1] loss: 2435.108
[51,     1] loss: 2381.160
[52,     1] loss: 2322.595
[53,     1] loss: 2420.181
[54,     1] loss: 2344.074
[55,     1] loss: 2237.003
[56,     1] loss: 2160.982
[57,     1] loss: 2034.459
[58,     1] loss: 1805.678
[59,     1] loss: 1968.347
[60,     1] loss: 1720.814
[61,     1] loss: 1773.251
[62,     1] loss: 2419.103
[63,     1] loss: 2962.910
[64,     1] loss: 1612.284
[65,     1] loss: 2599.800
[66,     1] loss: 1847.786
[67,     1] loss: 2130.188
[68,     1] loss: 2164.332
[69,     1] loss: 1976.434
[70,     1] loss: 2441.795
[71,     1] loss: 2044.530
[72,     1] loss: 1844.865
[73,     1] loss: 2078.860
Early stopping applied (best metric=0.8094546794891357)
Finished Training
Total time taken: 13.960046768188477
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3241.482
[2,     1] loss: 3224.051
[3,     1] loss: 3223.428
[4,     1] loss: 3232.579
[5,     1] loss: 3231.126
[6,     1] loss: 3220.994
[7,     1] loss: 3229.854
[8,     1] loss: 3224.737
[9,     1] loss: 3210.308
[10,     1] loss: 3209.271
[11,     1] loss: 3213.030
[12,     1] loss: 3204.795
[13,     1] loss: 3205.599
[14,     1] loss: 3185.506
[15,     1] loss: 3154.634
[16,     1] loss: 3094.044
[17,     1] loss: 3038.788
[18,     1] loss: 2945.935
[19,     1] loss: 2847.263
[20,     1] loss: 2876.243
[21,     1] loss: 2878.552
[22,     1] loss: 2921.184
[23,     1] loss: 2960.562
[24,     1] loss: 2622.418
[25,     1] loss: 2742.878
[26,     1] loss: 2556.952
[27,     1] loss: 2679.492
[28,     1] loss: 2557.432
[29,     1] loss: 2638.477
[30,     1] loss: 2549.888
[31,     1] loss: 2754.926
[32,     1] loss: 2511.929
[33,     1] loss: 2488.498
[34,     1] loss: 2534.106
[35,     1] loss: 2531.721
[36,     1] loss: 2605.449
[37,     1] loss: 2501.836
[38,     1] loss: 2544.104
[39,     1] loss: 2416.935
[40,     1] loss: 2485.044
[41,     1] loss: 2285.159
[42,     1] loss: 2335.769
[43,     1] loss: 2170.362
[44,     1] loss: 2171.583
[45,     1] loss: 2199.617
[46,     1] loss: 1892.107
[47,     1] loss: 2006.434
[48,     1] loss: 1707.977
[49,     1] loss: 1779.009
[50,     1] loss: 1669.687
[51,     1] loss: 2051.050
[52,     1] loss: 4466.152
[53,     1] loss: 2034.940
[54,     1] loss: 2460.728
[55,     1] loss: 1981.541
[56,     1] loss: 2234.330
[57,     1] loss: 2473.328
[58,     1] loss: 2428.391
[59,     1] loss: 2311.886
[60,     1] loss: 2155.324
[61,     1] loss: 2175.301
[62,     1] loss: 2238.634
[63,     1] loss: 2118.273
[64,     1] loss: 2331.114
[65,     1] loss: 2337.769
[66,     1] loss: 1870.240
[67,     1] loss: 2189.890
[68,     1] loss: 1910.653
Early stopping applied (best metric=0.8531686067581177)
Finished Training
Total time taken: 10.349076986312866
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3226.938
[2,     1] loss: 3215.443
[3,     1] loss: 3243.019
[4,     1] loss: 3231.446
[5,     1] loss: 3261.139
[6,     1] loss: 3222.798
[7,     1] loss: 3213.625
[8,     1] loss: 3227.844
[9,     1] loss: 3234.678
[10,     1] loss: 3216.779
[11,     1] loss: 3219.572
[12,     1] loss: 3221.769
[13,     1] loss: 3223.620
[14,     1] loss: 3216.497
[15,     1] loss: 3216.713
[16,     1] loss: 3216.044
[17,     1] loss: 3213.977
[18,     1] loss: 3210.546
[19,     1] loss: 3212.839
[20,     1] loss: 3207.179
[21,     1] loss: 3191.406
[22,     1] loss: 3169.436
[23,     1] loss: 3133.371
[24,     1] loss: 3110.642
[25,     1] loss: 3028.150
[26,     1] loss: 2952.959
[27,     1] loss: 2907.731
[28,     1] loss: 3009.284
[29,     1] loss: 2756.280
[30,     1] loss: 2799.703
[31,     1] loss: 2806.524
[32,     1] loss: 2698.863
[33,     1] loss: 2725.766
[34,     1] loss: 2814.500
[35,     1] loss: 2597.093
[36,     1] loss: 2796.020
[37,     1] loss: 2647.069
[38,     1] loss: 2856.678
[39,     1] loss: 2606.741
[40,     1] loss: 2671.289
[41,     1] loss: 2607.857
[42,     1] loss: 2389.234
[43,     1] loss: 2430.549
[44,     1] loss: 2482.967
[45,     1] loss: 2225.515
[46,     1] loss: 2307.244
[47,     1] loss: 2185.343
[48,     1] loss: 2698.298
[49,     1] loss: 2225.910
[50,     1] loss: 2103.945
[51,     1] loss: 2092.866
[52,     1] loss: 1941.087
[53,     1] loss: 2268.631
[54,     1] loss: 1945.265
[55,     1] loss: 1984.320
[56,     1] loss: 2055.692
[57,     1] loss: 2401.606
[58,     1] loss: 2525.004
[59,     1] loss: 2172.197
[60,     1] loss: 2471.230
[61,     1] loss: 2105.758
[62,     1] loss: 2150.850
[63,     1] loss: 2276.540
[64,     1] loss: 1963.688
[65,     1] loss: 2088.974
Early stopping applied (best metric=0.731257438659668)
Finished Training
Total time taken: 12.251803159713745
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3216.154
[2,     1] loss: 3277.925
[3,     1] loss: 3240.774
[4,     1] loss: 3245.816
[5,     1] loss: 3222.137
[6,     1] loss: 3224.606
[7,     1] loss: 3218.862
[8,     1] loss: 3230.849
[9,     1] loss: 3233.396
[10,     1] loss: 3206.096
[11,     1] loss: 3217.493
[12,     1] loss: 3220.258
[13,     1] loss: 3214.046
[14,     1] loss: 3225.190
[15,     1] loss: 3218.274
[16,     1] loss: 3212.154
[17,     1] loss: 3210.702
[18,     1] loss: 3228.865
[19,     1] loss: 3221.006
[20,     1] loss: 3227.740
[21,     1] loss: 3220.249
[22,     1] loss: 3210.716
[23,     1] loss: 3216.581
[24,     1] loss: 3213.314
[25,     1] loss: 3217.167
[26,     1] loss: 3220.654
[27,     1] loss: 3203.711
[28,     1] loss: 3182.851
[29,     1] loss: 3160.311
[30,     1] loss: 3131.459
[31,     1] loss: 3073.033
[32,     1] loss: 3032.494
[33,     1] loss: 2947.770
[34,     1] loss: 2806.817
[35,     1] loss: 2822.025
[36,     1] loss: 2816.809
[37,     1] loss: 2890.101
[38,     1] loss: 3048.658
[39,     1] loss: 2553.054
[40,     1] loss: 2663.117
[41,     1] loss: 2908.246
[42,     1] loss: 2821.421
[43,     1] loss: 2715.107
[44,     1] loss: 2678.298
[45,     1] loss: 2587.389
[46,     1] loss: 2722.700
[47,     1] loss: 2508.375
[48,     1] loss: 2451.889
[49,     1] loss: 2386.515
[50,     1] loss: 2527.036
[51,     1] loss: 2528.510
[52,     1] loss: 2410.957
[53,     1] loss: 2207.598
[54,     1] loss: 2095.748
[55,     1] loss: 2597.021
[56,     1] loss: 2778.608
[57,     1] loss: 2280.854
[58,     1] loss: 2220.589
[59,     1] loss: 2403.815
[60,     1] loss: 2166.411
[61,     1] loss: 2525.879
[62,     1] loss: 2112.222
[63,     1] loss: 2050.980
[64,     1] loss: 2105.031
[65,     1] loss: 2138.773
[66,     1] loss: 2124.106
[67,     1] loss: 1891.446
[68,     1] loss: 2007.207
[69,     1] loss: 1955.351
[70,     1] loss: 1898.375
[71,     1] loss: 1802.002
[72,     1] loss: 1597.621
[73,     1] loss: 1797.473
[74,     1] loss: 1962.003
[75,     1] loss: 1808.653
[76,     1] loss: 1919.029
Early stopping applied (best metric=0.7589588165283203)
Finished Training
Total time taken: 11.585771083831787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3228.773
[2,     1] loss: 3222.221
[3,     1] loss: 3205.704
[4,     1] loss: 3224.117
[5,     1] loss: 3267.047
[6,     1] loss: 3204.178
[7,     1] loss: 3223.127
[8,     1] loss: 3225.261
[9,     1] loss: 3216.763
[10,     1] loss: 3230.890
[11,     1] loss: 3212.693
[12,     1] loss: 3222.278
[13,     1] loss: 3225.271
[14,     1] loss: 3220.562
[15,     1] loss: 3222.014
[16,     1] loss: 3220.108
[17,     1] loss: 3215.075
[18,     1] loss: 3215.571
[19,     1] loss: 3216.940
[20,     1] loss: 3209.397
[21,     1] loss: 3223.348
[22,     1] loss: 3208.605
[23,     1] loss: 3207.533
[24,     1] loss: 3197.118
[25,     1] loss: 3173.023
[26,     1] loss: 3144.767
[27,     1] loss: 3105.004
[28,     1] loss: 3023.705
[29,     1] loss: 2996.138
[30,     1] loss: 2872.617
[31,     1] loss: 2846.084
[32,     1] loss: 2768.204
[33,     1] loss: 2708.659
[34,     1] loss: 3103.443
[35,     1] loss: 2612.973
[36,     1] loss: 2801.046
[37,     1] loss: 2641.829
[38,     1] loss: 2671.540
[39,     1] loss: 2752.116
[40,     1] loss: 2656.622
[41,     1] loss: 2634.362
[42,     1] loss: 2702.875
[43,     1] loss: 2627.124
[44,     1] loss: 2519.900
[45,     1] loss: 2631.515
[46,     1] loss: 2446.933
[47,     1] loss: 2374.622
[48,     1] loss: 2294.263
[49,     1] loss: 2316.916
[50,     1] loss: 2420.354
[51,     1] loss: 2368.393
[52,     1] loss: 2089.260
[53,     1] loss: 2077.089
[54,     1] loss: 1995.548
[55,     1] loss: 2062.178
[56,     1] loss: 1974.151
[57,     1] loss: 1968.571
Early stopping applied (best metric=0.9008265733718872)
Finished Training
Total time taken: 10.606531620025635
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3230.359
[2,     1] loss: 3238.052
[3,     1] loss: 3235.140
[4,     1] loss: 3228.278
[5,     1] loss: 3215.561
[6,     1] loss: 3223.226
[7,     1] loss: 3226.230
[8,     1] loss: 3200.191
[9,     1] loss: 3212.526
[10,     1] loss: 3210.150
[11,     1] loss: 3202.518
[12,     1] loss: 3202.055
[13,     1] loss: 3192.162
[14,     1] loss: 3177.032
[15,     1] loss: 3126.753
[16,     1] loss: 3111.872
[17,     1] loss: 2961.938
[18,     1] loss: 3127.816
[19,     1] loss: 2880.682
[20,     1] loss: 2993.106
[21,     1] loss: 2798.299
[22,     1] loss: 2846.153
[23,     1] loss: 2715.436
[24,     1] loss: 2594.045
[25,     1] loss: 2644.857
[26,     1] loss: 2698.358
[27,     1] loss: 2660.245
[28,     1] loss: 2455.446
[29,     1] loss: 2462.650
[30,     1] loss: 2295.073
[31,     1] loss: 2426.149
[32,     1] loss: 2535.154
[33,     1] loss: 2274.494
[34,     1] loss: 2392.733
[35,     1] loss: 2482.079
[36,     1] loss: 2189.045
[37,     1] loss: 2222.136
[38,     1] loss: 2234.139
[39,     1] loss: 2110.939
[40,     1] loss: 2129.552
[41,     1] loss: 2067.363
[42,     1] loss: 2257.337
[43,     1] loss: 2033.538
[44,     1] loss: 1758.019
[45,     1] loss: 1868.875
[46,     1] loss: 2084.915
[47,     1] loss: 1996.766
[48,     1] loss: 1848.547
[49,     1] loss: 1887.218
Early stopping applied (best metric=0.924672544002533)
Finished Training
Total time taken: 8.972400426864624
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3238.028
[2,     1] loss: 3208.561
[3,     1] loss: 3244.397
[4,     1] loss: 3219.124
[5,     1] loss: 3248.838
[6,     1] loss: 3238.228
[7,     1] loss: 3222.065
[8,     1] loss: 3216.894
[9,     1] loss: 3233.908
[10,     1] loss: 3224.260
[11,     1] loss: 3224.234
[12,     1] loss: 3219.445
[13,     1] loss: 3230.228
[14,     1] loss: 3228.791
[15,     1] loss: 3216.893
[16,     1] loss: 3221.129
[17,     1] loss: 3225.869
[18,     1] loss: 3216.380
[19,     1] loss: 3219.888
[20,     1] loss: 3218.140
[21,     1] loss: 3224.415
[22,     1] loss: 3222.978
Early stopping applied (best metric=1.0703654289245605)
Finished Training
Total time taken: 4.221599340438843
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3247.116
[2,     1] loss: 3221.500
[3,     1] loss: 3284.349
[4,     1] loss: 3237.370
[5,     1] loss: 3239.139
[6,     1] loss: 3216.988
[7,     1] loss: 3231.567
[8,     1] loss: 3222.729
[9,     1] loss: 3220.196
[10,     1] loss: 3219.932
[11,     1] loss: 3219.842
[12,     1] loss: 3226.901
[13,     1] loss: 3218.542
[14,     1] loss: 3217.244
[15,     1] loss: 3216.246
[16,     1] loss: 3204.683
[17,     1] loss: 3199.267
[18,     1] loss: 3204.819
[19,     1] loss: 3164.667
[20,     1] loss: 3120.641
[21,     1] loss: 3102.038
[22,     1] loss: 3034.913
[23,     1] loss: 2962.097
[24,     1] loss: 2916.361
[25,     1] loss: 2850.535
[26,     1] loss: 2861.800
[27,     1] loss: 2644.332
[28,     1] loss: 2715.331
[29,     1] loss: 2645.860
[30,     1] loss: 2521.062
[31,     1] loss: 2590.427
[32,     1] loss: 2652.879
[33,     1] loss: 2379.753
[34,     1] loss: 2628.763
[35,     1] loss: 2536.312
[36,     1] loss: 2401.869
[37,     1] loss: 2492.685
[38,     1] loss: 2349.516
[39,     1] loss: 2477.409
[40,     1] loss: 2320.089
[41,     1] loss: 2162.844
[42,     1] loss: 2166.992
[43,     1] loss: 2145.205
[44,     1] loss: 1920.106
[45,     1] loss: 1821.998
[46,     1] loss: 1908.765
[47,     1] loss: 1912.114
[48,     1] loss: 2327.071
[49,     1] loss: 3151.526
[50,     1] loss: 2813.849
[51,     1] loss: 2300.883
[52,     1] loss: 2245.211
[53,     1] loss: 2496.646
[54,     1] loss: 2434.331
[55,     1] loss: 2307.644
Early stopping applied (best metric=0.8690328001976013)
Finished Training
Total time taken: 8.566325902938843
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3231.110
[2,     1] loss: 3268.336
[3,     1] loss: 3231.561
[4,     1] loss: 3222.224
[5,     1] loss: 3220.095
[6,     1] loss: 3224.558
[7,     1] loss: 3218.706
[8,     1] loss: 3233.771
[9,     1] loss: 3221.500
[10,     1] loss: 3226.393
[11,     1] loss: 3216.096
[12,     1] loss: 3206.254
[13,     1] loss: 3207.918
[14,     1] loss: 3224.173
[15,     1] loss: 3226.578
[16,     1] loss: 3219.409
[17,     1] loss: 3211.785
[18,     1] loss: 3201.271
[19,     1] loss: 3199.182
[20,     1] loss: 3160.114
[21,     1] loss: 3160.180
[22,     1] loss: 3086.749
[23,     1] loss: 3051.635
[24,     1] loss: 2989.497
[25,     1] loss: 2956.784
[26,     1] loss: 2908.099
[27,     1] loss: 2740.371
[28,     1] loss: 2788.333
[29,     1] loss: 2897.375
[30,     1] loss: 2955.278
[31,     1] loss: 2768.101
[32,     1] loss: 2625.108
[33,     1] loss: 2640.318
[34,     1] loss: 2617.169
[35,     1] loss: 2550.048
[36,     1] loss: 2537.062
[37,     1] loss: 2440.843
[38,     1] loss: 2409.918
[39,     1] loss: 2484.949
[40,     1] loss: 2439.562
[41,     1] loss: 2404.279
[42,     1] loss: 2246.538
[43,     1] loss: 2246.182
[44,     1] loss: 2448.863
[45,     1] loss: 2584.649
[46,     1] loss: 2126.401
[47,     1] loss: 2123.385
[48,     1] loss: 2204.750
[49,     1] loss: 2245.532
[50,     1] loss: 2110.721
[51,     1] loss: 1997.908
[52,     1] loss: 1951.240
[53,     1] loss: 2452.025
[54,     1] loss: 2138.153
[55,     1] loss: 1953.721
[56,     1] loss: 2137.059
[57,     1] loss: 1937.912
[58,     1] loss: 2366.880
[59,     1] loss: 1844.191
[60,     1] loss: 2196.032
Early stopping applied (best metric=0.8038878440856934)
Finished Training
Total time taken: 9.973279476165771
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3229.830
[2,     1] loss: 3244.096
[3,     1] loss: 3207.825
[4,     1] loss: 3215.460
[5,     1] loss: 3243.700
[6,     1] loss: 3226.241
[7,     1] loss: 3237.952
[8,     1] loss: 3233.322
[9,     1] loss: 3211.734
[10,     1] loss: 3212.521
[11,     1] loss: 3213.703
[12,     1] loss: 3218.032
[13,     1] loss: 3206.059
[14,     1] loss: 3193.008
[15,     1] loss: 3179.276
[16,     1] loss: 3164.821
[17,     1] loss: 3097.492
[18,     1] loss: 3060.306
[19,     1] loss: 2978.599
[20,     1] loss: 2921.596
[21,     1] loss: 2898.594
[22,     1] loss: 2740.571
[23,     1] loss: 2745.099
[24,     1] loss: 2775.196
[25,     1] loss: 2499.243
[26,     1] loss: 2442.462
[27,     1] loss: 2673.693
[28,     1] loss: 2527.054
[29,     1] loss: 2559.869
[30,     1] loss: 2567.864
[31,     1] loss: 2561.700
[32,     1] loss: 2341.397
[33,     1] loss: 2551.553
[34,     1] loss: 2198.823
[35,     1] loss: 2100.900
[36,     1] loss: 2155.670
[37,     1] loss: 2158.806
[38,     1] loss: 2150.206
[39,     1] loss: 2155.452
[40,     1] loss: 1981.812
[41,     1] loss: 1844.161
[42,     1] loss: 1930.335
[43,     1] loss: 2253.212
[44,     1] loss: 2928.452
[45,     1] loss: 2327.047
[46,     1] loss: 2361.116
[47,     1] loss: 2090.958
[48,     1] loss: 2108.579
[49,     1] loss: 2126.831
Early stopping applied (best metric=0.9394000172615051)
Finished Training
Total time taken: 9.60847783088684
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3223.426
[2,     1] loss: 3227.302
[3,     1] loss: 3235.062
[4,     1] loss: 3212.947
[5,     1] loss: 3223.065
[6,     1] loss: 3205.243
[7,     1] loss: 3210.840
[8,     1] loss: 3197.927
[9,     1] loss: 3186.078
[10,     1] loss: 3148.224
[11,     1] loss: 3113.584
[12,     1] loss: 3040.348
[13,     1] loss: 2987.475
[14,     1] loss: 2829.497
[15,     1] loss: 2698.684
[16,     1] loss: 2765.843
[17,     1] loss: 2688.291
[18,     1] loss: 2885.323
[19,     1] loss: 2531.526
[20,     1] loss: 2791.383
[21,     1] loss: 2473.834
[22,     1] loss: 2595.297
[23,     1] loss: 2466.523
[24,     1] loss: 2580.734
[25,     1] loss: 2355.462
[26,     1] loss: 2275.347
[27,     1] loss: 2513.115
[28,     1] loss: 2244.629
[29,     1] loss: 2244.930
[30,     1] loss: 2214.943
[31,     1] loss: 2148.001
Early stopping applied (best metric=1.0323693752288818)
Finished Training
Total time taken: 4.918108940124512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3231.743
[2,     1] loss: 3221.381
[3,     1] loss: 3212.293
[4,     1] loss: 3244.068
[5,     1] loss: 3233.847
[6,     1] loss: 3229.384
[7,     1] loss: 3223.131
[8,     1] loss: 3214.670
[9,     1] loss: 3220.600
[10,     1] loss: 3219.264
[11,     1] loss: 3221.654
[12,     1] loss: 3218.047
[13,     1] loss: 3214.510
[14,     1] loss: 3220.747
[15,     1] loss: 3210.826
[16,     1] loss: 3212.450
[17,     1] loss: 3211.413
[18,     1] loss: 3202.766
[19,     1] loss: 3192.646
[20,     1] loss: 3177.280
[21,     1] loss: 3126.339
[22,     1] loss: 3043.103
[23,     1] loss: 2985.749
[24,     1] loss: 2943.553
[25,     1] loss: 2924.467
[26,     1] loss: 2829.876
[27,     1] loss: 2782.937
[28,     1] loss: 2857.712
[29,     1] loss: 2579.987
[30,     1] loss: 2736.138
[31,     1] loss: 2476.276
[32,     1] loss: 2722.350
[33,     1] loss: 2559.958
[34,     1] loss: 2687.716
[35,     1] loss: 2474.363
[36,     1] loss: 2531.629
[37,     1] loss: 2228.431
[38,     1] loss: 2677.049
[39,     1] loss: 2353.573
[40,     1] loss: 2385.969
[41,     1] loss: 2249.735
[42,     1] loss: 2229.083
[43,     1] loss: 2296.617
[44,     1] loss: 2499.427
[45,     1] loss: 2252.929
[46,     1] loss: 2167.899
[47,     1] loss: 2301.311
[48,     1] loss: 2105.907
Early stopping applied (best metric=0.9972450733184814)
Finished Training
Total time taken: 7.2761549949646
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3226.885
[2,     1] loss: 3235.411
[3,     1] loss: 3237.761
[4,     1] loss: 3217.996
[5,     1] loss: 3231.497
[6,     1] loss: 3230.706
[7,     1] loss: 3230.329
[8,     1] loss: 3230.010
[9,     1] loss: 3217.880
[10,     1] loss: 3227.854
[11,     1] loss: 3234.456
[12,     1] loss: 3224.014
[13,     1] loss: 3216.049
[14,     1] loss: 3222.132
[15,     1] loss: 3222.325
[16,     1] loss: 3224.619
[17,     1] loss: 3220.088
[18,     1] loss: 3222.258
[19,     1] loss: 3220.888
[20,     1] loss: 3213.035
[21,     1] loss: 3208.155
[22,     1] loss: 3205.903
[23,     1] loss: 3198.139
[24,     1] loss: 3182.490
[25,     1] loss: 3154.442
[26,     1] loss: 3100.670
[27,     1] loss: 3093.933
[28,     1] loss: 2952.104
[29,     1] loss: 2874.233
[30,     1] loss: 2859.072
[31,     1] loss: 2837.050
[32,     1] loss: 2750.667
[33,     1] loss: 2963.400
[34,     1] loss: 2764.128
[35,     1] loss: 2633.209
[36,     1] loss: 2635.153
[37,     1] loss: 2603.948
[38,     1] loss: 2517.765
[39,     1] loss: 2549.708
[40,     1] loss: 2490.875
[41,     1] loss: 2382.553
[42,     1] loss: 2397.990
[43,     1] loss: 2304.948
[44,     1] loss: 2229.836
[45,     1] loss: 2207.043
[46,     1] loss: 2373.366
[47,     1] loss: 2162.088
[48,     1] loss: 2275.878
[49,     1] loss: 2011.231
[50,     1] loss: 2065.936
[51,     1] loss: 2071.015
[52,     1] loss: 2272.546
[53,     1] loss: 2644.615
[54,     1] loss: 2047.459
[55,     1] loss: 2341.845
[56,     1] loss: 2085.946
[57,     1] loss: 2296.043
[58,     1] loss: 2144.350
[59,     1] loss: 2047.665
[60,     1] loss: 2070.578
[61,     1] loss: 2056.867
[62,     1] loss: 1900.307
[63,     1] loss: 2150.099
[64,     1] loss: 2091.665
[65,     1] loss: 1747.185
[66,     1] loss: 2151.469
[67,     1] loss: 1691.731
[68,     1] loss: 1933.821
[69,     1] loss: 1756.481
[70,     1] loss: 1703.380
[71,     1] loss: 1696.374
[72,     1] loss: 1486.744
[73,     1] loss: 1364.814
[74,     1] loss: 1388.552
[75,     1] loss: 1347.855
[76,     1] loss: 1317.636
[77,     1] loss: 2094.252
Early stopping applied (best metric=0.7604524493217468)
Finished Training
Total time taken: 13.772410154342651
{'Hydroxylation-K Validation Accuracy': 0.6955378250591017, 'Hydroxylation-K Validation Sensitivity': 0.7925925925925926, 'Hydroxylation-K Validation Specificity': 0.6719298245614035, 'Hydroxylation-K Validation Precision': 0.41924472938156376, 'Hydroxylation-K AUC ROC': 0.8282456140350877, 'Hydroxylation-K AUC PR': 0.6297300772624129, 'Hydroxylation-K MCC': 0.39580375005687835, 'Hydroxylation-K F1': 0.5362074874738866, 'Validation Loss (Hydroxylation-K)': 0.42639314333597816, 'Hydroxylation-P Validation Accuracy': 0.7115856555504797, 'Hydroxylation-P Validation Sensitivity': 0.7235978835978836, 'Hydroxylation-P Validation Specificity': 0.7090752655992818, 'Hydroxylation-P Validation Precision': 0.37789098851877007, 'Hydroxylation-P AUC ROC': 0.7755376866359958, 'Hydroxylation-P AUC PR': 0.5201174918237036, 'Hydroxylation-P MCC': 0.3565731984480139, 'Hydroxylation-P F1': 0.48736966590501435, 'Validation Loss (Hydroxylation-P)': 0.44527137875556944, 'Validation Loss (total)': 0.8716645240783691, 'TimeToTrain': 9.689932012557984}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00990356939119388,
 'learning_rate_Hydroxylation-K': 0.001524580301250472,
 'learning_rate_Hydroxylation-P': 0.009384682604206503,
 'log_base': 2.7749061019124897,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 824239459,
 'sample_weights': [9.944443954469332, 1.2404726446149141],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3943781336190995,
 'weight_decay_Hydroxylation-K': 1.8392918104392408,
 'weight_decay_Hydroxylation-P': 2.0615196825057236}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.170
[2,     1] loss: 1259.532
[3,     1] loss: 1252.073
[4,     1] loss: 1257.185
[5,     1] loss: 1255.032
[6,     1] loss: 1251.571
[7,     1] loss: 1251.686
[8,     1] loss: 1253.314
[9,     1] loss: 1251.268
[10,     1] loss: 1249.076
[11,     1] loss: 1249.536
[12,     1] loss: 1245.288
[13,     1] loss: 1245.528
[14,     1] loss: 1243.111
[15,     1] loss: 1235.807
[16,     1] loss: 1226.633
[17,     1] loss: 1205.810
[18,     1] loss: 1186.997
[19,     1] loss: 1150.494
[20,     1] loss: 1127.410
[21,     1] loss: 1114.940
[22,     1] loss: 1062.099
[23,     1] loss: 1066.409
[24,     1] loss: 1060.551
[25,     1] loss: 1088.478
[26,     1] loss: 1048.101
[27,     1] loss: 1019.400
[28,     1] loss: 1046.286
[29,     1] loss: 1020.459
[30,     1] loss: 994.815
[31,     1] loss: 999.373
[32,     1] loss: 1023.686
[33,     1] loss: 999.259
[34,     1] loss: 1003.368
[35,     1] loss: 949.962
[36,     1] loss: 987.848
[37,     1] loss: 927.732
[38,     1] loss: 983.869
[39,     1] loss: 951.098
[40,     1] loss: 925.304
[41,     1] loss: 935.835
[42,     1] loss: 903.392
[43,     1] loss: 926.562
[44,     1] loss: 857.834
[45,     1] loss: 882.413
[46,     1] loss: 880.826
[47,     1] loss: 829.837
[48,     1] loss: 802.794
[49,     1] loss: 768.333
[50,     1] loss: 835.497
[51,     1] loss: 1323.696
[52,     1] loss: 1558.413
[53,     1] loss: 1011.552
[54,     1] loss: 935.935
[55,     1] loss: 1031.078
[56,     1] loss: 1083.363
[57,     1] loss: 1072.952
[58,     1] loss: 1062.566
[59,     1] loss: 1061.837
[60,     1] loss: 1064.391
[61,     1] loss: 1033.322
[62,     1] loss: 1019.507
[63,     1] loss: 1000.076
[64,     1] loss: 963.869
[65,     1] loss: 980.855
[66,     1] loss: 944.814
[67,     1] loss: 952.653
[68,     1] loss: 931.972
Early stopping applied (best metric=0.7249197959899902)
Finished Training
Total time taken: 11.575326204299927
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.658
[2,     1] loss: 1262.280
[3,     1] loss: 1264.070
[4,     1] loss: 1253.298
[5,     1] loss: 1253.129
[6,     1] loss: 1254.745
[7,     1] loss: 1252.769
[8,     1] loss: 1253.747
[9,     1] loss: 1252.977
[10,     1] loss: 1254.277
[11,     1] loss: 1249.969
[12,     1] loss: 1253.257
[13,     1] loss: 1251.603
[14,     1] loss: 1251.736
[15,     1] loss: 1250.689
[16,     1] loss: 1251.374
[17,     1] loss: 1251.216
[18,     1] loss: 1248.617
[19,     1] loss: 1248.022
[20,     1] loss: 1244.117
[21,     1] loss: 1234.117
[22,     1] loss: 1224.844
[23,     1] loss: 1197.042
[24,     1] loss: 1180.915
[25,     1] loss: 1128.031
[26,     1] loss: 1131.653
[27,     1] loss: 1104.248
[28,     1] loss: 1045.799
[29,     1] loss: 1059.660
[30,     1] loss: 1022.034
[31,     1] loss: 1092.191
[32,     1] loss: 1027.219
[33,     1] loss: 1099.143
[34,     1] loss: 1025.111
[35,     1] loss: 1080.677
[36,     1] loss: 1011.827
[37,     1] loss: 1027.102
[38,     1] loss: 1017.063
[39,     1] loss: 1000.100
[40,     1] loss: 1014.969
[41,     1] loss: 979.179
[42,     1] loss: 962.618
[43,     1] loss: 923.931
[44,     1] loss: 984.194
[45,     1] loss: 982.232
[46,     1] loss: 972.125
[47,     1] loss: 900.073
[48,     1] loss: 887.790
[49,     1] loss: 949.308
[50,     1] loss: 922.516
[51,     1] loss: 896.618
[52,     1] loss: 950.127
[53,     1] loss: 949.780
[54,     1] loss: 897.643
[55,     1] loss: 864.919
[56,     1] loss: 867.442
[57,     1] loss: 892.865
[58,     1] loss: 836.417
[59,     1] loss: 867.757
[60,     1] loss: 946.296
[61,     1] loss: 835.258
[62,     1] loss: 820.669
[63,     1] loss: 865.259
[64,     1] loss: 764.874
[65,     1] loss: 960.750
[66,     1] loss: 892.991
[67,     1] loss: 859.949
[68,     1] loss: 910.813
[69,     1] loss: 819.353
[70,     1] loss: 849.138
[71,     1] loss: 806.327
[72,     1] loss: 782.476
[73,     1] loss: 813.131
[74,     1] loss: 735.790
[75,     1] loss: 737.453
[76,     1] loss: 786.580
[77,     1] loss: 679.891
[78,     1] loss: 654.038
[79,     1] loss: 847.670
[80,     1] loss: 704.076
[81,     1] loss: 601.692
[82,     1] loss: 621.319
[83,     1] loss: 588.586
Early stopping applied (best metric=0.814166784286499)
Finished Training
Total time taken: 13.032020568847656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.378
[2,     1] loss: 1257.469
[3,     1] loss: 1262.164
[4,     1] loss: 1251.975
[5,     1] loss: 1260.276
[6,     1] loss: 1253.464
[7,     1] loss: 1254.436
[8,     1] loss: 1254.237
[9,     1] loss: 1254.323
[10,     1] loss: 1253.349
[11,     1] loss: 1252.623
[12,     1] loss: 1250.442
[13,     1] loss: 1252.448
[14,     1] loss: 1251.422
[15,     1] loss: 1252.455
[16,     1] loss: 1250.252
[17,     1] loss: 1246.920
[18,     1] loss: 1246.543
[19,     1] loss: 1245.057
[20,     1] loss: 1239.157
[21,     1] loss: 1226.467
[22,     1] loss: 1206.876
[23,     1] loss: 1183.987
[24,     1] loss: 1133.412
[25,     1] loss: 1077.391
[26,     1] loss: 1085.150
[27,     1] loss: 1045.031
[28,     1] loss: 991.278
[29,     1] loss: 1011.624
[30,     1] loss: 1070.129
[31,     1] loss: 1006.821
[32,     1] loss: 984.543
[33,     1] loss: 975.166
[34,     1] loss: 997.512
[35,     1] loss: 1009.416
[36,     1] loss: 940.894
[37,     1] loss: 966.447
[38,     1] loss: 941.062
[39,     1] loss: 876.088
[40,     1] loss: 939.021
[41,     1] loss: 927.292
[42,     1] loss: 897.168
[43,     1] loss: 897.546
[44,     1] loss: 933.933
[45,     1] loss: 857.955
[46,     1] loss: 899.230
[47,     1] loss: 827.556
[48,     1] loss: 870.718
[49,     1] loss: 765.772
[50,     1] loss: 871.320
[51,     1] loss: 810.513
[52,     1] loss: 800.522
[53,     1] loss: 808.867
[54,     1] loss: 802.109
[55,     1] loss: 866.026
[56,     1] loss: 771.574
[57,     1] loss: 802.146
[58,     1] loss: 721.814
Early stopping applied (best metric=0.9847959280014038)
Finished Training
Total time taken: 9.91670298576355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.568
[2,     1] loss: 1253.837
[3,     1] loss: 1258.236
[4,     1] loss: 1252.657
[5,     1] loss: 1249.002
[6,     1] loss: 1255.685
[7,     1] loss: 1249.693
[8,     1] loss: 1248.639
[9,     1] loss: 1243.089
[10,     1] loss: 1238.713
[11,     1] loss: 1220.602
[12,     1] loss: 1210.368
[13,     1] loss: 1176.008
[14,     1] loss: 1149.483
[15,     1] loss: 1095.370
[16,     1] loss: 1069.243
[17,     1] loss: 1057.753
[18,     1] loss: 1024.488
[19,     1] loss: 1030.130
[20,     1] loss: 1032.765
[21,     1] loss: 988.046
[22,     1] loss: 959.937
[23,     1] loss: 1009.149
[24,     1] loss: 975.031
[25,     1] loss: 951.187
[26,     1] loss: 939.564
[27,     1] loss: 968.113
[28,     1] loss: 904.419
[29,     1] loss: 903.678
[30,     1] loss: 916.350
[31,     1] loss: 917.772
[32,     1] loss: 851.447
[33,     1] loss: 918.468
[34,     1] loss: 882.476
[35,     1] loss: 1119.775
[36,     1] loss: 924.273
[37,     1] loss: 884.610
[38,     1] loss: 844.032
[39,     1] loss: 889.398
[40,     1] loss: 844.938
[41,     1] loss: 867.728
[42,     1] loss: 834.585
[43,     1] loss: 846.318
[44,     1] loss: 796.128
[45,     1] loss: 853.953
[46,     1] loss: 805.673
[47,     1] loss: 808.377
[48,     1] loss: 801.372
[49,     1] loss: 739.704
[50,     1] loss: 774.676
[51,     1] loss: 825.498
[52,     1] loss: 719.365
[53,     1] loss: 735.583
[54,     1] loss: 816.934
[55,     1] loss: 707.388
[56,     1] loss: 725.299
[57,     1] loss: 687.872
[58,     1] loss: 670.037
[59,     1] loss: 709.853
[60,     1] loss: 958.864
[61,     1] loss: 877.069
[62,     1] loss: 663.245
[63,     1] loss: 728.089
[64,     1] loss: 658.036
[65,     1] loss: 685.286
[66,     1] loss: 632.262
[67,     1] loss: 712.059
[68,     1] loss: 588.125
Early stopping applied (best metric=0.8332722187042236)
Finished Training
Total time taken: 11.288346529006958
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1256.594
[2,     1] loss: 1273.832
[3,     1] loss: 1255.017
[4,     1] loss: 1261.526
[5,     1] loss: 1258.023
[6,     1] loss: 1258.054
[7,     1] loss: 1255.138
[8,     1] loss: 1254.976
[9,     1] loss: 1255.659
[10,     1] loss: 1254.172
[11,     1] loss: 1256.745
[12,     1] loss: 1254.897
[13,     1] loss: 1253.914
[14,     1] loss: 1254.431
[15,     1] loss: 1254.558
[16,     1] loss: 1254.192
[17,     1] loss: 1254.325
[18,     1] loss: 1253.736
[19,     1] loss: 1254.329
[20,     1] loss: 1253.485
[21,     1] loss: 1253.337
[22,     1] loss: 1253.746
[23,     1] loss: 1253.501
[24,     1] loss: 1253.426
[25,     1] loss: 1252.490
[26,     1] loss: 1251.315
[27,     1] loss: 1250.357
[28,     1] loss: 1246.768
[29,     1] loss: 1237.925
[30,     1] loss: 1226.113
[31,     1] loss: 1202.520
[32,     1] loss: 1174.699
[33,     1] loss: 1145.349
[34,     1] loss: 1111.719
[35,     1] loss: 1068.230
[36,     1] loss: 1048.190
[37,     1] loss: 1081.714
[38,     1] loss: 1039.415
[39,     1] loss: 1050.523
[40,     1] loss: 1004.807
[41,     1] loss: 986.243
[42,     1] loss: 1018.923
[43,     1] loss: 1018.011
[44,     1] loss: 1035.982
[45,     1] loss: 1017.813
[46,     1] loss: 991.964
[47,     1] loss: 982.510
[48,     1] loss: 993.085
[49,     1] loss: 906.994
[50,     1] loss: 954.556
[51,     1] loss: 921.500
[52,     1] loss: 856.048
[53,     1] loss: 863.050
[54,     1] loss: 972.506
[55,     1] loss: 936.720
[56,     1] loss: 906.560
[57,     1] loss: 913.186
[58,     1] loss: 884.554
[59,     1] loss: 840.638
[60,     1] loss: 834.600
[61,     1] loss: 863.102
[62,     1] loss: 829.848
[63,     1] loss: 818.725
[64,     1] loss: 863.844
[65,     1] loss: 1031.827
[66,     1] loss: 1133.017
[67,     1] loss: 827.971
[68,     1] loss: 942.048
[69,     1] loss: 946.530
[70,     1] loss: 872.730
[71,     1] loss: 915.242
[72,     1] loss: 936.862
[73,     1] loss: 855.548
[74,     1] loss: 832.479
[75,     1] loss: 906.270
[76,     1] loss: 793.971
[77,     1] loss: 859.752
[78,     1] loss: 778.088
[79,     1] loss: 808.879
[80,     1] loss: 769.166
[81,     1] loss: 732.854
[82,     1] loss: 754.924
[83,     1] loss: 778.849
[84,     1] loss: 699.390
[85,     1] loss: 713.449
[86,     1] loss: 745.965
[87,     1] loss: 761.068
[88,     1] loss: 920.383
[89,     1] loss: 820.696
[90,     1] loss: 704.313
[91,     1] loss: 772.253
[92,     1] loss: 671.951
[93,     1] loss: 786.847
[94,     1] loss: 728.521
[95,     1] loss: 690.209
[96,     1] loss: 788.279
[97,     1] loss: 757.715
[98,     1] loss: 667.987
[99,     1] loss: 835.351
[100,     1] loss: 872.537
[101,     1] loss: 765.758
[102,     1] loss: 733.438
[103,     1] loss: 743.285
Early stopping applied (best metric=0.7618930339813232)
Finished Training
Total time taken: 19.064106941223145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.323
[2,     1] loss: 1274.025
[3,     1] loss: 1258.812
[4,     1] loss: 1254.653
[5,     1] loss: 1253.183
[6,     1] loss: 1251.884
[7,     1] loss: 1251.126
[8,     1] loss: 1253.364
[9,     1] loss: 1253.128
[10,     1] loss: 1255.796
[11,     1] loss: 1253.963
[12,     1] loss: 1251.701
[13,     1] loss: 1251.407
[14,     1] loss: 1253.782
[15,     1] loss: 1253.049
[16,     1] loss: 1253.311
[17,     1] loss: 1253.474
[18,     1] loss: 1252.859
[19,     1] loss: 1252.399
[20,     1] loss: 1251.068
[21,     1] loss: 1251.205
[22,     1] loss: 1251.916
[23,     1] loss: 1250.051
[24,     1] loss: 1248.438
[25,     1] loss: 1244.583
[26,     1] loss: 1235.052
[27,     1] loss: 1223.573
[28,     1] loss: 1198.578
[29,     1] loss: 1167.459
[30,     1] loss: 1148.629
[31,     1] loss: 1102.030
[32,     1] loss: 1070.970
[33,     1] loss: 1094.704
[34,     1] loss: 1135.704
[35,     1] loss: 1047.344
[36,     1] loss: 1078.424
[37,     1] loss: 1068.570
[38,     1] loss: 1023.164
[39,     1] loss: 1034.647
[40,     1] loss: 1022.422
[41,     1] loss: 1042.310
[42,     1] loss: 968.870
[43,     1] loss: 1016.707
[44,     1] loss: 969.069
[45,     1] loss: 1042.539
[46,     1] loss: 947.908
[47,     1] loss: 970.385
[48,     1] loss: 869.875
[49,     1] loss: 981.384
[50,     1] loss: 887.264
[51,     1] loss: 989.235
[52,     1] loss: 921.138
[53,     1] loss: 947.617
[54,     1] loss: 899.536
[55,     1] loss: 959.471
[56,     1] loss: 917.331
[57,     1] loss: 930.525
[58,     1] loss: 910.675
[59,     1] loss: 854.807
[60,     1] loss: 939.167
[61,     1] loss: 865.261
[62,     1] loss: 888.060
[63,     1] loss: 867.783
[64,     1] loss: 827.842
[65,     1] loss: 859.039
[66,     1] loss: 799.600
[67,     1] loss: 793.310
[68,     1] loss: 1012.521
[69,     1] loss: 994.060
[70,     1] loss: 812.676
[71,     1] loss: 828.098
[72,     1] loss: 868.214
[73,     1] loss: 808.699
[74,     1] loss: 797.783
[75,     1] loss: 773.982
[76,     1] loss: 728.305
[77,     1] loss: 726.160
[78,     1] loss: 724.502
[79,     1] loss: 672.963
[80,     1] loss: 699.827
[81,     1] loss: 738.677
[82,     1] loss: 1505.359
[83,     1] loss: 942.299
[84,     1] loss: 818.063
[85,     1] loss: 854.459
[86,     1] loss: 907.084
Early stopping applied (best metric=0.7925964593887329)
Finished Training
Total time taken: 14.986339092254639
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.841
[2,     1] loss: 1256.234
[3,     1] loss: 1256.201
[4,     1] loss: 1247.650
[5,     1] loss: 1257.552
[6,     1] loss: 1247.878
[7,     1] loss: 1239.492
[8,     1] loss: 1239.533
[9,     1] loss: 1228.067
[10,     1] loss: 1207.639
[11,     1] loss: 1166.595
[12,     1] loss: 1105.931
[13,     1] loss: 1070.301
[14,     1] loss: 1045.746
[15,     1] loss: 1201.974
[16,     1] loss: 1096.893
[17,     1] loss: 1017.016
[18,     1] loss: 991.493
[19,     1] loss: 1058.050
[20,     1] loss: 1047.941
[21,     1] loss: 1039.300
[22,     1] loss: 1044.543
[23,     1] loss: 1009.479
[24,     1] loss: 986.623
[25,     1] loss: 983.971
[26,     1] loss: 973.583
[27,     1] loss: 938.866
[28,     1] loss: 965.023
[29,     1] loss: 938.730
[30,     1] loss: 927.588
[31,     1] loss: 890.193
[32,     1] loss: 931.700
[33,     1] loss: 854.151
[34,     1] loss: 868.784
[35,     1] loss: 910.634
[36,     1] loss: 881.773
[37,     1] loss: 950.379
[38,     1] loss: 796.800
[39,     1] loss: 897.978
[40,     1] loss: 917.392
[41,     1] loss: 829.869
[42,     1] loss: 817.029
[43,     1] loss: 761.071
[44,     1] loss: 922.470
[45,     1] loss: 803.614
[46,     1] loss: 811.406
[47,     1] loss: 768.479
[48,     1] loss: 757.550
[49,     1] loss: 749.916
[50,     1] loss: 729.410
[51,     1] loss: 709.930
[52,     1] loss: 706.893
[53,     1] loss: 694.546
[54,     1] loss: 804.530
[55,     1] loss: 1458.353
[56,     1] loss: 946.622
[57,     1] loss: 1000.694
[58,     1] loss: 971.456
[59,     1] loss: 980.258
[60,     1] loss: 993.095
[61,     1] loss: 987.756
[62,     1] loss: 969.057
[63,     1] loss: 887.723
[64,     1] loss: 863.041
[65,     1] loss: 927.601
Early stopping applied (best metric=0.8684067726135254)
Finished Training
Total time taken: 10.944538354873657
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.807
[2,     1] loss: 1255.781
[3,     1] loss: 1262.211
[4,     1] loss: 1251.204
[5,     1] loss: 1253.348
[6,     1] loss: 1252.801
[7,     1] loss: 1253.550
[8,     1] loss: 1249.803
[9,     1] loss: 1255.580
[10,     1] loss: 1249.010
[11,     1] loss: 1245.976
[12,     1] loss: 1246.611
[13,     1] loss: 1240.291
[14,     1] loss: 1230.594
[15,     1] loss: 1215.072
[16,     1] loss: 1193.165
[17,     1] loss: 1131.368
[18,     1] loss: 1093.677
[19,     1] loss: 1083.682
[20,     1] loss: 1055.372
[21,     1] loss: 1024.870
[22,     1] loss: 1033.907
[23,     1] loss: 1100.915
[24,     1] loss: 1039.045
[25,     1] loss: 1035.651
[26,     1] loss: 1013.746
[27,     1] loss: 985.891
[28,     1] loss: 1007.238
[29,     1] loss: 989.487
[30,     1] loss: 1000.439
[31,     1] loss: 963.742
[32,     1] loss: 1027.110
[33,     1] loss: 955.548
[34,     1] loss: 994.293
[35,     1] loss: 949.408
[36,     1] loss: 929.097
[37,     1] loss: 939.790
[38,     1] loss: 960.324
[39,     1] loss: 903.524
[40,     1] loss: 901.626
[41,     1] loss: 875.665
[42,     1] loss: 880.534
[43,     1] loss: 982.339
[44,     1] loss: 902.382
[45,     1] loss: 852.318
[46,     1] loss: 829.403
[47,     1] loss: 813.720
[48,     1] loss: 818.909
[49,     1] loss: 770.274
[50,     1] loss: 805.772
[51,     1] loss: 884.216
[52,     1] loss: 1134.775
[53,     1] loss: 786.281
[54,     1] loss: 973.251
[55,     1] loss: 838.721
[56,     1] loss: 921.964
[57,     1] loss: 879.183
[58,     1] loss: 827.296
[59,     1] loss: 894.280
[60,     1] loss: 819.518
[61,     1] loss: 811.817
[62,     1] loss: 805.583
[63,     1] loss: 775.291
[64,     1] loss: 792.083
[65,     1] loss: 713.068
[66,     1] loss: 743.689
[67,     1] loss: 690.786
[68,     1] loss: 703.967
[69,     1] loss: 683.277
[70,     1] loss: 698.882
[71,     1] loss: 711.427
[72,     1] loss: 822.968
[73,     1] loss: 901.470
[74,     1] loss: 651.423
[75,     1] loss: 904.550
[76,     1] loss: 705.382
[77,     1] loss: 786.772
[78,     1] loss: 691.698
[79,     1] loss: 738.268
[80,     1] loss: 655.415
[81,     1] loss: 714.225
[82,     1] loss: 663.825
[83,     1] loss: 623.467
[84,     1] loss: 651.909
[85,     1] loss: 590.663
[86,     1] loss: 694.738
[87,     1] loss: 871.763
[88,     1] loss: 548.059
[89,     1] loss: 971.882
[90,     1] loss: 806.563
[91,     1] loss: 889.687
[92,     1] loss: 675.143
[93,     1] loss: 810.526
Early stopping applied (best metric=0.8235706090927124)
Finished Training
Total time taken: 12.732330322265625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.812
[2,     1] loss: 1251.457
[3,     1] loss: 1253.338
[4,     1] loss: 1252.172
[5,     1] loss: 1257.089
[6,     1] loss: 1254.684
[7,     1] loss: 1253.923
[8,     1] loss: 1251.273
[9,     1] loss: 1249.627
[10,     1] loss: 1244.152
[11,     1] loss: 1238.857
[12,     1] loss: 1235.151
[13,     1] loss: 1220.834
[14,     1] loss: 1197.736
[15,     1] loss: 1151.989
[16,     1] loss: 1141.126
[17,     1] loss: 1110.360
[18,     1] loss: 1058.679
[19,     1] loss: 1137.920
[20,     1] loss: 1037.791
[21,     1] loss: 1079.622
[22,     1] loss: 1041.969
[23,     1] loss: 1054.192
[24,     1] loss: 992.856
[25,     1] loss: 1037.963
[26,     1] loss: 997.835
[27,     1] loss: 987.017
[28,     1] loss: 985.822
[29,     1] loss: 922.240
[30,     1] loss: 927.705
[31,     1] loss: 939.646
[32,     1] loss: 928.869
[33,     1] loss: 887.509
[34,     1] loss: 898.808
[35,     1] loss: 901.917
[36,     1] loss: 904.149
[37,     1] loss: 899.217
[38,     1] loss: 875.691
[39,     1] loss: 886.463
[40,     1] loss: 840.389
[41,     1] loss: 815.607
[42,     1] loss: 884.669
[43,     1] loss: 845.204
[44,     1] loss: 835.862
[45,     1] loss: 786.868
[46,     1] loss: 787.118
[47,     1] loss: 869.674
[48,     1] loss: 735.796
[49,     1] loss: 790.987
[50,     1] loss: 793.521
Early stopping applied (best metric=0.8611966967582703)
Finished Training
Total time taken: 8.498226642608643
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1255.668
[2,     1] loss: 1252.505
[3,     1] loss: 1260.741
[4,     1] loss: 1252.636
[5,     1] loss: 1256.136
[6,     1] loss: 1257.749
[7,     1] loss: 1252.883
[8,     1] loss: 1254.259
[9,     1] loss: 1252.356
[10,     1] loss: 1249.826
[11,     1] loss: 1245.438
[12,     1] loss: 1239.441
[13,     1] loss: 1225.907
[14,     1] loss: 1194.438
[15,     1] loss: 1181.728
[16,     1] loss: 1150.239
[17,     1] loss: 1141.329
[18,     1] loss: 1107.823
[19,     1] loss: 1051.624
[20,     1] loss: 1033.238
[21,     1] loss: 1053.826
[22,     1] loss: 1097.936
[23,     1] loss: 981.950
[24,     1] loss: 1030.109
[25,     1] loss: 1014.104
[26,     1] loss: 994.542
[27,     1] loss: 1014.465
[28,     1] loss: 1033.321
[29,     1] loss: 1001.084
[30,     1] loss: 957.616
[31,     1] loss: 965.331
[32,     1] loss: 978.426
[33,     1] loss: 893.451
[34,     1] loss: 953.379
[35,     1] loss: 900.975
[36,     1] loss: 953.817
[37,     1] loss: 860.899
[38,     1] loss: 899.498
[39,     1] loss: 919.843
[40,     1] loss: 995.009
[41,     1] loss: 879.763
[42,     1] loss: 886.980
[43,     1] loss: 904.642
[44,     1] loss: 843.543
[45,     1] loss: 880.433
[46,     1] loss: 833.488
[47,     1] loss: 898.642
[48,     1] loss: 833.528
[49,     1] loss: 827.500
[50,     1] loss: 941.815
[51,     1] loss: 844.914
[52,     1] loss: 865.931
[53,     1] loss: 835.989
[54,     1] loss: 804.125
[55,     1] loss: 808.062
[56,     1] loss: 748.131
Early stopping applied (best metric=0.7943693399429321)
Finished Training
Total time taken: 7.726740598678589
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.218
[2,     1] loss: 1250.678
[3,     1] loss: 1257.464
[4,     1] loss: 1257.348
[5,     1] loss: 1248.858
[6,     1] loss: 1254.513
[7,     1] loss: 1253.495
[8,     1] loss: 1253.248
[9,     1] loss: 1249.326
[10,     1] loss: 1246.639
[11,     1] loss: 1245.856
[12,     1] loss: 1237.522
[13,     1] loss: 1224.463
[14,     1] loss: 1200.506
[15,     1] loss: 1160.181
[16,     1] loss: 1131.200
[17,     1] loss: 1101.794
[18,     1] loss: 1059.276
[19,     1] loss: 997.868
[20,     1] loss: 1028.261
[21,     1] loss: 1009.487
[22,     1] loss: 999.832
[23,     1] loss: 944.275
[24,     1] loss: 984.287
[25,     1] loss: 991.567
[26,     1] loss: 994.447
[27,     1] loss: 990.732
[28,     1] loss: 929.797
[29,     1] loss: 955.663
[30,     1] loss: 955.742
[31,     1] loss: 952.120
[32,     1] loss: 917.557
[33,     1] loss: 930.741
[34,     1] loss: 877.415
[35,     1] loss: 883.134
[36,     1] loss: 881.147
[37,     1] loss: 814.384
[38,     1] loss: 943.277
[39,     1] loss: 1213.327
[40,     1] loss: 849.344
[41,     1] loss: 1105.832
[42,     1] loss: 911.709
[43,     1] loss: 959.929
[44,     1] loss: 1021.242
[45,     1] loss: 952.397
[46,     1] loss: 886.310
[47,     1] loss: 873.657
[48,     1] loss: 954.754
[49,     1] loss: 873.419
[50,     1] loss: 866.342
[51,     1] loss: 905.759
[52,     1] loss: 825.010
[53,     1] loss: 812.190
[54,     1] loss: 870.809
[55,     1] loss: 785.929
[56,     1] loss: 867.274
[57,     1] loss: 765.680
[58,     1] loss: 795.760
[59,     1] loss: 729.637
[60,     1] loss: 795.476
[61,     1] loss: 707.213
[62,     1] loss: 796.385
[63,     1] loss: 699.180
[64,     1] loss: 899.835
[65,     1] loss: 736.141
[66,     1] loss: 818.056
[67,     1] loss: 703.108
[68,     1] loss: 815.699
[69,     1] loss: 682.611
[70,     1] loss: 705.417
[71,     1] loss: 643.799
[72,     1] loss: 698.167
[73,     1] loss: 705.454
[74,     1] loss: 594.396
[75,     1] loss: 724.041
[76,     1] loss: 833.635
[77,     1] loss: 598.358
[78,     1] loss: 818.348
[79,     1] loss: 591.026
[80,     1] loss: 671.586
[81,     1] loss: 596.113
[82,     1] loss: 617.611
[83,     1] loss: 588.733
[84,     1] loss: 530.573
[85,     1] loss: 532.596
[86,     1] loss: 476.653
[87,     1] loss: 507.942
[88,     1] loss: 520.856
[89,     1] loss: 934.825
[90,     1] loss: 2456.414
[91,     1] loss: 1619.764
[92,     1] loss: 1046.887
[93,     1] loss: 1136.426
[94,     1] loss: 1107.821
[95,     1] loss: 1228.082
[96,     1] loss: 1094.813
[97,     1] loss: 1118.968
[98,     1] loss: 1108.356
[99,     1] loss: 1094.795
[100,     1] loss: 1091.455
[101,     1] loss: 1067.853
[102,     1] loss: 1083.664
Early stopping applied (best metric=0.7925674915313721)
Finished Training
Total time taken: 14.29238247871399
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.413
[2,     1] loss: 1250.523
[3,     1] loss: 1254.167
[4,     1] loss: 1258.031
[5,     1] loss: 1254.536
[6,     1] loss: 1251.665
[7,     1] loss: 1255.530
[8,     1] loss: 1251.567
[9,     1] loss: 1251.862
[10,     1] loss: 1250.958
[11,     1] loss: 1250.826
[12,     1] loss: 1247.306
[13,     1] loss: 1245.464
[14,     1] loss: 1243.540
[15,     1] loss: 1237.119
[16,     1] loss: 1224.259
[17,     1] loss: 1209.850
[18,     1] loss: 1170.142
[19,     1] loss: 1130.422
[20,     1] loss: 1103.391
[21,     1] loss: 1073.382
[22,     1] loss: 1022.543
[23,     1] loss: 1091.859
[24,     1] loss: 1052.216
[25,     1] loss: 1009.901
[26,     1] loss: 1001.892
[27,     1] loss: 1009.375
[28,     1] loss: 992.842
[29,     1] loss: 1026.879
[30,     1] loss: 957.885
[31,     1] loss: 971.177
[32,     1] loss: 963.833
[33,     1] loss: 956.515
[34,     1] loss: 917.254
[35,     1] loss: 936.428
[36,     1] loss: 935.372
[37,     1] loss: 924.544
[38,     1] loss: 917.411
[39,     1] loss: 936.883
[40,     1] loss: 907.433
[41,     1] loss: 829.366
[42,     1] loss: 873.381
[43,     1] loss: 855.025
[44,     1] loss: 826.895
[45,     1] loss: 846.246
[46,     1] loss: 894.646
[47,     1] loss: 875.656
[48,     1] loss: 855.684
[49,     1] loss: 813.695
[50,     1] loss: 757.509
[51,     1] loss: 751.713
[52,     1] loss: 800.786
[53,     1] loss: 918.286
[54,     1] loss: 968.639
[55,     1] loss: 759.597
[56,     1] loss: 922.739
[57,     1] loss: 797.924
[58,     1] loss: 846.534
[59,     1] loss: 840.333
[60,     1] loss: 801.046
[61,     1] loss: 781.119
[62,     1] loss: 814.902
[63,     1] loss: 743.754
[64,     1] loss: 783.660
[65,     1] loss: 698.212
[66,     1] loss: 718.430
[67,     1] loss: 728.159
[68,     1] loss: 652.939
[69,     1] loss: 706.659
[70,     1] loss: 747.193
[71,     1] loss: 630.161
[72,     1] loss: 735.622
[73,     1] loss: 841.755
[74,     1] loss: 599.001
[75,     1] loss: 881.212
[76,     1] loss: 736.077
[77,     1] loss: 809.087
[78,     1] loss: 655.649
[79,     1] loss: 849.118
[80,     1] loss: 664.923
[81,     1] loss: 774.855
[82,     1] loss: 663.981
[83,     1] loss: 650.760
[84,     1] loss: 721.955
[85,     1] loss: 636.546
[86,     1] loss: 699.038
[87,     1] loss: 590.561
[88,     1] loss: 614.193
[89,     1] loss: 644.498
[90,     1] loss: 536.871
[91,     1] loss: 645.727
[92,     1] loss: 478.140
[93,     1] loss: 532.266
[94,     1] loss: 658.446
[95,     1] loss: 651.539
[96,     1] loss: 471.115
[97,     1] loss: 549.399
[98,     1] loss: 484.263
[99,     1] loss: 449.314
[100,     1] loss: 517.432
[101,     1] loss: 531.626
[102,     1] loss: 529.445
[103,     1] loss: 473.465
[104,     1] loss: 423.574
Early stopping applied (best metric=0.8515351414680481)
Finished Training
Total time taken: 18.257494926452637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.629
[2,     1] loss: 1273.900
[3,     1] loss: 1259.774
[4,     1] loss: 1257.775
[5,     1] loss: 1255.303
[6,     1] loss: 1251.972
[7,     1] loss: 1254.250
[8,     1] loss: 1254.793
[9,     1] loss: 1254.068
[10,     1] loss: 1252.829
[11,     1] loss: 1251.828
[12,     1] loss: 1256.144
[13,     1] loss: 1252.185
[14,     1] loss: 1252.609
[15,     1] loss: 1253.109
[16,     1] loss: 1252.926
[17,     1] loss: 1254.332
[18,     1] loss: 1253.177
[19,     1] loss: 1251.864
[20,     1] loss: 1253.114
[21,     1] loss: 1252.918
[22,     1] loss: 1252.457
[23,     1] loss: 1253.963
[24,     1] loss: 1254.576
[25,     1] loss: 1252.584
[26,     1] loss: 1253.189
[27,     1] loss: 1252.958
[28,     1] loss: 1251.983
[29,     1] loss: 1252.902
[30,     1] loss: 1252.406
[31,     1] loss: 1252.880
[32,     1] loss: 1253.066
Early stopping applied (best metric=1.0920964479446411)
Finished Training
Total time taken: 5.691687107086182
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.633
[2,     1] loss: 1254.477
[3,     1] loss: 1250.490
[4,     1] loss: 1254.547
[5,     1] loss: 1249.492
[6,     1] loss: 1247.854
[7,     1] loss: 1251.588
[8,     1] loss: 1257.019
[9,     1] loss: 1244.675
[10,     1] loss: 1246.550
[11,     1] loss: 1248.571
[12,     1] loss: 1232.520
[13,     1] loss: 1216.781
[14,     1] loss: 1196.935
[15,     1] loss: 1182.764
[16,     1] loss: 1166.454
[17,     1] loss: 1098.930
[18,     1] loss: 1122.137
[19,     1] loss: 1080.324
[20,     1] loss: 1119.701
[21,     1] loss: 1049.124
[22,     1] loss: 1003.548
[23,     1] loss: 1079.109
[24,     1] loss: 1014.701
[25,     1] loss: 1031.607
[26,     1] loss: 1005.836
[27,     1] loss: 993.588
[28,     1] loss: 961.041
[29,     1] loss: 949.516
[30,     1] loss: 954.955
[31,     1] loss: 987.931
[32,     1] loss: 976.172
[33,     1] loss: 983.650
[34,     1] loss: 1043.758
[35,     1] loss: 970.642
[36,     1] loss: 1054.858
[37,     1] loss: 921.436
[38,     1] loss: 1017.465
[39,     1] loss: 957.142
[40,     1] loss: 905.229
[41,     1] loss: 997.727
[42,     1] loss: 953.881
[43,     1] loss: 928.019
[44,     1] loss: 866.854
[45,     1] loss: 882.116
[46,     1] loss: 904.210
[47,     1] loss: 821.959
[48,     1] loss: 860.456
[49,     1] loss: 868.839
[50,     1] loss: 841.737
[51,     1] loss: 891.722
[52,     1] loss: 866.448
[53,     1] loss: 797.957
[54,     1] loss: 796.251
[55,     1] loss: 858.499
[56,     1] loss: 923.859
[57,     1] loss: 778.296
[58,     1] loss: 835.174
[59,     1] loss: 746.929
[60,     1] loss: 767.896
[61,     1] loss: 865.241
[62,     1] loss: 659.079
[63,     1] loss: 770.438
[64,     1] loss: 738.397
[65,     1] loss: 759.798
[66,     1] loss: 806.337
[67,     1] loss: 654.287
Early stopping applied (best metric=0.6946718692779541)
Finished Training
Total time taken: 11.128790616989136
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1252.611
[2,     1] loss: 1258.875
[3,     1] loss: 1253.516
[4,     1] loss: 1257.327
[5,     1] loss: 1255.187
[6,     1] loss: 1256.280
[7,     1] loss: 1252.595
[8,     1] loss: 1252.501
[9,     1] loss: 1250.491
[10,     1] loss: 1247.923
[11,     1] loss: 1240.695
[12,     1] loss: 1237.629
[13,     1] loss: 1218.875
[14,     1] loss: 1199.112
[15,     1] loss: 1154.204
[16,     1] loss: 1110.552
[17,     1] loss: 1061.353
[18,     1] loss: 1061.822
[19,     1] loss: 1068.779
[20,     1] loss: 1038.115
[21,     1] loss: 979.473
[22,     1] loss: 1039.743
[23,     1] loss: 1015.915
[24,     1] loss: 980.689
[25,     1] loss: 972.177
[26,     1] loss: 974.118
[27,     1] loss: 944.324
[28,     1] loss: 951.643
[29,     1] loss: 924.801
[30,     1] loss: 914.759
[31,     1] loss: 891.624
[32,     1] loss: 861.473
[33,     1] loss: 946.242
[34,     1] loss: 924.395
[35,     1] loss: 921.883
[36,     1] loss: 887.741
[37,     1] loss: 886.714
[38,     1] loss: 864.459
[39,     1] loss: 854.564
[40,     1] loss: 822.705
[41,     1] loss: 867.787
[42,     1] loss: 825.559
[43,     1] loss: 843.049
[44,     1] loss: 783.131
[45,     1] loss: 864.308
[46,     1] loss: 821.793
[47,     1] loss: 788.588
[48,     1] loss: 773.754
[49,     1] loss: 797.991
[50,     1] loss: 869.388
[51,     1] loss: 984.595
[52,     1] loss: 729.349
[53,     1] loss: 859.684
[54,     1] loss: 799.355
[55,     1] loss: 826.124
[56,     1] loss: 762.768
[57,     1] loss: 793.738
[58,     1] loss: 741.376
[59,     1] loss: 739.190
[60,     1] loss: 714.426
[61,     1] loss: 746.790
[62,     1] loss: 750.287
[63,     1] loss: 627.893
[64,     1] loss: 703.673
[65,     1] loss: 646.454
[66,     1] loss: 637.837
[67,     1] loss: 878.777
[68,     1] loss: 1310.084
[69,     1] loss: 654.301
[70,     1] loss: 940.335
[71,     1] loss: 716.454
[72,     1] loss: 787.213
[73,     1] loss: 834.667
[74,     1] loss: 750.666
[75,     1] loss: 777.976
[76,     1] loss: 770.677
[77,     1] loss: 688.878
[78,     1] loss: 775.000
[79,     1] loss: 699.590
[80,     1] loss: 715.154
[81,     1] loss: 627.228
[82,     1] loss: 647.062
[83,     1] loss: 605.460
[84,     1] loss: 559.230
[85,     1] loss: 632.573
[86,     1] loss: 600.228
[87,     1] loss: 738.554
[88,     1] loss: 667.821
[89,     1] loss: 558.126
[90,     1] loss: 518.598
[91,     1] loss: 589.911
Early stopping applied (best metric=0.867445707321167)
Finished Training
Total time taken: 12.21628212928772
{'Hydroxylation-K Validation Accuracy': 0.7271867612293145, 'Hydroxylation-K Validation Sensitivity': 0.6614814814814814, 'Hydroxylation-K Validation Specificity': 0.743859649122807, 'Hydroxylation-K Validation Precision': 0.43488963049334567, 'Hydroxylation-K AUC ROC': 0.7865107212475634, 'Hydroxylation-K AUC PR': 0.596378775616738, 'Hydroxylation-K MCC': 0.3623020397643439, 'Hydroxylation-K F1': 0.5103981009828087, 'Validation Loss (Hydroxylation-K)': 0.446597013870875, 'Hydroxylation-P Validation Accuracy': 0.7456406950577805, 'Hydroxylation-P Validation Sensitivity': 0.7657671957671958, 'Hydroxylation-P Validation Specificity': 0.7415606763429597, 'Hydroxylation-P Validation Precision': 0.4373522766206983, 'Hydroxylation-P AUC ROC': 0.8278006730731929, 'Hydroxylation-P AUC PR': 0.5689375579090624, 'Hydroxylation-P MCC': 0.43050528783916997, 'Hydroxylation-P F1': 0.5416711443133105, 'Validation Loss (Hydroxylation-P)': 0.39056993325551354, 'Validation Loss (total)': 0.8371669530868531, 'TimeToTrain': 12.090087699890137}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005410537322324149,
 'learning_rate_Hydroxylation-K': 0.0067299130060228364,
 'learning_rate_Hydroxylation-P': 0.005007640008704388,
 'log_base': 1.1931820702160065,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2730683200,
 'sample_weights': [1.6369330374219604, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.606933238078179,
 'weight_decay_Hydroxylation-K': 5.391069222139824,
 'weight_decay_Hydroxylation-P': 2.4620677884253226}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3083.046
[2,     1] loss: 3072.562
[3,     1] loss: 3082.550
[4,     1] loss: 3056.995
[5,     1] loss: 3073.114
[6,     1] loss: 3063.604
[7,     1] loss: 3068.961
[8,     1] loss: 3061.780
[9,     1] loss: 3060.764
[10,     1] loss: 3060.065
[11,     1] loss: 3061.850
[12,     1] loss: 3063.605
[13,     1] loss: 3063.674
[14,     1] loss: 3069.492
[15,     1] loss: 3066.759
[16,     1] loss: 3066.119
[17,     1] loss: 3056.376
[18,     1] loss: 3059.283
[19,     1] loss: 3055.035
[20,     1] loss: 3051.974
[21,     1] loss: 3041.511
[22,     1] loss: 3032.279
[23,     1] loss: 3003.756
[24,     1] loss: 2977.145
[25,     1] loss: 2917.260
[26,     1] loss: 2898.503
[27,     1] loss: 2773.894
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008101844662379742,
 'learning_rate_Hydroxylation-K': 0.003490233737468989,
 'learning_rate_Hydroxylation-P': 0.005728102884371987,
 'log_base': 2.076498093134979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1585179189,
 'sample_weights': [9.45197447483519, 1.181542325355948],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.771534226773491,
 'weight_decay_Hydroxylation-K': 5.034467082106115,
 'weight_decay_Hydroxylation-P': 2.8502089247833586}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.148
[2,     1] loss: 1393.416
[3,     1] loss: 1388.699
[4,     1] loss: 1388.946
[5,     1] loss: 1392.291
[6,     1] loss: 1394.143
[7,     1] loss: 1388.828
[8,     1] loss: 1392.183
[9,     1] loss: 1387.813
[10,     1] loss: 1392.974
[11,     1] loss: 1391.989
[12,     1] loss: 1387.706
[13,     1] loss: 1389.543
[14,     1] loss: 1390.330
[15,     1] loss: 1391.656
[16,     1] loss: 1391.355
[17,     1] loss: 1390.874
[18,     1] loss: 1390.216
[19,     1] loss: 1388.260
[20,     1] loss: 1390.316
[21,     1] loss: 1389.612
[22,     1] loss: 1388.324
[23,     1] loss: 1390.420
[24,     1] loss: 1388.328
[25,     1] loss: 1390.590
[26,     1] loss: 1387.341
[27,     1] loss: 1389.986
[28,     1] loss: 1388.280
[29,     1] loss: 1386.657
[30,     1] loss: 1387.440
[31,     1] loss: 1385.571
[32,     1] loss: 1384.226
[33,     1] loss: 1379.673
[34,     1] loss: 1370.180
[35,     1] loss: 1357.681
[36,     1] loss: 1336.718
[37,     1] loss: 1296.434
[38,     1] loss: 1265.610
[39,     1] loss: 1242.791
[40,     1] loss: 1210.515
[41,     1] loss: 1209.716
[42,     1] loss: 1189.693
[43,     1] loss: 1231.773
[44,     1] loss: 1280.404
[45,     1] loss: 1137.394
[46,     1] loss: 1156.180
[47,     1] loss: 1124.813
[48,     1] loss: 1162.834
[49,     1] loss: 1130.875
[50,     1] loss: 1163.904
[51,     1] loss: 1170.021
[52,     1] loss: 1099.922
[53,     1] loss: 1102.062
[54,     1] loss: 1124.430
[55,     1] loss: 1035.443
[56,     1] loss: 1081.423
[57,     1] loss: 1055.470
[58,     1] loss: 1076.617
[59,     1] loss: 1062.993
[60,     1] loss: 1013.059
[61,     1] loss: 1050.614
[62,     1] loss: 993.126
[63,     1] loss: 1085.905
[64,     1] loss: 977.662
[65,     1] loss: 957.453
[66,     1] loss: 1002.158
[67,     1] loss: 1109.983
[68,     1] loss: 953.875
[69,     1] loss: 1010.068
[70,     1] loss: 885.913
[71,     1] loss: 903.230
[72,     1] loss: 882.694
[73,     1] loss: 885.432
[74,     1] loss: 809.759
[75,     1] loss: 885.536
[76,     1] loss: 1274.240
[77,     1] loss: 957.027
[78,     1] loss: 925.841
[79,     1] loss: 860.438
[80,     1] loss: 984.497
[81,     1] loss: 964.474
[82,     1] loss: 905.686
[83,     1] loss: 966.045
[84,     1] loss: 857.099
[85,     1] loss: 879.527
[86,     1] loss: 951.351
[87,     1] loss: 848.235
[88,     1] loss: 845.700
[89,     1] loss: 750.866
[90,     1] loss: 863.490
[91,     1] loss: 772.422
[92,     1] loss: 882.913
[93,     1] loss: 685.621
[94,     1] loss: 753.069
[95,     1] loss: 654.576
[96,     1] loss: 752.490
[97,     1] loss: 666.807
[98,     1] loss: 687.906
[99,     1] loss: 604.991
[100,     1] loss: 677.725
[101,     1] loss: 648.938
Early stopping applied (best metric=0.8138333559036255)
Finished Training
Total time taken: 13.46428370475769
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1388.398
[2,     1] loss: 1389.316
[3,     1] loss: 1398.112
[4,     1] loss: 1393.495
[5,     1] loss: 1394.277
[6,     1] loss: 1388.569
[7,     1] loss: 1386.095
[8,     1] loss: 1389.221
[9,     1] loss: 1384.975
[10,     1] loss: 1379.673
[11,     1] loss: 1376.211
[12,     1] loss: 1362.034
[13,     1] loss: 1342.663
[14,     1] loss: 1320.782
[15,     1] loss: 1279.062
[16,     1] loss: 1260.271
[17,     1] loss: 1242.610
[18,     1] loss: 1191.849
[19,     1] loss: 1190.108
[20,     1] loss: 1142.939
[21,     1] loss: 1137.541
[22,     1] loss: 1072.886
[23,     1] loss: 1134.521
[24,     1] loss: 1152.453
[25,     1] loss: 1168.440
[26,     1] loss: 1114.194
[27,     1] loss: 1072.965
[28,     1] loss: 1129.044
[29,     1] loss: 1020.292
[30,     1] loss: 1066.155
[31,     1] loss: 1072.986
[32,     1] loss: 1020.960
[33,     1] loss: 981.921
[34,     1] loss: 925.137
[35,     1] loss: 998.665
[36,     1] loss: 1027.524
[37,     1] loss: 955.144
[38,     1] loss: 948.060
[39,     1] loss: 939.473
[40,     1] loss: 958.822
[41,     1] loss: 963.352
[42,     1] loss: 892.068
[43,     1] loss: 936.294
[44,     1] loss: 874.607
Early stopping applied (best metric=1.0366795063018799)
Finished Training
Total time taken: 7.227151393890381
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.991
[2,     1] loss: 1391.719
[3,     1] loss: 1384.925
[4,     1] loss: 1386.797
[5,     1] loss: 1390.022
[6,     1] loss: 1395.723
[7,     1] loss: 1389.484
[8,     1] loss: 1385.799
[9,     1] loss: 1389.172
[10,     1] loss: 1386.589
[11,     1] loss: 1388.530
[12,     1] loss: 1381.039
[13,     1] loss: 1380.282
[14,     1] loss: 1369.760
[15,     1] loss: 1354.484
[16,     1] loss: 1341.565
[17,     1] loss: 1330.046
[18,     1] loss: 1304.955
[19,     1] loss: 1284.176
[20,     1] loss: 1245.866
[21,     1] loss: 1219.544
[22,     1] loss: 1231.461
[23,     1] loss: 1177.111
[24,     1] loss: 1157.773
[25,     1] loss: 1138.586
[26,     1] loss: 1142.743
[27,     1] loss: 1155.025
[28,     1] loss: 1122.052
[29,     1] loss: 1117.837
[30,     1] loss: 1111.271
[31,     1] loss: 1107.190
[32,     1] loss: 1073.068
[33,     1] loss: 1108.180
[34,     1] loss: 1055.491
[35,     1] loss: 1087.957
[36,     1] loss: 1066.542
[37,     1] loss: 1072.531
[38,     1] loss: 1054.427
[39,     1] loss: 1012.614
[40,     1] loss: 1039.644
[41,     1] loss: 1055.354
[42,     1] loss: 1021.360
[43,     1] loss: 1038.776
[44,     1] loss: 935.907
[45,     1] loss: 978.544
[46,     1] loss: 926.023
[47,     1] loss: 987.317
[48,     1] loss: 921.534
[49,     1] loss: 875.926
[50,     1] loss: 913.604
[51,     1] loss: 827.146
[52,     1] loss: 871.394
[53,     1] loss: 793.642
[54,     1] loss: 1055.818
[55,     1] loss: 1586.764
[56,     1] loss: 920.382
[57,     1] loss: 1187.629
[58,     1] loss: 1118.024
[59,     1] loss: 1132.090
[60,     1] loss: 1064.589
[61,     1] loss: 1025.257
[62,     1] loss: 959.762
[63,     1] loss: 976.873
[64,     1] loss: 924.049
Early stopping applied (best metric=0.7442259192466736)
Finished Training
Total time taken: 8.591183423995972
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.833
[2,     1] loss: 1393.571
[3,     1] loss: 1414.232
[4,     1] loss: 1390.992
[5,     1] loss: 1393.701
[6,     1] loss: 1390.399
[7,     1] loss: 1390.011
[8,     1] loss: 1389.867
[9,     1] loss: 1390.251
[10,     1] loss: 1388.639
[11,     1] loss: 1391.723
[12,     1] loss: 1389.653
[13,     1] loss: 1385.891
[14,     1] loss: 1385.125
[15,     1] loss: 1388.109
[16,     1] loss: 1393.017
[17,     1] loss: 1387.530
[18,     1] loss: 1387.149
[19,     1] loss: 1385.011
[20,     1] loss: 1382.289
[21,     1] loss: 1382.769
[22,     1] loss: 1379.297
[23,     1] loss: 1379.693
[24,     1] loss: 1364.662
[25,     1] loss: 1353.147
[26,     1] loss: 1337.645
[27,     1] loss: 1292.810
[28,     1] loss: 1267.329
[29,     1] loss: 1240.622
[30,     1] loss: 1209.610
[31,     1] loss: 1250.494
[32,     1] loss: 1169.727
[33,     1] loss: 1251.935
[34,     1] loss: 1143.666
[35,     1] loss: 1172.829
[36,     1] loss: 1108.778
[37,     1] loss: 1173.894
[38,     1] loss: 1109.156
[39,     1] loss: 1143.867
[40,     1] loss: 1151.741
[41,     1] loss: 1071.079
[42,     1] loss: 1118.733
[43,     1] loss: 1091.706
[44,     1] loss: 1093.736
[45,     1] loss: 1070.428
[46,     1] loss: 1083.634
[47,     1] loss: 1049.963
[48,     1] loss: 978.834
[49,     1] loss: 1037.450
[50,     1] loss: 1026.069
[51,     1] loss: 981.107
[52,     1] loss: 995.566
[53,     1] loss: 957.101
[54,     1] loss: 954.966
[55,     1] loss: 926.380
[56,     1] loss: 896.001
[57,     1] loss: 909.241
[58,     1] loss: 887.780
[59,     1] loss: 958.822
[60,     1] loss: 1140.225
[61,     1] loss: 1117.253
[62,     1] loss: 909.006
[63,     1] loss: 1024.769
[64,     1] loss: 975.678
[65,     1] loss: 904.215
[66,     1] loss: 939.157
[67,     1] loss: 846.406
[68,     1] loss: 949.977
[69,     1] loss: 909.123
[70,     1] loss: 861.347
[71,     1] loss: 899.208
[72,     1] loss: 889.886
[73,     1] loss: 849.867
[74,     1] loss: 909.494
[75,     1] loss: 836.581
[76,     1] loss: 910.387
[77,     1] loss: 786.665
[78,     1] loss: 771.484
[79,     1] loss: 788.489
[80,     1] loss: 802.214
[81,     1] loss: 775.513
Early stopping applied (best metric=0.720958948135376)
Finished Training
Total time taken: 10.85422658920288
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1392.027
[2,     1] loss: 1395.319
[3,     1] loss: 1398.496
[4,     1] loss: 1391.501
[5,     1] loss: 1388.882
[6,     1] loss: 1392.068
[7,     1] loss: 1392.981
[8,     1] loss: 1388.135
[9,     1] loss: 1387.130
[10,     1] loss: 1376.667
[11,     1] loss: 1373.950
[12,     1] loss: 1361.101
[13,     1] loss: 1347.525
[14,     1] loss: 1320.358
[15,     1] loss: 1281.406
[16,     1] loss: 1240.245
[17,     1] loss: 1181.910
[18,     1] loss: 1214.568
[19,     1] loss: 1159.025
[20,     1] loss: 1160.170
[21,     1] loss: 1209.093
[22,     1] loss: 1129.590
[23,     1] loss: 1167.246
[24,     1] loss: 1146.881
[25,     1] loss: 1132.993
[26,     1] loss: 1142.779
[27,     1] loss: 1107.085
[28,     1] loss: 1143.291
[29,     1] loss: 1061.218
[30,     1] loss: 1108.759
[31,     1] loss: 1030.326
[32,     1] loss: 1118.073
[33,     1] loss: 1091.145
[34,     1] loss: 1040.920
[35,     1] loss: 1034.622
[36,     1] loss: 1002.528
[37,     1] loss: 1008.463
[38,     1] loss: 1012.254
[39,     1] loss: 1013.194
[40,     1] loss: 1034.882
[41,     1] loss: 955.572
[42,     1] loss: 1001.564
[43,     1] loss: 1015.194
[44,     1] loss: 943.864
[45,     1] loss: 932.425
[46,     1] loss: 919.173
[47,     1] loss: 937.792
[48,     1] loss: 917.779
[49,     1] loss: 908.016
[50,     1] loss: 947.054
[51,     1] loss: 890.499
[52,     1] loss: 818.950
[53,     1] loss: 893.302
[54,     1] loss: 892.851
[55,     1] loss: 792.432
[56,     1] loss: 731.003
[57,     1] loss: 897.873
Early stopping applied (best metric=0.8669519424438477)
Finished Training
Total time taken: 9.486200332641602
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.821
[2,     1] loss: 1390.008
[3,     1] loss: 1389.151
[4,     1] loss: 1389.960
[5,     1] loss: 1394.352
[6,     1] loss: 1386.415
[7,     1] loss: 1378.503
[8,     1] loss: 1391.573
[9,     1] loss: 1376.739
[10,     1] loss: 1376.591
[11,     1] loss: 1355.043
[12,     1] loss: 1338.271
[13,     1] loss: 1305.478
[14,     1] loss: 1282.797
[15,     1] loss: 1222.078
[16,     1] loss: 1227.457
[17,     1] loss: 1228.960
[18,     1] loss: 1178.311
[19,     1] loss: 1166.618
[20,     1] loss: 1113.049
[21,     1] loss: 1181.331
[22,     1] loss: 1153.485
[23,     1] loss: 1167.181
[24,     1] loss: 1136.598
[25,     1] loss: 1120.949
[26,     1] loss: 1092.630
[27,     1] loss: 1118.043
[28,     1] loss: 1085.565
[29,     1] loss: 1121.658
[30,     1] loss: 1063.101
[31,     1] loss: 1108.219
[32,     1] loss: 1104.213
[33,     1] loss: 1152.623
[34,     1] loss: 1057.391
[35,     1] loss: 1052.686
[36,     1] loss: 1044.581
[37,     1] loss: 1028.243
[38,     1] loss: 1035.298
[39,     1] loss: 997.213
[40,     1] loss: 992.696
[41,     1] loss: 976.297
[42,     1] loss: 994.628
[43,     1] loss: 999.066
[44,     1] loss: 983.623
[45,     1] loss: 950.869
[46,     1] loss: 981.284
[47,     1] loss: 923.468
[48,     1] loss: 892.292
[49,     1] loss: 976.823
[50,     1] loss: 899.479
[51,     1] loss: 933.938
[52,     1] loss: 866.830
[53,     1] loss: 867.501
[54,     1] loss: 878.355
[55,     1] loss: 843.287
[56,     1] loss: 800.120
[57,     1] loss: 821.463
[58,     1] loss: 755.433
[59,     1] loss: 821.906
[60,     1] loss: 944.870
[61,     1] loss: 1600.244
[62,     1] loss: 946.325
[63,     1] loss: 1161.005
[64,     1] loss: 1163.091
[65,     1] loss: 1077.580
[66,     1] loss: 1061.958
[67,     1] loss: 993.900
[68,     1] loss: 928.124
[69,     1] loss: 940.289
[70,     1] loss: 925.767
Early stopping applied (best metric=0.7874032855033875)
Finished Training
Total time taken: 9.796207189559937
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.072
[2,     1] loss: 1390.488
[3,     1] loss: 1388.904
[4,     1] loss: 1394.558
[5,     1] loss: 1390.573
[6,     1] loss: 1386.665
[7,     1] loss: 1386.764
[8,     1] loss: 1386.096
[9,     1] loss: 1383.426
[10,     1] loss: 1370.505
[11,     1] loss: 1368.850
[12,     1] loss: 1343.065
[13,     1] loss: 1322.551
[14,     1] loss: 1289.906
[15,     1] loss: 1271.118
[16,     1] loss: 1259.241
[17,     1] loss: 1229.130
[18,     1] loss: 1155.311
[19,     1] loss: 1158.404
[20,     1] loss: 1185.189
[21,     1] loss: 1171.913
[22,     1] loss: 1190.027
[23,     1] loss: 1118.101
[24,     1] loss: 1146.973
[25,     1] loss: 1107.110
[26,     1] loss: 1146.191
[27,     1] loss: 1048.364
[28,     1] loss: 1087.768
[29,     1] loss: 1083.855
[30,     1] loss: 1038.867
[31,     1] loss: 1034.515
[32,     1] loss: 1101.011
[33,     1] loss: 995.432
[34,     1] loss: 1008.505
[35,     1] loss: 989.907
[36,     1] loss: 934.995
[37,     1] loss: 979.498
[38,     1] loss: 1018.795
[39,     1] loss: 972.982
Early stopping applied (best metric=0.8361068964004517)
Finished Training
Total time taken: 6.7181408405303955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.191
[2,     1] loss: 1390.539
[3,     1] loss: 1388.538
[4,     1] loss: 1388.487
[5,     1] loss: 1390.022
[6,     1] loss: 1379.529
[7,     1] loss: 1380.225
[8,     1] loss: 1366.785
[9,     1] loss: 1344.898
[10,     1] loss: 1313.287
[11,     1] loss: 1293.293
[12,     1] loss: 1234.375
[13,     1] loss: 1229.361
[14,     1] loss: 1203.091
[15,     1] loss: 1191.252
[16,     1] loss: 1145.021
[17,     1] loss: 1161.582
[18,     1] loss: 1114.593
[19,     1] loss: 1131.541
[20,     1] loss: 1139.042
[21,     1] loss: 1144.637
[22,     1] loss: 1090.636
[23,     1] loss: 1178.481
[24,     1] loss: 1090.891
[25,     1] loss: 1078.830
[26,     1] loss: 1026.583
[27,     1] loss: 1055.223
[28,     1] loss: 1055.209
[29,     1] loss: 1013.482
[30,     1] loss: 1048.151
[31,     1] loss: 1021.045
[32,     1] loss: 989.833
[33,     1] loss: 1004.342
[34,     1] loss: 981.331
[35,     1] loss: 1002.388
[36,     1] loss: 927.665
[37,     1] loss: 999.177
[38,     1] loss: 995.245
[39,     1] loss: 954.054
[40,     1] loss: 965.080
[41,     1] loss: 923.721
Early stopping applied (best metric=0.9072763323783875)
Finished Training
Total time taken: 5.758122444152832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.581
[2,     1] loss: 1387.170
[3,     1] loss: 1392.620
[4,     1] loss: 1392.369
[5,     1] loss: 1386.122
[6,     1] loss: 1393.157
[7,     1] loss: 1386.920
[8,     1] loss: 1384.407
[9,     1] loss: 1383.467
[10,     1] loss: 1375.762
[11,     1] loss: 1371.063
[12,     1] loss: 1356.694
[13,     1] loss: 1344.756
[14,     1] loss: 1321.468
[15,     1] loss: 1274.037
[16,     1] loss: 1250.505
[17,     1] loss: 1125.424
[18,     1] loss: 1137.693
[19,     1] loss: 1159.231
[20,     1] loss: 1167.837
[21,     1] loss: 1080.728
[22,     1] loss: 1138.858
[23,     1] loss: 1140.180
[24,     1] loss: 1184.908
[25,     1] loss: 1137.195
[26,     1] loss: 1137.266
[27,     1] loss: 1084.435
[28,     1] loss: 1064.349
[29,     1] loss: 1039.240
[30,     1] loss: 1112.000
[31,     1] loss: 1014.756
[32,     1] loss: 1085.377
[33,     1] loss: 1013.183
[34,     1] loss: 1032.685
[35,     1] loss: 1055.312
[36,     1] loss: 1019.473
[37,     1] loss: 1029.696
[38,     1] loss: 986.421
[39,     1] loss: 1029.192
[40,     1] loss: 1007.733
[41,     1] loss: 1057.449
[42,     1] loss: 990.201
[43,     1] loss: 1109.983
[44,     1] loss: 968.253
[45,     1] loss: 994.615
[46,     1] loss: 970.532
[47,     1] loss: 965.715
[48,     1] loss: 988.090
[49,     1] loss: 945.160
[50,     1] loss: 962.435
[51,     1] loss: 881.040
[52,     1] loss: 959.162
[53,     1] loss: 901.487
[54,     1] loss: 876.334
[55,     1] loss: 863.440
[56,     1] loss: 796.136
Early stopping applied (best metric=0.8582948446273804)
Finished Training
Total time taken: 9.177194833755493
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1402.625
[2,     1] loss: 1389.474
[3,     1] loss: 1412.556
[4,     1] loss: 1395.701
[5,     1] loss: 1390.438
[6,     1] loss: 1388.482
[7,     1] loss: 1389.860
[8,     1] loss: 1392.218
[9,     1] loss: 1389.866
[10,     1] loss: 1392.021
[11,     1] loss: 1395.992
[12,     1] loss: 1389.996
[13,     1] loss: 1389.828
[14,     1] loss: 1383.766
[15,     1] loss: 1388.033
[16,     1] loss: 1381.492
[17,     1] loss: 1379.175
[18,     1] loss: 1371.195
[19,     1] loss: 1358.855
[20,     1] loss: 1344.625
[21,     1] loss: 1314.368
[22,     1] loss: 1266.368
[23,     1] loss: 1257.324
[24,     1] loss: 1208.044
[25,     1] loss: 1156.865
[26,     1] loss: 1261.085
[27,     1] loss: 1204.465
[28,     1] loss: 1193.751
[29,     1] loss: 1128.656
[30,     1] loss: 1135.869
[31,     1] loss: 1133.674
[32,     1] loss: 1137.132
[33,     1] loss: 1119.817
[34,     1] loss: 1140.086
[35,     1] loss: 1083.418
[36,     1] loss: 1041.198
[37,     1] loss: 1067.174
[38,     1] loss: 1095.953
[39,     1] loss: 1053.152
[40,     1] loss: 1095.418
[41,     1] loss: 1023.288
[42,     1] loss: 1036.086
[43,     1] loss: 994.389
[44,     1] loss: 948.754
[45,     1] loss: 993.253
[46,     1] loss: 973.773
[47,     1] loss: 918.241
[48,     1] loss: 905.548
[49,     1] loss: 1047.111
[50,     1] loss: 1030.501
[51,     1] loss: 907.181
[52,     1] loss: 1061.694
[53,     1] loss: 882.981
[54,     1] loss: 994.506
[55,     1] loss: 859.440
[56,     1] loss: 947.495
[57,     1] loss: 876.046
[58,     1] loss: 936.734
[59,     1] loss: 881.596
[60,     1] loss: 997.607
[61,     1] loss: 801.294
[62,     1] loss: 911.409
[63,     1] loss: 878.810
[64,     1] loss: 880.833
[65,     1] loss: 850.474
[66,     1] loss: 799.178
[67,     1] loss: 787.553
[68,     1] loss: 846.101
[69,     1] loss: 703.976
[70,     1] loss: 727.011
[71,     1] loss: 826.516
[72,     1] loss: 703.865
[73,     1] loss: 688.018
[74,     1] loss: 618.778
[75,     1] loss: 752.160
[76,     1] loss: 794.713
[77,     1] loss: 1191.714
[78,     1] loss: 863.446
[79,     1] loss: 980.746
[80,     1] loss: 775.191
[81,     1] loss: 888.109
[82,     1] loss: 820.285
[83,     1] loss: 770.549
[84,     1] loss: 826.921
[85,     1] loss: 748.242
[86,     1] loss: 776.344
[87,     1] loss: 722.881
[88,     1] loss: 738.885
[89,     1] loss: 700.101
[90,     1] loss: 629.634
[91,     1] loss: 620.563
[92,     1] loss: 580.878
[93,     1] loss: 539.305
[94,     1] loss: 594.065
[95,     1] loss: 534.242
[96,     1] loss: 571.886
[97,     1] loss: 622.474
[98,     1] loss: 524.886
[99,     1] loss: 549.807
[100,     1] loss: 533.373
[101,     1] loss: 572.449
[102,     1] loss: 463.773
[103,     1] loss: 448.922
Early stopping applied (best metric=0.8218741416931152)
Finished Training
Total time taken: 14.130297899246216
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.515
[2,     1] loss: 1387.999
[3,     1] loss: 1392.588
[4,     1] loss: 1390.254
[5,     1] loss: 1382.290
[6,     1] loss: 1378.330
[7,     1] loss: 1373.650
[8,     1] loss: 1356.460
[9,     1] loss: 1335.305
[10,     1] loss: 1312.106
[11,     1] loss: 1256.268
[12,     1] loss: 1182.949
[13,     1] loss: 1216.512
[14,     1] loss: 1180.908
[15,     1] loss: 1164.033
[16,     1] loss: 1148.949
[17,     1] loss: 1155.127
[18,     1] loss: 1153.021
[19,     1] loss: 1103.383
[20,     1] loss: 1177.616
[21,     1] loss: 1111.282
[22,     1] loss: 1145.187
[23,     1] loss: 1110.587
[24,     1] loss: 1055.338
[25,     1] loss: 1097.420
[26,     1] loss: 1022.297
[27,     1] loss: 1039.885
[28,     1] loss: 1023.981
[29,     1] loss: 1035.932
[30,     1] loss: 974.440
[31,     1] loss: 996.571
[32,     1] loss: 1001.690
[33,     1] loss: 1003.238
[34,     1] loss: 962.685
[35,     1] loss: 1004.066
[36,     1] loss: 976.629
[37,     1] loss: 990.332
[38,     1] loss: 957.936
[39,     1] loss: 918.425
[40,     1] loss: 952.754
[41,     1] loss: 940.005
[42,     1] loss: 947.263
[43,     1] loss: 966.000
[44,     1] loss: 897.508
[45,     1] loss: 934.568
[46,     1] loss: 872.580
[47,     1] loss: 916.135
[48,     1] loss: 908.324
[49,     1] loss: 807.780
[50,     1] loss: 872.709
[51,     1] loss: 864.001
[52,     1] loss: 859.781
[53,     1] loss: 838.027
[54,     1] loss: 964.153
[55,     1] loss: 884.292
[56,     1] loss: 814.710
[57,     1] loss: 855.649
[58,     1] loss: 848.338
[59,     1] loss: 772.401
Early stopping applied (best metric=0.9712323546409607)
Finished Training
Total time taken: 9.717205047607422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.809
[2,     1] loss: 1404.094
[3,     1] loss: 1389.867
[4,     1] loss: 1388.585
[5,     1] loss: 1389.882
[6,     1] loss: 1392.516
[7,     1] loss: 1385.344
[8,     1] loss: 1392.132
[9,     1] loss: 1385.946
[10,     1] loss: 1390.529
[11,     1] loss: 1388.567
[12,     1] loss: 1385.341
[13,     1] loss: 1383.900
[14,     1] loss: 1381.938
[15,     1] loss: 1382.954
[16,     1] loss: 1371.453
[17,     1] loss: 1375.649
[18,     1] loss: 1360.787
[19,     1] loss: 1346.312
[20,     1] loss: 1314.078
[21,     1] loss: 1268.021
[22,     1] loss: 1217.557
[23,     1] loss: 1216.348
[24,     1] loss: 1145.558
[25,     1] loss: 1183.926
[26,     1] loss: 1205.357
[27,     1] loss: 1190.456
[28,     1] loss: 1154.325
[29,     1] loss: 1097.133
[30,     1] loss: 1101.334
[31,     1] loss: 1099.560
[32,     1] loss: 1078.742
[33,     1] loss: 1051.521
[34,     1] loss: 1071.155
[35,     1] loss: 1095.722
[36,     1] loss: 1059.720
[37,     1] loss: 1053.746
[38,     1] loss: 1073.989
[39,     1] loss: 1035.917
[40,     1] loss: 999.442
[41,     1] loss: 981.336
[42,     1] loss: 1057.362
[43,     1] loss: 1007.736
[44,     1] loss: 990.214
[45,     1] loss: 1006.835
[46,     1] loss: 927.483
[47,     1] loss: 945.953
[48,     1] loss: 953.129
[49,     1] loss: 946.665
[50,     1] loss: 986.376
[51,     1] loss: 926.921
[52,     1] loss: 945.995
[53,     1] loss: 1000.115
[54,     1] loss: 935.650
[55,     1] loss: 964.623
[56,     1] loss: 877.583
[57,     1] loss: 914.457
[58,     1] loss: 856.054
[59,     1] loss: 839.820
[60,     1] loss: 972.870
[61,     1] loss: 824.022
[62,     1] loss: 840.577
[63,     1] loss: 1011.644
[64,     1] loss: 795.069
[65,     1] loss: 894.157
[66,     1] loss: 780.201
[67,     1] loss: 910.584
[68,     1] loss: 771.114
[69,     1] loss: 789.914
[70,     1] loss: 732.600
[71,     1] loss: 744.488
[72,     1] loss: 782.994
Early stopping applied (best metric=0.7976857423782349)
Finished Training
Total time taken: 9.625202178955078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.163
[2,     1] loss: 1395.549
[3,     1] loss: 1391.751
[4,     1] loss: 1389.475
[5,     1] loss: 1391.195
[6,     1] loss: 1392.317
[7,     1] loss: 1388.842
[8,     1] loss: 1391.027
[9,     1] loss: 1388.388
[10,     1] loss: 1386.573
[11,     1] loss: 1387.878
[12,     1] loss: 1381.166
[13,     1] loss: 1379.148
[14,     1] loss: 1377.143
[15,     1] loss: 1367.401
[16,     1] loss: 1358.134
[17,     1] loss: 1322.838
[18,     1] loss: 1299.130
[19,     1] loss: 1250.463
[20,     1] loss: 1185.703
[21,     1] loss: 1158.697
[22,     1] loss: 1180.352
[23,     1] loss: 1167.063
[24,     1] loss: 1132.192
[25,     1] loss: 1096.557
[26,     1] loss: 1168.910
[27,     1] loss: 1126.382
[28,     1] loss: 1149.419
[29,     1] loss: 1083.136
[30,     1] loss: 1102.369
[31,     1] loss: 1046.475
[32,     1] loss: 1116.261
[33,     1] loss: 1069.484
[34,     1] loss: 1030.337
[35,     1] loss: 1012.675
[36,     1] loss: 1029.617
[37,     1] loss: 960.512
[38,     1] loss: 1047.012
[39,     1] loss: 1081.499
[40,     1] loss: 995.310
[41,     1] loss: 949.462
[42,     1] loss: 924.188
[43,     1] loss: 944.457
[44,     1] loss: 912.984
[45,     1] loss: 990.162
[46,     1] loss: 933.574
[47,     1] loss: 918.498
[48,     1] loss: 983.165
[49,     1] loss: 1009.852
[50,     1] loss: 883.545
[51,     1] loss: 1016.145
[52,     1] loss: 907.845
[53,     1] loss: 957.159
[54,     1] loss: 845.029
[55,     1] loss: 926.898
[56,     1] loss: 862.353
[57,     1] loss: 871.235
[58,     1] loss: 793.651
Early stopping applied (best metric=0.7963724136352539)
Finished Training
Total time taken: 9.486199855804443
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.713
[2,     1] loss: 1389.082
[3,     1] loss: 1390.312
[4,     1] loss: 1389.183
[5,     1] loss: 1386.241
[6,     1] loss: 1386.664
[7,     1] loss: 1387.580
[8,     1] loss: 1381.094
[9,     1] loss: 1377.706
[10,     1] loss: 1362.455
[11,     1] loss: 1345.565
[12,     1] loss: 1314.745
[13,     1] loss: 1274.291
[14,     1] loss: 1264.454
[15,     1] loss: 1223.158
[16,     1] loss: 1238.413
[17,     1] loss: 1176.730
[18,     1] loss: 1167.902
[19,     1] loss: 1141.151
[20,     1] loss: 1131.942
[21,     1] loss: 1132.841
[22,     1] loss: 1136.151
[23,     1] loss: 1131.561
[24,     1] loss: 1097.368
[25,     1] loss: 1071.132
[26,     1] loss: 1130.772
[27,     1] loss: 1042.588
[28,     1] loss: 1088.275
[29,     1] loss: 1075.429
[30,     1] loss: 1092.499
[31,     1] loss: 1059.913
[32,     1] loss: 1050.878
[33,     1] loss: 1054.796
[34,     1] loss: 1033.731
[35,     1] loss: 989.968
[36,     1] loss: 1053.474
[37,     1] loss: 1067.485
[38,     1] loss: 997.286
[39,     1] loss: 993.951
[40,     1] loss: 959.294
[41,     1] loss: 953.859
[42,     1] loss: 950.269
[43,     1] loss: 953.049
[44,     1] loss: 909.905
[45,     1] loss: 906.673
[46,     1] loss: 903.414
[47,     1] loss: 880.596
[48,     1] loss: 1039.976
[49,     1] loss: 1069.557
[50,     1] loss: 854.662
[51,     1] loss: 1096.309
[52,     1] loss: 851.033
[53,     1] loss: 997.435
[54,     1] loss: 868.539
[55,     1] loss: 825.220
[56,     1] loss: 885.401
[57,     1] loss: 855.302
[58,     1] loss: 880.496
Early stopping applied (best metric=0.8019957542419434)
Finished Training
Total time taken: 7.753163814544678
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1391.696
[2,     1] loss: 1398.561
[3,     1] loss: 1390.397
[4,     1] loss: 1388.789
[5,     1] loss: 1392.990
[6,     1] loss: 1385.776
[7,     1] loss: 1388.014
[8,     1] loss: 1384.152
[9,     1] loss: 1384.569
[10,     1] loss: 1376.948
[11,     1] loss: 1364.277
[12,     1] loss: 1339.810
[13,     1] loss: 1314.923
[14,     1] loss: 1269.107
[15,     1] loss: 1230.384
[16,     1] loss: 1238.786
[17,     1] loss: 1190.118
[18,     1] loss: 1182.228
[19,     1] loss: 1182.592
[20,     1] loss: 1180.829
[21,     1] loss: 1186.283
[22,     1] loss: 1146.584
[23,     1] loss: 1155.419
[24,     1] loss: 1168.371
[25,     1] loss: 1129.942
[26,     1] loss: 1160.825
[27,     1] loss: 1117.007
[28,     1] loss: 1121.921
[29,     1] loss: 1092.926
[30,     1] loss: 1114.296
[31,     1] loss: 1040.846
[32,     1] loss: 1068.858
[33,     1] loss: 1022.991
[34,     1] loss: 1062.688
[35,     1] loss: 1040.055
[36,     1] loss: 1027.726
[37,     1] loss: 1100.363
[38,     1] loss: 1011.157
[39,     1] loss: 1010.515
[40,     1] loss: 1023.681
[41,     1] loss: 1039.847
[42,     1] loss: 940.699
[43,     1] loss: 997.936
[44,     1] loss: 995.242
[45,     1] loss: 958.476
[46,     1] loss: 926.416
[47,     1] loss: 918.468
[48,     1] loss: 909.511
[49,     1] loss: 913.683
[50,     1] loss: 934.034
[51,     1] loss: 913.337
[52,     1] loss: 901.433
[53,     1] loss: 875.825
[54,     1] loss: 886.789
[55,     1] loss: 1087.860
[56,     1] loss: 1023.162
[57,     1] loss: 904.571
[58,     1] loss: 933.121
[59,     1] loss: 871.765
[60,     1] loss: 841.491
[61,     1] loss: 934.950
[62,     1] loss: 893.915
[63,     1] loss: 834.288
[64,     1] loss: 764.870
[65,     1] loss: 860.208
[66,     1] loss: 768.795
Early stopping applied (best metric=0.7044719457626343)
Finished Training
Total time taken: 10.85322880744934
{'Hydroxylation-K Validation Accuracy': 0.7661052009456265, 'Hydroxylation-K Validation Sensitivity': 0.6214814814814814, 'Hydroxylation-K Validation Specificity': 0.8035087719298245, 'Hydroxylation-K Validation Precision': 0.44459150326797386, 'Hydroxylation-K AUC ROC': 0.7795906432748538, 'Hydroxylation-K AUC PR': 0.5467848465765895, 'Hydroxylation-K MCC': 0.378935574410971, 'Hydroxylation-K F1': 0.5129864628125498, 'Validation Loss (Hydroxylation-K)': 0.4560437500476837, 'Hydroxylation-P Validation Accuracy': 0.7868650322318664, 'Hydroxylation-P Validation Sensitivity': 0.7708465608465609, 'Hydroxylation-P Validation Specificity': 0.7903311885879595, 'Hydroxylation-P Validation Precision': 0.45303239223930036, 'Hydroxylation-P AUC ROC': 0.8452282008286048, 'Hydroxylation-P AUC PR': 0.5840578548662707, 'Hydroxylation-P MCC': 0.46987093866489515, 'Hydroxylation-P F1': 0.566662361625113, 'Validation Loss (Hydroxylation-P)': 0.37498047947883606, 'Validation Loss (total)': 0.8310242255528768, 'TimeToTrain': 9.509200557072957}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005431644533065197,
 'learning_rate_Hydroxylation-K': 0.00494424133664872,
 'learning_rate_Hydroxylation-P': 0.004362643202957984,
 'log_base': 2.0698775234945406,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3128518333,
 'sample_weights': [2.286466011337365, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.310592092294581,
 'weight_decay_Hydroxylation-K': 9.216313105612063,
 'weight_decay_Hydroxylation-P': 3.5754809519513553}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1406.525
[2,     1] loss: 1392.824
[3,     1] loss: 1392.042
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007179063065020504,
 'learning_rate_Hydroxylation-K': 0.009675917599544406,
 'learning_rate_Hydroxylation-P': 0.0013599613628907223,
 'log_base': 1.5627731316595252,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 494996813,
 'sample_weights': [2.294800528937366, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7333800149346992,
 'weight_decay_Hydroxylation-K': 2.1261650854975347,
 'weight_decay_Hydroxylation-P': 3.404673764657092}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1700.726
[2,     1] loss: 1704.891
[3,     1] loss: 1698.313
[4,     1] loss: 1694.587
[5,     1] loss: 1702.918
[6,     1] loss: 1701.118
[7,     1] loss: 1697.013
[8,     1] loss: 1693.111
[9,     1] loss: 1690.181
[10,     1] loss: 1699.915
[11,     1] loss: 1692.715
[12,     1] loss: 1691.860
[13,     1] loss: 1694.450
[14,     1] loss: 1682.928
[15,     1] loss: 1685.983
[16,     1] loss: 1684.929
[17,     1] loss: 1668.722
[18,     1] loss: 1665.680
[19,     1] loss: 1664.751
[20,     1] loss: 1639.101
[21,     1] loss: 1598.984
[22,     1] loss: 1565.854
[23,     1] loss: 1567.615
[24,     1] loss: 1503.610
[25,     1] loss: 1477.192
[26,     1] loss: 1418.970
[27,     1] loss: 1461.146
[28,     1] loss: 1415.844
[29,     1] loss: 1452.873
[30,     1] loss: 1378.159
[31,     1] loss: 1421.204
[32,     1] loss: 1396.981
[33,     1] loss: 1430.950
[34,     1] loss: 1430.645
[35,     1] loss: 1356.890
[36,     1] loss: 1392.741
[37,     1] loss: 1377.915
[38,     1] loss: 1326.935
[39,     1] loss: 1379.619
[40,     1] loss: 1355.039
[41,     1] loss: 1304.913
[42,     1] loss: 1269.822
[43,     1] loss: 1261.279
[44,     1] loss: 1263.023
[45,     1] loss: 1236.031
[46,     1] loss: 1306.773
[47,     1] loss: 1209.934
[48,     1] loss: 1272.572
[49,     1] loss: 1293.420
[50,     1] loss: 1230.179
[51,     1] loss: 1273.742
[52,     1] loss: 1210.202
[53,     1] loss: 1201.494
[54,     1] loss: 1199.795
[55,     1] loss: 1235.172
[56,     1] loss: 1219.750
[57,     1] loss: 1230.352
[58,     1] loss: 1117.357
[59,     1] loss: 1207.914
[60,     1] loss: 1158.698
[61,     1] loss: 1092.094
[62,     1] loss: 1157.658
[63,     1] loss: 1175.433
[64,     1] loss: 1188.940
[65,     1] loss: 1179.388
[66,     1] loss: 1089.501
[67,     1] loss: 1106.456
[68,     1] loss: 1086.291
[69,     1] loss: 1187.441
[70,     1] loss: 1097.686
[71,     1] loss: 1073.319
Early stopping applied (best metric=0.8434759378433228)
Finished Training
Total time taken: 11.687247514724731
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1695.570
[2,     1] loss: 1694.795
[3,     1] loss: 1699.924
[4,     1] loss: 1707.270
[5,     1] loss: 1699.833
[6,     1] loss: 1700.426
[7,     1] loss: 1696.797
[8,     1] loss: 1691.878
[9,     1] loss: 1700.633
[10,     1] loss: 1694.933
[11,     1] loss: 1691.418
[12,     1] loss: 1693.443
[13,     1] loss: 1690.376
[14,     1] loss: 1693.229
[15,     1] loss: 1692.614
[16,     1] loss: 1691.832
[17,     1] loss: 1691.945
[18,     1] loss: 1690.660
[19,     1] loss: 1689.355
[20,     1] loss: 1688.758
[21,     1] loss: 1688.156
[22,     1] loss: 1678.900
[23,     1] loss: 1671.831
[24,     1] loss: 1668.449
[25,     1] loss: 1658.422
[26,     1] loss: 1637.515
[27,     1] loss: 1631.750
[28,     1] loss: 1608.838
[29,     1] loss: 1578.673
[30,     1] loss: 1522.809
[31,     1] loss: 1574.623
[32,     1] loss: 1466.798
[33,     1] loss: 1532.131
[34,     1] loss: 1403.763
[35,     1] loss: 1435.787
[36,     1] loss: 1389.786
[37,     1] loss: 1340.727
[38,     1] loss: 1459.827
[39,     1] loss: 1425.467
[40,     1] loss: 1433.876
[41,     1] loss: 1453.114
[42,     1] loss: 1465.118
[43,     1] loss: 1383.179
[44,     1] loss: 1359.432
[45,     1] loss: 1386.492
[46,     1] loss: 1327.935
[47,     1] loss: 1317.241
[48,     1] loss: 1255.458
[49,     1] loss: 1291.374
[50,     1] loss: 1299.782
[51,     1] loss: 1256.498
[52,     1] loss: 1292.399
[53,     1] loss: 1318.985
[54,     1] loss: 1269.525
[55,     1] loss: 1253.243
[56,     1] loss: 1195.360
[57,     1] loss: 1241.403
[58,     1] loss: 1228.758
[59,     1] loss: 1214.010
[60,     1] loss: 1215.335
[61,     1] loss: 1250.749
[62,     1] loss: 1218.251
[63,     1] loss: 1257.529
[64,     1] loss: 1273.366
[65,     1] loss: 1195.778
[66,     1] loss: 1222.114
[67,     1] loss: 1164.214
[68,     1] loss: 1184.891
[69,     1] loss: 1182.026
[70,     1] loss: 1121.885
[71,     1] loss: 1097.407
[72,     1] loss: 1131.984
[73,     1] loss: 1070.921
[74,     1] loss: 1169.098
[75,     1] loss: 1125.955
[76,     1] loss: 1036.716
Early stopping applied (best metric=0.8050127029418945)
Finished Training
Total time taken: 10.188213109970093
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1700.169
[2,     1] loss: 1707.317
[3,     1] loss: 1700.976
[4,     1] loss: 1702.455
[5,     1] loss: 1694.325
[6,     1] loss: 1697.222
[7,     1] loss: 1696.295
[8,     1] loss: 1692.911
[9,     1] loss: 1691.427
[10,     1] loss: 1692.596
[11,     1] loss: 1698.163
[12,     1] loss: 1694.443
[13,     1] loss: 1689.683
[14,     1] loss: 1690.504
[15,     1] loss: 1688.697
[16,     1] loss: 1691.589
[17,     1] loss: 1680.211
[18,     1] loss: 1671.659
[19,     1] loss: 1673.268
[20,     1] loss: 1658.637
[21,     1] loss: 1655.105
[22,     1] loss: 1617.966
[23,     1] loss: 1609.300
[24,     1] loss: 1576.317
[25,     1] loss: 1547.146
[26,     1] loss: 1475.555
[27,     1] loss: 1505.568
[28,     1] loss: 1494.041
[29,     1] loss: 1403.251
[30,     1] loss: 1454.783
[31,     1] loss: 1389.862
[32,     1] loss: 1443.634
[33,     1] loss: 1359.425
[34,     1] loss: 1426.608
[35,     1] loss: 1356.130
[36,     1] loss: 1364.530
[37,     1] loss: 1338.286
[38,     1] loss: 1383.415
[39,     1] loss: 1351.727
[40,     1] loss: 1269.073
[41,     1] loss: 1330.335
[42,     1] loss: 1341.271
[43,     1] loss: 1259.605
[44,     1] loss: 1333.589
[45,     1] loss: 1327.740
[46,     1] loss: 1375.951
[47,     1] loss: 1349.881
[48,     1] loss: 1238.235
[49,     1] loss: 1321.280
[50,     1] loss: 1392.969
[51,     1] loss: 1313.440
[52,     1] loss: 1327.656
[53,     1] loss: 1311.478
[54,     1] loss: 1305.125
[55,     1] loss: 1288.826
[56,     1] loss: 1357.609
[57,     1] loss: 1246.325
[58,     1] loss: 1315.302
[59,     1] loss: 1280.732
[60,     1] loss: 1250.448
[61,     1] loss: 1283.708
[62,     1] loss: 1231.069
[63,     1] loss: 1218.251
[64,     1] loss: 1230.882
[65,     1] loss: 1315.187
[66,     1] loss: 1145.013
[67,     1] loss: 1232.858
[68,     1] loss: 1218.579
[69,     1] loss: 1275.955
[70,     1] loss: 1210.115
[71,     1] loss: 1188.146
[72,     1] loss: 1272.994
[73,     1] loss: 1083.622
[74,     1] loss: 1147.238
[75,     1] loss: 1210.161
[76,     1] loss: 1181.047
[77,     1] loss: 1209.239
[78,     1] loss: 1174.215
[79,     1] loss: 1204.737
[80,     1] loss: 1251.016
[81,     1] loss: 1173.602
[82,     1] loss: 1221.510
[83,     1] loss: 1163.971
[84,     1] loss: 1200.816
[85,     1] loss: 1174.775
[86,     1] loss: 1159.131
[87,     1] loss: 1106.719
[88,     1] loss: 1143.676
[89,     1] loss: 1145.908
[90,     1] loss: 1132.168
[91,     1] loss: 1131.075
[92,     1] loss: 1103.715
[93,     1] loss: 1080.343
[94,     1] loss: 1076.042
[95,     1] loss: 1025.648
[96,     1] loss: 1142.118
[97,     1] loss: 1004.003
[98,     1] loss: 1034.169
[99,     1] loss: 1034.690
[100,     1] loss: 1103.764
[101,     1] loss: 1076.821
[102,     1] loss: 1194.770
Early stopping applied (best metric=0.8007833957672119)
Finished Training
Total time taken: 15.218322515487671
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1704.459
[2,     1] loss: 1698.382
[3,     1] loss: 1696.756
[4,     1] loss: 1698.235
[5,     1] loss: 1697.991
[6,     1] loss: 1690.018
[7,     1] loss: 1689.163
[8,     1] loss: 1683.310
[9,     1] loss: 1679.547
[10,     1] loss: 1669.094
[11,     1] loss: 1646.179
[12,     1] loss: 1630.453
[13,     1] loss: 1595.329
[14,     1] loss: 1557.626
[15,     1] loss: 1503.677
[16,     1] loss: 1501.352
[17,     1] loss: 1477.448
[18,     1] loss: 1462.838
[19,     1] loss: 1431.044
[20,     1] loss: 1453.523
[21,     1] loss: 1420.291
[22,     1] loss: 1438.912
[23,     1] loss: 1446.386
[24,     1] loss: 1445.884
[25,     1] loss: 1425.006
[26,     1] loss: 1419.616
[27,     1] loss: 1412.567
[28,     1] loss: 1414.625
[29,     1] loss: 1386.159
[30,     1] loss: 1333.932
[31,     1] loss: 1361.851
[32,     1] loss: 1416.203
[33,     1] loss: 1352.349
[34,     1] loss: 1343.318
[35,     1] loss: 1378.276
[36,     1] loss: 1327.970
[37,     1] loss: 1347.276
[38,     1] loss: 1310.844
[39,     1] loss: 1293.232
[40,     1] loss: 1269.504
[41,     1] loss: 1332.050
[42,     1] loss: 1365.698
[43,     1] loss: 1329.330
[44,     1] loss: 1302.044
[45,     1] loss: 1288.356
[46,     1] loss: 1299.068
[47,     1] loss: 1267.011
[48,     1] loss: 1232.442
[49,     1] loss: 1294.478
[50,     1] loss: 1295.697
[51,     1] loss: 1269.994
[52,     1] loss: 1263.176
[53,     1] loss: 1197.964
[54,     1] loss: 1348.470
[55,     1] loss: 1260.959
[56,     1] loss: 1187.202
[57,     1] loss: 1228.608
[58,     1] loss: 1233.906
[59,     1] loss: 1210.619
[60,     1] loss: 1214.725
[61,     1] loss: 1190.625
[62,     1] loss: 1251.243
[63,     1] loss: 1302.123
[64,     1] loss: 1279.697
[65,     1] loss: 1259.776
[66,     1] loss: 1173.544
[67,     1] loss: 1159.817
[68,     1] loss: 1184.818
[69,     1] loss: 1206.214
Early stopping applied (best metric=0.7356197834014893)
Finished Training
Total time taken: 11.348236799240112
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1696.600
[2,     1] loss: 1696.033
[3,     1] loss: 1703.390
[4,     1] loss: 1702.175
[5,     1] loss: 1709.254
[6,     1] loss: 1689.314
[7,     1] loss: 1694.286
[8,     1] loss: 1697.163
[9,     1] loss: 1691.905
[10,     1] loss: 1685.108
[11,     1] loss: 1685.499
[12,     1] loss: 1675.850
[13,     1] loss: 1659.547
[14,     1] loss: 1654.296
[15,     1] loss: 1633.366
[16,     1] loss: 1602.376
[17,     1] loss: 1556.475
[18,     1] loss: 1553.887
[19,     1] loss: 1534.102
[20,     1] loss: 1482.597
[21,     1] loss: 1460.823
[22,     1] loss: 1470.529
[23,     1] loss: 1515.949
[24,     1] loss: 1412.228
[25,     1] loss: 1435.639
[26,     1] loss: 1375.940
[27,     1] loss: 1423.516
[28,     1] loss: 1418.621
[29,     1] loss: 1366.973
[30,     1] loss: 1400.207
[31,     1] loss: 1360.834
[32,     1] loss: 1382.349
[33,     1] loss: 1388.437
[34,     1] loss: 1367.213
[35,     1] loss: 1335.645
[36,     1] loss: 1364.518
[37,     1] loss: 1351.563
[38,     1] loss: 1353.844
[39,     1] loss: 1372.687
[40,     1] loss: 1351.650
[41,     1] loss: 1352.145
[42,     1] loss: 1324.024
[43,     1] loss: 1337.404
[44,     1] loss: 1379.939
[45,     1] loss: 1269.763
[46,     1] loss: 1356.119
[47,     1] loss: 1264.535
[48,     1] loss: 1288.494
[49,     1] loss: 1284.844
[50,     1] loss: 1249.628
[51,     1] loss: 1264.686
[52,     1] loss: 1306.557
[53,     1] loss: 1199.695
[54,     1] loss: 1215.742
[55,     1] loss: 1295.541
[56,     1] loss: 1303.960
[57,     1] loss: 1297.592
[58,     1] loss: 1294.282
[59,     1] loss: 1232.220
[60,     1] loss: 1318.149
[61,     1] loss: 1249.601
[62,     1] loss: 1155.884
[63,     1] loss: 1186.379
[64,     1] loss: 1192.436
[65,     1] loss: 1224.537
[66,     1] loss: 1220.635
[67,     1] loss: 1173.958
[68,     1] loss: 1183.009
[69,     1] loss: 1170.033
[70,     1] loss: 1089.265
[71,     1] loss: 1122.902
[72,     1] loss: 1163.033
[73,     1] loss: 1123.606
[74,     1] loss: 1114.980
[75,     1] loss: 1052.410
[76,     1] loss: 1130.334
[77,     1] loss: 1231.142
[78,     1] loss: 1212.798
[79,     1] loss: 1062.419
[80,     1] loss: 1096.007
[81,     1] loss: 1058.685
Early stopping applied (best metric=0.8339648246765137)
Finished Training
Total time taken: 10.747225999832153
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1698.612
[2,     1] loss: 1699.099
[3,     1] loss: 1707.402
[4,     1] loss: 1692.424
[5,     1] loss: 1699.044
[6,     1] loss: 1692.467
[7,     1] loss: 1698.115
[8,     1] loss: 1693.092
[9,     1] loss: 1692.670
[10,     1] loss: 1699.317
[11,     1] loss: 1692.271
[12,     1] loss: 1689.238
[13,     1] loss: 1685.383
[14,     1] loss: 1677.295
[15,     1] loss: 1682.561
[16,     1] loss: 1650.272
[17,     1] loss: 1643.084
[18,     1] loss: 1627.541
[19,     1] loss: 1594.271
[20,     1] loss: 1556.204
[21,     1] loss: 1524.293
[22,     1] loss: 1484.265
[23,     1] loss: 1552.804
[24,     1] loss: 1540.050
[25,     1] loss: 1425.486
[26,     1] loss: 1457.987
[27,     1] loss: 1475.520
[28,     1] loss: 1431.532
[29,     1] loss: 1426.479
[30,     1] loss: 1409.266
[31,     1] loss: 1387.522
[32,     1] loss: 1416.281
[33,     1] loss: 1404.028
[34,     1] loss: 1373.634
[35,     1] loss: 1334.015
[36,     1] loss: 1329.477
[37,     1] loss: 1340.809
[38,     1] loss: 1328.994
[39,     1] loss: 1350.885
[40,     1] loss: 1352.868
[41,     1] loss: 1304.040
[42,     1] loss: 1296.589
[43,     1] loss: 1337.006
[44,     1] loss: 1225.367
[45,     1] loss: 1269.187
[46,     1] loss: 1329.857
[47,     1] loss: 1323.038
[48,     1] loss: 1320.565
[49,     1] loss: 1356.756
[50,     1] loss: 1312.699
[51,     1] loss: 1284.905
[52,     1] loss: 1359.615
[53,     1] loss: 1266.385
[54,     1] loss: 1275.280
[55,     1] loss: 1229.585
[56,     1] loss: 1298.849
[57,     1] loss: 1228.248
[58,     1] loss: 1216.143
[59,     1] loss: 1210.671
[60,     1] loss: 1216.760
[61,     1] loss: 1181.548
[62,     1] loss: 1255.197
[63,     1] loss: 1157.202
[64,     1] loss: 1179.819
[65,     1] loss: 1182.641
[66,     1] loss: 1159.609
[67,     1] loss: 1151.112
[68,     1] loss: 1189.182
[69,     1] loss: 1196.946
[70,     1] loss: 1160.024
[71,     1] loss: 1227.534
[72,     1] loss: 1152.200
[73,     1] loss: 1141.696
[74,     1] loss: 1144.168
[75,     1] loss: 1166.736
[76,     1] loss: 1158.625
[77,     1] loss: 1144.881
[78,     1] loss: 1094.455
[79,     1] loss: 1187.210
[80,     1] loss: 1147.498
[81,     1] loss: 1178.936
[82,     1] loss: 1237.173
[83,     1] loss: 1120.669
[84,     1] loss: 1198.827
[85,     1] loss: 1216.955
[86,     1] loss: 1081.344
[87,     1] loss: 1216.495
[88,     1] loss: 1085.565
[89,     1] loss: 1110.771
[90,     1] loss: 1280.890
[91,     1] loss: 1101.097
[92,     1] loss: 1221.753
Early stopping applied (best metric=0.7200061082839966)
Finished Training
Total time taken: 13.439283609390259
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1698.230
[2,     1] loss: 1698.326
[3,     1] loss: 1695.036
[4,     1] loss: 1701.938
[5,     1] loss: 1695.614
[6,     1] loss: 1697.139
[7,     1] loss: 1698.301
[8,     1] loss: 1694.632
[9,     1] loss: 1689.301
[10,     1] loss: 1695.340
[11,     1] loss: 1681.628
[12,     1] loss: 1687.306
[13,     1] loss: 1685.722
[14,     1] loss: 1677.531
[15,     1] loss: 1659.760
[16,     1] loss: 1642.983
[17,     1] loss: 1638.261
[18,     1] loss: 1604.198
[19,     1] loss: 1597.552
[20,     1] loss: 1553.831
[21,     1] loss: 1509.384
[22,     1] loss: 1464.293
[23,     1] loss: 1418.323
[24,     1] loss: 1469.614
[25,     1] loss: 1422.702
[26,     1] loss: 1480.028
[27,     1] loss: 1416.872
[28,     1] loss: 1429.590
[29,     1] loss: 1427.285
[30,     1] loss: 1451.033
[31,     1] loss: 1359.157
[32,     1] loss: 1437.382
[33,     1] loss: 1378.071
[34,     1] loss: 1342.539
[35,     1] loss: 1433.125
[36,     1] loss: 1360.233
[37,     1] loss: 1321.854
[38,     1] loss: 1348.619
[39,     1] loss: 1327.147
[40,     1] loss: 1275.171
[41,     1] loss: 1253.123
[42,     1] loss: 1310.913
[43,     1] loss: 1334.655
[44,     1] loss: 1204.185
[45,     1] loss: 1284.274
[46,     1] loss: 1228.525
[47,     1] loss: 1207.521
[48,     1] loss: 1212.750
[49,     1] loss: 1321.085
[50,     1] loss: 1174.248
Early stopping applied (best metric=0.8930043578147888)
Finished Training
Total time taken: 7.577162027359009
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1694.672
[2,     1] loss: 1700.749
[3,     1] loss: 1712.236
[4,     1] loss: 1690.470
[5,     1] loss: 1696.109
[6,     1] loss: 1701.901
[7,     1] loss: 1702.182
[8,     1] loss: 1696.463
[9,     1] loss: 1699.804
[10,     1] loss: 1696.018
[11,     1] loss: 1694.793
[12,     1] loss: 1690.788
[13,     1] loss: 1685.191
[14,     1] loss: 1689.212
[15,     1] loss: 1687.553
[16,     1] loss: 1689.096
[17,     1] loss: 1689.001
[18,     1] loss: 1674.600
[19,     1] loss: 1675.442
[20,     1] loss: 1665.588
[21,     1] loss: 1647.227
[22,     1] loss: 1634.715
[23,     1] loss: 1605.538
[24,     1] loss: 1583.160
[25,     1] loss: 1548.219
[26,     1] loss: 1492.771
[27,     1] loss: 1441.451
[28,     1] loss: 1515.009
[29,     1] loss: 1474.149
[30,     1] loss: 1418.348
[31,     1] loss: 1463.664
[32,     1] loss: 1383.461
[33,     1] loss: 1369.117
[34,     1] loss: 1339.564
[35,     1] loss: 1443.856
[36,     1] loss: 1344.041
[37,     1] loss: 1349.182
[38,     1] loss: 1347.400
[39,     1] loss: 1439.430
[40,     1] loss: 1364.437
[41,     1] loss: 1359.673
[42,     1] loss: 1385.920
[43,     1] loss: 1292.965
[44,     1] loss: 1337.241
[45,     1] loss: 1403.636
[46,     1] loss: 1338.923
[47,     1] loss: 1380.969
[48,     1] loss: 1337.229
[49,     1] loss: 1256.166
[50,     1] loss: 1354.763
[51,     1] loss: 1273.403
[52,     1] loss: 1251.548
[53,     1] loss: 1285.412
[54,     1] loss: 1210.557
[55,     1] loss: 1260.895
[56,     1] loss: 1282.829
[57,     1] loss: 1195.055
[58,     1] loss: 1228.352
[59,     1] loss: 1216.422
[60,     1] loss: 1184.487
[61,     1] loss: 1213.909
[62,     1] loss: 1186.869
[63,     1] loss: 1233.245
[64,     1] loss: 1139.622
[65,     1] loss: 1185.924
[66,     1] loss: 1185.363
[67,     1] loss: 1120.085
Early stopping applied (best metric=0.8257670402526855)
Finished Training
Total time taken: 10.530222177505493
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1694.174
[2,     1] loss: 1703.157
[3,     1] loss: 1703.653
[4,     1] loss: 1693.363
[5,     1] loss: 1690.092
[6,     1] loss: 1685.483
[7,     1] loss: 1681.120
[8,     1] loss: 1688.855
[9,     1] loss: 1694.427
[10,     1] loss: 1685.370
[11,     1] loss: 1689.256
[12,     1] loss: 1687.109
[13,     1] loss: 1684.882
[14,     1] loss: 1688.025
[15,     1] loss: 1669.064
[16,     1] loss: 1650.422
[17,     1] loss: 1641.843
[18,     1] loss: 1649.025
[19,     1] loss: 1568.259
[20,     1] loss: 1612.529
[21,     1] loss: 1587.371
[22,     1] loss: 1550.262
[23,     1] loss: 1469.734
[24,     1] loss: 1543.249
[25,     1] loss: 1529.877
[26,     1] loss: 1516.244
[27,     1] loss: 1474.533
[28,     1] loss: 1451.780
[29,     1] loss: 1486.098
[30,     1] loss: 1482.094
[31,     1] loss: 1445.482
[32,     1] loss: 1390.499
[33,     1] loss: 1468.180
[34,     1] loss: 1369.089
[35,     1] loss: 1416.797
[36,     1] loss: 1385.484
[37,     1] loss: 1355.984
[38,     1] loss: 1350.002
[39,     1] loss: 1463.556
[40,     1] loss: 1358.739
[41,     1] loss: 1326.977
[42,     1] loss: 1386.329
[43,     1] loss: 1356.767
[44,     1] loss: 1300.696
[45,     1] loss: 1332.303
[46,     1] loss: 1324.298
[47,     1] loss: 1317.741
[48,     1] loss: 1262.641
[49,     1] loss: 1351.878
[50,     1] loss: 1332.853
Early stopping applied (best metric=0.9500018358230591)
Finished Training
Total time taken: 8.552180051803589
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1702.232
[2,     1] loss: 1699.253
[3,     1] loss: 1701.871
[4,     1] loss: 1691.349
[5,     1] loss: 1706.097
[6,     1] loss: 1694.560
[7,     1] loss: 1694.828
[8,     1] loss: 1680.457
[9,     1] loss: 1678.340
[10,     1] loss: 1675.206
[11,     1] loss: 1634.325
[12,     1] loss: 1628.802
[13,     1] loss: 1592.693
[14,     1] loss: 1569.560
[15,     1] loss: 1502.723
[16,     1] loss: 1448.452
[17,     1] loss: 1501.885
[18,     1] loss: 1457.540
[19,     1] loss: 1539.507
[20,     1] loss: 1467.559
[21,     1] loss: 1436.466
[22,     1] loss: 1429.052
[23,     1] loss: 1441.607
[24,     1] loss: 1412.124
[25,     1] loss: 1418.990
[26,     1] loss: 1442.670
[27,     1] loss: 1366.201
[28,     1] loss: 1450.499
[29,     1] loss: 1380.329
[30,     1] loss: 1341.550
[31,     1] loss: 1319.686
[32,     1] loss: 1334.852
[33,     1] loss: 1287.977
[34,     1] loss: 1359.900
[35,     1] loss: 1397.297
[36,     1] loss: 1372.371
[37,     1] loss: 1349.577
[38,     1] loss: 1292.799
[39,     1] loss: 1289.308
[40,     1] loss: 1265.919
[41,     1] loss: 1210.644
[42,     1] loss: 1180.222
[43,     1] loss: 1263.077
[44,     1] loss: 1171.893
[45,     1] loss: 1237.222
[46,     1] loss: 1220.397
[47,     1] loss: 1123.949
[48,     1] loss: 1159.210
[49,     1] loss: 1236.736
[50,     1] loss: 1234.584
[51,     1] loss: 1191.023
[52,     1] loss: 1168.842
[53,     1] loss: 1147.527
[54,     1] loss: 1247.430
[55,     1] loss: 1159.749
[56,     1] loss: 1136.248
[57,     1] loss: 1083.805
[58,     1] loss: 1114.638
[59,     1] loss: 1096.537
Early stopping applied (best metric=0.8121688365936279)
Finished Training
Total time taken: 8.161174535751343
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1695.467
[2,     1] loss: 1696.429
[3,     1] loss: 1697.041
[4,     1] loss: 1705.911
[5,     1] loss: 1692.341
[6,     1] loss: 1691.449
[7,     1] loss: 1692.777
[8,     1] loss: 1693.056
[9,     1] loss: 1690.353
[10,     1] loss: 1691.594
[11,     1] loss: 1695.333
[12,     1] loss: 1679.233
[13,     1] loss: 1680.290
[14,     1] loss: 1667.546
[15,     1] loss: 1655.232
[16,     1] loss: 1636.507
[17,     1] loss: 1619.775
[18,     1] loss: 1592.918
[19,     1] loss: 1557.637
[20,     1] loss: 1468.639
[21,     1] loss: 1502.984
[22,     1] loss: 1495.226
[23,     1] loss: 1480.007
[24,     1] loss: 1452.855
[25,     1] loss: 1476.641
[26,     1] loss: 1438.986
[27,     1] loss: 1456.872
[28,     1] loss: 1437.223
[29,     1] loss: 1449.823
[30,     1] loss: 1444.330
[31,     1] loss: 1361.097
[32,     1] loss: 1392.028
[33,     1] loss: 1347.486
[34,     1] loss: 1360.703
[35,     1] loss: 1402.381
[36,     1] loss: 1380.187
[37,     1] loss: 1392.834
[38,     1] loss: 1398.003
[39,     1] loss: 1367.513
[40,     1] loss: 1227.090
[41,     1] loss: 1292.885
[42,     1] loss: 1354.989
[43,     1] loss: 1252.436
[44,     1] loss: 1317.301
[45,     1] loss: 1289.620
[46,     1] loss: 1362.555
[47,     1] loss: 1275.324
[48,     1] loss: 1270.008
[49,     1] loss: 1301.416
[50,     1] loss: 1238.095
[51,     1] loss: 1187.557
[52,     1] loss: 1246.869
Early stopping applied (best metric=0.8676799535751343)
Finished Training
Total time taken: 8.8851900100708
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1702.808
[2,     1] loss: 1698.025
[3,     1] loss: 1702.623
[4,     1] loss: 1698.858
[5,     1] loss: 1695.180
[6,     1] loss: 1699.157
[7,     1] loss: 1694.673
[8,     1] loss: 1690.231
[9,     1] loss: 1690.082
[10,     1] loss: 1695.814
[11,     1] loss: 1697.033
[12,     1] loss: 1682.217
[13,     1] loss: 1679.581
[14,     1] loss: 1671.242
[15,     1] loss: 1663.591
[16,     1] loss: 1650.604
[17,     1] loss: 1627.757
[18,     1] loss: 1594.376
[19,     1] loss: 1555.670
[20,     1] loss: 1550.435
[21,     1] loss: 1514.918
[22,     1] loss: 1501.477
[23,     1] loss: 1446.287
[24,     1] loss: 1477.920
[25,     1] loss: 1415.891
[26,     1] loss: 1436.750
[27,     1] loss: 1439.172
[28,     1] loss: 1384.498
[29,     1] loss: 1450.933
[30,     1] loss: 1360.368
[31,     1] loss: 1390.115
[32,     1] loss: 1354.219
[33,     1] loss: 1342.955
[34,     1] loss: 1319.264
[35,     1] loss: 1310.120
[36,     1] loss: 1377.026
[37,     1] loss: 1388.585
[38,     1] loss: 1283.474
[39,     1] loss: 1299.730
[40,     1] loss: 1276.542
[41,     1] loss: 1301.860
[42,     1] loss: 1313.066
[43,     1] loss: 1329.494
[44,     1] loss: 1234.852
[45,     1] loss: 1319.538
[46,     1] loss: 1306.289
Early stopping applied (best metric=0.8746445178985596)
Finished Training
Total time taken: 7.949167728424072
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1697.867
[2,     1] loss: 1705.479
[3,     1] loss: 1709.853
[4,     1] loss: 1701.107
[5,     1] loss: 1700.281
[6,     1] loss: 1707.506
[7,     1] loss: 1695.566
[8,     1] loss: 1695.450
[9,     1] loss: 1684.824
[10,     1] loss: 1696.954
[11,     1] loss: 1688.950
[12,     1] loss: 1697.107
[13,     1] loss: 1699.187
[14,     1] loss: 1698.241
[15,     1] loss: 1692.226
[16,     1] loss: 1681.388
[17,     1] loss: 1685.071
[18,     1] loss: 1683.864
[19,     1] loss: 1664.985
[20,     1] loss: 1660.190
[21,     1] loss: 1633.456
[22,     1] loss: 1617.281
[23,     1] loss: 1609.666
[24,     1] loss: 1544.721
[25,     1] loss: 1537.957
[26,     1] loss: 1483.008
[27,     1] loss: 1483.116
[28,     1] loss: 1457.249
[29,     1] loss: 1483.483
[30,     1] loss: 1460.214
[31,     1] loss: 1389.210
[32,     1] loss: 1469.316
[33,     1] loss: 1424.893
[34,     1] loss: 1341.853
[35,     1] loss: 1427.420
[36,     1] loss: 1340.792
[37,     1] loss: 1388.578
[38,     1] loss: 1426.177
[39,     1] loss: 1403.766
[40,     1] loss: 1382.851
[41,     1] loss: 1371.479
[42,     1] loss: 1391.472
[43,     1] loss: 1320.737
[44,     1] loss: 1336.658
[45,     1] loss: 1328.062
[46,     1] loss: 1291.548
[47,     1] loss: 1324.971
[48,     1] loss: 1325.393
[49,     1] loss: 1196.061
[50,     1] loss: 1310.526
[51,     1] loss: 1371.381
[52,     1] loss: 1270.540
[53,     1] loss: 1244.264
[54,     1] loss: 1360.620
[55,     1] loss: 1270.586
[56,     1] loss: 1288.463
[57,     1] loss: 1246.652
[58,     1] loss: 1286.742
[59,     1] loss: 1208.714
[60,     1] loss: 1216.674
[61,     1] loss: 1240.393
[62,     1] loss: 1210.274
[63,     1] loss: 1205.866
[64,     1] loss: 1272.570
[65,     1] loss: 1158.187
[66,     1] loss: 1186.267
[67,     1] loss: 1185.865
[68,     1] loss: 1228.488
[69,     1] loss: 1199.710
[70,     1] loss: 1290.931
[71,     1] loss: 1134.025
[72,     1] loss: 1136.153
[73,     1] loss: 1176.696
[74,     1] loss: 1225.989
[75,     1] loss: 1158.422
[76,     1] loss: 1142.079
Early stopping applied (best metric=0.7894034385681152)
Finished Training
Total time taken: 12.779263973236084
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1701.132
[2,     1] loss: 1696.646
[3,     1] loss: 1699.557
[4,     1] loss: 1699.699
[5,     1] loss: 1692.866
[6,     1] loss: 1698.072
[7,     1] loss: 1700.213
[8,     1] loss: 1702.388
[9,     1] loss: 1695.955
[10,     1] loss: 1695.474
[11,     1] loss: 1699.052
[12,     1] loss: 1693.600
[13,     1] loss: 1692.173
[14,     1] loss: 1692.036
[15,     1] loss: 1688.316
[16,     1] loss: 1689.410
[17,     1] loss: 1691.703
[18,     1] loss: 1676.953
[19,     1] loss: 1672.735
[20,     1] loss: 1653.719
[21,     1] loss: 1650.019
[22,     1] loss: 1638.798
[23,     1] loss: 1585.133
[24,     1] loss: 1559.462
[25,     1] loss: 1546.978
[26,     1] loss: 1549.991
[27,     1] loss: 1526.938
[28,     1] loss: 1496.968
[29,     1] loss: 1485.270
[30,     1] loss: 1454.903
[31,     1] loss: 1433.321
[32,     1] loss: 1456.934
[33,     1] loss: 1448.217
[34,     1] loss: 1381.247
[35,     1] loss: 1401.825
[36,     1] loss: 1430.995
[37,     1] loss: 1383.069
[38,     1] loss: 1411.043
[39,     1] loss: 1370.647
[40,     1] loss: 1390.055
[41,     1] loss: 1421.647
[42,     1] loss: 1368.513
[43,     1] loss: 1340.571
[44,     1] loss: 1373.839
[45,     1] loss: 1338.056
[46,     1] loss: 1362.607
[47,     1] loss: 1341.233
[48,     1] loss: 1402.237
[49,     1] loss: 1301.750
[50,     1] loss: 1236.219
[51,     1] loss: 1316.635
[52,     1] loss: 1343.190
[53,     1] loss: 1295.043
[54,     1] loss: 1284.720
[55,     1] loss: 1310.515
[56,     1] loss: 1254.135
[57,     1] loss: 1297.835
[58,     1] loss: 1358.021
[59,     1] loss: 1320.443
[60,     1] loss: 1338.820
[61,     1] loss: 1316.928
[62,     1] loss: 1295.255
[63,     1] loss: 1297.515
[64,     1] loss: 1274.446
[65,     1] loss: 1240.768
[66,     1] loss: 1263.064
[67,     1] loss: 1168.707
[68,     1] loss: 1267.430
[69,     1] loss: 1171.560
[70,     1] loss: 1177.294
[71,     1] loss: 1209.035
[72,     1] loss: 1197.501
[73,     1] loss: 1173.015
[74,     1] loss: 1185.196
[75,     1] loss: 1136.031
[76,     1] loss: 1113.934
[77,     1] loss: 1175.736
[78,     1] loss: 1143.286
[79,     1] loss: 1163.387
[80,     1] loss: 1146.416
[81,     1] loss: 1142.420
[82,     1] loss: 1090.000
[83,     1] loss: 1069.533
[84,     1] loss: 1121.693
[85,     1] loss: 1050.305
[86,     1] loss: 1116.011
[87,     1] loss: 1090.997
Early stopping applied (best metric=0.6834564805030823)
Finished Training
Total time taken: 12.140259027481079
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1700.185
[2,     1] loss: 1698.370
[3,     1] loss: 1705.232
[4,     1] loss: 1702.129
[5,     1] loss: 1698.189
[6,     1] loss: 1694.734
[7,     1] loss: 1697.923
[8,     1] loss: 1705.045
[9,     1] loss: 1699.711
[10,     1] loss: 1696.619
[11,     1] loss: 1694.211
[12,     1] loss: 1695.551
[13,     1] loss: 1686.582
[14,     1] loss: 1683.074
[15,     1] loss: 1672.356
[16,     1] loss: 1645.133
[17,     1] loss: 1637.009
[18,     1] loss: 1596.698
[19,     1] loss: 1582.415
[20,     1] loss: 1514.985
[21,     1] loss: 1530.097
[22,     1] loss: 1455.415
[23,     1] loss: 1451.037
[24,     1] loss: 1446.222
[25,     1] loss: 1436.954
[26,     1] loss: 1438.044
[27,     1] loss: 1419.152
[28,     1] loss: 1382.839
[29,     1] loss: 1363.700
[30,     1] loss: 1437.521
[31,     1] loss: 1446.185
[32,     1] loss: 1353.171
[33,     1] loss: 1411.390
[34,     1] loss: 1358.809
[35,     1] loss: 1344.256
[36,     1] loss: 1362.308
[37,     1] loss: 1386.911
[38,     1] loss: 1316.772
[39,     1] loss: 1312.401
[40,     1] loss: 1388.543
[41,     1] loss: 1326.286
[42,     1] loss: 1292.055
[43,     1] loss: 1343.296
[44,     1] loss: 1366.624
[45,     1] loss: 1297.490
[46,     1] loss: 1288.097
[47,     1] loss: 1346.482
[48,     1] loss: 1347.717
[49,     1] loss: 1273.736
[50,     1] loss: 1253.872
[51,     1] loss: 1330.821
[52,     1] loss: 1210.291
[53,     1] loss: 1228.882
[54,     1] loss: 1279.625
[55,     1] loss: 1257.511
[56,     1] loss: 1257.186
[57,     1] loss: 1255.727
[58,     1] loss: 1193.222
[59,     1] loss: 1247.404
[60,     1] loss: 1198.918
[61,     1] loss: 1293.702
[62,     1] loss: 1202.742
[63,     1] loss: 1160.968
[64,     1] loss: 1217.444
[65,     1] loss: 1173.061
[66,     1] loss: 1109.456
[67,     1] loss: 1186.894
[68,     1] loss: 1174.432
[69,     1] loss: 1190.999
Early stopping applied (best metric=0.8495753407478333)
Finished Training
Total time taken: 10.148213624954224
{'Hydroxylation-K Validation Accuracy': 0.7898936170212766, 'Hydroxylation-K Validation Sensitivity': 0.7096296296296296, 'Hydroxylation-K Validation Specificity': 0.8105263157894737, 'Hydroxylation-K Validation Precision': 0.4967786026609556, 'Hydroxylation-K AUC ROC': 0.80317738791423, 'Hydroxylation-K AUC PR': 0.5926601827620609, 'Hydroxylation-K MCC': 0.4632052995361319, 'Hydroxylation-K F1': 0.580488968894766, 'Validation Loss (Hydroxylation-K)': 0.4336789051691691, 'Hydroxylation-P Validation Accuracy': 0.7829244369998138, 'Hydroxylation-P Validation Sensitivity': 0.7596296296296297, 'Hydroxylation-P Validation Specificity': 0.7880043892463464, 'Hydroxylation-P Validation Precision': 0.4401347794054618, 'Hydroxylation-P AUC ROC': 0.8337040857043101, 'Hydroxylation-P AUC PR': 0.596642385050004, 'Hydroxylation-P MCC': 0.45519343769104414, 'Hydroxylation-P F1': 0.5548951578815294, 'Validation Loss (Hydroxylation-P)': 0.3852920671304067, 'Validation Loss (total)': 0.8189709703127543, 'TimeToTrain': 10.623424180348714}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008052007527208613,
 'learning_rate_Hydroxylation-K': 0.007156150182530902,
 'learning_rate_Hydroxylation-P': 0.005361260540081876,
 'log_base': 2.4513234251972955,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2685179258,
 'sample_weights': [3.742047348397506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.346551035818173,
 'weight_decay_Hydroxylation-K': 1.7015431114015789,
 'weight_decay_Hydroxylation-P': 5.562838178580954}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1307.087
[2,     1] loss: 1301.918
[3,     1] loss: 1305.741
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033683291369028855,
 'learning_rate_Hydroxylation-K': 0.0015304844799478045,
 'learning_rate_Hydroxylation-P': 0.006223325166826751,
 'log_base': 2.530093600653869,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3068516743,
 'sample_weights': [1.8619126885510795, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.519859855608962,
 'weight_decay_Hydroxylation-K': 7.462038527237569,
 'weight_decay_Hydroxylation-P': 2.808510736149046}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.591
[2,     1] loss: 1286.973
[3,     1] loss: 1290.744
[4,     1] loss: 1285.220
[5,     1] loss: 1285.452
[6,     1] loss: 1279.869
[7,     1] loss: 1274.313
[8,     1] loss: 1261.794
[9,     1] loss: 1247.815
[10,     1] loss: 1215.387
[11,     1] loss: 1177.096
[12,     1] loss: 1142.950
[13,     1] loss: 1090.656
[14,     1] loss: 1104.445
[15,     1] loss: 1073.351
[16,     1] loss: 1058.386
[17,     1] loss: 1067.825
[18,     1] loss: 1110.771
[19,     1] loss: 1074.196
[20,     1] loss: 1068.840
[21,     1] loss: 1049.259
[22,     1] loss: 1062.044
[23,     1] loss: 1030.463
[24,     1] loss: 1028.707
[25,     1] loss: 998.729
[26,     1] loss: 970.502
[27,     1] loss: 958.059
[28,     1] loss: 923.403
[29,     1] loss: 975.921
[30,     1] loss: 948.605
[31,     1] loss: 970.791
[32,     1] loss: 1000.061
[33,     1] loss: 941.431
[34,     1] loss: 938.293
[35,     1] loss: 924.030
[36,     1] loss: 893.698
[37,     1] loss: 908.450
[38,     1] loss: 919.746
[39,     1] loss: 863.359
[40,     1] loss: 909.559
[41,     1] loss: 882.310
[42,     1] loss: 811.168
Early stopping applied (best metric=0.8995375633239746)
Finished Training
Total time taken: 7.1851537227630615
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.246
[2,     1] loss: 1285.648
[3,     1] loss: 1290.100
[4,     1] loss: 1288.870
[5,     1] loss: 1284.856
[6,     1] loss: 1284.376
[7,     1] loss: 1285.262
[8,     1] loss: 1281.056
[9,     1] loss: 1273.592
[10,     1] loss: 1262.313
[11,     1] loss: 1246.754
[12,     1] loss: 1213.468
[13,     1] loss: 1196.126
[14,     1] loss: 1156.687
[15,     1] loss: 1146.442
[16,     1] loss: 1114.358
[17,     1] loss: 1109.031
[18,     1] loss: 1102.824
[19,     1] loss: 1087.043
[20,     1] loss: 1089.429
[21,     1] loss: 1120.198
[22,     1] loss: 1085.134
[23,     1] loss: 1066.116
[24,     1] loss: 1025.045
[25,     1] loss: 1013.683
[26,     1] loss: 1022.505
[27,     1] loss: 1029.654
[28,     1] loss: 1041.981
[29,     1] loss: 980.263
[30,     1] loss: 1005.498
[31,     1] loss: 967.672
[32,     1] loss: 991.773
[33,     1] loss: 931.058
[34,     1] loss: 939.900
[35,     1] loss: 957.281
[36,     1] loss: 908.301
[37,     1] loss: 917.602
[38,     1] loss: 904.540
[39,     1] loss: 922.102
[40,     1] loss: 905.604
[41,     1] loss: 874.321
[42,     1] loss: 970.726
[43,     1] loss: 857.355
Early stopping applied (best metric=0.8941217660903931)
Finished Training
Total time taken: 5.966123580932617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.570
[2,     1] loss: 1288.969
[3,     1] loss: 1291.187
[4,     1] loss: 1288.422
[5,     1] loss: 1285.880
[6,     1] loss: 1285.464
[7,     1] loss: 1278.486
[8,     1] loss: 1278.570
[9,     1] loss: 1266.999
[10,     1] loss: 1255.237
[11,     1] loss: 1228.680
[12,     1] loss: 1201.777
[13,     1] loss: 1175.803
[14,     1] loss: 1125.919
[15,     1] loss: 1129.101
[16,     1] loss: 1079.996
[17,     1] loss: 1101.551
[18,     1] loss: 1096.074
[19,     1] loss: 1042.911
[20,     1] loss: 1095.496
[21,     1] loss: 1078.530
[22,     1] loss: 1041.416
[23,     1] loss: 1057.439
[24,     1] loss: 1005.110
[25,     1] loss: 1015.065
[26,     1] loss: 1006.472
[27,     1] loss: 976.116
[28,     1] loss: 1052.180
[29,     1] loss: 1039.935
[30,     1] loss: 973.980
[31,     1] loss: 971.397
[32,     1] loss: 948.793
[33,     1] loss: 963.839
[34,     1] loss: 960.370
[35,     1] loss: 968.230
[36,     1] loss: 867.435
[37,     1] loss: 919.241
[38,     1] loss: 928.466
[39,     1] loss: 947.977
[40,     1] loss: 900.684
[41,     1] loss: 994.278
[42,     1] loss: 870.155
[43,     1] loss: 928.048
[44,     1] loss: 858.853
[45,     1] loss: 907.685
[46,     1] loss: 811.025
[47,     1] loss: 894.748
[48,     1] loss: 880.042
[49,     1] loss: 812.817
[50,     1] loss: 882.292
[51,     1] loss: 845.541
[52,     1] loss: 836.127
[53,     1] loss: 817.673
[54,     1] loss: 814.041
[55,     1] loss: 802.791
[56,     1] loss: 757.584
[57,     1] loss: 730.723
[58,     1] loss: 747.036
[59,     1] loss: 747.686
[60,     1] loss: 695.159
[61,     1] loss: 719.786
[62,     1] loss: 710.942
[63,     1] loss: 675.920
[64,     1] loss: 705.048
[65,     1] loss: 697.146
[66,     1] loss: 713.295
[67,     1] loss: 660.729
[68,     1] loss: 634.147
Early stopping applied (best metric=0.7276434898376465)
Finished Training
Total time taken: 9.679203033447266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.866
[2,     1] loss: 1291.530
[3,     1] loss: 1286.108
[4,     1] loss: 1290.313
[5,     1] loss: 1284.726
[6,     1] loss: 1279.214
[7,     1] loss: 1277.405
[8,     1] loss: 1270.698
[9,     1] loss: 1261.043
[10,     1] loss: 1239.170
[11,     1] loss: 1217.670
[12,     1] loss: 1178.875
[13,     1] loss: 1147.036
[14,     1] loss: 1110.762
[15,     1] loss: 1127.541
[16,     1] loss: 1044.707
[17,     1] loss: 1134.735
[18,     1] loss: 1045.868
[19,     1] loss: 1064.605
[20,     1] loss: 1066.652
[21,     1] loss: 1032.557
[22,     1] loss: 998.739
[23,     1] loss: 1041.129
[24,     1] loss: 972.663
[25,     1] loss: 1020.917
[26,     1] loss: 996.730
[27,     1] loss: 991.172
[28,     1] loss: 982.928
[29,     1] loss: 991.001
[30,     1] loss: 961.791
[31,     1] loss: 906.496
[32,     1] loss: 920.520
[33,     1] loss: 895.641
[34,     1] loss: 912.144
[35,     1] loss: 963.052
[36,     1] loss: 883.780
[37,     1] loss: 906.117
[38,     1] loss: 909.190
[39,     1] loss: 876.332
[40,     1] loss: 828.049
[41,     1] loss: 867.883
[42,     1] loss: 822.140
[43,     1] loss: 814.215
[44,     1] loss: 812.727
[45,     1] loss: 928.870
[46,     1] loss: 858.252
[47,     1] loss: 798.555
[48,     1] loss: 818.288
[49,     1] loss: 751.670
[50,     1] loss: 791.087
[51,     1] loss: 747.049
[52,     1] loss: 833.266
[53,     1] loss: 868.225
[54,     1] loss: 709.998
[55,     1] loss: 779.338
[56,     1] loss: 730.047
[57,     1] loss: 680.268
[58,     1] loss: 775.129
[59,     1] loss: 663.275
[60,     1] loss: 739.296
[61,     1] loss: 795.390
[62,     1] loss: 632.731
[63,     1] loss: 782.284
[64,     1] loss: 626.052
[65,     1] loss: 637.519
[66,     1] loss: 618.708
[67,     1] loss: 824.257
[68,     1] loss: 732.137
Early stopping applied (best metric=0.7354193925857544)
Finished Training
Total time taken: 11.295240640640259
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1292.195
[2,     1] loss: 1285.852
[3,     1] loss: 1293.615
[4,     1] loss: 1285.545
[5,     1] loss: 1286.420
[6,     1] loss: 1282.885
[7,     1] loss: 1276.029
[8,     1] loss: 1269.720
[9,     1] loss: 1254.469
[10,     1] loss: 1229.927
[11,     1] loss: 1189.492
[12,     1] loss: 1200.254
[13,     1] loss: 1116.316
[14,     1] loss: 1112.958
[15,     1] loss: 1066.618
[16,     1] loss: 1073.226
[17,     1] loss: 1072.136
[18,     1] loss: 1097.243
[19,     1] loss: 1063.715
[20,     1] loss: 1059.351
[21,     1] loss: 1008.410
[22,     1] loss: 1018.049
[23,     1] loss: 979.462
[24,     1] loss: 1011.595
[25,     1] loss: 1010.874
[26,     1] loss: 948.370
[27,     1] loss: 971.034
[28,     1] loss: 1002.009
[29,     1] loss: 997.403
[30,     1] loss: 951.009
[31,     1] loss: 981.323
[32,     1] loss: 982.432
[33,     1] loss: 948.143
[34,     1] loss: 916.434
[35,     1] loss: 883.767
[36,     1] loss: 917.504
[37,     1] loss: 907.718
[38,     1] loss: 887.856
[39,     1] loss: 968.038
[40,     1] loss: 869.563
[41,     1] loss: 1000.801
[42,     1] loss: 841.593
[43,     1] loss: 955.940
[44,     1] loss: 858.112
[45,     1] loss: 869.248
[46,     1] loss: 900.639
[47,     1] loss: 826.442
[48,     1] loss: 871.511
[49,     1] loss: 855.499
[50,     1] loss: 835.690
[51,     1] loss: 853.751
[52,     1] loss: 782.767
[53,     1] loss: 836.103
[54,     1] loss: 720.012
Early stopping applied (best metric=0.8030762672424316)
Finished Training
Total time taken: 7.213150501251221
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.025
[2,     1] loss: 1281.168
[3,     1] loss: 1283.333
[4,     1] loss: 1290.366
[5,     1] loss: 1290.242
[6,     1] loss: 1278.065
[7,     1] loss: 1269.162
[8,     1] loss: 1264.260
[9,     1] loss: 1248.776
[10,     1] loss: 1214.807
[11,     1] loss: 1201.510
[12,     1] loss: 1144.009
[13,     1] loss: 1134.599
[14,     1] loss: 1114.966
[15,     1] loss: 1101.067
[16,     1] loss: 1078.360
[17,     1] loss: 1110.733
[18,     1] loss: 1044.406
[19,     1] loss: 1079.965
[20,     1] loss: 1068.879
[21,     1] loss: 1002.957
[22,     1] loss: 1081.465
[23,     1] loss: 1001.339
[24,     1] loss: 1054.415
[25,     1] loss: 1006.218
[26,     1] loss: 1022.203
[27,     1] loss: 992.092
[28,     1] loss: 967.373
[29,     1] loss: 987.111
[30,     1] loss: 956.030
[31,     1] loss: 946.285
[32,     1] loss: 953.388
[33,     1] loss: 952.659
[34,     1] loss: 912.679
[35,     1] loss: 895.904
[36,     1] loss: 876.734
[37,     1] loss: 915.954
[38,     1] loss: 892.766
[39,     1] loss: 903.234
[40,     1] loss: 871.525
[41,     1] loss: 827.063
[42,     1] loss: 861.857
[43,     1] loss: 825.356
[44,     1] loss: 807.984
[45,     1] loss: 833.843
[46,     1] loss: 788.177
[47,     1] loss: 786.608
[48,     1] loss: 761.287
[49,     1] loss: 733.943
[50,     1] loss: 778.901
[51,     1] loss: 864.130
[52,     1] loss: 879.893
[53,     1] loss: 730.253
[54,     1] loss: 712.730
[55,     1] loss: 724.284
[56,     1] loss: 680.135
[57,     1] loss: 705.600
Early stopping applied (best metric=0.7711169719696045)
Finished Training
Total time taken: 8.792185306549072
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.044
[2,     1] loss: 1292.268
[3,     1] loss: 1288.636
[4,     1] loss: 1287.708
[5,     1] loss: 1283.460
[6,     1] loss: 1281.510
[7,     1] loss: 1282.517
[8,     1] loss: 1275.580
[9,     1] loss: 1268.806
[10,     1] loss: 1247.508
[11,     1] loss: 1217.196
[12,     1] loss: 1180.106
[13,     1] loss: 1152.070
[14,     1] loss: 1132.959
[15,     1] loss: 1093.735
[16,     1] loss: 1092.020
[17,     1] loss: 1113.054
[18,     1] loss: 1078.359
[19,     1] loss: 1038.912
[20,     1] loss: 1022.949
[21,     1] loss: 1039.692
[22,     1] loss: 1037.433
[23,     1] loss: 1053.124
[24,     1] loss: 1049.974
[25,     1] loss: 1031.803
[26,     1] loss: 985.072
[27,     1] loss: 1017.922
[28,     1] loss: 991.735
[29,     1] loss: 979.173
[30,     1] loss: 947.092
[31,     1] loss: 956.731
[32,     1] loss: 907.757
[33,     1] loss: 909.148
[34,     1] loss: 922.555
[35,     1] loss: 879.625
[36,     1] loss: 869.933
[37,     1] loss: 894.079
[38,     1] loss: 934.359
[39,     1] loss: 944.737
[40,     1] loss: 879.563
[41,     1] loss: 880.742
[42,     1] loss: 837.357
[43,     1] loss: 898.561
[44,     1] loss: 823.691
[45,     1] loss: 873.625
[46,     1] loss: 774.269
[47,     1] loss: 812.897
[48,     1] loss: 751.325
[49,     1] loss: 807.377
[50,     1] loss: 776.473
[51,     1] loss: 879.664
[52,     1] loss: 758.807
[53,     1] loss: 797.355
[54,     1] loss: 720.663
[55,     1] loss: 825.851
Early stopping applied (best metric=0.8584230542182922)
Finished Training
Total time taken: 7.9851696491241455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.592
[2,     1] loss: 1286.909
[3,     1] loss: 1286.563
[4,     1] loss: 1286.990
[5,     1] loss: 1288.864
[6,     1] loss: 1284.380
[7,     1] loss: 1282.168
[8,     1] loss: 1282.476
[9,     1] loss: 1276.085
[10,     1] loss: 1268.450
[11,     1] loss: 1253.623
[12,     1] loss: 1235.449
[13,     1] loss: 1208.740
[14,     1] loss: 1187.710
[15,     1] loss: 1158.713
[16,     1] loss: 1093.020
[17,     1] loss: 1141.182
[18,     1] loss: 1068.829
[19,     1] loss: 1085.524
[20,     1] loss: 1103.618
[21,     1] loss: 1048.824
[22,     1] loss: 1079.932
[23,     1] loss: 1073.834
[24,     1] loss: 1030.796
[25,     1] loss: 1065.537
[26,     1] loss: 1065.737
[27,     1] loss: 1061.986
[28,     1] loss: 1009.253
[29,     1] loss: 1002.750
[30,     1] loss: 968.281
[31,     1] loss: 1062.101
[32,     1] loss: 987.747
[33,     1] loss: 1002.587
[34,     1] loss: 979.362
[35,     1] loss: 952.455
[36,     1] loss: 1013.598
[37,     1] loss: 918.330
[38,     1] loss: 992.365
[39,     1] loss: 924.223
[40,     1] loss: 915.486
[41,     1] loss: 978.412
[42,     1] loss: 902.144
[43,     1] loss: 921.367
[44,     1] loss: 914.740
[45,     1] loss: 832.127
[46,     1] loss: 899.838
[47,     1] loss: 872.207
[48,     1] loss: 897.288
[49,     1] loss: 838.252
[50,     1] loss: 851.051
[51,     1] loss: 831.427
[52,     1] loss: 799.345
[53,     1] loss: 841.258
[54,     1] loss: 783.086
[55,     1] loss: 762.992
[56,     1] loss: 774.075
[57,     1] loss: 784.746
[58,     1] loss: 798.341
[59,     1] loss: 847.953
[60,     1] loss: 940.236
[61,     1] loss: 790.042
[62,     1] loss: 754.198
[63,     1] loss: 756.746
[64,     1] loss: 733.395
[65,     1] loss: 752.565
[66,     1] loss: 794.251
[67,     1] loss: 807.538
[68,     1] loss: 645.420
[69,     1] loss: 718.254
[70,     1] loss: 679.572
[71,     1] loss: 737.287
[72,     1] loss: 610.338
[73,     1] loss: 730.388
[74,     1] loss: 637.593
[75,     1] loss: 637.737
Early stopping applied (best metric=0.6797159910202026)
Finished Training
Total time taken: 10.078212022781372
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.472
[2,     1] loss: 1286.701
[3,     1] loss: 1291.251
[4,     1] loss: 1288.065
[5,     1] loss: 1287.079
[6,     1] loss: 1286.609
[7,     1] loss: 1288.159
[8,     1] loss: 1283.481
[9,     1] loss: 1278.439
[10,     1] loss: 1270.023
[11,     1] loss: 1267.586
[12,     1] loss: 1245.871
[13,     1] loss: 1226.436
[14,     1] loss: 1196.937
[15,     1] loss: 1165.176
[16,     1] loss: 1157.681
[17,     1] loss: 1120.854
[18,     1] loss: 1082.433
[19,     1] loss: 1089.365
[20,     1] loss: 1024.362
[21,     1] loss: 1116.129
[22,     1] loss: 1037.358
[23,     1] loss: 1043.287
[24,     1] loss: 1068.760
[25,     1] loss: 1059.390
[26,     1] loss: 1020.130
[27,     1] loss: 1062.990
[28,     1] loss: 981.401
[29,     1] loss: 1075.685
[30,     1] loss: 1003.487
[31,     1] loss: 1006.501
[32,     1] loss: 1026.774
[33,     1] loss: 965.999
[34,     1] loss: 933.575
[35,     1] loss: 958.034
[36,     1] loss: 951.029
[37,     1] loss: 938.969
[38,     1] loss: 953.571
[39,     1] loss: 980.769
[40,     1] loss: 903.997
[41,     1] loss: 891.371
[42,     1] loss: 865.401
[43,     1] loss: 868.661
[44,     1] loss: 821.135
[45,     1] loss: 868.903
[46,     1] loss: 836.768
[47,     1] loss: 851.722
[48,     1] loss: 864.951
[49,     1] loss: 859.651
[50,     1] loss: 937.427
[51,     1] loss: 975.629
[52,     1] loss: 802.012
[53,     1] loss: 903.055
[54,     1] loss: 882.107
[55,     1] loss: 832.685
[56,     1] loss: 838.204
[57,     1] loss: 800.534
[58,     1] loss: 877.977
[59,     1] loss: 747.145
[60,     1] loss: 802.090
Early stopping applied (best metric=0.7816901803016663)
Finished Training
Total time taken: 9.845207691192627
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1293.989
[2,     1] loss: 1290.003
[3,     1] loss: 1291.568
[4,     1] loss: 1288.873
[5,     1] loss: 1287.353
[6,     1] loss: 1287.126
[7,     1] loss: 1281.023
[8,     1] loss: 1280.759
[9,     1] loss: 1269.916
[10,     1] loss: 1254.667
[11,     1] loss: 1227.323
[12,     1] loss: 1205.223
[13,     1] loss: 1173.006
[14,     1] loss: 1152.415
[15,     1] loss: 1107.321
[16,     1] loss: 1140.738
[17,     1] loss: 1084.254
[18,     1] loss: 1058.546
[19,     1] loss: 1081.993
[20,     1] loss: 1080.727
[21,     1] loss: 1039.501
[22,     1] loss: 1056.818
[23,     1] loss: 1073.402
[24,     1] loss: 1078.372
[25,     1] loss: 1053.867
[26,     1] loss: 1037.902
[27,     1] loss: 1030.320
[28,     1] loss: 1054.558
[29,     1] loss: 1000.458
[30,     1] loss: 1011.349
[31,     1] loss: 1016.742
[32,     1] loss: 969.220
[33,     1] loss: 971.633
[34,     1] loss: 946.671
[35,     1] loss: 943.311
[36,     1] loss: 953.393
[37,     1] loss: 901.717
[38,     1] loss: 926.236
[39,     1] loss: 866.775
[40,     1] loss: 918.745
[41,     1] loss: 952.237
[42,     1] loss: 942.732
[43,     1] loss: 902.345
[44,     1] loss: 900.909
[45,     1] loss: 874.106
[46,     1] loss: 865.560
[47,     1] loss: 810.464
[48,     1] loss: 898.274
[49,     1] loss: 889.522
[50,     1] loss: 781.457
[51,     1] loss: 849.009
[52,     1] loss: 773.342
[53,     1] loss: 783.477
[54,     1] loss: 773.109
[55,     1] loss: 794.642
[56,     1] loss: 833.618
[57,     1] loss: 767.368
[58,     1] loss: 799.139
[59,     1] loss: 730.797
[60,     1] loss: 744.965
[61,     1] loss: 739.489
[62,     1] loss: 719.970
[63,     1] loss: 715.245
[64,     1] loss: 695.712
[65,     1] loss: 740.709
[66,     1] loss: 632.076
[67,     1] loss: 625.624
[68,     1] loss: 627.088
[69,     1] loss: 632.395
Early stopping applied (best metric=0.6779706478118896)
Finished Training
Total time taken: 9.741205930709839
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.264
[2,     1] loss: 1291.619
[3,     1] loss: 1288.772
[4,     1] loss: 1285.412
[5,     1] loss: 1286.295
[6,     1] loss: 1285.670
[7,     1] loss: 1285.742
[8,     1] loss: 1284.610
[9,     1] loss: 1287.004
[10,     1] loss: 1280.506
[11,     1] loss: 1276.242
[12,     1] loss: 1272.727
[13,     1] loss: 1266.391
[14,     1] loss: 1253.856
[15,     1] loss: 1233.099
[16,     1] loss: 1196.447
[17,     1] loss: 1182.433
[18,     1] loss: 1150.133
[19,     1] loss: 1119.426
[20,     1] loss: 1086.714
[21,     1] loss: 1092.416
[22,     1] loss: 1076.157
[23,     1] loss: 1068.341
[24,     1] loss: 1090.559
[25,     1] loss: 1035.987
[26,     1] loss: 1048.892
[27,     1] loss: 1028.309
[28,     1] loss: 1015.769
[29,     1] loss: 1021.436
[30,     1] loss: 1000.200
[31,     1] loss: 993.413
[32,     1] loss: 1012.132
[33,     1] loss: 975.971
[34,     1] loss: 1003.248
[35,     1] loss: 934.477
[36,     1] loss: 950.096
[37,     1] loss: 941.795
[38,     1] loss: 977.980
[39,     1] loss: 958.136
[40,     1] loss: 949.986
[41,     1] loss: 931.726
[42,     1] loss: 972.248
[43,     1] loss: 921.201
[44,     1] loss: 937.176
[45,     1] loss: 935.889
[46,     1] loss: 949.367
[47,     1] loss: 877.141
[48,     1] loss: 897.163
[49,     1] loss: 883.995
[50,     1] loss: 860.685
[51,     1] loss: 776.786
[52,     1] loss: 857.779
[53,     1] loss: 809.966
[54,     1] loss: 809.136
[55,     1] loss: 826.378
[56,     1] loss: 786.584
[57,     1] loss: 796.064
[58,     1] loss: 828.399
[59,     1] loss: 821.597
[60,     1] loss: 776.711
[61,     1] loss: 794.518
[62,     1] loss: 752.872
[63,     1] loss: 807.985
[64,     1] loss: 802.033
[65,     1] loss: 720.380
[66,     1] loss: 758.099
[67,     1] loss: 705.043
[68,     1] loss: 740.654
[69,     1] loss: 714.188
[70,     1] loss: 740.917
[71,     1] loss: 641.755
Early stopping applied (best metric=0.7428445219993591)
Finished Training
Total time taken: 11.690247774124146
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.197
[2,     1] loss: 1286.190
[3,     1] loss: 1281.444
[4,     1] loss: 1280.705
[5,     1] loss: 1273.567
[6,     1] loss: 1257.183
[7,     1] loss: 1236.489
[8,     1] loss: 1211.984
[9,     1] loss: 1156.397
[10,     1] loss: 1153.174
[11,     1] loss: 1151.531
[12,     1] loss: 1134.607
[13,     1] loss: 1077.478
[14,     1] loss: 1100.626
[15,     1] loss: 1046.357
[16,     1] loss: 1021.965
[17,     1] loss: 1090.434
[18,     1] loss: 1058.416
[19,     1] loss: 1084.668
[20,     1] loss: 1027.352
[21,     1] loss: 1047.615
[22,     1] loss: 1042.217
[23,     1] loss: 994.074
[24,     1] loss: 976.703
[25,     1] loss: 1008.046
[26,     1] loss: 959.803
[27,     1] loss: 997.509
[28,     1] loss: 956.135
[29,     1] loss: 898.773
[30,     1] loss: 929.374
[31,     1] loss: 910.885
[32,     1] loss: 867.148
[33,     1] loss: 842.777
[34,     1] loss: 913.284
[35,     1] loss: 887.774
[36,     1] loss: 867.519
[37,     1] loss: 897.768
[38,     1] loss: 895.431
[39,     1] loss: 862.743
[40,     1] loss: 834.586
[41,     1] loss: 831.481
[42,     1] loss: 805.920
[43,     1] loss: 848.838
[44,     1] loss: 770.741
[45,     1] loss: 791.740
[46,     1] loss: 746.151
[47,     1] loss: 782.583
[48,     1] loss: 813.062
[49,     1] loss: 786.595
[50,     1] loss: 795.777
[51,     1] loss: 738.864
[52,     1] loss: 801.761
[53,     1] loss: 740.390
[54,     1] loss: 710.657
[55,     1] loss: 754.364
[56,     1] loss: 740.950
[57,     1] loss: 701.179
[58,     1] loss: 666.714
[59,     1] loss: 707.860
[60,     1] loss: 751.125
[61,     1] loss: 605.984
[62,     1] loss: 882.911
[63,     1] loss: 683.597
[64,     1] loss: 809.403
[65,     1] loss: 633.284
[66,     1] loss: 738.511
Early stopping applied (best metric=0.9670627117156982)
Finished Training
Total time taken: 8.777184009552002
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.732
[2,     1] loss: 1287.288
[3,     1] loss: 1284.879
[4,     1] loss: 1284.417
[5,     1] loss: 1290.636
[6,     1] loss: 1287.723
[7,     1] loss: 1284.751
[8,     1] loss: 1285.522
[9,     1] loss: 1273.366
[10,     1] loss: 1271.422
[11,     1] loss: 1259.268
[12,     1] loss: 1234.029
[13,     1] loss: 1211.553
[14,     1] loss: 1175.377
[15,     1] loss: 1156.020
[16,     1] loss: 1106.839
[17,     1] loss: 1084.193
[18,     1] loss: 1100.893
[19,     1] loss: 1081.994
[20,     1] loss: 1064.762
[21,     1] loss: 1096.176
[22,     1] loss: 1065.577
[23,     1] loss: 1056.952
[24,     1] loss: 1050.067
[25,     1] loss: 1060.702
[26,     1] loss: 1028.052
[27,     1] loss: 1024.576
[28,     1] loss: 1020.553
[29,     1] loss: 1022.056
[30,     1] loss: 1012.171
[31,     1] loss: 991.652
[32,     1] loss: 997.070
[33,     1] loss: 984.747
[34,     1] loss: 993.740
[35,     1] loss: 945.706
[36,     1] loss: 946.563
[37,     1] loss: 1004.231
[38,     1] loss: 923.795
[39,     1] loss: 966.145
[40,     1] loss: 924.259
[41,     1] loss: 942.963
[42,     1] loss: 1013.260
[43,     1] loss: 947.913
[44,     1] loss: 938.902
[45,     1] loss: 950.884
[46,     1] loss: 923.966
[47,     1] loss: 967.247
[48,     1] loss: 883.109
[49,     1] loss: 901.684
[50,     1] loss: 854.249
[51,     1] loss: 826.540
[52,     1] loss: 835.025
[53,     1] loss: 833.278
[54,     1] loss: 813.081
[55,     1] loss: 779.566
[56,     1] loss: 810.745
[57,     1] loss: 776.014
[58,     1] loss: 822.368
[59,     1] loss: 843.838
[60,     1] loss: 745.053
[61,     1] loss: 868.876
[62,     1] loss: 813.998
Early stopping applied (best metric=0.6581141948699951)
Finished Training
Total time taken: 10.244216203689575
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.858
[2,     1] loss: 1289.827
[3,     1] loss: 1286.251
[4,     1] loss: 1287.951
[5,     1] loss: 1286.816
[6,     1] loss: 1280.242
[7,     1] loss: 1277.953
[8,     1] loss: 1270.075
[9,     1] loss: 1245.408
[10,     1] loss: 1221.238
[11,     1] loss: 1173.255
[12,     1] loss: 1144.405
[13,     1] loss: 1098.411
[14,     1] loss: 1087.919
[15,     1] loss: 1084.112
[16,     1] loss: 1079.125
[17,     1] loss: 1052.154
[18,     1] loss: 1070.461
[19,     1] loss: 1044.258
[20,     1] loss: 1066.027
[21,     1] loss: 1030.458
[22,     1] loss: 1027.653
[23,     1] loss: 1035.084
[24,     1] loss: 1031.795
[25,     1] loss: 1004.610
[26,     1] loss: 1007.781
[27,     1] loss: 995.699
[28,     1] loss: 947.842
[29,     1] loss: 955.147
[30,     1] loss: 900.311
[31,     1] loss: 1019.197
[32,     1] loss: 969.570
[33,     1] loss: 917.681
[34,     1] loss: 953.833
[35,     1] loss: 888.818
[36,     1] loss: 948.701
[37,     1] loss: 915.991
[38,     1] loss: 934.806
[39,     1] loss: 904.090
[40,     1] loss: 901.302
[41,     1] loss: 852.984
[42,     1] loss: 864.939
[43,     1] loss: 885.298
[44,     1] loss: 819.113
[45,     1] loss: 829.227
[46,     1] loss: 835.990
[47,     1] loss: 817.049
[48,     1] loss: 861.288
[49,     1] loss: 876.765
[50,     1] loss: 808.952
[51,     1] loss: 788.818
[52,     1] loss: 841.928
[53,     1] loss: 797.870
[54,     1] loss: 828.255
[55,     1] loss: 795.683
[56,     1] loss: 873.041
[57,     1] loss: 709.087
[58,     1] loss: 869.049
[59,     1] loss: 764.470
[60,     1] loss: 748.768
[61,     1] loss: 740.657
[62,     1] loss: 780.950
[63,     1] loss: 640.599
[64,     1] loss: 736.897
[65,     1] loss: 747.786
[66,     1] loss: 748.319
[67,     1] loss: 716.341
[68,     1] loss: 660.820
[69,     1] loss: 691.093
[70,     1] loss: 581.147
[71,     1] loss: 638.067
[72,     1] loss: 605.804
[73,     1] loss: 581.781
[74,     1] loss: 619.089
[75,     1] loss: 591.065
[76,     1] loss: 555.322
[77,     1] loss: 568.198
[78,     1] loss: 682.323
[79,     1] loss: 596.301
[80,     1] loss: 548.635
[81,     1] loss: 585.403
[82,     1] loss: 590.401
[83,     1] loss: 524.872
[84,     1] loss: 569.128
[85,     1] loss: 549.468
[86,     1] loss: 548.035
[87,     1] loss: 537.165
[88,     1] loss: 514.852
Early stopping applied (best metric=0.7462151050567627)
Finished Training
Total time taken: 11.787248373031616
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1288.583
[2,     1] loss: 1291.751
[3,     1] loss: 1288.622
[4,     1] loss: 1283.541
[5,     1] loss: 1282.539
[6,     1] loss: 1276.184
[7,     1] loss: 1266.533
[8,     1] loss: 1246.301
[9,     1] loss: 1222.421
[10,     1] loss: 1192.538
[11,     1] loss: 1137.669
[12,     1] loss: 1127.106
[13,     1] loss: 1130.946
[14,     1] loss: 1084.152
[15,     1] loss: 1099.951
[16,     1] loss: 1093.042
[17,     1] loss: 1100.603
[18,     1] loss: 1047.064
[19,     1] loss: 1082.570
[20,     1] loss: 1001.500
[21,     1] loss: 1020.625
[22,     1] loss: 1034.819
[23,     1] loss: 975.234
[24,     1] loss: 1004.122
[25,     1] loss: 1015.167
[26,     1] loss: 990.371
[27,     1] loss: 978.748
[28,     1] loss: 966.858
[29,     1] loss: 950.737
[30,     1] loss: 958.755
[31,     1] loss: 924.746
[32,     1] loss: 905.368
[33,     1] loss: 944.376
[34,     1] loss: 902.672
[35,     1] loss: 935.701
[36,     1] loss: 898.731
[37,     1] loss: 920.978
[38,     1] loss: 969.705
[39,     1] loss: 860.089
[40,     1] loss: 899.761
[41,     1] loss: 888.360
[42,     1] loss: 912.860
[43,     1] loss: 844.043
[44,     1] loss: 847.075
[45,     1] loss: 841.747
[46,     1] loss: 754.873
[47,     1] loss: 809.886
[48,     1] loss: 800.880
[49,     1] loss: 784.029
[50,     1] loss: 800.996
[51,     1] loss: 778.290
[52,     1] loss: 760.724
[53,     1] loss: 799.098
[54,     1] loss: 714.779
[55,     1] loss: 769.087
[56,     1] loss: 703.120
[57,     1] loss: 717.654
[58,     1] loss: 712.396
Early stopping applied (best metric=0.8017281293869019)
Finished Training
Total time taken: 9.48819899559021
{'Hydroxylation-K Validation Accuracy': 0.7857269503546099, 'Hydroxylation-K Validation Sensitivity': 0.7081481481481482, 'Hydroxylation-K Validation Specificity': 0.8052631578947368, 'Hydroxylation-K Validation Precision': 0.4961943666355431, 'Hydroxylation-K AUC ROC': 0.8089473684210526, 'Hydroxylation-K AUC PR': 0.6145651888642566, 'Hydroxylation-K MCC': 0.4589641764100958, 'Hydroxylation-K F1': 0.5760628645785912, 'Validation Loss (Hydroxylation-K)': 0.4245602786540985, 'Hydroxylation-P Validation Accuracy': 0.8087133140449724, 'Hydroxylation-P Validation Sensitivity': 0.8068253968253969, 'Hydroxylation-P Validation Specificity': 0.8091251433986733, 'Hydroxylation-P Validation Precision': 0.48136565771358797, 'Hydroxylation-P AUC ROC': 0.8690029507072751, 'Hydroxylation-P AUC PR': 0.6308036502851709, 'Hydroxylation-P MCC': 0.5161231954843081, 'Hydroxylation-P F1': 0.6006514669500886, 'Validation Loss (Hydroxylation-P)': 0.3584183812141418, 'Validation Loss (total)': 0.7829786658287048, 'TimeToTrain': 9.317863162358602}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022872418801908826,
 'learning_rate_Hydroxylation-K': 0.006858327308450673,
 'learning_rate_Hydroxylation-P': 0.0013006569226577578,
 'log_base': 2.8652923720059267,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 762234254,
 'sample_weights': [1.799806304169198, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6674696026254106,
 'weight_decay_Hydroxylation-K': 3.0583154653766265,
 'weight_decay_Hydroxylation-P': 0.9116689084150453}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.468
[2,     1] loss: 1241.038
[3,     1] loss: 1242.352
[4,     1] loss: 1244.411
[5,     1] loss: 1242.534
[6,     1] loss: 1238.096
[7,     1] loss: 1239.954
[8,     1] loss: 1245.452
[9,     1] loss: 1241.368
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005961986608243788,
 'learning_rate_Hydroxylation-K': 0.005102398121158215,
 'learning_rate_Hydroxylation-P': 0.0068688813399037105,
 'log_base': 2.3783410966707144,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1131833146,
 'sample_weights': [1.5859125098466949, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.75258027525147,
 'weight_decay_Hydroxylation-K': 8.200138169748833,
 'weight_decay_Hydroxylation-P': 2.007890492847819}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.400
[2,     1] loss: 1314.141
[3,     1] loss: 1308.782
[4,     1] loss: 1315.262
[5,     1] loss: 1314.814
[6,     1] loss: 1313.450
[7,     1] loss: 1315.809
[8,     1] loss: 1313.054
[9,     1] loss: 1309.264
[10,     1] loss: 1306.957
[11,     1] loss: 1299.061
[12,     1] loss: 1287.506
[13,     1] loss: 1261.372
[14,     1] loss: 1225.973
[15,     1] loss: 1187.638
[16,     1] loss: 1160.743
[17,     1] loss: 1084.596
[18,     1] loss: 1101.674
[19,     1] loss: 1101.817
[20,     1] loss: 1114.913
[21,     1] loss: 1053.121
[22,     1] loss: 1101.535
[23,     1] loss: 1033.737
[24,     1] loss: 1153.005
[25,     1] loss: 1060.322
[26,     1] loss: 1049.259
[27,     1] loss: 1068.986
[28,     1] loss: 1056.006
[29,     1] loss: 1040.911
[30,     1] loss: 989.373
[31,     1] loss: 957.979
[32,     1] loss: 990.963
[33,     1] loss: 936.658
[34,     1] loss: 921.246
[35,     1] loss: 960.039
[36,     1] loss: 1101.606
[37,     1] loss: 933.440
[38,     1] loss: 977.254
[39,     1] loss: 922.373
[40,     1] loss: 991.320
[41,     1] loss: 962.519
[42,     1] loss: 913.824
[43,     1] loss: 964.562
[44,     1] loss: 846.375
[45,     1] loss: 959.713
[46,     1] loss: 881.041
[47,     1] loss: 956.839
[48,     1] loss: 909.528
[49,     1] loss: 844.355
[50,     1] loss: 900.608
[51,     1] loss: 859.988
Early stopping applied (best metric=0.8407680988311768)
Finished Training
Total time taken: 6.822142839431763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.060
[2,     1] loss: 1319.305
[3,     1] loss: 1315.292
[4,     1] loss: 1315.154
[5,     1] loss: 1314.952
[6,     1] loss: 1316.437
[7,     1] loss: 1310.194
[8,     1] loss: 1310.162
[9,     1] loss: 1315.252
[10,     1] loss: 1309.704
[11,     1] loss: 1311.348
[12,     1] loss: 1304.311
[13,     1] loss: 1297.938
[14,     1] loss: 1285.402
[15,     1] loss: 1267.958
[16,     1] loss: 1249.093
[17,     1] loss: 1214.774
[18,     1] loss: 1177.373
[19,     1] loss: 1154.543
[20,     1] loss: 1096.163
[21,     1] loss: 1149.137
[22,     1] loss: 1168.847
[23,     1] loss: 1074.487
[24,     1] loss: 1073.488
[25,     1] loss: 1029.689
[26,     1] loss: 1116.302
[27,     1] loss: 1068.064
[28,     1] loss: 1078.799
[29,     1] loss: 1044.347
[30,     1] loss: 1050.529
[31,     1] loss: 1024.997
[32,     1] loss: 1021.569
[33,     1] loss: 978.945
[34,     1] loss: 1015.055
[35,     1] loss: 973.688
[36,     1] loss: 1017.850
[37,     1] loss: 1024.415
[38,     1] loss: 953.077
[39,     1] loss: 991.490
[40,     1] loss: 927.956
[41,     1] loss: 967.853
[42,     1] loss: 944.594
[43,     1] loss: 908.903
[44,     1] loss: 888.860
[45,     1] loss: 867.683
[46,     1] loss: 852.689
[47,     1] loss: 827.723
[48,     1] loss: 831.108
[49,     1] loss: 804.422
[50,     1] loss: 937.139
[51,     1] loss: 1608.536
[52,     1] loss: 872.879
[53,     1] loss: 1095.926
[54,     1] loss: 1071.776
[55,     1] loss: 1057.457
[56,     1] loss: 1060.201
[57,     1] loss: 1073.430
[58,     1] loss: 1046.581
[59,     1] loss: 1004.709
[60,     1] loss: 1057.729
[61,     1] loss: 979.972
[62,     1] loss: 1015.892
Early stopping applied (best metric=0.7603346705436707)
Finished Training
Total time taken: 10.146214008331299
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.897
[2,     1] loss: 1311.513
[3,     1] loss: 1317.618
[4,     1] loss: 1313.456
[5,     1] loss: 1310.103
[6,     1] loss: 1313.196
[7,     1] loss: 1310.324
[8,     1] loss: 1298.943
[9,     1] loss: 1290.200
[10,     1] loss: 1261.357
[11,     1] loss: 1237.563
[12,     1] loss: 1201.544
[13,     1] loss: 1175.594
[14,     1] loss: 1135.244
[15,     1] loss: 1104.066
[16,     1] loss: 1053.546
[17,     1] loss: 1100.485
[18,     1] loss: 1152.957
[19,     1] loss: 1074.485
[20,     1] loss: 1054.011
[21,     1] loss: 1065.292
[22,     1] loss: 1032.060
[23,     1] loss: 1032.166
[24,     1] loss: 1008.208
[25,     1] loss: 1005.849
[26,     1] loss: 996.485
[27,     1] loss: 1023.832
[28,     1] loss: 996.666
[29,     1] loss: 957.918
[30,     1] loss: 1009.853
[31,     1] loss: 929.583
[32,     1] loss: 991.116
[33,     1] loss: 918.473
[34,     1] loss: 916.935
[35,     1] loss: 914.414
[36,     1] loss: 881.167
[37,     1] loss: 872.303
[38,     1] loss: 865.476
[39,     1] loss: 904.751
[40,     1] loss: 1354.885
[41,     1] loss: 954.140
[42,     1] loss: 1073.878
[43,     1] loss: 1029.102
[44,     1] loss: 1058.225
[45,     1] loss: 1046.951
[46,     1] loss: 999.568
[47,     1] loss: 967.027
[48,     1] loss: 991.664
[49,     1] loss: 897.588
[50,     1] loss: 986.365
[51,     1] loss: 937.620
[52,     1] loss: 937.227
[53,     1] loss: 895.847
[54,     1] loss: 880.365
[55,     1] loss: 910.611
[56,     1] loss: 837.504
Early stopping applied (best metric=0.7630397081375122)
Finished Training
Total time taken: 7.49915885925293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.222
[2,     1] loss: 1319.248
[3,     1] loss: 1313.088
[4,     1] loss: 1313.608
[5,     1] loss: 1311.748
[6,     1] loss: 1306.556
[7,     1] loss: 1292.480
[8,     1] loss: 1270.743
[9,     1] loss: 1245.042
[10,     1] loss: 1229.380
[11,     1] loss: 1174.021
[12,     1] loss: 1152.009
[13,     1] loss: 1157.110
[14,     1] loss: 1236.672
[15,     1] loss: 1112.898
[16,     1] loss: 1169.131
[17,     1] loss: 1058.583
[18,     1] loss: 1109.102
[19,     1] loss: 1091.156
[20,     1] loss: 1072.117
[21,     1] loss: 1053.774
[22,     1] loss: 1037.728
[23,     1] loss: 1052.044
[24,     1] loss: 1052.323
[25,     1] loss: 1001.481
[26,     1] loss: 1010.126
[27,     1] loss: 981.228
[28,     1] loss: 993.810
[29,     1] loss: 973.930
[30,     1] loss: 927.924
[31,     1] loss: 949.430
[32,     1] loss: 970.422
[33,     1] loss: 953.729
[34,     1] loss: 989.585
[35,     1] loss: 1007.901
[36,     1] loss: 919.813
[37,     1] loss: 967.550
[38,     1] loss: 941.413
[39,     1] loss: 894.887
[40,     1] loss: 919.413
[41,     1] loss: 919.096
[42,     1] loss: 864.273
[43,     1] loss: 865.279
[44,     1] loss: 935.307
[45,     1] loss: 929.879
[46,     1] loss: 878.647
[47,     1] loss: 856.810
[48,     1] loss: 869.759
[49,     1] loss: 840.591
[50,     1] loss: 903.169
[51,     1] loss: 860.414
[52,     1] loss: 792.980
[53,     1] loss: 805.746
[54,     1] loss: 789.732
[55,     1] loss: 886.923
[56,     1] loss: 1457.708
[57,     1] loss: 895.530
[58,     1] loss: 1014.294
[59,     1] loss: 990.771
[60,     1] loss: 1006.364
[61,     1] loss: 988.306
[62,     1] loss: 991.770
[63,     1] loss: 920.093
[64,     1] loss: 962.440
[65,     1] loss: 957.831
[66,     1] loss: 978.949
[67,     1] loss: 942.857
[68,     1] loss: 898.862
[69,     1] loss: 913.467
[70,     1] loss: 901.291
[71,     1] loss: 886.606
[72,     1] loss: 866.886
[73,     1] loss: 879.709
[74,     1] loss: 890.437
[75,     1] loss: 828.073
[76,     1] loss: 858.056
[77,     1] loss: 970.372
[78,     1] loss: 895.947
[79,     1] loss: 870.865
[80,     1] loss: 809.326
[81,     1] loss: 871.618
[82,     1] loss: 795.730
[83,     1] loss: 783.605
[84,     1] loss: 1210.887
Early stopping applied (best metric=0.8306623101234436)
Finished Training
Total time taken: 13.254279375076294
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1318.266
[2,     1] loss: 1319.795
[3,     1] loss: 1331.333
[4,     1] loss: 1314.940
[5,     1] loss: 1317.352
[6,     1] loss: 1313.681
[7,     1] loss: 1313.750
[8,     1] loss: 1312.534
[9,     1] loss: 1310.742
[10,     1] loss: 1303.593
[11,     1] loss: 1292.220
[12,     1] loss: 1279.237
[13,     1] loss: 1244.781
[14,     1] loss: 1210.556
[15,     1] loss: 1185.843
[16,     1] loss: 1127.976
[17,     1] loss: 1086.026
[18,     1] loss: 1093.761
[19,     1] loss: 1044.024
[20,     1] loss: 1049.016
[21,     1] loss: 1038.250
[22,     1] loss: 1039.755
[23,     1] loss: 1014.338
[24,     1] loss: 1053.271
[25,     1] loss: 1015.222
[26,     1] loss: 1007.548
[27,     1] loss: 975.134
[28,     1] loss: 972.782
[29,     1] loss: 954.845
[30,     1] loss: 964.671
[31,     1] loss: 979.871
[32,     1] loss: 945.262
[33,     1] loss: 912.357
[34,     1] loss: 923.126
[35,     1] loss: 923.682
[36,     1] loss: 960.191
[37,     1] loss: 928.582
[38,     1] loss: 938.093
[39,     1] loss: 924.849
[40,     1] loss: 976.664
[41,     1] loss: 884.756
[42,     1] loss: 852.669
[43,     1] loss: 903.347
[44,     1] loss: 828.345
[45,     1] loss: 862.348
[46,     1] loss: 916.934
[47,     1] loss: 1439.615
[48,     1] loss: 916.636
[49,     1] loss: 1062.919
[50,     1] loss: 1044.881
[51,     1] loss: 1040.614
[52,     1] loss: 1051.124
[53,     1] loss: 1046.727
[54,     1] loss: 1039.243
[55,     1] loss: 955.527
[56,     1] loss: 996.327
[57,     1] loss: 976.435
[58,     1] loss: 960.072
[59,     1] loss: 954.426
[60,     1] loss: 979.331
[61,     1] loss: 945.949
[62,     1] loss: 961.003
[63,     1] loss: 937.907
[64,     1] loss: 918.667
Early stopping applied (best metric=0.7931457757949829)
Finished Training
Total time taken: 10.47022294998169
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.391
[2,     1] loss: 1315.567
[3,     1] loss: 1316.288
[4,     1] loss: 1313.851
[5,     1] loss: 1312.499
[6,     1] loss: 1310.637
[7,     1] loss: 1311.281
[8,     1] loss: 1310.076
[9,     1] loss: 1304.894
[10,     1] loss: 1297.584
[11,     1] loss: 1285.607
[12,     1] loss: 1263.192
[13,     1] loss: 1237.691
[14,     1] loss: 1180.643
[15,     1] loss: 1153.347
[16,     1] loss: 1203.878
[17,     1] loss: 1151.854
[18,     1] loss: 1107.662
[19,     1] loss: 1123.360
[20,     1] loss: 1077.806
[21,     1] loss: 1107.646
[22,     1] loss: 1072.358
[23,     1] loss: 1098.990
[24,     1] loss: 1044.324
[25,     1] loss: 1049.572
[26,     1] loss: 1022.522
[27,     1] loss: 1045.825
[28,     1] loss: 986.753
[29,     1] loss: 1079.674
[30,     1] loss: 920.534
[31,     1] loss: 1064.189
[32,     1] loss: 970.834
[33,     1] loss: 1031.735
[34,     1] loss: 930.791
[35,     1] loss: 972.831
[36,     1] loss: 947.383
[37,     1] loss: 926.672
[38,     1] loss: 994.420
[39,     1] loss: 903.340
[40,     1] loss: 963.522
[41,     1] loss: 930.382
[42,     1] loss: 915.592
[43,     1] loss: 917.212
[44,     1] loss: 890.359
[45,     1] loss: 881.699
[46,     1] loss: 830.045
[47,     1] loss: 733.615
[48,     1] loss: 786.637
[49,     1] loss: 791.085
[50,     1] loss: 1418.225
[51,     1] loss: 1086.728
[52,     1] loss: 863.646
[53,     1] loss: 996.642
[54,     1] loss: 1012.338
[55,     1] loss: 989.736
[56,     1] loss: 968.028
[57,     1] loss: 975.013
[58,     1] loss: 969.265
[59,     1] loss: 964.373
[60,     1] loss: 914.182
[61,     1] loss: 919.725
[62,     1] loss: 925.178
[63,     1] loss: 857.432
[64,     1] loss: 882.204
[65,     1] loss: 854.830
[66,     1] loss: 813.939
[67,     1] loss: 856.504
[68,     1] loss: 812.633
[69,     1] loss: 785.900
[70,     1] loss: 676.362
[71,     1] loss: 781.748
[72,     1] loss: 941.967
[73,     1] loss: 908.856
[74,     1] loss: 804.558
[75,     1] loss: 866.610
[76,     1] loss: 756.413
[77,     1] loss: 950.792
[78,     1] loss: 996.320
[79,     1] loss: 855.423
[80,     1] loss: 866.839
[81,     1] loss: 909.575
[82,     1] loss: 841.799
[83,     1] loss: 832.181
[84,     1] loss: 713.312
[85,     1] loss: 889.496
Early stopping applied (best metric=0.8292458057403564)
Finished Training
Total time taken: 13.114279508590698
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.416
[2,     1] loss: 1316.243
[3,     1] loss: 1318.679
[4,     1] loss: 1317.593
[5,     1] loss: 1310.866
[6,     1] loss: 1302.131
[7,     1] loss: 1280.986
[8,     1] loss: 1251.136
[9,     1] loss: 1210.254
[10,     1] loss: 1168.135
[11,     1] loss: 1108.506
[12,     1] loss: 1114.568
[13,     1] loss: 1150.375
[14,     1] loss: 1029.633
[15,     1] loss: 1052.918
[16,     1] loss: 1095.402
[17,     1] loss: 1060.744
[18,     1] loss: 1052.292
[19,     1] loss: 1039.382
[20,     1] loss: 1046.070
[21,     1] loss: 1075.548
[22,     1] loss: 955.963
[23,     1] loss: 1015.444
[24,     1] loss: 970.307
[25,     1] loss: 1030.299
[26,     1] loss: 958.602
[27,     1] loss: 945.755
[28,     1] loss: 939.473
[29,     1] loss: 887.235
[30,     1] loss: 867.955
[31,     1] loss: 889.609
[32,     1] loss: 886.042
[33,     1] loss: 863.062
[34,     1] loss: 829.379
[35,     1] loss: 951.939
[36,     1] loss: 1114.717
[37,     1] loss: 859.213
[38,     1] loss: 1003.670
[39,     1] loss: 1009.429
[40,     1] loss: 915.830
[41,     1] loss: 974.269
[42,     1] loss: 973.624
[43,     1] loss: 865.230
[44,     1] loss: 917.394
[45,     1] loss: 851.917
[46,     1] loss: 880.930
[47,     1] loss: 822.687
[48,     1] loss: 899.390
[49,     1] loss: 794.695
[50,     1] loss: 869.259
[51,     1] loss: 779.481
[52,     1] loss: 804.861
[53,     1] loss: 757.508
Early stopping applied (best metric=0.928404688835144)
Finished Training
Total time taken: 7.127149820327759
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.612
[2,     1] loss: 1319.454
[3,     1] loss: 1315.021
[4,     1] loss: 1313.454
[5,     1] loss: 1315.017
[6,     1] loss: 1311.416
[7,     1] loss: 1312.635
[8,     1] loss: 1313.178
[9,     1] loss: 1307.695
[10,     1] loss: 1302.060
[11,     1] loss: 1294.319
[12,     1] loss: 1276.446
[13,     1] loss: 1234.789
[14,     1] loss: 1225.239
[15,     1] loss: 1191.118
[16,     1] loss: 1175.801
[17,     1] loss: 1154.648
[18,     1] loss: 1108.985
[19,     1] loss: 1114.523
[20,     1] loss: 1100.763
[21,     1] loss: 1103.727
[22,     1] loss: 1131.201
[23,     1] loss: 1029.487
[24,     1] loss: 1075.508
[25,     1] loss: 1037.467
[26,     1] loss: 1070.539
[27,     1] loss: 1019.316
[28,     1] loss: 1047.253
[29,     1] loss: 979.220
[30,     1] loss: 1016.247
[31,     1] loss: 956.462
[32,     1] loss: 1056.327
[33,     1] loss: 946.744
[34,     1] loss: 974.417
[35,     1] loss: 976.184
[36,     1] loss: 956.565
[37,     1] loss: 892.376
[38,     1] loss: 943.536
[39,     1] loss: 1008.005
[40,     1] loss: 981.203
[41,     1] loss: 894.716
[42,     1] loss: 892.535
[43,     1] loss: 955.149
[44,     1] loss: 910.267
[45,     1] loss: 842.749
[46,     1] loss: 911.594
[47,     1] loss: 934.888
[48,     1] loss: 847.221
[49,     1] loss: 927.037
[50,     1] loss: 961.545
[51,     1] loss: 873.795
[52,     1] loss: 870.857
[53,     1] loss: 858.931
[54,     1] loss: 847.853
[55,     1] loss: 843.653
[56,     1] loss: 786.416
[57,     1] loss: 803.251
[58,     1] loss: 985.350
[59,     1] loss: 1449.869
[60,     1] loss: 824.867
[61,     1] loss: 1000.113
[62,     1] loss: 1036.307
[63,     1] loss: 996.345
[64,     1] loss: 994.093
[65,     1] loss: 983.099
[66,     1] loss: 989.510
[67,     1] loss: 973.070
[68,     1] loss: 952.339
[69,     1] loss: 1026.000
[70,     1] loss: 956.448
[71,     1] loss: 998.851
[72,     1] loss: 948.258
[73,     1] loss: 966.723
[74,     1] loss: 1000.120
[75,     1] loss: 915.956
Early stopping applied (best metric=0.7840679883956909)
Finished Training
Total time taken: 12.308257102966309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1313.020
[2,     1] loss: 1319.228
[3,     1] loss: 1316.096
[4,     1] loss: 1310.648
[5,     1] loss: 1308.867
[6,     1] loss: 1302.084
[7,     1] loss: 1287.943
[8,     1] loss: 1266.228
[9,     1] loss: 1234.487
[10,     1] loss: 1207.905
[11,     1] loss: 1173.702
[12,     1] loss: 1126.817
[13,     1] loss: 1133.665
[14,     1] loss: 1172.806
[15,     1] loss: 1115.819
[16,     1] loss: 1105.929
[17,     1] loss: 1105.702
[18,     1] loss: 1082.709
[19,     1] loss: 1035.822
[20,     1] loss: 1084.698
[21,     1] loss: 1039.020
[22,     1] loss: 1027.166
[23,     1] loss: 1028.592
[24,     1] loss: 991.551
[25,     1] loss: 1176.516
[26,     1] loss: 974.899
[27,     1] loss: 1122.759
[28,     1] loss: 1006.915
[29,     1] loss: 1049.475
[30,     1] loss: 1086.641
[31,     1] loss: 976.151
[32,     1] loss: 993.647
[33,     1] loss: 1017.047
[34,     1] loss: 981.662
[35,     1] loss: 977.982
[36,     1] loss: 929.569
[37,     1] loss: 906.712
[38,     1] loss: 951.286
[39,     1] loss: 978.047
[40,     1] loss: 915.110
[41,     1] loss: 951.384
[42,     1] loss: 907.389
[43,     1] loss: 862.465
[44,     1] loss: 963.199
[45,     1] loss: 940.819
[46,     1] loss: 860.282
[47,     1] loss: 897.506
[48,     1] loss: 838.648
[49,     1] loss: 826.368
[50,     1] loss: 838.127
[51,     1] loss: 1317.694
[52,     1] loss: 1107.132
[53,     1] loss: 856.334
[54,     1] loss: 978.189
[55,     1] loss: 995.006
[56,     1] loss: 1003.824
[57,     1] loss: 954.337
[58,     1] loss: 939.399
[59,     1] loss: 982.145
[60,     1] loss: 946.987
[61,     1] loss: 972.678
[62,     1] loss: 885.210
[63,     1] loss: 877.599
[64,     1] loss: 882.580
[65,     1] loss: 865.497
[66,     1] loss: 873.634
Early stopping applied (best metric=0.760539710521698)
Finished Training
Total time taken: 8.841185808181763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1320.391
[2,     1] loss: 1323.110
[3,     1] loss: 1316.199
[4,     1] loss: 1314.471
[5,     1] loss: 1311.456
[6,     1] loss: 1310.320
[7,     1] loss: 1304.130
[8,     1] loss: 1291.000
[9,     1] loss: 1260.999
[10,     1] loss: 1240.800
[11,     1] loss: 1203.643
[12,     1] loss: 1198.215
[13,     1] loss: 1137.728
[14,     1] loss: 1154.492
[15,     1] loss: 1158.340
[16,     1] loss: 1088.956
[17,     1] loss: 1115.727
[18,     1] loss: 1097.879
[19,     1] loss: 1066.843
[20,     1] loss: 1074.802
[21,     1] loss: 1046.786
[22,     1] loss: 1036.289
[23,     1] loss: 1034.568
[24,     1] loss: 1029.937
[25,     1] loss: 1000.857
[26,     1] loss: 1012.171
[27,     1] loss: 985.353
[28,     1] loss: 968.424
[29,     1] loss: 1039.994
[30,     1] loss: 1102.748
[31,     1] loss: 1001.347
[32,     1] loss: 997.901
[33,     1] loss: 986.155
[34,     1] loss: 1034.308
[35,     1] loss: 924.385
[36,     1] loss: 980.690
[37,     1] loss: 932.722
[38,     1] loss: 956.157
[39,     1] loss: 947.612
[40,     1] loss: 900.825
[41,     1] loss: 938.128
[42,     1] loss: 916.941
[43,     1] loss: 906.554
[44,     1] loss: 873.090
[45,     1] loss: 900.201
[46,     1] loss: 889.387
[47,     1] loss: 810.927
[48,     1] loss: 852.058
[49,     1] loss: 841.383
[50,     1] loss: 1079.853
[51,     1] loss: 1477.193
[52,     1] loss: 1032.272
[53,     1] loss: 1026.362
[54,     1] loss: 1121.641
[55,     1] loss: 1143.480
[56,     1] loss: 1139.117
Early stopping applied (best metric=0.7221561670303345)
Finished Training
Total time taken: 7.590162038803101
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.837
[2,     1] loss: 1318.156
[3,     1] loss: 1310.655
[4,     1] loss: 1311.489
[5,     1] loss: 1308.221
[6,     1] loss: 1301.745
[7,     1] loss: 1295.636
[8,     1] loss: 1277.753
[9,     1] loss: 1240.361
[10,     1] loss: 1189.111
[11,     1] loss: 1179.156
[12,     1] loss: 1176.303
[13,     1] loss: 1093.335
[14,     1] loss: 1122.741
[15,     1] loss: 1039.112
[16,     1] loss: 1133.529
[17,     1] loss: 1048.507
[18,     1] loss: 1054.234
[19,     1] loss: 1022.333
[20,     1] loss: 1020.474
[21,     1] loss: 988.398
[22,     1] loss: 1038.853
[23,     1] loss: 969.888
[24,     1] loss: 953.376
[25,     1] loss: 956.074
[26,     1] loss: 952.098
[27,     1] loss: 939.724
[28,     1] loss: 936.742
[29,     1] loss: 982.599
[30,     1] loss: 1026.225
[31,     1] loss: 935.027
[32,     1] loss: 958.755
[33,     1] loss: 935.064
[34,     1] loss: 947.368
[35,     1] loss: 904.303
[36,     1] loss: 924.941
[37,     1] loss: 899.752
[38,     1] loss: 884.903
[39,     1] loss: 873.905
[40,     1] loss: 1005.867
[41,     1] loss: 858.517
[42,     1] loss: 951.840
[43,     1] loss: 880.677
[44,     1] loss: 902.784
[45,     1] loss: 857.382
[46,     1] loss: 903.080
[47,     1] loss: 816.698
[48,     1] loss: 858.219
[49,     1] loss: 838.598
[50,     1] loss: 800.070
[51,     1] loss: 824.051
[52,     1] loss: 847.362
[53,     1] loss: 886.640
[54,     1] loss: 828.533
[55,     1] loss: 843.515
[56,     1] loss: 789.834
[57,     1] loss: 751.675
[58,     1] loss: 851.464
[59,     1] loss: 750.752
[60,     1] loss: 726.129
[61,     1] loss: 694.237
[62,     1] loss: 755.764
[63,     1] loss: 1401.095
[64,     1] loss: 752.807
[65,     1] loss: 1084.955
[66,     1] loss: 980.676
[67,     1] loss: 997.514
[68,     1] loss: 1002.350
[69,     1] loss: 931.192
[70,     1] loss: 918.354
[71,     1] loss: 864.165
[72,     1] loss: 877.043
[73,     1] loss: 894.001
[74,     1] loss: 860.281
Early stopping applied (best metric=0.8552612066268921)
Finished Training
Total time taken: 12.124253749847412
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.300
[2,     1] loss: 1318.575
[3,     1] loss: 1316.589
[4,     1] loss: 1310.792
[5,     1] loss: 1313.150
[6,     1] loss: 1309.373
[7,     1] loss: 1311.065
[8,     1] loss: 1303.620
[9,     1] loss: 1302.205
[10,     1] loss: 1282.380
[11,     1] loss: 1264.060
[12,     1] loss: 1230.991
[13,     1] loss: 1199.454
[14,     1] loss: 1139.087
[15,     1] loss: 1156.917
[16,     1] loss: 1107.698
[17,     1] loss: 1050.069
[18,     1] loss: 1145.862
[19,     1] loss: 1096.601
[20,     1] loss: 1070.892
[21,     1] loss: 1044.699
[22,     1] loss: 1012.462
[23,     1] loss: 1077.875
[24,     1] loss: 1023.546
[25,     1] loss: 1079.778
[26,     1] loss: 1027.387
[27,     1] loss: 1023.917
[28,     1] loss: 972.636
[29,     1] loss: 1027.109
[30,     1] loss: 968.023
[31,     1] loss: 1019.875
[32,     1] loss: 951.336
[33,     1] loss: 941.899
[34,     1] loss: 918.340
[35,     1] loss: 925.465
[36,     1] loss: 907.892
[37,     1] loss: 913.006
[38,     1] loss: 898.490
[39,     1] loss: 873.400
[40,     1] loss: 831.721
[41,     1] loss: 1015.083
[42,     1] loss: 1654.999
[43,     1] loss: 976.756
[44,     1] loss: 1060.826
[45,     1] loss: 1146.594
[46,     1] loss: 1133.117
[47,     1] loss: 1109.810
[48,     1] loss: 1140.289
[49,     1] loss: 1112.327
[50,     1] loss: 1103.634
[51,     1] loss: 1075.864
[52,     1] loss: 1060.695
[53,     1] loss: 1038.435
[54,     1] loss: 1038.954
[55,     1] loss: 1029.890
[56,     1] loss: 993.115
[57,     1] loss: 983.315
[58,     1] loss: 979.895
[59,     1] loss: 953.037
Early stopping applied (best metric=0.8267553448677063)
Finished Training
Total time taken: 7.8771653175354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1313.166
[2,     1] loss: 1320.020
[3,     1] loss: 1311.938
[4,     1] loss: 1314.317
[5,     1] loss: 1311.870
[6,     1] loss: 1312.645
[7,     1] loss: 1311.569
[8,     1] loss: 1308.302
[9,     1] loss: 1301.664
[10,     1] loss: 1288.790
[11,     1] loss: 1257.550
[12,     1] loss: 1228.385
[13,     1] loss: 1226.136
[14,     1] loss: 1150.735
[15,     1] loss: 1121.825
[16,     1] loss: 1110.393
[17,     1] loss: 1106.864
[18,     1] loss: 1050.362
[19,     1] loss: 1050.923
[20,     1] loss: 1069.796
[21,     1] loss: 1044.106
[22,     1] loss: 1041.035
[23,     1] loss: 1076.671
[24,     1] loss: 1046.375
[25,     1] loss: 1007.305
[26,     1] loss: 1038.251
[27,     1] loss: 983.837
[28,     1] loss: 996.868
[29,     1] loss: 970.849
[30,     1] loss: 998.797
[31,     1] loss: 1049.619
[32,     1] loss: 972.078
[33,     1] loss: 943.234
[34,     1] loss: 987.857
[35,     1] loss: 944.157
[36,     1] loss: 897.106
[37,     1] loss: 942.945
[38,     1] loss: 887.311
[39,     1] loss: 880.513
[40,     1] loss: 894.993
[41,     1] loss: 911.598
[42,     1] loss: 916.510
[43,     1] loss: 925.259
[44,     1] loss: 878.004
[45,     1] loss: 920.141
[46,     1] loss: 842.712
[47,     1] loss: 886.065
[48,     1] loss: 766.748
[49,     1] loss: 876.305
[50,     1] loss: 968.718
[51,     1] loss: 934.876
[52,     1] loss: 868.520
[53,     1] loss: 895.020
[54,     1] loss: 927.041
[55,     1] loss: 816.510
[56,     1] loss: 896.338
[57,     1] loss: 766.546
[58,     1] loss: 825.522
Early stopping applied (best metric=0.8322347402572632)
Finished Training
Total time taken: 9.528200626373291
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1313.442
[2,     1] loss: 1312.724
[3,     1] loss: 1313.415
[4,     1] loss: 1315.752
[5,     1] loss: 1314.429
[6,     1] loss: 1312.808
[7,     1] loss: 1306.256
[8,     1] loss: 1303.315
[9,     1] loss: 1284.933
[10,     1] loss: 1259.821
[11,     1] loss: 1229.648
[12,     1] loss: 1207.370
[13,     1] loss: 1140.371
[14,     1] loss: 1142.989
[15,     1] loss: 1166.694
[16,     1] loss: 1110.473
[17,     1] loss: 1107.936
[18,     1] loss: 1117.779
[19,     1] loss: 1060.771
[20,     1] loss: 1086.337
[21,     1] loss: 1074.775
[22,     1] loss: 1064.553
[23,     1] loss: 1056.381
[24,     1] loss: 988.782
[25,     1] loss: 1047.156
[26,     1] loss: 993.785
[27,     1] loss: 1006.744
[28,     1] loss: 986.974
[29,     1] loss: 1022.275
[30,     1] loss: 966.137
[31,     1] loss: 1033.243
[32,     1] loss: 932.416
[33,     1] loss: 989.978
[34,     1] loss: 922.437
[35,     1] loss: 999.275
[36,     1] loss: 906.312
[37,     1] loss: 988.644
[38,     1] loss: 904.944
[39,     1] loss: 935.761
[40,     1] loss: 905.603
[41,     1] loss: 977.878
[42,     1] loss: 885.269
[43,     1] loss: 1010.204
[44,     1] loss: 892.164
[45,     1] loss: 1014.960
[46,     1] loss: 869.846
[47,     1] loss: 962.105
[48,     1] loss: 904.628
[49,     1] loss: 911.861
[50,     1] loss: 903.340
[51,     1] loss: 843.886
[52,     1] loss: 898.643
[53,     1] loss: 857.178
[54,     1] loss: 783.732
[55,     1] loss: 781.292
[56,     1] loss: 769.005
[57,     1] loss: 722.935
[58,     1] loss: 727.954
[59,     1] loss: 1300.263
[60,     1] loss: 1644.985
[61,     1] loss: 1133.302
[62,     1] loss: 1106.259
[63,     1] loss: 1154.708
[64,     1] loss: 1188.369
[65,     1] loss: 1200.784
[66,     1] loss: 1220.375
[67,     1] loss: 1204.357
[68,     1] loss: 1229.399
[69,     1] loss: 1222.474
[70,     1] loss: 1213.708
[71,     1] loss: 1194.430
[72,     1] loss: 1206.250
[73,     1] loss: 1193.045
Early stopping applied (best metric=0.8469085693359375)
Finished Training
Total time taken: 9.825209379196167
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1314.764
[2,     1] loss: 1316.676
[3,     1] loss: 1313.571
[4,     1] loss: 1319.449
[5,     1] loss: 1322.876
[6,     1] loss: 1317.603
[7,     1] loss: 1315.663
[8,     1] loss: 1314.086
[9,     1] loss: 1310.225
[10,     1] loss: 1307.641
[11,     1] loss: 1297.977
[12,     1] loss: 1283.338
[13,     1] loss: 1257.135
[14,     1] loss: 1232.284
[15,     1] loss: 1186.244
[16,     1] loss: 1147.766
[17,     1] loss: 1131.600
[18,     1] loss: 1122.895
[19,     1] loss: 1147.189
[20,     1] loss: 1155.943
[21,     1] loss: 1073.002
[22,     1] loss: 1101.528
[23,     1] loss: 1108.611
[24,     1] loss: 1097.074
[25,     1] loss: 1063.713
[26,     1] loss: 1078.496
[27,     1] loss: 1032.996
[28,     1] loss: 1025.576
[29,     1] loss: 1001.942
[30,     1] loss: 1011.955
[31,     1] loss: 957.205
[32,     1] loss: 1040.083
[33,     1] loss: 972.413
[34,     1] loss: 980.217
[35,     1] loss: 966.930
[36,     1] loss: 932.103
[37,     1] loss: 972.230
[38,     1] loss: 960.377
[39,     1] loss: 944.985
[40,     1] loss: 941.011
[41,     1] loss: 1016.580
[42,     1] loss: 1072.143
[43,     1] loss: 939.072
[44,     1] loss: 973.445
[45,     1] loss: 932.315
[46,     1] loss: 977.474
[47,     1] loss: 923.124
[48,     1] loss: 971.295
[49,     1] loss: 883.158
[50,     1] loss: 868.802
[51,     1] loss: 867.548
[52,     1] loss: 817.935
[53,     1] loss: 822.480
[54,     1] loss: 794.196
[55,     1] loss: 798.538
[56,     1] loss: 795.516
[57,     1] loss: 1579.740
[58,     1] loss: 1632.767
[59,     1] loss: 1240.326
[60,     1] loss: 1145.615
[61,     1] loss: 1252.646
[62,     1] loss: 1276.240
[63,     1] loss: 1284.795
[64,     1] loss: 1279.344
[65,     1] loss: 1284.077
[66,     1] loss: 1277.131
[67,     1] loss: 1297.158
[68,     1] loss: 1286.581
[69,     1] loss: 1267.793
[70,     1] loss: 1269.250
[71,     1] loss: 1266.912
[72,     1] loss: 1266.505
[73,     1] loss: 1268.109
[74,     1] loss: 1244.559
Early stopping applied (best metric=0.7213866114616394)
Finished Training
Total time taken: 10.284218549728394
{'Hydroxylation-K Validation Accuracy': 0.7520685579196218, 'Hydroxylation-K Validation Sensitivity': 0.6881481481481482, 'Hydroxylation-K Validation Specificity': 0.768421052631579, 'Hydroxylation-K Validation Precision': 0.4377191567748843, 'Hydroxylation-K AUC ROC': 0.8040350877192982, 'Hydroxylation-K AUC PR': 0.5875032273067033, 'Hydroxylation-K MCC': 0.39643361817280265, 'Hydroxylation-K F1': 0.5303057419784862, 'Validation Loss (Hydroxylation-K)': 0.4391184687614441, 'Hydroxylation-P Validation Accuracy': 0.7856329120349221, 'Hydroxylation-P Validation Sensitivity': 0.7957142857142857, 'Hydroxylation-P Validation Specificity': 0.7835004239612948, 'Hydroxylation-P Validation Precision': 0.4481183512319826, 'Hydroxylation-P AUC ROC': 0.8578617229058647, 'Hydroxylation-P AUC PR': 0.5913442042805621, 'Hydroxylation-P MCC': 0.47821508053310446, 'Hydroxylation-P F1': 0.5704660816030122, 'Validation Loss (Hydroxylation-P)': 0.3672089517116547, 'Validation Loss (total)': 0.8063274264335633, 'TimeToTrain': 9.787473328908284}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008951873279868032,
 'learning_rate_Hydroxylation-K': 0.006605358347279893,
 'learning_rate_Hydroxylation-P': 0.004879948084924945,
 'log_base': 2.0850062959511835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3951396014,
 'sample_weights': [1.9282956095682529, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.597767124972444,
 'weight_decay_Hydroxylation-K': 6.820769992763869,
 'weight_decay_Hydroxylation-P': 2.466168485296282}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.706
[2,     1] loss: 1397.846
[3,     1] loss: 1394.834
[4,     1] loss: 1389.479
[5,     1] loss: 1385.271
[6,     1] loss: 1386.171
[7,     1] loss: 1385.647
[8,     1] loss: 1383.573
[9,     1] loss: 1379.650
[10,     1] loss: 1378.617
[11,     1] loss: 1368.331
[12,     1] loss: 1353.577
[13,     1] loss: 1325.191
[14,     1] loss: 1284.493
[15,     1] loss: 1236.553
[16,     1] loss: 1205.506
[17,     1] loss: 1192.974
[18,     1] loss: 1162.836
[19,     1] loss: 1173.509
[20,     1] loss: 1168.800
[21,     1] loss: 1176.559
[22,     1] loss: 1088.477
[23,     1] loss: 1159.578
[24,     1] loss: 1114.301
[25,     1] loss: 1119.599
[26,     1] loss: 1097.405
[27,     1] loss: 1109.142
[28,     1] loss: 1129.089
[29,     1] loss: 1106.002
[30,     1] loss: 1043.842
[31,     1] loss: 1045.475
[32,     1] loss: 1009.526
[33,     1] loss: 1090.154
[34,     1] loss: 1042.621
[35,     1] loss: 1000.717
[36,     1] loss: 1056.114
[37,     1] loss: 1088.031
[38,     1] loss: 981.962
[39,     1] loss: 1030.010
[40,     1] loss: 975.976
[41,     1] loss: 1004.464
[42,     1] loss: 968.336
[43,     1] loss: 985.982
[44,     1] loss: 962.466
[45,     1] loss: 930.065
[46,     1] loss: 957.760
[47,     1] loss: 904.986
[48,     1] loss: 869.690
[49,     1] loss: 996.808
[50,     1] loss: 1032.095
[51,     1] loss: 840.271
[52,     1] loss: 979.993
[53,     1] loss: 835.300
[54,     1] loss: 953.933
[55,     1] loss: 833.737
[56,     1] loss: 845.334
[57,     1] loss: 853.858
[58,     1] loss: 779.218
[59,     1] loss: 996.709
[60,     1] loss: 911.537
[61,     1] loss: 772.768
[62,     1] loss: 839.791
[63,     1] loss: 723.662
[64,     1] loss: 727.085
[65,     1] loss: 753.742
[66,     1] loss: 814.452
[67,     1] loss: 908.802
[68,     1] loss: 679.785
[69,     1] loss: 735.870
[70,     1] loss: 852.002
[71,     1] loss: 613.251
[72,     1] loss: 836.363
[73,     1] loss: 839.266
[74,     1] loss: 585.596
[75,     1] loss: 883.038
[76,     1] loss: 864.461
[77,     1] loss: 721.313
[78,     1] loss: 853.961
[79,     1] loss: 702.675
[80,     1] loss: 713.512
[81,     1] loss: 668.981
[82,     1] loss: 644.339
Early stopping applied (best metric=0.73797607421875)
Finished Training
Total time taken: 14.13129997253418
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.850
[2,     1] loss: 1393.092
[3,     1] loss: 1390.663
[4,     1] loss: 1385.365
[5,     1] loss: 1389.249
[6,     1] loss: 1387.097
[7,     1] loss: 1382.773
[8,     1] loss: 1378.224
[9,     1] loss: 1375.589
[10,     1] loss: 1352.838
[11,     1] loss: 1337.038
[12,     1] loss: 1282.617
[13,     1] loss: 1275.050
[14,     1] loss: 1235.419
[15,     1] loss: 1222.722
[16,     1] loss: 1173.920
[17,     1] loss: 1197.409
[18,     1] loss: 1155.012
[19,     1] loss: 1124.598
[20,     1] loss: 1132.370
[21,     1] loss: 1093.699
[22,     1] loss: 1133.954
[23,     1] loss: 1073.291
[24,     1] loss: 1115.250
[25,     1] loss: 1115.914
[26,     1] loss: 1114.845
[27,     1] loss: 1076.018
[28,     1] loss: 1098.095
[29,     1] loss: 1110.462
[30,     1] loss: 1075.290
[31,     1] loss: 1050.846
[32,     1] loss: 1012.565
[33,     1] loss: 1066.495
[34,     1] loss: 989.862
[35,     1] loss: 957.841
[36,     1] loss: 1011.775
[37,     1] loss: 1092.839
[38,     1] loss: 987.704
[39,     1] loss: 996.110
[40,     1] loss: 961.086
[41,     1] loss: 971.818
[42,     1] loss: 1006.289
[43,     1] loss: 954.335
[44,     1] loss: 908.416
[45,     1] loss: 934.074
[46,     1] loss: 926.921
[47,     1] loss: 860.465
[48,     1] loss: 861.995
[49,     1] loss: 909.731
[50,     1] loss: 1041.354
[51,     1] loss: 1438.734
[52,     1] loss: 979.192
[53,     1] loss: 1056.091
[54,     1] loss: 1077.474
[55,     1] loss: 1042.281
[56,     1] loss: 1013.385
[57,     1] loss: 1077.725
[58,     1] loss: 978.853
[59,     1] loss: 962.050
[60,     1] loss: 1033.056
[61,     1] loss: 955.714
[62,     1] loss: 946.545
[63,     1] loss: 1039.124
[64,     1] loss: 959.832
[65,     1] loss: 982.947
[66,     1] loss: 939.805
[67,     1] loss: 956.691
Early stopping applied (best metric=0.7855578660964966)
Finished Training
Total time taken: 9.4312002658844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.659
[2,     1] loss: 1390.296
[3,     1] loss: 1388.224
[4,     1] loss: 1391.345
[5,     1] loss: 1388.697
[6,     1] loss: 1382.383
[7,     1] loss: 1385.457
[8,     1] loss: 1379.540
[9,     1] loss: 1378.183
[10,     1] loss: 1370.219
[11,     1] loss: 1355.025
[12,     1] loss: 1320.194
[13,     1] loss: 1285.099
[14,     1] loss: 1251.677
[15,     1] loss: 1204.119
[16,     1] loss: 1204.931
[17,     1] loss: 1115.269
[18,     1] loss: 1186.138
[19,     1] loss: 1113.603
[20,     1] loss: 1187.960
[21,     1] loss: 1137.752
[22,     1] loss: 1112.187
[23,     1] loss: 1076.221
[24,     1] loss: 1128.719
[25,     1] loss: 1042.781
[26,     1] loss: 1105.201
[27,     1] loss: 1019.655
[28,     1] loss: 1016.481
[29,     1] loss: 1025.180
[30,     1] loss: 996.388
[31,     1] loss: 954.315
[32,     1] loss: 926.292
[33,     1] loss: 1023.991
[34,     1] loss: 924.202
[35,     1] loss: 937.406
[36,     1] loss: 895.609
[37,     1] loss: 1066.583
[38,     1] loss: 1548.054
[39,     1] loss: 964.633
[40,     1] loss: 1090.537
[41,     1] loss: 1074.430
[42,     1] loss: 1028.464
[43,     1] loss: 1084.315
[44,     1] loss: 1048.776
[45,     1] loss: 1045.208
[46,     1] loss: 1020.247
[47,     1] loss: 999.869
[48,     1] loss: 995.486
[49,     1] loss: 974.362
[50,     1] loss: 996.576
[51,     1] loss: 901.563
[52,     1] loss: 956.289
[53,     1] loss: 901.403
[54,     1] loss: 939.882
[55,     1] loss: 834.140
[56,     1] loss: 882.403
[57,     1] loss: 880.297
[58,     1] loss: 878.132
[59,     1] loss: 927.964
[60,     1] loss: 865.580
[61,     1] loss: 774.716
[62,     1] loss: 937.156
[63,     1] loss: 833.815
[64,     1] loss: 806.612
[65,     1] loss: 875.973
[66,     1] loss: 755.534
[67,     1] loss: 747.467
[68,     1] loss: 835.480
[69,     1] loss: 823.124
[70,     1] loss: 696.850
[71,     1] loss: 685.250
[72,     1] loss: 637.487
[73,     1] loss: 726.681
[74,     1] loss: 1094.162
[75,     1] loss: 1953.136
[76,     1] loss: 883.417
[77,     1] loss: 1186.053
[78,     1] loss: 1115.656
[79,     1] loss: 1090.474
Early stopping applied (best metric=0.9080507755279541)
Finished Training
Total time taken: 12.243257999420166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1386.196
[2,     1] loss: 1396.309
[3,     1] loss: 1393.634
[4,     1] loss: 1383.342
[5,     1] loss: 1386.841
[6,     1] loss: 1385.053
[7,     1] loss: 1384.858
[8,     1] loss: 1384.755
[9,     1] loss: 1380.497
[10,     1] loss: 1380.392
[11,     1] loss: 1372.325
[12,     1] loss: 1364.302
[13,     1] loss: 1339.793
[14,     1] loss: 1323.992
[15,     1] loss: 1276.935
[16,     1] loss: 1243.105
[17,     1] loss: 1204.205
[18,     1] loss: 1213.208
[19,     1] loss: 1215.270
[20,     1] loss: 1138.098
[21,     1] loss: 1154.121
[22,     1] loss: 1140.440
[23,     1] loss: 1105.157
[24,     1] loss: 1050.663
[25,     1] loss: 1111.437
[26,     1] loss: 1130.401
[27,     1] loss: 1038.201
[28,     1] loss: 1076.286
[29,     1] loss: 1079.502
[30,     1] loss: 1056.144
[31,     1] loss: 1018.520
[32,     1] loss: 1004.228
[33,     1] loss: 1050.905
[34,     1] loss: 939.799
[35,     1] loss: 1013.663
[36,     1] loss: 1027.857
[37,     1] loss: 1032.961
[38,     1] loss: 1061.789
[39,     1] loss: 962.915
[40,     1] loss: 998.391
[41,     1] loss: 959.211
[42,     1] loss: 960.559
[43,     1] loss: 963.736
[44,     1] loss: 987.075
[45,     1] loss: 1033.128
[46,     1] loss: 949.782
[47,     1] loss: 1044.485
[48,     1] loss: 953.811
[49,     1] loss: 950.180
[50,     1] loss: 915.468
[51,     1] loss: 912.223
[52,     1] loss: 895.967
[53,     1] loss: 931.031
[54,     1] loss: 844.854
[55,     1] loss: 1024.381
[56,     1] loss: 1048.309
[57,     1] loss: 937.313
[58,     1] loss: 933.999
[59,     1] loss: 879.713
[60,     1] loss: 920.606
[61,     1] loss: 842.896
[62,     1] loss: 944.272
[63,     1] loss: 856.461
[64,     1] loss: 880.985
[65,     1] loss: 744.978
[66,     1] loss: 806.492
[67,     1] loss: 1005.032
[68,     1] loss: 1258.872
[69,     1] loss: 852.500
[70,     1] loss: 993.848
[71,     1] loss: 839.313
[72,     1] loss: 922.443
[73,     1] loss: 901.228
[74,     1] loss: 857.002
[75,     1] loss: 868.440
[76,     1] loss: 787.148
[77,     1] loss: 850.605
[78,     1] loss: 785.474
[79,     1] loss: 751.051
[80,     1] loss: 870.296
[81,     1] loss: 1116.952
[82,     1] loss: 1010.985
[83,     1] loss: 831.783
[84,     1] loss: 888.307
[85,     1] loss: 879.442
[86,     1] loss: 806.007
[87,     1] loss: 893.356
[88,     1] loss: 792.486
[89,     1] loss: 867.466
[90,     1] loss: 769.974
[91,     1] loss: 729.754
[92,     1] loss: 955.446
[93,     1] loss: 706.887
[94,     1] loss: 720.398
[95,     1] loss: 649.029
[96,     1] loss: 714.193
[97,     1] loss: 1004.788
[98,     1] loss: 1188.506
[99,     1] loss: 763.264
[100,     1] loss: 886.055
[101,     1] loss: 756.934
[102,     1] loss: 795.012
[103,     1] loss: 754.188
[104,     1] loss: 772.168
Early stopping applied (best metric=0.8794510364532471)
Finished Training
Total time taken: 17.30736517906189
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1392.845
[2,     1] loss: 1391.598
[3,     1] loss: 1387.683
[4,     1] loss: 1391.438
[5,     1] loss: 1393.834
[6,     1] loss: 1387.345
[7,     1] loss: 1389.975
[8,     1] loss: 1386.434
[9,     1] loss: 1383.330
[10,     1] loss: 1387.035
[11,     1] loss: 1379.727
[12,     1] loss: 1377.608
[13,     1] loss: 1359.463
[14,     1] loss: 1361.812
[15,     1] loss: 1299.752
[16,     1] loss: 1256.775
[17,     1] loss: 1218.794
[18,     1] loss: 1208.138
[19,     1] loss: 1189.630
[20,     1] loss: 1227.504
[21,     1] loss: 1146.166
[22,     1] loss: 1108.510
[23,     1] loss: 1135.652
[24,     1] loss: 1115.296
[25,     1] loss: 1084.854
[26,     1] loss: 1084.766
[27,     1] loss: 1159.339
[28,     1] loss: 1053.033
[29,     1] loss: 1075.138
[30,     1] loss: 1026.013
[31,     1] loss: 1062.131
[32,     1] loss: 1034.966
[33,     1] loss: 1035.386
[34,     1] loss: 1088.112
[35,     1] loss: 1049.707
[36,     1] loss: 969.526
[37,     1] loss: 1076.634
[38,     1] loss: 974.133
[39,     1] loss: 1049.481
[40,     1] loss: 974.326
[41,     1] loss: 992.469
[42,     1] loss: 942.547
[43,     1] loss: 965.076
[44,     1] loss: 916.732
[45,     1] loss: 1036.359
[46,     1] loss: 928.582
[47,     1] loss: 876.237
[48,     1] loss: 988.743
[49,     1] loss: 898.734
[50,     1] loss: 823.798
[51,     1] loss: 823.813
[52,     1] loss: 797.933
[53,     1] loss: 842.905
[54,     1] loss: 1498.004
[55,     1] loss: 919.234
[56,     1] loss: 849.489
[57,     1] loss: 872.455
[58,     1] loss: 896.134
[59,     1] loss: 831.827
[60,     1] loss: 932.434
[61,     1] loss: 888.322
[62,     1] loss: 857.909
[63,     1] loss: 826.246
[64,     1] loss: 852.145
[65,     1] loss: 837.485
[66,     1] loss: 783.455
[67,     1] loss: 868.899
[68,     1] loss: 784.135
[69,     1] loss: 694.503
[70,     1] loss: 846.192
[71,     1] loss: 811.740
[72,     1] loss: 694.342
[73,     1] loss: 903.651
[74,     1] loss: 949.792
[75,     1] loss: 662.374
[76,     1] loss: 716.853
[77,     1] loss: 662.381
[78,     1] loss: 753.381
[79,     1] loss: 639.723
[80,     1] loss: 683.424
[81,     1] loss: 635.528
[82,     1] loss: 546.570
Early stopping applied (best metric=0.7486172914505005)
Finished Training
Total time taken: 11.18723750114441
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.316
[2,     1] loss: 1395.602
[3,     1] loss: 1396.019
[4,     1] loss: 1390.603
[5,     1] loss: 1383.726
[6,     1] loss: 1394.018
[7,     1] loss: 1386.106
[8,     1] loss: 1385.881
[9,     1] loss: 1383.735
[10,     1] loss: 1381.298
[11,     1] loss: 1382.474
[12,     1] loss: 1383.580
[13,     1] loss: 1375.692
[14,     1] loss: 1366.438
[15,     1] loss: 1362.023
[16,     1] loss: 1342.263
[17,     1] loss: 1298.455
[18,     1] loss: 1269.422
[19,     1] loss: 1228.581
[20,     1] loss: 1181.928
[21,     1] loss: 1208.955
[22,     1] loss: 1258.094
[23,     1] loss: 1160.764
[24,     1] loss: 1146.016
[25,     1] loss: 1179.867
[26,     1] loss: 1154.132
[27,     1] loss: 1167.050
[28,     1] loss: 1140.050
[29,     1] loss: 1109.268
[30,     1] loss: 1113.581
[31,     1] loss: 1071.515
[32,     1] loss: 1055.473
[33,     1] loss: 1079.034
[34,     1] loss: 1053.029
[35,     1] loss: 1062.495
[36,     1] loss: 1023.447
[37,     1] loss: 1019.576
[38,     1] loss: 1037.281
[39,     1] loss: 961.623
[40,     1] loss: 1066.508
[41,     1] loss: 991.923
[42,     1] loss: 968.191
[43,     1] loss: 962.496
[44,     1] loss: 905.220
[45,     1] loss: 899.097
[46,     1] loss: 1040.674
[47,     1] loss: 1262.979
[48,     1] loss: 922.781
[49,     1] loss: 976.742
[50,     1] loss: 929.092
[51,     1] loss: 996.742
[52,     1] loss: 870.651
[53,     1] loss: 949.818
[54,     1] loss: 897.297
[55,     1] loss: 843.381
[56,     1] loss: 799.722
[57,     1] loss: 885.026
[58,     1] loss: 920.589
[59,     1] loss: 847.334
[60,     1] loss: 809.229
Early stopping applied (best metric=0.7493647336959839)
Finished Training
Total time taken: 10.512222051620483
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.867
[2,     1] loss: 1390.174
[3,     1] loss: 1388.142
[4,     1] loss: 1389.872
[5,     1] loss: 1389.352
[6,     1] loss: 1389.485
[7,     1] loss: 1387.758
[8,     1] loss: 1389.598
[9,     1] loss: 1382.415
[10,     1] loss: 1384.554
[11,     1] loss: 1385.345
[12,     1] loss: 1385.097
[13,     1] loss: 1386.266
[14,     1] loss: 1383.636
[15,     1] loss: 1381.229
[16,     1] loss: 1374.548
[17,     1] loss: 1365.574
[18,     1] loss: 1361.229
[19,     1] loss: 1333.778
[20,     1] loss: 1265.723
[21,     1] loss: 1215.844
[22,     1] loss: 1186.004
[23,     1] loss: 1171.952
[24,     1] loss: 1182.765
[25,     1] loss: 1219.404
[26,     1] loss: 1135.021
[27,     1] loss: 1134.944
[28,     1] loss: 1135.273
[29,     1] loss: 1134.477
[30,     1] loss: 1135.456
[31,     1] loss: 1115.331
[32,     1] loss: 1130.530
[33,     1] loss: 1105.734
[34,     1] loss: 1044.164
[35,     1] loss: 1044.242
[36,     1] loss: 1004.561
[37,     1] loss: 1053.047
[38,     1] loss: 1032.034
[39,     1] loss: 1039.398
[40,     1] loss: 1030.884
[41,     1] loss: 1059.625
[42,     1] loss: 999.399
[43,     1] loss: 1093.672
[44,     1] loss: 1004.686
[45,     1] loss: 979.247
[46,     1] loss: 984.218
[47,     1] loss: 935.700
[48,     1] loss: 957.614
[49,     1] loss: 951.439
[50,     1] loss: 872.801
[51,     1] loss: 891.432
[52,     1] loss: 920.335
[53,     1] loss: 897.385
[54,     1] loss: 902.542
[55,     1] loss: 949.991
[56,     1] loss: 1095.478
[57,     1] loss: 989.607
[58,     1] loss: 975.715
[59,     1] loss: 960.813
[60,     1] loss: 976.418
[61,     1] loss: 895.848
Early stopping applied (best metric=0.8531780242919922)
Finished Training
Total time taken: 8.504180431365967
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.404
[2,     1] loss: 1404.628
[3,     1] loss: 1392.025
[4,     1] loss: 1386.509
[5,     1] loss: 1383.558
[6,     1] loss: 1385.286
[7,     1] loss: 1387.417
[8,     1] loss: 1386.151
[9,     1] loss: 1385.869
[10,     1] loss: 1388.349
[11,     1] loss: 1386.406
[12,     1] loss: 1385.600
[13,     1] loss: 1383.405
[14,     1] loss: 1382.555
[15,     1] loss: 1380.972
[16,     1] loss: 1378.237
[17,     1] loss: 1366.688
[18,     1] loss: 1362.794
[19,     1] loss: 1343.137
[20,     1] loss: 1291.377
[21,     1] loss: 1252.837
[22,     1] loss: 1235.401
[23,     1] loss: 1209.030
[24,     1] loss: 1176.834
[25,     1] loss: 1119.280
[26,     1] loss: 1146.295
[27,     1] loss: 1130.347
[28,     1] loss: 1079.168
[29,     1] loss: 1091.470
[30,     1] loss: 1114.911
[31,     1] loss: 1085.312
[32,     1] loss: 1100.905
[33,     1] loss: 1084.373
[34,     1] loss: 1062.341
[35,     1] loss: 1046.945
[36,     1] loss: 1080.203
[37,     1] loss: 1023.950
[38,     1] loss: 1031.968
[39,     1] loss: 1012.244
[40,     1] loss: 972.739
[41,     1] loss: 1027.752
[42,     1] loss: 941.590
[43,     1] loss: 890.162
[44,     1] loss: 989.066
[45,     1] loss: 873.300
[46,     1] loss: 1118.614
[47,     1] loss: 1101.532
[48,     1] loss: 893.045
[49,     1] loss: 1013.036
[50,     1] loss: 881.489
[51,     1] loss: 909.519
[52,     1] loss: 905.278
[53,     1] loss: 912.434
[54,     1] loss: 916.246
[55,     1] loss: 871.104
[56,     1] loss: 834.474
[57,     1] loss: 835.462
[58,     1] loss: 744.667
[59,     1] loss: 825.497
[60,     1] loss: 850.386
[61,     1] loss: 749.727
[62,     1] loss: 745.294
[63,     1] loss: 747.447
[64,     1] loss: 733.240
[65,     1] loss: 874.882
[66,     1] loss: 998.594
[67,     1] loss: 1080.158
Early stopping applied (best metric=0.8797845840454102)
Finished Training
Total time taken: 9.47119927406311
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.315
[2,     1] loss: 1391.004
[3,     1] loss: 1386.994
[4,     1] loss: 1382.886
[5,     1] loss: 1383.990
[6,     1] loss: 1387.771
[7,     1] loss: 1382.563
[8,     1] loss: 1378.755
[9,     1] loss: 1374.436
[10,     1] loss: 1365.665
[11,     1] loss: 1361.637
[12,     1] loss: 1321.804
[13,     1] loss: 1303.198
[14,     1] loss: 1241.514
[15,     1] loss: 1189.835
[16,     1] loss: 1179.826
[17,     1] loss: 1134.673
[18,     1] loss: 1073.870
[19,     1] loss: 1124.900
[20,     1] loss: 1085.250
[21,     1] loss: 1097.333
[22,     1] loss: 1074.434
[23,     1] loss: 1074.554
[24,     1] loss: 1217.626
[25,     1] loss: 1091.478
[26,     1] loss: 1090.866
[27,     1] loss: 1111.257
[28,     1] loss: 1122.093
[29,     1] loss: 1064.584
[30,     1] loss: 1111.634
[31,     1] loss: 1025.444
[32,     1] loss: 1005.205
[33,     1] loss: 979.242
[34,     1] loss: 1041.344
[35,     1] loss: 931.410
[36,     1] loss: 977.224
[37,     1] loss: 938.238
[38,     1] loss: 926.699
[39,     1] loss: 921.421
[40,     1] loss: 972.264
[41,     1] loss: 870.024
[42,     1] loss: 962.009
[43,     1] loss: 1267.108
[44,     1] loss: 1007.554
[45,     1] loss: 977.465
[46,     1] loss: 956.451
[47,     1] loss: 1021.330
[48,     1] loss: 955.669
[49,     1] loss: 956.289
[50,     1] loss: 944.014
[51,     1] loss: 876.209
[52,     1] loss: 901.565
[53,     1] loss: 832.862
[54,     1] loss: 793.272
[55,     1] loss: 902.033
[56,     1] loss: 857.406
[57,     1] loss: 784.586
[58,     1] loss: 825.482
[59,     1] loss: 810.056
[60,     1] loss: 732.477
[61,     1] loss: 730.757
[62,     1] loss: 758.321
[63,     1] loss: 855.329
[64,     1] loss: 908.008
[65,     1] loss: 792.023
[66,     1] loss: 728.676
[67,     1] loss: 803.759
[68,     1] loss: 693.909
[69,     1] loss: 691.849
[70,     1] loss: 651.625
[71,     1] loss: 616.092
Early stopping applied (best metric=0.7974571585655212)
Finished Training
Total time taken: 11.922253131866455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1397.339
[2,     1] loss: 1392.466
[3,     1] loss: 1395.629
[4,     1] loss: 1385.510
[5,     1] loss: 1389.431
[6,     1] loss: 1387.598
[7,     1] loss: 1389.444
[8,     1] loss: 1390.898
[9,     1] loss: 1389.289
[10,     1] loss: 1387.080
[11,     1] loss: 1388.347
[12,     1] loss: 1388.754
[13,     1] loss: 1388.999
[14,     1] loss: 1391.407
[15,     1] loss: 1386.591
[16,     1] loss: 1389.313
[17,     1] loss: 1389.336
[18,     1] loss: 1387.706
[19,     1] loss: 1385.044
[20,     1] loss: 1386.444
[21,     1] loss: 1382.588
[22,     1] loss: 1381.588
[23,     1] loss: 1375.194
[24,     1] loss: 1361.063
[25,     1] loss: 1333.411
[26,     1] loss: 1304.510
[27,     1] loss: 1296.136
[28,     1] loss: 1242.986
[29,     1] loss: 1248.077
[30,     1] loss: 1161.302
[31,     1] loss: 1146.913
[32,     1] loss: 1129.443
[33,     1] loss: 1140.683
[34,     1] loss: 1170.253
[35,     1] loss: 1091.759
[36,     1] loss: 1136.748
[37,     1] loss: 1086.825
[38,     1] loss: 1093.231
[39,     1] loss: 1092.197
[40,     1] loss: 1094.390
[41,     1] loss: 1079.456
[42,     1] loss: 1065.029
[43,     1] loss: 1066.122
[44,     1] loss: 1083.762
[45,     1] loss: 1043.290
[46,     1] loss: 997.613
[47,     1] loss: 957.689
[48,     1] loss: 1090.194
[49,     1] loss: 1080.065
[50,     1] loss: 990.355
[51,     1] loss: 1015.666
[52,     1] loss: 926.762
[53,     1] loss: 994.018
[54,     1] loss: 885.528
[55,     1] loss: 970.280
[56,     1] loss: 884.203
[57,     1] loss: 885.526
[58,     1] loss: 971.559
[59,     1] loss: 808.124
[60,     1] loss: 838.581
[61,     1] loss: 873.628
[62,     1] loss: 994.460
[63,     1] loss: 854.597
[64,     1] loss: 830.912
[65,     1] loss: 842.301
[66,     1] loss: 832.718
[67,     1] loss: 917.831
[68,     1] loss: 833.063
[69,     1] loss: 781.456
[70,     1] loss: 963.573
[71,     1] loss: 1308.328
[72,     1] loss: 733.451
[73,     1] loss: 1048.711
[74,     1] loss: 917.855
[75,     1] loss: 908.207
Early stopping applied (best metric=0.7931056618690491)
Finished Training
Total time taken: 10.569222927093506
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.702
[2,     1] loss: 1423.368
[3,     1] loss: 1386.955
[4,     1] loss: 1392.218
[5,     1] loss: 1394.327
[6,     1] loss: 1388.693
[7,     1] loss: 1391.053
[8,     1] loss: 1390.437
[9,     1] loss: 1387.145
[10,     1] loss: 1386.007
[11,     1] loss: 1387.445
[12,     1] loss: 1386.514
[13,     1] loss: 1386.826
[14,     1] loss: 1386.724
[15,     1] loss: 1387.797
[16,     1] loss: 1386.550
[17,     1] loss: 1387.375
[18,     1] loss: 1386.256
[19,     1] loss: 1386.613
[20,     1] loss: 1385.549
[21,     1] loss: 1383.193
[22,     1] loss: 1386.380
[23,     1] loss: 1383.348
[24,     1] loss: 1380.662
[25,     1] loss: 1376.095
[26,     1] loss: 1373.570
[27,     1] loss: 1366.196
[28,     1] loss: 1360.651
[29,     1] loss: 1339.441
[30,     1] loss: 1318.666
[31,     1] loss: 1299.375
[32,     1] loss: 1264.630
[33,     1] loss: 1306.459
[34,     1] loss: 1258.485
[35,     1] loss: 1245.324
[36,     1] loss: 1365.376
[37,     1] loss: 1237.978
[38,     1] loss: 1310.732
[39,     1] loss: 1228.276
[40,     1] loss: 1210.717
[41,     1] loss: 1277.127
[42,     1] loss: 1256.735
[43,     1] loss: 1220.456
[44,     1] loss: 1223.553
[45,     1] loss: 1236.309
[46,     1] loss: 1206.776
[47,     1] loss: 1188.782
[48,     1] loss: 1159.878
[49,     1] loss: 1159.890
[50,     1] loss: 1139.967
[51,     1] loss: 1200.163
[52,     1] loss: 1028.468
[53,     1] loss: 1081.649
[54,     1] loss: 1059.812
[55,     1] loss: 1105.730
[56,     1] loss: 1065.518
[57,     1] loss: 1029.713
[58,     1] loss: 1014.179
[59,     1] loss: 1044.441
[60,     1] loss: 1074.649
[61,     1] loss: 959.067
[62,     1] loss: 1029.292
[63,     1] loss: 1195.161
[64,     1] loss: 1229.581
[65,     1] loss: 1122.148
[66,     1] loss: 1090.513
[67,     1] loss: 1144.574
[68,     1] loss: 1088.205
[69,     1] loss: 1081.780
[70,     1] loss: 1067.108
[71,     1] loss: 1114.505
[72,     1] loss: 1062.049
[73,     1] loss: 1048.690
[74,     1] loss: 969.403
[75,     1] loss: 1053.466
[76,     1] loss: 960.857
[77,     1] loss: 1020.932
[78,     1] loss: 991.098
[79,     1] loss: 1003.388
[80,     1] loss: 893.077
[81,     1] loss: 969.054
[82,     1] loss: 950.569
[83,     1] loss: 910.109
[84,     1] loss: 920.470
Early stopping applied (best metric=0.8592215776443481)
Finished Training
Total time taken: 14.209301471710205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.566
[2,     1] loss: 1394.014
[3,     1] loss: 1398.093
[4,     1] loss: 1384.298
[5,     1] loss: 1387.241
[6,     1] loss: 1387.603
[7,     1] loss: 1384.931
[8,     1] loss: 1389.086
[9,     1] loss: 1385.529
[10,     1] loss: 1381.110
[11,     1] loss: 1378.186
[12,     1] loss: 1377.267
[13,     1] loss: 1368.328
[14,     1] loss: 1355.108
[15,     1] loss: 1332.274
[16,     1] loss: 1326.780
[17,     1] loss: 1303.991
[18,     1] loss: 1255.307
[19,     1] loss: 1267.816
[20,     1] loss: 1212.473
[21,     1] loss: 1203.219
[22,     1] loss: 1192.324
[23,     1] loss: 1113.906
[24,     1] loss: 1130.686
[25,     1] loss: 1144.445
[26,     1] loss: 1092.096
[27,     1] loss: 1073.849
[28,     1] loss: 1163.107
[29,     1] loss: 1100.996
[30,     1] loss: 1051.242
[31,     1] loss: 1113.176
[32,     1] loss: 1038.619
[33,     1] loss: 1084.621
[34,     1] loss: 1020.760
[35,     1] loss: 1091.170
[36,     1] loss: 956.101
[37,     1] loss: 935.893
[38,     1] loss: 962.115
[39,     1] loss: 959.218
[40,     1] loss: 1078.932
[41,     1] loss: 893.979
[42,     1] loss: 933.147
[43,     1] loss: 914.535
[44,     1] loss: 892.083
[45,     1] loss: 991.062
[46,     1] loss: 1083.833
[47,     1] loss: 1029.789
[48,     1] loss: 950.572
[49,     1] loss: 962.529
[50,     1] loss: 942.368
[51,     1] loss: 941.385
[52,     1] loss: 881.552
[53,     1] loss: 934.104
[54,     1] loss: 793.582
[55,     1] loss: 937.414
Early stopping applied (best metric=0.8604744672775269)
Finished Training
Total time taken: 7.902165412902832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.392
[2,     1] loss: 1392.293
[3,     1] loss: 1384.858
[4,     1] loss: 1393.521
[5,     1] loss: 1380.229
[6,     1] loss: 1388.504
[7,     1] loss: 1387.106
[8,     1] loss: 1385.356
[9,     1] loss: 1383.203
[10,     1] loss: 1385.272
[11,     1] loss: 1381.097
[12,     1] loss: 1381.384
[13,     1] loss: 1378.201
[14,     1] loss: 1388.058
[15,     1] loss: 1371.315
[16,     1] loss: 1370.472
[17,     1] loss: 1356.020
[18,     1] loss: 1341.601
[19,     1] loss: 1290.094
[20,     1] loss: 1269.748
[21,     1] loss: 1226.606
[22,     1] loss: 1193.009
[23,     1] loss: 1222.557
[24,     1] loss: 1220.066
[25,     1] loss: 1180.823
[26,     1] loss: 1155.352
[27,     1] loss: 1127.249
[28,     1] loss: 1121.017
[29,     1] loss: 1144.821
[30,     1] loss: 1090.202
[31,     1] loss: 1094.649
[32,     1] loss: 1125.237
[33,     1] loss: 1050.851
[34,     1] loss: 1057.758
[35,     1] loss: 1047.255
[36,     1] loss: 1088.190
[37,     1] loss: 1108.897
[38,     1] loss: 1013.253
[39,     1] loss: 1033.834
[40,     1] loss: 1006.681
[41,     1] loss: 1061.703
[42,     1] loss: 985.211
[43,     1] loss: 964.427
[44,     1] loss: 959.372
[45,     1] loss: 985.544
[46,     1] loss: 965.720
[47,     1] loss: 911.813
[48,     1] loss: 957.701
[49,     1] loss: 993.591
[50,     1] loss: 967.720
[51,     1] loss: 956.892
[52,     1] loss: 837.918
[53,     1] loss: 870.676
[54,     1] loss: 893.790
[55,     1] loss: 825.973
[56,     1] loss: 813.673
[57,     1] loss: 842.041
Early stopping applied (best metric=0.7640469074249268)
Finished Training
Total time taken: 8.682184219360352
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.091
[2,     1] loss: 1387.492
[3,     1] loss: 1387.494
[4,     1] loss: 1385.220
[5,     1] loss: 1383.095
[6,     1] loss: 1387.647
[7,     1] loss: 1390.427
[8,     1] loss: 1381.911
[9,     1] loss: 1384.246
[10,     1] loss: 1391.747
[11,     1] loss: 1384.619
[12,     1] loss: 1379.532
[13,     1] loss: 1379.222
[14,     1] loss: 1372.670
[15,     1] loss: 1366.456
[16,     1] loss: 1353.379
[17,     1] loss: 1328.832
[18,     1] loss: 1295.230
[19,     1] loss: 1247.068
[20,     1] loss: 1211.500
[21,     1] loss: 1161.564
[22,     1] loss: 1164.862
[23,     1] loss: 1226.320
[24,     1] loss: 1173.053
[25,     1] loss: 1153.816
[26,     1] loss: 1144.148
[27,     1] loss: 1130.386
[28,     1] loss: 1104.464
[29,     1] loss: 1108.159
[30,     1] loss: 1114.648
[31,     1] loss: 1048.958
[32,     1] loss: 1043.814
[33,     1] loss: 1043.495
[34,     1] loss: 1127.405
[35,     1] loss: 1034.235
[36,     1] loss: 1041.914
[37,     1] loss: 1044.646
[38,     1] loss: 1039.726
[39,     1] loss: 1016.216
[40,     1] loss: 1053.160
[41,     1] loss: 1059.188
[42,     1] loss: 940.056
[43,     1] loss: 919.476
[44,     1] loss: 949.549
[45,     1] loss: 1051.755
[46,     1] loss: 915.874
[47,     1] loss: 845.286
[48,     1] loss: 1019.421
[49,     1] loss: 948.268
[50,     1] loss: 876.628
[51,     1] loss: 1016.550
[52,     1] loss: 890.099
[53,     1] loss: 1042.243
[54,     1] loss: 857.458
[55,     1] loss: 934.457
[56,     1] loss: 819.903
[57,     1] loss: 922.627
[58,     1] loss: 913.141
[59,     1] loss: 772.423
[60,     1] loss: 935.468
[61,     1] loss: 1040.508
[62,     1] loss: 803.434
[63,     1] loss: 960.347
[64,     1] loss: 848.453
[65,     1] loss: 890.234
[66,     1] loss: 821.089
[67,     1] loss: 973.801
[68,     1] loss: 753.692
[69,     1] loss: 759.149
[70,     1] loss: 724.322
[71,     1] loss: 697.095
[72,     1] loss: 708.358
[73,     1] loss: 1045.936
[74,     1] loss: 1360.585
[75,     1] loss: 799.817
[76,     1] loss: 1097.065
[77,     1] loss: 1025.364
Early stopping applied (best metric=0.7469819784164429)
Finished Training
Total time taken: 10.913233041763306
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1395.708
[2,     1] loss: 1402.606
[3,     1] loss: 1388.052
[4,     1] loss: 1393.857
[5,     1] loss: 1385.705
[6,     1] loss: 1391.590
[7,     1] loss: 1391.695
[8,     1] loss: 1388.192
[9,     1] loss: 1389.624
[10,     1] loss: 1389.522
[11,     1] loss: 1389.895
[12,     1] loss: 1386.500
[13,     1] loss: 1388.168
[14,     1] loss: 1395.799
[15,     1] loss: 1387.386
[16,     1] loss: 1386.326
[17,     1] loss: 1388.923
[18,     1] loss: 1387.502
[19,     1] loss: 1388.776
[20,     1] loss: 1390.032
[21,     1] loss: 1388.657
[22,     1] loss: 1386.683
[23,     1] loss: 1386.441
[24,     1] loss: 1385.713
[25,     1] loss: 1384.423
[26,     1] loss: 1383.873
[27,     1] loss: 1379.404
[28,     1] loss: 1371.171
[29,     1] loss: 1353.802
[30,     1] loss: 1331.261
[31,     1] loss: 1302.303
[32,     1] loss: 1258.105
[33,     1] loss: 1233.785
[34,     1] loss: 1144.125
[35,     1] loss: 1091.857
[36,     1] loss: 1324.034
[37,     1] loss: 1180.174
[38,     1] loss: 1085.088
[39,     1] loss: 1180.839
[40,     1] loss: 1108.413
[41,     1] loss: 1123.102
[42,     1] loss: 1086.635
[43,     1] loss: 1126.217
[44,     1] loss: 1111.906
[45,     1] loss: 1092.536
[46,     1] loss: 1075.988
[47,     1] loss: 1088.009
[48,     1] loss: 1037.426
[49,     1] loss: 1058.506
[50,     1] loss: 1036.213
[51,     1] loss: 1025.187
[52,     1] loss: 973.086
[53,     1] loss: 982.860
[54,     1] loss: 909.327
[55,     1] loss: 1062.754
[56,     1] loss: 981.851
[57,     1] loss: 905.870
[58,     1] loss: 942.006
[59,     1] loss: 904.674
[60,     1] loss: 886.892
[61,     1] loss: 1007.087
[62,     1] loss: 975.770
[63,     1] loss: 905.470
[64,     1] loss: 963.478
[65,     1] loss: 855.583
[66,     1] loss: 894.805
[67,     1] loss: 818.653
[68,     1] loss: 881.017
[69,     1] loss: 804.049
[70,     1] loss: 769.483
[71,     1] loss: 872.326
[72,     1] loss: 1270.935
[73,     1] loss: 1323.792
[74,     1] loss: 920.744
[75,     1] loss: 1039.610
[76,     1] loss: 1118.660
[77,     1] loss: 1012.878
[78,     1] loss: 1018.754
[79,     1] loss: 1017.588
[80,     1] loss: 1071.589
[81,     1] loss: 981.163
[82,     1] loss: 967.968
[83,     1] loss: 945.740
[84,     1] loss: 932.355
[85,     1] loss: 936.170
[86,     1] loss: 894.360
[87,     1] loss: 889.345
[88,     1] loss: 879.313
[89,     1] loss: 823.155
[90,     1] loss: 869.084
[91,     1] loss: 857.700
[92,     1] loss: 831.617
[93,     1] loss: 773.861
[94,     1] loss: 728.796
[95,     1] loss: 766.331
[96,     1] loss: 879.639
[97,     1] loss: 1356.775
Early stopping applied (best metric=0.9360615015029907)
Finished Training
Total time taken: 16.44034767150879
{'Hydroxylation-K Validation Accuracy': 0.7630910165484633, 'Hydroxylation-K Validation Sensitivity': 0.7081481481481481, 'Hydroxylation-K Validation Specificity': 0.7771929824561403, 'Hydroxylation-K Validation Precision': 0.4540088446125598, 'Hydroxylation-K AUC ROC': 0.7999025341130604, 'Hydroxylation-K AUC PR': 0.576588264298371, 'Hydroxylation-K MCC': 0.42276106940983993, 'Hydroxylation-K F1': 0.5473241970684426, 'Validation Loss (Hydroxylation-K)': 0.4378408352533976, 'Hydroxylation-P Validation Accuracy': 0.7614581324129063, 'Hydroxylation-P Validation Sensitivity': 0.7860317460317461, 'Hydroxylation-P Validation Specificity': 0.7561948226844232, 'Hydroxylation-P Validation Precision': 0.41973567384060445, 'Hydroxylation-P AUC ROC': 0.838418220597631, 'Hydroxylation-P AUC PR': 0.5727271769031715, 'Hydroxylation-P MCC': 0.44307511271076633, 'Hydroxylation-P F1': 0.5428733330671954, 'Validation Loss (Hydroxylation-P)': 0.3821144680182139, 'Validation Loss (total)': 0.819955309232076, 'TimeToTrain': 11.561778036753337}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015702795279850155,
 'learning_rate_Hydroxylation-K': 0.006250614229055034,
 'learning_rate_Hydroxylation-P': 0.001836766448706961,
 'log_base': 1.497948255815748,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2078857884,
 'sample_weights': [2.2737418167123136, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.903968216741068,
 'weight_decay_Hydroxylation-K': 7.96406494722609,
 'weight_decay_Hydroxylation-P': 1.0203104487463432}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1777.005
[2,     1] loss: 1778.486
[3,     1] loss: 1783.625
[4,     1] loss: 1773.194
[5,     1] loss: 1781.279
[6,     1] loss: 1777.011
[7,     1] loss: 1772.035
[8,     1] loss: 1772.417
[9,     1] loss: 1764.241
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008438777219753087,
 'learning_rate_Hydroxylation-K': 0.0064934430556372455,
 'learning_rate_Hydroxylation-P': 0.0015988938735808397,
 'log_base': 1.9052262844569934,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3276340429,
 'sample_weights': [4.131299822813795, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0087845844151797,
 'weight_decay_Hydroxylation-K': 2.597303795087962,
 'weight_decay_Hydroxylation-P': 4.753763514779984}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1458.121
[2,     1] loss: 1458.764
[3,     1] loss: 1451.551
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009532916036635083,
 'learning_rate_Hydroxylation-K': 0.005310291863570998,
 'learning_rate_Hydroxylation-P': 0.00186738791208482,
 'log_base': 1.596298996803782,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 640113308,
 'sample_weights': [2.589886924806981, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.194255897888139,
 'weight_decay_Hydroxylation-K': 9.911470466324028,
 'weight_decay_Hydroxylation-P': 8.599274680597304}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1662.316
[2,     1] loss: 1674.381
[3,     1] loss: 1660.163
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006887719914272493,
 'learning_rate_Hydroxylation-K': 0.003274199341589158,
 'learning_rate_Hydroxylation-P': 0.0018478236550507957,
 'log_base': 1.5427241647303962,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3308175510,
 'sample_weights': [3.569567275231166, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.311993364107889,
 'weight_decay_Hydroxylation-K': 7.514667086544822,
 'weight_decay_Hydroxylation-P': 3.4191550104803516}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1718.526
[2,     1] loss: 1730.921
[3,     1] loss: 1717.112
[4,     1] loss: 1719.175
[5,     1] loss: 1719.696
[6,     1] loss: 1718.302
[7,     1] loss: 1709.694
[8,     1] loss: 1717.409
[9,     1] loss: 1712.436
[10,     1] loss: 1703.726
[11,     1] loss: 1692.928
[12,     1] loss: 1667.019
[13,     1] loss: 1626.021
[14,     1] loss: 1613.140
[15,     1] loss: 1580.295
[16,     1] loss: 1517.654
[17,     1] loss: 1481.935
[18,     1] loss: 1450.630
[19,     1] loss: 1420.040
[20,     1] loss: 1449.872
[21,     1] loss: 1403.249
[22,     1] loss: 1464.830
[23,     1] loss: 1447.486
[24,     1] loss: 1390.420
[25,     1] loss: 1423.689
[26,     1] loss: 1300.888
[27,     1] loss: 1344.562
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014981549868583774,
 'learning_rate_Hydroxylation-K': 0.0031477570276697434,
 'learning_rate_Hydroxylation-P': 0.008988259799658876,
 'log_base': 1.1408768373048657,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2494248145,
 'sample_weights': [3.8506376404025553, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.47125121066324,
 'weight_decay_Hydroxylation-K': 5.404325834488274,
 'weight_decay_Hydroxylation-P': 6.562841261132792}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4109.599
[2,     1] loss: 4104.528
[3,     1] loss: 4101.770
[4,     1] loss: 4091.644
[5,     1] loss: 4098.341
[6,     1] loss: 4118.985
[7,     1] loss: 4111.628
[8,     1] loss: 4110.481
[9,     1] loss: 4090.830
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009847485592891952,
 'learning_rate_Hydroxylation-K': 0.009109235256391374,
 'learning_rate_Hydroxylation-P': 0.0037008106662214917,
 'log_base': 1.8345938372077986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3603020052,
 'sample_weights': [12.666764785423611, 1.5834065950085219],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5208051132831102,
 'weight_decay_Hydroxylation-K': 8.703901797282125,
 'weight_decay_Hydroxylation-P': 6.763192506968645}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1501.771
[2,     1] loss: 1506.596
[3,     1] loss: 1492.707
[4,     1] loss: 1488.674
[5,     1] loss: 1496.846
[6,     1] loss: 1492.714
[7,     1] loss: 1491.101
[8,     1] loss: 1488.111
[9,     1] loss: 1490.493
[10,     1] loss: 1487.206
[11,     1] loss: 1487.326
[12,     1] loss: 1491.799
[13,     1] loss: 1488.536
[14,     1] loss: 1486.966
[15,     1] loss: 1488.366
[16,     1] loss: 1489.338
[17,     1] loss: 1491.067
[18,     1] loss: 1489.283
[19,     1] loss: 1488.924
[20,     1] loss: 1488.041
[21,     1] loss: 1488.125
[22,     1] loss: 1488.886
[23,     1] loss: 1486.476
[24,     1] loss: 1485.901
[25,     1] loss: 1487.587
[26,     1] loss: 1486.459
Early stopping applied (best metric=1.0955368280410767)
Finished Training
Total time taken: 3.598074197769165
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1488.338
[2,     1] loss: 1486.976
[3,     1] loss: 1502.828
[4,     1] loss: 1495.205
[5,     1] loss: 1489.632
[6,     1] loss: 1486.035
[7,     1] loss: 1484.566
[8,     1] loss: 1492.083
[9,     1] loss: 1486.802
[10,     1] loss: 1487.302
[11,     1] loss: 1487.614
[12,     1] loss: 1485.364
[13,     1] loss: 1485.608
[14,     1] loss: 1482.946
[15,     1] loss: 1483.702
[16,     1] loss: 1481.227
[17,     1] loss: 1483.571
[18,     1] loss: 1478.139
[19,     1] loss: 1472.585
[20,     1] loss: 1466.736
[21,     1] loss: 1459.388
[22,     1] loss: 1451.469
[23,     1] loss: 1442.534
[24,     1] loss: 1394.695
[25,     1] loss: 1362.201
[26,     1] loss: 1307.282
[27,     1] loss: 1317.847
[28,     1] loss: 1259.167
[29,     1] loss: 1336.566
[30,     1] loss: 1252.786
[31,     1] loss: 1231.666
[32,     1] loss: 1216.119
[33,     1] loss: 1180.398
[34,     1] loss: 1254.817
[35,     1] loss: 1186.414
[36,     1] loss: 1228.926
[37,     1] loss: 1190.810
[38,     1] loss: 1167.542
[39,     1] loss: 1181.927
[40,     1] loss: 1185.632
[41,     1] loss: 1178.553
[42,     1] loss: 1108.881
[43,     1] loss: 1114.286
[44,     1] loss: 1147.670
[45,     1] loss: 1107.006
[46,     1] loss: 1139.061
[47,     1] loss: 1083.060
[48,     1] loss: 1062.338
[49,     1] loss: 1090.002
[50,     1] loss: 1098.796
[51,     1] loss: 1062.322
[52,     1] loss: 1158.448
[53,     1] loss: 1067.831
[54,     1] loss: 1100.585
[55,     1] loss: 1076.849
[56,     1] loss: 1059.518
[57,     1] loss: 1089.131
[58,     1] loss: 1032.209
[59,     1] loss: 1080.884
[60,     1] loss: 1034.136
[61,     1] loss: 1009.120
[62,     1] loss: 1003.874
[63,     1] loss: 995.738
[64,     1] loss: 1002.950
Early stopping applied (best metric=0.8741419315338135)
Finished Training
Total time taken: 10.648227214813232
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1489.593
[2,     1] loss: 1495.613
[3,     1] loss: 1490.288
[4,     1] loss: 1489.923
[5,     1] loss: 1492.195
[6,     1] loss: 1487.677
[7,     1] loss: 1489.534
[8,     1] loss: 1491.517
[9,     1] loss: 1488.893
[10,     1] loss: 1486.128
[11,     1] loss: 1485.288
[12,     1] loss: 1489.329
[13,     1] loss: 1489.694
[14,     1] loss: 1485.925
[15,     1] loss: 1484.144
[16,     1] loss: 1484.966
[17,     1] loss: 1486.856
[18,     1] loss: 1485.179
[19,     1] loss: 1485.723
[20,     1] loss: 1483.324
[21,     1] loss: 1484.408
[22,     1] loss: 1484.095
[23,     1] loss: 1479.513
[24,     1] loss: 1477.538
[25,     1] loss: 1476.362
[26,     1] loss: 1459.428
[27,     1] loss: 1449.538
[28,     1] loss: 1442.300
[29,     1] loss: 1410.350
[30,     1] loss: 1381.645
[31,     1] loss: 1343.092
[32,     1] loss: 1353.369
[33,     1] loss: 1327.470
[34,     1] loss: 1275.035
[35,     1] loss: 1234.135
[36,     1] loss: 1248.902
[37,     1] loss: 1221.421
[38,     1] loss: 1166.729
[39,     1] loss: 1243.064
[40,     1] loss: 1271.961
[41,     1] loss: 1191.258
[42,     1] loss: 1205.710
[43,     1] loss: 1165.390
[44,     1] loss: 1170.445
[45,     1] loss: 1133.661
[46,     1] loss: 1111.304
[47,     1] loss: 1078.017
[48,     1] loss: 1135.735
[49,     1] loss: 1061.791
[50,     1] loss: 1048.100
[51,     1] loss: 1059.035
[52,     1] loss: 1057.528
[53,     1] loss: 1079.914
Early stopping applied (best metric=0.9638373851776123)
Finished Training
Total time taken: 8.983189821243286
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1491.912
[2,     1] loss: 1498.178
[3,     1] loss: 1487.594
[4,     1] loss: 1486.100
[5,     1] loss: 1491.131
[6,     1] loss: 1486.838
[7,     1] loss: 1480.644
[8,     1] loss: 1475.401
[9,     1] loss: 1477.656
[10,     1] loss: 1476.616
[11,     1] loss: 1470.069
[12,     1] loss: 1448.651
[13,     1] loss: 1423.286
[14,     1] loss: 1406.840
[15,     1] loss: 1346.776
[16,     1] loss: 1302.708
[17,     1] loss: 1293.387
[18,     1] loss: 1250.956
[19,     1] loss: 1247.781
[20,     1] loss: 1197.531
[21,     1] loss: 1202.250
[22,     1] loss: 1192.870
[23,     1] loss: 1221.947
[24,     1] loss: 1193.424
[25,     1] loss: 1200.871
[26,     1] loss: 1142.604
[27,     1] loss: 1128.008
[28,     1] loss: 1151.095
[29,     1] loss: 1026.052
[30,     1] loss: 1183.348
[31,     1] loss: 1089.600
[32,     1] loss: 1139.500
[33,     1] loss: 1071.655
[34,     1] loss: 1107.439
[35,     1] loss: 1078.549
Early stopping applied (best metric=0.9745753407478333)
Finished Training
Total time taken: 5.163110256195068
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1490.127
[2,     1] loss: 1499.634
[3,     1] loss: 1499.964
[4,     1] loss: 1482.005
[5,     1] loss: 1497.027
[6,     1] loss: 1492.058
[7,     1] loss: 1490.733
[8,     1] loss: 1485.026
[9,     1] loss: 1482.067
[10,     1] loss: 1486.801
[11,     1] loss: 1494.479
[12,     1] loss: 1495.222
[13,     1] loss: 1493.808
[14,     1] loss: 1491.016
[15,     1] loss: 1489.361
[16,     1] loss: 1483.470
[17,     1] loss: 1490.690
[18,     1] loss: 1489.006
[19,     1] loss: 1496.629
[20,     1] loss: 1487.908
[21,     1] loss: 1488.393
[22,     1] loss: 1489.346
[23,     1] loss: 1487.474
[24,     1] loss: 1485.139
[25,     1] loss: 1487.479
[26,     1] loss: 1489.036
[27,     1] loss: 1487.619
[28,     1] loss: 1489.270
[29,     1] loss: 1485.995
[30,     1] loss: 1485.286
[31,     1] loss: 1488.259
[32,     1] loss: 1482.545
[33,     1] loss: 1482.565
[34,     1] loss: 1481.385
[35,     1] loss: 1475.345
[36,     1] loss: 1469.469
[37,     1] loss: 1457.489
[38,     1] loss: 1446.722
[39,     1] loss: 1435.252
[40,     1] loss: 1419.178
[41,     1] loss: 1378.333
[42,     1] loss: 1363.367
[43,     1] loss: 1321.504
[44,     1] loss: 1349.752
[45,     1] loss: 1295.418
[46,     1] loss: 1288.375
[47,     1] loss: 1268.089
[48,     1] loss: 1288.886
[49,     1] loss: 1256.283
[50,     1] loss: 1223.818
[51,     1] loss: 1247.815
[52,     1] loss: 1213.050
[53,     1] loss: 1226.379
[54,     1] loss: 1172.693
[55,     1] loss: 1193.950
[56,     1] loss: 1219.619
[57,     1] loss: 1197.052
[58,     1] loss: 1219.291
[59,     1] loss: 1130.890
[60,     1] loss: 1142.023
[61,     1] loss: 1158.480
[62,     1] loss: 1070.038
[63,     1] loss: 1123.121
[64,     1] loss: 1143.331
[65,     1] loss: 1122.261
[66,     1] loss: 1142.615
[67,     1] loss: 1027.960
[68,     1] loss: 1077.655
[69,     1] loss: 1090.293
[70,     1] loss: 1069.891
[71,     1] loss: 1045.715
[72,     1] loss: 1076.358
[73,     1] loss: 1108.153
[74,     1] loss: 996.204
[75,     1] loss: 1067.940
[76,     1] loss: 984.971
[77,     1] loss: 1045.492
[78,     1] loss: 1073.761
[79,     1] loss: 1002.798
[80,     1] loss: 1019.313
[81,     1] loss: 1007.873
[82,     1] loss: 1044.610
[83,     1] loss: 1021.065
[84,     1] loss: 1039.149
[85,     1] loss: 1014.318
[86,     1] loss: 1021.001
[87,     1] loss: 1019.045
[88,     1] loss: 980.465
[89,     1] loss: 1029.807
[90,     1] loss: 963.980
[91,     1] loss: 904.058
[92,     1] loss: 878.751
[93,     1] loss: 932.182
[94,     1] loss: 862.651
[95,     1] loss: 864.725
[96,     1] loss: 858.382
[97,     1] loss: 860.854
Early stopping applied (best metric=0.691699743270874)
Finished Training
Total time taken: 14.54940938949585
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1488.093
[2,     1] loss: 1490.677
[3,     1] loss: 1491.329
[4,     1] loss: 1487.609
[5,     1] loss: 1489.732
[6,     1] loss: 1480.406
[7,     1] loss: 1484.863
[8,     1] loss: 1480.817
[9,     1] loss: 1482.053
[10,     1] loss: 1485.080
[11,     1] loss: 1481.062
[12,     1] loss: 1469.364
[13,     1] loss: 1456.058
[14,     1] loss: 1461.817
[15,     1] loss: 1446.250
[16,     1] loss: 1396.306
[17,     1] loss: 1397.958
[18,     1] loss: 1325.542
[19,     1] loss: 1341.440
[20,     1] loss: 1317.363
[21,     1] loss: 1284.668
[22,     1] loss: 1275.265
[23,     1] loss: 1248.964
[24,     1] loss: 1250.050
[25,     1] loss: 1253.771
[26,     1] loss: 1279.329
[27,     1] loss: 1213.630
[28,     1] loss: 1214.138
[29,     1] loss: 1217.895
[30,     1] loss: 1224.581
[31,     1] loss: 1212.964
[32,     1] loss: 1218.709
[33,     1] loss: 1190.900
[34,     1] loss: 1204.281
[35,     1] loss: 1136.395
[36,     1] loss: 1189.808
[37,     1] loss: 1144.695
[38,     1] loss: 1136.652
[39,     1] loss: 1149.283
[40,     1] loss: 1152.137
[41,     1] loss: 1114.222
[42,     1] loss: 1114.909
[43,     1] loss: 1106.360
[44,     1] loss: 1143.440
[45,     1] loss: 1112.668
[46,     1] loss: 1113.431
[47,     1] loss: 1152.127
[48,     1] loss: 1126.872
[49,     1] loss: 1099.744
[50,     1] loss: 1060.646
[51,     1] loss: 1059.350
[52,     1] loss: 1100.252
[53,     1] loss: 997.962
[54,     1] loss: 1031.448
[55,     1] loss: 1084.352
[56,     1] loss: 1086.271
[57,     1] loss: 1075.565
[58,     1] loss: 1053.017
[59,     1] loss: 1043.157
[60,     1] loss: 1089.640
[61,     1] loss: 1001.154
[62,     1] loss: 1019.022
[63,     1] loss: 1001.840
Early stopping applied (best metric=0.7484573125839233)
Finished Training
Total time taken: 10.179241418838501
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1490.304
[2,     1] loss: 1495.534
[3,     1] loss: 1486.646
[4,     1] loss: 1496.615
[5,     1] loss: 1490.633
[6,     1] loss: 1485.856
[7,     1] loss: 1487.649
[8,     1] loss: 1490.007
[9,     1] loss: 1489.988
[10,     1] loss: 1489.142
[11,     1] loss: 1488.438
[12,     1] loss: 1487.383
[13,     1] loss: 1485.771
[14,     1] loss: 1488.340
[15,     1] loss: 1488.130
[16,     1] loss: 1486.473
[17,     1] loss: 1488.103
[18,     1] loss: 1487.744
[19,     1] loss: 1488.134
[20,     1] loss: 1487.973
[21,     1] loss: 1487.306
[22,     1] loss: 1487.032
[23,     1] loss: 1487.623
[24,     1] loss: 1487.071
[25,     1] loss: 1486.977
[26,     1] loss: 1487.222
[27,     1] loss: 1485.160
[28,     1] loss: 1491.860
[29,     1] loss: 1485.992
[30,     1] loss: 1491.033
[31,     1] loss: 1486.139
[32,     1] loss: 1486.140
[33,     1] loss: 1486.331
[34,     1] loss: 1483.943
[35,     1] loss: 1485.625
[36,     1] loss: 1482.183
[37,     1] loss: 1482.896
[38,     1] loss: 1477.605
[39,     1] loss: 1470.753
[40,     1] loss: 1462.440
[41,     1] loss: 1443.904
[42,     1] loss: 1408.525
[43,     1] loss: 1372.000
[44,     1] loss: 1369.275
[45,     1] loss: 1354.673
[46,     1] loss: 1348.897
[47,     1] loss: 1340.204
[48,     1] loss: 1302.929
[49,     1] loss: 1308.578
[50,     1] loss: 1288.661
[51,     1] loss: 1280.314
[52,     1] loss: 1273.535
[53,     1] loss: 1280.391
[54,     1] loss: 1185.084
[55,     1] loss: 1241.300
[56,     1] loss: 1229.670
[57,     1] loss: 1251.506
[58,     1] loss: 1219.163
[59,     1] loss: 1183.296
[60,     1] loss: 1145.374
[61,     1] loss: 1189.832
[62,     1] loss: 1197.610
[63,     1] loss: 1161.105
[64,     1] loss: 1117.961
[65,     1] loss: 1184.947
[66,     1] loss: 1103.983
[67,     1] loss: 1183.818
[68,     1] loss: 1101.280
[69,     1] loss: 1128.912
[70,     1] loss: 1137.293
[71,     1] loss: 1074.161
[72,     1] loss: 1107.180
[73,     1] loss: 1046.030
[74,     1] loss: 1075.231
[75,     1] loss: 1026.130
[76,     1] loss: 997.287
[77,     1] loss: 984.781
[78,     1] loss: 1009.094
[79,     1] loss: 987.048
[80,     1] loss: 1033.549
[81,     1] loss: 1022.410
[82,     1] loss: 944.820
[83,     1] loss: 932.228
[84,     1] loss: 1069.660
[85,     1] loss: 983.247
[86,     1] loss: 1257.473
[87,     1] loss: 986.480
[88,     1] loss: 1057.231
[89,     1] loss: 948.364
[90,     1] loss: 1012.407
Early stopping applied (best metric=0.7738595008850098)
Finished Training
Total time taken: 12.021252632141113
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1492.602
[2,     1] loss: 1490.401
[3,     1] loss: 1495.136
[4,     1] loss: 1489.825
[5,     1] loss: 1490.563
[6,     1] loss: 1491.738
[7,     1] loss: 1488.765
[8,     1] loss: 1487.195
[9,     1] loss: 1486.013
[10,     1] loss: 1488.226
[11,     1] loss: 1486.109
[12,     1] loss: 1487.558
[13,     1] loss: 1482.093
[14,     1] loss: 1477.965
[15,     1] loss: 1475.382
[16,     1] loss: 1472.522
[17,     1] loss: 1464.356
[18,     1] loss: 1451.269
[19,     1] loss: 1428.704
[20,     1] loss: 1402.468
[21,     1] loss: 1386.026
[22,     1] loss: 1353.610
[23,     1] loss: 1339.789
[24,     1] loss: 1355.361
[25,     1] loss: 1291.754
[26,     1] loss: 1281.298
[27,     1] loss: 1313.448
[28,     1] loss: 1263.708
[29,     1] loss: 1273.749
[30,     1] loss: 1268.769
[31,     1] loss: 1265.252
[32,     1] loss: 1287.698
[33,     1] loss: 1240.445
[34,     1] loss: 1266.006
[35,     1] loss: 1230.512
[36,     1] loss: 1258.710
[37,     1] loss: 1202.812
[38,     1] loss: 1262.925
[39,     1] loss: 1257.736
[40,     1] loss: 1208.482
[41,     1] loss: 1214.057
[42,     1] loss: 1182.883
[43,     1] loss: 1246.922
[44,     1] loss: 1207.795
[45,     1] loss: 1116.535
[46,     1] loss: 1177.874
[47,     1] loss: 1125.707
[48,     1] loss: 1175.653
[49,     1] loss: 1110.427
[50,     1] loss: 1125.905
[51,     1] loss: 1083.767
[52,     1] loss: 1064.031
[53,     1] loss: 1134.732
[54,     1] loss: 1058.214
[55,     1] loss: 1153.447
[56,     1] loss: 1142.093
[57,     1] loss: 1155.600
[58,     1] loss: 1032.336
[59,     1] loss: 1035.413
[60,     1] loss: 1088.516
[61,     1] loss: 1024.946
[62,     1] loss: 1025.271
[63,     1] loss: 1036.521
[64,     1] loss: 1045.694
[65,     1] loss: 1030.244
[66,     1] loss: 1030.894
[67,     1] loss: 1106.266
[68,     1] loss: 994.326
[69,     1] loss: 960.285
[70,     1] loss: 1007.594
Early stopping applied (best metric=0.7416754961013794)
Finished Training
Total time taken: 10.78522777557373
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1492.462
[2,     1] loss: 1490.087
[3,     1] loss: 1488.898
[4,     1] loss: 1488.776
[5,     1] loss: 1500.467
[6,     1] loss: 1486.300
[7,     1] loss: 1489.208
[8,     1] loss: 1488.387
[9,     1] loss: 1486.979
[10,     1] loss: 1485.330
[11,     1] loss: 1488.071
[12,     1] loss: 1488.554
[13,     1] loss: 1485.807
[14,     1] loss: 1485.219
[15,     1] loss: 1484.381
[16,     1] loss: 1484.387
[17,     1] loss: 1481.220
[18,     1] loss: 1487.365
[19,     1] loss: 1485.323
[20,     1] loss: 1475.951
[21,     1] loss: 1469.725
[22,     1] loss: 1468.630
[23,     1] loss: 1459.826
[24,     1] loss: 1449.281
[25,     1] loss: 1426.218
[26,     1] loss: 1394.267
[27,     1] loss: 1364.284
[28,     1] loss: 1342.794
[29,     1] loss: 1289.213
[30,     1] loss: 1309.597
[31,     1] loss: 1298.827
[32,     1] loss: 1285.633
[33,     1] loss: 1229.365
[34,     1] loss: 1272.267
[35,     1] loss: 1209.624
[36,     1] loss: 1233.686
[37,     1] loss: 1190.942
[38,     1] loss: 1228.706
[39,     1] loss: 1184.712
[40,     1] loss: 1169.769
[41,     1] loss: 1127.774
[42,     1] loss: 1199.312
[43,     1] loss: 1158.005
[44,     1] loss: 1172.882
[45,     1] loss: 1166.122
[46,     1] loss: 1147.212
[47,     1] loss: 1179.252
[48,     1] loss: 1180.027
[49,     1] loss: 1149.608
[50,     1] loss: 1166.312
[51,     1] loss: 1092.660
[52,     1] loss: 1164.804
[53,     1] loss: 1189.933
[54,     1] loss: 1074.141
[55,     1] loss: 1121.024
[56,     1] loss: 1074.955
[57,     1] loss: 1051.953
[58,     1] loss: 1073.558
[59,     1] loss: 1107.864
[60,     1] loss: 1043.877
[61,     1] loss: 1023.798
[62,     1] loss: 982.834
[63,     1] loss: 1053.494
[64,     1] loss: 1086.459
[65,     1] loss: 992.805
[66,     1] loss: 1082.377
[67,     1] loss: 1029.078
[68,     1] loss: 989.549
[69,     1] loss: 981.746
[70,     1] loss: 982.780
[71,     1] loss: 999.951
[72,     1] loss: 1016.555
[73,     1] loss: 927.218
[74,     1] loss: 942.480
[75,     1] loss: 979.759
Early stopping applied (best metric=0.7897530794143677)
Finished Training
Total time taken: 12.367261171340942
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1485.618
[2,     1] loss: 1489.701
[3,     1] loss: 1489.145
[4,     1] loss: 1496.292
[5,     1] loss: 1496.121
[6,     1] loss: 1490.213
[7,     1] loss: 1488.774
[8,     1] loss: 1487.157
[9,     1] loss: 1482.628
[10,     1] loss: 1486.971
[11,     1] loss: 1481.467
[12,     1] loss: 1477.306
[13,     1] loss: 1477.027
[14,     1] loss: 1458.763
[15,     1] loss: 1460.052
[16,     1] loss: 1442.719
[17,     1] loss: 1419.926
[18,     1] loss: 1379.863
[19,     1] loss: 1340.829
[20,     1] loss: 1273.991
[21,     1] loss: 1278.445
[22,     1] loss: 1207.953
[23,     1] loss: 1212.088
[24,     1] loss: 1185.878
[25,     1] loss: 1233.046
[26,     1] loss: 1145.800
[27,     1] loss: 1202.518
[28,     1] loss: 1151.209
[29,     1] loss: 1143.929
[30,     1] loss: 1193.121
[31,     1] loss: 1168.217
[32,     1] loss: 1150.386
[33,     1] loss: 1145.887
[34,     1] loss: 1169.519
[35,     1] loss: 1094.110
[36,     1] loss: 1101.326
[37,     1] loss: 1046.558
[38,     1] loss: 1116.377
[39,     1] loss: 1052.223
Early stopping applied (best metric=0.9488807320594788)
Finished Training
Total time taken: 5.167108058929443
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1491.532
[2,     1] loss: 1490.247
[3,     1] loss: 1488.629
[4,     1] loss: 1486.171
[5,     1] loss: 1487.243
[6,     1] loss: 1485.388
[7,     1] loss: 1487.182
[8,     1] loss: 1478.111
[9,     1] loss: 1482.826
[10,     1] loss: 1478.374
[11,     1] loss: 1473.901
[12,     1] loss: 1461.808
[13,     1] loss: 1458.341
[14,     1] loss: 1438.114
[15,     1] loss: 1393.070
[16,     1] loss: 1366.439
[17,     1] loss: 1334.615
[18,     1] loss: 1322.310
[19,     1] loss: 1292.528
[20,     1] loss: 1292.338
[21,     1] loss: 1269.475
[22,     1] loss: 1252.548
[23,     1] loss: 1219.652
[24,     1] loss: 1221.938
[25,     1] loss: 1224.807
[26,     1] loss: 1247.966
[27,     1] loss: 1240.315
[28,     1] loss: 1196.752
[29,     1] loss: 1212.160
[30,     1] loss: 1203.007
[31,     1] loss: 1209.548
[32,     1] loss: 1170.452
[33,     1] loss: 1161.586
[34,     1] loss: 1123.449
[35,     1] loss: 1156.892
[36,     1] loss: 1121.511
[37,     1] loss: 1179.164
[38,     1] loss: 1108.212
[39,     1] loss: 1111.925
[40,     1] loss: 1055.054
[41,     1] loss: 1090.241
[42,     1] loss: 1129.255
[43,     1] loss: 1100.217
[44,     1] loss: 1132.316
[45,     1] loss: 1032.230
[46,     1] loss: 1065.805
[47,     1] loss: 1098.829
[48,     1] loss: 1045.336
[49,     1] loss: 1062.644
[50,     1] loss: 1023.527
Early stopping applied (best metric=0.864758312702179)
Finished Training
Total time taken: 8.207174301147461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1484.546
[2,     1] loss: 1492.188
[3,     1] loss: 1498.036
[4,     1] loss: 1486.771
[5,     1] loss: 1495.888
[6,     1] loss: 1490.322
[7,     1] loss: 1493.913
[8,     1] loss: 1491.704
[9,     1] loss: 1482.859
[10,     1] loss: 1487.642
[11,     1] loss: 1491.817
[12,     1] loss: 1491.334
[13,     1] loss: 1486.348
[14,     1] loss: 1487.381
[15,     1] loss: 1486.172
[16,     1] loss: 1485.928
[17,     1] loss: 1482.270
[18,     1] loss: 1482.007
[19,     1] loss: 1483.652
[20,     1] loss: 1476.068
[21,     1] loss: 1478.382
[22,     1] loss: 1476.483
[23,     1] loss: 1470.160
[24,     1] loss: 1463.258
[25,     1] loss: 1442.618
[26,     1] loss: 1430.978
[27,     1] loss: 1389.986
[28,     1] loss: 1373.102
[29,     1] loss: 1312.035
[30,     1] loss: 1295.481
[31,     1] loss: 1264.700
[32,     1] loss: 1387.748
[33,     1] loss: 1286.269
[34,     1] loss: 1263.012
[35,     1] loss: 1254.620
[36,     1] loss: 1257.446
[37,     1] loss: 1257.843
[38,     1] loss: 1212.351
[39,     1] loss: 1246.788
[40,     1] loss: 1233.311
[41,     1] loss: 1211.902
[42,     1] loss: 1215.003
[43,     1] loss: 1232.963
[44,     1] loss: 1169.109
[45,     1] loss: 1165.143
[46,     1] loss: 1180.358
[47,     1] loss: 1164.282
[48,     1] loss: 1155.019
[49,     1] loss: 1095.223
[50,     1] loss: 1131.919
[51,     1] loss: 1132.271
[52,     1] loss: 1149.926
[53,     1] loss: 1089.556
[54,     1] loss: 1096.767
[55,     1] loss: 1124.155
[56,     1] loss: 1059.627
[57,     1] loss: 1053.117
[58,     1] loss: 1086.911
[59,     1] loss: 1065.389
[60,     1] loss: 1071.250
[61,     1] loss: 1058.637
[62,     1] loss: 1017.735
[63,     1] loss: 976.825
[64,     1] loss: 1030.200
[65,     1] loss: 1088.808
[66,     1] loss: 983.604
[67,     1] loss: 1005.120
[68,     1] loss: 975.128
[69,     1] loss: 979.015
[70,     1] loss: 1108.480
[71,     1] loss: 964.690
[72,     1] loss: 1038.137
[73,     1] loss: 960.444
[74,     1] loss: 971.938
[75,     1] loss: 962.270
[76,     1] loss: 921.574
[77,     1] loss: 1032.002
[78,     1] loss: 898.373
[79,     1] loss: 944.123
[80,     1] loss: 1013.963
Early stopping applied (best metric=0.7386802434921265)
Finished Training
Total time taken: 10.728225946426392
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1494.686
[2,     1] loss: 1496.920
[3,     1] loss: 1488.241
[4,     1] loss: 1487.844
[5,     1] loss: 1486.663
[6,     1] loss: 1488.112
[7,     1] loss: 1486.996
[8,     1] loss: 1485.723
[9,     1] loss: 1484.940
[10,     1] loss: 1483.302
[11,     1] loss: 1476.823
[12,     1] loss: 1479.247
[13,     1] loss: 1473.823
[14,     1] loss: 1467.172
[15,     1] loss: 1451.967
[16,     1] loss: 1428.528
[17,     1] loss: 1377.555
[18,     1] loss: 1337.601
[19,     1] loss: 1305.490
[20,     1] loss: 1310.058
[21,     1] loss: 1323.136
[22,     1] loss: 1278.999
[23,     1] loss: 1333.658
[24,     1] loss: 1259.490
[25,     1] loss: 1246.123
[26,     1] loss: 1228.918
[27,     1] loss: 1235.278
[28,     1] loss: 1273.035
[29,     1] loss: 1240.832
[30,     1] loss: 1228.380
[31,     1] loss: 1212.603
[32,     1] loss: 1189.157
[33,     1] loss: 1216.924
[34,     1] loss: 1218.781
[35,     1] loss: 1194.960
[36,     1] loss: 1223.465
[37,     1] loss: 1169.538
[38,     1] loss: 1179.754
[39,     1] loss: 1187.152
[40,     1] loss: 1190.158
[41,     1] loss: 1152.203
[42,     1] loss: 1141.286
[43,     1] loss: 1113.797
[44,     1] loss: 1141.903
[45,     1] loss: 1166.536
[46,     1] loss: 1121.187
[47,     1] loss: 1126.922
[48,     1] loss: 1077.722
[49,     1] loss: 1091.827
[50,     1] loss: 1064.077
[51,     1] loss: 1080.927
[52,     1] loss: 1065.085
[53,     1] loss: 1135.607
[54,     1] loss: 1101.394
[55,     1] loss: 1153.549
[56,     1] loss: 1038.810
[57,     1] loss: 1038.710
[58,     1] loss: 978.930
[59,     1] loss: 992.894
[60,     1] loss: 1028.434
[61,     1] loss: 1083.181
[62,     1] loss: 1031.570
[63,     1] loss: 1076.408
[64,     1] loss: 1019.261
[65,     1] loss: 1006.171
[66,     1] loss: 1024.954
[67,     1] loss: 1012.595
[68,     1] loss: 987.854
[69,     1] loss: 1009.661
[70,     1] loss: 937.090
[71,     1] loss: 1006.730
[72,     1] loss: 970.248
[73,     1] loss: 964.329
[74,     1] loss: 968.453
[75,     1] loss: 923.201
[76,     1] loss: 945.266
[77,     1] loss: 825.008
[78,     1] loss: 873.238
Early stopping applied (best metric=0.8321437835693359)
Finished Training
Total time taken: 12.736268758773804
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1490.416
[2,     1] loss: 1491.941
[3,     1] loss: 1486.683
[4,     1] loss: 1494.375
[5,     1] loss: 1482.845
[6,     1] loss: 1482.281
[7,     1] loss: 1480.966
[8,     1] loss: 1474.223
[9,     1] loss: 1467.979
[10,     1] loss: 1447.458
[11,     1] loss: 1393.084
[12,     1] loss: 1361.243
[13,     1] loss: 1298.697
[14,     1] loss: 1262.780
[15,     1] loss: 1261.884
[16,     1] loss: 1281.682
[17,     1] loss: 1149.943
[18,     1] loss: 1231.408
[19,     1] loss: 1213.307
[20,     1] loss: 1211.560
[21,     1] loss: 1184.718
[22,     1] loss: 1162.213
[23,     1] loss: 1163.749
[24,     1] loss: 1214.812
[25,     1] loss: 1209.551
[26,     1] loss: 1111.639
[27,     1] loss: 1137.376
[28,     1] loss: 1066.915
[29,     1] loss: 1166.593
[30,     1] loss: 1100.415
[31,     1] loss: 1138.607
[32,     1] loss: 1109.555
[33,     1] loss: 1138.992
[34,     1] loss: 1150.556
[35,     1] loss: 1108.084
[36,     1] loss: 1065.266
[37,     1] loss: 1096.431
[38,     1] loss: 1092.121
[39,     1] loss: 1097.970
[40,     1] loss: 1031.960
[41,     1] loss: 1050.748
[42,     1] loss: 984.112
[43,     1] loss: 1027.551
[44,     1] loss: 1034.908
[45,     1] loss: 1122.339
[46,     1] loss: 1045.080
[47,     1] loss: 1009.315
[48,     1] loss: 1055.495
[49,     1] loss: 1013.485
[50,     1] loss: 1105.278
[51,     1] loss: 967.161
[52,     1] loss: 960.533
[53,     1] loss: 1026.438
[54,     1] loss: 910.248
[55,     1] loss: 919.600
[56,     1] loss: 998.402
[57,     1] loss: 920.937
[58,     1] loss: 920.699
[59,     1] loss: 880.659
Early stopping applied (best metric=0.932657778263092)
Finished Training
Total time taken: 9.746204614639282
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1506.562
[2,     1] loss: 1489.154
[3,     1] loss: 1491.818
[4,     1] loss: 1494.480
[5,     1] loss: 1494.094
[6,     1] loss: 1489.494
[7,     1] loss: 1490.733
[8,     1] loss: 1496.171
[9,     1] loss: 1492.967
[10,     1] loss: 1495.569
[11,     1] loss: 1489.737
[12,     1] loss: 1491.813
[13,     1] loss: 1490.270
[14,     1] loss: 1488.202
[15,     1] loss: 1493.666
[16,     1] loss: 1491.407
[17,     1] loss: 1489.659
[18,     1] loss: 1490.785
[19,     1] loss: 1488.726
[20,     1] loss: 1489.258
[21,     1] loss: 1488.803
[22,     1] loss: 1486.943
[23,     1] loss: 1491.698
[24,     1] loss: 1491.552
[25,     1] loss: 1490.261
[26,     1] loss: 1489.287
[27,     1] loss: 1488.207
[28,     1] loss: 1487.608
[29,     1] loss: 1485.850
[30,     1] loss: 1490.260
[31,     1] loss: 1492.478
[32,     1] loss: 1490.117
[33,     1] loss: 1490.469
[34,     1] loss: 1488.917
[35,     1] loss: 1490.117
[36,     1] loss: 1491.574
[37,     1] loss: 1490.050
[38,     1] loss: 1491.666
[39,     1] loss: 1491.790
[40,     1] loss: 1490.132
[41,     1] loss: 1485.995
[42,     1] loss: 1486.067
Early stopping applied (best metric=1.0720994472503662)
Finished Training
Total time taken: 5.621117115020752
{'Hydroxylation-K Validation Accuracy': 0.691725768321513, 'Hydroxylation-K Validation Sensitivity': 0.6785185185185185, 'Hydroxylation-K Validation Specificity': 0.6947368421052632, 'Hydroxylation-K Validation Precision': 0.42123810621307495, 'Hydroxylation-K AUC ROC': 0.7415204678362574, 'Hydroxylation-K AUC PR': 0.5098373613164447, 'Hydroxylation-K MCC': 0.3365567366837278, 'Hydroxylation-K F1': 0.49801362419352185, 'Validation Loss (Hydroxylation-K)': 0.4719773709774017, 'Hydroxylation-P Validation Accuracy': 0.7068416154848316, 'Hydroxylation-P Validation Sensitivity': 0.7932804232804233, 'Hydroxylation-P Validation Specificity': 0.6880343159259813, 'Hydroxylation-P Validation Precision': 0.4184598351130699, 'Hydroxylation-P AUC ROC': 0.8369278759103688, 'Hydroxylation-P AUC PR': 0.5435731849213276, 'Hydroxylation-P MCC': 0.40512957860838805, 'Hydroxylation-P F1': 0.5282989327875188, 'Validation Loss (Hydroxylation-P)': 0.3975397527217865, 'Validation Loss (total)': 0.8695171276728312, 'TimeToTrain': 9.366739511489868}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006793843361387198,
 'learning_rate_Hydroxylation-K': 0.0029666174992151774,
 'learning_rate_Hydroxylation-P': 0.006047387618644252,
 'log_base': 2.945354504047828,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3736618271,
 'sample_weights': [2.7531606767972483, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.59348910237866,
 'weight_decay_Hydroxylation-K': 4.26200048096741,
 'weight_decay_Hydroxylation-P': 1.6719146080535052}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.471
[2,     1] loss: 1234.684
[3,     1] loss: 1238.824
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00035678671549231426,
 'learning_rate_Hydroxylation-K': 0.0016111944934661727,
 'learning_rate_Hydroxylation-P': 0.005158560960314069,
 'log_base': 2.901590240457802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3743295895,
 'sample_weights': [1.5454527344705622, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4325282579928276,
 'weight_decay_Hydroxylation-K': 6.005466200597925,
 'weight_decay_Hydroxylation-P': 6.749588050616657}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.161
[2,     1] loss: 1239.002
[3,     1] loss: 1243.723
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007249905656665201,
 'learning_rate_Hydroxylation-K': 0.0012175705139734973,
 'learning_rate_Hydroxylation-P': 0.004493826728515869,
 'log_base': 1.186235058197706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3363635210,
 'sample_weights': [1.5671712070681043, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0109192664713715,
 'weight_decay_Hydroxylation-K': 6.933079033066159,
 'weight_decay_Hydroxylation-P': 2.9791375607977812}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3193.159
[2,     1] loss: 3193.053
[3,     1] loss: 3178.694
[4,     1] loss: 3185.068
[5,     1] loss: 3177.021
[6,     1] loss: 3179.888
[7,     1] loss: 3166.416
[8,     1] loss: 3174.031
[9,     1] loss: 3172.141
[10,     1] loss: 3164.561
[11,     1] loss: 3166.232
[12,     1] loss: 3163.268
[13,     1] loss: 3170.781
[14,     1] loss: 3172.383
[15,     1] loss: 3159.137
[16,     1] loss: 3163.344
[17,     1] loss: 3171.410
[18,     1] loss: 3156.658
[19,     1] loss: 3155.545
[20,     1] loss: 3159.090
[21,     1] loss: 3145.411
[22,     1] loss: 3141.449
[23,     1] loss: 3136.484
[24,     1] loss: 3083.368
[25,     1] loss: 3041.239
[26,     1] loss: 3011.891
[27,     1] loss: 2926.269
[28,     1] loss: 2932.016
[29,     1] loss: 2849.162
[30,     1] loss: 2804.069
[31,     1] loss: 2820.010
[32,     1] loss: 2790.544
[33,     1] loss: 2754.847
[34,     1] loss: 2692.184
[35,     1] loss: 2818.323
[36,     1] loss: 2755.632
[37,     1] loss: 2757.197
[38,     1] loss: 2751.035
[39,     1] loss: 2466.846
[40,     1] loss: 2613.717
[41,     1] loss: 2576.449
[42,     1] loss: 2595.856
[43,     1] loss: 2423.854
[44,     1] loss: 2540.535
[45,     1] loss: 2319.435
[46,     1] loss: 2311.760
[47,     1] loss: 2433.536
[48,     1] loss: 2436.088
[49,     1] loss: 2574.257
[50,     1] loss: 2242.041
[51,     1] loss: 2243.378
[52,     1] loss: 2314.854
[53,     1] loss: 2153.402
[54,     1] loss: 2107.289
[55,     1] loss: 2266.409
[56,     1] loss: 2170.854
[57,     1] loss: 2202.043
[58,     1] loss: 2413.904
[59,     1] loss: 2043.988
[60,     1] loss: 2130.816
[61,     1] loss: 2002.178
[62,     1] loss: 1858.839
[63,     1] loss: 2017.085
Early stopping applied (best metric=0.8396063446998596)
Finished Training
Total time taken: 8.675181150436401
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3190.326
[2,     1] loss: 3173.758
[3,     1] loss: 3177.582
[4,     1] loss: 3178.047
[5,     1] loss: 3181.993
[6,     1] loss: 3183.453
[7,     1] loss: 3156.160
[8,     1] loss: 3185.284
[9,     1] loss: 3175.267
[10,     1] loss: 3172.238
[11,     1] loss: 3164.121
[12,     1] loss: 3165.401
[13,     1] loss: 3164.799
[14,     1] loss: 3156.065
[15,     1] loss: 3152.859
[16,     1] loss: 3151.651
[17,     1] loss: 3138.710
[18,     1] loss: 3147.911
[19,     1] loss: 3125.659
[20,     1] loss: 3084.251
[21,     1] loss: 3099.781
[22,     1] loss: 3061.081
[23,     1] loss: 3006.410
[24,     1] loss: 2949.655
[25,     1] loss: 2892.640
[26,     1] loss: 2886.992
[27,     1] loss: 2718.139
[28,     1] loss: 2768.348
[29,     1] loss: 2474.885
[30,     1] loss: 2739.396
[31,     1] loss: 2534.237
[32,     1] loss: 2409.251
[33,     1] loss: 2392.062
[34,     1] loss: 2544.929
[35,     1] loss: 2450.510
[36,     1] loss: 2387.033
[37,     1] loss: 2299.985
[38,     1] loss: 2325.424
[39,     1] loss: 2284.591
[40,     1] loss: 2258.498
[41,     1] loss: 2362.397
[42,     1] loss: 2281.674
[43,     1] loss: 2455.943
[44,     1] loss: 2313.617
[45,     1] loss: 2071.409
[46,     1] loss: 1994.674
[47,     1] loss: 2107.219
[48,     1] loss: 1943.927
[49,     1] loss: 2032.976
[50,     1] loss: 1950.847
[51,     1] loss: 1817.132
[52,     1] loss: 1955.886
[53,     1] loss: 1762.276
[54,     1] loss: 2067.931
[55,     1] loss: 2452.420
[56,     1] loss: 1984.460
[57,     1] loss: 1714.467
Early stopping applied (best metric=0.952407956123352)
Finished Training
Total time taken: 9.262195110321045
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3169.188
[2,     1] loss: 3198.069
[3,     1] loss: 3175.215
[4,     1] loss: 3166.803
[5,     1] loss: 3168.685
[6,     1] loss: 3178.298
[7,     1] loss: 3184.160
[8,     1] loss: 3162.746
[9,     1] loss: 3161.291
[10,     1] loss: 3154.792
[11,     1] loss: 3172.585
[12,     1] loss: 3157.019
[13,     1] loss: 3152.978
[14,     1] loss: 3145.908
[15,     1] loss: 3139.848
[16,     1] loss: 3130.579
[17,     1] loss: 3122.005
[18,     1] loss: 3075.426
[19,     1] loss: 3034.108
[20,     1] loss: 2954.160
[21,     1] loss: 2828.627
[22,     1] loss: 2788.025
[23,     1] loss: 2702.388
[24,     1] loss: 2684.690
[25,     1] loss: 2570.746
[26,     1] loss: 2534.876
[27,     1] loss: 2537.616
[28,     1] loss: 2586.688
[29,     1] loss: 2418.782
[30,     1] loss: 2591.499
[31,     1] loss: 2432.729
[32,     1] loss: 2474.155
[33,     1] loss: 2434.032
[34,     1] loss: 2403.697
[35,     1] loss: 2266.430
[36,     1] loss: 2298.578
[37,     1] loss: 2177.647
[38,     1] loss: 2350.563
[39,     1] loss: 2129.761
[40,     1] loss: 2267.007
[41,     1] loss: 2118.200
[42,     1] loss: 1970.296
Early stopping applied (best metric=1.050750970840454)
Finished Training
Total time taken: 5.682119131088257
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3171.826
[2,     1] loss: 3196.480
[3,     1] loss: 3152.521
[4,     1] loss: 3167.596
[5,     1] loss: 3208.611
[6,     1] loss: 3168.004
[7,     1] loss: 3173.744
[8,     1] loss: 3168.143
[9,     1] loss: 3173.389
[10,     1] loss: 3171.023
[11,     1] loss: 3166.323
[12,     1] loss: 3162.458
[13,     1] loss: 3161.864
[14,     1] loss: 3161.159
[15,     1] loss: 3170.728
[16,     1] loss: 3122.316
[17,     1] loss: 3118.385
[18,     1] loss: 3078.225
[19,     1] loss: 3041.799
[20,     1] loss: 2950.904
[21,     1] loss: 2915.005
[22,     1] loss: 2750.941
[23,     1] loss: 2702.029
[24,     1] loss: 2776.693
[25,     1] loss: 2701.216
[26,     1] loss: 2926.504
[27,     1] loss: 2555.079
[28,     1] loss: 2458.156
[29,     1] loss: 2555.795
[30,     1] loss: 2565.168
[31,     1] loss: 2590.542
[32,     1] loss: 2511.670
[33,     1] loss: 2457.957
[34,     1] loss: 2355.435
[35,     1] loss: 2322.356
[36,     1] loss: 2385.557
[37,     1] loss: 2311.708
[38,     1] loss: 2325.330
[39,     1] loss: 2093.020
[40,     1] loss: 2189.662
[41,     1] loss: 1988.073
[42,     1] loss: 2048.364
[43,     1] loss: 2042.893
[44,     1] loss: 2014.805
[45,     1] loss: 2027.486
[46,     1] loss: 2013.572
[47,     1] loss: 1760.430
[48,     1] loss: 2113.341
[49,     1] loss: 1937.808
[50,     1] loss: 1929.246
[51,     1] loss: 2081.358
[52,     1] loss: 1596.226
Early stopping applied (best metric=0.9552040100097656)
Finished Training
Total time taken: 8.53318190574646
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3188.063
[2,     1] loss: 3174.778
[3,     1] loss: 3175.277
[4,     1] loss: 3211.602
[5,     1] loss: 3168.950
[6,     1] loss: 3178.979
[7,     1] loss: 3186.084
[8,     1] loss: 3177.142
[9,     1] loss: 3171.566
[10,     1] loss: 3169.063
[11,     1] loss: 3183.746
[12,     1] loss: 3165.979
[13,     1] loss: 3155.271
[14,     1] loss: 3161.202
[15,     1] loss: 3147.765
[16,     1] loss: 3163.508
[17,     1] loss: 3148.753
[18,     1] loss: 3144.279
[19,     1] loss: 3129.815
[20,     1] loss: 3109.579
[21,     1] loss: 3073.245
[22,     1] loss: 2989.888
[23,     1] loss: 2950.907
[24,     1] loss: 2856.233
[25,     1] loss: 2730.718
[26,     1] loss: 2749.499
[27,     1] loss: 2856.340
[28,     1] loss: 2651.287
[29,     1] loss: 2636.593
[30,     1] loss: 2633.126
[31,     1] loss: 2708.857
[32,     1] loss: 2591.339
[33,     1] loss: 2698.628
[34,     1] loss: 2591.715
[35,     1] loss: 2493.109
[36,     1] loss: 2447.762
[37,     1] loss: 2284.416
[38,     1] loss: 2612.310
[39,     1] loss: 2575.083
[40,     1] loss: 2576.333
[41,     1] loss: 2129.485
[42,     1] loss: 2474.803
[43,     1] loss: 2293.600
[44,     1] loss: 2267.432
[45,     1] loss: 2147.544
[46,     1] loss: 2148.148
[47,     1] loss: 2197.842
[48,     1] loss: 2121.357
[49,     1] loss: 2097.057
[50,     1] loss: 1983.124
[51,     1] loss: 2065.268
Early stopping applied (best metric=0.8935884237289429)
Finished Training
Total time taken: 6.86614465713501
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3156.069
[2,     1] loss: 3200.024
[3,     1] loss: 3181.548
[4,     1] loss: 3162.736
[5,     1] loss: 3184.318
[6,     1] loss: 3164.081
[7,     1] loss: 3163.028
[8,     1] loss: 3155.034
[9,     1] loss: 3159.237
[10,     1] loss: 3171.305
[11,     1] loss: 3165.759
[12,     1] loss: 3171.769
[13,     1] loss: 3155.885
[14,     1] loss: 3153.255
[15,     1] loss: 3150.012
[16,     1] loss: 3120.945
[17,     1] loss: 3119.252
[18,     1] loss: 3136.717
[19,     1] loss: 3089.703
[20,     1] loss: 3037.848
[21,     1] loss: 2999.003
[22,     1] loss: 2969.849
[23,     1] loss: 2955.160
[24,     1] loss: 2823.291
[25,     1] loss: 2683.756
[26,     1] loss: 2758.597
[27,     1] loss: 2773.943
[28,     1] loss: 2537.587
[29,     1] loss: 2587.494
[30,     1] loss: 2468.640
[31,     1] loss: 2622.053
[32,     1] loss: 2406.466
[33,     1] loss: 2607.353
[34,     1] loss: 2542.402
[35,     1] loss: 2632.302
[36,     1] loss: 2424.946
[37,     1] loss: 2445.146
[38,     1] loss: 2231.889
[39,     1] loss: 2331.165
[40,     1] loss: 2362.242
[41,     1] loss: 2179.742
[42,     1] loss: 2335.118
[43,     1] loss: 1917.072
[44,     1] loss: 2032.805
[45,     1] loss: 2396.350
[46,     1] loss: 2164.170
[47,     1] loss: 2401.543
[48,     1] loss: 1858.554
[49,     1] loss: 2141.893
[50,     1] loss: 1912.944
[51,     1] loss: 2100.568
[52,     1] loss: 2507.645
[53,     1] loss: 2075.891
[54,     1] loss: 2016.738
[55,     1] loss: 2198.081
[56,     1] loss: 2083.017
[57,     1] loss: 1985.725
[58,     1] loss: 1847.996
[59,     1] loss: 1809.406
[60,     1] loss: 1869.940
Early stopping applied (best metric=0.8826636672019958)
Finished Training
Total time taken: 9.789206743240356
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3168.821
[2,     1] loss: 3160.248
[3,     1] loss: 3185.912
[4,     1] loss: 3174.754
[5,     1] loss: 3157.808
[6,     1] loss: 3173.300
[7,     1] loss: 3173.044
[8,     1] loss: 3164.412
[9,     1] loss: 3169.613
[10,     1] loss: 3163.775
[11,     1] loss: 3163.689
[12,     1] loss: 3160.097
[13,     1] loss: 3158.681
[14,     1] loss: 3149.839
[15,     1] loss: 3141.906
[16,     1] loss: 3122.925
[17,     1] loss: 3080.078
[18,     1] loss: 3016.556
[19,     1] loss: 2989.617
[20,     1] loss: 2917.220
[21,     1] loss: 2836.047
[22,     1] loss: 2840.577
[23,     1] loss: 2879.124
[24,     1] loss: 2644.044
[25,     1] loss: 2789.973
[26,     1] loss: 2668.564
[27,     1] loss: 2603.945
[28,     1] loss: 2702.434
[29,     1] loss: 2443.703
[30,     1] loss: 2602.892
[31,     1] loss: 2561.600
[32,     1] loss: 2357.762
[33,     1] loss: 2502.822
[34,     1] loss: 2510.954
[35,     1] loss: 2356.620
[36,     1] loss: 2427.303
[37,     1] loss: 2269.583
[38,     1] loss: 2345.116
[39,     1] loss: 2430.471
[40,     1] loss: 2363.769
[41,     1] loss: 2195.811
[42,     1] loss: 2310.934
[43,     1] loss: 2258.317
[44,     1] loss: 2256.452
[45,     1] loss: 1992.589
Early stopping applied (best metric=0.8296478986740112)
Finished Training
Total time taken: 7.442156791687012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3159.697
[2,     1] loss: 3203.373
[3,     1] loss: 3205.458
[4,     1] loss: 3170.087
[5,     1] loss: 3168.522
[6,     1] loss: 3174.069
[7,     1] loss: 3178.457
[8,     1] loss: 3173.641
[9,     1] loss: 3173.716
[10,     1] loss: 3165.913
[11,     1] loss: 3163.485
[12,     1] loss: 3176.770
[13,     1] loss: 3168.885
[14,     1] loss: 3168.091
[15,     1] loss: 3166.070
[16,     1] loss: 3161.552
[17,     1] loss: 3174.171
[18,     1] loss: 3152.556
[19,     1] loss: 3144.512
[20,     1] loss: 3163.536
[21,     1] loss: 3152.317
[22,     1] loss: 3136.138
[23,     1] loss: 3132.316
[24,     1] loss: 3116.663
[25,     1] loss: 3129.006
[26,     1] loss: 3060.581
[27,     1] loss: 3030.712
[28,     1] loss: 2987.286
[29,     1] loss: 2963.446
[30,     1] loss: 2847.128
[31,     1] loss: 2752.270
[32,     1] loss: 2628.916
[33,     1] loss: 2784.969
[34,     1] loss: 2644.698
[35,     1] loss: 2636.677
[36,     1] loss: 2753.169
[37,     1] loss: 2502.105
[38,     1] loss: 2515.273
[39,     1] loss: 2466.682
[40,     1] loss: 2389.411
[41,     1] loss: 2626.360
[42,     1] loss: 2302.115
[43,     1] loss: 2329.730
[44,     1] loss: 2298.046
[45,     1] loss: 2274.396
[46,     1] loss: 2137.475
[47,     1] loss: 2219.638
[48,     1] loss: 2245.932
[49,     1] loss: 2423.303
[50,     1] loss: 2070.303
[51,     1] loss: 2089.447
[52,     1] loss: 1905.926
[53,     1] loss: 2290.401
[54,     1] loss: 1994.484
[55,     1] loss: 2177.099
Early stopping applied (best metric=0.8102460503578186)
Finished Training
Total time taken: 7.763163805007935
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3185.878
[2,     1] loss: 3170.038
[3,     1] loss: 3153.976
[4,     1] loss: 3191.553
[5,     1] loss: 3172.056
[6,     1] loss: 3169.977
[7,     1] loss: 3175.019
[8,     1] loss: 3174.045
[9,     1] loss: 3168.415
[10,     1] loss: 3169.877
[11,     1] loss: 3176.126
[12,     1] loss: 3167.336
[13,     1] loss: 3167.367
[14,     1] loss: 3166.295
[15,     1] loss: 3166.015
[16,     1] loss: 3158.047
[17,     1] loss: 3150.539
[18,     1] loss: 3153.829
[19,     1] loss: 3146.580
[20,     1] loss: 3132.839
[21,     1] loss: 3114.544
[22,     1] loss: 3104.529
[23,     1] loss: 3046.062
[24,     1] loss: 2998.846
[25,     1] loss: 2913.524
[26,     1] loss: 2863.444
[27,     1] loss: 2788.440
[28,     1] loss: 2747.458
[29,     1] loss: 2550.649
[30,     1] loss: 2790.869
[31,     1] loss: 2520.378
[32,     1] loss: 2647.480
[33,     1] loss: 2574.693
[34,     1] loss: 2801.673
[35,     1] loss: 2642.063
[36,     1] loss: 2534.318
[37,     1] loss: 2617.416
[38,     1] loss: 2625.925
[39,     1] loss: 2439.545
[40,     1] loss: 2604.044
[41,     1] loss: 2380.335
[42,     1] loss: 2586.419
[43,     1] loss: 2392.409
[44,     1] loss: 2442.042
[45,     1] loss: 2569.744
[46,     1] loss: 2272.186
[47,     1] loss: 2504.462
[48,     1] loss: 2438.133
[49,     1] loss: 2374.308
[50,     1] loss: 2342.645
[51,     1] loss: 2435.025
[52,     1] loss: 2112.863
[53,     1] loss: 2080.905
[54,     1] loss: 2132.964
[55,     1] loss: 2061.570
[56,     1] loss: 2386.609
[57,     1] loss: 1967.209
[58,     1] loss: 2012.668
[59,     1] loss: 2047.588
[60,     1] loss: 1848.143
[61,     1] loss: 1954.326
[62,     1] loss: 1755.210
[63,     1] loss: 1746.636
[64,     1] loss: 1683.566
[65,     1] loss: 1879.372
[66,     1] loss: 1712.829
[67,     1] loss: 1736.769
[68,     1] loss: 1824.872
[69,     1] loss: 1706.458
[70,     1] loss: 1629.045
[71,     1] loss: 1552.629
Early stopping applied (best metric=0.8335893154144287)
Finished Training
Total time taken: 9.899210214614868
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3176.570
[2,     1] loss: 3162.806
[3,     1] loss: 3168.250
[4,     1] loss: 3178.764
[5,     1] loss: 3173.672
[6,     1] loss: 3178.832
[7,     1] loss: 3161.329
[8,     1] loss: 3167.842
[9,     1] loss: 3163.393
[10,     1] loss: 3179.182
[11,     1] loss: 3159.058
[12,     1] loss: 3134.322
[13,     1] loss: 3158.461
[14,     1] loss: 3155.929
[15,     1] loss: 3110.244
[16,     1] loss: 3108.547
[17,     1] loss: 3120.825
[18,     1] loss: 3014.902
[19,     1] loss: 3011.830
[20,     1] loss: 2896.758
[21,     1] loss: 2822.834
[22,     1] loss: 2788.264
[23,     1] loss: 2846.011
[24,     1] loss: 2632.085
[25,     1] loss: 2729.583
[26,     1] loss: 2672.921
[27,     1] loss: 2638.274
[28,     1] loss: 2700.400
[29,     1] loss: 2665.584
[30,     1] loss: 2400.815
[31,     1] loss: 2507.959
[32,     1] loss: 2481.625
[33,     1] loss: 2369.411
[34,     1] loss: 2167.119
[35,     1] loss: 2319.921
[36,     1] loss: 2282.813
[37,     1] loss: 2335.690
[38,     1] loss: 2414.151
[39,     1] loss: 2132.691
[40,     1] loss: 2128.272
[41,     1] loss: 2261.285
[42,     1] loss: 2272.146
[43,     1] loss: 2209.938
[44,     1] loss: 2183.528
[45,     1] loss: 1930.048
[46,     1] loss: 2065.084
[47,     1] loss: 2056.778
[48,     1] loss: 1744.617
[49,     1] loss: 1756.518
Early stopping applied (best metric=0.8782256841659546)
Finished Training
Total time taken: 7.554161310195923
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3191.063
[2,     1] loss: 3195.469
[3,     1] loss: 3163.125
[4,     1] loss: 3171.433
[5,     1] loss: 3171.063
[6,     1] loss: 3161.238
[7,     1] loss: 3156.327
[8,     1] loss: 3162.786
[9,     1] loss: 3152.435
[10,     1] loss: 3177.052
[11,     1] loss: 3154.028
[12,     1] loss: 3151.052
[13,     1] loss: 3154.547
[14,     1] loss: 3136.341
[15,     1] loss: 3105.564
[16,     1] loss: 3081.770
[17,     1] loss: 3051.839
[18,     1] loss: 2902.520
[19,     1] loss: 2913.612
[20,     1] loss: 2834.964
[21,     1] loss: 2718.212
[22,     1] loss: 2776.808
[23,     1] loss: 2663.228
[24,     1] loss: 2720.130
[25,     1] loss: 2585.173
[26,     1] loss: 2626.600
[27,     1] loss: 2740.568
[28,     1] loss: 2661.376
[29,     1] loss: 2642.296
[30,     1] loss: 2620.316
[31,     1] loss: 2581.964
[32,     1] loss: 2595.233
[33,     1] loss: 2551.811
[34,     1] loss: 2454.628
[35,     1] loss: 2585.087
[36,     1] loss: 2459.580
[37,     1] loss: 2544.488
[38,     1] loss: 2506.602
[39,     1] loss: 2437.046
[40,     1] loss: 2266.767
[41,     1] loss: 2258.644
[42,     1] loss: 2387.102
[43,     1] loss: 2807.722
[44,     1] loss: 2236.721
[45,     1] loss: 2735.324
[46,     1] loss: 2126.148
[47,     1] loss: 2254.032
[48,     1] loss: 2199.047
[49,     1] loss: 2161.430
[50,     1] loss: 2117.574
[51,     1] loss: 2056.178
[52,     1] loss: 2089.657
[53,     1] loss: 1944.977
[54,     1] loss: 1908.516
[55,     1] loss: 1993.274
[56,     1] loss: 2109.575
[57,     1] loss: 1930.105
[58,     1] loss: 1863.804
[59,     1] loss: 2203.181
[60,     1] loss: 1750.992
[61,     1] loss: 1867.774
[62,     1] loss: 1648.240
[63,     1] loss: 2007.555
[64,     1] loss: 1565.038
[65,     1] loss: 1832.414
Early stopping applied (best metric=0.6864396333694458)
Finished Training
Total time taken: 10.594223976135254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3203.465
[2,     1] loss: 3176.226
[3,     1] loss: 3190.548
[4,     1] loss: 3160.172
[5,     1] loss: 3171.774
[6,     1] loss: 3176.616
[7,     1] loss: 3176.754
[8,     1] loss: 3173.029
[9,     1] loss: 3178.125
[10,     1] loss: 3165.384
[11,     1] loss: 3165.833
[12,     1] loss: 3174.159
[13,     1] loss: 3181.596
[14,     1] loss: 3168.062
[15,     1] loss: 3169.158
[16,     1] loss: 3172.794
[17,     1] loss: 3164.865
[18,     1] loss: 3177.027
[19,     1] loss: 3166.107
[20,     1] loss: 3167.142
[21,     1] loss: 3165.714
[22,     1] loss: 3165.563
[23,     1] loss: 3168.644
[24,     1] loss: 3164.206
[25,     1] loss: 3165.633
[26,     1] loss: 3162.447
[27,     1] loss: 3176.387
[28,     1] loss: 3180.011
Early stopping applied (best metric=1.0889079570770264)
Finished Training
Total time taken: 3.794079065322876
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3181.650
[2,     1] loss: 3186.934
[3,     1] loss: 3173.560
[4,     1] loss: 3168.267
[5,     1] loss: 3169.997
[6,     1] loss: 3158.927
[7,     1] loss: 3152.742
[8,     1] loss: 3169.353
[9,     1] loss: 3159.888
[10,     1] loss: 3163.202
[11,     1] loss: 3157.375
[12,     1] loss: 3141.624
[13,     1] loss: 3135.668
[14,     1] loss: 3114.698
[15,     1] loss: 3096.076
[16,     1] loss: 3028.594
[17,     1] loss: 2980.011
[18,     1] loss: 2897.501
[19,     1] loss: 2798.331
[20,     1] loss: 2752.956
[21,     1] loss: 2496.549
[22,     1] loss: 2736.729
[23,     1] loss: 2488.824
[24,     1] loss: 2567.056
[25,     1] loss: 2653.688
[26,     1] loss: 2534.544
[27,     1] loss: 2344.824
[28,     1] loss: 2417.219
[29,     1] loss: 2276.881
[30,     1] loss: 2228.361
[31,     1] loss: 2219.525
[32,     1] loss: 2376.193
[33,     1] loss: 2159.207
[34,     1] loss: 2182.721
[35,     1] loss: 2102.739
[36,     1] loss: 2079.596
[37,     1] loss: 1936.556
Early stopping applied (best metric=1.01847505569458)
Finished Training
Total time taken: 6.029127597808838
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3156.848
[2,     1] loss: 3235.298
[3,     1] loss: 3207.374
[4,     1] loss: 3175.943
[5,     1] loss: 3171.635
[6,     1] loss: 3166.453
[7,     1] loss: 3162.359
[8,     1] loss: 3155.028
[9,     1] loss: 3163.201
[10,     1] loss: 3176.439
[11,     1] loss: 3170.945
[12,     1] loss: 3169.140
[13,     1] loss: 3169.941
[14,     1] loss: 3171.174
[15,     1] loss: 3164.252
[16,     1] loss: 3173.536
[17,     1] loss: 3164.951
[18,     1] loss: 3165.462
[19,     1] loss: 3156.674
[20,     1] loss: 3154.793
[21,     1] loss: 3154.249
[22,     1] loss: 3149.495
[23,     1] loss: 3127.269
[24,     1] loss: 3128.250
[25,     1] loss: 3103.107
[26,     1] loss: 3059.433
[27,     1] loss: 3003.210
[28,     1] loss: 2903.480
[29,     1] loss: 2870.700
[30,     1] loss: 2869.428
[31,     1] loss: 2794.544
[32,     1] loss: 2871.975
[33,     1] loss: 2838.869
[34,     1] loss: 2725.378
[35,     1] loss: 2654.191
[36,     1] loss: 2794.623
[37,     1] loss: 2688.095
[38,     1] loss: 2578.980
[39,     1] loss: 2641.794
[40,     1] loss: 2654.623
[41,     1] loss: 2531.509
[42,     1] loss: 2592.171
[43,     1] loss: 2294.558
[44,     1] loss: 2416.018
[45,     1] loss: 2574.261
[46,     1] loss: 2401.490
[47,     1] loss: 2606.989
[48,     1] loss: 2477.271
[49,     1] loss: 2471.671
[50,     1] loss: 2456.934
[51,     1] loss: 2425.663
[52,     1] loss: 2200.968
[53,     1] loss: 2270.863
[54,     1] loss: 2273.472
[55,     1] loss: 2105.803
[56,     1] loss: 2031.855
[57,     1] loss: 1875.459
[58,     1] loss: 1974.491
[59,     1] loss: 2002.642
[60,     1] loss: 1997.238
[61,     1] loss: 2119.324
Early stopping applied (best metric=0.6857165098190308)
Finished Training
Total time taken: 8.17017388343811
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3202.679
[2,     1] loss: 3168.505
[3,     1] loss: 3209.841
[4,     1] loss: 3170.198
[5,     1] loss: 3169.295
[6,     1] loss: 3177.239
[7,     1] loss: 3171.181
[8,     1] loss: 3166.104
[9,     1] loss: 3168.820
[10,     1] loss: 3176.079
[11,     1] loss: 3164.018
[12,     1] loss: 3170.867
[13,     1] loss: 3174.428
[14,     1] loss: 3171.772
[15,     1] loss: 3166.280
[16,     1] loss: 3171.166
[17,     1] loss: 3169.231
[18,     1] loss: 3176.372
[19,     1] loss: 3165.946
[20,     1] loss: 3172.130
[21,     1] loss: 3167.637
[22,     1] loss: 3172.062
[23,     1] loss: 3165.177
[24,     1] loss: 3172.267
[25,     1] loss: 3167.214
[26,     1] loss: 3160.283
[27,     1] loss: 3157.693
[28,     1] loss: 3158.047
[29,     1] loss: 3152.085
[30,     1] loss: 3132.835
[31,     1] loss: 3109.821
[32,     1] loss: 3101.931
[33,     1] loss: 3050.694
[34,     1] loss: 2961.085
[35,     1] loss: 2924.704
[36,     1] loss: 2999.417
[37,     1] loss: 2753.499
[38,     1] loss: 2679.359
[39,     1] loss: 2661.795
[40,     1] loss: 2626.044
[41,     1] loss: 2928.780
[42,     1] loss: 2700.435
[43,     1] loss: 2781.262
[44,     1] loss: 2673.818
[45,     1] loss: 2696.228
[46,     1] loss: 2695.483
[47,     1] loss: 2536.071
[48,     1] loss: 2693.555
[49,     1] loss: 2846.648
[50,     1] loss: 2640.875
[51,     1] loss: 2556.065
[52,     1] loss: 2594.940
[53,     1] loss: 2688.186
[54,     1] loss: 2553.238
[55,     1] loss: 2544.881
[56,     1] loss: 2497.419
[57,     1] loss: 2344.319
[58,     1] loss: 2339.456
[59,     1] loss: 2341.050
[60,     1] loss: 2295.746
[61,     1] loss: 2347.638
[62,     1] loss: 2079.123
[63,     1] loss: 2087.023
[64,     1] loss: 2043.288
[65,     1] loss: 2446.543
[66,     1] loss: 2112.790
[67,     1] loss: 2448.931
[68,     1] loss: 1879.881
[69,     1] loss: 2259.960
[70,     1] loss: 2086.638
[71,     1] loss: 2463.582
[72,     1] loss: 1947.891
[73,     1] loss: 2253.015
[74,     1] loss: 2046.776
[75,     1] loss: 1949.073
[76,     1] loss: 1953.768
[77,     1] loss: 1861.005
[78,     1] loss: 1824.150
[79,     1] loss: 1710.004
[80,     1] loss: 1793.802
[81,     1] loss: 1669.926
[82,     1] loss: 1645.024
[83,     1] loss: 1514.089
[84,     1] loss: 1457.453
[85,     1] loss: 1520.550
[86,     1] loss: 1650.470
[87,     1] loss: 1875.703
[88,     1] loss: 1422.465
[89,     1] loss: 1816.750
Early stopping applied (best metric=0.8683180809020996)
Finished Training
Total time taken: 14.478307008743286
{'Hydroxylation-K Validation Accuracy': 0.7102541371158393, 'Hydroxylation-K Validation Sensitivity': 0.7644444444444444, 'Hydroxylation-K Validation Specificity': 0.6964912280701754, 'Hydroxylation-K Validation Precision': 0.4229663158267277, 'Hydroxylation-K AUC ROC': 0.8041130604288499, 'Hydroxylation-K AUC PR': 0.5675558408465274, 'Hydroxylation-K MCC': 0.3979571665370126, 'Hydroxylation-K F1': 0.5326937231317621, 'Validation Loss (Hydroxylation-K)': 0.43783812125523885, 'Hydroxylation-P Validation Accuracy': 0.6847072568228347, 'Hydroxylation-P Validation Sensitivity': 0.7408465608465609, 'Hydroxylation-P Validation Specificity': 0.6728041298817896, 'Hydroxylation-P Validation Precision': 0.35385399464727724, 'Hydroxylation-P AUC ROC': 0.7728540650802359, 'Hydroxylation-P AUC PR': 0.47176735317830065, 'Hydroxylation-P MCC': 0.3372515012546053, 'Hydroxylation-P F1': 0.46868495034858376, 'Validation Loss (Hydroxylation-P)': 0.4470810612042745, 'Validation Loss (total)': 0.8849191705385844, 'TimeToTrain': 8.302175490061442}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010848916714256613,
 'learning_rate_Hydroxylation-K': 0.008990783987263338,
 'learning_rate_Hydroxylation-P': 0.008789222942701427,
 'log_base': 2.4493027842207797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2375700294,
 'sample_weights': [9.78239700854893, 1.220258864490254],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.230403653420751,
 'weight_decay_Hydroxylation-K': 4.8253017775478195,
 'weight_decay_Hydroxylation-P': 5.095047096283252}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.858
[2,     1] loss: 1308.693
[3,     1] loss: 1299.866
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002692769934408712,
 'learning_rate_Hydroxylation-K': 0.007741411560482922,
 'learning_rate_Hydroxylation-P': 0.0030912439929893084,
 'log_base': 2.039998821708342,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3059202224,
 'sample_weights': [1.8636267018994288, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.7843846839167075,
 'weight_decay_Hydroxylation-K': 3.078984168238013,
 'weight_decay_Hydroxylation-P': 8.157569024085104}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.060
[2,     1] loss: 1404.194
[3,     1] loss: 1401.961
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014819254735192364,
 'learning_rate_Hydroxylation-K': 0.00978614871554877,
 'learning_rate_Hydroxylation-P': 0.005419330005908034,
 'log_base': 1.7598372779028157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1390082886,
 'sample_weights': [2.3416017250066368, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.304444803124834,
 'weight_decay_Hydroxylation-K': 2.287557643955771,
 'weight_decay_Hydroxylation-P': 7.084134358298901}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1535.021
[2,     1] loss: 1532.035
[3,     1] loss: 1531.111
[4,     1] loss: 1529.926
[5,     1] loss: 1527.358
[6,     1] loss: 1528.271
[7,     1] loss: 1525.150
[8,     1] loss: 1523.377
[9,     1] loss: 1524.081
[10,     1] loss: 1516.419
[11,     1] loss: 1518.094
[12,     1] loss: 1505.823
[13,     1] loss: 1491.821
[14,     1] loss: 1465.968
[15,     1] loss: 1448.104
[16,     1] loss: 1427.439
[17,     1] loss: 1395.789
[18,     1] loss: 1400.378
[19,     1] loss: 1372.239
[20,     1] loss: 1346.017
[21,     1] loss: 1355.887
[22,     1] loss: 1306.104
[23,     1] loss: 1290.010
[24,     1] loss: 1298.883
[25,     1] loss: 1320.451
[26,     1] loss: 1286.240
[27,     1] loss: 1266.839
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019672930972803133,
 'learning_rate_Hydroxylation-K': 0.0010668546200344603,
 'learning_rate_Hydroxylation-P': 0.006919747537552197,
 'log_base': 2.52132017875611,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3871374916,
 'sample_weights': [2.953609502300103, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.255042619499445,
 'weight_decay_Hydroxylation-K': 8.344938519010098,
 'weight_decay_Hydroxylation-P': 2.1028228295099396}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.427
[2,     1] loss: 1295.387
[3,     1] loss: 1290.313
[4,     1] loss: 1286.748
[5,     1] loss: 1288.865
[6,     1] loss: 1287.564
[7,     1] loss: 1283.649
[8,     1] loss: 1281.651
[9,     1] loss: 1274.208
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012500565666376101,
 'learning_rate_Hydroxylation-K': 0.008627829924660591,
 'learning_rate_Hydroxylation-P': 0.0038593279149803216,
 'log_base': 2.4749990850833994,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3278599950,
 'sample_weights': [1.8052275924537176, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.809417838886083,
 'weight_decay_Hydroxylation-K': 1.111949726051042,
 'weight_decay_Hydroxylation-P': 3.335251382786261}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.776
[2,     1] loss: 1298.767
[3,     1] loss: 1295.632
[4,     1] loss: 1293.515
[5,     1] loss: 1297.564
[6,     1] loss: 1292.521
[7,     1] loss: 1293.926
[8,     1] loss: 1294.068
[9,     1] loss: 1291.159
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004415406603608397,
 'learning_rate_Hydroxylation-K': 0.0018501612963340425,
 'learning_rate_Hydroxylation-P': 0.005088178972247824,
 'log_base': 2.0622784320323815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1169706031,
 'sample_weights': [1.8421644364287262, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.917854250492215,
 'weight_decay_Hydroxylation-K': 6.474193214348899,
 'weight_decay_Hydroxylation-P': 0.25638915931508155}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.480
[2,     1] loss: 1392.266
[3,     1] loss: 1395.289
[4,     1] loss: 1395.970
[5,     1] loss: 1390.012
[6,     1] loss: 1389.969
[7,     1] loss: 1385.476
[8,     1] loss: 1383.280
[9,     1] loss: 1379.367
[10,     1] loss: 1361.408
[11,     1] loss: 1329.695
[12,     1] loss: 1303.692
[13,     1] loss: 1262.609
[14,     1] loss: 1201.490
[15,     1] loss: 1218.579
[16,     1] loss: 1221.675
[17,     1] loss: 1140.518
[18,     1] loss: 1160.868
[19,     1] loss: 1165.039
[20,     1] loss: 1119.962
[21,     1] loss: 1127.678
[22,     1] loss: 1119.288
[23,     1] loss: 1156.597
[24,     1] loss: 1151.593
[25,     1] loss: 1072.358
[26,     1] loss: 1122.251
[27,     1] loss: 1055.825
[28,     1] loss: 1094.970
[29,     1] loss: 1059.611
[30,     1] loss: 1067.875
[31,     1] loss: 1003.172
[32,     1] loss: 1035.318
[33,     1] loss: 993.175
[34,     1] loss: 1016.611
[35,     1] loss: 994.175
[36,     1] loss: 910.451
[37,     1] loss: 1026.052
[38,     1] loss: 1004.390
[39,     1] loss: 987.019
[40,     1] loss: 985.766
[41,     1] loss: 934.852
[42,     1] loss: 928.793
[43,     1] loss: 862.406
[44,     1] loss: 935.243
[45,     1] loss: 893.118
[46,     1] loss: 844.025
[47,     1] loss: 967.731
[48,     1] loss: 986.015
[49,     1] loss: 805.823
[50,     1] loss: 961.241
[51,     1] loss: 815.862
[52,     1] loss: 907.100
[53,     1] loss: 782.618
[54,     1] loss: 899.423
[55,     1] loss: 771.396
[56,     1] loss: 844.951
[57,     1] loss: 736.364
[58,     1] loss: 838.832
[59,     1] loss: 704.650
[60,     1] loss: 852.403
[61,     1] loss: 704.615
Early stopping applied (best metric=0.771396279335022)
Finished Training
Total time taken: 8.350175380706787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.581
[2,     1] loss: 1395.943
[3,     1] loss: 1397.213
[4,     1] loss: 1394.454
[5,     1] loss: 1394.490
[6,     1] loss: 1393.245
[7,     1] loss: 1391.587
[8,     1] loss: 1390.510
[9,     1] loss: 1391.062
[10,     1] loss: 1384.719
[11,     1] loss: 1383.205
[12,     1] loss: 1369.546
[13,     1] loss: 1354.977
[14,     1] loss: 1329.682
[15,     1] loss: 1295.697
[16,     1] loss: 1253.259
[17,     1] loss: 1215.023
[18,     1] loss: 1163.029
[19,     1] loss: 1187.859
[20,     1] loss: 1137.878
[21,     1] loss: 1191.114
[22,     1] loss: 1117.007
[23,     1] loss: 1103.412
[24,     1] loss: 1131.977
[25,     1] loss: 1134.809
[26,     1] loss: 1072.538
[27,     1] loss: 1093.722
[28,     1] loss: 1087.307
[29,     1] loss: 1084.731
[30,     1] loss: 1057.522
[31,     1] loss: 1041.075
[32,     1] loss: 1039.257
[33,     1] loss: 975.938
[34,     1] loss: 1008.615
[35,     1] loss: 964.102
[36,     1] loss: 939.119
[37,     1] loss: 1028.719
[38,     1] loss: 952.762
[39,     1] loss: 1021.193
[40,     1] loss: 926.343
[41,     1] loss: 1018.974
[42,     1] loss: 935.576
[43,     1] loss: 936.069
[44,     1] loss: 908.097
[45,     1] loss: 903.043
[46,     1] loss: 887.413
[47,     1] loss: 889.416
[48,     1] loss: 880.841
[49,     1] loss: 797.583
[50,     1] loss: 920.944
Early stopping applied (best metric=0.9312140941619873)
Finished Training
Total time taken: 8.020168781280518
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.918
[2,     1] loss: 1395.722
[3,     1] loss: 1392.931
[4,     1] loss: 1394.716
[5,     1] loss: 1391.889
[6,     1] loss: 1390.732
[7,     1] loss: 1390.684
[8,     1] loss: 1377.496
[9,     1] loss: 1364.940
[10,     1] loss: 1343.383
[11,     1] loss: 1297.449
[12,     1] loss: 1264.281
[13,     1] loss: 1211.548
[14,     1] loss: 1158.412
[15,     1] loss: 1169.535
[16,     1] loss: 1220.702
[17,     1] loss: 1160.341
[18,     1] loss: 1231.073
[19,     1] loss: 1127.326
[20,     1] loss: 1176.366
[21,     1] loss: 1143.170
[22,     1] loss: 1141.303
[23,     1] loss: 1165.643
[24,     1] loss: 1117.734
[25,     1] loss: 1105.647
[26,     1] loss: 1081.130
[27,     1] loss: 1071.731
[28,     1] loss: 1109.325
[29,     1] loss: 1062.799
[30,     1] loss: 1025.432
[31,     1] loss: 1035.150
[32,     1] loss: 998.809
[33,     1] loss: 980.527
[34,     1] loss: 1011.180
[35,     1] loss: 1005.356
[36,     1] loss: 941.290
[37,     1] loss: 976.249
[38,     1] loss: 943.998
[39,     1] loss: 922.304
[40,     1] loss: 912.540
[41,     1] loss: 992.406
[42,     1] loss: 959.932
[43,     1] loss: 937.765
[44,     1] loss: 870.571
[45,     1] loss: 915.687
[46,     1] loss: 800.311
[47,     1] loss: 810.336
[48,     1] loss: 862.745
[49,     1] loss: 798.429
[50,     1] loss: 788.341
[51,     1] loss: 736.423
[52,     1] loss: 774.092
[53,     1] loss: 971.723
[54,     1] loss: 1241.000
[55,     1] loss: 802.045
[56,     1] loss: 1032.074
[57,     1] loss: 891.314
Early stopping applied (best metric=0.8885095119476318)
Finished Training
Total time taken: 9.320196628570557
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.805
[2,     1] loss: 1401.601
[3,     1] loss: 1399.299
[4,     1] loss: 1397.811
[5,     1] loss: 1393.570
[6,     1] loss: 1389.839
[7,     1] loss: 1390.123
[8,     1] loss: 1387.536
[9,     1] loss: 1389.931
[10,     1] loss: 1384.324
[11,     1] loss: 1379.080
[12,     1] loss: 1367.584
[13,     1] loss: 1355.925
[14,     1] loss: 1333.195
[15,     1] loss: 1299.375
[16,     1] loss: 1263.697
[17,     1] loss: 1244.275
[18,     1] loss: 1219.682
[19,     1] loss: 1194.278
[20,     1] loss: 1121.655
[21,     1] loss: 1148.888
[22,     1] loss: 1162.882
[23,     1] loss: 1180.813
[24,     1] loss: 1089.068
[25,     1] loss: 1161.746
[26,     1] loss: 1115.384
[27,     1] loss: 1131.135
[28,     1] loss: 1112.488
[29,     1] loss: 1088.707
[30,     1] loss: 1051.490
[31,     1] loss: 1031.045
[32,     1] loss: 1068.372
[33,     1] loss: 1041.818
[34,     1] loss: 1015.927
[35,     1] loss: 1000.676
[36,     1] loss: 1035.290
[37,     1] loss: 985.583
[38,     1] loss: 1016.359
[39,     1] loss: 999.576
[40,     1] loss: 1028.415
[41,     1] loss: 1011.946
[42,     1] loss: 1004.865
[43,     1] loss: 962.146
[44,     1] loss: 982.151
[45,     1] loss: 971.929
[46,     1] loss: 927.496
[47,     1] loss: 982.146
[48,     1] loss: 885.698
[49,     1] loss: 909.128
[50,     1] loss: 948.965
[51,     1] loss: 864.140
[52,     1] loss: 852.210
[53,     1] loss: 862.286
[54,     1] loss: 819.981
[55,     1] loss: 851.645
[56,     1] loss: 808.081
[57,     1] loss: 790.372
[58,     1] loss: 890.205
[59,     1] loss: 984.711
[60,     1] loss: 950.672
[61,     1] loss: 796.743
[62,     1] loss: 919.077
Early stopping applied (best metric=0.7874501943588257)
Finished Training
Total time taken: 10.199216604232788
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1396.674
[2,     1] loss: 1402.497
[3,     1] loss: 1398.134
[4,     1] loss: 1392.167
[5,     1] loss: 1395.264
[6,     1] loss: 1391.622
[7,     1] loss: 1390.252
[8,     1] loss: 1385.521
[9,     1] loss: 1375.571
[10,     1] loss: 1361.300
[11,     1] loss: 1337.461
[12,     1] loss: 1282.603
[13,     1] loss: 1270.546
[14,     1] loss: 1224.655
[15,     1] loss: 1189.375
[16,     1] loss: 1212.393
[17,     1] loss: 1203.646
[18,     1] loss: 1163.123
[19,     1] loss: 1160.803
[20,     1] loss: 1122.860
[21,     1] loss: 1156.097
[22,     1] loss: 1135.866
[23,     1] loss: 1099.571
[24,     1] loss: 1110.513
[25,     1] loss: 1063.456
[26,     1] loss: 1113.961
[27,     1] loss: 1086.286
[28,     1] loss: 1038.091
[29,     1] loss: 1010.747
[30,     1] loss: 1037.304
[31,     1] loss: 1055.984
[32,     1] loss: 1011.300
[33,     1] loss: 960.543
[34,     1] loss: 963.539
[35,     1] loss: 951.713
[36,     1] loss: 1019.314
[37,     1] loss: 974.265
[38,     1] loss: 963.559
[39,     1] loss: 964.405
[40,     1] loss: 945.059
[41,     1] loss: 941.053
[42,     1] loss: 943.624
[43,     1] loss: 902.227
[44,     1] loss: 893.990
[45,     1] loss: 934.285
[46,     1] loss: 788.649
[47,     1] loss: 864.880
[48,     1] loss: 869.400
[49,     1] loss: 833.208
[50,     1] loss: 853.782
[51,     1] loss: 782.031
[52,     1] loss: 886.692
[53,     1] loss: 763.390
[54,     1] loss: 762.513
[55,     1] loss: 744.643
[56,     1] loss: 821.908
[57,     1] loss: 995.404
[58,     1] loss: 841.618
[59,     1] loss: 822.648
[60,     1] loss: 802.018
[61,     1] loss: 843.846
[62,     1] loss: 748.967
[63,     1] loss: 814.077
[64,     1] loss: 729.498
[65,     1] loss: 776.884
[66,     1] loss: 647.864
[67,     1] loss: 707.745
[68,     1] loss: 747.530
[69,     1] loss: 636.474
[70,     1] loss: 811.158
[71,     1] loss: 719.956
[72,     1] loss: 609.890
[73,     1] loss: 631.016
[74,     1] loss: 619.526
[75,     1] loss: 649.839
[76,     1] loss: 554.191
[77,     1] loss: 573.721
[78,     1] loss: 556.729
[79,     1] loss: 572.081
[80,     1] loss: 635.028
[81,     1] loss: 743.600
[82,     1] loss: 578.219
Early stopping applied (best metric=0.7980959415435791)
Finished Training
Total time taken: 13.375281572341919
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.023
[2,     1] loss: 1393.181
[3,     1] loss: 1396.280
[4,     1] loss: 1394.885
[5,     1] loss: 1396.027
[6,     1] loss: 1397.742
[7,     1] loss: 1390.266
[8,     1] loss: 1386.986
[9,     1] loss: 1380.998
[10,     1] loss: 1372.664
[11,     1] loss: 1356.316
[12,     1] loss: 1328.319
[13,     1] loss: 1302.396
[14,     1] loss: 1255.794
[15,     1] loss: 1210.169
[16,     1] loss: 1162.690
[17,     1] loss: 1192.821
[18,     1] loss: 1158.667
[19,     1] loss: 1086.137
[20,     1] loss: 1147.360
[21,     1] loss: 1131.949
[22,     1] loss: 1114.659
[23,     1] loss: 1149.580
[24,     1] loss: 1137.801
[25,     1] loss: 1130.408
[26,     1] loss: 1108.548
[27,     1] loss: 1110.671
[28,     1] loss: 1070.489
[29,     1] loss: 1056.466
[30,     1] loss: 1050.414
[31,     1] loss: 1068.285
[32,     1] loss: 998.906
[33,     1] loss: 1064.472
[34,     1] loss: 1035.331
[35,     1] loss: 982.052
[36,     1] loss: 1011.871
[37,     1] loss: 964.489
[38,     1] loss: 996.356
[39,     1] loss: 936.454
[40,     1] loss: 945.335
[41,     1] loss: 999.764
[42,     1] loss: 922.503
[43,     1] loss: 925.051
[44,     1] loss: 903.765
[45,     1] loss: 876.982
[46,     1] loss: 936.352
[47,     1] loss: 891.845
[48,     1] loss: 925.001
[49,     1] loss: 877.492
[50,     1] loss: 864.094
[51,     1] loss: 857.599
[52,     1] loss: 849.149
[53,     1] loss: 798.423
[54,     1] loss: 804.227
[55,     1] loss: 784.441
[56,     1] loss: 755.878
[57,     1] loss: 753.618
[58,     1] loss: 905.778
[59,     1] loss: 913.114
Early stopping applied (best metric=0.7841342687606812)
Finished Training
Total time taken: 7.909169673919678
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.173
[2,     1] loss: 1406.759
[3,     1] loss: 1392.293
[4,     1] loss: 1395.283
[5,     1] loss: 1389.791
[6,     1] loss: 1394.587
[7,     1] loss: 1391.379
[8,     1] loss: 1390.836
[9,     1] loss: 1387.761
[10,     1] loss: 1381.669
[11,     1] loss: 1372.160
[12,     1] loss: 1363.603
[13,     1] loss: 1338.609
[14,     1] loss: 1295.723
[15,     1] loss: 1261.479
[16,     1] loss: 1219.903
[17,     1] loss: 1196.696
[18,     1] loss: 1171.802
[19,     1] loss: 1141.147
[20,     1] loss: 1164.335
[21,     1] loss: 1135.448
[22,     1] loss: 1172.097
[23,     1] loss: 1168.221
[24,     1] loss: 1137.603
[25,     1] loss: 1120.107
[26,     1] loss: 1151.481
[27,     1] loss: 1080.059
[28,     1] loss: 1065.589
[29,     1] loss: 1088.335
[30,     1] loss: 1049.248
[31,     1] loss: 1056.740
[32,     1] loss: 1062.406
[33,     1] loss: 1024.553
[34,     1] loss: 987.333
[35,     1] loss: 998.337
[36,     1] loss: 994.863
[37,     1] loss: 1006.507
[38,     1] loss: 984.733
[39,     1] loss: 997.233
[40,     1] loss: 957.905
[41,     1] loss: 975.928
[42,     1] loss: 1003.280
[43,     1] loss: 949.004
[44,     1] loss: 909.610
[45,     1] loss: 870.516
[46,     1] loss: 928.686
[47,     1] loss: 906.443
[48,     1] loss: 907.969
[49,     1] loss: 911.823
[50,     1] loss: 1007.746
[51,     1] loss: 985.767
[52,     1] loss: 841.359
[53,     1] loss: 1048.534
[54,     1] loss: 852.623
[55,     1] loss: 943.121
[56,     1] loss: 876.218
Early stopping applied (best metric=0.7446104288101196)
Finished Training
Total time taken: 9.21919298171997
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.875
[2,     1] loss: 1396.247
[3,     1] loss: 1389.435
[4,     1] loss: 1387.281
[5,     1] loss: 1394.024
[6,     1] loss: 1396.839
[7,     1] loss: 1389.222
[8,     1] loss: 1385.160
[9,     1] loss: 1364.412
[10,     1] loss: 1358.351
[11,     1] loss: 1326.969
[12,     1] loss: 1296.589
[13,     1] loss: 1231.334
[14,     1] loss: 1256.629
[15,     1] loss: 1212.656
[16,     1] loss: 1199.226
[17,     1] loss: 1183.205
[18,     1] loss: 1187.864
[19,     1] loss: 1179.287
[20,     1] loss: 1188.533
[21,     1] loss: 1189.681
[22,     1] loss: 1165.544
[23,     1] loss: 1144.123
[24,     1] loss: 1085.929
[25,     1] loss: 1164.399
[26,     1] loss: 1111.997
[27,     1] loss: 1112.380
[28,     1] loss: 1081.425
[29,     1] loss: 1100.194
[30,     1] loss: 1021.697
[31,     1] loss: 1096.065
[32,     1] loss: 1040.438
[33,     1] loss: 996.276
[34,     1] loss: 998.775
[35,     1] loss: 1018.987
[36,     1] loss: 973.948
[37,     1] loss: 964.935
[38,     1] loss: 1010.675
[39,     1] loss: 911.424
[40,     1] loss: 931.624
[41,     1] loss: 923.018
[42,     1] loss: 988.983
[43,     1] loss: 914.738
[44,     1] loss: 907.979
[45,     1] loss: 881.594
[46,     1] loss: 913.761
[47,     1] loss: 836.769
[48,     1] loss: 810.298
[49,     1] loss: 868.257
[50,     1] loss: 865.226
[51,     1] loss: 900.815
[52,     1] loss: 1053.530
Early stopping applied (best metric=0.7212040424346924)
Finished Training
Total time taken: 8.543179273605347
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.972
[2,     1] loss: 1411.948
[3,     1] loss: 1395.309
[4,     1] loss: 1398.410
[5,     1] loss: 1395.338
[6,     1] loss: 1397.230
[7,     1] loss: 1394.680
[8,     1] loss: 1389.128
[9,     1] loss: 1395.465
[10,     1] loss: 1392.780
[11,     1] loss: 1391.921
[12,     1] loss: 1389.001
[13,     1] loss: 1388.856
[14,     1] loss: 1385.993
[15,     1] loss: 1379.770
[16,     1] loss: 1371.353
[17,     1] loss: 1358.572
[18,     1] loss: 1337.795
[19,     1] loss: 1309.875
[20,     1] loss: 1264.650
[21,     1] loss: 1227.454
[22,     1] loss: 1192.556
[23,     1] loss: 1201.346
[24,     1] loss: 1155.165
[25,     1] loss: 1168.085
[26,     1] loss: 1142.548
[27,     1] loss: 1148.968
[28,     1] loss: 1171.334
[29,     1] loss: 1142.815
[30,     1] loss: 1117.848
[31,     1] loss: 1093.718
[32,     1] loss: 1109.401
[33,     1] loss: 1075.818
[34,     1] loss: 1061.598
[35,     1] loss: 1087.291
[36,     1] loss: 1089.262
[37,     1] loss: 1095.737
[38,     1] loss: 1059.048
[39,     1] loss: 1038.605
[40,     1] loss: 994.519
[41,     1] loss: 1042.753
[42,     1] loss: 1003.852
[43,     1] loss: 1032.526
[44,     1] loss: 997.676
[45,     1] loss: 936.297
[46,     1] loss: 953.691
[47,     1] loss: 947.402
[48,     1] loss: 1020.773
[49,     1] loss: 940.293
[50,     1] loss: 964.192
[51,     1] loss: 869.683
[52,     1] loss: 969.365
[53,     1] loss: 940.228
[54,     1] loss: 867.754
[55,     1] loss: 968.327
[56,     1] loss: 851.075
[57,     1] loss: 855.951
[58,     1] loss: 818.639
[59,     1] loss: 831.551
[60,     1] loss: 914.584
[61,     1] loss: 856.858
[62,     1] loss: 804.678
[63,     1] loss: 883.070
[64,     1] loss: 764.294
[65,     1] loss: 841.333
[66,     1] loss: 888.279
[67,     1] loss: 740.994
[68,     1] loss: 795.103
[69,     1] loss: 731.612
[70,     1] loss: 786.617
Early stopping applied (best metric=0.8897819519042969)
Finished Training
Total time taken: 11.421240091323853
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1397.897
[2,     1] loss: 1399.666
[3,     1] loss: 1399.128
[4,     1] loss: 1397.717
[5,     1] loss: 1407.231
[6,     1] loss: 1396.549
[7,     1] loss: 1397.628
[8,     1] loss: 1396.120
[9,     1] loss: 1395.227
[10,     1] loss: 1391.356
[11,     1] loss: 1394.629
[12,     1] loss: 1388.818
[13,     1] loss: 1386.484
[14,     1] loss: 1380.169
[15,     1] loss: 1373.164
[16,     1] loss: 1360.141
[17,     1] loss: 1336.408
[18,     1] loss: 1322.889
[19,     1] loss: 1296.270
[20,     1] loss: 1265.848
[21,     1] loss: 1219.375
[22,     1] loss: 1227.659
[23,     1] loss: 1227.500
[24,     1] loss: 1146.677
[25,     1] loss: 1191.802
[26,     1] loss: 1193.437
[27,     1] loss: 1138.719
[28,     1] loss: 1136.680
[29,     1] loss: 1113.511
[30,     1] loss: 1090.164
[31,     1] loss: 1098.360
[32,     1] loss: 1063.418
[33,     1] loss: 1060.683
[34,     1] loss: 1086.174
[35,     1] loss: 1012.208
[36,     1] loss: 1052.490
[37,     1] loss: 1024.701
[38,     1] loss: 1014.876
[39,     1] loss: 1032.806
[40,     1] loss: 1043.443
[41,     1] loss: 970.474
[42,     1] loss: 940.986
[43,     1] loss: 932.497
[44,     1] loss: 932.701
[45,     1] loss: 918.799
[46,     1] loss: 991.821
[47,     1] loss: 1001.279
[48,     1] loss: 884.452
[49,     1] loss: 990.190
[50,     1] loss: 858.622
[51,     1] loss: 985.432
[52,     1] loss: 880.099
[53,     1] loss: 919.390
[54,     1] loss: 889.970
[55,     1] loss: 926.313
[56,     1] loss: 860.411
[57,     1] loss: 886.093
[58,     1] loss: 758.968
[59,     1] loss: 790.652
[60,     1] loss: 765.325
Early stopping applied (best metric=0.8364681005477905)
Finished Training
Total time taken: 9.705204963684082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.795
[2,     1] loss: 1400.155
[3,     1] loss: 1395.578
[4,     1] loss: 1395.511
[5,     1] loss: 1397.500
[6,     1] loss: 1392.276
[7,     1] loss: 1393.523
[8,     1] loss: 1394.600
[9,     1] loss: 1387.354
[10,     1] loss: 1388.669
[11,     1] loss: 1386.727
[12,     1] loss: 1375.811
[13,     1] loss: 1369.714
[14,     1] loss: 1354.034
[15,     1] loss: 1340.973
[16,     1] loss: 1303.152
[17,     1] loss: 1273.078
[18,     1] loss: 1267.800
[19,     1] loss: 1261.859
[20,     1] loss: 1230.830
[21,     1] loss: 1205.492
[22,     1] loss: 1195.291
[23,     1] loss: 1157.457
[24,     1] loss: 1211.400
[25,     1] loss: 1162.617
[26,     1] loss: 1157.938
[27,     1] loss: 1132.697
[28,     1] loss: 1131.095
[29,     1] loss: 1118.968
[30,     1] loss: 1084.264
[31,     1] loss: 1077.562
[32,     1] loss: 1041.772
[33,     1] loss: 1059.573
[34,     1] loss: 1035.515
[35,     1] loss: 1061.145
[36,     1] loss: 1022.433
[37,     1] loss: 1020.166
[38,     1] loss: 982.289
[39,     1] loss: 1021.655
[40,     1] loss: 1022.037
[41,     1] loss: 1028.639
[42,     1] loss: 970.676
[43,     1] loss: 892.563
[44,     1] loss: 975.129
[45,     1] loss: 955.239
[46,     1] loss: 930.874
[47,     1] loss: 1049.898
[48,     1] loss: 911.947
[49,     1] loss: 984.537
[50,     1] loss: 874.096
[51,     1] loss: 915.645
[52,     1] loss: 910.925
[53,     1] loss: 918.778
[54,     1] loss: 928.003
[55,     1] loss: 878.268
[56,     1] loss: 892.940
[57,     1] loss: 799.102
[58,     1] loss: 854.710
[59,     1] loss: 792.236
[60,     1] loss: 738.560
Early stopping applied (best metric=0.7200088500976562)
Finished Training
Total time taken: 8.18017292022705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.536
[2,     1] loss: 1393.356
[3,     1] loss: 1397.269
[4,     1] loss: 1398.990
[5,     1] loss: 1391.395
[6,     1] loss: 1387.353
[7,     1] loss: 1389.192
[8,     1] loss: 1384.463
[9,     1] loss: 1381.165
[10,     1] loss: 1363.005
[11,     1] loss: 1344.552
[12,     1] loss: 1312.534
[13,     1] loss: 1273.714
[14,     1] loss: 1232.150
[15,     1] loss: 1192.157
[16,     1] loss: 1197.243
[17,     1] loss: 1179.126
[18,     1] loss: 1187.639
[19,     1] loss: 1151.358
[20,     1] loss: 1164.414
[21,     1] loss: 1114.534
[22,     1] loss: 1150.843
[23,     1] loss: 1102.619
[24,     1] loss: 1127.646
[25,     1] loss: 1103.665
[26,     1] loss: 1103.897
[27,     1] loss: 1099.209
[28,     1] loss: 1098.263
[29,     1] loss: 1058.010
[30,     1] loss: 1088.711
[31,     1] loss: 1074.062
[32,     1] loss: 1033.709
[33,     1] loss: 1038.281
[34,     1] loss: 1009.125
[35,     1] loss: 958.331
[36,     1] loss: 1019.535
[37,     1] loss: 982.588
[38,     1] loss: 954.895
[39,     1] loss: 935.626
[40,     1] loss: 887.251
[41,     1] loss: 967.214
[42,     1] loss: 927.753
[43,     1] loss: 871.836
[44,     1] loss: 922.487
[45,     1] loss: 907.077
[46,     1] loss: 932.162
[47,     1] loss: 813.655
[48,     1] loss: 893.492
[49,     1] loss: 870.855
[50,     1] loss: 788.987
[51,     1] loss: 816.322
[52,     1] loss: 780.120
[53,     1] loss: 782.154
[54,     1] loss: 714.885
[55,     1] loss: 710.567
[56,     1] loss: 704.044
[57,     1] loss: 716.215
[58,     1] loss: 845.374
[59,     1] loss: 767.911
Early stopping applied (best metric=0.6820101737976074)
Finished Training
Total time taken: 8.69118356704712
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.088
[2,     1] loss: 1398.035
[3,     1] loss: 1392.755
[4,     1] loss: 1390.791
[5,     1] loss: 1397.855
[6,     1] loss: 1392.116
[7,     1] loss: 1388.343
[8,     1] loss: 1385.388
[9,     1] loss: 1390.571
[10,     1] loss: 1385.165
[11,     1] loss: 1378.723
[12,     1] loss: 1368.117
[13,     1] loss: 1337.395
[14,     1] loss: 1306.602
[15,     1] loss: 1290.549
[16,     1] loss: 1266.488
[17,     1] loss: 1250.715
[18,     1] loss: 1207.088
[19,     1] loss: 1185.716
[20,     1] loss: 1212.298
[21,     1] loss: 1165.323
[22,     1] loss: 1197.197
[23,     1] loss: 1181.539
[24,     1] loss: 1136.354
[25,     1] loss: 1207.738
[26,     1] loss: 1148.393
[27,     1] loss: 1218.477
[28,     1] loss: 1152.922
[29,     1] loss: 1115.345
[30,     1] loss: 1112.351
[31,     1] loss: 1112.484
[32,     1] loss: 1113.143
[33,     1] loss: 1066.720
[34,     1] loss: 1077.695
[35,     1] loss: 1019.862
[36,     1] loss: 1032.208
[37,     1] loss: 1015.398
[38,     1] loss: 1012.740
[39,     1] loss: 993.975
[40,     1] loss: 1009.645
[41,     1] loss: 990.916
[42,     1] loss: 1009.467
[43,     1] loss: 1016.985
[44,     1] loss: 925.773
[45,     1] loss: 936.553
[46,     1] loss: 926.923
[47,     1] loss: 949.564
[48,     1] loss: 873.543
[49,     1] loss: 939.169
[50,     1] loss: 1151.601
[51,     1] loss: 979.468
[52,     1] loss: 927.191
[53,     1] loss: 897.066
[54,     1] loss: 904.484
[55,     1] loss: 848.287
[56,     1] loss: 854.534
Early stopping applied (best metric=0.6950884461402893)
Finished Training
Total time taken: 7.8241658210754395
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.340
[2,     1] loss: 1394.638
[3,     1] loss: 1393.383
[4,     1] loss: 1398.223
[5,     1] loss: 1390.188
[6,     1] loss: 1384.939
[7,     1] loss: 1379.137
[8,     1] loss: 1370.784
[9,     1] loss: 1340.860
[10,     1] loss: 1328.466
[11,     1] loss: 1288.506
[12,     1] loss: 1245.794
[13,     1] loss: 1233.673
[14,     1] loss: 1184.280
[15,     1] loss: 1175.619
[16,     1] loss: 1189.994
[17,     1] loss: 1160.163
[18,     1] loss: 1144.719
[19,     1] loss: 1166.321
[20,     1] loss: 1180.618
[21,     1] loss: 1164.734
[22,     1] loss: 1152.997
[23,     1] loss: 1169.948
[24,     1] loss: 1068.021
[25,     1] loss: 1065.333
[26,     1] loss: 1060.974
[27,     1] loss: 1051.148
[28,     1] loss: 1088.929
[29,     1] loss: 1009.950
[30,     1] loss: 1032.340
[31,     1] loss: 1060.688
[32,     1] loss: 1045.008
[33,     1] loss: 1016.450
[34,     1] loss: 979.576
[35,     1] loss: 1011.503
[36,     1] loss: 973.065
[37,     1] loss: 1055.745
[38,     1] loss: 1116.431
[39,     1] loss: 891.916
[40,     1] loss: 1025.535
[41,     1] loss: 923.404
[42,     1] loss: 925.746
[43,     1] loss: 862.202
[44,     1] loss: 955.938
[45,     1] loss: 861.547
[46,     1] loss: 902.537
[47,     1] loss: 862.005
[48,     1] loss: 803.003
[49,     1] loss: 787.998
[50,     1] loss: 767.856
[51,     1] loss: 752.039
[52,     1] loss: 826.089
[53,     1] loss: 811.896
[54,     1] loss: 732.833
[55,     1] loss: 746.204
[56,     1] loss: 747.781
[57,     1] loss: 787.311
[58,     1] loss: 727.841
[59,     1] loss: 723.930
[60,     1] loss: 684.468
[61,     1] loss: 684.007
[62,     1] loss: 678.537
[63,     1] loss: 894.405
[64,     1] loss: 1271.025
[65,     1] loss: 900.223
[66,     1] loss: 940.854
[67,     1] loss: 871.174
[68,     1] loss: 913.476
[69,     1] loss: 1039.658
[70,     1] loss: 913.077
[71,     1] loss: 846.526
[72,     1] loss: 883.957
[73,     1] loss: 925.802
[74,     1] loss: 887.987
[75,     1] loss: 755.178
[76,     1] loss: 874.057
[77,     1] loss: 819.399
[78,     1] loss: 765.237
[79,     1] loss: 769.100
[80,     1] loss: 680.495
[81,     1] loss: 731.509
[82,     1] loss: 673.247
[83,     1] loss: 647.493
[84,     1] loss: 652.901
[85,     1] loss: 678.876
[86,     1] loss: 601.232
Early stopping applied (best metric=0.7427722811698914)
Finished Training
Total time taken: 12.741271257400513
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1394.528
[2,     1] loss: 1399.383
[3,     1] loss: 1399.354
[4,     1] loss: 1395.720
[5,     1] loss: 1397.904
[6,     1] loss: 1389.009
[7,     1] loss: 1391.998
[8,     1] loss: 1377.441
[9,     1] loss: 1372.012
[10,     1] loss: 1357.164
[11,     1] loss: 1323.083
[12,     1] loss: 1293.943
[13,     1] loss: 1251.572
[14,     1] loss: 1210.920
[15,     1] loss: 1213.266
[16,     1] loss: 1185.311
[17,     1] loss: 1201.290
[18,     1] loss: 1220.045
[19,     1] loss: 1191.843
[20,     1] loss: 1162.848
[21,     1] loss: 1182.431
[22,     1] loss: 1161.821
[23,     1] loss: 1136.292
[24,     1] loss: 1156.860
[25,     1] loss: 1102.049
[26,     1] loss: 1134.430
[27,     1] loss: 1119.916
[28,     1] loss: 1089.104
[29,     1] loss: 1045.741
[30,     1] loss: 1046.115
[31,     1] loss: 1086.714
[32,     1] loss: 982.735
[33,     1] loss: 1064.286
[34,     1] loss: 994.054
[35,     1] loss: 1015.251
[36,     1] loss: 1010.094
[37,     1] loss: 1027.333
[38,     1] loss: 998.944
[39,     1] loss: 975.355
[40,     1] loss: 1046.339
[41,     1] loss: 1019.294
[42,     1] loss: 970.072
[43,     1] loss: 964.486
[44,     1] loss: 971.611
[45,     1] loss: 924.595
[46,     1] loss: 909.707
[47,     1] loss: 884.780
[48,     1] loss: 899.543
[49,     1] loss: 976.011
[50,     1] loss: 1163.588
[51,     1] loss: 881.631
[52,     1] loss: 935.037
[53,     1] loss: 929.885
[54,     1] loss: 953.485
[55,     1] loss: 875.392
[56,     1] loss: 951.754
[57,     1] loss: 865.349
[58,     1] loss: 899.827
[59,     1] loss: 778.246
[60,     1] loss: 901.768
[61,     1] loss: 789.025
[62,     1] loss: 980.906
[63,     1] loss: 737.939
[64,     1] loss: 918.455
[65,     1] loss: 837.261
[66,     1] loss: 858.353
[67,     1] loss: 733.273
[68,     1] loss: 816.904
[69,     1] loss: 747.778
[70,     1] loss: 779.679
[71,     1] loss: 702.268
[72,     1] loss: 720.543
[73,     1] loss: 648.835
[74,     1] loss: 686.400
Early stopping applied (best metric=0.7700737118721008)
Finished Training
Total time taken: 11.105236053466797
{'Hydroxylation-K Validation Accuracy': 0.7671099290780142, 'Hydroxylation-K Validation Sensitivity': 0.7044444444444444, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.4582591823381297, 'Hydroxylation-K AUC ROC': 0.8085575048732944, 'Hydroxylation-K AUC PR': 0.611345809500217, 'Hydroxylation-K MCC': 0.42537622585236834, 'Hydroxylation-K F1': 0.5513257926481314, 'Validation Loss (Hydroxylation-K)': 0.42677443822224936, 'Hydroxylation-P Validation Accuracy': 0.7970063448555911, 'Hydroxylation-P Validation Sensitivity': 0.806984126984127, 'Hydroxylation-P Validation Specificity': 0.7948800438924635, 'Hydroxylation-P Validation Precision': 0.4629253446593486, 'Hydroxylation-P AUC ROC': 0.8613146996208495, 'Hydroxylation-P AUC PR': 0.6090203949690194, 'Hydroxylation-P MCC': 0.4985621077511038, 'Hydroxylation-P F1': 0.5865367411566847, 'Validation Loss (Hydroxylation-P)': 0.35741344690322874, 'Validation Loss (total)': 0.7841878851254781, 'TimeToTrain': 9.640337038040162}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00400040437758865,
 'learning_rate_Hydroxylation-K': 0.00040064778807305025,
 'learning_rate_Hydroxylation-P': 0.003804069332164926,
 'log_base': 2.1203503977860905,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 391698020,
 'sample_weights': [2.3081724362550053, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.958486370856422,
 'weight_decay_Hydroxylation-K': 4.895931942436208,
 'weight_decay_Hydroxylation-P': 2.6109590667904605}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1373.810
[2,     1] loss: 1378.405
[3,     1] loss: 1369.185
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033940622967461643,
 'learning_rate_Hydroxylation-K': 0.00029179084355327703,
 'learning_rate_Hydroxylation-P': 0.0077234909574372505,
 'log_base': 2.5054277973384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 355947108,
 'sample_weights': [2.2212407637216094, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4461206706479075,
 'weight_decay_Hydroxylation-K': 7.744432561866839,
 'weight_decay_Hydroxylation-P': 0.9039304785013565}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1291.258
[2,     1] loss: 1292.608
[3,     1] loss: 1288.123
[4,     1] loss: 1286.664
[5,     1] loss: 1289.630
[6,     1] loss: 1285.873
[7,     1] loss: 1280.917
[8,     1] loss: 1275.917
[9,     1] loss: 1264.805
[10,     1] loss: 1229.633
[11,     1] loss: 1199.839
[12,     1] loss: 1175.680
[13,     1] loss: 1160.156
[14,     1] loss: 1117.643
[15,     1] loss: 1081.886
[16,     1] loss: 1128.222
[17,     1] loss: 1117.027
[18,     1] loss: 1097.219
[19,     1] loss: 1054.494
[20,     1] loss: 1074.884
[21,     1] loss: 1058.356
[22,     1] loss: 1047.080
[23,     1] loss: 1036.977
[24,     1] loss: 1061.605
[25,     1] loss: 1030.753
[26,     1] loss: 1015.403
[27,     1] loss: 1015.753
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0049897842370117,
 'learning_rate_Hydroxylation-K': 0.0030462161544396677,
 'learning_rate_Hydroxylation-P': 0.007248463135746753,
 'log_base': 1.4386624024823682,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1019666620,
 'sample_weights': [1.8176557074805826, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.002328255161109,
 'weight_decay_Hydroxylation-K': 5.4224410572540975,
 'weight_decay_Hydroxylation-P': 1.552068877739502}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1878.749
[2,     1] loss: 1882.575
[3,     1] loss: 1885.957
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009086718288648044,
 'learning_rate_Hydroxylation-K': 0.0017999807165960157,
 'learning_rate_Hydroxylation-P': 0.0067808388178173026,
 'log_base': 1.8311304426372979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1684517477,
 'sample_weights': [4.589991283167332, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.018998334646325,
 'weight_decay_Hydroxylation-K': 1.4112907516619617,
 'weight_decay_Hydroxylation-P': 1.7426413703412162}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1491.102
[2,     1] loss: 1494.532
[3,     1] loss: 1495.226
[4,     1] loss: 1487.733
[5,     1] loss: 1488.908
[6,     1] loss: 1490.683
[7,     1] loss: 1494.148
[8,     1] loss: 1484.786
[9,     1] loss: 1484.614
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005053548989302527,
 'learning_rate_Hydroxylation-K': 0.0023787912872888085,
 'learning_rate_Hydroxylation-P': 0.0035674495493227586,
 'log_base': 1.6151067377501125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 189778102,
 'sample_weights': [2.759713481573453, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.511812207480391,
 'weight_decay_Hydroxylation-K': 9.20874396896809,
 'weight_decay_Hydroxylation-P': 1.2589195621318285}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1641.432
[2,     1] loss: 1644.609
[3,     1] loss: 1640.415
[4,     1] loss: 1642.463
[5,     1] loss: 1636.836
[6,     1] loss: 1637.454
[7,     1] loss: 1637.038
[8,     1] loss: 1624.676
[9,     1] loss: 1594.656
[10,     1] loss: 1570.842
[11,     1] loss: 1508.814
[12,     1] loss: 1497.473
[13,     1] loss: 1433.047
[14,     1] loss: 1365.548
[15,     1] loss: 1375.755
[16,     1] loss: 1355.745
[17,     1] loss: 1576.085
[18,     1] loss: 1338.138
[19,     1] loss: 1407.893
[20,     1] loss: 1329.088
[21,     1] loss: 1362.303
[22,     1] loss: 1325.799
[23,     1] loss: 1332.096
[24,     1] loss: 1326.822
[25,     1] loss: 1266.678
[26,     1] loss: 1260.819
[27,     1] loss: 1229.326
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004361489005545,
 'learning_rate_Hydroxylation-K': 0.0031997665622988548,
 'learning_rate_Hydroxylation-P': 0.007721341494262095,
 'log_base': 2.8437487952848253,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2129209690,
 'sample_weights': [3.482351908561381, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.371070089869251,
 'weight_decay_Hydroxylation-K': 7.457900636183135,
 'weight_decay_Hydroxylation-P': 3.4611677156533442}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.685
[2,     1] loss: 1244.429
[3,     1] loss: 1250.653
[4,     1] loss: 1243.603
[5,     1] loss: 1244.090
[6,     1] loss: 1239.683
[7,     1] loss: 1243.457
[8,     1] loss: 1243.748
[9,     1] loss: 1236.694
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016072120750742558,
 'learning_rate_Hydroxylation-K': 0.006448026984610791,
 'learning_rate_Hydroxylation-P': 0.00713741918107349,
 'log_base': 1.8662028952801082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2304971255,
 'sample_weights': [1.5973649606493276, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.157551089239918,
 'weight_decay_Hydroxylation-K': 4.290452186577431,
 'weight_decay_Hydroxylation-P': 6.1734197328274725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1471.724
[2,     1] loss: 1476.640
[3,     1] loss: 1470.751
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052459783334490384,
 'learning_rate_Hydroxylation-K': 0.001292414417750612,
 'learning_rate_Hydroxylation-P': 0.00541446213742192,
 'log_base': 2.231629771035559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4145798726,
 'sample_weights': [2.6757934758160506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.557858901264611,
 'weight_decay_Hydroxylation-K': 8.310972081622959,
 'weight_decay_Hydroxylation-P': 0.3308562600161704}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.057
[2,     1] loss: 1351.542
[3,     1] loss: 1350.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008961846127864402,
 'learning_rate_Hydroxylation-K': 0.008298477757010219,
 'learning_rate_Hydroxylation-P': 0.0066598918103073805,
 'log_base': 2.892518809621654,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 253689493,
 'sample_weights': [2.079701344568272, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.86003657131611,
 'weight_decay_Hydroxylation-K': 8.733029360946954,
 'weight_decay_Hydroxylation-P': 6.173889032126697}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.475
[2,     1] loss: 1236.603
[3,     1] loss: 1252.053
[4,     1] loss: 1237.396
[5,     1] loss: 1239.414
[6,     1] loss: 1243.839
[7,     1] loss: 1239.965
[8,     1] loss: 1238.230
[9,     1] loss: 1240.268
[10,     1] loss: 1238.781
[11,     1] loss: 1235.664
[12,     1] loss: 1236.844
[13,     1] loss: 1233.218
[14,     1] loss: 1222.807
[15,     1] loss: 1214.958
[16,     1] loss: 1189.470
[17,     1] loss: 1158.234
[18,     1] loss: 1129.025
[19,     1] loss: 1073.861
[20,     1] loss: 1083.788
[21,     1] loss: 1081.802
[22,     1] loss: 1034.949
[23,     1] loss: 1094.228
[24,     1] loss: 1057.526
[25,     1] loss: 1054.412
[26,     1] loss: 1019.882
[27,     1] loss: 1057.545
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0051489915677584205,
 'learning_rate_Hydroxylation-K': 0.003004823554706623,
 'learning_rate_Hydroxylation-P': 0.004681670440791968,
 'log_base': 2.2776706856040407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 601282726,
 'sample_weights': [1.5717913902526626, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1807812218774085,
 'weight_decay_Hydroxylation-K': 2.371396130839707,
 'weight_decay_Hydroxylation-P': 8.503362022597042}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1345.688
[2,     1] loss: 1342.114
[3,     1] loss: 1335.655
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016833414879340075,
 'learning_rate_Hydroxylation-K': 0.006134743222504758,
 'learning_rate_Hydroxylation-P': 0.00863998233170972,
 'log_base': 2.518649373989762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4006445464,
 'sample_weights': [2.028107236411335, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.219285012637502,
 'weight_decay_Hydroxylation-K': 0.2935878875107689,
 'weight_decay_Hydroxylation-P': 5.408481486887452}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.469
[2,     1] loss: 1287.336
[3,     1] loss: 1287.937
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008173212680661852,
 'learning_rate_Hydroxylation-K': 2.247307870636056e-05,
 'learning_rate_Hydroxylation-P': 0.0031644859255877183,
 'log_base': 2.227484143115263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 679240739,
 'sample_weights': [1.8072988522145614, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.496354594876752,
 'weight_decay_Hydroxylation-K': 6.701093346639971,
 'weight_decay_Hydroxylation-P': 1.36431373570849}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1352.781
[2,     1] loss: 1348.795
[3,     1] loss: 1351.063
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015187643950413682,
 'learning_rate_Hydroxylation-K': 0.00838887553196012,
 'learning_rate_Hydroxylation-P': 0.008745053489640775,
 'log_base': 2.4028969763908745,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 454747331,
 'sample_weights': [2.0845298121914104, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.940702889551718,
 'weight_decay_Hydroxylation-K': 3.5547019336863794,
 'weight_decay_Hydroxylation-P': 0.7191844075155507}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.648
[2,     1] loss: 1312.917
[3,     1] loss: 1311.493
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004535523333780678,
 'learning_rate_Hydroxylation-K': 0.009216168901094546,
 'learning_rate_Hydroxylation-P': 0.006983563716677112,
 'log_base': 1.9422302029097736,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 282241827,
 'sample_weights': [1.9042894909915706, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.2171735205040368,
 'weight_decay_Hydroxylation-K': 3.3807020393095675,
 'weight_decay_Hydroxylation-P': 1.3894850288341554}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1441.928
[2,     1] loss: 1438.186
[3,     1] loss: 1438.638
[4,     1] loss: 1438.062
[5,     1] loss: 1441.617
[6,     1] loss: 1440.928
[7,     1] loss: 1441.673
[8,     1] loss: 1439.566
[9,     1] loss: 1435.891
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007049542483637245,
 'learning_rate_Hydroxylation-K': 0.005061079953376284,
 'learning_rate_Hydroxylation-P': 0.00030934806842484953,
 'log_base': 2.239244269251356,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3527884602,
 'sample_weights': [2.5148393265936875, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.760574034573283,
 'weight_decay_Hydroxylation-K': 2.77840830230198,
 'weight_decay_Hydroxylation-P': 0.013329375525872322}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1350.716
[2,     1] loss: 1348.822
[3,     1] loss: 1343.606
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00012834177225696733,
 'learning_rate_Hydroxylation-K': 0.007980131405572653,
 'learning_rate_Hydroxylation-P': 0.005979273289577371,
 'log_base': 2.61712729786084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3154724488,
 'sample_weights': [2.070913737728994, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.104242488568092,
 'weight_decay_Hydroxylation-K': 6.327553471000524,
 'weight_decay_Hydroxylation-P': 9.563885142260565}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.164
[2,     1] loss: 1279.958
[3,     1] loss: 1277.022
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00011737033548135523,
 'learning_rate_Hydroxylation-K': 0.009030503751145239,
 'learning_rate_Hydroxylation-P': 0.008457483154927687,
 'log_base': 2.894568425749319,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 629365198,
 'sample_weights': [1.7352485170380725, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.49889799415586,
 'weight_decay_Hydroxylation-K': 2.0194218507632167,
 'weight_decay_Hydroxylation-P': 7.439228622917357}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.323
[2,     1] loss: 1240.943
[3,     1] loss: 1239.554
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002907235243868838,
 'learning_rate_Hydroxylation-K': 0.008984408198117962,
 'learning_rate_Hydroxylation-P': 0.004062079235163709,
 'log_base': 2.369197183411296,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2535856614,
 'sample_weights': [1.570743848953677, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.493820568232267,
 'weight_decay_Hydroxylation-K': 6.786163673097572,
 'weight_decay_Hydroxylation-P': 3.384143642246551}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.642
[2,     1] loss: 1315.405
[3,     1] loss: 1311.831
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004262455754004035,
 'learning_rate_Hydroxylation-K': 0.004719088151088041,
 'learning_rate_Hydroxylation-P': 0.0062963726484899915,
 'log_base': 2.195328937104406,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2282806148,
 'sample_weights': [1.9354714612337243, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.838102673647483,
 'weight_decay_Hydroxylation-K': 7.271616171669713,
 'weight_decay_Hydroxylation-P': 3.376305585910977}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1362.693
[2,     1] loss: 1357.780
[3,     1] loss: 1355.458
[4,     1] loss: 1354.248
[5,     1] loss: 1355.453
[6,     1] loss: 1353.828
[7,     1] loss: 1354.415
[8,     1] loss: 1351.628
[9,     1] loss: 1350.806
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006293916082124313,
 'learning_rate_Hydroxylation-K': 0.0089820001672295,
 'learning_rate_Hydroxylation-P': 0.002164641774435858,
 'log_base': 1.3762386022791302,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1450936957,
 'sample_weights': [2.1230769892651784, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.04868486165917618,
 'weight_decay_Hydroxylation-K': 1.143444835979305,
 'weight_decay_Hydroxylation-P': 5.461933525862599}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2021.660
[2,     1] loss: 2010.705
[3,     1] loss: 2003.915
[4,     1] loss: 2014.488
[5,     1] loss: 2007.960
[6,     1] loss: 2007.691
[7,     1] loss: 2005.876
[8,     1] loss: 1997.141
[9,     1] loss: 1996.180
[10,     1] loss: 1987.787
[11,     1] loss: 1983.042
[12,     1] loss: 1946.333
[13,     1] loss: 1932.421
[14,     1] loss: 1907.311
[15,     1] loss: 1856.401
[16,     1] loss: 1851.654
[17,     1] loss: 1787.801
[18,     1] loss: 1737.919
[19,     1] loss: 1624.371
[20,     1] loss: 1706.329
[21,     1] loss: 1720.988
[22,     1] loss: 1626.292
[23,     1] loss: 1711.908
[24,     1] loss: 1700.083
[25,     1] loss: 1470.198
[26,     1] loss: 1606.783
[27,     1] loss: 1633.623
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001766251741628877,
 'learning_rate_Hydroxylation-K': 0.0038178763906108995,
 'learning_rate_Hydroxylation-P': 0.004722572008158188,
 'log_base': 2.154494277671423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1328502722,
 'sample_weights': [5.227560894702745, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.513426596660145,
 'weight_decay_Hydroxylation-K': 8.096619881144933,
 'weight_decay_Hydroxylation-P': 2.628500909285043}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1368.516
[2,     1] loss: 1366.248
[3,     1] loss: 1365.893
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003527151991356256,
 'learning_rate_Hydroxylation-K': 0.0020241273093591964,
 'learning_rate_Hydroxylation-P': 0.0012014402166796736,
 'log_base': 2.1174414308093734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 199483689,
 'sample_weights': [2.1750114641759386, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.297853979048895,
 'weight_decay_Hydroxylation-K': 8.681203024324711,
 'weight_decay_Hydroxylation-P': 7.406267805396106}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.594
[2,     1] loss: 1378.451
[3,     1] loss: 1380.016
[4,     1] loss: 1377.217
[5,     1] loss: 1378.729
[6,     1] loss: 1377.260
[7,     1] loss: 1370.230
[8,     1] loss: 1371.190
[9,     1] loss: 1365.495
[10,     1] loss: 1354.916
[11,     1] loss: 1350.597
[12,     1] loss: 1324.231
[13,     1] loss: 1292.952
[14,     1] loss: 1255.408
[15,     1] loss: 1223.586
[16,     1] loss: 1221.864
[17,     1] loss: 1175.848
[18,     1] loss: 1193.189
[19,     1] loss: 1140.804
[20,     1] loss: 1153.662
[21,     1] loss: 1124.160
[22,     1] loss: 1146.673
[23,     1] loss: 1117.404
[24,     1] loss: 1107.040
[25,     1] loss: 1130.146
[26,     1] loss: 1071.823
[27,     1] loss: 1095.890
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005207922056394486,
 'learning_rate_Hydroxylation-K': 0.009682135638235646,
 'learning_rate_Hydroxylation-P': 0.0012379281111763844,
 'log_base': 2.148533937687137,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3077908337,
 'sample_weights': [2.225305598497013, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7731609573218718,
 'weight_decay_Hydroxylation-K': 3.9733508678898524,
 'weight_decay_Hydroxylation-P': 0.4672381644079624}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1369.111
[2,     1] loss: 1370.350
[3,     1] loss: 1364.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005579811315667731,
 'learning_rate_Hydroxylation-K': 0.0042502901020676385,
 'learning_rate_Hydroxylation-P': 0.003746545989228316,
 'log_base': 2.0620436075264106,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 967716529,
 'sample_weights': [2.1828900612929094, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.408001147819592,
 'weight_decay_Hydroxylation-K': 5.254561645114871,
 'weight_decay_Hydroxylation-P': 0.3856748535864143}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.238
[2,     1] loss: 1402.475
[3,     1] loss: 1393.637
[4,     1] loss: 1391.879
[5,     1] loss: 1392.082
[6,     1] loss: 1398.872
[7,     1] loss: 1391.180
[8,     1] loss: 1392.411
[9,     1] loss: 1388.294
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003880891936824389,
 'learning_rate_Hydroxylation-K': 0.009680695051521594,
 'learning_rate_Hydroxylation-P': 0.0062697598738581835,
 'log_base': 2.082313954915535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4074604068,
 'sample_weights': [2.3068244260209214, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.001946791112653,
 'weight_decay_Hydroxylation-K': 1.457163374927843,
 'weight_decay_Hydroxylation-P': 4.441304856085084}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.001
[2,     1] loss: 1388.562
[3,     1] loss: 1388.484
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009270372540379686,
 'learning_rate_Hydroxylation-K': 0.007965577595846655,
 'learning_rate_Hydroxylation-P': 0.009383396098738206,
 'log_base': 2.4242543645448333,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 775123374,
 'sample_weights': [2.2760589355319207, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.526880559110154,
 'weight_decay_Hydroxylation-K': 8.70611597093561,
 'weight_decay_Hydroxylation-P': 2.4684601325540596}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1311.928
[2,     1] loss: 1315.306
[3,     1] loss: 1309.447
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009515321041425205,
 'learning_rate_Hydroxylation-K': 0.009092321988304062,
 'learning_rate_Hydroxylation-P': 0.0009141461943138304,
 'log_base': 1.1406614741370014,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2830591115,
 'sample_weights': [1.8852601963578235, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6397294903309176,
 'weight_decay_Hydroxylation-K': 1.6491236622867724,
 'weight_decay_Hydroxylation-P': 0.6507938267107454}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4156.286
[2,     1] loss: 4118.237
[3,     1] loss: 4109.653
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004238680208750405,
 'learning_rate_Hydroxylation-K': 0.0011385522465753987,
 'learning_rate_Hydroxylation-P': 0.004151421925247169,
 'log_base': 2.9589587938226782,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 716474251,
 'sample_weights': [12.684934829234114, 1.5856779379827162],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.387563530868918,
 'weight_decay_Hydroxylation-K': 7.231831931905812,
 'weight_decay_Hydroxylation-P': 4.158145621703026}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.128
[2,     1] loss: 1231.713
[3,     1] loss: 1230.110
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007711121491744827,
 'learning_rate_Hydroxylation-K': 0.0005457942002035576,
 'learning_rate_Hydroxylation-P': 0.005297367340476243,
 'log_base': 1.0274712377376352,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 397253009,
 'sample_weights': [1.5388878316292016, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.460451968796048,
 'weight_decay_Hydroxylation-K': 8.188234538356738,
 'weight_decay_Hydroxylation-P': 5.989503180283913}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20249.367
Exploding loss, terminate run (best metric=1.09694242477417)
Finished Training
Total time taken: 0.21100687980651855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20034.703
Exploding loss, terminate run (best metric=1.0970162153244019)
Finished Training
Total time taken: 0.19900202751159668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19812.102
Exploding loss, terminate run (best metric=1.1471748352050781)
Finished Training
Total time taken: 0.20400500297546387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19997.178
Exploding loss, terminate run (best metric=1.072300672531128)
Finished Training
Total time taken: 0.19700312614440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20019.379
Exploding loss, terminate run (best metric=1.0732390880584717)
Finished Training
Total time taken: 0.21200299263000488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20016.455
Exploding loss, terminate run (best metric=1.0985465049743652)
Finished Training
Total time taken: 0.22200441360473633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19935.373
Exploding loss, terminate run (best metric=1.095606803894043)
Finished Training
Total time taken: 0.21300506591796875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19996.625
Exploding loss, terminate run (best metric=1.0968444347381592)
Finished Training
Total time taken: 0.20000195503234863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19975.311
Exploding loss, terminate run (best metric=1.0754292011260986)
Finished Training
Total time taken: 0.2290048599243164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20104.756
Exploding loss, terminate run (best metric=1.1267039775848389)
Finished Training
Total time taken: 0.23300457000732422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20045.330
Exploding loss, terminate run (best metric=1.1456600427627563)
Finished Training
Total time taken: 0.19300293922424316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19968.781
Exploding loss, terminate run (best metric=1.0915756225585938)
Finished Training
Total time taken: 0.21300458908081055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20079.168
Exploding loss, terminate run (best metric=1.106499433517456)
Finished Training
Total time taken: 0.22600698471069336
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19964.434
Exploding loss, terminate run (best metric=1.0756797790527344)
Finished Training
Total time taken: 0.19900298118591309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20094.689
Exploding loss, terminate run (best metric=1.0830833911895752)
Finished Training
Total time taken: 0.21500396728515625
{'Hydroxylation-K Validation Accuracy': 0.48622931442080375, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.47368421052631576, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5685380116959065, 'Hydroxylation-K AUC PR': 0.2674004856918314, 'Hydroxylation-K MCC': 0.009895413919905869, 'Hydroxylation-K F1': 0.18087659466969813, 'Validation Loss (Hydroxylation-K)': 0.5624269445737203, 'Hydroxylation-P Validation Accuracy': 0.4961158655229007, 'Hydroxylation-P Validation Sensitivity': 0.5238095238095238, 'Hydroxylation-P Validation Specificity': 0.4899795501022495, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5553969768115735, 'Hydroxylation-P AUC PR': 0.24038089031309928, 'Hydroxylation-P MCC': 0.01134220845890573, 'Hydroxylation-P F1': 0.16378487868229172, 'Validation Loss (Hydroxylation-P)': 0.5363932092984517, 'Validation Loss (total)': 1.098820161819458, 'TimeToTrain': 0.21107082366943358}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00806276929730083,
 'learning_rate_Hydroxylation-K': 0.0027915880912365,
 'learning_rate_Hydroxylation-P': 0.006493738173151108,
 'log_base': 2.1966903035998424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2698187532,
 'sample_weights': [61.647230810076834, 7.689892344538113],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.40804370992976,
 'weight_decay_Hydroxylation-K': 9.278546625748488,
 'weight_decay_Hydroxylation-P': 3.6835542807650747}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1358.992
[2,     1] loss: 1359.374
[3,     1] loss: 1353.431
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006113781082262693,
 'learning_rate_Hydroxylation-K': 0.00012989671733515894,
 'learning_rate_Hydroxylation-P': 0.00012840938047592888,
 'log_base': 1.3339131487629305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1588299032,
 'sample_weights': [2.1214045188943, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.4498546763832785,
 'weight_decay_Hydroxylation-K': 7.417104986643926,
 'weight_decay_Hydroxylation-P': 2.1402146235030104}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2131.980
[2,     1] loss: 2127.711
[3,     1] loss: 2135.199
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006078713653703935,
 'learning_rate_Hydroxylation-K': 0.0012766962499329345,
 'learning_rate_Hydroxylation-P': 0.005634155096318592,
 'log_base': 1.479300145550122,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3200246037,
 'sample_weights': [5.794326879074896, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.520562974357953,
 'weight_decay_Hydroxylation-K': 4.413610076996003,
 'weight_decay_Hydroxylation-P': 6.346657350489885}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1808.750
[2,     1] loss: 1802.793
[3,     1] loss: 1810.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074614734713940685,
 'learning_rate_Hydroxylation-K': 0.005943226838059271,
 'learning_rate_Hydroxylation-P': 0.0023906487919421694,
 'log_base': 1.8345488371965046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 963693360,
 'sample_weights': [4.263470079355602, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.10004472987226509,
 'weight_decay_Hydroxylation-K': 0.3309179352795504,
 'weight_decay_Hydroxylation-P': 1.962160826110885}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1487.154
[2,     1] loss: 1490.643
[3,     1] loss: 1486.978
[4,     1] loss: 1487.583
[5,     1] loss: 1493.156
[6,     1] loss: 1488.843
[7,     1] loss: 1484.002
[8,     1] loss: 1481.339
[9,     1] loss: 1480.918
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010110859616232898,
 'learning_rate_Hydroxylation-K': 0.001742751053345333,
 'learning_rate_Hydroxylation-P': 0.005189372280224293,
 'log_base': 1.8502939505502531,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3183458411,
 'sample_weights': [2.751231110243162, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.634513392447721,
 'weight_decay_Hydroxylation-K': 4.101921170789128,
 'weight_decay_Hydroxylation-P': 0.19367642154386433}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1488.304
[2,     1] loss: 1480.456
[3,     1] loss: 1479.598
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012306557232361422,
 'learning_rate_Hydroxylation-K': 0.006157186713982575,
 'learning_rate_Hydroxylation-P': 0.009747227801177547,
 'log_base': 1.3105190474189914,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3832317685,
 'sample_weights': [2.7130218874230256, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7091377158393919,
 'weight_decay_Hydroxylation-K': 8.814187300638478,
 'weight_decay_Hydroxylation-P': 5.360508160554151}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2207.741
[2,     1] loss: 2223.917
[3,     1] loss: 2202.458
[4,     1] loss: 2211.894
[5,     1] loss: 2211.536
[6,     1] loss: 2217.185
[7,     1] loss: 2203.776
[8,     1] loss: 2205.683
[9,     1] loss: 2203.439
[10,     1] loss: 2201.996
[11,     1] loss: 2203.651
[12,     1] loss: 2205.316
[13,     1] loss: 2202.258
[14,     1] loss: 2195.641
[15,     1] loss: 2193.895
[16,     1] loss: 2192.841
[17,     1] loss: 2186.393
[18,     1] loss: 2164.372
[19,     1] loss: 2161.635
[20,     1] loss: 2143.334
[21,     1] loss: 2153.081
[22,     1] loss: 2121.360
[23,     1] loss: 2119.186
[24,     1] loss: 2051.883
[25,     1] loss: 1980.936
[26,     1] loss: 2030.438
[27,     1] loss: 1991.285
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006905692341937658,
 'learning_rate_Hydroxylation-K': 0.0018332160961157533,
 'learning_rate_Hydroxylation-P': 0.002548757848842516,
 'log_base': 1.0267478427489865,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2508004356,
 'sample_weights': [6.173444678615208, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.760772810283685,
 'weight_decay_Hydroxylation-K': 4.463643010241116,
 'weight_decay_Hydroxylation-P': 4.013978098244926}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20571.152
Exploding loss, terminate run (best metric=1.111120343208313)
Finished Training
Total time taken: 0.19400548934936523
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20446.816
Exploding loss, terminate run (best metric=1.0931330919265747)
Finished Training
Total time taken: 0.21300625801086426
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20653.500
Exploding loss, terminate run (best metric=1.0920990705490112)
Finished Training
Total time taken: 0.22600221633911133
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20474.225
Exploding loss, terminate run (best metric=1.0729271173477173)
Finished Training
Total time taken: 0.21300435066223145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20591.408
Exploding loss, terminate run (best metric=1.0970046520233154)
Finished Training
Total time taken: 0.20000290870666504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20513.805
Exploding loss, terminate run (best metric=1.1103971004486084)
Finished Training
Total time taken: 0.22600317001342773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20591.693
Exploding loss, terminate run (best metric=1.103849172592163)
Finished Training
Total time taken: 0.23400473594665527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20463.617
Exploding loss, terminate run (best metric=1.0977181196212769)
Finished Training
Total time taken: 0.2160043716430664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20579.768
Exploding loss, terminate run (best metric=1.0977044105529785)
Finished Training
Total time taken: 0.20900392532348633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20616.016
Exploding loss, terminate run (best metric=1.0723317861557007)
Finished Training
Total time taken: 0.20900440216064453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20498.803
Exploding loss, terminate run (best metric=1.098829746246338)
Finished Training
Total time taken: 0.20200514793395996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20544.762
Exploding loss, terminate run (best metric=1.0932626724243164)
Finished Training
Total time taken: 0.20800232887268066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20612.770
Exploding loss, terminate run (best metric=1.1118828058242798)
Finished Training
Total time taken: 0.21800613403320312
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20617.143
Exploding loss, terminate run (best metric=1.1050605773925781)
Finished Training
Total time taken: 0.21900367736816406
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20591.621
Exploding loss, terminate run (best metric=1.0797383785247803)
Finished Training
Total time taken: 0.19600367546081543
{'Hydroxylation-K Validation Accuracy': 0.6349290780141844, 'Hydroxylation-K Validation Sensitivity': 0.2962962962962963, 'Hydroxylation-K Validation Specificity': 0.7192982456140351, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.554327485380117, 'Hydroxylation-K AUC PR': 0.2772402079215973, 'Hydroxylation-K MCC': 0.014072168596917857, 'Hydroxylation-K F1': 0.11579091406677615, 'Validation Loss (Hydroxylation-K)': 0.5598568280537923, 'Hydroxylation-P Validation Accuracy': 0.6495823223863425, 'Hydroxylation-P Validation Sensitivity': 0.2838095238095238, 'Hydroxylation-P Validation Specificity': 0.7280487804878049, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5609490419108106, 'Hydroxylation-P AUC PR': 0.2507219832792881, 'Hydroxylation-P MCC': 0.014397283722710942, 'Hydroxylation-P F1': 0.10130626681912513, 'Validation Loss (Hydroxylation-P)': 0.5359471201896667, 'Validation Loss (total)': 1.09580393632253, 'TimeToTrain': 0.21220418612162273}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006742568215295411,
 'learning_rate_Hydroxylation-K': 0.008359633051486823,
 'learning_rate_Hydroxylation-P': 0.0018789609518258242,
 'log_base': 1.4305040320252196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3283089690,
 'sample_weights': [63.29208751741459, 7.8950722177523245],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6116330876012077,
 'weight_decay_Hydroxylation-K': 0.9684734936108441,
 'weight_decay_Hydroxylation-P': 8.610845723817027}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1899.613
[2,     1] loss: 1907.949
[3,     1] loss: 1887.796
[4,     1] loss: 1888.863
[5,     1] loss: 1888.561
[6,     1] loss: 1888.920
[7,     1] loss: 1898.828
[8,     1] loss: 1883.923
[9,     1] loss: 1886.696
[10,     1] loss: 1889.023
[11,     1] loss: 1886.346
[12,     1] loss: 1891.660
[13,     1] loss: 1880.925
[14,     1] loss: 1882.975
[15,     1] loss: 1863.607
[16,     1] loss: 1859.052
[17,     1] loss: 1860.228
[18,     1] loss: 1837.551
[19,     1] loss: 1818.969
[20,     1] loss: 1781.557
[21,     1] loss: 1762.791
[22,     1] loss: 1697.689
[23,     1] loss: 1643.096
[24,     1] loss: 1650.738
[25,     1] loss: 1634.519
[26,     1] loss: 1606.714
[27,     1] loss: 1634.769
[28,     1] loss: 1635.860
[29,     1] loss: 1604.690
[30,     1] loss: 1551.861
[31,     1] loss: 1573.050
[32,     1] loss: 1503.623
[33,     1] loss: 1564.877
[34,     1] loss: 1548.770
[35,     1] loss: 1508.358
[36,     1] loss: 1527.283
[37,     1] loss: 1499.381
[38,     1] loss: 1526.007
[39,     1] loss: 1557.981
[40,     1] loss: 1440.415
[41,     1] loss: 1432.626
[42,     1] loss: 1447.520
[43,     1] loss: 1415.889
[44,     1] loss: 1380.692
[45,     1] loss: 1399.363
[46,     1] loss: 1336.447
[47,     1] loss: 1410.081
[48,     1] loss: 1353.838
[49,     1] loss: 1342.144
[50,     1] loss: 1347.271
[51,     1] loss: 1328.309
[52,     1] loss: 1362.184
[53,     1] loss: 1445.511
[54,     1] loss: 1410.419
[55,     1] loss: 1332.653
[56,     1] loss: 1390.318
[57,     1] loss: 1437.218
Early stopping applied (best metric=0.9439923763275146)
Finished Training
Total time taken: 9.369196891784668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1900.895
[2,     1] loss: 1905.948
[3,     1] loss: 1894.518
[4,     1] loss: 1886.967
[5,     1] loss: 1891.222
[6,     1] loss: 1893.752
[7,     1] loss: 1891.416
[8,     1] loss: 1890.976
[9,     1] loss: 1892.100
[10,     1] loss: 1893.251
[11,     1] loss: 1889.501
[12,     1] loss: 1890.110
[13,     1] loss: 1888.299
[14,     1] loss: 1888.508
[15,     1] loss: 1893.695
[16,     1] loss: 1887.761
[17,     1] loss: 1888.621
[18,     1] loss: 1886.119
[19,     1] loss: 1890.658
[20,     1] loss: 1885.152
[21,     1] loss: 1883.575
[22,     1] loss: 1884.500
[23,     1] loss: 1879.563
[24,     1] loss: 1878.281
[25,     1] loss: 1868.907
[26,     1] loss: 1855.112
[27,     1] loss: 1844.954
[28,     1] loss: 1825.225
[29,     1] loss: 1804.891
[30,     1] loss: 1784.765
[31,     1] loss: 1731.005
[32,     1] loss: 1743.283
[33,     1] loss: 1720.972
[34,     1] loss: 1690.392
[35,     1] loss: 1695.721
[36,     1] loss: 1658.954
[37,     1] loss: 1634.219
[38,     1] loss: 1610.901
[39,     1] loss: 1599.213
[40,     1] loss: 1571.641
[41,     1] loss: 1622.979
[42,     1] loss: 1585.879
[43,     1] loss: 1612.023
[44,     1] loss: 1573.022
[45,     1] loss: 1631.111
[46,     1] loss: 1589.416
[47,     1] loss: 1532.385
[48,     1] loss: 1561.785
[49,     1] loss: 1545.234
[50,     1] loss: 1532.169
[51,     1] loss: 1601.071
[52,     1] loss: 1489.271
[53,     1] loss: 1526.385
[54,     1] loss: 1542.613
[55,     1] loss: 1563.766
[56,     1] loss: 1501.784
[57,     1] loss: 1455.256
[58,     1] loss: 1487.124
[59,     1] loss: 1523.525
[60,     1] loss: 1470.039
[61,     1] loss: 1414.710
[62,     1] loss: 1505.823
[63,     1] loss: 1465.671
[64,     1] loss: 1519.937
[65,     1] loss: 1408.559
[66,     1] loss: 1437.704
[67,     1] loss: 1425.702
[68,     1] loss: 1428.315
[69,     1] loss: 1452.636
[70,     1] loss: 1403.063
[71,     1] loss: 1362.630
[72,     1] loss: 1436.680
[73,     1] loss: 1365.216
[74,     1] loss: 1339.221
[75,     1] loss: 1424.422
[76,     1] loss: 1414.081
[77,     1] loss: 1366.718
[78,     1] loss: 1337.576
[79,     1] loss: 1284.949
[80,     1] loss: 1302.942
[81,     1] loss: 1250.171
[82,     1] loss: 1436.366
[83,     1] loss: 1304.498
[84,     1] loss: 1415.155
[85,     1] loss: 1388.013
[86,     1] loss: 1322.600
[87,     1] loss: 1437.401
[88,     1] loss: 1382.106
[89,     1] loss: 1282.320
[90,     1] loss: 1407.790
[91,     1] loss: 1286.729
[92,     1] loss: 1255.465
[93,     1] loss: 1345.901
[94,     1] loss: 1225.709
[95,     1] loss: 1262.823
[96,     1] loss: 1341.587
[97,     1] loss: 1258.302
[98,     1] loss: 1450.294
Early stopping applied (best metric=0.8094362020492554)
Finished Training
Total time taken: 16.10433840751648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1896.949
[2,     1] loss: 1890.827
[3,     1] loss: 1900.349
[4,     1] loss: 1888.727
[5,     1] loss: 1889.697
[6,     1] loss: 1894.830
[7,     1] loss: 1888.837
[8,     1] loss: 1899.776
[9,     1] loss: 1890.933
[10,     1] loss: 1882.800
[11,     1] loss: 1889.602
[12,     1] loss: 1890.015
[13,     1] loss: 1887.823
[14,     1] loss: 1887.179
[15,     1] loss: 1888.479
[16,     1] loss: 1886.624
[17,     1] loss: 1882.819
[18,     1] loss: 1874.628
[19,     1] loss: 1879.071
[20,     1] loss: 1856.439
[21,     1] loss: 1860.696
[22,     1] loss: 1847.147
[23,     1] loss: 1819.233
[24,     1] loss: 1786.543
[25,     1] loss: 1722.256
[26,     1] loss: 1703.231
[27,     1] loss: 1634.281
[28,     1] loss: 1621.133
[29,     1] loss: 1625.356
[30,     1] loss: 1623.725
[31,     1] loss: 1512.729
[32,     1] loss: 1553.224
[33,     1] loss: 1604.960
[34,     1] loss: 1572.019
[35,     1] loss: 1563.845
[36,     1] loss: 1654.044
[37,     1] loss: 1569.664
[38,     1] loss: 1505.020
[39,     1] loss: 1500.045
[40,     1] loss: 1498.578
[41,     1] loss: 1499.619
[42,     1] loss: 1410.775
[43,     1] loss: 1397.997
[44,     1] loss: 1445.998
[45,     1] loss: 1551.566
[46,     1] loss: 1508.479
[47,     1] loss: 1412.937
[48,     1] loss: 1414.233
[49,     1] loss: 1459.835
[50,     1] loss: 1399.546
[51,     1] loss: 1444.072
[52,     1] loss: 1457.514
[53,     1] loss: 1372.899
[54,     1] loss: 1475.847
[55,     1] loss: 1404.113
[56,     1] loss: 1386.379
[57,     1] loss: 1332.729
[58,     1] loss: 1330.929
[59,     1] loss: 1305.097
[60,     1] loss: 1309.821
[61,     1] loss: 1390.897
Early stopping applied (best metric=1.0213656425476074)
Finished Training
Total time taken: 8.11617112159729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1897.286
[2,     1] loss: 1895.036
[3,     1] loss: 1896.721
[4,     1] loss: 1884.073
[5,     1] loss: 1888.766
[6,     1] loss: 1895.803
[7,     1] loss: 1885.157
[8,     1] loss: 1887.261
[9,     1] loss: 1885.312
[10,     1] loss: 1877.781
[11,     1] loss: 1870.666
[12,     1] loss: 1879.205
[13,     1] loss: 1835.478
[14,     1] loss: 1823.097
[15,     1] loss: 1800.437
[16,     1] loss: 1718.240
[17,     1] loss: 1747.368
[18,     1] loss: 1693.354
[19,     1] loss: 1670.142
[20,     1] loss: 1718.857
[21,     1] loss: 1640.977
[22,     1] loss: 1564.417
[23,     1] loss: 1608.492
[24,     1] loss: 1669.595
[25,     1] loss: 1550.717
[26,     1] loss: 1560.212
[27,     1] loss: 1607.898
[28,     1] loss: 1599.770
[29,     1] loss: 1562.244
[30,     1] loss: 1561.405
[31,     1] loss: 1565.265
[32,     1] loss: 1530.691
[33,     1] loss: 1551.562
[34,     1] loss: 1476.620
[35,     1] loss: 1470.226
[36,     1] loss: 1509.490
[37,     1] loss: 1504.454
[38,     1] loss: 1465.253
[39,     1] loss: 1426.058
[40,     1] loss: 1417.678
[41,     1] loss: 1505.533
[42,     1] loss: 1476.370
[43,     1] loss: 1416.003
[44,     1] loss: 1438.596
[45,     1] loss: 1566.059
[46,     1] loss: 1319.897
[47,     1] loss: 1403.687
[48,     1] loss: 1423.361
[49,     1] loss: 1405.991
[50,     1] loss: 1412.737
[51,     1] loss: 1424.514
[52,     1] loss: 1354.790
[53,     1] loss: 1442.684
[54,     1] loss: 1309.680
[55,     1] loss: 1453.381
[56,     1] loss: 1348.609
[57,     1] loss: 1408.088
[58,     1] loss: 1409.818
[59,     1] loss: 1389.547
[60,     1] loss: 1324.996
[61,     1] loss: 1337.885
[62,     1] loss: 1299.070
[63,     1] loss: 1345.222
[64,     1] loss: 1300.826
[65,     1] loss: 1323.744
[66,     1] loss: 1340.945
[67,     1] loss: 1263.375
[68,     1] loss: 1324.031
[69,     1] loss: 1188.301
[70,     1] loss: 1328.313
[71,     1] loss: 1225.112
[72,     1] loss: 1304.322
[73,     1] loss: 1324.412
[74,     1] loss: 1284.928
[75,     1] loss: 1327.743
[76,     1] loss: 1125.513
[77,     1] loss: 1210.553
[78,     1] loss: 1212.067
[79,     1] loss: 1210.466
[80,     1] loss: 1244.058
[81,     1] loss: 1223.789
Early stopping applied (best metric=0.8317388296127319)
Finished Training
Total time taken: 11.9752516746521
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1893.318
[2,     1] loss: 1900.411
[3,     1] loss: 1901.118
[4,     1] loss: 1898.541
[5,     1] loss: 1889.672
[6,     1] loss: 1887.440
[7,     1] loss: 1885.597
[8,     1] loss: 1887.182
[9,     1] loss: 1890.195
[10,     1] loss: 1866.029
[11,     1] loss: 1874.535
[12,     1] loss: 1857.575
[13,     1] loss: 1817.361
[14,     1] loss: 1788.504
[15,     1] loss: 1755.822
[16,     1] loss: 1680.546
[17,     1] loss: 1677.268
[18,     1] loss: 1671.684
[19,     1] loss: 1682.288
[20,     1] loss: 1674.836
[21,     1] loss: 1675.705
[22,     1] loss: 1562.851
[23,     1] loss: 1632.016
[24,     1] loss: 1635.805
[25,     1] loss: 1621.182
[26,     1] loss: 1610.744
[27,     1] loss: 1599.890
[28,     1] loss: 1597.544
[29,     1] loss: 1625.535
[30,     1] loss: 1564.549
[31,     1] loss: 1515.793
[32,     1] loss: 1496.787
[33,     1] loss: 1553.020
[34,     1] loss: 1564.492
[35,     1] loss: 1486.317
[36,     1] loss: 1487.734
[37,     1] loss: 1513.437
[38,     1] loss: 1522.719
[39,     1] loss: 1556.191
[40,     1] loss: 1522.452
[41,     1] loss: 1405.952
[42,     1] loss: 1335.501
[43,     1] loss: 1476.416
[44,     1] loss: 1435.931
[45,     1] loss: 1391.176
[46,     1] loss: 1488.748
[47,     1] loss: 1435.359
[48,     1] loss: 1396.981
[49,     1] loss: 1356.082
[50,     1] loss: 1371.424
[51,     1] loss: 1324.266
[52,     1] loss: 1408.823
[53,     1] loss: 1315.389
[54,     1] loss: 1486.394
[55,     1] loss: 1285.357
[56,     1] loss: 1241.891
[57,     1] loss: 1395.038
[58,     1] loss: 1252.474
Early stopping applied (best metric=0.7403424382209778)
Finished Training
Total time taken: 8.144174575805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1889.594
[2,     1] loss: 1897.658
[3,     1] loss: 1893.166
[4,     1] loss: 1887.100
[5,     1] loss: 1894.322
[6,     1] loss: 1892.863
[7,     1] loss: 1879.870
[8,     1] loss: 1881.522
[9,     1] loss: 1895.223
[10,     1] loss: 1892.392
[11,     1] loss: 1868.604
[12,     1] loss: 1860.988
[13,     1] loss: 1891.593
[14,     1] loss: 1857.054
[15,     1] loss: 1849.841
[16,     1] loss: 1837.172
[17,     1] loss: 1810.212
[18,     1] loss: 1791.405
[19,     1] loss: 1780.519
[20,     1] loss: 1733.888
[21,     1] loss: 1703.808
[22,     1] loss: 1673.990
[23,     1] loss: 1675.164
[24,     1] loss: 1717.041
[25,     1] loss: 1582.212
[26,     1] loss: 1633.657
[27,     1] loss: 1623.054
[28,     1] loss: 1550.699
[29,     1] loss: 1680.600
[30,     1] loss: 1673.462
[31,     1] loss: 1638.542
[32,     1] loss: 1641.870
[33,     1] loss: 1578.795
[34,     1] loss: 1590.473
[35,     1] loss: 1603.325
[36,     1] loss: 1574.203
[37,     1] loss: 1551.456
[38,     1] loss: 1524.515
[39,     1] loss: 1464.050
[40,     1] loss: 1499.122
[41,     1] loss: 1503.424
[42,     1] loss: 1461.554
[43,     1] loss: 1516.399
[44,     1] loss: 1438.920
[45,     1] loss: 1457.875
[46,     1] loss: 1530.917
[47,     1] loss: 1477.298
[48,     1] loss: 1360.925
[49,     1] loss: 1471.159
[50,     1] loss: 1381.004
[51,     1] loss: 1406.590
Early stopping applied (best metric=0.8673286437988281)
Finished Training
Total time taken: 8.398175477981567
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1893.003
[2,     1] loss: 1893.506
[3,     1] loss: 1898.572
[4,     1] loss: 1893.480
[5,     1] loss: 1879.926
[6,     1] loss: 1857.958
[7,     1] loss: 1850.195
[8,     1] loss: 1813.009
[9,     1] loss: 1825.536
[10,     1] loss: 1759.739
[11,     1] loss: 1706.632
[12,     1] loss: 1661.073
[13,     1] loss: 1643.435
[14,     1] loss: 1627.788
[15,     1] loss: 1606.032
[16,     1] loss: 1616.905
[17,     1] loss: 1546.162
[18,     1] loss: 1504.649
[19,     1] loss: 1589.510
[20,     1] loss: 1551.339
[21,     1] loss: 1588.402
[22,     1] loss: 1574.047
[23,     1] loss: 1511.562
[24,     1] loss: 1504.186
[25,     1] loss: 1589.756
[26,     1] loss: 1572.635
[27,     1] loss: 1502.108
[28,     1] loss: 1399.788
[29,     1] loss: 1495.495
[30,     1] loss: 1388.957
[31,     1] loss: 1454.107
[32,     1] loss: 1454.775
[33,     1] loss: 1428.489
[34,     1] loss: 1487.900
[35,     1] loss: 1409.875
[36,     1] loss: 1485.502
[37,     1] loss: 1464.988
[38,     1] loss: 1378.064
[39,     1] loss: 1381.890
[40,     1] loss: 1354.395
[41,     1] loss: 1383.773
[42,     1] loss: 1331.826
[43,     1] loss: 1482.207
[44,     1] loss: 1365.639
[45,     1] loss: 1281.140
[46,     1] loss: 1506.450
[47,     1] loss: 1295.983
[48,     1] loss: 1262.231
[49,     1] loss: 1315.631
[50,     1] loss: 1254.215
[51,     1] loss: 1300.232
[52,     1] loss: 1420.422
[53,     1] loss: 1193.996
[54,     1] loss: 1284.364
[55,     1] loss: 1193.957
[56,     1] loss: 1295.123
Early stopping applied (best metric=0.8310180902481079)
Finished Training
Total time taken: 7.789162874221802
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1893.126
[2,     1] loss: 1905.088
[3,     1] loss: 1897.558
[4,     1] loss: 1895.265
[5,     1] loss: 1893.612
[6,     1] loss: 1891.985
[7,     1] loss: 1895.387
[8,     1] loss: 1893.909
[9,     1] loss: 1887.962
[10,     1] loss: 1888.276
[11,     1] loss: 1892.064
[12,     1] loss: 1891.747
[13,     1] loss: 1895.130
[14,     1] loss: 1890.014
[15,     1] loss: 1888.041
[16,     1] loss: 1883.049
[17,     1] loss: 1887.363
[18,     1] loss: 1882.544
[19,     1] loss: 1888.099
[20,     1] loss: 1879.157
[21,     1] loss: 1889.182
[22,     1] loss: 1892.113
[23,     1] loss: 1883.749
[24,     1] loss: 1877.819
[25,     1] loss: 1879.489
[26,     1] loss: 1876.979
[27,     1] loss: 1874.009
[28,     1] loss: 1863.811
[29,     1] loss: 1854.710
[30,     1] loss: 1868.143
[31,     1] loss: 1859.608
[32,     1] loss: 1843.284
[33,     1] loss: 1829.746
[34,     1] loss: 1814.969
[35,     1] loss: 1802.586
[36,     1] loss: 1776.890
[37,     1] loss: 1724.932
[38,     1] loss: 1756.025
[39,     1] loss: 1690.695
[40,     1] loss: 1670.077
[41,     1] loss: 1704.234
[42,     1] loss: 1647.082
[43,     1] loss: 1729.839
[44,     1] loss: 1735.365
[45,     1] loss: 1608.765
[46,     1] loss: 1689.762
[47,     1] loss: 1555.515
[48,     1] loss: 1621.505
[49,     1] loss: 1670.516
[50,     1] loss: 1680.839
[51,     1] loss: 1574.512
[52,     1] loss: 1607.804
[53,     1] loss: 1608.559
[54,     1] loss: 1585.354
[55,     1] loss: 1593.630
[56,     1] loss: 1621.803
[57,     1] loss: 1541.513
[58,     1] loss: 1533.704
[59,     1] loss: 1524.280
[60,     1] loss: 1602.698
[61,     1] loss: 1475.280
[62,     1] loss: 1423.769
[63,     1] loss: 1498.879
[64,     1] loss: 1567.486
[65,     1] loss: 1428.254
[66,     1] loss: 1532.561
[67,     1] loss: 1516.670
[68,     1] loss: 1511.671
[69,     1] loss: 1337.706
[70,     1] loss: 1471.944
[71,     1] loss: 1495.498
[72,     1] loss: 1442.933
[73,     1] loss: 1311.605
[74,     1] loss: 1467.450
[75,     1] loss: 1433.699
[76,     1] loss: 1410.798
[77,     1] loss: 1459.216
[78,     1] loss: 1548.708
[79,     1] loss: 1373.688
[80,     1] loss: 1314.591
[81,     1] loss: 1312.049
Early stopping applied (best metric=0.7948042154312134)
Finished Training
Total time taken: 13.279279470443726
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1897.302
[2,     1] loss: 1899.302
[3,     1] loss: 1893.589
[4,     1] loss: 1900.468
[5,     1] loss: 1885.845
[6,     1] loss: 1886.163
[7,     1] loss: 1892.448
[8,     1] loss: 1889.712
[9,     1] loss: 1878.942
[10,     1] loss: 1868.423
[11,     1] loss: 1867.467
[12,     1] loss: 1857.061
[13,     1] loss: 1829.163
[14,     1] loss: 1818.685
[15,     1] loss: 1784.633
[16,     1] loss: 1739.581
[17,     1] loss: 1701.531
[18,     1] loss: 1695.562
[19,     1] loss: 1696.500
[20,     1] loss: 1621.768
[21,     1] loss: 1597.090
[22,     1] loss: 1555.732
[23,     1] loss: 1609.971
[24,     1] loss: 1657.913
[25,     1] loss: 1611.478
[26,     1] loss: 1485.886
[27,     1] loss: 1523.105
[28,     1] loss: 1545.680
[29,     1] loss: 1565.942
[30,     1] loss: 1504.535
[31,     1] loss: 1495.815
[32,     1] loss: 1545.024
[33,     1] loss: 1414.773
[34,     1] loss: 1452.966
[35,     1] loss: 1594.893
[36,     1] loss: 1413.694
[37,     1] loss: 1451.012
[38,     1] loss: 1409.699
[39,     1] loss: 1366.929
[40,     1] loss: 1437.798
[41,     1] loss: 1442.768
[42,     1] loss: 1370.628
[43,     1] loss: 1369.175
[44,     1] loss: 1342.335
[45,     1] loss: 1324.689
[46,     1] loss: 1416.347
[47,     1] loss: 1295.609
[48,     1] loss: 1390.271
Early stopping applied (best metric=0.9250789880752563)
Finished Training
Total time taken: 6.412134647369385
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1902.908
[2,     1] loss: 1904.132
[3,     1] loss: 1891.449
[4,     1] loss: 1909.241
[5,     1] loss: 1895.937
[6,     1] loss: 1898.329
[7,     1] loss: 1897.452
[8,     1] loss: 1887.117
[9,     1] loss: 1901.602
[10,     1] loss: 1902.248
[11,     1] loss: 1898.656
[12,     1] loss: 1895.084
[13,     1] loss: 1891.941
[14,     1] loss: 1894.689
[15,     1] loss: 1891.123
[16,     1] loss: 1899.514
[17,     1] loss: 1893.018
[18,     1] loss: 1893.836
[19,     1] loss: 1902.034
[20,     1] loss: 1885.527
[21,     1] loss: 1889.642
[22,     1] loss: 1894.967
[23,     1] loss: 1888.775
[24,     1] loss: 1891.326
[25,     1] loss: 1885.314
[26,     1] loss: 1886.917
[27,     1] loss: 1888.862
[28,     1] loss: 1884.972
[29,     1] loss: 1892.822
[30,     1] loss: 1887.547
[31,     1] loss: 1877.402
[32,     1] loss: 1889.962
[33,     1] loss: 1888.389
[34,     1] loss: 1878.156
[35,     1] loss: 1877.830
[36,     1] loss: 1865.154
[37,     1] loss: 1862.559
[38,     1] loss: 1857.583
[39,     1] loss: 1843.469
[40,     1] loss: 1839.475
[41,     1] loss: 1815.602
[42,     1] loss: 1784.946
[43,     1] loss: 1749.684
[44,     1] loss: 1733.742
[45,     1] loss: 1734.646
[46,     1] loss: 1722.091
[47,     1] loss: 1665.724
[48,     1] loss: 1636.211
[49,     1] loss: 1663.704
[50,     1] loss: 1603.072
[51,     1] loss: 1566.922
[52,     1] loss: 1559.533
[53,     1] loss: 1584.791
[54,     1] loss: 1564.360
[55,     1] loss: 1614.211
[56,     1] loss: 1574.009
[57,     1] loss: 1605.389
[58,     1] loss: 1526.332
[59,     1] loss: 1616.148
[60,     1] loss: 1613.058
[61,     1] loss: 1490.146
[62,     1] loss: 1549.367
[63,     1] loss: 1561.017
[64,     1] loss: 1533.356
[65,     1] loss: 1526.565
[66,     1] loss: 1498.231
[67,     1] loss: 1489.865
[68,     1] loss: 1479.294
[69,     1] loss: 1390.756
[70,     1] loss: 1579.552
Early stopping applied (best metric=0.6915197372436523)
Finished Training
Total time taken: 11.54024338722229
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1895.319
[2,     1] loss: 1891.549
[3,     1] loss: 1889.362
[4,     1] loss: 1908.525
[5,     1] loss: 1883.069
[6,     1] loss: 1890.721
[7,     1] loss: 1887.846
[8,     1] loss: 1888.048
[9,     1] loss: 1897.242
[10,     1] loss: 1888.543
[11,     1] loss: 1903.271
[12,     1] loss: 1886.685
[13,     1] loss: 1898.740
[14,     1] loss: 1884.373
[15,     1] loss: 1890.131
[16,     1] loss: 1888.214
[17,     1] loss: 1886.216
[18,     1] loss: 1890.088
[19,     1] loss: 1880.421
[20,     1] loss: 1880.815
[21,     1] loss: 1879.439
[22,     1] loss: 1878.774
[23,     1] loss: 1870.459
[24,     1] loss: 1881.564
[25,     1] loss: 1876.164
[26,     1] loss: 1856.925
[27,     1] loss: 1875.079
[28,     1] loss: 1849.432
[29,     1] loss: 1856.809
[30,     1] loss: 1844.590
[31,     1] loss: 1812.395
[32,     1] loss: 1798.074
[33,     1] loss: 1815.799
[34,     1] loss: 1766.889
[35,     1] loss: 1756.905
[36,     1] loss: 1751.362
[37,     1] loss: 1739.123
[38,     1] loss: 1724.288
[39,     1] loss: 1666.391
[40,     1] loss: 1685.258
[41,     1] loss: 1606.870
[42,     1] loss: 1674.983
[43,     1] loss: 1613.915
[44,     1] loss: 1606.417
[45,     1] loss: 1519.133
[46,     1] loss: 1618.985
[47,     1] loss: 1546.070
[48,     1] loss: 1620.897
[49,     1] loss: 1482.629
[50,     1] loss: 1537.759
[51,     1] loss: 1541.219
[52,     1] loss: 1500.430
[53,     1] loss: 1496.559
[54,     1] loss: 1515.143
[55,     1] loss: 1443.095
[56,     1] loss: 1475.813
[57,     1] loss: 1405.153
Early stopping applied (best metric=0.90593421459198)
Finished Training
Total time taken: 8.973189353942871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1896.932
[2,     1] loss: 1887.953
[3,     1] loss: 1886.362
[4,     1] loss: 1897.961
[5,     1] loss: 1882.991
[6,     1] loss: 1891.979
[7,     1] loss: 1890.175
[8,     1] loss: 1883.719
[9,     1] loss: 1889.495
[10,     1] loss: 1870.185
[11,     1] loss: 1873.748
[12,     1] loss: 1862.855
[13,     1] loss: 1844.265
[14,     1] loss: 1831.127
[15,     1] loss: 1835.902
[16,     1] loss: 1768.937
[17,     1] loss: 1773.309
[18,     1] loss: 1708.986
[19,     1] loss: 1727.309
[20,     1] loss: 1594.436
[21,     1] loss: 1694.447
[22,     1] loss: 1654.804
[23,     1] loss: 1553.347
[24,     1] loss: 1627.564
[25,     1] loss: 1590.349
[26,     1] loss: 1641.186
[27,     1] loss: 1560.824
[28,     1] loss: 1608.336
[29,     1] loss: 1594.812
[30,     1] loss: 1585.137
[31,     1] loss: 1531.804
[32,     1] loss: 1528.753
[33,     1] loss: 1539.238
[34,     1] loss: 1535.009
[35,     1] loss: 1600.160
[36,     1] loss: 1482.703
[37,     1] loss: 1476.154
[38,     1] loss: 1560.373
[39,     1] loss: 1516.956
[40,     1] loss: 1490.091
[41,     1] loss: 1509.986
[42,     1] loss: 1599.108
[43,     1] loss: 1435.391
[44,     1] loss: 1450.672
[45,     1] loss: 1459.709
[46,     1] loss: 1503.739
[47,     1] loss: 1451.571
[48,     1] loss: 1477.993
[49,     1] loss: 1424.349
[50,     1] loss: 1391.195
[51,     1] loss: 1434.226
[52,     1] loss: 1404.843
[53,     1] loss: 1459.098
[54,     1] loss: 1436.151
[55,     1] loss: 1481.906
[56,     1] loss: 1487.266
[57,     1] loss: 1397.787
[58,     1] loss: 1344.106
[59,     1] loss: 1415.212
[60,     1] loss: 1408.136
[61,     1] loss: 1309.686
[62,     1] loss: 1378.945
[63,     1] loss: 1321.170
[64,     1] loss: 1402.959
[65,     1] loss: 1373.280
[66,     1] loss: 1384.195
[67,     1] loss: 1318.095
Early stopping applied (best metric=0.7968200445175171)
Finished Training
Total time taken: 9.486198425292969
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1901.786
[2,     1] loss: 1901.306
[3,     1] loss: 1892.261
[4,     1] loss: 1893.897
[5,     1] loss: 1895.227
[6,     1] loss: 1899.008
[7,     1] loss: 1887.445
[8,     1] loss: 1893.186
[9,     1] loss: 1889.446
[10,     1] loss: 1892.272
[11,     1] loss: 1885.148
[12,     1] loss: 1904.317
[13,     1] loss: 1887.249
[14,     1] loss: 1895.583
[15,     1] loss: 1885.469
[16,     1] loss: 1893.161
[17,     1] loss: 1896.120
[18,     1] loss: 1890.400
[19,     1] loss: 1887.496
[20,     1] loss: 1889.197
[21,     1] loss: 1887.803
[22,     1] loss: 1890.811
[23,     1] loss: 1880.750
[24,     1] loss: 1888.049
[25,     1] loss: 1885.615
[26,     1] loss: 1883.242
[27,     1] loss: 1880.918
[28,     1] loss: 1885.728
[29,     1] loss: 1881.823
[30,     1] loss: 1880.076
[31,     1] loss: 1887.607
[32,     1] loss: 1880.038
[33,     1] loss: 1881.313
[34,     1] loss: 1864.819
[35,     1] loss: 1860.461
[36,     1] loss: 1838.560
[37,     1] loss: 1831.254
[38,     1] loss: 1803.542
[39,     1] loss: 1763.017
[40,     1] loss: 1734.981
[41,     1] loss: 1718.727
[42,     1] loss: 1701.065
[43,     1] loss: 1711.759
[44,     1] loss: 1671.689
[45,     1] loss: 1712.061
[46,     1] loss: 1634.853
[47,     1] loss: 1670.477
[48,     1] loss: 1689.889
[49,     1] loss: 1652.656
[50,     1] loss: 1589.099
[51,     1] loss: 1588.917
[52,     1] loss: 1618.342
[53,     1] loss: 1606.028
[54,     1] loss: 1585.650
[55,     1] loss: 1516.382
[56,     1] loss: 1530.142
[57,     1] loss: 1496.545
[58,     1] loss: 1509.150
[59,     1] loss: 1603.543
[60,     1] loss: 1538.638
[61,     1] loss: 1488.885
[62,     1] loss: 1544.864
[63,     1] loss: 1517.498
[64,     1] loss: 1493.207
[65,     1] loss: 1502.075
[66,     1] loss: 1487.741
[67,     1] loss: 1441.197
[68,     1] loss: 1449.952
[69,     1] loss: 1495.144
[70,     1] loss: 1431.546
[71,     1] loss: 1427.318
[72,     1] loss: 1469.516
[73,     1] loss: 1434.175
[74,     1] loss: 1465.767
[75,     1] loss: 1351.169
[76,     1] loss: 1420.304
[77,     1] loss: 1336.489
[78,     1] loss: 1367.855
[79,     1] loss: 1376.706
[80,     1] loss: 1407.857
[81,     1] loss: 1335.026
[82,     1] loss: 1317.127
[83,     1] loss: 1335.225
[84,     1] loss: 1268.725
[85,     1] loss: 1364.803
[86,     1] loss: 1265.415
[87,     1] loss: 1471.050
Early stopping applied (best metric=0.7852317094802856)
Finished Training
Total time taken: 12.778270244598389
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1900.386
[2,     1] loss: 1901.591
[3,     1] loss: 1887.979
[4,     1] loss: 1889.965
[5,     1] loss: 1902.016
[6,     1] loss: 1901.733
[7,     1] loss: 1893.882
[8,     1] loss: 1893.427
[9,     1] loss: 1889.612
[10,     1] loss: 1886.976
[11,     1] loss: 1886.252
[12,     1] loss: 1891.553
[13,     1] loss: 1884.932
[14,     1] loss: 1887.281
[15,     1] loss: 1885.253
[16,     1] loss: 1892.793
[17,     1] loss: 1883.836
[18,     1] loss: 1886.533
[19,     1] loss: 1881.232
[20,     1] loss: 1874.180
[21,     1] loss: 1862.906
[22,     1] loss: 1856.971
[23,     1] loss: 1842.747
[24,     1] loss: 1831.423
[25,     1] loss: 1815.422
[26,     1] loss: 1784.686
[27,     1] loss: 1740.033
[28,     1] loss: 1711.781
[29,     1] loss: 1733.570
[30,     1] loss: 1678.203
[31,     1] loss: 1646.662
[32,     1] loss: 1634.688
[33,     1] loss: 1709.295
[34,     1] loss: 1618.350
[35,     1] loss: 1619.427
[36,     1] loss: 1564.400
[37,     1] loss: 1545.686
[38,     1] loss: 1575.166
[39,     1] loss: 1565.977
[40,     1] loss: 1553.077
[41,     1] loss: 1627.414
[42,     1] loss: 1569.846
[43,     1] loss: 1560.166
[44,     1] loss: 1483.639
[45,     1] loss: 1434.533
[46,     1] loss: 1476.780
[47,     1] loss: 1488.227
[48,     1] loss: 1485.885
[49,     1] loss: 1438.970
[50,     1] loss: 1451.035
[51,     1] loss: 1399.088
[52,     1] loss: 1372.729
[53,     1] loss: 1400.658
[54,     1] loss: 1446.219
[55,     1] loss: 1379.775
Early stopping applied (best metric=0.8925089836120605)
Finished Training
Total time taken: 8.747182607650757
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1893.525
[2,     1] loss: 1894.844
[3,     1] loss: 1898.376
[4,     1] loss: 1892.310
[5,     1] loss: 1901.583
[6,     1] loss: 1893.119
[7,     1] loss: 1891.481
[8,     1] loss: 1893.082
[9,     1] loss: 1891.109
[10,     1] loss: 1889.635
[11,     1] loss: 1888.595
[12,     1] loss: 1890.958
[13,     1] loss: 1891.468
[14,     1] loss: 1885.522
[15,     1] loss: 1888.967
[16,     1] loss: 1883.682
[17,     1] loss: 1881.890
[18,     1] loss: 1876.663
[19,     1] loss: 1874.083
[20,     1] loss: 1874.397
[21,     1] loss: 1856.984
[22,     1] loss: 1836.708
[23,     1] loss: 1796.222
[24,     1] loss: 1772.430
[25,     1] loss: 1744.360
[26,     1] loss: 1692.647
[27,     1] loss: 1706.989
[28,     1] loss: 1668.611
[29,     1] loss: 1606.043
[30,     1] loss: 1643.494
[31,     1] loss: 1608.965
[32,     1] loss: 1586.435
[33,     1] loss: 1561.664
[34,     1] loss: 1565.249
[35,     1] loss: 1495.155
[36,     1] loss: 1556.073
[37,     1] loss: 1495.853
[38,     1] loss: 1558.612
[39,     1] loss: 1506.201
[40,     1] loss: 1474.230
[41,     1] loss: 1529.174
[42,     1] loss: 1454.322
[43,     1] loss: 1427.832
[44,     1] loss: 1463.875
[45,     1] loss: 1460.023
[46,     1] loss: 1434.904
[47,     1] loss: 1474.062
[48,     1] loss: 1432.623
[49,     1] loss: 1432.598
[50,     1] loss: 1444.258
[51,     1] loss: 1444.253
[52,     1] loss: 1445.677
[53,     1] loss: 1423.250
[54,     1] loss: 1431.025
[55,     1] loss: 1356.590
[56,     1] loss: 1452.452
Early stopping applied (best metric=0.8830299377441406)
Finished Training
Total time taken: 7.487157106399536
{'Hydroxylation-K Validation Accuracy': 0.7435874704491726, 'Hydroxylation-K Validation Sensitivity': 0.7162962962962963, 'Hydroxylation-K Validation Specificity': 0.7508771929824561, 'Hydroxylation-K Validation Precision': 0.425346775687333, 'Hydroxylation-K AUC ROC': 0.8028070175438596, 'Hydroxylation-K AUC PR': 0.5582578960399653, 'Hydroxylation-K MCC': 0.3975657653194386, 'Hydroxylation-K F1': 0.5293723109954994, 'Validation Loss (Hydroxylation-K)': 0.4396542092164358, 'Hydroxylation-P Validation Accuracy': 0.7691885352689373, 'Hydroxylation-P Validation Sensitivity': 0.7315343915343916, 'Hydroxylation-P Validation Specificity': 0.7773430096264152, 'Hydroxylation-P Validation Precision': 0.42293865338027337, 'Hydroxylation-P AUC ROC': 0.810044403117125, 'Hydroxylation-P AUC PR': 0.51471917820632, 'Hydroxylation-P MCC': 0.4241588868889653, 'Hydroxylation-P F1': 0.5318981698134648, 'Validation Loss (Hydroxylation-P)': 0.4083557963371277, 'Validation Loss (total)': 0.8480100035667419, 'TimeToTrain': 9.906675084431965}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054942060609904945,
 'learning_rate_Hydroxylation-K': 0.00263788391258255,
 'learning_rate_Hydroxylation-P': 0.0063155525900978745,
 'log_base': 1.9733720828218546,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 259484794,
 'sample_weights': [4.666358201341847, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6302597421176377,
 'weight_decay_Hydroxylation-K': 7.928646971361933,
 'weight_decay_Hydroxylation-P': 3.9344433657292415}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1428.109
[2,     1] loss: 1423.775
[3,     1] loss: 1431.053
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008540863492280474,
 'learning_rate_Hydroxylation-K': 0.003786126900929843,
 'learning_rate_Hydroxylation-P': 0.0028330194538847093,
 'log_base': 1.3982389708967728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3406985505,
 'sample_weights': [2.4559887942688015, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.10229072043239,
 'weight_decay_Hydroxylation-K': 8.486024918192628,
 'weight_decay_Hydroxylation-P': 4.533108095058542}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1962.388
[2,     1] loss: 1950.633
[3,     1] loss: 1971.049
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007615105004256467,
 'learning_rate_Hydroxylation-K': 0.006158741743949019,
 'learning_rate_Hydroxylation-P': 0.005801128338785134,
 'log_base': 1.642715231973279,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1286459053,
 'sample_weights': [4.980237413638064, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.608674659672266,
 'weight_decay_Hydroxylation-K': 4.869618953436645,
 'weight_decay_Hydroxylation-P': 2.958800357653747}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1623.993
[2,     1] loss: 1624.276
[3,     1] loss: 1620.091
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035792768566103297,
 'learning_rate_Hydroxylation-K': 0.002358861667235169,
 'learning_rate_Hydroxylation-P': 0.007614171085482098,
 'log_base': 2.451635241090248,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 108158834,
 'sample_weights': [3.363436000459094, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.954786308643083,
 'weight_decay_Hydroxylation-K': 9.105290954176937,
 'weight_decay_Hydroxylation-P': 0.018048758961987765}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.938
[2,     1] loss: 1301.852
[3,     1] loss: 1298.787
[4,     1] loss: 1297.020
[5,     1] loss: 1296.080
[6,     1] loss: 1293.405
[7,     1] loss: 1291.178
[8,     1] loss: 1277.729
[9,     1] loss: 1256.554
[10,     1] loss: 1241.446
[11,     1] loss: 1188.802
[12,     1] loss: 1165.146
[13,     1] loss: 1133.656
[14,     1] loss: 1133.218
[15,     1] loss: 1090.432
[16,     1] loss: 1082.670
[17,     1] loss: 1146.413
[18,     1] loss: 1172.360
[19,     1] loss: 1060.824
[20,     1] loss: 1046.493
[21,     1] loss: 1079.297
[22,     1] loss: 1086.795
[23,     1] loss: 1069.120
[24,     1] loss: 1028.711
[25,     1] loss: 1035.553
[26,     1] loss: 1010.763
[27,     1] loss: 1030.324
[28,     1] loss: 998.373
[29,     1] loss: 1030.365
[30,     1] loss: 986.724
[31,     1] loss: 1022.781
[32,     1] loss: 969.316
[33,     1] loss: 924.941
[34,     1] loss: 905.724
[35,     1] loss: 919.684
[36,     1] loss: 942.699
[37,     1] loss: 935.475
[38,     1] loss: 914.616
[39,     1] loss: 880.311
[40,     1] loss: 849.870
[41,     1] loss: 850.751
[42,     1] loss: 909.928
[43,     1] loss: 870.961
[44,     1] loss: 817.442
[45,     1] loss: 804.086
[46,     1] loss: 845.432
[47,     1] loss: 813.821
[48,     1] loss: 785.600
[49,     1] loss: 798.854
[50,     1] loss: 810.083
[51,     1] loss: 836.981
[52,     1] loss: 766.955
Early stopping applied (best metric=0.8318699598312378)
Finished Training
Total time taken: 8.492177486419678
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.926
[2,     1] loss: 1299.014
[3,     1] loss: 1297.911
[4,     1] loss: 1298.777
[5,     1] loss: 1294.469
[6,     1] loss: 1292.933
[7,     1] loss: 1282.002
[8,     1] loss: 1265.984
[9,     1] loss: 1238.145
[10,     1] loss: 1190.771
[11,     1] loss: 1167.132
[12,     1] loss: 1166.712
[13,     1] loss: 1120.777
[14,     1] loss: 1095.275
[15,     1] loss: 1036.448
[16,     1] loss: 1013.116
[17,     1] loss: 1050.915
[18,     1] loss: 1043.464
[19,     1] loss: 1031.711
[20,     1] loss: 996.546
[21,     1] loss: 1011.111
[22,     1] loss: 969.725
[23,     1] loss: 1025.338
[24,     1] loss: 980.514
[25,     1] loss: 941.997
[26,     1] loss: 947.143
[27,     1] loss: 950.228
[28,     1] loss: 950.817
[29,     1] loss: 932.608
[30,     1] loss: 957.288
[31,     1] loss: 926.076
[32,     1] loss: 886.045
[33,     1] loss: 850.447
[34,     1] loss: 863.273
[35,     1] loss: 853.281
[36,     1] loss: 864.301
[37,     1] loss: 822.865
[38,     1] loss: 855.710
[39,     1] loss: 809.011
[40,     1] loss: 870.010
[41,     1] loss: 833.283
[42,     1] loss: 833.127
[43,     1] loss: 919.140
[44,     1] loss: 742.564
[45,     1] loss: 894.769
[46,     1] loss: 780.681
[47,     1] loss: 816.966
[48,     1] loss: 770.799
[49,     1] loss: 821.528
[50,     1] loss: 734.722
[51,     1] loss: 813.855
[52,     1] loss: 699.867
[53,     1] loss: 775.320
[54,     1] loss: 743.596
Early stopping applied (best metric=0.9926667213439941)
Finished Training
Total time taken: 7.203154563903809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.631
[2,     1] loss: 1304.104
[3,     1] loss: 1295.964
[4,     1] loss: 1300.042
[5,     1] loss: 1297.195
[6,     1] loss: 1301.215
[7,     1] loss: 1293.512
[8,     1] loss: 1277.878
[9,     1] loss: 1259.998
[10,     1] loss: 1233.283
[11,     1] loss: 1212.763
[12,     1] loss: 1172.580
[13,     1] loss: 1160.310
[14,     1] loss: 1164.294
[15,     1] loss: 1091.352
[16,     1] loss: 1129.567
[17,     1] loss: 1072.528
[18,     1] loss: 1068.330
[19,     1] loss: 1058.425
[20,     1] loss: 1025.359
[21,     1] loss: 1074.663
[22,     1] loss: 1177.472
[23,     1] loss: 1077.979
[24,     1] loss: 1063.810
[25,     1] loss: 1030.457
[26,     1] loss: 1023.676
[27,     1] loss: 1044.921
[28,     1] loss: 1004.792
[29,     1] loss: 1014.192
[30,     1] loss: 981.984
[31,     1] loss: 960.805
[32,     1] loss: 971.337
[33,     1] loss: 941.291
[34,     1] loss: 952.824
[35,     1] loss: 924.367
[36,     1] loss: 894.995
[37,     1] loss: 956.392
[38,     1] loss: 1036.898
[39,     1] loss: 904.421
[40,     1] loss: 1035.902
[41,     1] loss: 908.314
[42,     1] loss: 988.172
[43,     1] loss: 877.663
[44,     1] loss: 1016.134
[45,     1] loss: 875.649
[46,     1] loss: 873.568
[47,     1] loss: 963.447
[48,     1] loss: 871.977
[49,     1] loss: 863.356
[50,     1] loss: 897.923
[51,     1] loss: 884.642
[52,     1] loss: 875.710
[53,     1] loss: 851.916
[54,     1] loss: 865.214
[55,     1] loss: 865.866
[56,     1] loss: 857.546
[57,     1] loss: 793.556
[58,     1] loss: 746.993
[59,     1] loss: 777.462
[60,     1] loss: 727.563
[61,     1] loss: 748.094
[62,     1] loss: 744.259
[63,     1] loss: 812.291
[64,     1] loss: 904.809
[65,     1] loss: 1078.760
[66,     1] loss: 716.680
[67,     1] loss: 888.781
[68,     1] loss: 782.275
[69,     1] loss: 817.932
[70,     1] loss: 872.223
[71,     1] loss: 784.750
[72,     1] loss: 775.210
[73,     1] loss: 797.327
[74,     1] loss: 766.078
[75,     1] loss: 782.414
[76,     1] loss: 685.322
[77,     1] loss: 776.624
[78,     1] loss: 715.825
[79,     1] loss: 683.053
Early stopping applied (best metric=0.8105506896972656)
Finished Training
Total time taken: 12.965271711349487
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.965
[2,     1] loss: 1300.224
[3,     1] loss: 1301.412
[4,     1] loss: 1304.354
[5,     1] loss: 1301.922
[6,     1] loss: 1301.570
[7,     1] loss: 1298.196
[8,     1] loss: 1297.871
[9,     1] loss: 1296.066
[10,     1] loss: 1298.221
[11,     1] loss: 1291.009
[12,     1] loss: 1287.040
[13,     1] loss: 1278.051
[14,     1] loss: 1262.619
[15,     1] loss: 1230.675
[16,     1] loss: 1198.732
[17,     1] loss: 1168.932
[18,     1] loss: 1139.089
[19,     1] loss: 1120.966
[20,     1] loss: 1094.168
[21,     1] loss: 1114.741
[22,     1] loss: 1056.897
[23,     1] loss: 1022.958
[24,     1] loss: 1015.286
[25,     1] loss: 1044.746
[26,     1] loss: 1021.780
[27,     1] loss: 1022.710
[28,     1] loss: 1055.817
[29,     1] loss: 1015.825
[30,     1] loss: 1008.877
[31,     1] loss: 975.890
[32,     1] loss: 1027.372
[33,     1] loss: 994.270
[34,     1] loss: 995.141
[35,     1] loss: 938.773
[36,     1] loss: 977.070
[37,     1] loss: 973.298
[38,     1] loss: 950.389
[39,     1] loss: 1035.627
[40,     1] loss: 955.130
[41,     1] loss: 907.903
[42,     1] loss: 938.235
[43,     1] loss: 923.003
[44,     1] loss: 934.063
[45,     1] loss: 877.487
[46,     1] loss: 870.915
[47,     1] loss: 868.515
[48,     1] loss: 880.104
[49,     1] loss: 833.147
[50,     1] loss: 848.742
[51,     1] loss: 862.310
[52,     1] loss: 1007.677
[53,     1] loss: 1037.859
[54,     1] loss: 818.404
[55,     1] loss: 913.250
[56,     1] loss: 857.570
[57,     1] loss: 919.081
[58,     1] loss: 845.115
[59,     1] loss: 775.521
[60,     1] loss: 792.819
[61,     1] loss: 797.553
[62,     1] loss: 813.127
[63,     1] loss: 830.486
[64,     1] loss: 737.570
[65,     1] loss: 821.047
[66,     1] loss: 745.263
[67,     1] loss: 780.550
[68,     1] loss: 716.895
[69,     1] loss: 728.239
[70,     1] loss: 684.981
[71,     1] loss: 686.428
[72,     1] loss: 697.801
[73,     1] loss: 1040.414
[74,     1] loss: 893.710
[75,     1] loss: 666.151
[76,     1] loss: 858.132
[77,     1] loss: 705.037
[78,     1] loss: 775.654
[79,     1] loss: 789.442
[80,     1] loss: 704.897
[81,     1] loss: 776.026
Early stopping applied (best metric=0.7420902252197266)
Finished Training
Total time taken: 13.187277793884277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1302.025
[2,     1] loss: 1302.960
[3,     1] loss: 1302.757
[4,     1] loss: 1300.488
[5,     1] loss: 1298.341
[6,     1] loss: 1288.365
[7,     1] loss: 1274.976
[8,     1] loss: 1238.989
[9,     1] loss: 1218.302
[10,     1] loss: 1178.419
[11,     1] loss: 1148.230
[12,     1] loss: 1116.736
[13,     1] loss: 1060.379
[14,     1] loss: 1075.046
[15,     1] loss: 1099.046
[16,     1] loss: 1058.621
[17,     1] loss: 1020.948
[18,     1] loss: 1082.004
[19,     1] loss: 1093.786
[20,     1] loss: 1008.936
[21,     1] loss: 1095.736
[22,     1] loss: 1022.780
[23,     1] loss: 1035.528
[24,     1] loss: 1029.123
[25,     1] loss: 1037.347
[26,     1] loss: 946.507
[27,     1] loss: 991.245
[28,     1] loss: 987.490
[29,     1] loss: 918.060
[30,     1] loss: 958.851
[31,     1] loss: 929.874
[32,     1] loss: 964.945
[33,     1] loss: 956.548
[34,     1] loss: 910.845
[35,     1] loss: 910.218
[36,     1] loss: 939.177
[37,     1] loss: 896.179
[38,     1] loss: 884.597
[39,     1] loss: 851.325
[40,     1] loss: 849.242
[41,     1] loss: 797.181
[42,     1] loss: 857.285
[43,     1] loss: 830.327
[44,     1] loss: 822.564
[45,     1] loss: 823.665
[46,     1] loss: 828.540
[47,     1] loss: 819.944
[48,     1] loss: 995.065
[49,     1] loss: 805.741
[50,     1] loss: 784.850
[51,     1] loss: 777.264
[52,     1] loss: 782.762
[53,     1] loss: 822.388
[54,     1] loss: 760.739
[55,     1] loss: 735.613
[56,     1] loss: 753.605
[57,     1] loss: 764.952
[58,     1] loss: 726.629
[59,     1] loss: 682.858
[60,     1] loss: 771.603
[61,     1] loss: 785.633
[62,     1] loss: 670.557
[63,     1] loss: 696.489
[64,     1] loss: 690.602
[65,     1] loss: 713.467
[66,     1] loss: 706.919
[67,     1] loss: 634.556
[68,     1] loss: 741.900
[69,     1] loss: 788.624
[70,     1] loss: 771.937
Early stopping applied (best metric=0.9097874164581299)
Finished Training
Total time taken: 9.301196098327637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.603
[2,     1] loss: 1301.284
[3,     1] loss: 1301.661
[4,     1] loss: 1301.084
[5,     1] loss: 1296.728
[6,     1] loss: 1302.937
[7,     1] loss: 1301.225
[8,     1] loss: 1296.089
[9,     1] loss: 1292.874
[10,     1] loss: 1289.288
[11,     1] loss: 1283.755
[12,     1] loss: 1274.739
[13,     1] loss: 1252.496
[14,     1] loss: 1227.722
[15,     1] loss: 1207.718
[16,     1] loss: 1167.468
[17,     1] loss: 1147.405
[18,     1] loss: 1116.846
[19,     1] loss: 1080.618
[20,     1] loss: 1086.596
[21,     1] loss: 1104.775
[22,     1] loss: 1049.880
[23,     1] loss: 1037.066
[24,     1] loss: 1010.151
[25,     1] loss: 1054.107
[26,     1] loss: 1017.132
[27,     1] loss: 1009.856
[28,     1] loss: 1030.930
[29,     1] loss: 1005.199
[30,     1] loss: 994.820
[31,     1] loss: 979.726
[32,     1] loss: 938.974
[33,     1] loss: 1014.052
[34,     1] loss: 922.014
[35,     1] loss: 971.399
[36,     1] loss: 976.879
[37,     1] loss: 918.254
[38,     1] loss: 939.861
[39,     1] loss: 902.802
[40,     1] loss: 963.226
[41,     1] loss: 912.691
[42,     1] loss: 969.678
[43,     1] loss: 925.394
[44,     1] loss: 884.260
[45,     1] loss: 883.137
[46,     1] loss: 877.837
[47,     1] loss: 869.757
[48,     1] loss: 878.827
[49,     1] loss: 878.157
[50,     1] loss: 862.180
[51,     1] loss: 907.941
[52,     1] loss: 813.609
[53,     1] loss: 884.042
[54,     1] loss: 828.678
[55,     1] loss: 852.947
[56,     1] loss: 828.650
[57,     1] loss: 775.327
[58,     1] loss: 768.298
[59,     1] loss: 853.898
[60,     1] loss: 766.977
[61,     1] loss: 822.900
[62,     1] loss: 797.269
[63,     1] loss: 732.291
[64,     1] loss: 775.267
[65,     1] loss: 691.043
[66,     1] loss: 720.335
[67,     1] loss: 682.795
[68,     1] loss: 627.269
[69,     1] loss: 703.649
[70,     1] loss: 667.165
[71,     1] loss: 693.346
[72,     1] loss: 694.101
[73,     1] loss: 703.491
[74,     1] loss: 682.783
[75,     1] loss: 660.820
[76,     1] loss: 712.854
[77,     1] loss: 669.812
[78,     1] loss: 787.434
[79,     1] loss: 635.769
[80,     1] loss: 773.995
[81,     1] loss: 740.041
[82,     1] loss: 673.227
[83,     1] loss: 737.141
[84,     1] loss: 599.784
[85,     1] loss: 636.698
[86,     1] loss: 596.714
[87,     1] loss: 623.832
[88,     1] loss: 639.929
[89,     1] loss: 594.275
[90,     1] loss: 675.446
[91,     1] loss: 693.013
[92,     1] loss: 549.302
[93,     1] loss: 632.635
[94,     1] loss: 581.210
[95,     1] loss: 519.702
[96,     1] loss: 518.516
Early stopping applied (best metric=0.7379520535469055)
Finished Training
Total time taken: 15.726332187652588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.390
[2,     1] loss: 1308.086
[3,     1] loss: 1304.135
[4,     1] loss: 1302.679
[5,     1] loss: 1301.200
[6,     1] loss: 1296.590
[7,     1] loss: 1296.207
[8,     1] loss: 1294.033
[9,     1] loss: 1277.822
[10,     1] loss: 1264.269
[11,     1] loss: 1235.137
[12,     1] loss: 1204.892
[13,     1] loss: 1149.513
[14,     1] loss: 1135.549
[15,     1] loss: 1099.029
[16,     1] loss: 1063.931
[17,     1] loss: 1050.695
[18,     1] loss: 1070.397
[19,     1] loss: 1034.546
[20,     1] loss: 1002.373
[21,     1] loss: 1094.083
[22,     1] loss: 1008.584
[23,     1] loss: 1025.013
[24,     1] loss: 1013.307
[25,     1] loss: 1012.895
[26,     1] loss: 981.355
[27,     1] loss: 958.035
[28,     1] loss: 926.260
[29,     1] loss: 926.675
[30,     1] loss: 952.335
[31,     1] loss: 903.825
[32,     1] loss: 919.858
[33,     1] loss: 1008.992
[34,     1] loss: 916.227
[35,     1] loss: 900.419
[36,     1] loss: 911.114
[37,     1] loss: 906.882
[38,     1] loss: 855.631
[39,     1] loss: 847.057
[40,     1] loss: 887.285
[41,     1] loss: 827.374
[42,     1] loss: 816.921
[43,     1] loss: 834.483
[44,     1] loss: 834.560
[45,     1] loss: 818.250
[46,     1] loss: 779.589
[47,     1] loss: 831.800
[48,     1] loss: 788.050
[49,     1] loss: 773.076
[50,     1] loss: 713.086
[51,     1] loss: 751.470
[52,     1] loss: 863.473
[53,     1] loss: 925.611
[54,     1] loss: 711.754
[55,     1] loss: 863.453
[56,     1] loss: 717.950
[57,     1] loss: 813.406
[58,     1] loss: 747.477
[59,     1] loss: 847.299
[60,     1] loss: 746.635
[61,     1] loss: 825.479
[62,     1] loss: 690.871
[63,     1] loss: 778.799
[64,     1] loss: 689.308
[65,     1] loss: 690.283
[66,     1] loss: 670.825
[67,     1] loss: 674.679
Early stopping applied (best metric=0.9788376092910767)
Finished Training
Total time taken: 8.959190368652344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.288
[2,     1] loss: 1301.271
[3,     1] loss: 1298.631
[4,     1] loss: 1298.359
[5,     1] loss: 1295.065
[6,     1] loss: 1293.086
[7,     1] loss: 1289.908
[8,     1] loss: 1269.467
[9,     1] loss: 1252.578
[10,     1] loss: 1215.838
[11,     1] loss: 1198.425
[12,     1] loss: 1132.982
[13,     1] loss: 1087.733
[14,     1] loss: 1090.735
[15,     1] loss: 1061.217
[16,     1] loss: 1180.461
[17,     1] loss: 1067.828
[18,     1] loss: 1104.325
[19,     1] loss: 1041.254
[20,     1] loss: 1032.594
[21,     1] loss: 1077.496
[22,     1] loss: 1069.144
[23,     1] loss: 1037.434
[24,     1] loss: 1047.351
[25,     1] loss: 1039.156
[26,     1] loss: 1028.616
[27,     1] loss: 1009.631
[28,     1] loss: 969.682
[29,     1] loss: 991.805
[30,     1] loss: 1010.251
[31,     1] loss: 970.573
[32,     1] loss: 979.158
[33,     1] loss: 962.393
[34,     1] loss: 996.951
[35,     1] loss: 968.418
[36,     1] loss: 895.169
[37,     1] loss: 949.367
[38,     1] loss: 913.970
[39,     1] loss: 949.809
[40,     1] loss: 871.436
[41,     1] loss: 982.550
[42,     1] loss: 892.742
[43,     1] loss: 957.750
[44,     1] loss: 870.710
[45,     1] loss: 974.988
[46,     1] loss: 907.225
[47,     1] loss: 873.963
[48,     1] loss: 896.114
[49,     1] loss: 821.534
[50,     1] loss: 863.689
[51,     1] loss: 807.816
[52,     1] loss: 839.158
[53,     1] loss: 802.156
[54,     1] loss: 801.452
[55,     1] loss: 861.492
[56,     1] loss: 817.135
[57,     1] loss: 773.765
[58,     1] loss: 888.001
[59,     1] loss: 763.193
[60,     1] loss: 890.145
[61,     1] loss: 769.215
[62,     1] loss: 809.966
[63,     1] loss: 734.141
[64,     1] loss: 718.827
[65,     1] loss: 773.033
[66,     1] loss: 679.328
[67,     1] loss: 711.881
[68,     1] loss: 673.528
[69,     1] loss: 637.754
[70,     1] loss: 682.791
[71,     1] loss: 674.467
[72,     1] loss: 920.779
[73,     1] loss: 1007.415
Early stopping applied (best metric=0.8367098569869995)
Finished Training
Total time taken: 12.045253038406372
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1298.868
[2,     1] loss: 1301.656
[3,     1] loss: 1300.009
[4,     1] loss: 1299.933
[5,     1] loss: 1292.462
[6,     1] loss: 1290.073
[7,     1] loss: 1278.632
[8,     1] loss: 1281.906
[9,     1] loss: 1231.091
[10,     1] loss: 1208.993
[11,     1] loss: 1170.475
[12,     1] loss: 1147.260
[13,     1] loss: 1148.303
[14,     1] loss: 1083.473
[15,     1] loss: 1094.845
[16,     1] loss: 1064.495
[17,     1] loss: 1086.961
[18,     1] loss: 1054.249
[19,     1] loss: 1068.099
[20,     1] loss: 982.538
[21,     1] loss: 1065.701
[22,     1] loss: 1017.857
[23,     1] loss: 997.608
[24,     1] loss: 956.287
[25,     1] loss: 980.411
[26,     1] loss: 988.724
[27,     1] loss: 998.860
[28,     1] loss: 936.004
[29,     1] loss: 969.356
[30,     1] loss: 970.840
[31,     1] loss: 884.815
[32,     1] loss: 916.039
[33,     1] loss: 910.780
[34,     1] loss: 907.094
[35,     1] loss: 848.054
[36,     1] loss: 930.328
[37,     1] loss: 855.718
[38,     1] loss: 903.917
[39,     1] loss: 861.154
[40,     1] loss: 813.960
[41,     1] loss: 848.835
[42,     1] loss: 911.465
[43,     1] loss: 865.489
[44,     1] loss: 874.143
[45,     1] loss: 785.000
[46,     1] loss: 807.409
[47,     1] loss: 853.531
[48,     1] loss: 889.191
Early stopping applied (best metric=0.8174640536308289)
Finished Training
Total time taken: 7.880165338516235
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1309.609
[2,     1] loss: 1302.190
[3,     1] loss: 1304.617
[4,     1] loss: 1301.899
[5,     1] loss: 1299.665
[6,     1] loss: 1299.394
[7,     1] loss: 1287.495
[8,     1] loss: 1279.704
[9,     1] loss: 1263.210
[10,     1] loss: 1235.503
[11,     1] loss: 1191.943
[12,     1] loss: 1171.021
[13,     1] loss: 1155.680
[14,     1] loss: 1128.937
[15,     1] loss: 1087.750
[16,     1] loss: 1122.276
[17,     1] loss: 1078.400
[18,     1] loss: 1073.142
[19,     1] loss: 1086.916
[20,     1] loss: 1026.166
[21,     1] loss: 1055.728
[22,     1] loss: 1040.997
[23,     1] loss: 1013.750
[24,     1] loss: 1004.426
[25,     1] loss: 1016.723
[26,     1] loss: 1015.342
[27,     1] loss: 972.116
[28,     1] loss: 1000.201
[29,     1] loss: 949.804
[30,     1] loss: 986.419
[31,     1] loss: 979.344
[32,     1] loss: 914.604
[33,     1] loss: 915.735
[34,     1] loss: 924.369
[35,     1] loss: 906.910
[36,     1] loss: 861.477
[37,     1] loss: 874.300
[38,     1] loss: 839.092
[39,     1] loss: 883.753
[40,     1] loss: 855.025
[41,     1] loss: 840.963
[42,     1] loss: 927.358
[43,     1] loss: 893.039
[44,     1] loss: 780.420
[45,     1] loss: 925.317
[46,     1] loss: 802.102
[47,     1] loss: 878.753
[48,     1] loss: 909.623
[49,     1] loss: 857.233
[50,     1] loss: 825.309
[51,     1] loss: 807.929
[52,     1] loss: 763.421
[53,     1] loss: 771.191
Early stopping applied (best metric=0.8541520833969116)
Finished Training
Total time taken: 7.086149454116821
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.093
[2,     1] loss: 1304.907
[3,     1] loss: 1301.735
[4,     1] loss: 1300.603
[5,     1] loss: 1296.633
[6,     1] loss: 1297.168
[7,     1] loss: 1288.420
[8,     1] loss: 1288.319
[9,     1] loss: 1267.847
[10,     1] loss: 1245.533
[11,     1] loss: 1207.358
[12,     1] loss: 1173.384
[13,     1] loss: 1149.051
[14,     1] loss: 1133.847
[15,     1] loss: 1138.446
[16,     1] loss: 1073.953
[17,     1] loss: 1096.622
[18,     1] loss: 1081.115
[19,     1] loss: 1094.578
[20,     1] loss: 1065.145
[21,     1] loss: 1059.231
[22,     1] loss: 1063.757
[23,     1] loss: 1062.144
[24,     1] loss: 1010.511
[25,     1] loss: 1045.897
[26,     1] loss: 1036.692
[27,     1] loss: 1046.615
[28,     1] loss: 967.238
[29,     1] loss: 1028.642
[30,     1] loss: 957.476
[31,     1] loss: 1023.637
[32,     1] loss: 952.316
[33,     1] loss: 1022.156
[34,     1] loss: 982.736
[35,     1] loss: 971.054
[36,     1] loss: 1014.201
[37,     1] loss: 925.731
[38,     1] loss: 954.578
[39,     1] loss: 932.271
[40,     1] loss: 907.577
[41,     1] loss: 885.147
[42,     1] loss: 871.067
[43,     1] loss: 846.215
[44,     1] loss: 845.379
[45,     1] loss: 794.668
[46,     1] loss: 836.752
[47,     1] loss: 799.775
[48,     1] loss: 780.746
[49,     1] loss: 786.435
[50,     1] loss: 992.778
[51,     1] loss: 1336.512
[52,     1] loss: 877.539
[53,     1] loss: 1055.075
[54,     1] loss: 1019.050
[55,     1] loss: 937.632
[56,     1] loss: 967.093
[57,     1] loss: 1033.709
[58,     1] loss: 1006.926
[59,     1] loss: 946.976
[60,     1] loss: 873.806
[61,     1] loss: 909.939
[62,     1] loss: 934.735
[63,     1] loss: 913.467
Early stopping applied (best metric=0.743282675743103)
Finished Training
Total time taken: 10.337220191955566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.382
[2,     1] loss: 1304.212
[3,     1] loss: 1302.758
[4,     1] loss: 1301.060
[5,     1] loss: 1296.414
[6,     1] loss: 1292.383
[7,     1] loss: 1285.618
[8,     1] loss: 1276.903
[9,     1] loss: 1252.611
[10,     1] loss: 1236.709
[11,     1] loss: 1201.080
[12,     1] loss: 1182.058
[13,     1] loss: 1158.433
[14,     1] loss: 1102.087
[15,     1] loss: 1108.464
[16,     1] loss: 1100.950
[17,     1] loss: 1088.137
[18,     1] loss: 1076.974
[19,     1] loss: 1061.775
[20,     1] loss: 1049.263
[21,     1] loss: 1061.445
[22,     1] loss: 1036.985
[23,     1] loss: 1033.663
[24,     1] loss: 998.856
[25,     1] loss: 1081.227
[26,     1] loss: 1030.933
[27,     1] loss: 1020.028
[28,     1] loss: 994.111
[29,     1] loss: 995.219
[30,     1] loss: 1012.764
[31,     1] loss: 962.375
[32,     1] loss: 963.134
[33,     1] loss: 944.052
[34,     1] loss: 908.995
[35,     1] loss: 884.891
[36,     1] loss: 915.794
[37,     1] loss: 873.833
[38,     1] loss: 852.198
[39,     1] loss: 862.714
[40,     1] loss: 951.133
[41,     1] loss: 897.415
[42,     1] loss: 921.054
[43,     1] loss: 828.218
[44,     1] loss: 808.345
[45,     1] loss: 782.173
[46,     1] loss: 846.530
[47,     1] loss: 794.295
[48,     1] loss: 752.961
[49,     1] loss: 799.581
[50,     1] loss: 815.108
[51,     1] loss: 1004.213
[52,     1] loss: 1011.714
[53,     1] loss: 799.544
[54,     1] loss: 979.914
[55,     1] loss: 864.093
[56,     1] loss: 856.069
[57,     1] loss: 944.519
[58,     1] loss: 863.462
[59,     1] loss: 839.904
[60,     1] loss: 870.980
[61,     1] loss: 779.533
[62,     1] loss: 731.542
[63,     1] loss: 900.024
[64,     1] loss: 780.365
[65,     1] loss: 797.139
[66,     1] loss: 687.834
[67,     1] loss: 715.000
[68,     1] loss: 688.085
[69,     1] loss: 667.621
[70,     1] loss: 667.146
[71,     1] loss: 665.056
[72,     1] loss: 683.070
[73,     1] loss: 615.392
[74,     1] loss: 649.987
[75,     1] loss: 673.112
[76,     1] loss: 716.665
[77,     1] loss: 757.235
[78,     1] loss: 670.521
[79,     1] loss: 615.414
[80,     1] loss: 618.637
[81,     1] loss: 657.696
Early stopping applied (best metric=0.7764842510223389)
Finished Training
Total time taken: 10.796226263046265
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.344
[2,     1] loss: 1302.627
[3,     1] loss: 1305.182
[4,     1] loss: 1301.204
[5,     1] loss: 1302.375
[6,     1] loss: 1296.771
[7,     1] loss: 1301.672
[8,     1] loss: 1298.460
[9,     1] loss: 1290.395
[10,     1] loss: 1287.052
[11,     1] loss: 1276.581
[12,     1] loss: 1256.420
[13,     1] loss: 1234.827
[14,     1] loss: 1204.462
[15,     1] loss: 1173.088
[16,     1] loss: 1138.169
[17,     1] loss: 1101.324
[18,     1] loss: 1058.606
[19,     1] loss: 1091.124
[20,     1] loss: 1067.150
[21,     1] loss: 1036.910
[22,     1] loss: 1041.416
[23,     1] loss: 999.667
[24,     1] loss: 1039.425
[25,     1] loss: 1027.285
[26,     1] loss: 1075.793
[27,     1] loss: 1032.480
[28,     1] loss: 1000.217
[29,     1] loss: 966.969
[30,     1] loss: 958.959
[31,     1] loss: 965.293
[32,     1] loss: 918.703
[33,     1] loss: 1012.122
[34,     1] loss: 899.661
[35,     1] loss: 923.228
[36,     1] loss: 893.756
[37,     1] loss: 874.558
Early stopping applied (best metric=1.0264333486557007)
Finished Training
Total time taken: 6.39513635635376
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.861
[2,     1] loss: 1299.536
[3,     1] loss: 1299.728
[4,     1] loss: 1299.528
[5,     1] loss: 1300.838
[6,     1] loss: 1295.514
[7,     1] loss: 1301.191
[8,     1] loss: 1297.046
[9,     1] loss: 1297.431
[10,     1] loss: 1288.964
[11,     1] loss: 1288.619
[12,     1] loss: 1277.553
[13,     1] loss: 1259.266
[14,     1] loss: 1229.815
[15,     1] loss: 1202.042
[16,     1] loss: 1175.877
[17,     1] loss: 1150.373
[18,     1] loss: 1126.497
[19,     1] loss: 1079.964
[20,     1] loss: 1075.131
[21,     1] loss: 1066.293
[22,     1] loss: 1160.835
[23,     1] loss: 1101.546
[24,     1] loss: 1129.479
[25,     1] loss: 1055.876
[26,     1] loss: 1032.872
[27,     1] loss: 1046.111
[28,     1] loss: 1046.549
[29,     1] loss: 1019.379
[30,     1] loss: 1049.290
[31,     1] loss: 1010.776
[32,     1] loss: 962.458
[33,     1] loss: 1008.920
[34,     1] loss: 937.078
[35,     1] loss: 961.864
[36,     1] loss: 918.121
[37,     1] loss: 970.554
[38,     1] loss: 944.178
[39,     1] loss: 913.857
[40,     1] loss: 869.135
[41,     1] loss: 951.567
[42,     1] loss: 933.321
[43,     1] loss: 860.371
[44,     1] loss: 915.952
[45,     1] loss: 840.774
[46,     1] loss: 872.158
[47,     1] loss: 843.045
[48,     1] loss: 853.488
[49,     1] loss: 891.847
[50,     1] loss: 776.034
[51,     1] loss: 812.186
[52,     1] loss: 804.713
[53,     1] loss: 722.980
[54,     1] loss: 731.562
[55,     1] loss: 768.451
[56,     1] loss: 978.174
[57,     1] loss: 1228.214
[58,     1] loss: 764.999
[59,     1] loss: 985.221
[60,     1] loss: 960.553
[61,     1] loss: 855.284
[62,     1] loss: 933.707
Early stopping applied (best metric=0.7453427314758301)
Finished Training
Total time taken: 9.43919849395752
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1304.152
[2,     1] loss: 1300.758
[3,     1] loss: 1301.448
[4,     1] loss: 1301.055
[5,     1] loss: 1297.212
[6,     1] loss: 1294.243
[7,     1] loss: 1285.163
[8,     1] loss: 1265.609
[9,     1] loss: 1242.031
[10,     1] loss: 1197.891
[11,     1] loss: 1162.521
[12,     1] loss: 1140.164
[13,     1] loss: 1071.343
[14,     1] loss: 1091.077
[15,     1] loss: 1088.726
[16,     1] loss: 1043.590
[17,     1] loss: 1034.690
[18,     1] loss: 996.719
[19,     1] loss: 1035.717
[20,     1] loss: 1033.701
[21,     1] loss: 1042.123
[22,     1] loss: 1013.212
[23,     1] loss: 986.590
[24,     1] loss: 999.169
[25,     1] loss: 921.081
[26,     1] loss: 993.665
[27,     1] loss: 978.476
[28,     1] loss: 958.718
[29,     1] loss: 1027.086
[30,     1] loss: 888.399
[31,     1] loss: 1018.521
[32,     1] loss: 907.196
[33,     1] loss: 929.358
[34,     1] loss: 969.859
[35,     1] loss: 886.130
[36,     1] loss: 924.643
[37,     1] loss: 925.154
[38,     1] loss: 879.470
[39,     1] loss: 914.545
[40,     1] loss: 853.732
[41,     1] loss: 864.208
[42,     1] loss: 849.979
[43,     1] loss: 829.050
[44,     1] loss: 793.265
[45,     1] loss: 783.742
[46,     1] loss: 781.013
[47,     1] loss: 763.785
[48,     1] loss: 795.366
[49,     1] loss: 696.007
[50,     1] loss: 840.883
[51,     1] loss: 885.674
Early stopping applied (best metric=0.9140868186950684)
Finished Training
Total time taken: 8.050171136856079
{'Hydroxylation-K Validation Accuracy': 0.736790780141844, 'Hydroxylation-K Validation Sensitivity': 0.6251851851851852, 'Hydroxylation-K Validation Specificity': 0.7649122807017544, 'Hydroxylation-K Validation Precision': 0.40118939339527576, 'Hydroxylation-K AUC ROC': 0.7701754385964912, 'Hydroxylation-K AUC PR': 0.5294475507635968, 'Hydroxylation-K MCC': 0.33737411590757377, 'Hydroxylation-K F1': 0.48639409329264405, 'Validation Loss (Hydroxylation-K)': 0.4659379740556081, 'Hydroxylation-P Validation Accuracy': 0.7912760604368645, 'Hydroxylation-P Validation Sensitivity': 0.7706878306878308, 'Hydroxylation-P Validation Specificity': 0.7957030275824231, 'Hydroxylation-P Validation Precision': 0.45935348229687023, 'Hydroxylation-P AUC ROC': 0.8419508078224224, 'Hydroxylation-P AUC PR': 0.5786065321545708, 'Hydroxylation-P MCC': 0.4761619524519963, 'Hydroxylation-P F1': 0.5718820886049651, 'Validation Loss (Hydroxylation-P)': 0.3819093863169352, 'Validation Loss (total)': 0.8478473663330078, 'TimeToTrain': 9.857608032226562}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008204603941274041,
 'learning_rate_Hydroxylation-K': 0.001724599788883658,
 'learning_rate_Hydroxylation-P': 0.005138076465858557,
 'log_base': 1.725860594384229,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1526497325,
 'sample_weights': [1.863029564404651, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2669341182644116,
 'weight_decay_Hydroxylation-K': 4.758501472088982,
 'weight_decay_Hydroxylation-P': 4.876649137573137}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1553.405
[2,     1] loss: 1562.571
[3,     1] loss: 1562.988
[4,     1] loss: 1556.945
[5,     1] loss: 1552.456
[6,     1] loss: 1559.309
[7,     1] loss: 1552.033
[8,     1] loss: 1557.892
[9,     1] loss: 1551.115
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031055481297164013,
 'learning_rate_Hydroxylation-K': 0.0015463232625427487,
 'learning_rate_Hydroxylation-P': 0.006159644409148225,
 'log_base': 2.1884008504382884,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2757336770,
 'sample_weights': [3.059124347877908, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.817281746928722,
 'weight_decay_Hydroxylation-K': 2.7267839795237956,
 'weight_decay_Hydroxylation-P': 0.4068506057442094}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1356.329
[2,     1] loss: 1363.458
[3,     1] loss: 1358.790
[4,     1] loss: 1354.458
[5,     1] loss: 1359.004
[6,     1] loss: 1354.247
[7,     1] loss: 1354.402
[8,     1] loss: 1350.270
[9,     1] loss: 1339.259
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004091743127575147,
 'learning_rate_Hydroxylation-K': 0.008709421245107409,
 'learning_rate_Hydroxylation-P': 0.0006275695822125835,
 'log_base': 1.8363049436694625,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1754700510,
 'sample_weights': [2.1316455720784466, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.2945747830201055,
 'weight_decay_Hydroxylation-K': 3.504949535184222,
 'weight_decay_Hydroxylation-P': 8.045223750552598}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1500.622
[2,     1] loss: 1485.815
[3,     1] loss: 1482.976
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002537355557907017,
 'learning_rate_Hydroxylation-K': 0.009180302108333727,
 'learning_rate_Hydroxylation-P': 0.008118245225217778,
 'log_base': 2.6838899820797293,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 416275170,
 'sample_weights': [2.7468998721732625, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.249292638675183,
 'weight_decay_Hydroxylation-K': 9.66222563688695,
 'weight_decay_Hydroxylation-P': 5.1214807419842066}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.472
[2,     1] loss: 1265.170
[3,     1] loss: 1265.063
[4,     1] loss: 1261.281
[5,     1] loss: 1262.385
[6,     1] loss: 1262.339
[7,     1] loss: 1260.468
[8,     1] loss: 1258.786
[9,     1] loss: 1251.344
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007423335512171075,
 'learning_rate_Hydroxylation-K': 0.0026440172948527436,
 'learning_rate_Hydroxylation-P': 0.00436188034735622,
 'log_base': 1.7063381768189627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2429716958,
 'sample_weights': [1.690973932994017, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.8005439107545587,
 'weight_decay_Hydroxylation-K': 6.479461314762693,
 'weight_decay_Hydroxylation-P': 5.089171896101941}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1572.042
[2,     1] loss: 1573.644
[3,     1] loss: 1569.983
[4,     1] loss: 1581.250
[5,     1] loss: 1570.958
[6,     1] loss: 1568.720
[7,     1] loss: 1565.354
[8,     1] loss: 1567.203
[9,     1] loss: 1567.259
[10,     1] loss: 1561.893
[11,     1] loss: 1560.483
[12,     1] loss: 1562.729
[13,     1] loss: 1564.833
[14,     1] loss: 1562.949
[15,     1] loss: 1556.089
[16,     1] loss: 1561.820
[17,     1] loss: 1563.206
[18,     1] loss: 1555.389
[19,     1] loss: 1557.603
[20,     1] loss: 1538.679
[21,     1] loss: 1535.784
[22,     1] loss: 1525.282
[23,     1] loss: 1508.373
[24,     1] loss: 1488.776
[25,     1] loss: 1452.752
[26,     1] loss: 1427.056
[27,     1] loss: 1395.321
[28,     1] loss: 1399.802
[29,     1] loss: 1318.914
[30,     1] loss: 1395.085
[31,     1] loss: 1325.804
[32,     1] loss: 1315.823
[33,     1] loss: 1332.822
[34,     1] loss: 1366.836
[35,     1] loss: 1277.855
[36,     1] loss: 1297.735
[37,     1] loss: 1294.980
[38,     1] loss: 1364.024
[39,     1] loss: 1267.027
[40,     1] loss: 1264.616
[41,     1] loss: 1270.284
[42,     1] loss: 1269.122
[43,     1] loss: 1233.971
[44,     1] loss: 1257.760
[45,     1] loss: 1228.779
[46,     1] loss: 1250.416
[47,     1] loss: 1196.654
[48,     1] loss: 1211.360
[49,     1] loss: 1192.629
[50,     1] loss: 1183.756
[51,     1] loss: 1276.574
[52,     1] loss: 1233.750
[53,     1] loss: 1157.886
[54,     1] loss: 1184.662
[55,     1] loss: 1154.497
[56,     1] loss: 1172.194
[57,     1] loss: 1171.353
[58,     1] loss: 1138.204
[59,     1] loss: 1144.829
[60,     1] loss: 1099.862
[61,     1] loss: 1136.057
[62,     1] loss: 1157.286
[63,     1] loss: 1127.267
[64,     1] loss: 1137.905
[65,     1] loss: 1118.606
[66,     1] loss: 1118.233
[67,     1] loss: 1102.066
[68,     1] loss: 1154.883
[69,     1] loss: 1113.204
[70,     1] loss: 1082.379
[71,     1] loss: 1139.122
[72,     1] loss: 1073.287
[73,     1] loss: 1094.384
[74,     1] loss: 1098.218
[75,     1] loss: 1073.468
[76,     1] loss: 1110.595
[77,     1] loss: 1060.084
[78,     1] loss: 1085.743
[79,     1] loss: 1093.772
[80,     1] loss: 1051.784
[81,     1] loss: 1031.434
[82,     1] loss: 1072.059
[83,     1] loss: 1084.456
[84,     1] loss: 1057.766
[85,     1] loss: 1047.156
[86,     1] loss: 1059.606
[87,     1] loss: 989.135
[88,     1] loss: 1063.498
[89,     1] loss: 985.212
[90,     1] loss: 1033.135
[91,     1] loss: 1020.606
[92,     1] loss: 1008.444
[93,     1] loss: 948.015
[94,     1] loss: 955.603
[95,     1] loss: 957.142
[96,     1] loss: 956.369
[97,     1] loss: 963.919
[98,     1] loss: 1002.167
[99,     1] loss: 988.323
[100,     1] loss: 914.095
[101,     1] loss: 936.836
[102,     1] loss: 897.846
[103,     1] loss: 871.157
[104,     1] loss: 858.915
[105,     1] loss: 826.826
[106,     1] loss: 833.437
Early stopping applied (best metric=0.8416227102279663)
Finished Training
Total time taken: 18.069383144378662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.259
[2,     1] loss: 1565.500
[3,     1] loss: 1571.975
[4,     1] loss: 1570.596
[5,     1] loss: 1566.648
[6,     1] loss: 1566.513
[7,     1] loss: 1564.314
[8,     1] loss: 1561.706
[9,     1] loss: 1569.041
[10,     1] loss: 1569.199
[11,     1] loss: 1560.542
[12,     1] loss: 1561.867
[13,     1] loss: 1559.596
[14,     1] loss: 1556.009
[15,     1] loss: 1554.516
[16,     1] loss: 1553.876
[17,     1] loss: 1547.204
[18,     1] loss: 1545.981
[19,     1] loss: 1520.703
[20,     1] loss: 1513.458
[21,     1] loss: 1460.033
[22,     1] loss: 1414.255
[23,     1] loss: 1380.428
[24,     1] loss: 1355.868
[25,     1] loss: 1411.651
[26,     1] loss: 1360.481
[27,     1] loss: 1333.143
[28,     1] loss: 1300.565
[29,     1] loss: 1365.229
[30,     1] loss: 1353.262
[31,     1] loss: 1264.139
[32,     1] loss: 1305.634
[33,     1] loss: 1267.386
[34,     1] loss: 1296.108
[35,     1] loss: 1218.524
[36,     1] loss: 1200.104
[37,     1] loss: 1241.132
[38,     1] loss: 1315.586
[39,     1] loss: 1277.287
[40,     1] loss: 1236.879
[41,     1] loss: 1236.271
[42,     1] loss: 1240.866
[43,     1] loss: 1231.051
[44,     1] loss: 1240.807
[45,     1] loss: 1175.420
[46,     1] loss: 1239.548
[47,     1] loss: 1185.753
[48,     1] loss: 1173.284
[49,     1] loss: 1225.210
[50,     1] loss: 1222.240
[51,     1] loss: 1181.882
[52,     1] loss: 1172.211
[53,     1] loss: 1151.257
[54,     1] loss: 1197.412
[55,     1] loss: 1128.149
[56,     1] loss: 1158.159
[57,     1] loss: 1124.842
[58,     1] loss: 1173.872
[59,     1] loss: 1098.238
[60,     1] loss: 1114.664
[61,     1] loss: 1089.417
[62,     1] loss: 1122.212
[63,     1] loss: 1083.398
[64,     1] loss: 1025.985
Early stopping applied (best metric=0.927886962890625)
Finished Training
Total time taken: 8.880187273025513
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1566.148
[2,     1] loss: 1565.952
[3,     1] loss: 1568.280
[4,     1] loss: 1567.892
[5,     1] loss: 1564.122
[6,     1] loss: 1560.177
[7,     1] loss: 1560.561
[8,     1] loss: 1552.473
[9,     1] loss: 1545.601
[10,     1] loss: 1530.856
[11,     1] loss: 1515.448
[12,     1] loss: 1491.519
[13,     1] loss: 1452.798
[14,     1] loss: 1402.978
[15,     1] loss: 1362.987
[16,     1] loss: 1332.887
[17,     1] loss: 1408.542
[18,     1] loss: 1320.002
[19,     1] loss: 1330.051
[20,     1] loss: 1343.292
[21,     1] loss: 1339.537
[22,     1] loss: 1318.156
[23,     1] loss: 1312.670
[24,     1] loss: 1264.889
[25,     1] loss: 1314.135
[26,     1] loss: 1264.390
[27,     1] loss: 1274.077
[28,     1] loss: 1317.047
[29,     1] loss: 1316.019
[30,     1] loss: 1250.860
[31,     1] loss: 1266.676
[32,     1] loss: 1279.507
[33,     1] loss: 1273.166
[34,     1] loss: 1238.468
[35,     1] loss: 1236.461
[36,     1] loss: 1211.082
[37,     1] loss: 1218.540
[38,     1] loss: 1206.398
[39,     1] loss: 1234.014
[40,     1] loss: 1190.459
[41,     1] loss: 1244.891
[42,     1] loss: 1202.514
[43,     1] loss: 1169.848
[44,     1] loss: 1178.665
[45,     1] loss: 1193.777
[46,     1] loss: 1172.828
[47,     1] loss: 1171.764
[48,     1] loss: 1256.388
[49,     1] loss: 1192.735
[50,     1] loss: 1096.470
[51,     1] loss: 1144.736
[52,     1] loss: 1159.613
[53,     1] loss: 1175.581
[54,     1] loss: 1188.590
[55,     1] loss: 1126.341
[56,     1] loss: 1113.624
[57,     1] loss: 1195.103
[58,     1] loss: 1145.088
[59,     1] loss: 1089.489
[60,     1] loss: 1143.748
[61,     1] loss: 1140.787
[62,     1] loss: 1127.071
[63,     1] loss: 1093.857
[64,     1] loss: 1067.136
[65,     1] loss: 1047.321
[66,     1] loss: 1130.010
[67,     1] loss: 1036.010
[68,     1] loss: 1069.238
[69,     1] loss: 1103.004
[70,     1] loss: 1117.103
[71,     1] loss: 1109.172
[72,     1] loss: 1084.367
[73,     1] loss: 1011.685
[74,     1] loss: 1049.180
Early stopping applied (best metric=0.7876598238945007)
Finished Training
Total time taken: 12.286258935928345
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1571.073
[2,     1] loss: 1567.733
[3,     1] loss: 1570.649
[4,     1] loss: 1569.669
[5,     1] loss: 1563.700
[6,     1] loss: 1567.394
[7,     1] loss: 1565.632
[8,     1] loss: 1566.270
[9,     1] loss: 1569.541
[10,     1] loss: 1569.892
[11,     1] loss: 1565.126
[12,     1] loss: 1560.411
[13,     1] loss: 1556.528
[14,     1] loss: 1552.174
[15,     1] loss: 1549.204
[16,     1] loss: 1544.694
[17,     1] loss: 1529.582
[18,     1] loss: 1519.733
[19,     1] loss: 1494.014
[20,     1] loss: 1472.451
[21,     1] loss: 1435.152
[22,     1] loss: 1434.085
[23,     1] loss: 1401.330
[24,     1] loss: 1360.771
[25,     1] loss: 1358.246
[26,     1] loss: 1390.770
[27,     1] loss: 1380.408
[28,     1] loss: 1306.065
[29,     1] loss: 1239.456
[30,     1] loss: 1334.798
[31,     1] loss: 1308.726
[32,     1] loss: 1259.571
[33,     1] loss: 1327.040
[34,     1] loss: 1263.237
[35,     1] loss: 1295.235
[36,     1] loss: 1320.471
[37,     1] loss: 1234.890
[38,     1] loss: 1300.455
[39,     1] loss: 1273.646
[40,     1] loss: 1305.542
[41,     1] loss: 1239.200
[42,     1] loss: 1234.875
[43,     1] loss: 1296.269
[44,     1] loss: 1219.928
[45,     1] loss: 1230.045
[46,     1] loss: 1209.127
[47,     1] loss: 1228.765
[48,     1] loss: 1243.680
[49,     1] loss: 1257.000
[50,     1] loss: 1195.224
[51,     1] loss: 1248.432
[52,     1] loss: 1215.768
[53,     1] loss: 1172.723
[54,     1] loss: 1222.030
[55,     1] loss: 1240.521
[56,     1] loss: 1231.085
[57,     1] loss: 1219.333
[58,     1] loss: 1160.740
[59,     1] loss: 1281.913
[60,     1] loss: 1202.011
[61,     1] loss: 1214.747
[62,     1] loss: 1262.033
[63,     1] loss: 1196.254
[64,     1] loss: 1170.743
[65,     1] loss: 1145.398
[66,     1] loss: 1208.754
[67,     1] loss: 1119.885
[68,     1] loss: 1143.835
[69,     1] loss: 1186.825
[70,     1] loss: 1183.673
[71,     1] loss: 1153.458
[72,     1] loss: 1071.371
[73,     1] loss: 1225.253
[74,     1] loss: 1105.697
[75,     1] loss: 1172.556
[76,     1] loss: 1153.671
[77,     1] loss: 1152.809
[78,     1] loss: 1185.963
[79,     1] loss: 1145.511
[80,     1] loss: 1120.585
[81,     1] loss: 1093.275
[82,     1] loss: 1126.885
[83,     1] loss: 1131.552
[84,     1] loss: 1085.392
[85,     1] loss: 1066.613
[86,     1] loss: 1090.209
[87,     1] loss: 1080.703
[88,     1] loss: 1058.537
[89,     1] loss: 1040.873
[90,     1] loss: 1012.037
[91,     1] loss: 991.588
[92,     1] loss: 1018.454
[93,     1] loss: 997.978
[94,     1] loss: 968.880
[95,     1] loss: 1018.718
[96,     1] loss: 938.855
[97,     1] loss: 1014.622
[98,     1] loss: 960.060
[99,     1] loss: 932.271
[100,     1] loss: 981.243
[101,     1] loss: 930.682
[102,     1] loss: 884.561
[103,     1] loss: 954.056
[104,     1] loss: 894.696
[105,     1] loss: 919.483
[106,     1] loss: 895.136
[107,     1] loss: 844.431
[108,     1] loss: 869.089
[109,     1] loss: 791.663
[110,     1] loss: 806.246
[111,     1] loss: 806.569
Early stopping applied (best metric=0.6823869943618774)
Finished Training
Total time taken: 15.177321434020996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1567.480
[2,     1] loss: 1580.167
[3,     1] loss: 1571.942
[4,     1] loss: 1573.550
[5,     1] loss: 1568.676
[6,     1] loss: 1565.570
[7,     1] loss: 1566.168
[8,     1] loss: 1566.969
[9,     1] loss: 1562.400
[10,     1] loss: 1555.179
[11,     1] loss: 1547.239
[12,     1] loss: 1545.402
[13,     1] loss: 1534.575
[14,     1] loss: 1531.670
[15,     1] loss: 1491.867
[16,     1] loss: 1465.197
[17,     1] loss: 1428.360
[18,     1] loss: 1443.926
[19,     1] loss: 1384.220
[20,     1] loss: 1421.434
[21,     1] loss: 1409.286
[22,     1] loss: 1337.403
[23,     1] loss: 1399.373
[24,     1] loss: 1336.278
[25,     1] loss: 1348.650
[26,     1] loss: 1342.957
[27,     1] loss: 1343.836
[28,     1] loss: 1348.490
[29,     1] loss: 1339.897
[30,     1] loss: 1303.545
[31,     1] loss: 1299.968
[32,     1] loss: 1295.001
[33,     1] loss: 1229.177
[34,     1] loss: 1260.797
[35,     1] loss: 1266.688
[36,     1] loss: 1311.556
[37,     1] loss: 1289.727
[38,     1] loss: 1278.117
[39,     1] loss: 1217.999
[40,     1] loss: 1239.967
[41,     1] loss: 1278.283
[42,     1] loss: 1175.891
[43,     1] loss: 1227.524
[44,     1] loss: 1206.278
[45,     1] loss: 1216.890
[46,     1] loss: 1211.947
[47,     1] loss: 1144.208
[48,     1] loss: 1172.001
[49,     1] loss: 1138.786
[50,     1] loss: 1241.579
[51,     1] loss: 1235.066
[52,     1] loss: 1181.377
[53,     1] loss: 1097.811
[54,     1] loss: 1128.274
[55,     1] loss: 1149.672
[56,     1] loss: 1232.060
[57,     1] loss: 1099.474
[58,     1] loss: 1132.485
[59,     1] loss: 1098.724
[60,     1] loss: 1136.796
[61,     1] loss: 1185.217
[62,     1] loss: 1098.430
[63,     1] loss: 1105.550
[64,     1] loss: 1122.350
[65,     1] loss: 1098.735
[66,     1] loss: 1104.875
[67,     1] loss: 1039.389
[68,     1] loss: 1071.242
[69,     1] loss: 1162.952
[70,     1] loss: 1070.794
[71,     1] loss: 953.390
[72,     1] loss: 1082.132
[73,     1] loss: 1064.001
[74,     1] loss: 1080.546
[75,     1] loss: 1018.111
[76,     1] loss: 995.634
[77,     1] loss: 957.785
Early stopping applied (best metric=0.7527635097503662)
Finished Training
Total time taken: 10.960232973098755
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1566.773
[2,     1] loss: 1577.794
[3,     1] loss: 1565.339
[4,     1] loss: 1561.033
[5,     1] loss: 1569.956
[6,     1] loss: 1566.300
[7,     1] loss: 1571.707
[8,     1] loss: 1566.208
[9,     1] loss: 1570.341
[10,     1] loss: 1566.297
[11,     1] loss: 1563.837
[12,     1] loss: 1563.633
[13,     1] loss: 1565.722
[14,     1] loss: 1567.258
[15,     1] loss: 1561.360
[16,     1] loss: 1560.958
[17,     1] loss: 1556.892
[18,     1] loss: 1558.185
[19,     1] loss: 1556.887
[20,     1] loss: 1546.663
[21,     1] loss: 1540.698
[22,     1] loss: 1532.738
[23,     1] loss: 1503.862
[24,     1] loss: 1488.997
[25,     1] loss: 1453.854
[26,     1] loss: 1441.679
[27,     1] loss: 1388.021
[28,     1] loss: 1406.657
[29,     1] loss: 1448.099
[30,     1] loss: 1323.045
[31,     1] loss: 1346.938
[32,     1] loss: 1327.025
[33,     1] loss: 1325.583
[34,     1] loss: 1326.831
[35,     1] loss: 1323.144
[36,     1] loss: 1375.610
[37,     1] loss: 1307.195
[38,     1] loss: 1357.620
[39,     1] loss: 1314.811
[40,     1] loss: 1341.581
[41,     1] loss: 1289.333
[42,     1] loss: 1281.836
[43,     1] loss: 1301.035
[44,     1] loss: 1280.116
[45,     1] loss: 1268.522
[46,     1] loss: 1275.060
[47,     1] loss: 1247.868
[48,     1] loss: 1252.848
[49,     1] loss: 1244.165
[50,     1] loss: 1238.130
[51,     1] loss: 1224.101
[52,     1] loss: 1254.281
[53,     1] loss: 1218.149
[54,     1] loss: 1256.187
[55,     1] loss: 1212.709
[56,     1] loss: 1169.819
[57,     1] loss: 1231.388
[58,     1] loss: 1214.787
[59,     1] loss: 1203.026
[60,     1] loss: 1186.689
[61,     1] loss: 1179.708
[62,     1] loss: 1214.274
[63,     1] loss: 1143.281
[64,     1] loss: 1169.550
[65,     1] loss: 1175.100
[66,     1] loss: 1183.024
[67,     1] loss: 1117.945
[68,     1] loss: 1142.823
[69,     1] loss: 1100.897
[70,     1] loss: 1142.529
[71,     1] loss: 1081.325
[72,     1] loss: 1095.757
[73,     1] loss: 1147.530
[74,     1] loss: 1128.522
[75,     1] loss: 1161.524
[76,     1] loss: 1138.961
[77,     1] loss: 1145.830
[78,     1] loss: 1135.302
[79,     1] loss: 1077.071
[80,     1] loss: 1143.078
[81,     1] loss: 1093.354
[82,     1] loss: 1064.967
[83,     1] loss: 1078.036
[84,     1] loss: 1088.008
[85,     1] loss: 1137.168
[86,     1] loss: 1117.131
[87,     1] loss: 1052.911
[88,     1] loss: 1074.585
Early stopping applied (best metric=0.7782607078552246)
Finished Training
Total time taken: 13.430418729782104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1563.820
[2,     1] loss: 1563.461
[3,     1] loss: 1565.941
[4,     1] loss: 1562.076
[5,     1] loss: 1569.837
[6,     1] loss: 1573.348
[7,     1] loss: 1562.175
[8,     1] loss: 1564.864
[9,     1] loss: 1550.869
[10,     1] loss: 1558.228
[11,     1] loss: 1542.604
[12,     1] loss: 1537.709
[13,     1] loss: 1543.250
[14,     1] loss: 1530.103
[15,     1] loss: 1520.110
[16,     1] loss: 1488.783
[17,     1] loss: 1497.443
[18,     1] loss: 1448.742
[19,     1] loss: 1405.783
[20,     1] loss: 1378.923
[21,     1] loss: 1399.462
[22,     1] loss: 1346.764
[23,     1] loss: 1361.136
[24,     1] loss: 1369.553
[25,     1] loss: 1410.315
[26,     1] loss: 1278.166
[27,     1] loss: 1349.919
[28,     1] loss: 1320.662
[29,     1] loss: 1284.876
[30,     1] loss: 1342.955
[31,     1] loss: 1326.051
[32,     1] loss: 1297.531
[33,     1] loss: 1263.518
[34,     1] loss: 1252.354
[35,     1] loss: 1234.635
[36,     1] loss: 1297.031
[37,     1] loss: 1327.925
[38,     1] loss: 1241.522
[39,     1] loss: 1225.759
[40,     1] loss: 1214.964
[41,     1] loss: 1231.857
[42,     1] loss: 1194.870
[43,     1] loss: 1171.806
[44,     1] loss: 1236.723
[45,     1] loss: 1163.022
[46,     1] loss: 1244.520
[47,     1] loss: 1130.125
[48,     1] loss: 1131.861
[49,     1] loss: 1219.098
[50,     1] loss: 1203.532
[51,     1] loss: 1105.472
[52,     1] loss: 1116.070
[53,     1] loss: 1131.734
[54,     1] loss: 1099.879
[55,     1] loss: 1063.918
[56,     1] loss: 1076.550
[57,     1] loss: 1092.585
[58,     1] loss: 1098.694
[59,     1] loss: 1052.001
[60,     1] loss: 1009.677
[61,     1] loss: 1051.252
[62,     1] loss: 1091.729
[63,     1] loss: 1016.961
[64,     1] loss: 1018.883
[65,     1] loss: 1114.945
[66,     1] loss: 990.418
[67,     1] loss: 1152.524
[68,     1] loss: 1010.165
Early stopping applied (best metric=0.6815736889839172)
Finished Training
Total time taken: 10.700612306594849
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.028
[2,     1] loss: 1573.696
[3,     1] loss: 1570.972
[4,     1] loss: 1569.005
[5,     1] loss: 1565.360
[6,     1] loss: 1567.266
[7,     1] loss: 1569.144
[8,     1] loss: 1567.559
[9,     1] loss: 1569.865
[10,     1] loss: 1566.280
[11,     1] loss: 1566.290
[12,     1] loss: 1563.461
[13,     1] loss: 1569.149
[14,     1] loss: 1567.357
[15,     1] loss: 1566.844
[16,     1] loss: 1565.713
[17,     1] loss: 1562.947
[18,     1] loss: 1566.815
[19,     1] loss: 1563.632
[20,     1] loss: 1562.152
[21,     1] loss: 1558.265
[22,     1] loss: 1559.929
[23,     1] loss: 1554.662
[24,     1] loss: 1554.491
[25,     1] loss: 1545.687
[26,     1] loss: 1530.837
[27,     1] loss: 1517.528
[28,     1] loss: 1491.037
[29,     1] loss: 1480.997
[30,     1] loss: 1455.423
[31,     1] loss: 1436.196
[32,     1] loss: 1394.058
[33,     1] loss: 1374.803
[34,     1] loss: 1366.153
[35,     1] loss: 1312.767
[36,     1] loss: 1329.912
[37,     1] loss: 1295.202
[38,     1] loss: 1351.010
[39,     1] loss: 1290.848
[40,     1] loss: 1286.238
[41,     1] loss: 1271.369
[42,     1] loss: 1297.560
[43,     1] loss: 1307.794
[44,     1] loss: 1287.209
[45,     1] loss: 1295.100
[46,     1] loss: 1307.952
[47,     1] loss: 1213.594
[48,     1] loss: 1249.360
[49,     1] loss: 1273.231
[50,     1] loss: 1219.516
[51,     1] loss: 1248.596
[52,     1] loss: 1248.346
[53,     1] loss: 1238.229
[54,     1] loss: 1230.976
[55,     1] loss: 1269.919
[56,     1] loss: 1216.302
[57,     1] loss: 1221.808
[58,     1] loss: 1186.846
[59,     1] loss: 1197.098
[60,     1] loss: 1217.656
[61,     1] loss: 1186.514
[62,     1] loss: 1139.392
[63,     1] loss: 1218.941
[64,     1] loss: 1168.023
[65,     1] loss: 1185.593
[66,     1] loss: 1170.010
[67,     1] loss: 1134.740
[68,     1] loss: 1175.317
[69,     1] loss: 1143.374
[70,     1] loss: 1186.445
[71,     1] loss: 1153.788
[72,     1] loss: 1216.184
[73,     1] loss: 1086.562
[74,     1] loss: 1072.738
[75,     1] loss: 1168.763
[76,     1] loss: 1132.949
[77,     1] loss: 1127.072
[78,     1] loss: 1118.623
[79,     1] loss: 1140.957
[80,     1] loss: 1092.299
[81,     1] loss: 1113.030
[82,     1] loss: 1075.222
[83,     1] loss: 1117.548
[84,     1] loss: 1055.787
[85,     1] loss: 1056.516
[86,     1] loss: 1003.956
[87,     1] loss: 1042.745
[88,     1] loss: 1055.937
[89,     1] loss: 1058.889
[90,     1] loss: 1092.201
[91,     1] loss: 1026.245
[92,     1] loss: 1096.325
[93,     1] loss: 940.393
[94,     1] loss: 982.616
[95,     1] loss: 1015.480
[96,     1] loss: 942.544
[97,     1] loss: 967.379
[98,     1] loss: 947.297
[99,     1] loss: 1049.123
[100,     1] loss: 958.379
[101,     1] loss: 1016.230
Early stopping applied (best metric=0.7224555611610413)
Finished Training
Total time taken: 18.616450548171997
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1569.978
[2,     1] loss: 1584.101
[3,     1] loss: 1575.558
[4,     1] loss: 1569.563
[5,     1] loss: 1567.139
[6,     1] loss: 1563.403
[7,     1] loss: 1576.486
[8,     1] loss: 1565.174
[9,     1] loss: 1568.973
[10,     1] loss: 1569.583
[11,     1] loss: 1565.513
[12,     1] loss: 1564.493
[13,     1] loss: 1565.094
[14,     1] loss: 1563.284
[15,     1] loss: 1560.327
[16,     1] loss: 1559.244
[17,     1] loss: 1560.066
[18,     1] loss: 1564.536
[19,     1] loss: 1556.802
[20,     1] loss: 1555.069
[21,     1] loss: 1546.470
[22,     1] loss: 1537.887
[23,     1] loss: 1519.334
[24,     1] loss: 1500.436
[25,     1] loss: 1480.936
[26,     1] loss: 1448.170
[27,     1] loss: 1426.734
[28,     1] loss: 1379.366
[29,     1] loss: 1388.549
[30,     1] loss: 1322.099
[31,     1] loss: 1340.494
[32,     1] loss: 1364.489
[33,     1] loss: 1315.398
[34,     1] loss: 1286.087
[35,     1] loss: 1309.549
[36,     1] loss: 1300.708
[37,     1] loss: 1250.193
[38,     1] loss: 1302.183
[39,     1] loss: 1290.455
[40,     1] loss: 1285.781
[41,     1] loss: 1283.703
[42,     1] loss: 1241.196
[43,     1] loss: 1275.629
[44,     1] loss: 1236.979
[45,     1] loss: 1287.217
[46,     1] loss: 1232.201
[47,     1] loss: 1188.171
[48,     1] loss: 1244.834
[49,     1] loss: 1307.318
[50,     1] loss: 1165.144
[51,     1] loss: 1176.271
[52,     1] loss: 1227.148
[53,     1] loss: 1188.672
[54,     1] loss: 1172.173
[55,     1] loss: 1182.528
[56,     1] loss: 1164.349
[57,     1] loss: 1199.991
[58,     1] loss: 1139.115
[59,     1] loss: 1160.768
[60,     1] loss: 1144.799
[61,     1] loss: 1108.465
[62,     1] loss: 1104.369
[63,     1] loss: 1075.197
[64,     1] loss: 1046.597
[65,     1] loss: 1093.538
[66,     1] loss: 1206.110
[67,     1] loss: 1229.593
[68,     1] loss: 1172.919
[69,     1] loss: 1120.697
[70,     1] loss: 1113.338
[71,     1] loss: 1124.685
[72,     1] loss: 1145.371
[73,     1] loss: 1086.599
[74,     1] loss: 1084.248
[75,     1] loss: 1079.261
[76,     1] loss: 1039.105
[77,     1] loss: 1074.223
[78,     1] loss: 1101.193
[79,     1] loss: 1096.888
[80,     1] loss: 1051.989
[81,     1] loss: 1048.850
[82,     1] loss: 1102.349
[83,     1] loss: 1069.071
[84,     1] loss: 1118.252
[85,     1] loss: 1038.017
[86,     1] loss: 1027.932
[87,     1] loss: 1006.079
[88,     1] loss: 1118.104
[89,     1] loss: 1041.967
[90,     1] loss: 1021.356
[91,     1] loss: 1052.898
[92,     1] loss: 1016.167
[93,     1] loss: 1081.463
Early stopping applied (best metric=0.8308545351028442)
Finished Training
Total time taken: 14.020418643951416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1575.921
[2,     1] loss: 1577.254
[3,     1] loss: 1569.755
[4,     1] loss: 1572.495
[5,     1] loss: 1567.779
[6,     1] loss: 1570.233
[7,     1] loss: 1565.035
[8,     1] loss: 1572.783
[9,     1] loss: 1564.566
[10,     1] loss: 1566.540
[11,     1] loss: 1567.547
[12,     1] loss: 1567.621
[13,     1] loss: 1566.669
[14,     1] loss: 1563.315
[15,     1] loss: 1567.595
[16,     1] loss: 1559.272
[17,     1] loss: 1557.637
[18,     1] loss: 1546.544
[19,     1] loss: 1546.468
[20,     1] loss: 1533.716
[21,     1] loss: 1530.781
[22,     1] loss: 1516.449
[23,     1] loss: 1498.976
[24,     1] loss: 1470.297
[25,     1] loss: 1431.277
[26,     1] loss: 1445.171
[27,     1] loss: 1431.203
[28,     1] loss: 1381.125
[29,     1] loss: 1439.256
[30,     1] loss: 1374.565
[31,     1] loss: 1401.174
[32,     1] loss: 1350.732
[33,     1] loss: 1367.787
[34,     1] loss: 1291.577
[35,     1] loss: 1356.175
[36,     1] loss: 1370.694
[37,     1] loss: 1314.622
[38,     1] loss: 1297.392
[39,     1] loss: 1311.377
[40,     1] loss: 1349.095
[41,     1] loss: 1296.847
[42,     1] loss: 1268.967
[43,     1] loss: 1320.357
[44,     1] loss: 1245.424
[45,     1] loss: 1296.306
[46,     1] loss: 1281.855
[47,     1] loss: 1202.878
[48,     1] loss: 1337.754
[49,     1] loss: 1181.537
[50,     1] loss: 1245.101
[51,     1] loss: 1251.120
[52,     1] loss: 1228.132
[53,     1] loss: 1206.537
[54,     1] loss: 1214.651
[55,     1] loss: 1193.531
[56,     1] loss: 1194.074
[57,     1] loss: 1243.080
[58,     1] loss: 1185.864
[59,     1] loss: 1223.715
[60,     1] loss: 1208.243
[61,     1] loss: 1167.515
[62,     1] loss: 1123.444
[63,     1] loss: 1177.791
[64,     1] loss: 1098.188
[65,     1] loss: 1133.032
[66,     1] loss: 1160.035
[67,     1] loss: 1202.958
[68,     1] loss: 1184.123
[69,     1] loss: 1168.715
[70,     1] loss: 1047.121
[71,     1] loss: 1117.455
[72,     1] loss: 1074.718
[73,     1] loss: 1083.072
[74,     1] loss: 1091.458
Early stopping applied (best metric=0.8070008754730225)
Finished Training
Total time taken: 12.048429727554321
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1578.559
[2,     1] loss: 1573.134
[3,     1] loss: 1566.506
[4,     1] loss: 1568.783
[5,     1] loss: 1570.953
[6,     1] loss: 1572.240
[7,     1] loss: 1570.120
[8,     1] loss: 1568.443
[9,     1] loss: 1568.928
[10,     1] loss: 1565.177
[11,     1] loss: 1564.608
[12,     1] loss: 1567.328
[13,     1] loss: 1567.162
[14,     1] loss: 1559.224
[15,     1] loss: 1557.106
[16,     1] loss: 1559.756
[17,     1] loss: 1564.046
[18,     1] loss: 1553.677
[19,     1] loss: 1550.350
[20,     1] loss: 1541.040
[21,     1] loss: 1530.661
[22,     1] loss: 1504.445
[23,     1] loss: 1486.644
[24,     1] loss: 1474.287
[25,     1] loss: 1424.578
[26,     1] loss: 1418.478
[27,     1] loss: 1410.127
[28,     1] loss: 1381.011
[29,     1] loss: 1339.376
[30,     1] loss: 1352.852
[31,     1] loss: 1399.531
[32,     1] loss: 1324.831
[33,     1] loss: 1310.362
[34,     1] loss: 1325.432
[35,     1] loss: 1280.728
[36,     1] loss: 1272.615
[37,     1] loss: 1327.959
[38,     1] loss: 1297.064
[39,     1] loss: 1305.715
[40,     1] loss: 1315.071
[41,     1] loss: 1273.598
[42,     1] loss: 1272.192
[43,     1] loss: 1305.166
[44,     1] loss: 1266.288
[45,     1] loss: 1280.720
[46,     1] loss: 1248.354
[47,     1] loss: 1242.235
[48,     1] loss: 1271.205
[49,     1] loss: 1259.856
[50,     1] loss: 1271.973
[51,     1] loss: 1143.720
[52,     1] loss: 1230.198
[53,     1] loss: 1188.102
[54,     1] loss: 1162.630
[55,     1] loss: 1195.900
[56,     1] loss: 1169.824
[57,     1] loss: 1174.939
[58,     1] loss: 1236.922
[59,     1] loss: 1231.853
[60,     1] loss: 1216.576
[61,     1] loss: 1180.594
[62,     1] loss: 1227.103
[63,     1] loss: 1170.489
[64,     1] loss: 1175.860
[65,     1] loss: 1155.275
[66,     1] loss: 1153.262
[67,     1] loss: 1156.611
[68,     1] loss: 1160.616
[69,     1] loss: 1132.312
[70,     1] loss: 1171.723
[71,     1] loss: 1124.482
[72,     1] loss: 1170.755
[73,     1] loss: 1092.715
[74,     1] loss: 1118.926
[75,     1] loss: 1108.496
[76,     1] loss: 1139.953
[77,     1] loss: 1074.537
Early stopping applied (best metric=0.8019304275512695)
Finished Training
Total time taken: 12.403321027755737
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.229
[2,     1] loss: 1567.658
[3,     1] loss: 1574.598
[4,     1] loss: 1572.493
[5,     1] loss: 1563.161
[6,     1] loss: 1561.309
[7,     1] loss: 1567.845
[8,     1] loss: 1561.849
[9,     1] loss: 1573.957
[10,     1] loss: 1566.562
[11,     1] loss: 1567.092
[12,     1] loss: 1562.283
[13,     1] loss: 1564.467
[14,     1] loss: 1562.713
[15,     1] loss: 1555.551
[16,     1] loss: 1556.778
[17,     1] loss: 1557.041
[18,     1] loss: 1547.067
[19,     1] loss: 1548.266
[20,     1] loss: 1544.627
[21,     1] loss: 1513.072
[22,     1] loss: 1494.921
[23,     1] loss: 1473.386
[24,     1] loss: 1439.593
[25,     1] loss: 1433.397
[26,     1] loss: 1392.283
[27,     1] loss: 1334.540
[28,     1] loss: 1342.779
[29,     1] loss: 1391.128
[30,     1] loss: 1346.173
[31,     1] loss: 1361.020
[32,     1] loss: 1330.495
[33,     1] loss: 1335.973
[34,     1] loss: 1336.847
[35,     1] loss: 1278.788
[36,     1] loss: 1309.744
[37,     1] loss: 1343.867
[38,     1] loss: 1292.301
[39,     1] loss: 1243.321
[40,     1] loss: 1234.831
[41,     1] loss: 1267.876
[42,     1] loss: 1268.260
[43,     1] loss: 1257.439
[44,     1] loss: 1169.857
[45,     1] loss: 1209.884
[46,     1] loss: 1283.829
[47,     1] loss: 1256.637
[48,     1] loss: 1266.025
[49,     1] loss: 1209.163
[50,     1] loss: 1238.024
[51,     1] loss: 1189.396
[52,     1] loss: 1204.684
[53,     1] loss: 1189.990
[54,     1] loss: 1144.667
[55,     1] loss: 1233.397
[56,     1] loss: 1215.412
[57,     1] loss: 1187.240
[58,     1] loss: 1139.178
[59,     1] loss: 1169.025
[60,     1] loss: 1159.202
[61,     1] loss: 1184.964
Early stopping applied (best metric=0.7964855432510376)
Finished Training
Total time taken: 9.587201833724976
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1567.741
[2,     1] loss: 1567.058
[3,     1] loss: 1569.007
[4,     1] loss: 1569.369
[5,     1] loss: 1565.125
[6,     1] loss: 1566.697
[7,     1] loss: 1567.007
[8,     1] loss: 1563.439
[9,     1] loss: 1567.074
[10,     1] loss: 1564.278
[11,     1] loss: 1563.171
[12,     1] loss: 1559.626
[13,     1] loss: 1556.570
[14,     1] loss: 1558.958
[15,     1] loss: 1546.210
[16,     1] loss: 1542.341
[17,     1] loss: 1542.710
[18,     1] loss: 1519.017
[19,     1] loss: 1496.747
[20,     1] loss: 1465.007
[21,     1] loss: 1440.058
[22,     1] loss: 1396.641
[23,     1] loss: 1366.954
[24,     1] loss: 1417.361
[25,     1] loss: 1379.211
[26,     1] loss: 1338.180
[27,     1] loss: 1322.624
[28,     1] loss: 1311.111
[29,     1] loss: 1288.691
[30,     1] loss: 1232.517
[31,     1] loss: 1228.918
[32,     1] loss: 1340.659
[33,     1] loss: 1244.437
[34,     1] loss: 1265.054
[35,     1] loss: 1258.832
[36,     1] loss: 1209.005
[37,     1] loss: 1239.890
[38,     1] loss: 1280.302
[39,     1] loss: 1237.640
[40,     1] loss: 1231.903
[41,     1] loss: 1153.437
[42,     1] loss: 1236.759
[43,     1] loss: 1207.446
[44,     1] loss: 1167.901
[45,     1] loss: 1185.987
[46,     1] loss: 1143.662
[47,     1] loss: 1232.182
[48,     1] loss: 1115.964
[49,     1] loss: 1168.102
[50,     1] loss: 1130.517
[51,     1] loss: 1128.987
[52,     1] loss: 1131.089
[53,     1] loss: 1176.559
[54,     1] loss: 1135.381
Early stopping applied (best metric=0.9269320964813232)
Finished Training
Total time taken: 8.71070408821106
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1580.420
[2,     1] loss: 1592.951
[3,     1] loss: 1565.535
[4,     1] loss: 1569.175
[5,     1] loss: 1572.806
[6,     1] loss: 1568.655
[7,     1] loss: 1569.509
[8,     1] loss: 1563.422
[9,     1] loss: 1560.518
[10,     1] loss: 1564.709
[11,     1] loss: 1566.346
[12,     1] loss: 1560.631
[13,     1] loss: 1563.188
[14,     1] loss: 1558.761
[15,     1] loss: 1557.850
[16,     1] loss: 1553.511
[17,     1] loss: 1548.147
[18,     1] loss: 1543.139
[19,     1] loss: 1529.852
[20,     1] loss: 1514.947
[21,     1] loss: 1485.051
[22,     1] loss: 1461.862
[23,     1] loss: 1447.767
[24,     1] loss: 1406.423
[25,     1] loss: 1354.007
[26,     1] loss: 1363.802
[27,     1] loss: 1413.992
[28,     1] loss: 1365.076
[29,     1] loss: 1360.686
[30,     1] loss: 1360.052
[31,     1] loss: 1311.829
[32,     1] loss: 1338.189
[33,     1] loss: 1312.922
[34,     1] loss: 1274.977
[35,     1] loss: 1296.670
[36,     1] loss: 1381.628
[37,     1] loss: 1331.107
[38,     1] loss: 1250.103
[39,     1] loss: 1306.547
[40,     1] loss: 1333.648
[41,     1] loss: 1275.928
[42,     1] loss: 1236.057
[43,     1] loss: 1237.589
[44,     1] loss: 1202.774
[45,     1] loss: 1230.771
[46,     1] loss: 1216.923
[47,     1] loss: 1234.136
[48,     1] loss: 1261.852
[49,     1] loss: 1203.820
[50,     1] loss: 1283.201
[51,     1] loss: 1218.531
[52,     1] loss: 1218.219
[53,     1] loss: 1203.929
[54,     1] loss: 1202.458
[55,     1] loss: 1233.556
[56,     1] loss: 1189.461
[57,     1] loss: 1199.301
[58,     1] loss: 1170.939
[59,     1] loss: 1136.446
[60,     1] loss: 1225.147
[61,     1] loss: 1196.517
[62,     1] loss: 1184.948
[63,     1] loss: 1153.061
[64,     1] loss: 1201.597
[65,     1] loss: 1176.898
[66,     1] loss: 1183.230
[67,     1] loss: 1132.143
[68,     1] loss: 1099.633
[69,     1] loss: 1085.724
[70,     1] loss: 1123.823
[71,     1] loss: 1035.818
[72,     1] loss: 1132.397
[73,     1] loss: 1156.646
[74,     1] loss: 1096.916
[75,     1] loss: 1017.731
[76,     1] loss: 1139.317
[77,     1] loss: 1076.515
[78,     1] loss: 1077.632
[79,     1] loss: 1058.342
[80,     1] loss: 1069.550
[81,     1] loss: 1029.636
[82,     1] loss: 1057.394
[83,     1] loss: 1126.723
[84,     1] loss: 1082.685
Early stopping applied (best metric=0.6846873760223389)
Finished Training
Total time taken: 15.174949169158936
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1571.484
[2,     1] loss: 1577.979
[3,     1] loss: 1573.265
[4,     1] loss: 1576.930
[5,     1] loss: 1570.491
[6,     1] loss: 1569.505
[7,     1] loss: 1567.100
[8,     1] loss: 1564.241
[9,     1] loss: 1561.443
[10,     1] loss: 1565.680
[11,     1] loss: 1570.715
[12,     1] loss: 1567.192
[13,     1] loss: 1555.735
[14,     1] loss: 1549.930
[15,     1] loss: 1536.374
[16,     1] loss: 1524.811
[17,     1] loss: 1492.300
[18,     1] loss: 1465.420
[19,     1] loss: 1427.503
[20,     1] loss: 1368.439
[21,     1] loss: 1375.743
[22,     1] loss: 1356.512
[23,     1] loss: 1387.213
[24,     1] loss: 1424.781
[25,     1] loss: 1330.189
[26,     1] loss: 1298.462
[27,     1] loss: 1362.704
[28,     1] loss: 1269.044
[29,     1] loss: 1269.062
[30,     1] loss: 1343.292
[31,     1] loss: 1297.463
[32,     1] loss: 1293.558
[33,     1] loss: 1251.481
[34,     1] loss: 1223.723
[35,     1] loss: 1233.052
[36,     1] loss: 1233.314
[37,     1] loss: 1248.492
[38,     1] loss: 1184.528
[39,     1] loss: 1265.064
[40,     1] loss: 1213.198
[41,     1] loss: 1163.531
[42,     1] loss: 1154.034
[43,     1] loss: 1179.509
[44,     1] loss: 1188.144
[45,     1] loss: 1157.050
[46,     1] loss: 1179.660
[47,     1] loss: 1199.388
[48,     1] loss: 1155.299
[49,     1] loss: 1116.844
[50,     1] loss: 1149.645
[51,     1] loss: 1136.874
[52,     1] loss: 1173.439
[53,     1] loss: 1178.523
[54,     1] loss: 1080.374
[55,     1] loss: 1127.196
[56,     1] loss: 1136.760
[57,     1] loss: 1077.280
[58,     1] loss: 1126.747
[59,     1] loss: 1076.907
Early stopping applied (best metric=0.8562259078025818)
Finished Training
Total time taken: 11.167293310165405
{'Hydroxylation-K Validation Accuracy': 0.7859338061465722, 'Hydroxylation-K Validation Sensitivity': 0.6977777777777778, 'Hydroxylation-K Validation Specificity': 0.8087719298245614, 'Hydroxylation-K Validation Precision': 0.48272847522847523, 'Hydroxylation-K AUC ROC': 0.8323196881091618, 'Hydroxylation-K AUC PR': 0.623789262054788, 'Hydroxylation-K MCC': 0.44773267020696134, 'Hydroxylation-K F1': 0.5676275200912883, 'Validation Loss (Hydroxylation-K)': 0.41009195844332375, 'Hydroxylation-P Validation Accuracy': 0.7915873136727408, 'Hydroxylation-P Validation Sensitivity': 0.735026455026455, 'Hydroxylation-P Validation Specificity': 0.8038056760935708, 'Hydroxylation-P Validation Precision': 0.45209446616225263, 'Hydroxylation-P AUC ROC': 0.8447132996381088, 'Hydroxylation-P AUC PR': 0.5711934798608072, 'Hydroxylation-P MCC': 0.455747375808611, 'Hydroxylation-P F1': 0.5565218776327134, 'Validation Loss (Hydroxylation-P)': 0.3818231463432312, 'Validation Loss (total)': 0.7919151147206624, 'TimeToTrain': 12.748878876368204}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002832161387710426,
 'learning_rate_Hydroxylation-K': 0.00927051728571305,
 'learning_rate_Hydroxylation-P': 0.00433821244882894,
 'log_base': 1.5329461000912068,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3286347557,
 'sample_weights': [3.126569868125474, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0055225652184276,
 'weight_decay_Hydroxylation-K': 2.0288084711051777,
 'weight_decay_Hydroxylation-P': 6.391064574225666}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1736.289
[2,     1] loss: 1732.203
[3,     1] loss: 1737.510
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001220876549951973,
 'learning_rate_Hydroxylation-K': 0.0091158020660677,
 'learning_rate_Hydroxylation-P': 0.0060822927979690645,
 'log_base': 1.9486759829020797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 300500670,
 'sample_weights': [3.907950845934787, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.855883062178716,
 'weight_decay_Hydroxylation-K': 5.950453970228391,
 'weight_decay_Hydroxylation-P': 4.717493083503382}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1433.361
[2,     1] loss: 1434.429
[3,     1] loss: 1433.574
[4,     1] loss: 1436.801
[5,     1] loss: 1434.195
[6,     1] loss: 1434.285
[7,     1] loss: 1434.744
[8,     1] loss: 1431.625
[9,     1] loss: 1429.006
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006586036089039758,
 'learning_rate_Hydroxylation-K': 0.0023412649338619934,
 'learning_rate_Hydroxylation-P': 0.0012470869861638668,
 'log_base': 1.370942286475762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1808895900,
 'sample_weights': [2.5023499212511804, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.486305856529719,
 'weight_decay_Hydroxylation-K': 2.9257065197556438,
 'weight_decay_Hydroxylation-P': 8.385784566499403}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2032.938
[2,     1] loss: 2025.258
[3,     1] loss: 2010.710
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008103389932260825,
 'learning_rate_Hydroxylation-K': 0.004295587711807717,
 'learning_rate_Hydroxylation-P': 0.005436670209278105,
 'log_base': 1.157238361718095,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3249298222,
 'sample_weights': [5.291448884264262, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.866236097934724,
 'weight_decay_Hydroxylation-K': 3.935942438902602,
 'weight_decay_Hydroxylation-P': 4.019098326158284}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3714.466
[2,     1] loss: 3713.050
[3,     1] loss: 3719.306
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00936923339939966,
 'learning_rate_Hydroxylation-K': 0.0011005136046096037,
 'learning_rate_Hydroxylation-P': 0.0014519609271570531,
 'log_base': 1.0434990985073602,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3947531540,
 'sample_weights': [11.431688565487573, 1.429016120004567],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.221111090374788,
 'weight_decay_Hydroxylation-K': 2.226467879599691,
 'weight_decay_Hydroxylation-P': 4.389078163897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12677.973
[2,     1] loss: 12656.992
[3,     1] loss: 12813.010
[4,     1] loss: 12806.686
[5,     1] loss: 12701.064
[6,     1] loss: 12716.340
[7,     1] loss: 12761.319
[8,     1] loss: 12747.820
[9,     1] loss: 12715.312
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00797850070241497,
 'learning_rate_Hydroxylation-K': 0.006326822576018586,
 'learning_rate_Hydroxylation-P': 0.006045460608859589,
 'log_base': 2.102310117352295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2911047060,
 'sample_weights': [39.20759681993374, 4.9011384067508015],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7607741749546695,
 'weight_decay_Hydroxylation-K': 5.0195145820923175,
 'weight_decay_Hydroxylation-P': 6.008729006100255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1383.884
[2,     1] loss: 1390.771
[3,     1] loss: 1381.570
[4,     1] loss: 1377.603
[5,     1] loss: 1378.234
[6,     1] loss: 1378.629
[7,     1] loss: 1364.104
[8,     1] loss: 1362.542
[9,     1] loss: 1350.146
[10,     1] loss: 1296.768
[11,     1] loss: 1271.838
[12,     1] loss: 1221.962
[13,     1] loss: 1186.986
[14,     1] loss: 1205.055
[15,     1] loss: 1190.267
[16,     1] loss: 1230.400
[17,     1] loss: 1216.174
[18,     1] loss: 1159.556
[19,     1] loss: 1141.280
[20,     1] loss: 1145.303
[21,     1] loss: 1131.393
[22,     1] loss: 1164.558
[23,     1] loss: 1139.609
[24,     1] loss: 1143.174
[25,     1] loss: 1086.359
[26,     1] loss: 1061.727
[27,     1] loss: 1121.015
[28,     1] loss: 1074.266
[29,     1] loss: 1142.558
[30,     1] loss: 1044.809
[31,     1] loss: 1076.847
[32,     1] loss: 1089.453
[33,     1] loss: 1009.567
[34,     1] loss: 1054.692
[35,     1] loss: 1055.265
[36,     1] loss: 1037.311
[37,     1] loss: 1033.275
[38,     1] loss: 982.650
[39,     1] loss: 977.304
[40,     1] loss: 947.719
[41,     1] loss: 963.324
[42,     1] loss: 915.434
[43,     1] loss: 964.230
[44,     1] loss: 1075.519
[45,     1] loss: 918.530
[46,     1] loss: 1005.691
[47,     1] loss: 907.459
[48,     1] loss: 1006.593
[49,     1] loss: 897.434
[50,     1] loss: 965.727
[51,     1] loss: 856.310
[52,     1] loss: 990.278
[53,     1] loss: 840.523
[54,     1] loss: 933.557
[55,     1] loss: 847.459
[56,     1] loss: 919.995
[57,     1] loss: 882.593
[58,     1] loss: 818.985
[59,     1] loss: 809.825
[60,     1] loss: 807.979
[61,     1] loss: 736.704
[62,     1] loss: 806.946
[63,     1] loss: 862.362
[64,     1] loss: 811.086
[65,     1] loss: 712.383
[66,     1] loss: 766.136
[67,     1] loss: 696.661
[68,     1] loss: 764.613
[69,     1] loss: 872.624
[70,     1] loss: 780.202
[71,     1] loss: 695.201
[72,     1] loss: 740.242
[73,     1] loss: 654.453
[74,     1] loss: 648.387
Early stopping applied (best metric=0.729965329170227)
Finished Training
Total time taken: 11.313360691070557
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.691
[2,     1] loss: 1383.641
[3,     1] loss: 1382.504
[4,     1] loss: 1382.869
[5,     1] loss: 1377.783
[6,     1] loss: 1375.900
[7,     1] loss: 1383.733
[8,     1] loss: 1382.728
[9,     1] loss: 1377.777
[10,     1] loss: 1381.042
[11,     1] loss: 1377.784
[12,     1] loss: 1368.594
[13,     1] loss: 1368.488
[14,     1] loss: 1353.463
[15,     1] loss: 1330.426
[16,     1] loss: 1289.229
[17,     1] loss: 1282.911
[18,     1] loss: 1245.572
[19,     1] loss: 1206.682
[20,     1] loss: 1222.095
[21,     1] loss: 1164.800
[22,     1] loss: 1173.793
[23,     1] loss: 1139.657
[24,     1] loss: 1162.571
[25,     1] loss: 1146.983
[26,     1] loss: 1167.140
[27,     1] loss: 1138.360
[28,     1] loss: 1101.847
[29,     1] loss: 1083.726
[30,     1] loss: 1103.715
[31,     1] loss: 1057.328
[32,     1] loss: 1126.177
[33,     1] loss: 1062.022
[34,     1] loss: 1039.102
[35,     1] loss: 1075.481
[36,     1] loss: 1059.696
[37,     1] loss: 1038.670
[38,     1] loss: 1031.137
[39,     1] loss: 1046.811
[40,     1] loss: 1018.994
[41,     1] loss: 966.391
[42,     1] loss: 983.167
[43,     1] loss: 1034.424
[44,     1] loss: 1001.238
[45,     1] loss: 947.080
[46,     1] loss: 981.305
[47,     1] loss: 980.616
[48,     1] loss: 995.215
[49,     1] loss: 924.369
[50,     1] loss: 979.909
[51,     1] loss: 891.731
[52,     1] loss: 921.380
[53,     1] loss: 878.574
[54,     1] loss: 862.983
[55,     1] loss: 847.600
Early stopping applied (best metric=0.8225100636482239)
Finished Training
Total time taken: 9.97414755821228
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.915
[2,     1] loss: 1383.404
[3,     1] loss: 1383.998
[4,     1] loss: 1382.818
[5,     1] loss: 1381.852
[6,     1] loss: 1389.691
[7,     1] loss: 1382.747
[8,     1] loss: 1380.020
[9,     1] loss: 1382.832
[10,     1] loss: 1382.420
[11,     1] loss: 1379.728
[12,     1] loss: 1379.279
[13,     1] loss: 1377.727
[14,     1] loss: 1375.043
[15,     1] loss: 1376.099
[16,     1] loss: 1368.019
[17,     1] loss: 1355.452
[18,     1] loss: 1354.611
[19,     1] loss: 1322.955
[20,     1] loss: 1305.066
[21,     1] loss: 1251.826
[22,     1] loss: 1225.921
[23,     1] loss: 1205.453
[24,     1] loss: 1235.894
[25,     1] loss: 1199.726
[26,     1] loss: 1161.512
[27,     1] loss: 1198.790
[28,     1] loss: 1183.553
[29,     1] loss: 1159.458
[30,     1] loss: 1153.449
[31,     1] loss: 1132.340
[32,     1] loss: 1147.908
[33,     1] loss: 1136.656
[34,     1] loss: 1115.974
[35,     1] loss: 1106.625
[36,     1] loss: 1087.865
[37,     1] loss: 1092.798
[38,     1] loss: 1133.121
[39,     1] loss: 1055.354
[40,     1] loss: 1078.191
[41,     1] loss: 1038.156
[42,     1] loss: 1136.761
[43,     1] loss: 1016.200
[44,     1] loss: 1038.783
[45,     1] loss: 1025.863
[46,     1] loss: 1032.366
[47,     1] loss: 957.047
[48,     1] loss: 999.212
[49,     1] loss: 1022.418
[50,     1] loss: 977.118
[51,     1] loss: 961.382
[52,     1] loss: 945.311
[53,     1] loss: 1007.462
[54,     1] loss: 944.456
[55,     1] loss: 953.468
[56,     1] loss: 904.831
[57,     1] loss: 923.458
[58,     1] loss: 927.451
[59,     1] loss: 870.917
[60,     1] loss: 841.959
[61,     1] loss: 897.635
[62,     1] loss: 854.322
[63,     1] loss: 855.271
[64,     1] loss: 821.213
[65,     1] loss: 833.907
[66,     1] loss: 761.753
[67,     1] loss: 837.065
[68,     1] loss: 1110.817
[69,     1] loss: 907.943
[70,     1] loss: 773.416
[71,     1] loss: 895.113
[72,     1] loss: 789.551
[73,     1] loss: 849.870
[74,     1] loss: 742.238
[75,     1] loss: 769.112
[76,     1] loss: 757.650
[77,     1] loss: 839.602
[78,     1] loss: 895.603
Early stopping applied (best metric=0.8069489598274231)
Finished Training
Total time taken: 14.537893056869507
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.033
[2,     1] loss: 1382.821
[3,     1] loss: 1381.458
[4,     1] loss: 1383.734
[5,     1] loss: 1378.555
[6,     1] loss: 1386.487
[7,     1] loss: 1381.504
[8,     1] loss: 1379.705
[9,     1] loss: 1373.615
[10,     1] loss: 1363.879
[11,     1] loss: 1352.576
[12,     1] loss: 1327.753
[13,     1] loss: 1307.845
[14,     1] loss: 1241.992
[15,     1] loss: 1226.960
[16,     1] loss: 1209.461
[17,     1] loss: 1167.154
[18,     1] loss: 1199.449
[19,     1] loss: 1174.191
[20,     1] loss: 1190.659
[21,     1] loss: 1161.354
[22,     1] loss: 1140.778
[23,     1] loss: 1109.374
[24,     1] loss: 1143.155
[25,     1] loss: 1112.909
[26,     1] loss: 1151.036
[27,     1] loss: 1068.869
[28,     1] loss: 1126.429
[29,     1] loss: 1067.435
[30,     1] loss: 1097.030
[31,     1] loss: 1026.573
[32,     1] loss: 1004.818
[33,     1] loss: 1083.254
[34,     1] loss: 1022.928
[35,     1] loss: 1025.478
[36,     1] loss: 1029.620
[37,     1] loss: 1090.697
[38,     1] loss: 1119.482
[39,     1] loss: 1003.517
[40,     1] loss: 1046.167
[41,     1] loss: 997.438
[42,     1] loss: 952.659
[43,     1] loss: 975.609
[44,     1] loss: 967.612
[45,     1] loss: 968.933
[46,     1] loss: 946.655
[47,     1] loss: 912.628
[48,     1] loss: 935.646
[49,     1] loss: 937.617
[50,     1] loss: 872.979
[51,     1] loss: 907.754
[52,     1] loss: 858.667
[53,     1] loss: 899.796
[54,     1] loss: 952.752
[55,     1] loss: 804.852
[56,     1] loss: 835.144
[57,     1] loss: 822.299
[58,     1] loss: 793.294
Early stopping applied (best metric=0.7772707939147949)
Finished Training
Total time taken: 11.109503507614136
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1388.076
[2,     1] loss: 1392.031
[3,     1] loss: 1384.122
[4,     1] loss: 1389.207
[5,     1] loss: 1385.377
[6,     1] loss: 1380.665
[7,     1] loss: 1381.297
[8,     1] loss: 1380.225
[9,     1] loss: 1380.910
[10,     1] loss: 1380.144
[11,     1] loss: 1375.117
[12,     1] loss: 1369.469
[13,     1] loss: 1363.513
[14,     1] loss: 1349.303
[15,     1] loss: 1323.575
[16,     1] loss: 1282.926
[17,     1] loss: 1249.507
[18,     1] loss: 1205.379
[19,     1] loss: 1165.986
[20,     1] loss: 1145.596
[21,     1] loss: 1160.966
[22,     1] loss: 1212.283
[23,     1] loss: 1124.704
[24,     1] loss: 1129.865
[25,     1] loss: 1092.430
[26,     1] loss: 1080.287
[27,     1] loss: 1104.308
[28,     1] loss: 1116.003
[29,     1] loss: 1097.610
[30,     1] loss: 1071.998
[31,     1] loss: 1062.981
[32,     1] loss: 1063.114
[33,     1] loss: 1041.408
[34,     1] loss: 993.589
[35,     1] loss: 1067.871
[36,     1] loss: 1033.847
[37,     1] loss: 960.534
[38,     1] loss: 995.925
[39,     1] loss: 960.794
[40,     1] loss: 980.454
[41,     1] loss: 1004.484
[42,     1] loss: 986.654
[43,     1] loss: 869.723
[44,     1] loss: 972.581
[45,     1] loss: 953.862
[46,     1] loss: 927.338
[47,     1] loss: 941.680
[48,     1] loss: 873.267
[49,     1] loss: 908.372
[50,     1] loss: 869.671
[51,     1] loss: 875.779
[52,     1] loss: 906.749
[53,     1] loss: 1164.781
[54,     1] loss: 1124.302
[55,     1] loss: 922.302
[56,     1] loss: 1003.042
[57,     1] loss: 1009.501
[58,     1] loss: 960.528
[59,     1] loss: 960.971
[60,     1] loss: 1021.336
[61,     1] loss: 882.006
[62,     1] loss: 903.817
[63,     1] loss: 944.228
[64,     1] loss: 832.403
[65,     1] loss: 858.964
[66,     1] loss: 897.833
[67,     1] loss: 818.043
[68,     1] loss: 839.477
[69,     1] loss: 807.493
[70,     1] loss: 781.667
[71,     1] loss: 764.539
[72,     1] loss: 787.490
[73,     1] loss: 759.557
[74,     1] loss: 692.804
[75,     1] loss: 727.571
[76,     1] loss: 794.179
[77,     1] loss: 623.459
[78,     1] loss: 763.199
[79,     1] loss: 862.129
[80,     1] loss: 606.706
[81,     1] loss: 822.832
[82,     1] loss: 715.202
[83,     1] loss: 702.280
[84,     1] loss: 653.457
[85,     1] loss: 701.810
[86,     1] loss: 622.638
[87,     1] loss: 600.816
[88,     1] loss: 653.286
Early stopping applied (best metric=0.7015088796615601)
Finished Training
Total time taken: 13.37538480758667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.927
[2,     1] loss: 1379.006
[3,     1] loss: 1385.078
[4,     1] loss: 1378.649
[5,     1] loss: 1380.745
[6,     1] loss: 1381.482
[7,     1] loss: 1384.696
[8,     1] loss: 1384.181
[9,     1] loss: 1379.402
[10,     1] loss: 1382.583
[11,     1] loss: 1379.826
[12,     1] loss: 1378.520
[13,     1] loss: 1381.112
[14,     1] loss: 1383.593
[15,     1] loss: 1383.083
[16,     1] loss: 1381.176
[17,     1] loss: 1380.201
[18,     1] loss: 1380.002
[19,     1] loss: 1379.833
[20,     1] loss: 1380.799
[21,     1] loss: 1378.439
[22,     1] loss: 1378.397
[23,     1] loss: 1378.199
[24,     1] loss: 1375.322
[25,     1] loss: 1366.929
[26,     1] loss: 1371.167
[27,     1] loss: 1360.966
[28,     1] loss: 1352.739
[29,     1] loss: 1326.906
[30,     1] loss: 1300.457
[31,     1] loss: 1281.895
[32,     1] loss: 1262.001
[33,     1] loss: 1214.539
[34,     1] loss: 1232.056
[35,     1] loss: 1233.277
[36,     1] loss: 1200.497
[37,     1] loss: 1155.244
[38,     1] loss: 1172.936
[39,     1] loss: 1183.118
[40,     1] loss: 1151.318
[41,     1] loss: 1150.541
[42,     1] loss: 1083.994
[43,     1] loss: 1163.779
[44,     1] loss: 1076.186
[45,     1] loss: 1152.450
[46,     1] loss: 1086.254
[47,     1] loss: 1055.608
[48,     1] loss: 1050.286
[49,     1] loss: 1039.058
[50,     1] loss: 1028.521
[51,     1] loss: 988.058
[52,     1] loss: 1041.883
[53,     1] loss: 1024.958
[54,     1] loss: 1017.582
[55,     1] loss: 1027.404
[56,     1] loss: 1019.440
[57,     1] loss: 996.271
[58,     1] loss: 971.546
[59,     1] loss: 981.420
[60,     1] loss: 993.534
[61,     1] loss: 907.007
[62,     1] loss: 1042.866
[63,     1] loss: 972.909
[64,     1] loss: 943.218
[65,     1] loss: 1027.616
[66,     1] loss: 978.455
[67,     1] loss: 904.347
[68,     1] loss: 1007.890
[69,     1] loss: 861.614
[70,     1] loss: 943.075
[71,     1] loss: 855.746
[72,     1] loss: 813.748
[73,     1] loss: 864.888
[74,     1] loss: 830.188
Early stopping applied (best metric=0.8936121463775635)
Finished Training
Total time taken: 11.410387992858887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.033
[2,     1] loss: 1390.663
[3,     1] loss: 1386.884
[4,     1] loss: 1380.127
[5,     1] loss: 1385.690
[6,     1] loss: 1385.205
[7,     1] loss: 1379.746
[8,     1] loss: 1382.532
[9,     1] loss: 1380.052
[10,     1] loss: 1380.922
[11,     1] loss: 1383.343
[12,     1] loss: 1379.314
[13,     1] loss: 1379.493
[14,     1] loss: 1378.555
[15,     1] loss: 1377.847
[16,     1] loss: 1382.975
[17,     1] loss: 1371.254
[18,     1] loss: 1375.301
[19,     1] loss: 1369.654
[20,     1] loss: 1365.376
[21,     1] loss: 1351.472
[22,     1] loss: 1332.604
[23,     1] loss: 1318.225
[24,     1] loss: 1302.095
[25,     1] loss: 1252.294
[26,     1] loss: 1244.525
[27,     1] loss: 1198.592
[28,     1] loss: 1185.689
[29,     1] loss: 1214.259
[30,     1] loss: 1111.071
[31,     1] loss: 1133.147
[32,     1] loss: 1173.405
[33,     1] loss: 1163.462
[34,     1] loss: 1110.170
[35,     1] loss: 1120.512
[36,     1] loss: 1120.793
[37,     1] loss: 1107.106
[38,     1] loss: 1074.020
[39,     1] loss: 1100.625
[40,     1] loss: 1052.330
[41,     1] loss: 1059.214
[42,     1] loss: 1006.031
[43,     1] loss: 1035.404
[44,     1] loss: 1022.328
[45,     1] loss: 1020.146
[46,     1] loss: 989.281
[47,     1] loss: 1037.897
[48,     1] loss: 1053.881
[49,     1] loss: 996.952
[50,     1] loss: 967.079
[51,     1] loss: 954.482
[52,     1] loss: 926.816
[53,     1] loss: 931.472
[54,     1] loss: 931.651
[55,     1] loss: 997.341
[56,     1] loss: 1006.724
[57,     1] loss: 906.742
[58,     1] loss: 878.153
[59,     1] loss: 859.375
[60,     1] loss: 858.441
[61,     1] loss: 863.727
[62,     1] loss: 839.380
[63,     1] loss: 796.255
[64,     1] loss: 827.352
[65,     1] loss: 879.834
[66,     1] loss: 850.456
[67,     1] loss: 852.854
[68,     1] loss: 703.615
[69,     1] loss: 836.897
[70,     1] loss: 712.265
[71,     1] loss: 703.125
[72,     1] loss: 776.937
[73,     1] loss: 729.870
[74,     1] loss: 685.741
[75,     1] loss: 658.717
[76,     1] loss: 584.712
[77,     1] loss: 690.711
[78,     1] loss: 745.096
Early stopping applied (best metric=0.7515900135040283)
Finished Training
Total time taken: 13.836836814880371
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.732
[2,     1] loss: 1384.420
[3,     1] loss: 1385.875
[4,     1] loss: 1384.202
[5,     1] loss: 1386.117
[6,     1] loss: 1383.523
[7,     1] loss: 1381.650
[8,     1] loss: 1381.348
[9,     1] loss: 1377.316
[10,     1] loss: 1383.432
[11,     1] loss: 1380.210
[12,     1] loss: 1377.280
[13,     1] loss: 1371.643
[14,     1] loss: 1373.397
[15,     1] loss: 1362.274
[16,     1] loss: 1348.743
[17,     1] loss: 1328.411
[18,     1] loss: 1326.735
[19,     1] loss: 1295.425
[20,     1] loss: 1287.654
[21,     1] loss: 1234.579
[22,     1] loss: 1217.701
[23,     1] loss: 1198.043
[24,     1] loss: 1154.572
[25,     1] loss: 1202.119
[26,     1] loss: 1132.577
[27,     1] loss: 1188.351
[28,     1] loss: 1161.985
[29,     1] loss: 1163.051
[30,     1] loss: 1137.703
[31,     1] loss: 1128.893
[32,     1] loss: 1149.266
[33,     1] loss: 1164.522
[34,     1] loss: 1139.521
[35,     1] loss: 1144.935
[36,     1] loss: 1109.836
[37,     1] loss: 1086.000
[38,     1] loss: 1063.825
[39,     1] loss: 1045.587
[40,     1] loss: 997.098
[41,     1] loss: 1031.276
[42,     1] loss: 988.097
[43,     1] loss: 1056.146
[44,     1] loss: 1007.972
[45,     1] loss: 1038.236
[46,     1] loss: 1013.568
[47,     1] loss: 1013.678
[48,     1] loss: 1007.798
[49,     1] loss: 987.239
[50,     1] loss: 985.306
[51,     1] loss: 912.711
[52,     1] loss: 919.252
[53,     1] loss: 993.225
[54,     1] loss: 882.428
[55,     1] loss: 965.305
[56,     1] loss: 1004.542
[57,     1] loss: 899.502
[58,     1] loss: 965.661
[59,     1] loss: 948.240
[60,     1] loss: 904.275
[61,     1] loss: 919.209
[62,     1] loss: 879.819
[63,     1] loss: 915.012
[64,     1] loss: 783.314
[65,     1] loss: 850.852
[66,     1] loss: 859.401
[67,     1] loss: 798.276
[68,     1] loss: 776.810
[69,     1] loss: 964.831
[70,     1] loss: 992.358
[71,     1] loss: 734.351
[72,     1] loss: 889.636
[73,     1] loss: 721.389
[74,     1] loss: 804.671
[75,     1] loss: 734.989
[76,     1] loss: 712.238
[77,     1] loss: 762.279
[78,     1] loss: 783.959
[79,     1] loss: 662.447
[80,     1] loss: 710.293
[81,     1] loss: 762.131
[82,     1] loss: 652.142
[83,     1] loss: 593.665
[84,     1] loss: 606.380
[85,     1] loss: 579.371
[86,     1] loss: 614.336
[87,     1] loss: 658.465
Early stopping applied (best metric=0.7680544853210449)
Finished Training
Total time taken: 14.027872800827026
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.595
[2,     1] loss: 1387.956
[3,     1] loss: 1383.723
[4,     1] loss: 1379.623
[5,     1] loss: 1382.373
[6,     1] loss: 1383.977
[7,     1] loss: 1376.693
[8,     1] loss: 1377.839
[9,     1] loss: 1375.675
[10,     1] loss: 1376.018
[11,     1] loss: 1376.744
[12,     1] loss: 1367.364
[13,     1] loss: 1349.919
[14,     1] loss: 1337.005
[15,     1] loss: 1319.820
[16,     1] loss: 1279.136
[17,     1] loss: 1274.554
[18,     1] loss: 1211.203
[19,     1] loss: 1207.769
[20,     1] loss: 1144.480
[21,     1] loss: 1228.178
[22,     1] loss: 1153.532
[23,     1] loss: 1142.156
[24,     1] loss: 1118.039
[25,     1] loss: 1175.749
[26,     1] loss: 1106.143
[27,     1] loss: 1101.599
[28,     1] loss: 1155.324
[29,     1] loss: 1123.005
[30,     1] loss: 1112.334
[31,     1] loss: 1032.831
[32,     1] loss: 1029.618
[33,     1] loss: 1091.786
[34,     1] loss: 1026.285
[35,     1] loss: 1032.224
[36,     1] loss: 1025.553
[37,     1] loss: 1013.593
[38,     1] loss: 1049.634
[39,     1] loss: 1002.545
[40,     1] loss: 978.005
[41,     1] loss: 981.970
[42,     1] loss: 1014.859
[43,     1] loss: 990.684
[44,     1] loss: 1009.650
[45,     1] loss: 995.810
[46,     1] loss: 874.172
[47,     1] loss: 977.550
[48,     1] loss: 915.202
[49,     1] loss: 916.969
[50,     1] loss: 978.237
[51,     1] loss: 870.205
[52,     1] loss: 847.046
[53,     1] loss: 877.543
[54,     1] loss: 865.494
[55,     1] loss: 839.714
[56,     1] loss: 849.279
[57,     1] loss: 840.206
[58,     1] loss: 932.733
[59,     1] loss: 984.560
[60,     1] loss: 762.581
[61,     1] loss: 822.254
[62,     1] loss: 792.252
[63,     1] loss: 812.843
[64,     1] loss: 788.457
Early stopping applied (best metric=0.9191649556159973)
Finished Training
Total time taken: 10.95125937461853
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1393.566
[2,     1] loss: 1378.448
[3,     1] loss: 1401.315
[4,     1] loss: 1379.192
[5,     1] loss: 1386.183
[6,     1] loss: 1387.329
[7,     1] loss: 1384.735
[8,     1] loss: 1387.289
[9,     1] loss: 1385.112
[10,     1] loss: 1380.415
[11,     1] loss: 1378.494
[12,     1] loss: 1378.830
[13,     1] loss: 1381.334
[14,     1] loss: 1372.405
[15,     1] loss: 1365.414
[16,     1] loss: 1364.187
[17,     1] loss: 1346.381
[18,     1] loss: 1312.283
[19,     1] loss: 1259.811
[20,     1] loss: 1221.775
[21,     1] loss: 1187.970
[22,     1] loss: 1163.798
[23,     1] loss: 1189.666
[24,     1] loss: 1115.853
[25,     1] loss: 1115.662
[26,     1] loss: 1150.259
[27,     1] loss: 1085.478
[28,     1] loss: 1123.637
[29,     1] loss: 1105.794
[30,     1] loss: 1149.253
[31,     1] loss: 1085.157
[32,     1] loss: 1070.536
[33,     1] loss: 1076.267
[34,     1] loss: 1084.370
[35,     1] loss: 1032.963
[36,     1] loss: 1071.295
[37,     1] loss: 1077.601
[38,     1] loss: 978.429
[39,     1] loss: 989.987
[40,     1] loss: 990.950
[41,     1] loss: 1018.728
[42,     1] loss: 914.740
[43,     1] loss: 976.770
[44,     1] loss: 1006.006
[45,     1] loss: 948.427
[46,     1] loss: 1038.350
[47,     1] loss: 977.227
[48,     1] loss: 978.826
[49,     1] loss: 938.019
[50,     1] loss: 964.114
[51,     1] loss: 952.264
[52,     1] loss: 890.161
[53,     1] loss: 928.668
[54,     1] loss: 873.455
[55,     1] loss: 828.936
[56,     1] loss: 948.989
[57,     1] loss: 864.782
[58,     1] loss: 915.932
[59,     1] loss: 872.446
[60,     1] loss: 846.271
[61,     1] loss: 784.844
[62,     1] loss: 779.287
[63,     1] loss: 826.616
[64,     1] loss: 888.427
[65,     1] loss: 828.339
[66,     1] loss: 813.054
[67,     1] loss: 798.812
[68,     1] loss: 809.334
[69,     1] loss: 716.881
[70,     1] loss: 695.969
[71,     1] loss: 946.053
[72,     1] loss: 798.805
[73,     1] loss: 754.064
[74,     1] loss: 777.913
[75,     1] loss: 704.379
[76,     1] loss: 696.790
[77,     1] loss: 761.482
[78,     1] loss: 655.987
[79,     1] loss: 732.547
[80,     1] loss: 748.699
[81,     1] loss: 596.677
[82,     1] loss: 708.120
[83,     1] loss: 702.701
[84,     1] loss: 587.358
[85,     1] loss: 564.384
Early stopping applied (best metric=0.7662192583084106)
Finished Training
Total time taken: 12.551466703414917
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1383.039
[2,     1] loss: 1411.850
[3,     1] loss: 1381.926
[4,     1] loss: 1387.447
[5,     1] loss: 1388.548
[6,     1] loss: 1381.668
[7,     1] loss: 1384.106
[8,     1] loss: 1381.777
[9,     1] loss: 1381.014
[10,     1] loss: 1380.367
[11,     1] loss: 1376.594
[12,     1] loss: 1382.096
[13,     1] loss: 1382.488
[14,     1] loss: 1384.556
[15,     1] loss: 1382.685
[16,     1] loss: 1384.523
[17,     1] loss: 1381.177
[18,     1] loss: 1380.513
[19,     1] loss: 1379.514
[20,     1] loss: 1378.364
[21,     1] loss: 1382.322
[22,     1] loss: 1380.106
[23,     1] loss: 1379.905
[24,     1] loss: 1379.394
[25,     1] loss: 1378.580
[26,     1] loss: 1378.309
[27,     1] loss: 1373.768
[28,     1] loss: 1370.711
[29,     1] loss: 1363.852
[30,     1] loss: 1351.931
[31,     1] loss: 1338.196
[32,     1] loss: 1308.227
[33,     1] loss: 1269.861
[34,     1] loss: 1227.606
[35,     1] loss: 1217.801
[36,     1] loss: 1185.995
[37,     1] loss: 1161.981
[38,     1] loss: 1139.598
[39,     1] loss: 1109.868
[40,     1] loss: 1188.812
[41,     1] loss: 1130.502
[42,     1] loss: 1104.309
[43,     1] loss: 1069.209
[44,     1] loss: 1085.817
[45,     1] loss: 1043.156
[46,     1] loss: 1112.865
[47,     1] loss: 1098.999
[48,     1] loss: 1055.495
[49,     1] loss: 1008.047
[50,     1] loss: 1049.012
[51,     1] loss: 969.030
[52,     1] loss: 997.312
[53,     1] loss: 1014.280
[54,     1] loss: 994.257
[55,     1] loss: 958.033
[56,     1] loss: 947.831
[57,     1] loss: 1021.579
[58,     1] loss: 930.046
[59,     1] loss: 954.548
[60,     1] loss: 900.260
[61,     1] loss: 882.085
[62,     1] loss: 851.692
[63,     1] loss: 818.624
[64,     1] loss: 853.966
[65,     1] loss: 917.914
[66,     1] loss: 936.869
[67,     1] loss: 786.386
[68,     1] loss: 926.618
[69,     1] loss: 802.456
Early stopping applied (best metric=0.8800712823867798)
Finished Training
Total time taken: 13.32918119430542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1383.645
[2,     1] loss: 1383.446
[3,     1] loss: 1386.543
[4,     1] loss: 1381.527
[5,     1] loss: 1379.271
[6,     1] loss: 1385.656
[7,     1] loss: 1376.069
[8,     1] loss: 1376.248
[9,     1] loss: 1362.368
[10,     1] loss: 1359.143
[11,     1] loss: 1337.058
[12,     1] loss: 1319.073
[13,     1] loss: 1278.999
[14,     1] loss: 1224.726
[15,     1] loss: 1272.875
[16,     1] loss: 1141.189
[17,     1] loss: 1201.392
[18,     1] loss: 1132.888
[19,     1] loss: 1114.118
[20,     1] loss: 1096.660
[21,     1] loss: 1156.597
[22,     1] loss: 1127.080
[23,     1] loss: 1140.223
[24,     1] loss: 1146.159
[25,     1] loss: 1121.471
[26,     1] loss: 1120.392
[27,     1] loss: 1060.675
[28,     1] loss: 1072.239
[29,     1] loss: 1063.746
[30,     1] loss: 1014.869
[31,     1] loss: 1071.300
[32,     1] loss: 1049.892
[33,     1] loss: 1033.296
[34,     1] loss: 1047.486
[35,     1] loss: 1013.020
[36,     1] loss: 1057.853
[37,     1] loss: 998.598
[38,     1] loss: 977.744
[39,     1] loss: 945.224
[40,     1] loss: 946.182
[41,     1] loss: 1023.561
[42,     1] loss: 937.933
[43,     1] loss: 980.312
[44,     1] loss: 931.011
[45,     1] loss: 927.215
[46,     1] loss: 880.626
[47,     1] loss: 932.537
[48,     1] loss: 895.331
[49,     1] loss: 899.644
[50,     1] loss: 912.620
[51,     1] loss: 823.459
[52,     1] loss: 878.328
[53,     1] loss: 905.945
[54,     1] loss: 860.936
[55,     1] loss: 904.465
[56,     1] loss: 861.497
[57,     1] loss: 856.053
[58,     1] loss: 776.492
[59,     1] loss: 809.432
[60,     1] loss: 857.857
[61,     1] loss: 727.101
[62,     1] loss: 707.715
[63,     1] loss: 788.528
[64,     1] loss: 790.654
[65,     1] loss: 745.981
[66,     1] loss: 682.962
[67,     1] loss: 805.528
[68,     1] loss: 813.874
[69,     1] loss: 636.405
[70,     1] loss: 623.525
[71,     1] loss: 648.991
[72,     1] loss: 627.265
[73,     1] loss: 691.613
[74,     1] loss: 696.571
[75,     1] loss: 963.963
[76,     1] loss: 989.964
[77,     1] loss: 799.064
Early stopping applied (best metric=0.8945378065109253)
Finished Training
Total time taken: 12.802302122116089
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.573
[2,     1] loss: 1382.390
[3,     1] loss: 1379.473
[4,     1] loss: 1388.949
[5,     1] loss: 1381.254
[6,     1] loss: 1383.373
[7,     1] loss: 1378.595
[8,     1] loss: 1381.402
[9,     1] loss: 1383.740
[10,     1] loss: 1379.987
[11,     1] loss: 1376.039
[12,     1] loss: 1372.424
[13,     1] loss: 1382.605
[14,     1] loss: 1372.832
[15,     1] loss: 1369.566
[16,     1] loss: 1361.227
[17,     1] loss: 1349.058
[18,     1] loss: 1324.634
[19,     1] loss: 1292.583
[20,     1] loss: 1259.723
[21,     1] loss: 1217.898
[22,     1] loss: 1185.613
[23,     1] loss: 1171.884
[24,     1] loss: 1157.886
[25,     1] loss: 1127.396
[26,     1] loss: 1129.264
[27,     1] loss: 1189.108
[28,     1] loss: 1143.514
[29,     1] loss: 1086.317
[30,     1] loss: 1103.333
[31,     1] loss: 1123.068
[32,     1] loss: 1082.329
[33,     1] loss: 1068.351
[34,     1] loss: 1082.303
[35,     1] loss: 1007.539
[36,     1] loss: 1033.001
[37,     1] loss: 1048.333
[38,     1] loss: 1034.921
[39,     1] loss: 1056.281
[40,     1] loss: 994.970
[41,     1] loss: 985.266
[42,     1] loss: 1016.962
[43,     1] loss: 956.102
[44,     1] loss: 947.328
[45,     1] loss: 1012.526
[46,     1] loss: 980.773
[47,     1] loss: 981.243
[48,     1] loss: 965.271
[49,     1] loss: 980.775
[50,     1] loss: 962.277
[51,     1] loss: 941.271
[52,     1] loss: 906.710
[53,     1] loss: 905.153
[54,     1] loss: 943.871
Early stopping applied (best metric=0.7879339456558228)
Finished Training
Total time taken: 9.014704942703247
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1382.214
[2,     1] loss: 1385.707
[3,     1] loss: 1387.415
[4,     1] loss: 1385.759
[5,     1] loss: 1382.296
[6,     1] loss: 1381.967
[7,     1] loss: 1382.650
[8,     1] loss: 1379.124
[9,     1] loss: 1379.325
[10,     1] loss: 1380.198
[11,     1] loss: 1381.140
[12,     1] loss: 1377.506
[13,     1] loss: 1373.084
[14,     1] loss: 1369.272
[15,     1] loss: 1363.600
[16,     1] loss: 1353.365
[17,     1] loss: 1329.756
[18,     1] loss: 1293.250
[19,     1] loss: 1267.835
[20,     1] loss: 1214.609
[21,     1] loss: 1179.814
[22,     1] loss: 1155.270
[23,     1] loss: 1133.757
[24,     1] loss: 1192.030
[25,     1] loss: 1088.050
[26,     1] loss: 1121.864
[27,     1] loss: 1108.778
[28,     1] loss: 1120.192
[29,     1] loss: 1113.970
[30,     1] loss: 1106.716
[31,     1] loss: 1064.841
[32,     1] loss: 1077.001
[33,     1] loss: 959.084
[34,     1] loss: 947.985
[35,     1] loss: 1048.995
[36,     1] loss: 1091.889
[37,     1] loss: 1046.451
[38,     1] loss: 1023.720
[39,     1] loss: 978.324
[40,     1] loss: 1022.547
[41,     1] loss: 1006.109
[42,     1] loss: 1004.485
[43,     1] loss: 959.098
[44,     1] loss: 1020.403
[45,     1] loss: 938.525
[46,     1] loss: 990.251
[47,     1] loss: 934.612
[48,     1] loss: 928.029
[49,     1] loss: 917.556
[50,     1] loss: 921.834
[51,     1] loss: 898.813
[52,     1] loss: 871.582
[53,     1] loss: 888.741
[54,     1] loss: 807.390
[55,     1] loss: 1060.391
[56,     1] loss: 1014.776
[57,     1] loss: 875.066
[58,     1] loss: 1032.528
[59,     1] loss: 843.344
[60,     1] loss: 978.418
Early stopping applied (best metric=0.9026910066604614)
Finished Training
Total time taken: 9.879753828048706
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1386.806
[2,     1] loss: 1392.338
[3,     1] loss: 1383.133
[4,     1] loss: 1383.157
[5,     1] loss: 1386.740
[6,     1] loss: 1384.389
[7,     1] loss: 1374.206
[8,     1] loss: 1378.032
[9,     1] loss: 1372.180
[10,     1] loss: 1366.302
[11,     1] loss: 1349.292
[12,     1] loss: 1339.543
[13,     1] loss: 1299.342
[14,     1] loss: 1247.806
[15,     1] loss: 1238.002
[16,     1] loss: 1164.666
[17,     1] loss: 1162.217
[18,     1] loss: 1201.041
[19,     1] loss: 1220.241
[20,     1] loss: 1134.607
[21,     1] loss: 1131.249
[22,     1] loss: 1097.635
[23,     1] loss: 1175.636
[24,     1] loss: 1112.860
[25,     1] loss: 1122.864
[26,     1] loss: 1119.211
[27,     1] loss: 1104.835
[28,     1] loss: 1088.616
[29,     1] loss: 1093.978
[30,     1] loss: 1076.250
[31,     1] loss: 1074.964
[32,     1] loss: 1071.343
[33,     1] loss: 1059.424
[34,     1] loss: 1023.204
[35,     1] loss: 974.228
[36,     1] loss: 1014.792
[37,     1] loss: 1015.067
[38,     1] loss: 999.263
[39,     1] loss: 1007.627
[40,     1] loss: 1016.274
[41,     1] loss: 1013.163
[42,     1] loss: 1079.493
[43,     1] loss: 976.017
[44,     1] loss: 1005.808
[45,     1] loss: 1002.191
[46,     1] loss: 1000.081
[47,     1] loss: 993.190
[48,     1] loss: 967.007
[49,     1] loss: 960.449
[50,     1] loss: 911.126
[51,     1] loss: 913.731
[52,     1] loss: 903.211
[53,     1] loss: 848.579
[54,     1] loss: 851.671
[55,     1] loss: 884.260
[56,     1] loss: 824.101
[57,     1] loss: 840.184
[58,     1] loss: 859.944
[59,     1] loss: 863.635
[60,     1] loss: 805.547
[61,     1] loss: 819.266
[62,     1] loss: 921.903
[63,     1] loss: 853.652
[64,     1] loss: 799.527
[65,     1] loss: 798.557
[66,     1] loss: 868.254
[67,     1] loss: 742.218
[68,     1] loss: 940.572
[69,     1] loss: 890.187
Early stopping applied (best metric=0.837554931640625)
Finished Training
Total time taken: 13.458839416503906
{'Hydroxylation-K Validation Accuracy': 0.760579196217494, 'Hydroxylation-K Validation Sensitivity': 0.6785185185185185, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.444685477920772, 'Hydroxylation-K AUC ROC': 0.8017153996101365, 'Hydroxylation-K AUC PR': 0.6190880279596467, 'Hydroxylation-K MCC': 0.40120728698719016, 'Hydroxylation-K F1': 0.5355089676828807, 'Validation Loss (Hydroxylation-K)': 0.455178439617157, 'Hydroxylation-P Validation Accuracy': 0.7836091907348188, 'Hydroxylation-P Validation Sensitivity': 0.8124867724867725, 'Hydroxylation-P Validation Specificity': 0.7773629607461718, 'Hydroxylation-P Validation Precision': 0.4479172267678778, 'Hydroxylation-P AUC ROC': 0.8590183910530312, 'Hydroxylation-P AUC PR': 0.6191982108569223, 'Hydroxylation-P MCC': 0.48509680402196115, 'Hydroxylation-P F1': 0.5729526968301158, 'Validation Loss (Hydroxylation-P)': 0.3607971588770548, 'Validation Loss (total)': 0.8159755905469258, 'TimeToTrain': 12.104859654108683}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007379756468107609,
 'learning_rate_Hydroxylation-K': 0.0005440016355411326,
 'learning_rate_Hydroxylation-P': 0.003694626160171326,
 'log_base': 1.1142551021255838,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2963656561,
 'sample_weights': [2.2484506106341473, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.676777916295965,
 'weight_decay_Hydroxylation-K': 4.916339735161102,
 'weight_decay_Hydroxylation-P': 4.523185562220434}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5018.731
[2,     1] loss: 5002.838
[3,     1] loss: 5040.695
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005497868718113392,
 'learning_rate_Hydroxylation-K': 0.004444522261694297,
 'learning_rate_Hydroxylation-P': 0.0016876731051170242,
 'log_base': 1.2974680709208566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3500938948,
 'sample_weights': [15.431214963461219, 1.9289761794784557],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.313434520400404,
 'weight_decay_Hydroxylation-K': 5.404153978750372,
 'weight_decay_Hydroxylation-P': 4.670475062694668}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2271.108
[2,     1] loss: 2267.220
[3,     1] loss: 2268.465
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009168137344892835,
 'learning_rate_Hydroxylation-K': 0.009050244923950105,
 'learning_rate_Hydroxylation-P': 0.0029789896393333836,
 'log_base': 1.7090621776865795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1475077196,
 'sample_weights': [6.410709420311477, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.392618744085021,
 'weight_decay_Hydroxylation-K': 3.5505808305943027,
 'weight_decay_Hydroxylation-P': 4.735661934979164}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.547
[2,     1] loss: 1561.179
[3,     1] loss: 1569.373
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007851884751999995,
 'learning_rate_Hydroxylation-K': 0.0002483976752030554,
 'learning_rate_Hydroxylation-P': 0.003098659165791518,
 'log_base': 1.7461361822243602,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1651059110,
 'sample_weights': [3.1149536130067785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.5411912560437195,
 'weight_decay_Hydroxylation-K': 7.773142965096368,
 'weight_decay_Hydroxylation-P': 6.274129482645662}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1544.618
[2,     1] loss: 1549.232
[3,     1] loss: 1536.135
[4,     1] loss: 1552.760
[5,     1] loss: 1539.062
[6,     1] loss: 1542.381
[7,     1] loss: 1535.453
[8,     1] loss: 1530.985
[9,     1] loss: 1534.113
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002609668349498338,
 'learning_rate_Hydroxylation-K': 0.0020431159032085677,
 'learning_rate_Hydroxylation-P': 0.006514787558558672,
 'log_base': 2.456078545735667,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 93182971,
 'sample_weights': [2.9950247960892473, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.07551773387771,
 'weight_decay_Hydroxylation-K': 5.843059203875953,
 'weight_decay_Hydroxylation-P': 2.5019679289167307}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1298.767
[2,     1] loss: 1301.726
[3,     1] loss: 1296.323
[4,     1] loss: 1297.324
[5,     1] loss: 1294.034
[6,     1] loss: 1297.763
[7,     1] loss: 1296.982
[8,     1] loss: 1291.207
[9,     1] loss: 1287.324
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007198974975780623,
 'learning_rate_Hydroxylation-K': 0.008816132177727119,
 'learning_rate_Hydroxylation-P': 0.007839325589065792,
 'log_base': 2.4807612092294318,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4195463975,
 'sample_weights': [1.8578970985583751, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.902692258259598,
 'weight_decay_Hydroxylation-K': 7.511943161064369,
 'weight_decay_Hydroxylation-P': 4.316003032807882}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.764
[2,     1] loss: 1292.661
[3,     1] loss: 1294.343
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037693326584585015,
 'learning_rate_Hydroxylation-K': 0.0014831881916470278,
 'learning_rate_Hydroxylation-P': 0.0025055828321756232,
 'log_base': 1.1196310893584482,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 119864560,
 'sample_weights': [1.8374495125508774, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.706156747478694,
 'weight_decay_Hydroxylation-K': 4.7406244898647625,
 'weight_decay_Hydroxylation-P': 5.2095401463388935}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4807.488
[2,     1] loss: 4803.780
[3,     1] loss: 4806.527
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008171921864781656,
 'learning_rate_Hydroxylation-K': 0.005314141701496022,
 'learning_rate_Hydroxylation-P': 0.005577803582907065,
 'log_base': 2.2189485985640065,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2621983282,
 'sample_weights': [14.773931671613378, 1.8468126028484997],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.211384022070521,
 'weight_decay_Hydroxylation-K': 6.161807005274154,
 'weight_decay_Hydroxylation-P': 6.244611284590372}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.391
[2,     1] loss: 1355.855
[3,     1] loss: 1349.722
[4,     1] loss: 1354.055
[5,     1] loss: 1347.364
[6,     1] loss: 1349.964
[7,     1] loss: 1341.991
[8,     1] loss: 1344.505
[9,     1] loss: 1341.876
[10,     1] loss: 1326.294
[11,     1] loss: 1307.804
[12,     1] loss: 1286.052
[13,     1] loss: 1260.129
[14,     1] loss: 1225.469
[15,     1] loss: 1193.639
[16,     1] loss: 1160.247
[17,     1] loss: 1187.671
[18,     1] loss: 1156.985
[19,     1] loss: 1117.388
[20,     1] loss: 1125.769
[21,     1] loss: 1093.020
[22,     1] loss: 1117.252
[23,     1] loss: 1098.467
[24,     1] loss: 1106.409
[25,     1] loss: 1061.932
[26,     1] loss: 1100.008
[27,     1] loss: 1036.471
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004387000309692992,
 'learning_rate_Hydroxylation-K': 0.0071165890027917785,
 'learning_rate_Hydroxylation-P': 0.006651796630287078,
 'log_base': 2.9952241025841246,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2451349768,
 'sample_weights': [2.0945709188781407, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9875938980589005,
 'weight_decay_Hydroxylation-K': 4.708081160624299,
 'weight_decay_Hydroxylation-P': 9.489628906800766}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.295
[2,     1] loss: 1229.333
[3,     1] loss: 1225.604
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0063284064253566865,
 'learning_rate_Hydroxylation-K': 0.0018753974620626865,
 'learning_rate_Hydroxylation-P': 0.0021714327748629564,
 'log_base': 1.464983015396586,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 170641373,
 'sample_weights': [1.5217995902814276, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.023107854335269,
 'weight_decay_Hydroxylation-K': 1.9422178937205077,
 'weight_decay_Hydroxylation-P': 1.1777217696051165}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1841.872
[2,     1] loss: 1832.182
[3,     1] loss: 1835.834
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007062514771905289,
 'learning_rate_Hydroxylation-K': 0.008556054877289278,
 'learning_rate_Hydroxylation-P': 0.007503692087340179,
 'log_base': 1.8653003211246635,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 407215142,
 'sample_weights': [4.372059487176837, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8932003436609466,
 'weight_decay_Hydroxylation-K': 3.2714315764110387,
 'weight_decay_Hydroxylation-P': 8.875264324977977}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1481.721
[2,     1] loss: 1471.586
[3,     1] loss: 1474.046
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006264751929006035,
 'learning_rate_Hydroxylation-K': 0.007016303423428638,
 'learning_rate_Hydroxylation-P': 0.005007929444359642,
 'log_base': 2.5610519459021335,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 420719510,
 'sample_weights': [2.677869820713711, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.293456463398306,
 'weight_decay_Hydroxylation-K': 6.92792835188469,
 'weight_decay_Hydroxylation-P': 0.24396036139092003}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.053
[2,     1] loss: 1279.684
[3,     1] loss: 1281.786
[4,     1] loss: 1276.732
[5,     1] loss: 1270.748
[6,     1] loss: 1256.116
[7,     1] loss: 1233.839
[8,     1] loss: 1202.762
[9,     1] loss: 1162.348
[10,     1] loss: 1107.263
[11,     1] loss: 1038.728
[12,     1] loss: 1149.532
[13,     1] loss: 1066.435
[14,     1] loss: 999.609
[15,     1] loss: 1077.695
[16,     1] loss: 1022.510
[17,     1] loss: 1004.720
[18,     1] loss: 1007.165
[19,     1] loss: 1002.180
[20,     1] loss: 968.782
[21,     1] loss: 965.059
[22,     1] loss: 959.604
[23,     1] loss: 956.991
[24,     1] loss: 864.323
[25,     1] loss: 901.626
[26,     1] loss: 910.894
[27,     1] loss: 882.925
[28,     1] loss: 919.800
[29,     1] loss: 1056.847
[30,     1] loss: 900.313
[31,     1] loss: 949.429
[32,     1] loss: 880.505
[33,     1] loss: 937.230
[34,     1] loss: 894.211
[35,     1] loss: 867.446
[36,     1] loss: 870.844
[37,     1] loss: 899.585
[38,     1] loss: 832.562
[39,     1] loss: 829.701
[40,     1] loss: 868.152
[41,     1] loss: 874.996
[42,     1] loss: 769.642
[43,     1] loss: 897.906
[44,     1] loss: 915.732
[45,     1] loss: 770.956
[46,     1] loss: 889.425
[47,     1] loss: 772.446
[48,     1] loss: 922.552
[49,     1] loss: 801.122
[50,     1] loss: 778.549
[51,     1] loss: 799.815
[52,     1] loss: 739.709
[53,     1] loss: 770.465
[54,     1] loss: 705.724
[55,     1] loss: 717.172
[56,     1] loss: 779.445
[57,     1] loss: 748.580
[58,     1] loss: 749.097
[59,     1] loss: 686.384
[60,     1] loss: 740.813
[61,     1] loss: 759.317
[62,     1] loss: 661.106
[63,     1] loss: 707.885
[64,     1] loss: 686.571
[65,     1] loss: 603.314
[66,     1] loss: 575.240
[67,     1] loss: 517.333
[68,     1] loss: 725.571
[69,     1] loss: 2091.935
[70,     1] loss: 713.486
[71,     1] loss: 1184.533
[72,     1] loss: 952.361
[73,     1] loss: 1050.902
[74,     1] loss: 1080.145
[75,     1] loss: 1062.928
[76,     1] loss: 1024.383
[77,     1] loss: 982.708
[78,     1] loss: 904.367
[79,     1] loss: 988.842
[80,     1] loss: 988.304
[81,     1] loss: 902.020
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003138758454402495,
 'learning_rate_Hydroxylation-K': 0.002026500718414896,
 'learning_rate_Hydroxylation-P': 0.00576545037550613,
 'log_base': 1.7423008051686104,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2941459113,
 'sample_weights': [1.7752137739218936, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3606540067752886,
 'weight_decay_Hydroxylation-K': 6.611476406533138,
 'weight_decay_Hydroxylation-P': 0.5731796500551776}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1544.707
[2,     1] loss: 1542.173
[3,     1] loss: 1537.881
[4,     1] loss: 1546.727
[5,     1] loss: 1537.883
[6,     1] loss: 1537.359
[7,     1] loss: 1539.975
[8,     1] loss: 1544.004
[9,     1] loss: 1534.117
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006970456861721976,
 'learning_rate_Hydroxylation-K': 0.0017208541157258197,
 'learning_rate_Hydroxylation-P': 0.004796476077272949,
 'log_base': 1.0837836561238507,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 323461594,
 'sample_weights': [3.0068866672280388, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.36545072077163,
 'weight_decay_Hydroxylation-K': 8.178765912639392,
 'weight_decay_Hydroxylation-P': 6.744909834242934}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6783.094
[2,     1] loss: 6747.749
[3,     1] loss: 6759.944
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007511523875813068,
 'learning_rate_Hydroxylation-K': 0.0005715638322244218,
 'learning_rate_Hydroxylation-P': 0.008139911151343248,
 'log_base': 1.1346553914068784,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1619063098,
 'sample_weights': [20.749171520803408, 2.5937463577764204],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.085681780714965,
 'weight_decay_Hydroxylation-K': 7.652801253524225,
 'weight_decay_Hydroxylation-P': 3.294325398387898}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4317.445
[2,     1] loss: 4284.867
[3,     1] loss: 4278.079
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035911966982085956,
 'learning_rate_Hydroxylation-K': 0.0013363427431226677,
 'learning_rate_Hydroxylation-P': 0.0028333105856421484,
 'log_base': 1.7775574393904188,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 644177994,
 'sample_weights': [13.21504442077058, 1.6519441896685478],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.237705867733518,
 'weight_decay_Hydroxylation-K': 1.7897794745889561,
 'weight_decay_Hydroxylation-P': 0.2300846716431837}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1517.250
[2,     1] loss: 1524.427
[3,     1] loss: 1524.125
[4,     1] loss: 1514.625
[5,     1] loss: 1518.421
[6,     1] loss: 1515.553
[7,     1] loss: 1522.738
[8,     1] loss: 1516.917
[9,     1] loss: 1506.174
[10,     1] loss: 1498.468
[11,     1] loss: 1479.907
[12,     1] loss: 1453.608
[13,     1] loss: 1419.046
[14,     1] loss: 1378.512
[15,     1] loss: 1338.456
[16,     1] loss: 1347.193
[17,     1] loss: 1310.934
[18,     1] loss: 1288.493
[19,     1] loss: 1308.760
[20,     1] loss: 1241.022
[21,     1] loss: 1287.802
[22,     1] loss: 1225.391
[23,     1] loss: 1216.680
[24,     1] loss: 1274.035
[25,     1] loss: 1209.240
[26,     1] loss: 1241.542
[27,     1] loss: 1233.691
[28,     1] loss: 1235.617
[29,     1] loss: 1202.131
[30,     1] loss: 1185.687
[31,     1] loss: 1152.408
[32,     1] loss: 1170.646
[33,     1] loss: 1118.379
[34,     1] loss: 1094.326
[35,     1] loss: 1099.104
[36,     1] loss: 1163.625
[37,     1] loss: 1117.102
[38,     1] loss: 1078.440
[39,     1] loss: 1120.538
[40,     1] loss: 1099.683
[41,     1] loss: 1017.018
[42,     1] loss: 1114.465
[43,     1] loss: 1083.674
[44,     1] loss: 1136.771
[45,     1] loss: 1098.075
[46,     1] loss: 989.029
[47,     1] loss: 1013.282
[48,     1] loss: 1000.430
[49,     1] loss: 1002.061
[50,     1] loss: 907.306
[51,     1] loss: 876.663
[52,     1] loss: 871.697
[53,     1] loss: 907.797
[54,     1] loss: 900.762
[55,     1] loss: 1084.956
[56,     1] loss: 1110.170
[57,     1] loss: 843.828
[58,     1] loss: 984.273
[59,     1] loss: 899.895
[60,     1] loss: 940.482
[61,     1] loss: 901.083
[62,     1] loss: 853.132
[63,     1] loss: 893.283
[64,     1] loss: 796.887
[65,     1] loss: 878.719
[66,     1] loss: 865.838
[67,     1] loss: 760.554
Early stopping applied (best metric=0.7869856357574463)
Finished Training
Total time taken: 13.083832502365112
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1520.917
[2,     1] loss: 1516.955
[3,     1] loss: 1523.222
[4,     1] loss: 1519.054
[5,     1] loss: 1522.329
[6,     1] loss: 1525.960
[7,     1] loss: 1519.820
[8,     1] loss: 1517.262
[9,     1] loss: 1520.474
[10,     1] loss: 1514.802
[11,     1] loss: 1510.147
[12,     1] loss: 1514.000
[13,     1] loss: 1507.238
[14,     1] loss: 1498.023
[15,     1] loss: 1481.443
[16,     1] loss: 1463.685
[17,     1] loss: 1446.226
[18,     1] loss: 1398.233
[19,     1] loss: 1395.105
[20,     1] loss: 1360.406
[21,     1] loss: 1343.173
[22,     1] loss: 1317.690
[23,     1] loss: 1275.237
[24,     1] loss: 1240.860
[25,     1] loss: 1304.940
[26,     1] loss: 1280.480
[27,     1] loss: 1280.711
[28,     1] loss: 1228.031
[29,     1] loss: 1228.894
[30,     1] loss: 1266.810
[31,     1] loss: 1155.155
[32,     1] loss: 1178.992
[33,     1] loss: 1160.550
[34,     1] loss: 1163.640
[35,     1] loss: 1149.025
[36,     1] loss: 1128.922
[37,     1] loss: 1163.959
[38,     1] loss: 1112.359
[39,     1] loss: 1174.235
[40,     1] loss: 1110.782
[41,     1] loss: 1175.909
[42,     1] loss: 1103.398
[43,     1] loss: 1120.942
[44,     1] loss: 1087.231
[45,     1] loss: 1079.045
[46,     1] loss: 1057.948
[47,     1] loss: 1091.392
[48,     1] loss: 1087.285
[49,     1] loss: 1053.905
[50,     1] loss: 966.046
[51,     1] loss: 985.152
[52,     1] loss: 1023.811
[53,     1] loss: 1050.373
[54,     1] loss: 1030.905
[55,     1] loss: 955.567
[56,     1] loss: 923.770
[57,     1] loss: 1031.657
[58,     1] loss: 927.521
[59,     1] loss: 964.099
[60,     1] loss: 915.997
[61,     1] loss: 917.399
[62,     1] loss: 959.126
[63,     1] loss: 876.725
[64,     1] loss: 923.442
[65,     1] loss: 881.067
[66,     1] loss: 821.181
[67,     1] loss: 919.955
[68,     1] loss: 970.323
[69,     1] loss: 800.890
[70,     1] loss: 926.186
[71,     1] loss: 806.508
[72,     1] loss: 876.354
[73,     1] loss: 755.669
[74,     1] loss: 795.399
[75,     1] loss: 838.995
[76,     1] loss: 805.750
[77,     1] loss: 758.589
[78,     1] loss: 721.292
[79,     1] loss: 662.653
[80,     1] loss: 745.515
[81,     1] loss: 815.775
Early stopping applied (best metric=0.8151670694351196)
Finished Training
Total time taken: 12.547835350036621
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1521.502
[2,     1] loss: 1530.770
[3,     1] loss: 1520.200
[4,     1] loss: 1518.392
[5,     1] loss: 1518.788
[6,     1] loss: 1518.015
[7,     1] loss: 1514.840
[8,     1] loss: 1514.868
[9,     1] loss: 1510.555
[10,     1] loss: 1494.699
[11,     1] loss: 1477.220
[12,     1] loss: 1464.536
[13,     1] loss: 1416.531
[14,     1] loss: 1390.011
[15,     1] loss: 1365.755
[16,     1] loss: 1320.468
[17,     1] loss: 1276.190
[18,     1] loss: 1269.943
[19,     1] loss: 1245.822
[20,     1] loss: 1248.652
[21,     1] loss: 1233.279
[22,     1] loss: 1297.550
[23,     1] loss: 1254.832
[24,     1] loss: 1207.404
[25,     1] loss: 1164.735
[26,     1] loss: 1175.056
[27,     1] loss: 1141.743
[28,     1] loss: 1255.689
[29,     1] loss: 1146.980
[30,     1] loss: 1172.625
[31,     1] loss: 1178.762
[32,     1] loss: 1119.786
[33,     1] loss: 1092.806
[34,     1] loss: 1060.923
[35,     1] loss: 1065.035
[36,     1] loss: 1069.768
[37,     1] loss: 1095.516
[38,     1] loss: 1027.412
[39,     1] loss: 1016.886
[40,     1] loss: 1060.632
[41,     1] loss: 996.291
[42,     1] loss: 1028.461
[43,     1] loss: 1082.281
[44,     1] loss: 963.539
[45,     1] loss: 948.945
[46,     1] loss: 937.133
[47,     1] loss: 945.602
[48,     1] loss: 914.399
[49,     1] loss: 987.878
[50,     1] loss: 827.835
[51,     1] loss: 889.093
[52,     1] loss: 973.530
[53,     1] loss: 841.112
[54,     1] loss: 849.074
[55,     1] loss: 865.115
[56,     1] loss: 848.233
[57,     1] loss: 870.555
Early stopping applied (best metric=0.8559790253639221)
Finished Training
Total time taken: 10.726873636245728
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1521.385
[2,     1] loss: 1522.314
[3,     1] loss: 1519.929
[4,     1] loss: 1522.829
[5,     1] loss: 1511.127
[6,     1] loss: 1516.459
[7,     1] loss: 1509.815
[8,     1] loss: 1501.607
[9,     1] loss: 1482.088
[10,     1] loss: 1467.558
[11,     1] loss: 1432.159
[12,     1] loss: 1400.184
[13,     1] loss: 1356.929
[14,     1] loss: 1332.746
[15,     1] loss: 1282.757
[16,     1] loss: 1283.154
[17,     1] loss: 1317.781
[18,     1] loss: 1303.928
[19,     1] loss: 1263.235
[20,     1] loss: 1201.710
[21,     1] loss: 1237.988
[22,     1] loss: 1234.469
[23,     1] loss: 1205.610
[24,     1] loss: 1242.807
[25,     1] loss: 1208.331
[26,     1] loss: 1159.740
[27,     1] loss: 1140.336
[28,     1] loss: 1148.371
[29,     1] loss: 1098.398
[30,     1] loss: 1144.126
[31,     1] loss: 1180.116
[32,     1] loss: 1052.215
[33,     1] loss: 1191.973
[34,     1] loss: 1111.198
[35,     1] loss: 1091.201
[36,     1] loss: 1100.863
[37,     1] loss: 1087.431
[38,     1] loss: 1032.347
[39,     1] loss: 1075.074
[40,     1] loss: 1014.091
[41,     1] loss: 1042.238
[42,     1] loss: 1032.362
[43,     1] loss: 1039.402
[44,     1] loss: 1105.482
[45,     1] loss: 1005.063
[46,     1] loss: 912.720
[47,     1] loss: 1027.889
[48,     1] loss: 962.392
[49,     1] loss: 981.698
[50,     1] loss: 874.047
[51,     1] loss: 980.208
[52,     1] loss: 902.304
[53,     1] loss: 943.347
[54,     1] loss: 895.203
[55,     1] loss: 805.822
[56,     1] loss: 853.170
[57,     1] loss: 910.675
[58,     1] loss: 917.936
[59,     1] loss: 1287.187
[60,     1] loss: 786.544
[61,     1] loss: 1113.287
[62,     1] loss: 899.225
[63,     1] loss: 1022.583
[64,     1] loss: 1041.608
[65,     1] loss: 939.334
[66,     1] loss: 1032.835
[67,     1] loss: 870.148
[68,     1] loss: 911.459
[69,     1] loss: 959.628
[70,     1] loss: 775.990
[71,     1] loss: 930.158
[72,     1] loss: 770.529
[73,     1] loss: 848.585
[74,     1] loss: 745.506
Early stopping applied (best metric=0.6751783490180969)
Finished Training
Total time taken: 14.458913564682007
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1529.576
[2,     1] loss: 1519.751
[3,     1] loss: 1519.116
[4,     1] loss: 1521.818
[5,     1] loss: 1517.341
[6,     1] loss: 1514.295
[7,     1] loss: 1515.745
[8,     1] loss: 1500.363
[9,     1] loss: 1473.291
[10,     1] loss: 1445.620
[11,     1] loss: 1400.418
[12,     1] loss: 1340.378
[13,     1] loss: 1339.746
[14,     1] loss: 1328.663
[15,     1] loss: 1310.517
[16,     1] loss: 1283.800
[17,     1] loss: 1280.046
[18,     1] loss: 1289.873
[19,     1] loss: 1276.089
[20,     1] loss: 1255.508
[21,     1] loss: 1231.349
[22,     1] loss: 1237.588
[23,     1] loss: 1253.635
[24,     1] loss: 1191.037
[25,     1] loss: 1221.143
[26,     1] loss: 1173.874
[27,     1] loss: 1205.771
[28,     1] loss: 1184.986
[29,     1] loss: 1099.166
[30,     1] loss: 1173.890
[31,     1] loss: 1147.199
[32,     1] loss: 1179.033
[33,     1] loss: 1109.527
[34,     1] loss: 1202.844
[35,     1] loss: 1127.153
[36,     1] loss: 1132.801
[37,     1] loss: 1076.729
[38,     1] loss: 1074.941
[39,     1] loss: 1081.247
[40,     1] loss: 999.554
[41,     1] loss: 994.954
[42,     1] loss: 1085.151
[43,     1] loss: 971.396
[44,     1] loss: 1068.996
[45,     1] loss: 1050.092
[46,     1] loss: 953.166
[47,     1] loss: 1011.939
[48,     1] loss: 973.115
[49,     1] loss: 963.839
[50,     1] loss: 1076.937
[51,     1] loss: 998.480
[52,     1] loss: 914.341
[53,     1] loss: 928.136
[54,     1] loss: 953.532
[55,     1] loss: 909.704
[56,     1] loss: 823.117
[57,     1] loss: 876.370
[58,     1] loss: 922.085
[59,     1] loss: 1130.638
[60,     1] loss: 1109.723
[61,     1] loss: 869.109
Early stopping applied (best metric=0.7958199977874756)
Finished Training
Total time taken: 10.814292192459106
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1521.803
[2,     1] loss: 1520.605
[3,     1] loss: 1519.597
[4,     1] loss: 1517.292
[5,     1] loss: 1520.136
[6,     1] loss: 1518.097
[7,     1] loss: 1518.886
[8,     1] loss: 1515.285
[9,     1] loss: 1515.519
[10,     1] loss: 1512.130
[11,     1] loss: 1507.272
[12,     1] loss: 1485.834
[13,     1] loss: 1471.563
[14,     1] loss: 1443.415
[15,     1] loss: 1425.498
[16,     1] loss: 1390.198
[17,     1] loss: 1355.269
[18,     1] loss: 1328.319
[19,     1] loss: 1331.349
[20,     1] loss: 1276.788
[21,     1] loss: 1211.550
[22,     1] loss: 1237.698
[23,     1] loss: 1299.153
[24,     1] loss: 1227.195
[25,     1] loss: 1272.873
[26,     1] loss: 1216.350
[27,     1] loss: 1199.399
[28,     1] loss: 1208.496
[29,     1] loss: 1165.326
[30,     1] loss: 1123.085
[31,     1] loss: 1201.111
[32,     1] loss: 1127.411
[33,     1] loss: 1242.195
[34,     1] loss: 1135.705
[35,     1] loss: 1206.374
[36,     1] loss: 1121.808
[37,     1] loss: 1154.174
[38,     1] loss: 1084.067
[39,     1] loss: 1103.045
[40,     1] loss: 1122.131
[41,     1] loss: 1038.261
[42,     1] loss: 1134.197
[43,     1] loss: 1087.823
[44,     1] loss: 1030.792
[45,     1] loss: 1016.938
[46,     1] loss: 951.019
[47,     1] loss: 992.511
[48,     1] loss: 933.555
[49,     1] loss: 980.529
[50,     1] loss: 937.263
[51,     1] loss: 940.925
[52,     1] loss: 1012.320
[53,     1] loss: 1038.137
[54,     1] loss: 898.006
[55,     1] loss: 863.829
[56,     1] loss: 944.819
[57,     1] loss: 847.902
[58,     1] loss: 839.047
[59,     1] loss: 850.748
[60,     1] loss: 822.360
[61,     1] loss: 767.895
[62,     1] loss: 800.068
[63,     1] loss: 1066.096
[64,     1] loss: 1270.520
[65,     1] loss: 863.109
[66,     1] loss: 928.931
[67,     1] loss: 901.051
[68,     1] loss: 978.390
[69,     1] loss: 935.892
[70,     1] loss: 866.087
[71,     1] loss: 885.257
[72,     1] loss: 907.118
[73,     1] loss: 795.520
Early stopping applied (best metric=0.8017050623893738)
Finished Training
Total time taken: 11.749298810958862
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.235
[2,     1] loss: 1518.760
[3,     1] loss: 1516.616
[4,     1] loss: 1530.448
[5,     1] loss: 1515.307
[6,     1] loss: 1511.947
[7,     1] loss: 1508.291
[8,     1] loss: 1499.870
[9,     1] loss: 1488.438
[10,     1] loss: 1450.407
[11,     1] loss: 1426.742
[12,     1] loss: 1349.323
[13,     1] loss: 1341.576
[14,     1] loss: 1306.583
[15,     1] loss: 1306.914
[16,     1] loss: 1243.930
[17,     1] loss: 1265.684
[18,     1] loss: 1280.387
[19,     1] loss: 1188.124
[20,     1] loss: 1216.261
[21,     1] loss: 1247.288
[22,     1] loss: 1186.823
[23,     1] loss: 1178.711
[24,     1] loss: 1216.473
[25,     1] loss: 1182.864
[26,     1] loss: 1144.946
[27,     1] loss: 1121.724
[28,     1] loss: 1097.248
[29,     1] loss: 1082.979
[30,     1] loss: 1050.852
[31,     1] loss: 1076.587
[32,     1] loss: 1007.833
[33,     1] loss: 1113.983
[34,     1] loss: 1083.925
[35,     1] loss: 1039.002
[36,     1] loss: 1013.185
[37,     1] loss: 1043.473
[38,     1] loss: 1068.124
[39,     1] loss: 977.213
[40,     1] loss: 963.226
[41,     1] loss: 907.143
[42,     1] loss: 861.788
[43,     1] loss: 845.630
[44,     1] loss: 830.458
[45,     1] loss: 886.969
[46,     1] loss: 1528.052
[47,     1] loss: 1246.524
Early stopping applied (best metric=0.9195858836174011)
Finished Training
Total time taken: 7.378155469894409
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1531.500
[2,     1] loss: 1522.031
[3,     1] loss: 1518.247
[4,     1] loss: 1528.472
[5,     1] loss: 1521.199
[6,     1] loss: 1514.975
[7,     1] loss: 1515.469
[8,     1] loss: 1517.498
[9,     1] loss: 1519.666
[10,     1] loss: 1519.693
[11,     1] loss: 1515.176
[12,     1] loss: 1513.038
[13,     1] loss: 1510.255
[14,     1] loss: 1508.199
[15,     1] loss: 1496.359
[16,     1] loss: 1480.920
[17,     1] loss: 1469.115
[18,     1] loss: 1433.549
[19,     1] loss: 1409.073
[20,     1] loss: 1373.715
[21,     1] loss: 1344.260
[22,     1] loss: 1297.312
[23,     1] loss: 1289.261
[24,     1] loss: 1291.034
[25,     1] loss: 1280.914
[26,     1] loss: 1291.997
[27,     1] loss: 1277.072
[28,     1] loss: 1262.427
[29,     1] loss: 1290.542
[30,     1] loss: 1224.368
[31,     1] loss: 1212.057
[32,     1] loss: 1231.767
[33,     1] loss: 1226.063
[34,     1] loss: 1209.545
[35,     1] loss: 1157.554
[36,     1] loss: 1182.429
[37,     1] loss: 1196.211
[38,     1] loss: 1149.850
[39,     1] loss: 1166.636
[40,     1] loss: 1146.623
[41,     1] loss: 1148.447
[42,     1] loss: 1141.166
[43,     1] loss: 1323.583
[44,     1] loss: 1040.491
[45,     1] loss: 1194.384
[46,     1] loss: 1098.226
[47,     1] loss: 1098.437
[48,     1] loss: 1181.591
[49,     1] loss: 1075.006
[50,     1] loss: 1081.583
[51,     1] loss: 1080.848
[52,     1] loss: 1001.931
[53,     1] loss: 1093.790
[54,     1] loss: 1027.830
[55,     1] loss: 965.613
[56,     1] loss: 1000.854
[57,     1] loss: 1001.431
[58,     1] loss: 951.547
[59,     1] loss: 986.658
[60,     1] loss: 993.382
[61,     1] loss: 907.899
[62,     1] loss: 997.411
[63,     1] loss: 857.957
[64,     1] loss: 1024.419
[65,     1] loss: 878.230
[66,     1] loss: 924.741
[67,     1] loss: 904.684
[68,     1] loss: 921.156
[69,     1] loss: 939.033
[70,     1] loss: 864.992
[71,     1] loss: 892.343
[72,     1] loss: 835.087
[73,     1] loss: 837.344
[74,     1] loss: 793.267
Early stopping applied (best metric=0.739784300327301)
Finished Training
Total time taken: 11.52279543876648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.795
[2,     1] loss: 1527.335
[3,     1] loss: 1514.374
[4,     1] loss: 1518.375
[5,     1] loss: 1511.364
[6,     1] loss: 1514.396
[7,     1] loss: 1507.964
[8,     1] loss: 1496.822
[9,     1] loss: 1483.677
[10,     1] loss: 1448.787
[11,     1] loss: 1429.913
[12,     1] loss: 1389.761
[13,     1] loss: 1338.996
[14,     1] loss: 1312.273
[15,     1] loss: 1327.438
[16,     1] loss: 1312.342
[17,     1] loss: 1317.376
[18,     1] loss: 1269.964
[19,     1] loss: 1244.964
[20,     1] loss: 1268.212
[21,     1] loss: 1226.846
[22,     1] loss: 1231.449
[23,     1] loss: 1258.544
[24,     1] loss: 1231.196
[25,     1] loss: 1186.823
[26,     1] loss: 1257.393
[27,     1] loss: 1209.074
[28,     1] loss: 1206.055
[29,     1] loss: 1137.746
[30,     1] loss: 1202.461
[31,     1] loss: 1218.899
[32,     1] loss: 1162.420
[33,     1] loss: 1157.450
[34,     1] loss: 1085.533
[35,     1] loss: 1105.201
[36,     1] loss: 1113.281
[37,     1] loss: 1079.517
[38,     1] loss: 1078.006
[39,     1] loss: 1019.357
[40,     1] loss: 1027.958
[41,     1] loss: 1084.449
[42,     1] loss: 1029.784
[43,     1] loss: 1110.880
[44,     1] loss: 1024.862
[45,     1] loss: 1012.000
[46,     1] loss: 927.233
[47,     1] loss: 950.538
[48,     1] loss: 965.702
[49,     1] loss: 937.757
[50,     1] loss: 828.397
[51,     1] loss: 903.461
[52,     1] loss: 960.827
[53,     1] loss: 1210.271
[54,     1] loss: 1355.921
[55,     1] loss: 909.098
[56,     1] loss: 1114.996
[57,     1] loss: 1031.733
[58,     1] loss: 1016.755
[59,     1] loss: 1059.438
[60,     1] loss: 1055.814
[61,     1] loss: 1040.346
[62,     1] loss: 966.522
[63,     1] loss: 960.382
Early stopping applied (best metric=0.8350078463554382)
Finished Training
Total time taken: 11.205644607543945
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1516.672
[2,     1] loss: 1526.862
[3,     1] loss: 1524.923
[4,     1] loss: 1523.607
[5,     1] loss: 1519.430
[6,     1] loss: 1523.614
[7,     1] loss: 1517.507
[8,     1] loss: 1518.353
[9,     1] loss: 1518.043
[10,     1] loss: 1514.205
[11,     1] loss: 1505.621
[12,     1] loss: 1509.201
[13,     1] loss: 1487.193
[14,     1] loss: 1466.864
[15,     1] loss: 1435.572
[16,     1] loss: 1414.140
[17,     1] loss: 1380.375
[18,     1] loss: 1360.873
[19,     1] loss: 1338.767
[20,     1] loss: 1281.153
[21,     1] loss: 1276.147
[22,     1] loss: 1297.696
[23,     1] loss: 1231.069
[24,     1] loss: 1235.533
[25,     1] loss: 1290.982
[26,     1] loss: 1224.972
[27,     1] loss: 1222.266
[28,     1] loss: 1196.944
[29,     1] loss: 1211.267
[30,     1] loss: 1163.914
[31,     1] loss: 1199.807
[32,     1] loss: 1181.344
[33,     1] loss: 1182.582
[34,     1] loss: 1129.375
[35,     1] loss: 1129.451
[36,     1] loss: 1086.911
[37,     1] loss: 1100.496
[38,     1] loss: 1097.333
[39,     1] loss: 1195.226
[40,     1] loss: 1117.179
[41,     1] loss: 1284.257
[42,     1] loss: 1028.191
[43,     1] loss: 1189.188
[44,     1] loss: 1031.244
[45,     1] loss: 1081.841
[46,     1] loss: 1079.873
[47,     1] loss: 1025.586
[48,     1] loss: 1066.449
[49,     1] loss: 972.217
[50,     1] loss: 1010.856
[51,     1] loss: 966.768
[52,     1] loss: 1029.613
[53,     1] loss: 968.130
[54,     1] loss: 995.632
[55,     1] loss: 888.349
[56,     1] loss: 914.042
[57,     1] loss: 868.839
[58,     1] loss: 927.486
[59,     1] loss: 845.194
[60,     1] loss: 904.413
[61,     1] loss: 893.500
[62,     1] loss: 815.167
[63,     1] loss: 817.911
[64,     1] loss: 953.047
[65,     1] loss: 998.627
[66,     1] loss: 810.901
[67,     1] loss: 894.438
[68,     1] loss: 905.156
[69,     1] loss: 922.507
[70,     1] loss: 822.340
[71,     1] loss: 844.880
[72,     1] loss: 822.164
[73,     1] loss: 836.676
[74,     1] loss: 721.617
[75,     1] loss: 777.993
[76,     1] loss: 742.394
[77,     1] loss: 684.126
[78,     1] loss: 691.477
[79,     1] loss: 642.135
[80,     1] loss: 699.695
[81,     1] loss: 808.497
[82,     1] loss: 1121.375
[83,     1] loss: 1319.713
[84,     1] loss: 874.344
[85,     1] loss: 878.148
[86,     1] loss: 1013.060
[87,     1] loss: 948.632
[88,     1] loss: 883.504
[89,     1] loss: 948.088
[90,     1] loss: 881.641
[91,     1] loss: 781.600
[92,     1] loss: 841.631
[93,     1] loss: 812.941
[94,     1] loss: 744.650
Early stopping applied (best metric=0.7007354497909546)
Finished Training
Total time taken: 17.442207098007202
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1523.090
[2,     1] loss: 1524.860
[3,     1] loss: 1518.272
[4,     1] loss: 1520.371
[5,     1] loss: 1518.143
[6,     1] loss: 1516.212
[7,     1] loss: 1510.857
[8,     1] loss: 1519.571
[9,     1] loss: 1511.441
[10,     1] loss: 1503.171
[11,     1] loss: 1493.247
[12,     1] loss: 1462.100
[13,     1] loss: 1437.879
[14,     1] loss: 1400.912
[15,     1] loss: 1374.101
[16,     1] loss: 1352.517
[17,     1] loss: 1294.396
[18,     1] loss: 1321.305
[19,     1] loss: 1268.455
[20,     1] loss: 1263.968
[21,     1] loss: 1298.308
[22,     1] loss: 1272.320
[23,     1] loss: 1237.555
[24,     1] loss: 1286.141
[25,     1] loss: 1238.699
[26,     1] loss: 1276.793
[27,     1] loss: 1251.145
[28,     1] loss: 1207.553
[29,     1] loss: 1266.930
[30,     1] loss: 1189.129
[31,     1] loss: 1254.018
[32,     1] loss: 1202.415
[33,     1] loss: 1222.531
[34,     1] loss: 1213.848
[35,     1] loss: 1140.687
[36,     1] loss: 1160.397
[37,     1] loss: 1097.300
[38,     1] loss: 1141.946
[39,     1] loss: 1150.669
[40,     1] loss: 1069.092
[41,     1] loss: 1093.328
[42,     1] loss: 1035.844
[43,     1] loss: 1018.386
[44,     1] loss: 1012.112
[45,     1] loss: 1061.700
[46,     1] loss: 1052.141
[47,     1] loss: 970.682
[48,     1] loss: 933.508
[49,     1] loss: 962.844
[50,     1] loss: 973.426
[51,     1] loss: 1021.648
[52,     1] loss: 1178.287
[53,     1] loss: 1196.686
[54,     1] loss: 964.025
[55,     1] loss: 1027.869
[56,     1] loss: 1017.157
[57,     1] loss: 989.117
[58,     1] loss: 1074.318
[59,     1] loss: 941.599
[60,     1] loss: 1002.102
[61,     1] loss: 891.928
[62,     1] loss: 951.447
[63,     1] loss: 878.121
[64,     1] loss: 918.547
[65,     1] loss: 809.391
[66,     1] loss: 905.659
[67,     1] loss: 824.049
[68,     1] loss: 946.221
[69,     1] loss: 821.009
[70,     1] loss: 828.098
[71,     1] loss: 817.949
[72,     1] loss: 751.765
[73,     1] loss: 822.232
Early stopping applied (best metric=0.785896897315979)
Finished Training
Total time taken: 11.472867965698242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1531.888
[2,     1] loss: 1518.192
[3,     1] loss: 1519.578
[4,     1] loss: 1514.369
[5,     1] loss: 1527.846
[6,     1] loss: 1523.384
[7,     1] loss: 1515.262
[8,     1] loss: 1517.504
[9,     1] loss: 1516.149
[10,     1] loss: 1517.812
[11,     1] loss: 1514.413
[12,     1] loss: 1517.926
[13,     1] loss: 1507.682
[14,     1] loss: 1501.130
[15,     1] loss: 1494.489
[16,     1] loss: 1492.981
[17,     1] loss: 1472.417
[18,     1] loss: 1445.386
[19,     1] loss: 1414.923
[20,     1] loss: 1394.981
[21,     1] loss: 1352.228
[22,     1] loss: 1339.304
[23,     1] loss: 1334.237
[24,     1] loss: 1333.887
[25,     1] loss: 1341.156
[26,     1] loss: 1227.834
[27,     1] loss: 1241.670
[28,     1] loss: 1285.352
[29,     1] loss: 1263.110
[30,     1] loss: 1368.971
[31,     1] loss: 1214.316
[32,     1] loss: 1247.001
[33,     1] loss: 1214.458
[34,     1] loss: 1190.019
[35,     1] loss: 1242.757
[36,     1] loss: 1179.747
[37,     1] loss: 1187.611
[38,     1] loss: 1204.363
[39,     1] loss: 1178.447
[40,     1] loss: 1145.887
[41,     1] loss: 1104.229
[42,     1] loss: 1092.582
[43,     1] loss: 1067.318
[44,     1] loss: 1108.722
[45,     1] loss: 1113.837
[46,     1] loss: 1126.830
[47,     1] loss: 1083.890
[48,     1] loss: 1086.148
[49,     1] loss: 1078.146
[50,     1] loss: 1000.231
[51,     1] loss: 1057.004
[52,     1] loss: 955.706
[53,     1] loss: 968.472
[54,     1] loss: 994.918
[55,     1] loss: 898.712
[56,     1] loss: 995.446
[57,     1] loss: 983.391
[58,     1] loss: 1004.518
[59,     1] loss: 877.369
[60,     1] loss: 948.677
[61,     1] loss: 856.370
[62,     1] loss: 880.278
[63,     1] loss: 877.843
[64,     1] loss: 853.481
[65,     1] loss: 920.875
Early stopping applied (best metric=0.7701303362846375)
Finished Training
Total time taken: 11.921373128890991
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1525.386
[2,     1] loss: 1520.651
[3,     1] loss: 1520.348
[4,     1] loss: 1519.505
[5,     1] loss: 1518.261
[6,     1] loss: 1517.054
[7,     1] loss: 1515.993
[8,     1] loss: 1510.057
[9,     1] loss: 1500.450
[10,     1] loss: 1490.696
[11,     1] loss: 1451.052
[12,     1] loss: 1406.115
[13,     1] loss: 1384.867
[14,     1] loss: 1367.565
[15,     1] loss: 1298.340
[16,     1] loss: 1316.666
[17,     1] loss: 1337.211
[18,     1] loss: 1259.358
[19,     1] loss: 1256.948
[20,     1] loss: 1218.900
[21,     1] loss: 1260.307
[22,     1] loss: 1186.662
[23,     1] loss: 1224.268
[24,     1] loss: 1218.322
[25,     1] loss: 1185.270
[26,     1] loss: 1170.992
[27,     1] loss: 1158.411
[28,     1] loss: 1114.727
[29,     1] loss: 1083.772
[30,     1] loss: 1165.606
[31,     1] loss: 1115.732
[32,     1] loss: 1109.503
[33,     1] loss: 1122.652
[34,     1] loss: 1101.276
[35,     1] loss: 1080.411
[36,     1] loss: 1066.764
[37,     1] loss: 1081.890
[38,     1] loss: 1133.858
[39,     1] loss: 1055.312
[40,     1] loss: 1037.309
[41,     1] loss: 1051.891
[42,     1] loss: 976.741
[43,     1] loss: 1060.393
[44,     1] loss: 958.318
[45,     1] loss: 875.741
[46,     1] loss: 986.441
[47,     1] loss: 1095.524
[48,     1] loss: 1046.070
[49,     1] loss: 944.526
[50,     1] loss: 968.394
[51,     1] loss: 931.264
[52,     1] loss: 913.140
[53,     1] loss: 912.639
[54,     1] loss: 901.220
[55,     1] loss: 916.448
[56,     1] loss: 892.285
[57,     1] loss: 824.411
[58,     1] loss: 865.346
[59,     1] loss: 907.195
[60,     1] loss: 816.120
[61,     1] loss: 813.992
Early stopping applied (best metric=0.8089103698730469)
Finished Training
Total time taken: 11.020992755889893
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1523.100
[2,     1] loss: 1518.342
[3,     1] loss: 1513.448
[4,     1] loss: 1536.048
[5,     1] loss: 1516.193
[6,     1] loss: 1520.779
[7,     1] loss: 1512.844
[8,     1] loss: 1515.526
[9,     1] loss: 1507.642
[10,     1] loss: 1496.886
[11,     1] loss: 1477.248
[12,     1] loss: 1462.551
[13,     1] loss: 1419.584
[14,     1] loss: 1378.342
[15,     1] loss: 1341.038
[16,     1] loss: 1332.514
[17,     1] loss: 1284.510
[18,     1] loss: 1257.718
[19,     1] loss: 1296.244
[20,     1] loss: 1232.688
[21,     1] loss: 1222.550
[22,     1] loss: 1229.518
[23,     1] loss: 1201.455
[24,     1] loss: 1279.954
[25,     1] loss: 1209.736
[26,     1] loss: 1156.692
[27,     1] loss: 1170.748
[28,     1] loss: 1154.994
[29,     1] loss: 1128.312
[30,     1] loss: 1108.935
[31,     1] loss: 1170.054
[32,     1] loss: 1060.262
[33,     1] loss: 1096.704
[34,     1] loss: 1118.415
[35,     1] loss: 1099.424
[36,     1] loss: 1129.695
[37,     1] loss: 1033.674
[38,     1] loss: 994.941
[39,     1] loss: 1074.799
[40,     1] loss: 1027.017
[41,     1] loss: 1022.556
[42,     1] loss: 1039.545
[43,     1] loss: 993.190
[44,     1] loss: 1041.126
[45,     1] loss: 958.476
[46,     1] loss: 993.623
[47,     1] loss: 937.959
[48,     1] loss: 1012.171
[49,     1] loss: 894.362
[50,     1] loss: 948.046
[51,     1] loss: 894.804
[52,     1] loss: 895.120
[53,     1] loss: 850.604
[54,     1] loss: 846.821
[55,     1] loss: 931.034
[56,     1] loss: 950.307
[57,     1] loss: 887.365
[58,     1] loss: 841.115
Early stopping applied (best metric=0.8834879398345947)
Finished Training
Total time taken: 9.48030161857605
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1525.192
[2,     1] loss: 1523.130
[3,     1] loss: 1523.080
[4,     1] loss: 1519.873
[5,     1] loss: 1523.350
[6,     1] loss: 1517.960
[7,     1] loss: 1520.643
[8,     1] loss: 1519.307
[9,     1] loss: 1511.139
[10,     1] loss: 1508.124
[11,     1] loss: 1494.333
[12,     1] loss: 1471.008
[13,     1] loss: 1441.542
[14,     1] loss: 1414.512
[15,     1] loss: 1385.820
[16,     1] loss: 1307.374
[17,     1] loss: 1307.957
[18,     1] loss: 1289.146
[19,     1] loss: 1233.866
[20,     1] loss: 1221.911
[21,     1] loss: 1292.271
[22,     1] loss: 1222.654
[23,     1] loss: 1267.179
[24,     1] loss: 1224.339
[25,     1] loss: 1221.059
[26,     1] loss: 1174.918
[27,     1] loss: 1219.784
[28,     1] loss: 1186.352
[29,     1] loss: 1183.417
[30,     1] loss: 1171.652
[31,     1] loss: 1136.651
[32,     1] loss: 1120.464
[33,     1] loss: 1109.740
[34,     1] loss: 1119.657
[35,     1] loss: 1095.684
[36,     1] loss: 1123.060
[37,     1] loss: 1091.265
[38,     1] loss: 1037.434
[39,     1] loss: 1067.661
[40,     1] loss: 1029.858
[41,     1] loss: 1063.298
[42,     1] loss: 1006.973
[43,     1] loss: 1025.114
[44,     1] loss: 1042.209
[45,     1] loss: 1024.759
[46,     1] loss: 945.776
[47,     1] loss: 967.070
[48,     1] loss: 962.554
[49,     1] loss: 887.505
[50,     1] loss: 929.546
[51,     1] loss: 918.214
[52,     1] loss: 866.753
[53,     1] loss: 852.109
[54,     1] loss: 915.266
[55,     1] loss: 856.144
[56,     1] loss: 846.908
[57,     1] loss: 902.124
[58,     1] loss: 888.314
[59,     1] loss: 905.741
[60,     1] loss: 1005.499
[61,     1] loss: 877.094
[62,     1] loss: 854.389
Early stopping applied (best metric=0.8518036603927612)
Finished Training
Total time taken: 11.837951898574829
{'Hydroxylation-K Validation Accuracy': 0.764775413711584, 'Hydroxylation-K Validation Sensitivity': 0.7022222222222222, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.45040975364504776, 'Hydroxylation-K AUC ROC': 0.8194346978557505, 'Hydroxylation-K AUC PR': 0.5843911001026052, 'Hydroxylation-K MCC': 0.4185565464816742, 'Hydroxylation-K F1': 0.5451154307172614, 'Validation Loss (Hydroxylation-K)': 0.41750401457150776, 'Hydroxylation-P Validation Accuracy': 0.7835874997885048, 'Hydroxylation-P Validation Sensitivity': 0.7635978835978836, 'Hydroxylation-P Validation Specificity': 0.7879570053369246, 'Hydroxylation-P Validation Precision': 0.4439077194041067, 'Hydroxylation-P AUC ROC': 0.8361805461539862, 'Hydroxylation-P AUC PR': 0.5739408661187478, 'Hydroxylation-P MCC': 0.45938433798404327, 'Hydroxylation-P F1': 0.5552034300542239, 'Validation Loss (Hydroxylation-P)': 0.38424117167790733, 'Validation Loss (total)': 0.8017451882362365, 'TimeToTrain': 11.777555735905965}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00775233295588116,
 'learning_rate_Hydroxylation-K': 0.0017337514268778327,
 'learning_rate_Hydroxylation-P': 0.002353899525953582,
 'log_base': 2.606581282198721,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1929997484,
 'sample_weights': [2.9043198767018414, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.1086019090736388,
 'weight_decay_Hydroxylation-K': 6.07525599666443,
 'weight_decay_Hydroxylation-P': 8.254301214673218}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.014
[2,     1] loss: 1287.205
[3,     1] loss: 1280.607
[4,     1] loss: 1278.937
[5,     1] loss: 1278.199
[6,     1] loss: 1274.109
[7,     1] loss: 1274.111
[8,     1] loss: 1274.377
[9,     1] loss: 1274.493
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00028812031014567694,
 'learning_rate_Hydroxylation-K': 0.003765223900071027,
 'learning_rate_Hydroxylation-P': 0.0044488789123028364,
 'log_base': 1.4206198705539528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 352698723,
 'sample_weights': [1.7425619000644674, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2969621378674905,
 'weight_decay_Hydroxylation-K': 2.725197809750186,
 'weight_decay_Hydroxylation-P': 6.469481426175125}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1906.055
[2,     1] loss: 1915.409
[3,     1] loss: 1923.486
[4,     1] loss: 1921.852
[5,     1] loss: 1916.517
[6,     1] loss: 1914.494
[7,     1] loss: 1912.923
[8,     1] loss: 1914.254
[9,     1] loss: 1914.536
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004351417316859459,
 'learning_rate_Hydroxylation-K': 0.00397840069605727,
 'learning_rate_Hydroxylation-P': 0.002479948387751919,
 'log_base': 2.976420438201292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1857480666,
 'sample_weights': [4.754984284867883, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.59009316153239,
 'weight_decay_Hydroxylation-K': 9.154346560188934,
 'weight_decay_Hydroxylation-P': 2.44241180382583}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.549
[2,     1] loss: 1234.409
[3,     1] loss: 1229.281
[4,     1] loss: 1228.844
[5,     1] loss: 1229.015
[6,     1] loss: 1226.995
[7,     1] loss: 1221.646
[8,     1] loss: 1211.364
[9,     1] loss: 1197.228
[10,     1] loss: 1170.562
[11,     1] loss: 1145.482
[12,     1] loss: 1098.174
[13,     1] loss: 1063.928
[14,     1] loss: 1027.172
[15,     1] loss: 1040.772
[16,     1] loss: 1058.072
[17,     1] loss: 998.208
[18,     1] loss: 1048.585
[19,     1] loss: 982.788
[20,     1] loss: 993.036
[21,     1] loss: 999.316
[22,     1] loss: 957.678
[23,     1] loss: 952.607
[24,     1] loss: 975.789
[25,     1] loss: 974.104
[26,     1] loss: 978.631
[27,     1] loss: 895.949
[28,     1] loss: 936.993
[29,     1] loss: 899.139
[30,     1] loss: 928.990
[31,     1] loss: 900.519
[32,     1] loss: 875.071
[33,     1] loss: 929.540
[34,     1] loss: 862.780
[35,     1] loss: 881.409
[36,     1] loss: 843.945
[37,     1] loss: 801.615
[38,     1] loss: 793.563
[39,     1] loss: 808.051
[40,     1] loss: 799.320
[41,     1] loss: 785.611
[42,     1] loss: 876.314
[43,     1] loss: 1057.229
[44,     1] loss: 851.794
[45,     1] loss: 907.701
[46,     1] loss: 818.006
[47,     1] loss: 846.781
[48,     1] loss: 848.945
[49,     1] loss: 796.611
[50,     1] loss: 767.443
[51,     1] loss: 757.818
[52,     1] loss: 774.540
[53,     1] loss: 798.910
[54,     1] loss: 752.980
[55,     1] loss: 767.204
[56,     1] loss: 754.207
[57,     1] loss: 677.230
[58,     1] loss: 719.449
[59,     1] loss: 723.120
[60,     1] loss: 680.572
[61,     1] loss: 688.383
[62,     1] loss: 704.483
[63,     1] loss: 722.368
[64,     1] loss: 711.452
[65,     1] loss: 583.800
[66,     1] loss: 695.223
[67,     1] loss: 698.891
[68,     1] loss: 656.368
[69,     1] loss: 693.583
[70,     1] loss: 674.643
[71,     1] loss: 615.749
[72,     1] loss: 692.465
Early stopping applied (best metric=0.7800451517105103)
Finished Training
Total time taken: 10.532818794250488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.485
[2,     1] loss: 1234.222
[3,     1] loss: 1233.411
[4,     1] loss: 1226.998
[5,     1] loss: 1226.674
[6,     1] loss: 1224.434
[7,     1] loss: 1210.003
[8,     1] loss: 1183.171
[9,     1] loss: 1157.922
[10,     1] loss: 1112.177
[11,     1] loss: 1082.163
[12,     1] loss: 1052.281
[13,     1] loss: 1031.867
[14,     1] loss: 989.989
[15,     1] loss: 983.011
[16,     1] loss: 975.956
[17,     1] loss: 993.911
[18,     1] loss: 938.154
[19,     1] loss: 943.257
[20,     1] loss: 946.635
[21,     1] loss: 972.113
[22,     1] loss: 930.314
[23,     1] loss: 934.978
[24,     1] loss: 869.416
[25,     1] loss: 953.090
[26,     1] loss: 957.305
[27,     1] loss: 917.925
[28,     1] loss: 899.426
[29,     1] loss: 886.729
[30,     1] loss: 866.181
[31,     1] loss: 916.239
[32,     1] loss: 870.366
[33,     1] loss: 891.264
[34,     1] loss: 874.834
[35,     1] loss: 895.912
[36,     1] loss: 874.311
[37,     1] loss: 819.577
[38,     1] loss: 851.668
[39,     1] loss: 795.947
[40,     1] loss: 837.207
[41,     1] loss: 790.633
[42,     1] loss: 752.662
[43,     1] loss: 768.633
[44,     1] loss: 757.477
[45,     1] loss: 736.023
[46,     1] loss: 795.465
[47,     1] loss: 885.225
[48,     1] loss: 746.079
[49,     1] loss: 792.046
[50,     1] loss: 745.260
[51,     1] loss: 751.320
[52,     1] loss: 719.728
[53,     1] loss: 802.661
[54,     1] loss: 724.821
[55,     1] loss: 724.094
[56,     1] loss: 728.354
[57,     1] loss: 651.922
[58,     1] loss: 673.179
[59,     1] loss: 708.492
[60,     1] loss: 649.405
[61,     1] loss: 628.521
[62,     1] loss: 733.021
[63,     1] loss: 649.026
[64,     1] loss: 618.284
[65,     1] loss: 597.308
[66,     1] loss: 622.000
[67,     1] loss: 593.060
[68,     1] loss: 643.886
[69,     1] loss: 755.083
[70,     1] loss: 904.950
[71,     1] loss: 598.037
[72,     1] loss: 717.230
[73,     1] loss: 673.267
[74,     1] loss: 673.580
[75,     1] loss: 682.063
[76,     1] loss: 617.212
Early stopping applied (best metric=0.7834945321083069)
Finished Training
Total time taken: 12.834954023361206
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.874
[2,     1] loss: 1231.069
[3,     1] loss: 1231.333
[4,     1] loss: 1228.230
[5,     1] loss: 1223.763
[6,     1] loss: 1219.051
[7,     1] loss: 1210.937
[8,     1] loss: 1192.792
[9,     1] loss: 1156.965
[10,     1] loss: 1103.733
[11,     1] loss: 1107.598
[12,     1] loss: 1079.072
[13,     1] loss: 1025.599
[14,     1] loss: 1024.168
[15,     1] loss: 991.488
[16,     1] loss: 1016.717
[17,     1] loss: 973.327
[18,     1] loss: 974.266
[19,     1] loss: 949.554
[20,     1] loss: 978.142
[21,     1] loss: 953.460
[22,     1] loss: 923.270
[23,     1] loss: 973.376
[24,     1] loss: 945.688
[25,     1] loss: 928.286
[26,     1] loss: 911.461
[27,     1] loss: 921.313
[28,     1] loss: 866.634
[29,     1] loss: 851.883
[30,     1] loss: 881.228
[31,     1] loss: 855.915
[32,     1] loss: 988.900
[33,     1] loss: 984.970
[34,     1] loss: 834.151
[35,     1] loss: 998.482
[36,     1] loss: 866.182
[37,     1] loss: 872.653
[38,     1] loss: 929.013
[39,     1] loss: 829.374
[40,     1] loss: 877.815
[41,     1] loss: 883.043
[42,     1] loss: 843.776
[43,     1] loss: 790.114
[44,     1] loss: 813.660
[45,     1] loss: 803.859
[46,     1] loss: 809.882
[47,     1] loss: 794.492
[48,     1] loss: 758.240
[49,     1] loss: 736.961
[50,     1] loss: 720.141
[51,     1] loss: 724.341
[52,     1] loss: 766.911
[53,     1] loss: 753.541
[54,     1] loss: 775.597
[55,     1] loss: 763.772
[56,     1] loss: 721.431
[57,     1] loss: 715.215
[58,     1] loss: 722.135
[59,     1] loss: 684.566
[60,     1] loss: 691.088
[61,     1] loss: 646.762
[62,     1] loss: 874.202
[63,     1] loss: 1009.138
[64,     1] loss: 756.563
[65,     1] loss: 874.206
[66,     1] loss: 711.630
[67,     1] loss: 854.890
[68,     1] loss: 819.413
[69,     1] loss: 839.589
[70,     1] loss: 787.795
[71,     1] loss: 843.218
[72,     1] loss: 756.886
[73,     1] loss: 765.147
[74,     1] loss: 788.015
[75,     1] loss: 706.811
[76,     1] loss: 730.299
[77,     1] loss: 683.776
[78,     1] loss: 729.025
[79,     1] loss: 630.126
[80,     1] loss: 684.307
[81,     1] loss: 621.797
[82,     1] loss: 690.138
[83,     1] loss: 619.521
[84,     1] loss: 605.296
[85,     1] loss: 555.984
[86,     1] loss: 591.911
[87,     1] loss: 715.674
[88,     1] loss: 632.827
[89,     1] loss: 494.026
[90,     1] loss: 695.018
[91,     1] loss: 649.858
[92,     1] loss: 543.863
[93,     1] loss: 583.213
[94,     1] loss: 548.386
[95,     1] loss: 483.498
[96,     1] loss: 523.876
[97,     1] loss: 490.334
[98,     1] loss: 511.459
[99,     1] loss: 564.204
[100,     1] loss: 793.160
[101,     1] loss: 838.487
[102,     1] loss: 612.708
[103,     1] loss: 669.173
[104,     1] loss: 594.984
[105,     1] loss: 635.859
[106,     1] loss: 567.862
[107,     1] loss: 650.177
[108,     1] loss: 609.583
[109,     1] loss: 516.118
[110,     1] loss: 594.886
[111,     1] loss: 577.809
[112,     1] loss: 530.240
[113,     1] loss: 497.533
[114,     1] loss: 568.259
[115,     1] loss: 509.316
[116,     1] loss: 528.264
[117,     1] loss: 497.932
[118,     1] loss: 445.983
[119,     1] loss: 491.653
[120,     1] loss: 698.681
[121,     1] loss: 1163.939
Early stopping applied (best metric=0.7762479782104492)
Finished Training
Total time taken: 16.550516605377197
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.189
[2,     1] loss: 1232.735
[3,     1] loss: 1227.789
[4,     1] loss: 1230.787
[5,     1] loss: 1231.235
[6,     1] loss: 1229.339
[7,     1] loss: 1223.166
[8,     1] loss: 1224.968
[9,     1] loss: 1215.987
[10,     1] loss: 1201.142
[11,     1] loss: 1173.911
[12,     1] loss: 1128.955
[13,     1] loss: 1112.434
[14,     1] loss: 1060.637
[15,     1] loss: 1075.623
[16,     1] loss: 1049.285
[17,     1] loss: 1022.964
[18,     1] loss: 1024.568
[19,     1] loss: 994.434
[20,     1] loss: 1029.611
[21,     1] loss: 993.665
[22,     1] loss: 985.094
[23,     1] loss: 941.091
[24,     1] loss: 1007.623
[25,     1] loss: 948.832
[26,     1] loss: 959.884
[27,     1] loss: 908.015
[28,     1] loss: 942.286
[29,     1] loss: 921.006
[30,     1] loss: 913.322
[31,     1] loss: 862.455
[32,     1] loss: 903.044
[33,     1] loss: 871.930
[34,     1] loss: 882.546
[35,     1] loss: 925.002
[36,     1] loss: 864.380
[37,     1] loss: 854.637
[38,     1] loss: 935.214
[39,     1] loss: 847.009
[40,     1] loss: 845.566
[41,     1] loss: 886.814
[42,     1] loss: 820.247
[43,     1] loss: 818.272
[44,     1] loss: 839.215
[45,     1] loss: 745.804
[46,     1] loss: 851.841
[47,     1] loss: 925.199
[48,     1] loss: 761.346
[49,     1] loss: 833.431
[50,     1] loss: 759.416
[51,     1] loss: 789.322
[52,     1] loss: 769.937
[53,     1] loss: 708.293
Early stopping applied (best metric=0.7837487459182739)
Finished Training
Total time taken: 9.405227422714233
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.301
[2,     1] loss: 1231.557
[3,     1] loss: 1236.144
[4,     1] loss: 1231.372
[5,     1] loss: 1232.020
[6,     1] loss: 1230.301
[7,     1] loss: 1228.101
[8,     1] loss: 1232.228
[9,     1] loss: 1226.558
[10,     1] loss: 1221.156
[11,     1] loss: 1213.351
[12,     1] loss: 1194.123
[13,     1] loss: 1176.189
[14,     1] loss: 1134.752
[15,     1] loss: 1116.511
[16,     1] loss: 1067.129
[17,     1] loss: 1051.602
[18,     1] loss: 1017.126
[19,     1] loss: 1000.304
[20,     1] loss: 1036.465
[21,     1] loss: 970.191
[22,     1] loss: 997.420
[23,     1] loss: 973.064
[24,     1] loss: 937.062
[25,     1] loss: 986.230
[26,     1] loss: 946.120
[27,     1] loss: 949.837
[28,     1] loss: 917.197
[29,     1] loss: 905.466
[30,     1] loss: 929.767
[31,     1] loss: 908.190
[32,     1] loss: 850.841
[33,     1] loss: 881.206
[34,     1] loss: 828.506
[35,     1] loss: 855.241
[36,     1] loss: 867.145
[37,     1] loss: 826.248
[38,     1] loss: 847.593
[39,     1] loss: 807.214
[40,     1] loss: 800.573
[41,     1] loss: 802.365
[42,     1] loss: 823.259
[43,     1] loss: 841.810
[44,     1] loss: 809.013
[45,     1] loss: 837.120
[46,     1] loss: 775.237
[47,     1] loss: 731.487
[48,     1] loss: 833.027
[49,     1] loss: 737.638
[50,     1] loss: 780.494
[51,     1] loss: 883.006
[52,     1] loss: 716.248
[53,     1] loss: 810.809
[54,     1] loss: 713.344
[55,     1] loss: 749.594
[56,     1] loss: 733.922
[57,     1] loss: 706.614
[58,     1] loss: 720.633
[59,     1] loss: 641.501
[60,     1] loss: 708.373
[61,     1] loss: 640.894
[62,     1] loss: 697.920
[63,     1] loss: 668.698
[64,     1] loss: 665.730
[65,     1] loss: 628.310
[66,     1] loss: 583.450
[67,     1] loss: 617.257
[68,     1] loss: 706.302
[69,     1] loss: 711.217
[70,     1] loss: 633.292
[71,     1] loss: 602.300
[72,     1] loss: 622.709
[73,     1] loss: 603.177
[74,     1] loss: 611.895
[75,     1] loss: 535.418
[76,     1] loss: 553.314
[77,     1] loss: 707.461
[78,     1] loss: 978.757
[79,     1] loss: 565.236
[80,     1] loss: 768.018
[81,     1] loss: 606.968
[82,     1] loss: 718.855
[83,     1] loss: 625.089
[84,     1] loss: 615.878
[85,     1] loss: 619.157
[86,     1] loss: 523.069
Early stopping applied (best metric=0.8695693016052246)
Finished Training
Total time taken: 14.078903913497925
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.908
[2,     1] loss: 1238.571
[3,     1] loss: 1225.562
[4,     1] loss: 1233.211
[5,     1] loss: 1229.845
[6,     1] loss: 1230.047
[7,     1] loss: 1229.752
[8,     1] loss: 1228.631
[9,     1] loss: 1228.947
[10,     1] loss: 1228.239
[11,     1] loss: 1223.837
[12,     1] loss: 1220.641
[13,     1] loss: 1214.928
[14,     1] loss: 1201.195
[15,     1] loss: 1172.494
[16,     1] loss: 1154.189
[17,     1] loss: 1112.361
[18,     1] loss: 1090.674
[19,     1] loss: 1089.503
[20,     1] loss: 1080.460
[21,     1] loss: 1020.807
[22,     1] loss: 1031.035
[23,     1] loss: 1028.551
[24,     1] loss: 1001.221
[25,     1] loss: 1024.358
[26,     1] loss: 989.089
[27,     1] loss: 994.025
[28,     1] loss: 983.060
[29,     1] loss: 974.907
[30,     1] loss: 956.986
[31,     1] loss: 935.412
[32,     1] loss: 981.364
[33,     1] loss: 936.747
[34,     1] loss: 894.478
[35,     1] loss: 880.386
[36,     1] loss: 878.528
[37,     1] loss: 932.116
[38,     1] loss: 897.778
[39,     1] loss: 890.676
[40,     1] loss: 894.423
[41,     1] loss: 847.622
[42,     1] loss: 836.279
[43,     1] loss: 861.988
[44,     1] loss: 835.196
[45,     1] loss: 806.146
[46,     1] loss: 799.523
[47,     1] loss: 815.334
[48,     1] loss: 750.006
[49,     1] loss: 737.484
[50,     1] loss: 767.978
[51,     1] loss: 731.900
[52,     1] loss: 745.089
[53,     1] loss: 788.644
[54,     1] loss: 896.337
[55,     1] loss: 1059.372
[56,     1] loss: 805.344
[57,     1] loss: 928.925
[58,     1] loss: 906.923
[59,     1] loss: 861.447
[60,     1] loss: 865.959
[61,     1] loss: 861.851
[62,     1] loss: 840.749
[63,     1] loss: 819.378
[64,     1] loss: 845.712
Early stopping applied (best metric=0.7883507013320923)
Finished Training
Total time taken: 11.547386884689331
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.796
[2,     1] loss: 1231.128
[3,     1] loss: 1231.664
[4,     1] loss: 1226.543
[5,     1] loss: 1223.022
[6,     1] loss: 1211.478
[7,     1] loss: 1186.115
[8,     1] loss: 1151.625
[9,     1] loss: 1099.603
[10,     1] loss: 1102.793
[11,     1] loss: 1023.530
[12,     1] loss: 1012.782
[13,     1] loss: 1003.217
[14,     1] loss: 1000.781
[15,     1] loss: 972.744
[16,     1] loss: 973.889
[17,     1] loss: 992.716
[18,     1] loss: 982.473
[19,     1] loss: 938.179
[20,     1] loss: 970.677
[21,     1] loss: 927.652
[22,     1] loss: 913.866
[23,     1] loss: 946.711
[24,     1] loss: 944.598
[25,     1] loss: 923.104
[26,     1] loss: 916.493
[27,     1] loss: 889.669
[28,     1] loss: 889.099
[29,     1] loss: 874.901
[30,     1] loss: 832.952
[31,     1] loss: 845.651
[32,     1] loss: 843.165
[33,     1] loss: 841.911
[34,     1] loss: 831.673
[35,     1] loss: 807.386
[36,     1] loss: 724.492
[37,     1] loss: 774.516
[38,     1] loss: 830.158
[39,     1] loss: 855.647
[40,     1] loss: 1104.908
[41,     1] loss: 810.787
[42,     1] loss: 936.472
[43,     1] loss: 865.413
[44,     1] loss: 831.921
[45,     1] loss: 846.377
[46,     1] loss: 817.005
[47,     1] loss: 808.689
[48,     1] loss: 809.527
[49,     1] loss: 779.125
[50,     1] loss: 754.192
[51,     1] loss: 733.053
[52,     1] loss: 744.328
[53,     1] loss: 774.406
[54,     1] loss: 758.370
[55,     1] loss: 698.904
[56,     1] loss: 726.258
Early stopping applied (best metric=0.8698104619979858)
Finished Training
Total time taken: 8.560182571411133
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.233
[2,     1] loss: 1229.533
[3,     1] loss: 1229.081
[4,     1] loss: 1230.107
[5,     1] loss: 1226.808
[6,     1] loss: 1228.687
[7,     1] loss: 1222.443
[8,     1] loss: 1219.691
[9,     1] loss: 1209.062
[10,     1] loss: 1179.491
[11,     1] loss: 1152.224
[12,     1] loss: 1124.542
[13,     1] loss: 1077.719
[14,     1] loss: 1066.940
[15,     1] loss: 1030.940
[16,     1] loss: 1026.429
[17,     1] loss: 1029.613
[18,     1] loss: 984.064
[19,     1] loss: 1026.210
[20,     1] loss: 1040.123
[21,     1] loss: 1003.158
[22,     1] loss: 1078.193
[23,     1] loss: 966.832
[24,     1] loss: 1044.078
[25,     1] loss: 1017.096
[26,     1] loss: 962.467
[27,     1] loss: 1018.062
[28,     1] loss: 988.049
[29,     1] loss: 942.137
[30,     1] loss: 954.052
[31,     1] loss: 934.362
[32,     1] loss: 873.218
[33,     1] loss: 899.026
[34,     1] loss: 895.859
[35,     1] loss: 895.804
[36,     1] loss: 855.865
[37,     1] loss: 857.321
[38,     1] loss: 857.296
[39,     1] loss: 883.782
[40,     1] loss: 856.609
[41,     1] loss: 795.520
[42,     1] loss: 835.134
[43,     1] loss: 709.683
[44,     1] loss: 799.356
[45,     1] loss: 791.240
[46,     1] loss: 799.822
[47,     1] loss: 745.596
[48,     1] loss: 770.214
[49,     1] loss: 914.470
[50,     1] loss: 1083.897
[51,     1] loss: 751.724
[52,     1] loss: 887.927
[53,     1] loss: 828.399
[54,     1] loss: 825.026
[55,     1] loss: 860.606
[56,     1] loss: 869.236
[57,     1] loss: 799.389
Early stopping applied (best metric=0.9033132791519165)
Finished Training
Total time taken: 9.818778038024902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1228.653
[2,     1] loss: 1233.506
[3,     1] loss: 1232.739
[4,     1] loss: 1230.028
[5,     1] loss: 1228.212
[6,     1] loss: 1225.765
[7,     1] loss: 1226.006
[8,     1] loss: 1222.698
[9,     1] loss: 1209.873
[10,     1] loss: 1189.834
[11,     1] loss: 1162.406
[12,     1] loss: 1136.274
[13,     1] loss: 1093.577
[14,     1] loss: 1072.384
[15,     1] loss: 1067.222
[16,     1] loss: 1027.319
[17,     1] loss: 1029.584
[18,     1] loss: 1019.847
[19,     1] loss: 1043.196
[20,     1] loss: 1048.421
[21,     1] loss: 1003.130
[22,     1] loss: 1000.824
[23,     1] loss: 999.380
[24,     1] loss: 955.816
[25,     1] loss: 947.777
[26,     1] loss: 984.066
[27,     1] loss: 943.080
[28,     1] loss: 927.141
[29,     1] loss: 907.986
[30,     1] loss: 933.270
[31,     1] loss: 937.455
[32,     1] loss: 942.183
[33,     1] loss: 888.920
[34,     1] loss: 930.815
[35,     1] loss: 897.320
[36,     1] loss: 888.838
[37,     1] loss: 889.479
[38,     1] loss: 855.879
[39,     1] loss: 868.560
[40,     1] loss: 888.422
[41,     1] loss: 836.731
[42,     1] loss: 816.089
[43,     1] loss: 791.007
[44,     1] loss: 797.655
[45,     1] loss: 790.576
[46,     1] loss: 795.576
[47,     1] loss: 852.359
[48,     1] loss: 933.814
[49,     1] loss: 828.305
[50,     1] loss: 906.873
[51,     1] loss: 819.785
[52,     1] loss: 855.043
[53,     1] loss: 773.546
[54,     1] loss: 767.509
[55,     1] loss: 830.717
[56,     1] loss: 770.805
[57,     1] loss: 785.874
[58,     1] loss: 749.573
[59,     1] loss: 695.508
[60,     1] loss: 876.742
[61,     1] loss: 833.740
[62,     1] loss: 734.010
[63,     1] loss: 798.122
[64,     1] loss: 784.226
[65,     1] loss: 771.531
[66,     1] loss: 747.634
[67,     1] loss: 734.008
[68,     1] loss: 758.235
[69,     1] loss: 693.495
[70,     1] loss: 692.609
[71,     1] loss: 640.048
[72,     1] loss: 623.224
[73,     1] loss: 635.554
[74,     1] loss: 594.951
[75,     1] loss: 696.638
[76,     1] loss: 1211.086
[77,     1] loss: 715.216
[78,     1] loss: 882.433
Early stopping applied (best metric=0.7728284597396851)
Finished Training
Total time taken: 10.688256025314331
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1232.990
[2,     1] loss: 1229.806
[3,     1] loss: 1236.867
[4,     1] loss: 1232.099
[5,     1] loss: 1231.257
[6,     1] loss: 1230.204
[7,     1] loss: 1226.666
[8,     1] loss: 1214.937
[9,     1] loss: 1199.799
[10,     1] loss: 1179.373
[11,     1] loss: 1139.978
[12,     1] loss: 1109.415
[13,     1] loss: 1072.268
[14,     1] loss: 1058.004
[15,     1] loss: 1080.008
[16,     1] loss: 995.670
[17,     1] loss: 997.601
[18,     1] loss: 971.890
[19,     1] loss: 981.622
[20,     1] loss: 969.701
[21,     1] loss: 1018.060
[22,     1] loss: 1027.067
[23,     1] loss: 954.000
[24,     1] loss: 958.338
[25,     1] loss: 928.579
[26,     1] loss: 950.489
[27,     1] loss: 941.039
[28,     1] loss: 948.205
[29,     1] loss: 898.378
[30,     1] loss: 880.417
[31,     1] loss: 901.208
[32,     1] loss: 928.634
[33,     1] loss: 866.557
[34,     1] loss: 869.455
[35,     1] loss: 862.430
[36,     1] loss: 815.074
[37,     1] loss: 807.546
[38,     1] loss: 861.158
[39,     1] loss: 815.016
[40,     1] loss: 802.012
[41,     1] loss: 844.027
[42,     1] loss: 850.038
[43,     1] loss: 824.661
[44,     1] loss: 765.627
[45,     1] loss: 868.127
[46,     1] loss: 864.684
[47,     1] loss: 809.904
[48,     1] loss: 810.743
[49,     1] loss: 758.225
[50,     1] loss: 797.337
[51,     1] loss: 830.272
[52,     1] loss: 739.257
[53,     1] loss: 818.043
[54,     1] loss: 720.101
[55,     1] loss: 832.540
[56,     1] loss: 751.110
[57,     1] loss: 771.558
[58,     1] loss: 796.720
[59,     1] loss: 664.230
Early stopping applied (best metric=0.7602097988128662)
Finished Training
Total time taken: 9.782453298568726
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.318
[2,     1] loss: 1231.146
[3,     1] loss: 1232.334
[4,     1] loss: 1233.498
[5,     1] loss: 1227.226
[6,     1] loss: 1221.039
[7,     1] loss: 1217.064
[8,     1] loss: 1194.145
[9,     1] loss: 1178.542
[10,     1] loss: 1142.508
[11,     1] loss: 1099.656
[12,     1] loss: 1084.182
[13,     1] loss: 1044.221
[14,     1] loss: 999.588
[15,     1] loss: 1030.088
[16,     1] loss: 1023.688
[17,     1] loss: 1006.869
[18,     1] loss: 999.336
[19,     1] loss: 981.579
[20,     1] loss: 978.668
[21,     1] loss: 987.709
[22,     1] loss: 985.823
[23,     1] loss: 955.440
[24,     1] loss: 938.579
[25,     1] loss: 966.040
[26,     1] loss: 945.322
[27,     1] loss: 913.172
[28,     1] loss: 926.976
[29,     1] loss: 952.948
[30,     1] loss: 966.085
[31,     1] loss: 885.488
[32,     1] loss: 950.258
[33,     1] loss: 867.873
[34,     1] loss: 886.437
[35,     1] loss: 861.710
[36,     1] loss: 839.674
[37,     1] loss: 887.962
[38,     1] loss: 840.626
[39,     1] loss: 865.514
[40,     1] loss: 779.038
[41,     1] loss: 877.415
[42,     1] loss: 772.247
[43,     1] loss: 872.176
[44,     1] loss: 880.128
[45,     1] loss: 762.969
[46,     1] loss: 810.130
[47,     1] loss: 835.031
[48,     1] loss: 752.293
[49,     1] loss: 795.776
[50,     1] loss: 726.387
[51,     1] loss: 738.721
[52,     1] loss: 910.970
[53,     1] loss: 789.185
[54,     1] loss: 712.100
[55,     1] loss: 806.010
[56,     1] loss: 740.993
[57,     1] loss: 769.780
[58,     1] loss: 725.358
[59,     1] loss: 700.745
[60,     1] loss: 677.398
[61,     1] loss: 682.966
[62,     1] loss: 675.621
[63,     1] loss: 626.769
[64,     1] loss: 628.562
[65,     1] loss: 606.884
[66,     1] loss: 621.777
[67,     1] loss: 667.353
[68,     1] loss: 889.710
[69,     1] loss: 1087.600
[70,     1] loss: 845.242
[71,     1] loss: 772.850
[72,     1] loss: 930.578
[73,     1] loss: 915.854
[74,     1] loss: 875.516
[75,     1] loss: 831.403
[76,     1] loss: 839.266
[77,     1] loss: 895.806
[78,     1] loss: 821.652
[79,     1] loss: 744.633
[80,     1] loss: 807.115
[81,     1] loss: 776.619
[82,     1] loss: 732.325
[83,     1] loss: 712.450
[84,     1] loss: 740.245
Early stopping applied (best metric=0.7736955881118774)
Finished Training
Total time taken: 14.940411567687988
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1226.301
[2,     1] loss: 1234.299
[3,     1] loss: 1228.104
[4,     1] loss: 1230.848
[5,     1] loss: 1219.114
[6,     1] loss: 1210.758
[7,     1] loss: 1195.196
[8,     1] loss: 1171.024
[9,     1] loss: 1130.527
[10,     1] loss: 1136.659
[11,     1] loss: 1081.202
[12,     1] loss: 1075.361
[13,     1] loss: 1022.508
[14,     1] loss: 1056.126
[15,     1] loss: 1026.801
[16,     1] loss: 1034.865
[17,     1] loss: 989.806
[18,     1] loss: 1053.058
[19,     1] loss: 1019.991
[20,     1] loss: 1018.366
[21,     1] loss: 1066.161
[22,     1] loss: 989.185
[23,     1] loss: 1014.231
[24,     1] loss: 999.978
[25,     1] loss: 939.452
[26,     1] loss: 981.386
[27,     1] loss: 983.310
[28,     1] loss: 975.603
[29,     1] loss: 928.889
[30,     1] loss: 926.294
[31,     1] loss: 912.247
[32,     1] loss: 911.768
[33,     1] loss: 923.454
[34,     1] loss: 844.911
[35,     1] loss: 904.750
[36,     1] loss: 881.877
[37,     1] loss: 879.105
[38,     1] loss: 863.547
[39,     1] loss: 821.836
[40,     1] loss: 883.321
[41,     1] loss: 847.622
[42,     1] loss: 827.560
[43,     1] loss: 848.942
[44,     1] loss: 817.870
[45,     1] loss: 803.268
[46,     1] loss: 804.216
[47,     1] loss: 963.726
[48,     1] loss: 801.044
[49,     1] loss: 827.068
[50,     1] loss: 811.734
[51,     1] loss: 778.249
[52,     1] loss: 788.401
[53,     1] loss: 732.513
[54,     1] loss: 762.666
[55,     1] loss: 760.133
[56,     1] loss: 703.141
[57,     1] loss: 689.298
[58,     1] loss: 713.967
[59,     1] loss: 712.756
[60,     1] loss: 954.798
[61,     1] loss: 1235.642
[62,     1] loss: 855.428
[63,     1] loss: 832.498
[64,     1] loss: 936.655
[65,     1] loss: 942.112
[66,     1] loss: 926.910
[67,     1] loss: 885.837
[68,     1] loss: 884.300
[69,     1] loss: 865.353
[70,     1] loss: 866.086
[71,     1] loss: 869.895
[72,     1] loss: 831.466
[73,     1] loss: 868.391
[74,     1] loss: 817.013
[75,     1] loss: 805.779
[76,     1] loss: 785.361
[77,     1] loss: 772.376
Early stopping applied (best metric=0.726409912109375)
Finished Training
Total time taken: 14.617903470993042
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.415
[2,     1] loss: 1228.329
[3,     1] loss: 1231.094
[4,     1] loss: 1229.252
[5,     1] loss: 1216.459
[6,     1] loss: 1205.805
[7,     1] loss: 1171.928
[8,     1] loss: 1115.520
[9,     1] loss: 1069.242
[10,     1] loss: 1069.198
[11,     1] loss: 1079.913
[12,     1] loss: 1043.715
[13,     1] loss: 1058.487
[14,     1] loss: 1109.004
[15,     1] loss: 1013.230
[16,     1] loss: 1032.348
[17,     1] loss: 1000.308
[18,     1] loss: 1025.402
[19,     1] loss: 1021.730
[20,     1] loss: 973.628
[21,     1] loss: 993.159
[22,     1] loss: 977.860
[23,     1] loss: 944.772
[24,     1] loss: 934.416
[25,     1] loss: 921.049
[26,     1] loss: 942.687
[27,     1] loss: 923.172
[28,     1] loss: 913.679
[29,     1] loss: 907.432
[30,     1] loss: 871.302
[31,     1] loss: 854.941
[32,     1] loss: 870.311
[33,     1] loss: 879.500
[34,     1] loss: 897.182
[35,     1] loss: 860.210
[36,     1] loss: 875.232
[37,     1] loss: 808.682
[38,     1] loss: 876.637
[39,     1] loss: 884.106
[40,     1] loss: 873.855
[41,     1] loss: 804.632
[42,     1] loss: 881.216
[43,     1] loss: 836.312
[44,     1] loss: 837.770
[45,     1] loss: 767.832
[46,     1] loss: 825.185
[47,     1] loss: 727.158
[48,     1] loss: 862.688
[49,     1] loss: 756.102
[50,     1] loss: 799.888
[51,     1] loss: 717.448
[52,     1] loss: 776.183
[53,     1] loss: 738.125
[54,     1] loss: 741.711
[55,     1] loss: 769.279
[56,     1] loss: 674.846
[57,     1] loss: 752.079
[58,     1] loss: 680.232
[59,     1] loss: 688.116
[60,     1] loss: 718.673
[61,     1] loss: 652.246
[62,     1] loss: 652.924
[63,     1] loss: 629.244
[64,     1] loss: 706.672
[65,     1] loss: 806.997
[66,     1] loss: 788.787
[67,     1] loss: 658.512
[68,     1] loss: 702.288
[69,     1] loss: 649.779
[70,     1] loss: 702.122
[71,     1] loss: 641.212
[72,     1] loss: 650.054
[73,     1] loss: 631.620
[74,     1] loss: 639.600
[75,     1] loss: 546.658
[76,     1] loss: 610.273
[77,     1] loss: 775.991
[78,     1] loss: 803.855
[79,     1] loss: 596.932
[80,     1] loss: 619.757
[81,     1] loss: 600.971
[82,     1] loss: 618.511
[83,     1] loss: 570.896
[84,     1] loss: 598.666
[85,     1] loss: 519.961
[86,     1] loss: 528.535
[87,     1] loss: 667.615
[88,     1] loss: 755.174
[89,     1] loss: 533.842
[90,     1] loss: 563.643
[91,     1] loss: 571.590
[92,     1] loss: 551.909
[93,     1] loss: 582.844
[94,     1] loss: 506.768
[95,     1] loss: 597.095
[96,     1] loss: 577.476
[97,     1] loss: 450.092
[98,     1] loss: 493.935
[99,     1] loss: 696.929
[100,     1] loss: 485.356
[101,     1] loss: 568.190
[102,     1] loss: 628.144
[103,     1] loss: 500.560
[104,     1] loss: 689.530
[105,     1] loss: 574.991
[106,     1] loss: 516.860
[107,     1] loss: 652.685
[108,     1] loss: 471.077
[109,     1] loss: 548.181
[110,     1] loss: 490.123
[111,     1] loss: 561.798
[112,     1] loss: 558.722
[113,     1] loss: 484.526
Early stopping applied (best metric=0.7045531272888184)
Finished Training
Total time taken: 19.198591709136963
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.486
[2,     1] loss: 1230.309
[3,     1] loss: 1231.859
[4,     1] loss: 1237.194
[5,     1] loss: 1227.871
[6,     1] loss: 1226.226
[7,     1] loss: 1227.307
[8,     1] loss: 1229.165
[9,     1] loss: 1220.245
[10,     1] loss: 1218.772
[11,     1] loss: 1202.288
[12,     1] loss: 1176.661
[13,     1] loss: 1156.724
[14,     1] loss: 1126.155
[15,     1] loss: 1117.050
[16,     1] loss: 1088.767
[17,     1] loss: 1056.177
[18,     1] loss: 1010.219
[19,     1] loss: 1004.529
[20,     1] loss: 1002.198
[21,     1] loss: 991.882
[22,     1] loss: 1010.209
[23,     1] loss: 952.016
[24,     1] loss: 1010.697
[25,     1] loss: 959.766
[26,     1] loss: 939.407
[27,     1] loss: 902.333
[28,     1] loss: 922.592
[29,     1] loss: 883.825
[30,     1] loss: 897.695
[31,     1] loss: 914.495
[32,     1] loss: 942.916
[33,     1] loss: 948.308
[34,     1] loss: 899.435
[35,     1] loss: 860.318
[36,     1] loss: 872.800
[37,     1] loss: 857.224
[38,     1] loss: 857.729
[39,     1] loss: 816.736
[40,     1] loss: 851.262
[41,     1] loss: 791.308
[42,     1] loss: 796.624
[43,     1] loss: 767.287
[44,     1] loss: 745.602
[45,     1] loss: 734.149
[46,     1] loss: 725.893
[47,     1] loss: 761.669
[48,     1] loss: 1153.951
[49,     1] loss: 1190.643
[50,     1] loss: 803.692
[51,     1] loss: 864.742
[52,     1] loss: 958.267
[53,     1] loss: 955.801
[54,     1] loss: 907.598
[55,     1] loss: 904.424
[56,     1] loss: 918.347
[57,     1] loss: 919.327
[58,     1] loss: 907.719
[59,     1] loss: 812.889
[60,     1] loss: 823.172
[61,     1] loss: 846.281
[62,     1] loss: 864.942
Early stopping applied (best metric=0.8320153951644897)
Finished Training
Total time taken: 11.529305934906006
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1234.399
[2,     1] loss: 1236.461
[3,     1] loss: 1235.197
[4,     1] loss: 1231.682
[5,     1] loss: 1230.417
[6,     1] loss: 1227.589
[7,     1] loss: 1224.209
[8,     1] loss: 1218.885
[9,     1] loss: 1201.253
[10,     1] loss: 1174.746
[11,     1] loss: 1136.577
[12,     1] loss: 1112.014
[13,     1] loss: 1070.323
[14,     1] loss: 1046.673
[15,     1] loss: 1005.537
[16,     1] loss: 1018.830
[17,     1] loss: 1051.453
[18,     1] loss: 1021.591
[19,     1] loss: 1071.700
[20,     1] loss: 997.698
[21,     1] loss: 1046.944
[22,     1] loss: 993.941
[23,     1] loss: 999.820
[24,     1] loss: 999.892
[25,     1] loss: 954.105
[26,     1] loss: 955.393
[27,     1] loss: 984.953
[28,     1] loss: 908.342
[29,     1] loss: 939.668
[30,     1] loss: 965.148
[31,     1] loss: 863.865
[32,     1] loss: 942.780
[33,     1] loss: 896.002
[34,     1] loss: 895.252
[35,     1] loss: 912.012
[36,     1] loss: 913.408
[37,     1] loss: 870.100
[38,     1] loss: 844.409
[39,     1] loss: 829.083
[40,     1] loss: 840.145
[41,     1] loss: 793.919
[42,     1] loss: 796.478
[43,     1] loss: 782.525
[44,     1] loss: 835.838
[45,     1] loss: 818.027
[46,     1] loss: 1182.432
[47,     1] loss: 790.856
[48,     1] loss: 1008.130
[49,     1] loss: 862.544
[50,     1] loss: 914.806
[51,     1] loss: 965.152
[52,     1] loss: 869.859
[53,     1] loss: 853.517
[54,     1] loss: 850.865
[55,     1] loss: 868.456
[56,     1] loss: 821.312
[57,     1] loss: 801.728
[58,     1] loss: 847.213
[59,     1] loss: 760.574
[60,     1] loss: 845.661
[61,     1] loss: 786.303
[62,     1] loss: 790.305
[63,     1] loss: 799.025
Early stopping applied (best metric=0.8390108942985535)
Finished Training
Total time taken: 10.009376525878906
{'Hydroxylation-K Validation Accuracy': 0.7578014184397163, 'Hydroxylation-K Validation Sensitivity': 0.6799999999999999, 'Hydroxylation-K Validation Specificity': 0.7771929824561403, 'Hydroxylation-K Validation Precision': 0.4441666684471907, 'Hydroxylation-K AUC ROC': 0.8157309941520468, 'Hydroxylation-K AUC PR': 0.5687456665585586, 'Hydroxylation-K MCC': 0.4002741971795171, 'Hydroxylation-K F1': 0.5318281441620825, 'Validation Loss (Hydroxylation-K)': 0.4298501471678416, 'Hydroxylation-P Validation Accuracy': 0.8066495270967633, 'Hydroxylation-P Validation Sensitivity': 0.7762962962962963, 'Hydroxylation-P Validation Specificity': 0.8131777145992319, 'Hydroxylation-P Validation Precision': 0.47688699350759983, 'Hydroxylation-P AUC ROC': 0.8536959766033535, 'Hydroxylation-P AUC PR': 0.5996994344479976, 'Hydroxylation-P MCC': 0.4980818898949832, 'Hydroxylation-P F1': 0.5882179872304607, 'Validation Loss (Hydroxylation-P)': 0.3677034060160319, 'Validation Loss (total)': 0.797553555170695, 'TimeToTrain': 12.273004452387491}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005341915122290126,
 'learning_rate_Hydroxylation-K': 7.807254674816101e-05,
 'learning_rate_Hydroxylation-P': 0.0042297506935447,
 'log_base': 1.097826458529702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3132166022,
 'sample_weights': [1.53172163239955, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.204580827841705,
 'weight_decay_Hydroxylation-K': 9.361237843747727,
 'weight_decay_Hydroxylation-P': 6.247268972768475}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5805.992
[2,     1] loss: 5780.132
[3,     1] loss: 5827.723
[4,     1] loss: 5794.721
[5,     1] loss: 5831.922
[6,     1] loss: 5806.976
[7,     1] loss: 5810.719
[8,     1] loss: 5781.358
[9,     1] loss: 5789.904
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020348532775379216,
 'learning_rate_Hydroxylation-K': 0.009069350737963555,
 'learning_rate_Hydroxylation-P': 0.006259447608786628,
 'log_base': 2.3231555176684533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1157571898,
 'sample_weights': [17.887093068326863, 2.235972768872473],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.637955842780968,
 'weight_decay_Hydroxylation-K': 3.4818091426396824,
 'weight_decay_Hydroxylation-P': 6.2028984274763665}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.985
[2,     1] loss: 1325.327
[3,     1] loss: 1330.098
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007476006394795914,
 'learning_rate_Hydroxylation-K': 0.009107870586234213,
 'learning_rate_Hydroxylation-P': 0.006313744743553012,
 'log_base': 2.732494648227193,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3455314849,
 'sample_weights': [1.980532523932992, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.843064681933258,
 'weight_decay_Hydroxylation-K': 5.88047825301264,
 'weight_decay_Hydroxylation-P': 4.891324245872857}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.880
[2,     1] loss: 1258.751
[3,     1] loss: 1259.063
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007744032959052145,
 'learning_rate_Hydroxylation-K': 0.0006349688614009543,
 'learning_rate_Hydroxylation-P': 0.004018617275709692,
 'log_base': 1.098915954306872,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 969541911,
 'sample_weights': [1.6607821973219254, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.400485890202329,
 'weight_decay_Hydroxylation-K': 7.35732724667761,
 'weight_decay_Hydroxylation-P': 3.2492412996071875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5778.509
[2,     1] loss: 5761.133
[3,     1] loss: 5721.812
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 6.877469737634118e-05,
 'learning_rate_Hydroxylation-K': 0.009916519298602116,
 'learning_rate_Hydroxylation-P': 0.008397680292318264,
 'log_base': 2.6834200242086155,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3565765071,
 'sample_weights': [17.69899120667861, 2.2124591303615593],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.114870115350689,
 'weight_decay_Hydroxylation-K': 3.8252557984900597,
 'weight_decay_Hydroxylation-P': 2.573602401503984}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.927
[2,     1] loss: 1265.899
[3,     1] loss: 1261.186
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038571121816408124,
 'learning_rate_Hydroxylation-K': 0.004663448195442688,
 'learning_rate_Hydroxylation-P': 0.0013964733375273928,
 'log_base': 2.6490759600706144,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2697356987,
 'sample_weights': [1.6912739262399221, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.777220181273726,
 'weight_decay_Hydroxylation-K': 7.292494496804949,
 'weight_decay_Hydroxylation-P': 2.4934759002370814}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.704
[2,     1] loss: 1274.055
[3,     1] loss: 1269.173
[4,     1] loss: 1264.234
[5,     1] loss: 1268.782
[6,     1] loss: 1259.412
[7,     1] loss: 1248.095
[8,     1] loss: 1233.302
[9,     1] loss: 1194.405
[10,     1] loss: 1155.425
[11,     1] loss: 1130.845
[12,     1] loss: 1093.530
[13,     1] loss: 1106.761
[14,     1] loss: 1033.137
[15,     1] loss: 1068.369
[16,     1] loss: 1057.282
[17,     1] loss: 1048.193
[18,     1] loss: 1036.474
[19,     1] loss: 1000.289
[20,     1] loss: 1017.907
[21,     1] loss: 1028.036
[22,     1] loss: 1022.523
[23,     1] loss: 1008.285
[24,     1] loss: 1013.915
[25,     1] loss: 987.094
[26,     1] loss: 948.409
[27,     1] loss: 954.711
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00017867574190432865,
 'learning_rate_Hydroxylation-K': 0.007469536484765427,
 'learning_rate_Hydroxylation-P': 0.0028513908105340722,
 'log_base': 2.106812508269119,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2380462882,
 'sample_weights': [1.7136363115406785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.303995693011287,
 'weight_decay_Hydroxylation-K': 6.219790656990217,
 'weight_decay_Hydroxylation-P': 4.918385076179951}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.864
[2,     1] loss: 1389.012
[3,     1] loss: 1388.079
[4,     1] loss: 1385.888
[5,     1] loss: 1383.138
[6,     1] loss: 1380.600
[7,     1] loss: 1386.535
[8,     1] loss: 1377.116
[9,     1] loss: 1379.934
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011885619263425347,
 'learning_rate_Hydroxylation-K': 0.0015054368385021798,
 'learning_rate_Hydroxylation-P': 0.0009575298233428136,
 'log_base': 2.9330916496677566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2292661520,
 'sample_weights': [2.240333586964478, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.465743948759039,
 'weight_decay_Hydroxylation-K': 0.7914825455611538,
 'weight_decay_Hydroxylation-P': 0.4002678576580916}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.932
[2,     1] loss: 1240.941
[3,     1] loss: 1237.409
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007748682882238866,
 'learning_rate_Hydroxylation-K': 5.993558825903068e-05,
 'learning_rate_Hydroxylation-P': 0.004672128326481451,
 'log_base': 1.8626527970427413,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 553055665,
 'sample_weights': [1.551444848750875, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.947525746485136,
 'weight_decay_Hydroxylation-K': 6.685571289132639,
 'weight_decay_Hydroxylation-P': 5.270976390963767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1475.426
[2,     1] loss: 1481.451
[3,     1] loss: 1466.998
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017172833068397105,
 'learning_rate_Hydroxylation-K': 5.863348001167236e-05,
 'learning_rate_Hydroxylation-P': 0.007863043066611761,
 'log_base': 2.4976975121992275,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 539194108,
 'sample_weights': [2.6839848343558845, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0186564942164744,
 'weight_decay_Hydroxylation-K': 7.416889695573739,
 'weight_decay_Hydroxylation-P': 1.779633441549622}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.938
[2,     1] loss: 1295.745
[3,     1] loss: 1294.445
[4,     1] loss: 1294.892
[5,     1] loss: 1290.000
[6,     1] loss: 1289.520
[7,     1] loss: 1292.085
[8,     1] loss: 1290.585
[9,     1] loss: 1286.447
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005806893363858795,
 'learning_rate_Hydroxylation-K': 0.0002811594405797728,
 'learning_rate_Hydroxylation-P': 0.00901949673965864,
 'log_base': 2.3666175850649283,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1567850996,
 'sample_weights': [1.823791910926154, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.0853778072385576,
 'weight_decay_Hydroxylation-K': 7.887604566772622,
 'weight_decay_Hydroxylation-P': 1.14577911631359}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.974
[2,     1] loss: 1317.958
[3,     1] loss: 1317.889
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005858504001249983,
 'learning_rate_Hydroxylation-K': 0.002359970809857446,
 'learning_rate_Hydroxylation-P': 0.0034907294991100646,
 'log_base': 1.061439980478793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1432376065,
 'sample_weights': [1.9379190485315942, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.820866111459482,
 'weight_decay_Hydroxylation-K': 2.9567604049462446,
 'weight_decay_Hydroxylation-P': 4.625847878489416}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9086.121
[2,     1] loss: 9139.982
[3,     1] loss: 9061.533
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003907530102075439,
 'learning_rate_Hydroxylation-K': 0.003969470749282436,
 'learning_rate_Hydroxylation-P': 0.004164770335843481,
 'log_base': 2.9355348895421973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3320199645,
 'sample_weights': [27.99836167756023, 3.4999300358603853],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.8823362496433695,
 'weight_decay_Hydroxylation-K': 9.358916472533432,
 'weight_decay_Hydroxylation-P': 6.173026933491488}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.501
[2,     1] loss: 1234.523
[3,     1] loss: 1231.242
[4,     1] loss: 1235.996
[5,     1] loss: 1231.532
[6,     1] loss: 1224.760
[7,     1] loss: 1223.878
[8,     1] loss: 1202.639
[9,     1] loss: 1190.831
[10,     1] loss: 1148.739
[11,     1] loss: 1128.198
[12,     1] loss: 1070.871
[13,     1] loss: 1046.727
[14,     1] loss: 1022.133
[15,     1] loss: 992.013
[16,     1] loss: 1026.744
[17,     1] loss: 989.487
[18,     1] loss: 1012.395
[19,     1] loss: 1015.436
[20,     1] loss: 988.283
[21,     1] loss: 995.642
[22,     1] loss: 981.400
[23,     1] loss: 984.452
[24,     1] loss: 1007.599
[25,     1] loss: 995.177
[26,     1] loss: 1009.611
[27,     1] loss: 907.646
[28,     1] loss: 931.030
[29,     1] loss: 862.687
[30,     1] loss: 930.612
[31,     1] loss: 883.017
[32,     1] loss: 900.649
[33,     1] loss: 878.312
[34,     1] loss: 895.336
[35,     1] loss: 868.928
[36,     1] loss: 904.689
[37,     1] loss: 877.423
[38,     1] loss: 822.574
[39,     1] loss: 856.877
[40,     1] loss: 823.702
[41,     1] loss: 848.770
[42,     1] loss: 823.178
[43,     1] loss: 867.328
[44,     1] loss: 836.930
[45,     1] loss: 771.678
[46,     1] loss: 825.450
[47,     1] loss: 797.812
[48,     1] loss: 718.304
[49,     1] loss: 834.211
[50,     1] loss: 839.658
[51,     1] loss: 768.311
[52,     1] loss: 803.797
[53,     1] loss: 715.857
[54,     1] loss: 743.656
[55,     1] loss: 724.379
[56,     1] loss: 704.965
[57,     1] loss: 667.130
[58,     1] loss: 653.448
[59,     1] loss: 632.889
[60,     1] loss: 675.933
[61,     1] loss: 1014.449
[62,     1] loss: 1606.293
[63,     1] loss: 962.525
[64,     1] loss: 804.593
[65,     1] loss: 1049.189
[66,     1] loss: 994.021
[67,     1] loss: 963.876
Early stopping applied (best metric=0.756397008895874)
Finished Training
Total time taken: 12.583836555480957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.322
[2,     1] loss: 1237.287
[3,     1] loss: 1235.079
[4,     1] loss: 1233.173
[5,     1] loss: 1227.554
[6,     1] loss: 1225.488
[7,     1] loss: 1199.932
[8,     1] loss: 1172.419
[9,     1] loss: 1147.313
[10,     1] loss: 1096.670
[11,     1] loss: 1091.047
[12,     1] loss: 1072.348
[13,     1] loss: 1034.466
[14,     1] loss: 1042.681
[15,     1] loss: 998.713
[16,     1] loss: 1064.194
[17,     1] loss: 990.270
[18,     1] loss: 1013.044
[19,     1] loss: 1006.882
[20,     1] loss: 988.719
[21,     1] loss: 1018.282
[22,     1] loss: 999.546
[23,     1] loss: 992.094
[24,     1] loss: 981.437
[25,     1] loss: 943.188
[26,     1] loss: 999.632
[27,     1] loss: 947.353
[28,     1] loss: 925.779
[29,     1] loss: 995.753
[30,     1] loss: 939.053
[31,     1] loss: 968.490
[32,     1] loss: 918.475
[33,     1] loss: 871.711
[34,     1] loss: 895.294
[35,     1] loss: 915.093
[36,     1] loss: 846.487
[37,     1] loss: 901.558
[38,     1] loss: 918.949
[39,     1] loss: 855.949
[40,     1] loss: 938.553
[41,     1] loss: 857.901
[42,     1] loss: 861.088
[43,     1] loss: 848.145
[44,     1] loss: 758.843
[45,     1] loss: 810.343
[46,     1] loss: 815.528
[47,     1] loss: 790.323
[48,     1] loss: 786.400
[49,     1] loss: 772.157
[50,     1] loss: 742.815
[51,     1] loss: 802.548
[52,     1] loss: 790.633
[53,     1] loss: 723.830
[54,     1] loss: 747.803
[55,     1] loss: 702.595
[56,     1] loss: 688.785
[57,     1] loss: 725.490
[58,     1] loss: 693.944
[59,     1] loss: 675.394
[60,     1] loss: 790.360
[61,     1] loss: 936.884
[62,     1] loss: 672.744
[63,     1] loss: 751.037
[64,     1] loss: 690.515
[65,     1] loss: 753.682
[66,     1] loss: 656.216
[67,     1] loss: 683.006
[68,     1] loss: 624.615
[69,     1] loss: 712.767
[70,     1] loss: 586.036
[71,     1] loss: 634.214
[72,     1] loss: 615.405
[73,     1] loss: 590.530
[74,     1] loss: 550.461
[75,     1] loss: 511.940
[76,     1] loss: 530.507
[77,     1] loss: 515.713
[78,     1] loss: 517.093
[79,     1] loss: 578.169
[80,     1] loss: 547.838
[81,     1] loss: 829.012
[82,     1] loss: 1290.671
[83,     1] loss: 728.768
[84,     1] loss: 797.831
[85,     1] loss: 1003.958
[86,     1] loss: 845.671
[87,     1] loss: 856.313
[88,     1] loss: 874.496
[89,     1] loss: 835.262
Early stopping applied (best metric=0.7797330021858215)
Finished Training
Total time taken: 17.090937614440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.290
[2,     1] loss: 1232.827
[3,     1] loss: 1234.254
[4,     1] loss: 1234.177
[5,     1] loss: 1228.234
[6,     1] loss: 1225.015
[7,     1] loss: 1211.620
[8,     1] loss: 1191.065
[9,     1] loss: 1159.968
[10,     1] loss: 1123.571
[11,     1] loss: 1086.815
[12,     1] loss: 1019.375
[13,     1] loss: 1103.288
[14,     1] loss: 1012.385
[15,     1] loss: 976.725
[16,     1] loss: 980.353
[17,     1] loss: 967.039
[18,     1] loss: 958.869
[19,     1] loss: 956.003
[20,     1] loss: 931.501
[21,     1] loss: 937.950
[22,     1] loss: 941.317
[23,     1] loss: 915.510
[24,     1] loss: 936.234
[25,     1] loss: 908.604
[26,     1] loss: 868.167
[27,     1] loss: 905.022
[28,     1] loss: 863.171
[29,     1] loss: 897.046
[30,     1] loss: 876.684
[31,     1] loss: 875.730
[32,     1] loss: 852.901
[33,     1] loss: 852.859
[34,     1] loss: 853.882
[35,     1] loss: 841.013
[36,     1] loss: 818.296
[37,     1] loss: 852.095
[38,     1] loss: 794.951
[39,     1] loss: 764.319
[40,     1] loss: 805.188
[41,     1] loss: 784.407
[42,     1] loss: 867.728
[43,     1] loss: 789.968
[44,     1] loss: 779.910
[45,     1] loss: 830.431
[46,     1] loss: 766.640
[47,     1] loss: 784.305
[48,     1] loss: 778.151
[49,     1] loss: 759.612
[50,     1] loss: 836.082
[51,     1] loss: 731.979
[52,     1] loss: 766.872
[53,     1] loss: 817.374
[54,     1] loss: 759.339
[55,     1] loss: 747.490
[56,     1] loss: 680.005
[57,     1] loss: 742.929
[58,     1] loss: 667.057
Early stopping applied (best metric=0.8200466632843018)
Finished Training
Total time taken: 8.933742046356201
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.229
[2,     1] loss: 1234.129
[3,     1] loss: 1234.114
[4,     1] loss: 1234.162
[5,     1] loss: 1231.439
[6,     1] loss: 1233.154
[7,     1] loss: 1229.438
[8,     1] loss: 1225.567
[9,     1] loss: 1214.588
[10,     1] loss: 1202.110
[11,     1] loss: 1173.639
[12,     1] loss: 1125.734
[13,     1] loss: 1083.140
[14,     1] loss: 1047.232
[15,     1] loss: 1060.481
[16,     1] loss: 1043.786
[17,     1] loss: 1038.264
[18,     1] loss: 1024.872
[19,     1] loss: 987.836
[20,     1] loss: 963.079
[21,     1] loss: 984.369
[22,     1] loss: 951.552
[23,     1] loss: 933.067
[24,     1] loss: 946.177
[25,     1] loss: 878.205
[26,     1] loss: 932.227
[27,     1] loss: 907.169
[28,     1] loss: 889.779
[29,     1] loss: 938.574
[30,     1] loss: 869.765
[31,     1] loss: 861.324
[32,     1] loss: 888.351
[33,     1] loss: 879.459
[34,     1] loss: 870.316
[35,     1] loss: 973.883
[36,     1] loss: 804.981
[37,     1] loss: 829.492
[38,     1] loss: 875.415
[39,     1] loss: 787.717
[40,     1] loss: 829.478
[41,     1] loss: 778.952
[42,     1] loss: 822.550
[43,     1] loss: 783.408
[44,     1] loss: 875.447
[45,     1] loss: 792.238
[46,     1] loss: 783.993
[47,     1] loss: 787.460
[48,     1] loss: 708.129
[49,     1] loss: 708.333
[50,     1] loss: 722.549
[51,     1] loss: 761.346
[52,     1] loss: 716.487
[53,     1] loss: 732.818
Early stopping applied (best metric=0.8908888101577759)
Finished Training
Total time taken: 9.020265102386475
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1241.460
[2,     1] loss: 1238.339
[3,     1] loss: 1229.962
[4,     1] loss: 1243.480
[5,     1] loss: 1230.488
[6,     1] loss: 1238.073
[7,     1] loss: 1236.661
[8,     1] loss: 1229.424
[9,     1] loss: 1227.555
[10,     1] loss: 1210.425
[11,     1] loss: 1198.954
[12,     1] loss: 1161.771
[13,     1] loss: 1138.358
[14,     1] loss: 1107.174
[15,     1] loss: 1068.055
[16,     1] loss: 1047.019
[17,     1] loss: 1004.603
[18,     1] loss: 999.179
[19,     1] loss: 955.811
[20,     1] loss: 1042.658
[21,     1] loss: 998.333
[22,     1] loss: 944.422
[23,     1] loss: 996.088
[24,     1] loss: 973.361
[25,     1] loss: 958.988
[26,     1] loss: 982.835
[27,     1] loss: 925.140
[28,     1] loss: 934.928
[29,     1] loss: 894.459
[30,     1] loss: 942.031
[31,     1] loss: 899.327
[32,     1] loss: 857.942
[33,     1] loss: 943.748
[34,     1] loss: 982.250
[35,     1] loss: 896.926
[36,     1] loss: 859.521
[37,     1] loss: 932.918
[38,     1] loss: 868.607
[39,     1] loss: 862.277
[40,     1] loss: 909.941
[41,     1] loss: 845.933
[42,     1] loss: 851.165
[43,     1] loss: 838.241
[44,     1] loss: 766.661
[45,     1] loss: 807.156
[46,     1] loss: 798.849
[47,     1] loss: 826.473
[48,     1] loss: 782.383
[49,     1] loss: 798.177
[50,     1] loss: 769.225
[51,     1] loss: 804.067
[52,     1] loss: 774.082
[53,     1] loss: 732.135
[54,     1] loss: 729.879
[55,     1] loss: 703.195
[56,     1] loss: 710.363
[57,     1] loss: 738.613
[58,     1] loss: 721.402
[59,     1] loss: 703.445
[60,     1] loss: 676.244
[61,     1] loss: 573.869
[62,     1] loss: 701.865
[63,     1] loss: 684.973
[64,     1] loss: 634.777
[65,     1] loss: 603.577
[66,     1] loss: 647.648
[67,     1] loss: 747.168
[68,     1] loss: 733.922
[69,     1] loss: 653.720
[70,     1] loss: 576.911
[71,     1] loss: 697.307
[72,     1] loss: 694.045
[73,     1] loss: 566.122
[74,     1] loss: 690.633
[75,     1] loss: 587.459
[76,     1] loss: 598.311
[77,     1] loss: 519.088
[78,     1] loss: 564.520
[79,     1] loss: 555.311
[80,     1] loss: 497.551
[81,     1] loss: 644.936
[82,     1] loss: 683.936
[83,     1] loss: 454.510
[84,     1] loss: 715.199
[85,     1] loss: 581.246
[86,     1] loss: 637.986
[87,     1] loss: 543.646
[88,     1] loss: 601.750
[89,     1] loss: 532.337
[90,     1] loss: 644.614
[91,     1] loss: 503.755
[92,     1] loss: 586.474
[93,     1] loss: 462.370
[94,     1] loss: 533.690
[95,     1] loss: 447.486
[96,     1] loss: 529.444
[97,     1] loss: 590.345
[98,     1] loss: 482.394
[99,     1] loss: 678.109
[100,     1] loss: 502.853
[101,     1] loss: 577.923
[102,     1] loss: 479.023
[103,     1] loss: 570.002
[104,     1] loss: 442.090
[105,     1] loss: 521.230
[106,     1] loss: 415.853
[107,     1] loss: 464.934
[108,     1] loss: 465.723
[109,     1] loss: 372.290
[110,     1] loss: 402.783
[111,     1] loss: 381.945
[112,     1] loss: 383.797
[113,     1] loss: 362.726
Early stopping applied (best metric=0.6703789234161377)
Finished Training
Total time taken: 20.42595410346985
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.810
[2,     1] loss: 1239.458
[3,     1] loss: 1235.872
[4,     1] loss: 1233.630
[5,     1] loss: 1231.446
[6,     1] loss: 1227.574
[7,     1] loss: 1222.931
[8,     1] loss: 1206.725
[9,     1] loss: 1175.311
[10,     1] loss: 1148.237
[11,     1] loss: 1094.973
[12,     1] loss: 1064.315
[13,     1] loss: 1076.212
[14,     1] loss: 1013.818
[15,     1] loss: 1069.667
[16,     1] loss: 1029.982
[17,     1] loss: 1033.440
[18,     1] loss: 1056.142
[19,     1] loss: 992.418
[20,     1] loss: 1019.859
[21,     1] loss: 978.354
[22,     1] loss: 973.883
[23,     1] loss: 985.325
[24,     1] loss: 952.517
[25,     1] loss: 947.269
[26,     1] loss: 958.933
[27,     1] loss: 936.034
[28,     1] loss: 888.531
[29,     1] loss: 869.895
[30,     1] loss: 885.885
[31,     1] loss: 853.534
[32,     1] loss: 898.396
[33,     1] loss: 869.772
[34,     1] loss: 846.343
[35,     1] loss: 875.845
[36,     1] loss: 879.679
[37,     1] loss: 843.422
[38,     1] loss: 822.595
[39,     1] loss: 855.773
[40,     1] loss: 839.205
[41,     1] loss: 780.682
[42,     1] loss: 851.180
[43,     1] loss: 785.169
[44,     1] loss: 808.705
[45,     1] loss: 817.928
[46,     1] loss: 720.573
[47,     1] loss: 786.088
[48,     1] loss: 852.029
[49,     1] loss: 739.966
[50,     1] loss: 726.267
[51,     1] loss: 767.754
[52,     1] loss: 701.464
[53,     1] loss: 808.589
[54,     1] loss: 707.702
[55,     1] loss: 691.839
[56,     1] loss: 663.704
[57,     1] loss: 647.866
Early stopping applied (best metric=0.8600520491600037)
Finished Training
Total time taken: 9.165708065032959
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.032
[2,     1] loss: 1233.168
[3,     1] loss: 1238.095
[4,     1] loss: 1234.985
[5,     1] loss: 1226.841
[6,     1] loss: 1218.954
[7,     1] loss: 1211.387
[8,     1] loss: 1182.472
[9,     1] loss: 1151.803
[10,     1] loss: 1107.358
[11,     1] loss: 1086.739
[12,     1] loss: 1070.206
[13,     1] loss: 1108.207
[14,     1] loss: 1017.220
[15,     1] loss: 1105.851
[16,     1] loss: 1044.186
[17,     1] loss: 1059.442
[18,     1] loss: 987.852
[19,     1] loss: 1018.981
[20,     1] loss: 996.541
[21,     1] loss: 995.000
[22,     1] loss: 942.110
[23,     1] loss: 952.771
[24,     1] loss: 961.962
[25,     1] loss: 911.941
[26,     1] loss: 914.513
[27,     1] loss: 935.624
[28,     1] loss: 943.287
[29,     1] loss: 901.769
[30,     1] loss: 920.471
[31,     1] loss: 925.279
[32,     1] loss: 884.778
[33,     1] loss: 877.062
[34,     1] loss: 821.011
[35,     1] loss: 826.862
[36,     1] loss: 814.404
[37,     1] loss: 794.901
[38,     1] loss: 715.219
[39,     1] loss: 810.708
[40,     1] loss: 797.530
[41,     1] loss: 723.867
[42,     1] loss: 763.880
[43,     1] loss: 749.824
[44,     1] loss: 741.440
Early stopping applied (best metric=0.8498929738998413)
Finished Training
Total time taken: 8.640246152877808
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.134
[2,     1] loss: 1235.927
[3,     1] loss: 1237.400
[4,     1] loss: 1234.704
[5,     1] loss: 1234.950
[6,     1] loss: 1232.620
[7,     1] loss: 1229.034
[8,     1] loss: 1225.433
[9,     1] loss: 1207.052
[10,     1] loss: 1186.039
[11,     1] loss: 1165.717
[12,     1] loss: 1119.604
[13,     1] loss: 1108.714
[14,     1] loss: 1103.209
[15,     1] loss: 1069.414
[16,     1] loss: 1037.960
[17,     1] loss: 1036.843
[18,     1] loss: 1074.281
[19,     1] loss: 999.820
[20,     1] loss: 1086.270
[21,     1] loss: 986.991
[22,     1] loss: 1057.544
[23,     1] loss: 1000.193
[24,     1] loss: 972.501
[25,     1] loss: 968.070
[26,     1] loss: 1016.812
[27,     1] loss: 969.052
[28,     1] loss: 977.535
[29,     1] loss: 921.935
[30,     1] loss: 955.043
[31,     1] loss: 915.443
[32,     1] loss: 938.441
[33,     1] loss: 902.496
[34,     1] loss: 930.704
[35,     1] loss: 848.944
[36,     1] loss: 842.041
[37,     1] loss: 833.848
[38,     1] loss: 829.871
[39,     1] loss: 831.795
[40,     1] loss: 848.827
[41,     1] loss: 838.294
[42,     1] loss: 787.341
[43,     1] loss: 768.891
[44,     1] loss: 806.577
[45,     1] loss: 757.807
[46,     1] loss: 800.580
[47,     1] loss: 778.100
[48,     1] loss: 838.919
[49,     1] loss: 824.599
[50,     1] loss: 760.286
[51,     1] loss: 766.052
[52,     1] loss: 732.699
[53,     1] loss: 739.038
[54,     1] loss: 723.263
[55,     1] loss: 681.815
[56,     1] loss: 762.349
[57,     1] loss: 882.093
[58,     1] loss: 700.259
[59,     1] loss: 821.114
[60,     1] loss: 727.959
[61,     1] loss: 825.371
[62,     1] loss: 723.096
[63,     1] loss: 839.065
[64,     1] loss: 741.106
[65,     1] loss: 722.664
[66,     1] loss: 758.994
[67,     1] loss: 662.200
[68,     1] loss: 679.207
[69,     1] loss: 674.934
[70,     1] loss: 705.827
[71,     1] loss: 615.286
[72,     1] loss: 656.218
[73,     1] loss: 660.226
[74,     1] loss: 704.241
[75,     1] loss: 619.981
[76,     1] loss: 624.335
[77,     1] loss: 586.044
Early stopping applied (best metric=0.6399614810943604)
Finished Training
Total time taken: 12.139771699905396
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.860
[2,     1] loss: 1234.307
[3,     1] loss: 1232.972
[4,     1] loss: 1230.252
[5,     1] loss: 1215.274
[6,     1] loss: 1189.052
[7,     1] loss: 1150.926
[8,     1] loss: 1118.602
[9,     1] loss: 1082.418
[10,     1] loss: 1070.037
[11,     1] loss: 1035.507
[12,     1] loss: 1016.665
[13,     1] loss: 988.642
[14,     1] loss: 985.472
[15,     1] loss: 968.690
[16,     1] loss: 1010.762
[17,     1] loss: 933.228
[18,     1] loss: 985.734
[19,     1] loss: 971.341
[20,     1] loss: 938.182
[21,     1] loss: 951.255
[22,     1] loss: 921.801
[23,     1] loss: 886.797
[24,     1] loss: 942.744
[25,     1] loss: 931.673
[26,     1] loss: 902.736
[27,     1] loss: 891.411
[28,     1] loss: 913.535
[29,     1] loss: 878.418
[30,     1] loss: 858.748
[31,     1] loss: 837.447
[32,     1] loss: 839.430
[33,     1] loss: 833.767
[34,     1] loss: 804.971
[35,     1] loss: 941.868
[36,     1] loss: 879.723
[37,     1] loss: 769.463
[38,     1] loss: 814.459
[39,     1] loss: 846.277
[40,     1] loss: 772.399
[41,     1] loss: 889.201
[42,     1] loss: 747.850
[43,     1] loss: 823.921
[44,     1] loss: 709.815
[45,     1] loss: 760.225
[46,     1] loss: 719.804
[47,     1] loss: 708.768
[48,     1] loss: 719.544
[49,     1] loss: 731.895
[50,     1] loss: 630.501
[51,     1] loss: 686.366
[52,     1] loss: 660.648
[53,     1] loss: 677.535
[54,     1] loss: 610.636
[55,     1] loss: 588.606
[56,     1] loss: 602.955
[57,     1] loss: 564.491
[58,     1] loss: 596.337
[59,     1] loss: 663.378
[60,     1] loss: 795.906
[61,     1] loss: 755.355
Early stopping applied (best metric=0.8331543207168579)
Finished Training
Total time taken: 10.742226123809814
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1240.385
[2,     1] loss: 1232.943
[3,     1] loss: 1237.349
[4,     1] loss: 1237.387
[5,     1] loss: 1229.591
[6,     1] loss: 1228.666
[7,     1] loss: 1216.656
[8,     1] loss: 1194.872
[9,     1] loss: 1161.757
[10,     1] loss: 1132.178
[11,     1] loss: 1087.639
[12,     1] loss: 1070.918
[13,     1] loss: 1043.588
[14,     1] loss: 1033.288
[15,     1] loss: 1046.228
[16,     1] loss: 1018.896
[17,     1] loss: 1025.557
[18,     1] loss: 997.490
[19,     1] loss: 969.534
[20,     1] loss: 1011.337
[21,     1] loss: 961.347
[22,     1] loss: 1019.152
[23,     1] loss: 947.421
[24,     1] loss: 961.914
[25,     1] loss: 876.858
[26,     1] loss: 913.550
[27,     1] loss: 924.675
[28,     1] loss: 910.225
[29,     1] loss: 921.908
[30,     1] loss: 851.511
[31,     1] loss: 957.689
[32,     1] loss: 829.587
[33,     1] loss: 909.840
[34,     1] loss: 844.720
[35,     1] loss: 903.581
[36,     1] loss: 851.164
[37,     1] loss: 822.498
[38,     1] loss: 813.931
[39,     1] loss: 781.135
[40,     1] loss: 810.969
[41,     1] loss: 828.185
[42,     1] loss: 793.731
[43,     1] loss: 747.458
[44,     1] loss: 758.094
[45,     1] loss: 829.465
[46,     1] loss: 779.485
[47,     1] loss: 761.138
[48,     1] loss: 767.950
[49,     1] loss: 718.637
[50,     1] loss: 694.638
[51,     1] loss: 847.521
[52,     1] loss: 875.284
[53,     1] loss: 677.961
[54,     1] loss: 794.643
[55,     1] loss: 662.306
[56,     1] loss: 763.365
[57,     1] loss: 690.838
[58,     1] loss: 759.582
[59,     1] loss: 684.623
[60,     1] loss: 670.472
[61,     1] loss: 645.564
[62,     1] loss: 680.201
[63,     1] loss: 608.641
[64,     1] loss: 637.610
[65,     1] loss: 661.164
[66,     1] loss: 612.585
[67,     1] loss: 695.266
[68,     1] loss: 616.291
[69,     1] loss: 595.050
[70,     1] loss: 594.717
[71,     1] loss: 600.390
[72,     1] loss: 508.226
[73,     1] loss: 554.122
[74,     1] loss: 560.747
[75,     1] loss: 749.074
[76,     1] loss: 827.240
Early stopping applied (best metric=0.8409225344657898)
Finished Training
Total time taken: 12.679783821105957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.671
[2,     1] loss: 1235.109
[3,     1] loss: 1235.178
[4,     1] loss: 1233.404
[5,     1] loss: 1236.215
[6,     1] loss: 1235.244
[7,     1] loss: 1237.229
[8,     1] loss: 1232.707
[9,     1] loss: 1229.395
[10,     1] loss: 1227.927
[11,     1] loss: 1223.287
[12,     1] loss: 1214.655
[13,     1] loss: 1197.992
[14,     1] loss: 1184.645
[15,     1] loss: 1153.181
[16,     1] loss: 1140.797
[17,     1] loss: 1108.589
[18,     1] loss: 1107.271
[19,     1] loss: 1070.528
[20,     1] loss: 1048.567
[21,     1] loss: 1027.493
[22,     1] loss: 1043.453
[23,     1] loss: 1018.345
[24,     1] loss: 1042.164
[25,     1] loss: 1012.134
[26,     1] loss: 1000.519
[27,     1] loss: 992.799
[28,     1] loss: 962.451
[29,     1] loss: 988.067
[30,     1] loss: 1000.657
[31,     1] loss: 937.507
[32,     1] loss: 916.745
[33,     1] loss: 975.346
[34,     1] loss: 958.346
[35,     1] loss: 936.910
[36,     1] loss: 922.343
[37,     1] loss: 894.290
[38,     1] loss: 858.942
[39,     1] loss: 837.989
[40,     1] loss: 875.024
[41,     1] loss: 894.449
[42,     1] loss: 845.908
[43,     1] loss: 861.230
[44,     1] loss: 810.583
[45,     1] loss: 829.518
[46,     1] loss: 807.688
[47,     1] loss: 797.232
[48,     1] loss: 804.520
[49,     1] loss: 726.990
[50,     1] loss: 890.370
[51,     1] loss: 1327.779
[52,     1] loss: 774.740
[53,     1] loss: 1010.026
[54,     1] loss: 789.117
[55,     1] loss: 859.742
[56,     1] loss: 920.329
[57,     1] loss: 855.271
[58,     1] loss: 838.767
[59,     1] loss: 873.480
[60,     1] loss: 868.410
[61,     1] loss: 858.908
[62,     1] loss: 795.223
Early stopping applied (best metric=0.7690925598144531)
Finished Training
Total time taken: 9.882785558700562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.828
[2,     1] loss: 1238.765
[3,     1] loss: 1232.883
[4,     1] loss: 1235.030
[5,     1] loss: 1234.629
[6,     1] loss: 1231.136
[7,     1] loss: 1226.212
[8,     1] loss: 1229.944
[9,     1] loss: 1216.917
[10,     1] loss: 1202.074
[11,     1] loss: 1182.885
[12,     1] loss: 1145.470
[13,     1] loss: 1111.047
[14,     1] loss: 1078.793
[15,     1] loss: 1048.200
[16,     1] loss: 1029.565
[17,     1] loss: 1035.164
[18,     1] loss: 995.269
[19,     1] loss: 1047.242
[20,     1] loss: 999.333
[21,     1] loss: 992.563
[22,     1] loss: 983.131
[23,     1] loss: 949.049
[24,     1] loss: 943.506
[25,     1] loss: 957.922
[26,     1] loss: 989.577
[27,     1] loss: 932.560
[28,     1] loss: 934.874
[29,     1] loss: 908.316
[30,     1] loss: 911.417
[31,     1] loss: 890.337
[32,     1] loss: 857.475
[33,     1] loss: 893.682
[34,     1] loss: 875.990
[35,     1] loss: 875.887
[36,     1] loss: 875.075
[37,     1] loss: 866.187
[38,     1] loss: 822.063
[39,     1] loss: 858.644
[40,     1] loss: 861.020
[41,     1] loss: 846.605
[42,     1] loss: 840.652
[43,     1] loss: 785.817
[44,     1] loss: 820.317
[45,     1] loss: 838.776
[46,     1] loss: 778.404
[47,     1] loss: 792.775
[48,     1] loss: 768.699
[49,     1] loss: 745.804
[50,     1] loss: 710.711
[51,     1] loss: 721.449
[52,     1] loss: 737.114
[53,     1] loss: 748.191
[54,     1] loss: 792.948
[55,     1] loss: 796.125
[56,     1] loss: 720.063
[57,     1] loss: 694.026
[58,     1] loss: 697.527
[59,     1] loss: 699.979
[60,     1] loss: 687.963
[61,     1] loss: 667.659
[62,     1] loss: 662.201
[63,     1] loss: 627.689
[64,     1] loss: 594.317
[65,     1] loss: 593.636
[66,     1] loss: 645.000
[67,     1] loss: 690.773
[68,     1] loss: 855.436
[69,     1] loss: 614.530
[70,     1] loss: 594.510
Early stopping applied (best metric=0.7748715877532959)
Finished Training
Total time taken: 12.069255352020264
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.444
[2,     1] loss: 1233.235
[3,     1] loss: 1232.927
[4,     1] loss: 1232.810
[5,     1] loss: 1234.994
[6,     1] loss: 1227.887
[7,     1] loss: 1223.134
[8,     1] loss: 1218.135
[9,     1] loss: 1200.517
[10,     1] loss: 1175.115
[11,     1] loss: 1131.589
[12,     1] loss: 1116.138
[13,     1] loss: 1070.037
[14,     1] loss: 1032.847
[15,     1] loss: 1042.399
[16,     1] loss: 1018.670
[17,     1] loss: 1037.690
[18,     1] loss: 964.117
[19,     1] loss: 1000.363
[20,     1] loss: 988.652
[21,     1] loss: 986.750
[22,     1] loss: 962.151
[23,     1] loss: 950.335
[24,     1] loss: 967.328
[25,     1] loss: 915.245
[26,     1] loss: 911.198
[27,     1] loss: 946.296
[28,     1] loss: 873.063
[29,     1] loss: 918.525
[30,     1] loss: 927.787
[31,     1] loss: 878.145
[32,     1] loss: 858.058
[33,     1] loss: 873.369
[34,     1] loss: 830.305
[35,     1] loss: 861.471
[36,     1] loss: 821.904
[37,     1] loss: 767.336
[38,     1] loss: 813.148
[39,     1] loss: 826.687
[40,     1] loss: 802.407
[41,     1] loss: 800.828
[42,     1] loss: 799.323
[43,     1] loss: 742.793
[44,     1] loss: 795.896
[45,     1] loss: 820.276
[46,     1] loss: 791.483
[47,     1] loss: 741.874
[48,     1] loss: 780.456
[49,     1] loss: 776.585
[50,     1] loss: 691.504
[51,     1] loss: 783.878
[52,     1] loss: 767.707
[53,     1] loss: 682.647
[54,     1] loss: 727.289
[55,     1] loss: 684.568
[56,     1] loss: 751.927
[57,     1] loss: 648.859
[58,     1] loss: 656.567
[59,     1] loss: 624.427
[60,     1] loss: 686.666
[61,     1] loss: 643.128
[62,     1] loss: 634.672
Early stopping applied (best metric=0.8039028644561768)
Finished Training
Total time taken: 8.821184635162354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.902
[2,     1] loss: 1234.795
[3,     1] loss: 1231.340
[4,     1] loss: 1224.208
[5,     1] loss: 1211.906
[6,     1] loss: 1180.768
[7,     1] loss: 1148.792
[8,     1] loss: 1103.283
[9,     1] loss: 1044.705
[10,     1] loss: 1064.459
[11,     1] loss: 993.983
[12,     1] loss: 1077.330
[13,     1] loss: 1021.358
[14,     1] loss: 1065.250
[15,     1] loss: 949.524
[16,     1] loss: 1015.407
[17,     1] loss: 984.566
[18,     1] loss: 971.742
[19,     1] loss: 1008.282
[20,     1] loss: 972.920
[21,     1] loss: 966.993
[22,     1] loss: 939.513
[23,     1] loss: 942.841
[24,     1] loss: 909.520
[25,     1] loss: 932.051
[26,     1] loss: 947.419
[27,     1] loss: 933.489
[28,     1] loss: 910.928
[29,     1] loss: 888.635
[30,     1] loss: 864.728
[31,     1] loss: 891.993
[32,     1] loss: 856.767
[33,     1] loss: 850.072
[34,     1] loss: 836.916
[35,     1] loss: 873.869
[36,     1] loss: 864.611
[37,     1] loss: 881.449
[38,     1] loss: 835.445
[39,     1] loss: 800.642
[40,     1] loss: 795.469
[41,     1] loss: 808.401
[42,     1] loss: 788.088
[43,     1] loss: 744.147
[44,     1] loss: 758.225
[45,     1] loss: 678.422
[46,     1] loss: 735.591
[47,     1] loss: 957.112
[48,     1] loss: 1078.599
[49,     1] loss: 713.966
[50,     1] loss: 939.271
[51,     1] loss: 842.705
[52,     1] loss: 824.160
[53,     1] loss: 886.098
[54,     1] loss: 827.837
[55,     1] loss: 848.947
[56,     1] loss: 792.543
[57,     1] loss: 824.738
[58,     1] loss: 769.931
[59,     1] loss: 741.546
[60,     1] loss: 768.285
[61,     1] loss: 716.128
[62,     1] loss: 792.022
Early stopping applied (best metric=0.7890179753303528)
Finished Training
Total time taken: 8.400176763534546
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1242.708
[2,     1] loss: 1238.203
[3,     1] loss: 1242.329
[4,     1] loss: 1236.958
[5,     1] loss: 1237.933
[6,     1] loss: 1233.152
[7,     1] loss: 1234.806
[8,     1] loss: 1232.247
[9,     1] loss: 1231.679
[10,     1] loss: 1227.710
[11,     1] loss: 1217.546
[12,     1] loss: 1207.683
[13,     1] loss: 1186.115
[14,     1] loss: 1152.898
[15,     1] loss: 1119.176
[16,     1] loss: 1086.805
[17,     1] loss: 1058.554
[18,     1] loss: 1027.710
[19,     1] loss: 1035.554
[20,     1] loss: 955.621
[21,     1] loss: 1009.912
[22,     1] loss: 965.813
[23,     1] loss: 1006.174
[24,     1] loss: 973.703
[25,     1] loss: 942.237
[26,     1] loss: 936.229
[27,     1] loss: 960.993
[28,     1] loss: 932.204
[29,     1] loss: 938.173
[30,     1] loss: 926.307
[31,     1] loss: 924.509
[32,     1] loss: 918.214
[33,     1] loss: 853.456
[34,     1] loss: 895.894
[35,     1] loss: 899.982
[36,     1] loss: 862.572
[37,     1] loss: 853.399
[38,     1] loss: 833.975
[39,     1] loss: 846.822
[40,     1] loss: 850.406
[41,     1] loss: 858.238
[42,     1] loss: 818.218
[43,     1] loss: 816.553
[44,     1] loss: 826.074
[45,     1] loss: 828.284
[46,     1] loss: 818.299
[47,     1] loss: 844.158
[48,     1] loss: 906.104
[49,     1] loss: 764.270
[50,     1] loss: 799.914
[51,     1] loss: 785.634
[52,     1] loss: 787.547
[53,     1] loss: 791.420
[54,     1] loss: 743.477
[55,     1] loss: 766.321
[56,     1] loss: 694.412
[57,     1] loss: 703.921
[58,     1] loss: 655.234
[59,     1] loss: 714.302
[60,     1] loss: 714.396
[61,     1] loss: 630.508
[62,     1] loss: 742.647
[63,     1] loss: 695.557
[64,     1] loss: 644.863
[65,     1] loss: 733.463
[66,     1] loss: 611.937
[67,     1] loss: 666.187
[68,     1] loss: 629.194
Early stopping applied (best metric=0.9795061349868774)
Finished Training
Total time taken: 11.267239332199097
{'Hydroxylation-K Validation Accuracy': 0.7687056737588652, 'Hydroxylation-K Validation Sensitivity': 0.6437037037037037, 'Hydroxylation-K Validation Specificity': 0.8, 'Hydroxylation-K Validation Precision': 0.4524843674843675, 'Hydroxylation-K AUC ROC': 0.8007797270955166, 'Hydroxylation-K AUC PR': 0.6003852704758393, 'Hydroxylation-K MCC': 0.39502132278899593, 'Hydroxylation-K F1': 0.528526372178546, 'Validation Loss (Hydroxylation-K)': 0.43708039124806725, 'Hydroxylation-P Validation Accuracy': 0.8006697629561951, 'Hydroxylation-P Validation Sensitivity': 0.7688359788359789, 'Hydroxylation-P Validation Specificity': 0.807496633248541, 'Hydroxylation-P Validation Precision': 0.46695890485321284, 'Hydroxylation-P AUC ROC': 0.8576126050699595, 'Hydroxylation-P AUC PR': 0.5920026280683826, 'Hydroxylation-P MCC': 0.48531753986464116, 'Hydroxylation-P F1': 0.5788926757284704, 'Validation Loss (Hydroxylation-P)': 0.366774195432663, 'Validation Loss (total)': 0.8038545926411946, 'TimeToTrain': 11.457540861765544}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002476172574900475,
 'learning_rate_Hydroxylation-K': 0.0002263971504215474,
 'learning_rate_Hydroxylation-P': 0.003046677678483059,
 'log_base': 2.6164209500905944,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2292208965,
 'sample_weights': [1.5513952504667095, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7738630229264603,
 'weight_decay_Hydroxylation-K': 8.854606863791055,
 'weight_decay_Hydroxylation-P': 4.365418848989452}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.739
[2,     1] loss: 1272.826
[3,     1] loss: 1278.066
[4,     1] loss: 1269.922
[5,     1] loss: 1273.207
[6,     1] loss: 1267.682
[7,     1] loss: 1274.896
[8,     1] loss: 1270.444
[9,     1] loss: 1266.473
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00391377585561345,
 'learning_rate_Hydroxylation-K': 0.0021943017873800137,
 'learning_rate_Hydroxylation-P': 0.006291904867407521,
 'log_base': 2.2529886987339305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 311994150,
 'sample_weights': [1.7357355136524837, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.626256719129942,
 'weight_decay_Hydroxylation-K': 4.620739654978006,
 'weight_decay_Hydroxylation-P': 1.5221418003752547}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1352.863
[2,     1] loss: 1341.991
[3,     1] loss: 1339.766
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0076469645621905944,
 'learning_rate_Hydroxylation-K': 2.501984206609582e-05,
 'learning_rate_Hydroxylation-P': 0.00693197617107361,
 'log_base': 1.4409099372602259,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2089242351,
 'sample_weights': [2.0553123223892262, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.194729096080788,
 'weight_decay_Hydroxylation-K': 8.599608831078214,
 'weight_decay_Hydroxylation-P': 6.542982539715202}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1888.256
[2,     1] loss: 1880.574
[3,     1] loss: 1867.967
[4,     1] loss: 1869.746
[5,     1] loss: 1873.551
[6,     1] loss: 1871.826
[7,     1] loss: 1866.432
[8,     1] loss: 1870.173
[9,     1] loss: 1878.357
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032640106093588897,
 'learning_rate_Hydroxylation-K': 0.006647200701938277,
 'learning_rate_Hydroxylation-P': 0.002015820968475167,
 'log_base': 2.8520298044432875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 27686591,
 'sample_weights': [4.570375726262112, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7852713346250506,
 'weight_decay_Hydroxylation-K': 9.929556204643093,
 'weight_decay_Hydroxylation-P': 3.679751405533378}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.382
[2,     1] loss: 1241.882
[3,     1] loss: 1244.718
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001996781011201893,
 'learning_rate_Hydroxylation-K': 0.00331038833263351,
 'learning_rate_Hydroxylation-P': 0.0074071468486239965,
 'log_base': 2.843403811148367,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 901724788,
 'sample_weights': [1.5929330549471852, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6143936711974225,
 'weight_decay_Hydroxylation-K': 9.047692947794467,
 'weight_decay_Hydroxylation-P': 2.0677005720993575}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.762
[2,     1] loss: 1245.429
[3,     1] loss: 1246.563
[4,     1] loss: 1243.396
[5,     1] loss: 1243.495
[6,     1] loss: 1239.459
[7,     1] loss: 1240.400
[8,     1] loss: 1235.940
[9,     1] loss: 1231.200
[10,     1] loss: 1225.223
[11,     1] loss: 1210.610
[12,     1] loss: 1198.427
[13,     1] loss: 1169.578
[14,     1] loss: 1149.358
[15,     1] loss: 1118.499
[16,     1] loss: 1076.424
[17,     1] loss: 1059.157
[18,     1] loss: 1069.189
[19,     1] loss: 1021.960
[20,     1] loss: 1065.616
[21,     1] loss: 1019.273
[22,     1] loss: 1007.159
[23,     1] loss: 1029.697
[24,     1] loss: 1024.341
[25,     1] loss: 971.478
[26,     1] loss: 990.508
[27,     1] loss: 995.696
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031977939028653007,
 'learning_rate_Hydroxylation-K': 0.0017657343695817451,
 'learning_rate_Hydroxylation-P': 0.003451233791672172,
 'log_base': 2.9170060981271773,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3304377194,
 'sample_weights': [1.5975504082973613, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.142382123259342,
 'weight_decay_Hydroxylation-K': 8.92248886770285,
 'weight_decay_Hydroxylation-P': 2.5140322689672017}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.427
[2,     1] loss: 1236.356
[3,     1] loss: 1232.553
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005485575860847471,
 'learning_rate_Hydroxylation-K': 0.00031617007524694735,
 'learning_rate_Hydroxylation-P': 0.0073095473664691706,
 'log_base': 2.9668455297216556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2952840869,
 'sample_weights': [1.5594143318375995, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.819180261816731,
 'weight_decay_Hydroxylation-K': 6.341268846750519,
 'weight_decay_Hydroxylation-P': 0.9863305183273148}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.799
[2,     1] loss: 1239.459
[3,     1] loss: 1231.181
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00374941722169993,
 'learning_rate_Hydroxylation-K': 0.00259394886002529,
 'learning_rate_Hydroxylation-P': 0.005223528928453183,
 'log_base': 2.9565194633495246,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2619172572,
 'sample_weights': [1.5351211559129567, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.602707594285738,
 'weight_decay_Hydroxylation-K': 8.145271781613218,
 'weight_decay_Hydroxylation-P': 7.4337054780285206}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.003
[2,     1] loss: 1244.387
[3,     1] loss: 1229.580
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009700542049463736,
 'learning_rate_Hydroxylation-K': 0.005986663620184025,
 'learning_rate_Hydroxylation-P': 0.00576528810593811,
 'log_base': 2.295411098919053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3954243585,
 'sample_weights': [1.5400586334429356, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2582119471978612,
 'weight_decay_Hydroxylation-K': 2.430879834278565,
 'weight_decay_Hydroxylation-P': 4.254000661403819}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.595
[2,     1] loss: 1335.125
[3,     1] loss: 1359.619
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033044519432176294,
 'learning_rate_Hydroxylation-K': 0.00039999446000723877,
 'learning_rate_Hydroxylation-P': 0.00887313788443926,
 'log_base': 2.0201924838831085,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4122358993,
 'sample_weights': [2.009169727661611, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.089989388753912,
 'weight_decay_Hydroxylation-K': 7.091476222204397,
 'weight_decay_Hydroxylation-P': 7.6116708573622285}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1415.444
[2,     1] loss: 1408.255
[3,     1] loss: 1407.481
[4,     1] loss: 1409.554
[5,     1] loss: 1406.419
[6,     1] loss: 1406.801
[7,     1] loss: 1402.132
[8,     1] loss: 1399.269
[9,     1] loss: 1394.536
[10,     1] loss: 1385.341
[11,     1] loss: 1369.227
[12,     1] loss: 1334.240
[13,     1] loss: 1313.776
[14,     1] loss: 1290.628
[15,     1] loss: 1230.915
[16,     1] loss: 1222.281
[17,     1] loss: 1210.035
[18,     1] loss: 1154.902
[19,     1] loss: 1196.232
[20,     1] loss: 1201.713
[21,     1] loss: 1203.857
[22,     1] loss: 1206.211
[23,     1] loss: 1185.717
[24,     1] loss: 1127.194
[25,     1] loss: 1115.050
[26,     1] loss: 1138.714
[27,     1] loss: 1073.490
[28,     1] loss: 1094.985
[29,     1] loss: 1074.036
[30,     1] loss: 1087.372
[31,     1] loss: 1043.965
[32,     1] loss: 1002.709
[33,     1] loss: 1015.778
[34,     1] loss: 1015.983
[35,     1] loss: 986.475
[36,     1] loss: 1001.575
[37,     1] loss: 915.223
[38,     1] loss: 974.822
[39,     1] loss: 951.575
[40,     1] loss: 957.069
[41,     1] loss: 1033.122
[42,     1] loss: 1242.545
[43,     1] loss: 908.119
[44,     1] loss: 1080.432
[45,     1] loss: 1112.413
[46,     1] loss: 1002.085
[47,     1] loss: 980.497
[48,     1] loss: 1012.213
[49,     1] loss: 1041.755
[50,     1] loss: 968.558
[51,     1] loss: 989.186
[52,     1] loss: 992.038
[53,     1] loss: 919.805
[54,     1] loss: 894.356
[55,     1] loss: 965.436
[56,     1] loss: 901.906
[57,     1] loss: 957.165
[58,     1] loss: 850.797
[59,     1] loss: 890.555
[60,     1] loss: 925.049
[61,     1] loss: 932.516
[62,     1] loss: 954.658
[63,     1] loss: 899.171
[64,     1] loss: 937.347
[65,     1] loss: 875.474
[66,     1] loss: 891.287
[67,     1] loss: 892.240
[68,     1] loss: 837.704
[69,     1] loss: 865.869
[70,     1] loss: 755.048
[71,     1] loss: 870.093
[72,     1] loss: 742.344
[73,     1] loss: 759.421
[74,     1] loss: 713.279
[75,     1] loss: 786.074
[76,     1] loss: 740.973
[77,     1] loss: 721.796
[78,     1] loss: 790.491
[79,     1] loss: 693.046
Early stopping applied (best metric=0.8104994893074036)
Finished Training
Total time taken: 12.45226263999939
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1412.242
[2,     1] loss: 1408.620
[3,     1] loss: 1408.965
[4,     1] loss: 1406.146
[5,     1] loss: 1407.004
[6,     1] loss: 1405.355
[7,     1] loss: 1400.511
[8,     1] loss: 1395.705
[9,     1] loss: 1380.647
[10,     1] loss: 1365.613
[11,     1] loss: 1339.546
[12,     1] loss: 1303.438
[13,     1] loss: 1291.945
[14,     1] loss: 1258.941
[15,     1] loss: 1255.957
[16,     1] loss: 1220.347
[17,     1] loss: 1201.243
[18,     1] loss: 1216.738
[19,     1] loss: 1153.147
[20,     1] loss: 1118.578
[21,     1] loss: 1139.116
[22,     1] loss: 1113.690
[23,     1] loss: 1106.216
[24,     1] loss: 1096.422
[25,     1] loss: 1090.663
[26,     1] loss: 1090.243
[27,     1] loss: 1118.816
[28,     1] loss: 1040.704
[29,     1] loss: 1100.951
[30,     1] loss: 988.469
[31,     1] loss: 1093.890
[32,     1] loss: 988.168
[33,     1] loss: 1032.390
[34,     1] loss: 1033.406
[35,     1] loss: 1010.643
[36,     1] loss: 1008.024
[37,     1] loss: 983.849
[38,     1] loss: 915.911
[39,     1] loss: 1013.058
[40,     1] loss: 965.552
[41,     1] loss: 899.230
[42,     1] loss: 950.571
[43,     1] loss: 905.140
[44,     1] loss: 890.967
[45,     1] loss: 930.237
[46,     1] loss: 863.243
[47,     1] loss: 881.761
[48,     1] loss: 1068.912
[49,     1] loss: 858.229
[50,     1] loss: 879.829
[51,     1] loss: 837.462
[52,     1] loss: 820.586
[53,     1] loss: 859.591
[54,     1] loss: 813.848
[55,     1] loss: 783.877
[56,     1] loss: 827.224
[57,     1] loss: 791.386
[58,     1] loss: 698.330
[59,     1] loss: 771.267
[60,     1] loss: 827.183
[61,     1] loss: 931.015
[62,     1] loss: 918.826
[63,     1] loss: 772.778
[64,     1] loss: 901.925
[65,     1] loss: 764.974
[66,     1] loss: 919.901
[67,     1] loss: 795.142
[68,     1] loss: 797.585
[69,     1] loss: 718.301
[70,     1] loss: 739.682
[71,     1] loss: 654.390
[72,     1] loss: 795.714
[73,     1] loss: 668.643
[74,     1] loss: 722.993
[75,     1] loss: 763.930
[76,     1] loss: 647.655
[77,     1] loss: 732.672
[78,     1] loss: 688.691
[79,     1] loss: 621.096
[80,     1] loss: 604.665
[81,     1] loss: 653.927
[82,     1] loss: 590.062
[83,     1] loss: 553.245
[84,     1] loss: 548.439
[85,     1] loss: 565.754
[86,     1] loss: 580.913
[87,     1] loss: 954.282
[88,     1] loss: 1614.326
[89,     1] loss: 715.688
[90,     1] loss: 1084.811
[91,     1] loss: 1011.464
Early stopping applied (best metric=0.7770807147026062)
Finished Training
Total time taken: 15.000319480895996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1417.564
[2,     1] loss: 1412.526
[3,     1] loss: 1411.374
[4,     1] loss: 1407.787
[5,     1] loss: 1404.452
[6,     1] loss: 1406.467
[7,     1] loss: 1403.964
[8,     1] loss: 1397.637
[9,     1] loss: 1389.780
[10,     1] loss: 1372.042
[11,     1] loss: 1348.368
[12,     1] loss: 1327.389
[13,     1] loss: 1305.502
[14,     1] loss: 1270.590
[15,     1] loss: 1236.274
[16,     1] loss: 1209.548
[17,     1] loss: 1221.338
[18,     1] loss: 1196.950
[19,     1] loss: 1194.587
[20,     1] loss: 1154.171
[21,     1] loss: 1168.202
[22,     1] loss: 1149.517
[23,     1] loss: 1208.023
[24,     1] loss: 1158.152
[25,     1] loss: 1126.167
[26,     1] loss: 1165.919
[27,     1] loss: 1108.865
[28,     1] loss: 1130.053
[29,     1] loss: 1099.947
[30,     1] loss: 1095.694
[31,     1] loss: 1141.200
[32,     1] loss: 1055.502
[33,     1] loss: 1037.266
[34,     1] loss: 1042.312
[35,     1] loss: 1065.679
[36,     1] loss: 1050.719
[37,     1] loss: 1059.248
[38,     1] loss: 1010.357
[39,     1] loss: 1025.635
[40,     1] loss: 1000.561
[41,     1] loss: 964.011
[42,     1] loss: 992.272
[43,     1] loss: 959.049
[44,     1] loss: 989.616
[45,     1] loss: 956.100
[46,     1] loss: 916.702
[47,     1] loss: 953.058
[48,     1] loss: 998.809
[49,     1] loss: 892.193
[50,     1] loss: 913.198
[51,     1] loss: 882.690
[52,     1] loss: 871.937
[53,     1] loss: 842.996
[54,     1] loss: 818.034
[55,     1] loss: 812.886
[56,     1] loss: 805.564
[57,     1] loss: 1191.250
[58,     1] loss: 984.182
[59,     1] loss: 903.971
[60,     1] loss: 949.085
[61,     1] loss: 1034.545
[62,     1] loss: 970.607
[63,     1] loss: 990.663
[64,     1] loss: 983.389
[65,     1] loss: 895.778
[66,     1] loss: 854.547
[67,     1] loss: 898.746
[68,     1] loss: 803.000
[69,     1] loss: 827.674
[70,     1] loss: 788.853
[71,     1] loss: 769.673
[72,     1] loss: 839.215
[73,     1] loss: 827.519
[74,     1] loss: 756.793
[75,     1] loss: 793.846
[76,     1] loss: 740.674
[77,     1] loss: 747.164
[78,     1] loss: 687.416
[79,     1] loss: 793.565
[80,     1] loss: 782.247
[81,     1] loss: 663.776
[82,     1] loss: 713.625
[83,     1] loss: 756.767
[84,     1] loss: 679.339
[85,     1] loss: 639.491
[86,     1] loss: 738.669
[87,     1] loss: 791.437
[88,     1] loss: 612.602
[89,     1] loss: 762.790
[90,     1] loss: 715.467
[91,     1] loss: 651.984
[92,     1] loss: 688.275
[93,     1] loss: 584.567
[94,     1] loss: 676.023
[95,     1] loss: 645.377
[96,     1] loss: 647.894
[97,     1] loss: 561.866
[98,     1] loss: 573.223
[99,     1] loss: 576.640
[100,     1] loss: 551.068
[101,     1] loss: 522.585
[102,     1] loss: 518.315
[103,     1] loss: 541.497
Early stopping applied (best metric=0.638171374797821)
Finished Training
Total time taken: 13.830290079116821
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.623
[2,     1] loss: 1409.357
[3,     1] loss: 1409.337
[4,     1] loss: 1410.552
[5,     1] loss: 1405.253
[6,     1] loss: 1404.769
[7,     1] loss: 1406.871
[8,     1] loss: 1403.060
[9,     1] loss: 1399.650
[10,     1] loss: 1401.413
[11,     1] loss: 1383.864
[12,     1] loss: 1381.018
[13,     1] loss: 1352.140
[14,     1] loss: 1333.225
[15,     1] loss: 1313.570
[16,     1] loss: 1288.470
[17,     1] loss: 1263.481
[18,     1] loss: 1240.182
[19,     1] loss: 1214.115
[20,     1] loss: 1185.575
[21,     1] loss: 1190.127
[22,     1] loss: 1166.831
[23,     1] loss: 1182.219
[24,     1] loss: 1155.161
[25,     1] loss: 1146.260
[26,     1] loss: 1169.966
[27,     1] loss: 1159.232
[28,     1] loss: 1133.827
[29,     1] loss: 1143.293
[30,     1] loss: 1118.160
[31,     1] loss: 1142.078
[32,     1] loss: 1086.410
[33,     1] loss: 1173.779
[34,     1] loss: 1072.318
[35,     1] loss: 1100.699
[36,     1] loss: 1024.220
[37,     1] loss: 1079.152
[38,     1] loss: 1035.438
[39,     1] loss: 1073.148
[40,     1] loss: 1035.329
[41,     1] loss: 1056.390
[42,     1] loss: 1037.212
[43,     1] loss: 998.250
[44,     1] loss: 984.882
[45,     1] loss: 979.843
[46,     1] loss: 964.496
[47,     1] loss: 959.823
[48,     1] loss: 890.789
[49,     1] loss: 918.285
[50,     1] loss: 950.747
[51,     1] loss: 995.620
[52,     1] loss: 1041.417
[53,     1] loss: 943.325
[54,     1] loss: 925.496
[55,     1] loss: 922.108
[56,     1] loss: 911.869
[57,     1] loss: 881.486
[58,     1] loss: 852.734
[59,     1] loss: 861.792
[60,     1] loss: 911.666
[61,     1] loss: 871.451
[62,     1] loss: 855.453
[63,     1] loss: 937.732
[64,     1] loss: 1018.019
[65,     1] loss: 875.720
[66,     1] loss: 837.132
[67,     1] loss: 869.599
[68,     1] loss: 798.911
[69,     1] loss: 808.309
[70,     1] loss: 783.364
[71,     1] loss: 835.262
[72,     1] loss: 748.840
[73,     1] loss: 773.468
[74,     1] loss: 738.066
[75,     1] loss: 745.708
[76,     1] loss: 818.054
[77,     1] loss: 1040.523
[78,     1] loss: 824.417
[79,     1] loss: 745.823
[80,     1] loss: 835.343
[81,     1] loss: 761.703
[82,     1] loss: 880.043
[83,     1] loss: 807.870
[84,     1] loss: 759.838
[85,     1] loss: 707.864
[86,     1] loss: 854.363
[87,     1] loss: 732.727
[88,     1] loss: 731.685
[89,     1] loss: 758.341
[90,     1] loss: 697.042
Early stopping applied (best metric=0.6346437931060791)
Finished Training
Total time taken: 14.77731466293335
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1420.273
[2,     1] loss: 1409.192
[3,     1] loss: 1409.927
[4,     1] loss: 1413.538
[5,     1] loss: 1411.357
[6,     1] loss: 1410.912
[7,     1] loss: 1410.964
[8,     1] loss: 1408.216
[9,     1] loss: 1404.319
[10,     1] loss: 1401.276
[11,     1] loss: 1399.771
[12,     1] loss: 1383.634
[13,     1] loss: 1374.820
[14,     1] loss: 1335.200
[15,     1] loss: 1329.818
[16,     1] loss: 1286.559
[17,     1] loss: 1249.906
[18,     1] loss: 1227.861
[19,     1] loss: 1220.042
[20,     1] loss: 1155.216
[21,     1] loss: 1192.411
[22,     1] loss: 1135.595
[23,     1] loss: 1149.234
[24,     1] loss: 1142.377
[25,     1] loss: 1138.390
[26,     1] loss: 1078.383
[27,     1] loss: 1111.418
[28,     1] loss: 1092.716
[29,     1] loss: 1094.412
[30,     1] loss: 1079.520
[31,     1] loss: 1064.356
[32,     1] loss: 1058.314
[33,     1] loss: 1055.405
[34,     1] loss: 1041.192
[35,     1] loss: 1003.464
[36,     1] loss: 989.845
[37,     1] loss: 989.416
[38,     1] loss: 1006.327
[39,     1] loss: 965.726
[40,     1] loss: 977.571
[41,     1] loss: 912.718
[42,     1] loss: 954.519
[43,     1] loss: 953.793
[44,     1] loss: 922.626
[45,     1] loss: 966.515
[46,     1] loss: 1017.774
[47,     1] loss: 923.174
[48,     1] loss: 995.234
[49,     1] loss: 887.675
[50,     1] loss: 986.656
[51,     1] loss: 845.318
[52,     1] loss: 849.297
[53,     1] loss: 859.471
[54,     1] loss: 915.073
[55,     1] loss: 878.389
[56,     1] loss: 842.091
[57,     1] loss: 876.288
[58,     1] loss: 825.530
[59,     1] loss: 849.703
[60,     1] loss: 927.683
Early stopping applied (best metric=0.7590532302856445)
Finished Training
Total time taken: 8.876185417175293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1426.018
[2,     1] loss: 1410.231
[3,     1] loss: 1409.833
[4,     1] loss: 1405.483
[5,     1] loss: 1409.786
[6,     1] loss: 1404.963
[7,     1] loss: 1410.684
[8,     1] loss: 1404.673
[9,     1] loss: 1399.867
[10,     1] loss: 1397.620
[11,     1] loss: 1384.909
[12,     1] loss: 1379.488
[13,     1] loss: 1350.177
[14,     1] loss: 1332.106
[15,     1] loss: 1310.314
[16,     1] loss: 1281.118
[17,     1] loss: 1251.723
[18,     1] loss: 1251.970
[19,     1] loss: 1193.228
[20,     1] loss: 1208.117
[21,     1] loss: 1185.090
[22,     1] loss: 1221.794
[23,     1] loss: 1197.172
[24,     1] loss: 1172.972
[25,     1] loss: 1156.589
[26,     1] loss: 1151.623
[27,     1] loss: 1150.699
[28,     1] loss: 1145.172
[29,     1] loss: 1115.234
[30,     1] loss: 1131.638
[31,     1] loss: 1083.697
[32,     1] loss: 1111.299
[33,     1] loss: 1074.966
[34,     1] loss: 1038.854
[35,     1] loss: 1017.260
[36,     1] loss: 1028.563
[37,     1] loss: 1088.111
[38,     1] loss: 1000.619
[39,     1] loss: 1014.385
[40,     1] loss: 1010.048
[41,     1] loss: 924.648
[42,     1] loss: 978.961
[43,     1] loss: 953.986
[44,     1] loss: 948.281
[45,     1] loss: 970.300
[46,     1] loss: 1038.770
[47,     1] loss: 1193.792
[48,     1] loss: 1030.888
[49,     1] loss: 931.278
[50,     1] loss: 977.306
[51,     1] loss: 941.845
[52,     1] loss: 887.035
[53,     1] loss: 923.720
[54,     1] loss: 941.202
[55,     1] loss: 883.787
[56,     1] loss: 854.911
[57,     1] loss: 920.969
[58,     1] loss: 837.395
[59,     1] loss: 812.741
[60,     1] loss: 840.486
[61,     1] loss: 802.173
[62,     1] loss: 770.111
[63,     1] loss: 776.498
[64,     1] loss: 746.474
[65,     1] loss: 788.983
[66,     1] loss: 768.566
[67,     1] loss: 837.180
[68,     1] loss: 1095.196
[69,     1] loss: 1074.889
[70,     1] loss: 919.747
[71,     1] loss: 908.820
[72,     1] loss: 1065.276
[73,     1] loss: 890.468
Early stopping applied (best metric=0.7286703586578369)
Finished Training
Total time taken: 9.817206144332886
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.479
[2,     1] loss: 1406.786
[3,     1] loss: 1411.014
[4,     1] loss: 1409.667
[5,     1] loss: 1402.525
[6,     1] loss: 1407.930
[7,     1] loss: 1407.124
[8,     1] loss: 1406.047
[9,     1] loss: 1396.156
[10,     1] loss: 1387.050
[11,     1] loss: 1375.094
[12,     1] loss: 1346.764
[13,     1] loss: 1318.565
[14,     1] loss: 1285.380
[15,     1] loss: 1268.812
[16,     1] loss: 1241.623
[17,     1] loss: 1216.351
[18,     1] loss: 1224.304
[19,     1] loss: 1246.816
[20,     1] loss: 1191.278
[21,     1] loss: 1194.468
[22,     1] loss: 1152.853
[23,     1] loss: 1161.061
[24,     1] loss: 1122.251
[25,     1] loss: 1126.817
[26,     1] loss: 1121.484
[27,     1] loss: 1130.903
[28,     1] loss: 1130.450
[29,     1] loss: 1094.604
[30,     1] loss: 1141.940
[31,     1] loss: 1069.241
[32,     1] loss: 1053.865
[33,     1] loss: 1037.535
[34,     1] loss: 1083.687
[35,     1] loss: 1014.846
[36,     1] loss: 1072.503
[37,     1] loss: 1016.190
[38,     1] loss: 1050.261
[39,     1] loss: 1022.703
[40,     1] loss: 1025.533
[41,     1] loss: 994.662
[42,     1] loss: 948.098
[43,     1] loss: 926.158
[44,     1] loss: 934.421
[45,     1] loss: 903.915
[46,     1] loss: 913.024
[47,     1] loss: 858.770
[48,     1] loss: 887.968
[49,     1] loss: 913.177
[50,     1] loss: 809.928
Early stopping applied (best metric=0.823575496673584)
Finished Training
Total time taken: 8.336175441741943
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1417.986
[2,     1] loss: 1409.995
[3,     1] loss: 1410.991
[4,     1] loss: 1417.589
[5,     1] loss: 1406.865
[6,     1] loss: 1406.126
[7,     1] loss: 1406.522
[8,     1] loss: 1406.255
[9,     1] loss: 1401.975
[10,     1] loss: 1398.146
[11,     1] loss: 1396.200
[12,     1] loss: 1385.390
[13,     1] loss: 1372.918
[14,     1] loss: 1345.976
[15,     1] loss: 1320.232
[16,     1] loss: 1291.470
[17,     1] loss: 1270.740
[18,     1] loss: 1244.745
[19,     1] loss: 1206.052
[20,     1] loss: 1190.797
[21,     1] loss: 1144.808
[22,     1] loss: 1179.553
[23,     1] loss: 1208.127
[24,     1] loss: 1121.613
[25,     1] loss: 1223.971
[26,     1] loss: 1105.570
[27,     1] loss: 1185.305
[28,     1] loss: 1105.441
[29,     1] loss: 1183.188
[30,     1] loss: 1104.266
[31,     1] loss: 1081.373
[32,     1] loss: 1128.235
[33,     1] loss: 1107.794
[34,     1] loss: 1094.613
[35,     1] loss: 1022.029
[36,     1] loss: 1059.199
[37,     1] loss: 1129.299
[38,     1] loss: 1014.208
[39,     1] loss: 1040.055
[40,     1] loss: 1065.585
[41,     1] loss: 1008.316
[42,     1] loss: 997.068
[43,     1] loss: 1008.283
[44,     1] loss: 964.425
[45,     1] loss: 918.190
[46,     1] loss: 986.006
[47,     1] loss: 940.296
[48,     1] loss: 976.909
[49,     1] loss: 933.134
[50,     1] loss: 905.242
[51,     1] loss: 876.532
[52,     1] loss: 983.119
[53,     1] loss: 883.453
[54,     1] loss: 907.300
[55,     1] loss: 863.463
[56,     1] loss: 910.082
[57,     1] loss: 840.979
[58,     1] loss: 916.766
[59,     1] loss: 817.967
[60,     1] loss: 930.157
[61,     1] loss: 884.545
[62,     1] loss: 798.682
[63,     1] loss: 810.824
[64,     1] loss: 823.535
[65,     1] loss: 802.157
[66,     1] loss: 833.221
[67,     1] loss: 833.601
[68,     1] loss: 767.860
[69,     1] loss: 891.914
[70,     1] loss: 843.500
[71,     1] loss: 759.736
[72,     1] loss: 824.400
[73,     1] loss: 785.793
[74,     1] loss: 720.244
[75,     1] loss: 714.495
[76,     1] loss: 778.782
[77,     1] loss: 727.997
[78,     1] loss: 665.809
[79,     1] loss: 821.368
[80,     1] loss: 755.955
[81,     1] loss: 680.214
[82,     1] loss: 695.943
[83,     1] loss: 700.920
[84,     1] loss: 674.601
[85,     1] loss: 732.719
[86,     1] loss: 664.286
[87,     1] loss: 744.586
[88,     1] loss: 714.691
[89,     1] loss: 663.210
[90,     1] loss: 607.389
Early stopping applied (best metric=0.7448747158050537)
Finished Training
Total time taken: 12.205256223678589
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1412.000
[2,     1] loss: 1409.400
[3,     1] loss: 1406.384
[4,     1] loss: 1406.616
[5,     1] loss: 1406.259
[6,     1] loss: 1405.799
[7,     1] loss: 1391.908
[8,     1] loss: 1383.713
[9,     1] loss: 1360.738
[10,     1] loss: 1329.367
[11,     1] loss: 1287.657
[12,     1] loss: 1247.944
[13,     1] loss: 1204.880
[14,     1] loss: 1158.539
[15,     1] loss: 1201.620
[16,     1] loss: 1125.225
[17,     1] loss: 1162.101
[18,     1] loss: 1114.278
[19,     1] loss: 1196.919
[20,     1] loss: 1117.445
[21,     1] loss: 1062.224
[22,     1] loss: 1107.320
[23,     1] loss: 1088.349
[24,     1] loss: 1157.330
[25,     1] loss: 1045.457
[26,     1] loss: 1066.643
[27,     1] loss: 1040.566
[28,     1] loss: 1025.526
[29,     1] loss: 960.593
[30,     1] loss: 1016.233
[31,     1] loss: 993.376
[32,     1] loss: 970.976
[33,     1] loss: 969.645
[34,     1] loss: 928.766
[35,     1] loss: 932.316
[36,     1] loss: 915.630
[37,     1] loss: 911.887
[38,     1] loss: 920.714
[39,     1] loss: 931.793
[40,     1] loss: 912.141
[41,     1] loss: 894.201
[42,     1] loss: 886.215
[43,     1] loss: 912.555
[44,     1] loss: 926.824
[45,     1] loss: 893.920
[46,     1] loss: 862.592
[47,     1] loss: 816.665
[48,     1] loss: 860.020
[49,     1] loss: 978.055
[50,     1] loss: 1055.013
[51,     1] loss: 795.013
[52,     1] loss: 874.891
[53,     1] loss: 839.653
[54,     1] loss: 880.669
[55,     1] loss: 858.796
[56,     1] loss: 854.400
[57,     1] loss: 905.119
[58,     1] loss: 791.722
[59,     1] loss: 771.522
[60,     1] loss: 784.084
[61,     1] loss: 826.147
[62,     1] loss: 785.780
Early stopping applied (best metric=0.9100269079208374)
Finished Training
Total time taken: 8.4221773147583
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1411.650
[2,     1] loss: 1411.405
[3,     1] loss: 1412.616
[4,     1] loss: 1409.267
[5,     1] loss: 1410.076
[6,     1] loss: 1405.813
[7,     1] loss: 1404.921
[8,     1] loss: 1403.788
[9,     1] loss: 1387.316
[10,     1] loss: 1376.524
[11,     1] loss: 1350.340
[12,     1] loss: 1328.885
[13,     1] loss: 1282.420
[14,     1] loss: 1271.786
[15,     1] loss: 1223.102
[16,     1] loss: 1185.472
[17,     1] loss: 1205.241
[18,     1] loss: 1149.390
[19,     1] loss: 1131.544
[20,     1] loss: 1125.791
[21,     1] loss: 1185.766
[22,     1] loss: 1086.474
[23,     1] loss: 1109.651
[24,     1] loss: 1131.979
[25,     1] loss: 1100.571
[26,     1] loss: 1089.502
[27,     1] loss: 1055.092
[28,     1] loss: 1047.703
[29,     1] loss: 1017.959
[30,     1] loss: 1012.512
[31,     1] loss: 1092.042
[32,     1] loss: 1010.994
[33,     1] loss: 1007.212
[34,     1] loss: 970.249
[35,     1] loss: 971.427
[36,     1] loss: 969.441
[37,     1] loss: 981.795
[38,     1] loss: 948.631
[39,     1] loss: 924.845
[40,     1] loss: 921.473
[41,     1] loss: 928.061
[42,     1] loss: 957.505
[43,     1] loss: 939.007
[44,     1] loss: 912.800
[45,     1] loss: 903.969
[46,     1] loss: 906.600
[47,     1] loss: 832.899
[48,     1] loss: 803.197
[49,     1] loss: 776.662
[50,     1] loss: 773.401
[51,     1] loss: 795.940
[52,     1] loss: 771.387
Early stopping applied (best metric=0.9038342237472534)
Finished Training
Total time taken: 8.663182735443115
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.047
[2,     1] loss: 1412.612
[3,     1] loss: 1407.593
[4,     1] loss: 1404.837
[5,     1] loss: 1399.672
[6,     1] loss: 1399.171
[7,     1] loss: 1398.703
[8,     1] loss: 1380.702
[9,     1] loss: 1365.146
[10,     1] loss: 1329.772
[11,     1] loss: 1299.805
[12,     1] loss: 1244.131
[13,     1] loss: 1210.663
[14,     1] loss: 1178.076
[15,     1] loss: 1201.620
[16,     1] loss: 1124.830
[17,     1] loss: 1170.188
[18,     1] loss: 1171.066
[19,     1] loss: 1154.288
[20,     1] loss: 1091.381
[21,     1] loss: 1153.245
[22,     1] loss: 1051.200
[23,     1] loss: 1061.794
[24,     1] loss: 1093.324
[25,     1] loss: 1065.812
[26,     1] loss: 1086.545
[27,     1] loss: 1046.211
[28,     1] loss: 1017.144
[29,     1] loss: 1014.022
[30,     1] loss: 949.091
[31,     1] loss: 948.669
[32,     1] loss: 944.116
[33,     1] loss: 1004.116
[34,     1] loss: 931.897
[35,     1] loss: 960.397
[36,     1] loss: 1076.580
[37,     1] loss: 884.336
[38,     1] loss: 1047.070
[39,     1] loss: 909.118
[40,     1] loss: 1005.162
[41,     1] loss: 933.712
[42,     1] loss: 953.961
[43,     1] loss: 922.597
[44,     1] loss: 865.400
[45,     1] loss: 888.957
[46,     1] loss: 893.339
[47,     1] loss: 912.585
[48,     1] loss: 836.016
[49,     1] loss: 867.108
[50,     1] loss: 811.205
[51,     1] loss: 848.041
[52,     1] loss: 826.516
[53,     1] loss: 807.216
[54,     1] loss: 782.047
[55,     1] loss: 875.534
[56,     1] loss: 860.372
[57,     1] loss: 799.901
[58,     1] loss: 858.012
[59,     1] loss: 846.322
[60,     1] loss: 767.032
[61,     1] loss: 851.842
[62,     1] loss: 764.364
[63,     1] loss: 747.606
[64,     1] loss: 774.220
[65,     1] loss: 684.450
[66,     1] loss: 844.420
[67,     1] loss: 782.189
[68,     1] loss: 692.690
[69,     1] loss: 742.117
[70,     1] loss: 655.114
[71,     1] loss: 666.905
[72,     1] loss: 757.103
[73,     1] loss: 1007.270
[74,     1] loss: 1020.425
[75,     1] loss: 716.046
[76,     1] loss: 865.375
[77,     1] loss: 802.586
[78,     1] loss: 751.593
[79,     1] loss: 910.921
[80,     1] loss: 762.200
[81,     1] loss: 785.147
[82,     1] loss: 730.818
[83,     1] loss: 731.073
[84,     1] loss: 694.596
[85,     1] loss: 706.651
[86,     1] loss: 733.159
[87,     1] loss: 657.819
[88,     1] loss: 654.316
[89,     1] loss: 574.721
[90,     1] loss: 588.925
[91,     1] loss: 610.857
[92,     1] loss: 665.726
[93,     1] loss: 606.714
[94,     1] loss: 588.565
[95,     1] loss: 587.668
[96,     1] loss: 671.669
Early stopping applied (best metric=0.925258994102478)
Finished Training
Total time taken: 12.916274547576904
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1414.757
[2,     1] loss: 1407.474
[3,     1] loss: 1410.712
[4,     1] loss: 1411.056
[5,     1] loss: 1407.086
[6,     1] loss: 1404.254
[7,     1] loss: 1401.319
[8,     1] loss: 1393.938
[9,     1] loss: 1374.020
[10,     1] loss: 1361.686
[11,     1] loss: 1331.581
[12,     1] loss: 1278.538
[13,     1] loss: 1263.566
[14,     1] loss: 1246.461
[15,     1] loss: 1213.989
[16,     1] loss: 1179.238
[17,     1] loss: 1170.132
[18,     1] loss: 1180.699
[19,     1] loss: 1169.789
[20,     1] loss: 1129.755
[21,     1] loss: 1174.485
[22,     1] loss: 1130.182
[23,     1] loss: 1146.820
[24,     1] loss: 1103.021
[25,     1] loss: 1113.652
[26,     1] loss: 1089.200
[27,     1] loss: 1086.225
[28,     1] loss: 1026.995
[29,     1] loss: 1045.006
[30,     1] loss: 1025.427
[31,     1] loss: 1022.446
[32,     1] loss: 995.417
[33,     1] loss: 925.159
[34,     1] loss: 1005.451
[35,     1] loss: 1015.234
[36,     1] loss: 991.166
[37,     1] loss: 947.097
[38,     1] loss: 1037.343
[39,     1] loss: 955.841
[40,     1] loss: 903.368
[41,     1] loss: 974.603
[42,     1] loss: 902.998
[43,     1] loss: 927.759
[44,     1] loss: 900.975
[45,     1] loss: 895.847
[46,     1] loss: 1005.348
[47,     1] loss: 959.798
[48,     1] loss: 835.305
[49,     1] loss: 860.927
[50,     1] loss: 838.331
[51,     1] loss: 813.783
[52,     1] loss: 833.811
Early stopping applied (best metric=0.8663464784622192)
Finished Training
Total time taken: 8.27117371559143
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1409.968
[2,     1] loss: 1405.811
[3,     1] loss: 1408.925
[4,     1] loss: 1408.316
[5,     1] loss: 1408.640
[6,     1] loss: 1405.461
[7,     1] loss: 1402.740
[8,     1] loss: 1396.101
[9,     1] loss: 1384.400
[10,     1] loss: 1369.250
[11,     1] loss: 1340.550
[12,     1] loss: 1304.740
[13,     1] loss: 1263.598
[14,     1] loss: 1246.724
[15,     1] loss: 1194.550
[16,     1] loss: 1220.264
[17,     1] loss: 1178.780
[18,     1] loss: 1175.518
[19,     1] loss: 1176.966
[20,     1] loss: 1185.307
[21,     1] loss: 1130.927
[22,     1] loss: 1163.148
[23,     1] loss: 1124.669
[24,     1] loss: 1106.726
[25,     1] loss: 1099.715
[26,     1] loss: 1133.480
[27,     1] loss: 1067.504
[28,     1] loss: 1073.172
[29,     1] loss: 1071.170
[30,     1] loss: 1066.587
[31,     1] loss: 1027.904
[32,     1] loss: 988.355
[33,     1] loss: 979.398
[34,     1] loss: 972.526
[35,     1] loss: 973.768
[36,     1] loss: 971.178
[37,     1] loss: 958.116
[38,     1] loss: 866.518
[39,     1] loss: 892.166
[40,     1] loss: 967.582
[41,     1] loss: 921.889
[42,     1] loss: 982.297
[43,     1] loss: 931.378
[44,     1] loss: 861.589
[45,     1] loss: 900.764
[46,     1] loss: 888.622
[47,     1] loss: 886.537
[48,     1] loss: 823.279
[49,     1] loss: 863.354
[50,     1] loss: 837.824
[51,     1] loss: 792.872
[52,     1] loss: 827.038
[53,     1] loss: 827.408
[54,     1] loss: 783.231
[55,     1] loss: 758.026
[56,     1] loss: 729.053
[57,     1] loss: 691.191
Early stopping applied (best metric=0.868902325630188)
Finished Training
Total time taken: 9.430198192596436
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1424.044
[2,     1] loss: 1413.958
[3,     1] loss: 1408.788
[4,     1] loss: 1405.233
[5,     1] loss: 1404.853
[6,     1] loss: 1405.857
[7,     1] loss: 1403.814
[8,     1] loss: 1399.173
[9,     1] loss: 1394.798
[10,     1] loss: 1384.649
[11,     1] loss: 1367.924
[12,     1] loss: 1342.536
[13,     1] loss: 1306.200
[14,     1] loss: 1288.611
[15,     1] loss: 1249.424
[16,     1] loss: 1223.755
[17,     1] loss: 1185.838
[18,     1] loss: 1196.534
[19,     1] loss: 1197.094
[20,     1] loss: 1149.283
[21,     1] loss: 1157.345
[22,     1] loss: 1153.110
[23,     1] loss: 1126.057
[24,     1] loss: 1126.167
[25,     1] loss: 1137.227
[26,     1] loss: 1098.048
[27,     1] loss: 1073.852
[28,     1] loss: 1091.313
[29,     1] loss: 1083.112
[30,     1] loss: 1101.415
[31,     1] loss: 1051.115
[32,     1] loss: 1054.938
[33,     1] loss: 1001.881
[34,     1] loss: 1061.596
[35,     1] loss: 1086.067
[36,     1] loss: 935.684
[37,     1] loss: 1015.967
[38,     1] loss: 1002.307
[39,     1] loss: 967.604
[40,     1] loss: 988.518
[41,     1] loss: 1004.749
[42,     1] loss: 963.012
[43,     1] loss: 934.432
[44,     1] loss: 999.300
[45,     1] loss: 904.214
[46,     1] loss: 921.843
[47,     1] loss: 867.078
[48,     1] loss: 912.228
[49,     1] loss: 956.888
[50,     1] loss: 898.558
[51,     1] loss: 798.872
[52,     1] loss: 970.935
[53,     1] loss: 895.607
[54,     1] loss: 875.390
[55,     1] loss: 937.389
[56,     1] loss: 849.341
[57,     1] loss: 905.854
[58,     1] loss: 900.338
[59,     1] loss: 820.043
[60,     1] loss: 797.810
[61,     1] loss: 804.433
[62,     1] loss: 841.293
[63,     1] loss: 860.306
[64,     1] loss: 762.920
[65,     1] loss: 715.519
[66,     1] loss: 838.193
[67,     1] loss: 826.457
[68,     1] loss: 715.875
[69,     1] loss: 776.630
[70,     1] loss: 710.224
[71,     1] loss: 699.543
[72,     1] loss: 786.816
[73,     1] loss: 721.606
[74,     1] loss: 714.158
[75,     1] loss: 840.508
[76,     1] loss: 740.365
[77,     1] loss: 683.866
[78,     1] loss: 717.266
[79,     1] loss: 637.954
[80,     1] loss: 660.701
[81,     1] loss: 683.306
[82,     1] loss: 774.069
[83,     1] loss: 847.579
[84,     1] loss: 685.531
[85,     1] loss: 618.006
[86,     1] loss: 641.624
[87,     1] loss: 644.041
[88,     1] loss: 623.038
[89,     1] loss: 605.778
[90,     1] loss: 704.586
[91,     1] loss: 584.216
[92,     1] loss: 624.896
[93,     1] loss: 657.432
[94,     1] loss: 598.457
[95,     1] loss: 560.357
[96,     1] loss: 529.554
[97,     1] loss: 505.029
[98,     1] loss: 527.146
[99,     1] loss: 484.694
[100,     1] loss: 458.622
Early stopping applied (best metric=0.7370310425758362)
Finished Training
Total time taken: 13.559288024902344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1416.234
[2,     1] loss: 1413.132
[3,     1] loss: 1412.375
[4,     1] loss: 1410.866
[5,     1] loss: 1407.254
[6,     1] loss: 1403.225
[7,     1] loss: 1407.698
[8,     1] loss: 1392.166
[9,     1] loss: 1375.507
[10,     1] loss: 1349.834
[11,     1] loss: 1315.468
[12,     1] loss: 1299.897
[13,     1] loss: 1253.960
[14,     1] loss: 1201.983
[15,     1] loss: 1180.928
[16,     1] loss: 1168.214
[17,     1] loss: 1099.238
[18,     1] loss: 1129.757
[19,     1] loss: 1147.222
[20,     1] loss: 1091.706
[21,     1] loss: 1065.607
[22,     1] loss: 1105.591
[23,     1] loss: 1094.166
[24,     1] loss: 1046.126
[25,     1] loss: 1059.941
[26,     1] loss: 1060.703
[27,     1] loss: 1040.899
[28,     1] loss: 1044.415
[29,     1] loss: 1072.458
[30,     1] loss: 1031.094
[31,     1] loss: 1045.122
[32,     1] loss: 1065.456
[33,     1] loss: 1038.736
[34,     1] loss: 1008.554
[35,     1] loss: 957.392
[36,     1] loss: 962.035
[37,     1] loss: 981.796
[38,     1] loss: 942.947
[39,     1] loss: 911.598
[40,     1] loss: 952.238
[41,     1] loss: 880.716
[42,     1] loss: 896.361
[43,     1] loss: 932.146
[44,     1] loss: 884.110
[45,     1] loss: 898.142
[46,     1] loss: 1157.141
[47,     1] loss: 1295.908
[48,     1] loss: 940.266
[49,     1] loss: 1035.027
[50,     1] loss: 1121.747
[51,     1] loss: 1022.865
[52,     1] loss: 985.317
[53,     1] loss: 1035.626
[54,     1] loss: 1014.423
[55,     1] loss: 1006.859
[56,     1] loss: 953.301
[57,     1] loss: 920.626
[58,     1] loss: 955.971
[59,     1] loss: 964.417
[60,     1] loss: 897.552
[61,     1] loss: 964.408
[62,     1] loss: 934.261
[63,     1] loss: 847.817
[64,     1] loss: 839.012
[65,     1] loss: 912.716
[66,     1] loss: 811.604
[67,     1] loss: 886.528
[68,     1] loss: 793.695
[69,     1] loss: 785.724
[70,     1] loss: 857.668
[71,     1] loss: 756.291
[72,     1] loss: 881.197
[73,     1] loss: 780.575
[74,     1] loss: 784.237
[75,     1] loss: 823.868
[76,     1] loss: 754.713
[77,     1] loss: 810.207
[78,     1] loss: 812.700
[79,     1] loss: 718.004
[80,     1] loss: 719.437
[81,     1] loss: 701.081
[82,     1] loss: 694.260
[83,     1] loss: 694.997
Early stopping applied (best metric=0.8609083294868469)
Finished Training
Total time taken: 13.68328857421875
{'Hydroxylation-K Validation Accuracy': 0.7703605200945627, 'Hydroxylation-K Validation Sensitivity': 0.7088888888888889, 'Hydroxylation-K Validation Specificity': 0.7859649122807018, 'Hydroxylation-K Validation Precision': 0.4588376764847353, 'Hydroxylation-K AUC ROC': 0.8226900584795321, 'Hydroxylation-K AUC PR': 0.6109005589615641, 'Hydroxylation-K MCC': 0.4302969014288942, 'Hydroxylation-K F1': 0.5507268609007739, 'Validation Loss (Hydroxylation-K)': 0.4126959284146627, 'Hydroxylation-P Validation Accuracy': 0.7812674821921053, 'Hydroxylation-P Validation Sensitivity': 0.7631746031746032, 'Hydroxylation-P Validation Specificity': 0.7851064891017009, 'Hydroxylation-P Validation Precision': 0.4375831362822629, 'Hydroxylation-P AUC ROC': 0.8338491628050958, 'Hydroxylation-P AUC PR': 0.5615594768107007, 'Hydroxylation-P MCC': 0.4544871500680869, 'Hydroxylation-P F1': 0.553111956212605, 'Validation Loss (Hydroxylation-P)': 0.3865625739097595, 'Validation Loss (total)': 0.7992584983507792, 'TimeToTrain': 11.349372879664104}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006860808821949922,
 'learning_rate_Hydroxylation-K': 0.0051874410616723435,
 'learning_rate_Hydroxylation-P': 0.004142840859677606,
 'log_base': 1.4243051707905838,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4012451833,
 'sample_weights': [2.3758513274847726, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.570444202252759,
 'weight_decay_Hydroxylation-K': 5.751521250654986,
 'weight_decay_Hydroxylation-P': 8.048135265125183}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1905.663
[2,     1] loss: 1905.531
[3,     1] loss: 1909.200
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009567214124111822,
 'learning_rate_Hydroxylation-K': 0.0004337881417201724,
 'learning_rate_Hydroxylation-P': 0.004375648695456953,
 'log_base': 1.0109728135355351,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 987202254,
 'sample_weights': [4.720153293078507, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.039682771077638,
 'weight_decay_Hydroxylation-K': 8.53713011413213,
 'weight_decay_Hydroxylation-P': 5.053229825032663}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49650.727
Exploding loss, terminate run (best metric=1.1060357093811035)
Finished Training
Total time taken: 0.20600461959838867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49634.910
Exploding loss, terminate run (best metric=1.0915870666503906)
Finished Training
Total time taken: 0.19800329208374023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49664.938
Exploding loss, terminate run (best metric=1.0916780233383179)
Finished Training
Total time taken: 0.21900463104248047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49852.219
Exploding loss, terminate run (best metric=1.0753142833709717)
Finished Training
Total time taken: 0.22100567817687988
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49727.234
Exploding loss, terminate run (best metric=1.080864667892456)
Finished Training
Total time taken: 0.19700360298156738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49650.805
Exploding loss, terminate run (best metric=1.1094658374786377)
Finished Training
Total time taken: 0.22800421714782715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49734.922
Exploding loss, terminate run (best metric=1.1151421070098877)
Finished Training
Total time taken: 0.23900485038757324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49630.754
Exploding loss, terminate run (best metric=1.093667984008789)
Finished Training
Total time taken: 0.20000433921813965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49744.227
Exploding loss, terminate run (best metric=1.0739214420318604)
Finished Training
Total time taken: 0.2200028896331787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49963.758
Exploding loss, terminate run (best metric=1.0993266105651855)
Finished Training
Total time taken: 0.21700501441955566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49647.879
Exploding loss, terminate run (best metric=1.0960520505905151)
Finished Training
Total time taken: 0.2010033130645752
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49981.641
Exploding loss, terminate run (best metric=1.0951004028320312)
Finished Training
Total time taken: 0.2230057716369629
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49910.781
Exploding loss, terminate run (best metric=1.0968632698059082)
Finished Training
Total time taken: 0.22200441360473633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49831.133
Exploding loss, terminate run (best metric=1.0816423892974854)
Finished Training
Total time taken: 0.19700336456298828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49689.227
Exploding loss, terminate run (best metric=1.0761687755584717)
Finished Training
Total time taken: 0.22200465202331543
{'Hydroxylation-K Validation Accuracy': 0.5583333333333333, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6003508771929824, 'Hydroxylation-K AUC PR': 0.3024941387848709, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.13325123152709362, 'Validation Loss (Hydroxylation-K)': 0.5582747459411621, 'Hydroxylation-P Validation Accuracy': 0.5644901950831599, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5648571852136868, 'Hydroxylation-P AUC PR': 0.2544756592879771, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12013967512621766, 'Validation Loss (Hydroxylation-P)': 0.5339139699935913, 'Validation Loss (total)': 1.0921887079874675, 'TimeToTrain': 0.21400430997212727}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005189987531071709,
 'learning_rate_Hydroxylation-K': 0.0014862785589244937,
 'learning_rate_Hydroxylation-P': 0.004284959546131371,
 'log_base': 1.8893995768351548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2318590677,
 'sample_weights': [153.09026251031145, 19.096520999106122],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.340028289681989,
 'weight_decay_Hydroxylation-K': 6.895432360966661,
 'weight_decay_Hydroxylation-P': 0.1337570320065401}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1460.734
[2,     1] loss: 1468.079
[3,     1] loss: 1456.867
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 8.029047950624022e-05,
 'learning_rate_Hydroxylation-K': 0.006083215719254605,
 'learning_rate_Hydroxylation-P': 0.004388464094010504,
 'log_base': 2.422479080249084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 570501472,
 'sample_weights': [2.6238417056140673, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.95570652692602,
 'weight_decay_Hydroxylation-K': 3.3415553283106587,
 'weight_decay_Hydroxylation-P': 5.450602080665064}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.408
[2,     1] loss: 1309.767
[3,     1] loss: 1309.567
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004995264859582523,
 'learning_rate_Hydroxylation-K': 0.0007911933569006712,
 'learning_rate_Hydroxylation-P': 0.004437771739198818,
 'log_base': 2.4084988024480745,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2037119056,
 'sample_weights': [1.8868211110992448, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.0225361905955905,
 'weight_decay_Hydroxylation-K': 0.2404354760594014,
 'weight_decay_Hydroxylation-P': 0.24212893576710057}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1313.679
[2,     1] loss: 1316.823
[3,     1] loss: 1305.946
[4,     1] loss: 1310.460
[5,     1] loss: 1307.252
[6,     1] loss: 1306.929
[7,     1] loss: 1305.182
[8,     1] loss: 1306.551
[9,     1] loss: 1305.251
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021005933323123994,
 'learning_rate_Hydroxylation-K': 0.00303097179613894,
 'learning_rate_Hydroxylation-P': 0.0030275954318447085,
 'log_base': 2.823773091815303,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1746473305,
 'sample_weights': [1.8992448412158662, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.784066377047893,
 'weight_decay_Hydroxylation-K': 9.878783475318635,
 'weight_decay_Hydroxylation-P': 4.561179747728973}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.762
[2,     1] loss: 1247.822
[3,     1] loss: 1247.347
[4,     1] loss: 1244.831
[5,     1] loss: 1249.451
[6,     1] loss: 1244.584
[7,     1] loss: 1240.897
[8,     1] loss: 1235.170
[9,     1] loss: 1233.477
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008511359786701115,
 'learning_rate_Hydroxylation-K': 0.0004492189984476005,
 'learning_rate_Hydroxylation-P': 0.004625573230561718,
 'log_base': 1.240970807105903,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789455687,
 'sample_weights': [1.6082121325960754, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.53153536558827,
 'weight_decay_Hydroxylation-K': 8.015158175339023,
 'weight_decay_Hydroxylation-P': 4.138335833726914}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2560.204
[2,     1] loss: 2534.081
[3,     1] loss: 2550.977
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009460618724852182,
 'learning_rate_Hydroxylation-K': 0.003914572578868325,
 'learning_rate_Hydroxylation-P': 0.006137765850929492,
 'log_base': 1.115230711240091,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1708530496,
 'sample_weights': [7.732698845739088, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.038710410079528,
 'weight_decay_Hydroxylation-K': 8.59488716293064,
 'weight_decay_Hydroxylation-P': 4.136253418878659}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5011.526
[2,     1] loss: 4951.782
[3,     1] loss: 4960.541
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009666010998303661,
 'learning_rate_Hydroxylation-K': 0.002635471744256637,
 'learning_rate_Hydroxylation-P': 0.0045408696511219995,
 'log_base': 1.4527001893031537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1133499567,
 'sample_weights': [15.30738361203718, 1.9134966642403362],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.530660635817783,
 'weight_decay_Hydroxylation-K': 9.942297332590833,
 'weight_decay_Hydroxylation-P': 8.974510250494015}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1845.125
[2,     1] loss: 1884.787
[3,     1] loss: 1858.188
[4,     1] loss: 1845.355
[5,     1] loss: 1850.576
[6,     1] loss: 1854.832
[7,     1] loss: 1846.438
[8,     1] loss: 1851.107
[9,     1] loss: 1841.945
[10,     1] loss: 1853.377
[11,     1] loss: 1856.371
[12,     1] loss: 1848.489
[13,     1] loss: 1848.663
[14,     1] loss: 1847.853
[15,     1] loss: 1846.853
[16,     1] loss: 1843.082
[17,     1] loss: 1840.242
[18,     1] loss: 1827.414
[19,     1] loss: 1803.159
[20,     1] loss: 1768.753
[21,     1] loss: 1741.869
[22,     1] loss: 1736.647
[23,     1] loss: 1622.150
[24,     1] loss: 1736.877
[25,     1] loss: 1613.110
[26,     1] loss: 1682.765
[27,     1] loss: 1587.369
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0077922703159947395,
 'learning_rate_Hydroxylation-K': 0.003052776966588671,
 'learning_rate_Hydroxylation-P': 0.0021439694476207746,
 'log_base': 1.0716771210953213,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3250229093,
 'sample_weights': [4.470636700369432, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.916274524169763,
 'weight_decay_Hydroxylation-K': 5.624506606826575,
 'weight_decay_Hydroxylation-P': 3.1225654605938873}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7832.264
[2,     1] loss: 7847.424
[3,     1] loss: 7782.311
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00917514937886233,
 'learning_rate_Hydroxylation-K': 0.0006432039889337005,
 'learning_rate_Hydroxylation-P': 0.009674938146220615,
 'log_base': 2.3296635446860603,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1552589492,
 'sample_weights': [24.11624969743495, 3.014647344026901],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8232144481984047,
 'weight_decay_Hydroxylation-K': 2.1282279037461116,
 'weight_decay_Hydroxylation-P': 4.757405545782036}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.396
[2,     1] loss: 1333.745
[3,     1] loss: 1328.152
[4,     1] loss: 1322.734
[5,     1] loss: 1324.942
[6,     1] loss: 1322.091
[7,     1] loss: 1321.096
[8,     1] loss: 1324.726
[9,     1] loss: 1322.289
[10,     1] loss: 1322.777
[11,     1] loss: 1321.737
[12,     1] loss: 1319.065
[13,     1] loss: 1317.919
[14,     1] loss: 1313.993
[15,     1] loss: 1306.867
[16,     1] loss: 1289.465
[17,     1] loss: 1264.407
[18,     1] loss: 1239.906
[19,     1] loss: 1214.054
[20,     1] loss: 1138.804
[21,     1] loss: 1110.312
[22,     1] loss: 1087.829
[23,     1] loss: 1085.149
[24,     1] loss: 1070.546
[25,     1] loss: 1119.847
[26,     1] loss: 1068.669
[27,     1] loss: 1074.194
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009545033025050036,
 'learning_rate_Hydroxylation-K': 0.0016067959766146419,
 'learning_rate_Hydroxylation-P': 0.0031734996085248613,
 'log_base': 1.460535612290908,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4164988958,
 'sample_weights': [1.9739813842495533, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.058077841717862,
 'weight_decay_Hydroxylation-K': 7.896517810324034,
 'weight_decay_Hydroxylation-P': 5.854651593987741}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1844.340
[2,     1] loss: 1864.484
[3,     1] loss: 1843.021
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061572882051333456,
 'learning_rate_Hydroxylation-K': 0.002480317647153568,
 'learning_rate_Hydroxylation-P': 0.0026505032069841328,
 'log_base': 2.950729084820446,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1614509050,
 'sample_weights': [4.407151344164229, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.280171186556037,
 'weight_decay_Hydroxylation-K': 9.011922434600743,
 'weight_decay_Hydroxylation-P': 3.340362812732066}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.907
[2,     1] loss: 1234.617
[3,     1] loss: 1235.304
[4,     1] loss: 1234.627
[5,     1] loss: 1231.302
[6,     1] loss: 1233.607
[7,     1] loss: 1231.971
[8,     1] loss: 1228.495
[9,     1] loss: 1222.865
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009175877542293147,
 'learning_rate_Hydroxylation-K': 2.013138360078433e-05,
 'learning_rate_Hydroxylation-P': 0.001931532888375808,
 'log_base': 1.110560244372896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3524566512,
 'sample_weights': [1.5428488689065152, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9284532435837125,
 'weight_decay_Hydroxylation-K': 7.969270443769137,
 'weight_decay_Hydroxylation-P': 4.941474322726192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5158.535
[2,     1] loss: 5175.534
[3,     1] loss: 5177.636
[4,     1] loss: 5169.229
[5,     1] loss: 5169.440
[6,     1] loss: 5159.948
[7,     1] loss: 5159.127
[8,     1] loss: 5181.308
[9,     1] loss: 5172.745
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00291239652889598,
 'learning_rate_Hydroxylation-K': 0.00611684617325642,
 'learning_rate_Hydroxylation-P': 0.00214720067198755,
 'log_base': 2.3161041858987614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3929694513,
 'sample_weights': [15.919985829101407, 1.9900748913605377],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.81241724295435,
 'weight_decay_Hydroxylation-K': 9.84921968590374,
 'weight_decay_Hydroxylation-P': 6.923114492204446}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1328.333
[2,     1] loss: 1327.319
[3,     1] loss: 1327.684
[4,     1] loss: 1325.863
[5,     1] loss: 1322.739
[6,     1] loss: 1315.907
[7,     1] loss: 1319.178
[8,     1] loss: 1303.095
[9,     1] loss: 1276.731
[10,     1] loss: 1251.179
[11,     1] loss: 1237.569
[12,     1] loss: 1215.255
[13,     1] loss: 1175.035
[14,     1] loss: 1182.211
[15,     1] loss: 1120.090
[16,     1] loss: 1143.553
[17,     1] loss: 1107.817
[18,     1] loss: 1072.192
[19,     1] loss: 1102.015
[20,     1] loss: 1087.564
[21,     1] loss: 1107.852
[22,     1] loss: 1095.326
[23,     1] loss: 1031.917
[24,     1] loss: 1068.232
[25,     1] loss: 1016.948
[26,     1] loss: 999.737
[27,     1] loss: 950.219
[28,     1] loss: 986.144
[29,     1] loss: 984.257
[30,     1] loss: 1015.826
[31,     1] loss: 1010.292
[32,     1] loss: 989.310
[33,     1] loss: 978.029
[34,     1] loss: 967.314
[35,     1] loss: 958.427
[36,     1] loss: 950.791
[37,     1] loss: 943.606
[38,     1] loss: 932.520
[39,     1] loss: 894.749
[40,     1] loss: 935.711
[41,     1] loss: 907.609
[42,     1] loss: 872.772
[43,     1] loss: 823.371
[44,     1] loss: 870.661
[45,     1] loss: 867.680
[46,     1] loss: 859.074
[47,     1] loss: 833.046
[48,     1] loss: 855.213
[49,     1] loss: 782.617
[50,     1] loss: 821.475
[51,     1] loss: 814.601
[52,     1] loss: 863.472
[53,     1] loss: 835.292
[54,     1] loss: 815.087
[55,     1] loss: 783.451
[56,     1] loss: 762.356
[57,     1] loss: 856.266
[58,     1] loss: 714.319
[59,     1] loss: 746.102
[60,     1] loss: 767.871
[61,     1] loss: 760.862
[62,     1] loss: 809.103
[63,     1] loss: 703.197
[64,     1] loss: 702.063
[65,     1] loss: 741.193
Early stopping applied (best metric=0.7900939583778381)
Finished Training
Total time taken: 8.773184776306152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.439
[2,     1] loss: 1325.345
[3,     1] loss: 1329.748
[4,     1] loss: 1324.579
[5,     1] loss: 1330.889
[6,     1] loss: 1327.915
[7,     1] loss: 1327.852
[8,     1] loss: 1320.691
[9,     1] loss: 1321.594
[10,     1] loss: 1321.066
[11,     1] loss: 1311.045
[12,     1] loss: 1309.003
[13,     1] loss: 1303.045
[14,     1] loss: 1290.327
[15,     1] loss: 1258.556
[16,     1] loss: 1232.722
[17,     1] loss: 1208.658
[18,     1] loss: 1169.503
[19,     1] loss: 1158.274
[20,     1] loss: 1150.276
[21,     1] loss: 1135.249
[22,     1] loss: 1123.200
[23,     1] loss: 1083.901
[24,     1] loss: 1098.248
[25,     1] loss: 1141.631
[26,     1] loss: 1116.583
[27,     1] loss: 1050.029
[28,     1] loss: 1064.913
[29,     1] loss: 1012.128
[30,     1] loss: 1077.929
[31,     1] loss: 1033.749
[32,     1] loss: 978.477
[33,     1] loss: 985.401
[34,     1] loss: 994.448
[35,     1] loss: 977.321
[36,     1] loss: 877.679
[37,     1] loss: 979.843
[38,     1] loss: 884.689
[39,     1] loss: 952.133
[40,     1] loss: 963.010
[41,     1] loss: 977.358
[42,     1] loss: 993.434
[43,     1] loss: 836.405
[44,     1] loss: 959.449
[45,     1] loss: 882.887
[46,     1] loss: 861.473
[47,     1] loss: 870.032
[48,     1] loss: 845.749
[49,     1] loss: 847.632
[50,     1] loss: 889.343
[51,     1] loss: 815.290
[52,     1] loss: 851.984
[53,     1] loss: 861.879
[54,     1] loss: 818.409
[55,     1] loss: 757.968
[56,     1] loss: 789.234
[57,     1] loss: 769.586
[58,     1] loss: 783.900
[59,     1] loss: 831.691
[60,     1] loss: 719.000
Early stopping applied (best metric=0.8314578533172607)
Finished Training
Total time taken: 10.092211961746216
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1332.322
[2,     1] loss: 1329.021
[3,     1] loss: 1325.090
[4,     1] loss: 1326.762
[5,     1] loss: 1324.829
[6,     1] loss: 1322.585
[7,     1] loss: 1323.427
[8,     1] loss: 1311.688
[9,     1] loss: 1307.983
[10,     1] loss: 1295.272
[11,     1] loss: 1278.696
[12,     1] loss: 1244.756
[13,     1] loss: 1212.664
[14,     1] loss: 1172.741
[15,     1] loss: 1135.110
[16,     1] loss: 1146.896
[17,     1] loss: 1107.822
[18,     1] loss: 1072.353
[19,     1] loss: 1035.872
[20,     1] loss: 1098.810
[21,     1] loss: 1048.882
[22,     1] loss: 1057.939
[23,     1] loss: 1037.708
[24,     1] loss: 1073.123
[25,     1] loss: 1000.079
[26,     1] loss: 1009.301
[27,     1] loss: 1010.159
[28,     1] loss: 1007.144
[29,     1] loss: 995.312
[30,     1] loss: 955.550
[31,     1] loss: 960.676
[32,     1] loss: 1006.309
[33,     1] loss: 915.696
[34,     1] loss: 934.248
[35,     1] loss: 931.998
[36,     1] loss: 899.426
[37,     1] loss: 926.535
[38,     1] loss: 861.187
[39,     1] loss: 892.478
[40,     1] loss: 843.502
[41,     1] loss: 883.372
[42,     1] loss: 808.056
[43,     1] loss: 844.972
[44,     1] loss: 834.019
[45,     1] loss: 903.527
[46,     1] loss: 893.254
[47,     1] loss: 833.991
[48,     1] loss: 733.979
[49,     1] loss: 777.982
[50,     1] loss: 769.711
[51,     1] loss: 753.352
[52,     1] loss: 761.509
[53,     1] loss: 750.317
[54,     1] loss: 743.137
Early stopping applied (best metric=0.8593463897705078)
Finished Training
Total time taken: 7.714162349700928
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.522
[2,     1] loss: 1333.230
[3,     1] loss: 1328.579
[4,     1] loss: 1328.686
[5,     1] loss: 1329.756
[6,     1] loss: 1325.818
[7,     1] loss: 1324.253
[8,     1] loss: 1327.217
[9,     1] loss: 1324.613
[10,     1] loss: 1325.811
[11,     1] loss: 1324.101
[12,     1] loss: 1322.932
[13,     1] loss: 1321.786
[14,     1] loss: 1316.368
[15,     1] loss: 1308.709
[16,     1] loss: 1296.827
[17,     1] loss: 1280.304
[18,     1] loss: 1252.751
[19,     1] loss: 1228.585
[20,     1] loss: 1217.689
[21,     1] loss: 1195.728
[22,     1] loss: 1137.243
[23,     1] loss: 1116.747
[24,     1] loss: 1127.896
[25,     1] loss: 1112.378
[26,     1] loss: 1081.740
[27,     1] loss: 1100.363
[28,     1] loss: 1086.180
[29,     1] loss: 1087.526
[30,     1] loss: 1082.566
[31,     1] loss: 1074.497
[32,     1] loss: 1021.898
[33,     1] loss: 1040.281
[34,     1] loss: 1026.834
[35,     1] loss: 1019.831
[36,     1] loss: 1073.235
[37,     1] loss: 1001.991
[38,     1] loss: 1034.347
[39,     1] loss: 937.075
[40,     1] loss: 1014.595
[41,     1] loss: 996.125
[42,     1] loss: 979.934
[43,     1] loss: 972.536
[44,     1] loss: 978.189
[45,     1] loss: 974.546
[46,     1] loss: 937.908
[47,     1] loss: 924.516
[48,     1] loss: 924.290
[49,     1] loss: 881.292
[50,     1] loss: 960.463
[51,     1] loss: 885.984
[52,     1] loss: 916.273
[53,     1] loss: 875.391
[54,     1] loss: 826.616
[55,     1] loss: 834.466
[56,     1] loss: 838.788
[57,     1] loss: 884.732
[58,     1] loss: 873.606
[59,     1] loss: 867.133
[60,     1] loss: 830.507
[61,     1] loss: 773.666
[62,     1] loss: 765.239
[63,     1] loss: 790.422
[64,     1] loss: 745.966
[65,     1] loss: 743.371
[66,     1] loss: 774.406
[67,     1] loss: 764.495
[68,     1] loss: 882.522
[69,     1] loss: 796.334
[70,     1] loss: 760.557
[71,     1] loss: 821.876
[72,     1] loss: 718.706
[73,     1] loss: 762.323
[74,     1] loss: 696.095
Early stopping applied (best metric=0.7660791873931885)
Finished Training
Total time taken: 10.375218629837036
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1330.826
[2,     1] loss: 1326.676
[3,     1] loss: 1324.786
[4,     1] loss: 1323.818
[5,     1] loss: 1321.818
[6,     1] loss: 1311.990
[7,     1] loss: 1286.988
[8,     1] loss: 1262.378
[9,     1] loss: 1248.625
[10,     1] loss: 1195.943
[11,     1] loss: 1163.438
[12,     1] loss: 1127.381
[13,     1] loss: 1095.201
[14,     1] loss: 1107.437
[15,     1] loss: 1088.919
[16,     1] loss: 1073.464
[17,     1] loss: 1034.942
[18,     1] loss: 1056.557
[19,     1] loss: 1058.026
[20,     1] loss: 1075.970
[21,     1] loss: 1031.337
[22,     1] loss: 1032.125
[23,     1] loss: 1055.388
[24,     1] loss: 1022.259
[25,     1] loss: 990.487
[26,     1] loss: 1002.601
[27,     1] loss: 958.078
[28,     1] loss: 998.616
[29,     1] loss: 974.671
[30,     1] loss: 952.845
[31,     1] loss: 939.881
[32,     1] loss: 969.892
Early stopping applied (best metric=1.009690761566162)
Finished Training
Total time taken: 5.134106397628784
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1336.476
[2,     1] loss: 1329.368
[3,     1] loss: 1327.420
[4,     1] loss: 1326.015
[5,     1] loss: 1326.916
[6,     1] loss: 1323.358
[7,     1] loss: 1318.598
[8,     1] loss: 1315.367
[9,     1] loss: 1305.884
[10,     1] loss: 1293.240
[11,     1] loss: 1264.708
[12,     1] loss: 1237.051
[13,     1] loss: 1229.404
[14,     1] loss: 1186.956
[15,     1] loss: 1177.562
[16,     1] loss: 1127.526
[17,     1] loss: 1147.327
[18,     1] loss: 1135.160
[19,     1] loss: 1132.092
[20,     1] loss: 1124.896
[21,     1] loss: 1119.901
[22,     1] loss: 1088.053
[23,     1] loss: 1098.175
[24,     1] loss: 1140.444
[25,     1] loss: 1060.682
[26,     1] loss: 1047.693
[27,     1] loss: 1071.208
[28,     1] loss: 1016.507
[29,     1] loss: 1062.358
[30,     1] loss: 1035.519
[31,     1] loss: 1028.838
[32,     1] loss: 1064.648
[33,     1] loss: 1007.799
[34,     1] loss: 1033.827
[35,     1] loss: 971.748
[36,     1] loss: 979.340
[37,     1] loss: 963.498
[38,     1] loss: 936.756
[39,     1] loss: 961.373
[40,     1] loss: 959.094
[41,     1] loss: 913.321
[42,     1] loss: 892.480
[43,     1] loss: 946.335
[44,     1] loss: 886.018
[45,     1] loss: 858.587
[46,     1] loss: 822.487
[47,     1] loss: 911.348
[48,     1] loss: 1163.387
[49,     1] loss: 887.285
[50,     1] loss: 884.840
[51,     1] loss: 847.688
[52,     1] loss: 928.790
[53,     1] loss: 895.663
[54,     1] loss: 875.784
[55,     1] loss: 895.635
[56,     1] loss: 841.926
Early stopping applied (best metric=0.8385623693466187)
Finished Training
Total time taken: 7.749163389205933
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.910
[2,     1] loss: 1325.421
[3,     1] loss: 1335.615
[4,     1] loss: 1334.020
[5,     1] loss: 1328.380
[6,     1] loss: 1322.408
[7,     1] loss: 1324.093
[8,     1] loss: 1325.410
[9,     1] loss: 1327.588
[10,     1] loss: 1326.418
[11,     1] loss: 1328.239
[12,     1] loss: 1322.261
[13,     1] loss: 1319.030
[14,     1] loss: 1312.006
[15,     1] loss: 1302.073
[16,     1] loss: 1295.553
[17,     1] loss: 1273.073
[18,     1] loss: 1248.019
[19,     1] loss: 1240.793
[20,     1] loss: 1196.624
[21,     1] loss: 1188.568
[22,     1] loss: 1145.301
[23,     1] loss: 1118.311
[24,     1] loss: 1142.798
[25,     1] loss: 1110.550
[26,     1] loss: 1148.906
[27,     1] loss: 1106.981
[28,     1] loss: 1069.505
[29,     1] loss: 1106.734
[30,     1] loss: 1032.602
[31,     1] loss: 1081.963
[32,     1] loss: 1072.468
[33,     1] loss: 1015.554
[34,     1] loss: 1044.446
[35,     1] loss: 1028.017
[36,     1] loss: 1013.967
[37,     1] loss: 1014.757
[38,     1] loss: 990.175
[39,     1] loss: 921.798
[40,     1] loss: 1014.910
[41,     1] loss: 950.486
[42,     1] loss: 992.390
[43,     1] loss: 881.071
[44,     1] loss: 940.384
[45,     1] loss: 921.884
[46,     1] loss: 916.347
[47,     1] loss: 883.354
[48,     1] loss: 947.540
[49,     1] loss: 942.150
[50,     1] loss: 831.416
[51,     1] loss: 894.447
[52,     1] loss: 831.991
[53,     1] loss: 846.330
[54,     1] loss: 797.915
[55,     1] loss: 777.133
[56,     1] loss: 841.177
[57,     1] loss: 764.665
Early stopping applied (best metric=0.8371531963348389)
Finished Training
Total time taken: 8.642698287963867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.464
[2,     1] loss: 1329.481
[3,     1] loss: 1332.382
[4,     1] loss: 1326.022
[5,     1] loss: 1328.142
[6,     1] loss: 1326.765
[7,     1] loss: 1325.776
[8,     1] loss: 1320.874
[9,     1] loss: 1318.707
[10,     1] loss: 1317.719
[11,     1] loss: 1303.927
[12,     1] loss: 1300.897
[13,     1] loss: 1269.097
[14,     1] loss: 1238.860
[15,     1] loss: 1213.436
[16,     1] loss: 1172.193
[17,     1] loss: 1134.568
[18,     1] loss: 1093.450
[19,     1] loss: 1110.384
[20,     1] loss: 1115.330
[21,     1] loss: 1145.367
[22,     1] loss: 1146.120
[23,     1] loss: 1086.802
[24,     1] loss: 1053.489
[25,     1] loss: 1052.432
[26,     1] loss: 1081.992
[27,     1] loss: 1072.436
[28,     1] loss: 1087.281
[29,     1] loss: 1067.780
[30,     1] loss: 1031.704
[31,     1] loss: 1031.506
[32,     1] loss: 1014.047
[33,     1] loss: 1010.414
[34,     1] loss: 1038.130
[35,     1] loss: 1013.627
[36,     1] loss: 1003.424
[37,     1] loss: 1006.736
[38,     1] loss: 908.196
[39,     1] loss: 1022.997
[40,     1] loss: 949.760
[41,     1] loss: 950.439
[42,     1] loss: 977.321
[43,     1] loss: 891.452
[44,     1] loss: 973.271
[45,     1] loss: 919.427
[46,     1] loss: 933.178
[47,     1] loss: 906.186
[48,     1] loss: 860.719
[49,     1] loss: 916.073
[50,     1] loss: 830.782
[51,     1] loss: 850.008
Early stopping applied (best metric=0.8601679801940918)
Finished Training
Total time taken: 7.7691650390625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.524
[2,     1] loss: 1329.553
[3,     1] loss: 1328.874
[4,     1] loss: 1326.655
[5,     1] loss: 1329.001
[6,     1] loss: 1325.980
[7,     1] loss: 1321.430
[8,     1] loss: 1314.038
[9,     1] loss: 1306.023
[10,     1] loss: 1291.840
[11,     1] loss: 1263.599
[12,     1] loss: 1231.451
[13,     1] loss: 1211.967
[14,     1] loss: 1178.442
[15,     1] loss: 1155.580
[16,     1] loss: 1121.217
[17,     1] loss: 1115.417
[18,     1] loss: 1085.915
[19,     1] loss: 1082.206
[20,     1] loss: 1086.614
[21,     1] loss: 1078.990
[22,     1] loss: 1042.219
[23,     1] loss: 1017.511
[24,     1] loss: 1012.610
[25,     1] loss: 1036.766
[26,     1] loss: 1025.184
[27,     1] loss: 1025.707
[28,     1] loss: 1035.274
[29,     1] loss: 988.095
[30,     1] loss: 984.292
[31,     1] loss: 1005.940
[32,     1] loss: 910.448
[33,     1] loss: 977.326
[34,     1] loss: 942.527
[35,     1] loss: 939.646
[36,     1] loss: 902.385
[37,     1] loss: 835.367
[38,     1] loss: 894.004
[39,     1] loss: 988.185
[40,     1] loss: 957.804
[41,     1] loss: 933.971
[42,     1] loss: 865.172
[43,     1] loss: 884.084
[44,     1] loss: 868.798
[45,     1] loss: 945.126
[46,     1] loss: 858.480
[47,     1] loss: 868.893
[48,     1] loss: 893.489
[49,     1] loss: 849.118
[50,     1] loss: 867.374
[51,     1] loss: 793.204
[52,     1] loss: 818.006
[53,     1] loss: 783.172
[54,     1] loss: 778.942
[55,     1] loss: 772.166
[56,     1] loss: 761.391
[57,     1] loss: 728.983
[58,     1] loss: 833.431
[59,     1] loss: 1013.131
[60,     1] loss: 754.226
[61,     1] loss: 876.204
[62,     1] loss: 811.794
[63,     1] loss: 801.686
[64,     1] loss: 765.256
[65,     1] loss: 816.916
[66,     1] loss: 722.286
[67,     1] loss: 773.363
[68,     1] loss: 716.217
[69,     1] loss: 721.974
[70,     1] loss: 755.370
[71,     1] loss: 725.444
[72,     1] loss: 667.081
[73,     1] loss: 650.947
[74,     1] loss: 652.560
[75,     1] loss: 647.785
[76,     1] loss: 623.227
[77,     1] loss: 657.813
[78,     1] loss: 745.673
[79,     1] loss: 603.424
[80,     1] loss: 675.293
[81,     1] loss: 615.971
[82,     1] loss: 669.628
[83,     1] loss: 746.123
[84,     1] loss: 585.836
[85,     1] loss: 664.529
[86,     1] loss: 554.427
[87,     1] loss: 617.833
[88,     1] loss: 536.779
[89,     1] loss: 633.789
[90,     1] loss: 619.458
[91,     1] loss: 505.176
[92,     1] loss: 547.974
[93,     1] loss: 525.498
[94,     1] loss: 597.057
[95,     1] loss: 755.390
[96,     1] loss: 516.893
[97,     1] loss: 562.339
[98,     1] loss: 610.311
[99,     1] loss: 541.628
[100,     1] loss: 604.642
[101,     1] loss: 497.271
[102,     1] loss: 598.078
[103,     1] loss: 465.072
[104,     1] loss: 518.031
[105,     1] loss: 521.250
[106,     1] loss: 469.243
[107,     1] loss: 607.505
[108,     1] loss: 539.782
[109,     1] loss: 511.367
[110,     1] loss: 576.455
[111,     1] loss: 478.730
[112,     1] loss: 476.905
[113,     1] loss: 491.196
[114,     1] loss: 436.566
[115,     1] loss: 497.875
[116,     1] loss: 399.494
[117,     1] loss: 466.318
Early stopping applied (best metric=0.6891552209854126)
Finished Training
Total time taken: 18.180382251739502
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1341.076
[2,     1] loss: 1329.408
[3,     1] loss: 1332.136
[4,     1] loss: 1328.081
[5,     1] loss: 1328.514
[6,     1] loss: 1326.325
[7,     1] loss: 1322.329
[8,     1] loss: 1319.137
[9,     1] loss: 1317.493
[10,     1] loss: 1292.203
[11,     1] loss: 1277.924
[12,     1] loss: 1240.286
[13,     1] loss: 1224.025
[14,     1] loss: 1199.860
[15,     1] loss: 1167.852
[16,     1] loss: 1114.143
[17,     1] loss: 1132.205
[18,     1] loss: 1091.752
[19,     1] loss: 1095.339
[20,     1] loss: 1080.458
[21,     1] loss: 1086.740
[22,     1] loss: 1082.746
[23,     1] loss: 1076.114
[24,     1] loss: 1075.939
[25,     1] loss: 1049.331
[26,     1] loss: 1113.389
[27,     1] loss: 1046.019
[28,     1] loss: 1083.827
[29,     1] loss: 1005.088
[30,     1] loss: 1035.376
[31,     1] loss: 994.614
[32,     1] loss: 1015.931
[33,     1] loss: 1001.616
[34,     1] loss: 1021.195
[35,     1] loss: 986.944
[36,     1] loss: 953.422
[37,     1] loss: 1031.882
[38,     1] loss: 917.866
[39,     1] loss: 979.193
[40,     1] loss: 871.040
[41,     1] loss: 891.435
[42,     1] loss: 883.586
[43,     1] loss: 910.868
[44,     1] loss: 872.605
[45,     1] loss: 890.039
[46,     1] loss: 896.079
[47,     1] loss: 796.684
[48,     1] loss: 804.463
[49,     1] loss: 827.742
[50,     1] loss: 816.820
[51,     1] loss: 841.968
Early stopping applied (best metric=0.981968879699707)
Finished Training
Total time taken: 6.880143642425537
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.616
[2,     1] loss: 1328.115
[3,     1] loss: 1326.450
[4,     1] loss: 1327.961
[5,     1] loss: 1327.982
[6,     1] loss: 1321.561
[7,     1] loss: 1320.913
[8,     1] loss: 1305.466
[9,     1] loss: 1291.426
[10,     1] loss: 1273.661
[11,     1] loss: 1239.935
[12,     1] loss: 1209.827
[13,     1] loss: 1158.993
[14,     1] loss: 1158.522
[15,     1] loss: 1116.665
[16,     1] loss: 1104.193
[17,     1] loss: 1101.005
[18,     1] loss: 1074.948
[19,     1] loss: 1085.166
[20,     1] loss: 1034.468
[21,     1] loss: 1093.564
[22,     1] loss: 1054.677
[23,     1] loss: 1035.844
[24,     1] loss: 1043.103
[25,     1] loss: 1041.533
[26,     1] loss: 959.317
[27,     1] loss: 991.014
[28,     1] loss: 924.811
[29,     1] loss: 971.851
[30,     1] loss: 952.755
[31,     1] loss: 939.989
[32,     1] loss: 945.629
[33,     1] loss: 974.622
[34,     1] loss: 961.373
[35,     1] loss: 860.153
[36,     1] loss: 946.765
[37,     1] loss: 838.261
[38,     1] loss: 935.507
[39,     1] loss: 858.681
[40,     1] loss: 918.052
[41,     1] loss: 849.542
[42,     1] loss: 889.767
[43,     1] loss: 827.179
[44,     1] loss: 829.345
[45,     1] loss: 800.122
Early stopping applied (best metric=0.9592896699905396)
Finished Training
Total time taken: 7.526158571243286
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.835
[2,     1] loss: 1326.580
[3,     1] loss: 1327.831
[4,     1] loss: 1322.214
[5,     1] loss: 1322.320
[6,     1] loss: 1309.967
[7,     1] loss: 1303.934
[8,     1] loss: 1275.635
[9,     1] loss: 1251.195
[10,     1] loss: 1222.964
[11,     1] loss: 1198.148
[12,     1] loss: 1187.694
[13,     1] loss: 1171.393
[14,     1] loss: 1114.629
[15,     1] loss: 1135.889
[16,     1] loss: 1090.719
[17,     1] loss: 1102.818
[18,     1] loss: 1081.988
[19,     1] loss: 1088.407
[20,     1] loss: 1083.449
[21,     1] loss: 1023.501
[22,     1] loss: 1013.394
[23,     1] loss: 1020.727
[24,     1] loss: 1036.080
[25,     1] loss: 984.928
[26,     1] loss: 976.484
[27,     1] loss: 1001.026
[28,     1] loss: 970.246
[29,     1] loss: 996.738
[30,     1] loss: 963.964
[31,     1] loss: 924.007
[32,     1] loss: 935.378
[33,     1] loss: 975.000
[34,     1] loss: 920.507
[35,     1] loss: 987.364
[36,     1] loss: 917.786
[37,     1] loss: 970.402
[38,     1] loss: 903.589
[39,     1] loss: 887.833
[40,     1] loss: 876.268
[41,     1] loss: 879.320
[42,     1] loss: 877.640
[43,     1] loss: 940.434
[44,     1] loss: 831.211
Early stopping applied (best metric=0.9230687618255615)
Finished Training
Total time taken: 7.249153137207031
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.342
[2,     1] loss: 1326.859
[3,     1] loss: 1327.504
[4,     1] loss: 1322.530
[5,     1] loss: 1323.819
[6,     1] loss: 1323.661
[7,     1] loss: 1321.507
[8,     1] loss: 1311.031
[9,     1] loss: 1296.973
[10,     1] loss: 1279.233
[11,     1] loss: 1228.507
[12,     1] loss: 1203.395
[13,     1] loss: 1170.227
[14,     1] loss: 1139.474
[15,     1] loss: 1076.531
[16,     1] loss: 1105.858
[17,     1] loss: 1095.833
[18,     1] loss: 1075.684
[19,     1] loss: 1045.321
[20,     1] loss: 1054.380
[21,     1] loss: 1077.382
[22,     1] loss: 1076.491
[23,     1] loss: 1045.003
[24,     1] loss: 1035.130
[25,     1] loss: 1050.198
[26,     1] loss: 1028.005
[27,     1] loss: 1012.396
[28,     1] loss: 988.482
[29,     1] loss: 998.691
[30,     1] loss: 959.410
[31,     1] loss: 987.278
[32,     1] loss: 929.261
[33,     1] loss: 953.213
[34,     1] loss: 930.590
[35,     1] loss: 964.793
[36,     1] loss: 956.051
[37,     1] loss: 952.024
[38,     1] loss: 924.562
[39,     1] loss: 865.740
[40,     1] loss: 871.526
[41,     1] loss: 871.180
[42,     1] loss: 886.392
[43,     1] loss: 839.048
[44,     1] loss: 840.328
[45,     1] loss: 845.256
[46,     1] loss: 918.767
[47,     1] loss: 874.472
[48,     1] loss: 893.062
[49,     1] loss: 892.528
[50,     1] loss: 784.235
[51,     1] loss: 863.428
[52,     1] loss: 809.843
[53,     1] loss: 861.292
[54,     1] loss: 780.208
[55,     1] loss: 850.640
[56,     1] loss: 709.968
[57,     1] loss: 759.827
[58,     1] loss: 806.622
[59,     1] loss: 779.646
[60,     1] loss: 726.692
[61,     1] loss: 749.509
[62,     1] loss: 714.033
[63,     1] loss: 728.802
[64,     1] loss: 732.367
[65,     1] loss: 695.675
[66,     1] loss: 730.196
[67,     1] loss: 814.845
[68,     1] loss: 739.567
[69,     1] loss: 703.727
[70,     1] loss: 730.134
[71,     1] loss: 641.572
[72,     1] loss: 729.208
[73,     1] loss: 653.034
[74,     1] loss: 730.149
Early stopping applied (best metric=0.7830009460449219)
Finished Training
Total time taken: 9.943209648132324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.306
[2,     1] loss: 1332.367
[3,     1] loss: 1326.486
[4,     1] loss: 1331.882
[5,     1] loss: 1326.025
[6,     1] loss: 1325.614
[7,     1] loss: 1320.685
[8,     1] loss: 1315.157
[9,     1] loss: 1304.815
[10,     1] loss: 1297.410
[11,     1] loss: 1272.438
[12,     1] loss: 1238.707
[13,     1] loss: 1214.532
[14,     1] loss: 1178.071
[15,     1] loss: 1153.475
[16,     1] loss: 1102.765
[17,     1] loss: 1114.631
[18,     1] loss: 1100.966
[19,     1] loss: 1104.549
[20,     1] loss: 1103.568
[21,     1] loss: 1082.978
[22,     1] loss: 1093.585
[23,     1] loss: 1057.538
[24,     1] loss: 1073.889
[25,     1] loss: 1098.267
[26,     1] loss: 1046.845
[27,     1] loss: 1031.902
[28,     1] loss: 1042.439
[29,     1] loss: 1040.786
[30,     1] loss: 1049.672
[31,     1] loss: 1001.137
[32,     1] loss: 945.551
[33,     1] loss: 976.305
[34,     1] loss: 992.033
[35,     1] loss: 939.926
[36,     1] loss: 904.105
[37,     1] loss: 942.923
[38,     1] loss: 948.607
[39,     1] loss: 923.107
[40,     1] loss: 927.410
[41,     1] loss: 918.996
[42,     1] loss: 935.225
[43,     1] loss: 919.538
[44,     1] loss: 826.852
[45,     1] loss: 851.328
[46,     1] loss: 965.617
[47,     1] loss: 995.283
[48,     1] loss: 866.281
[49,     1] loss: 985.343
[50,     1] loss: 844.437
[51,     1] loss: 904.908
[52,     1] loss: 839.282
[53,     1] loss: 843.138
[54,     1] loss: 879.833
[55,     1] loss: 844.020
[56,     1] loss: 842.925
[57,     1] loss: 814.973
[58,     1] loss: 816.155
[59,     1] loss: 816.961
Early stopping applied (best metric=0.8382847309112549)
Finished Training
Total time taken: 9.765206098556519
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1330.607
[2,     1] loss: 1333.176
[3,     1] loss: 1330.610
[4,     1] loss: 1332.171
[5,     1] loss: 1327.191
[6,     1] loss: 1318.757
[7,     1] loss: 1317.590
[8,     1] loss: 1305.038
[9,     1] loss: 1290.834
[10,     1] loss: 1263.517
[11,     1] loss: 1227.364
[12,     1] loss: 1192.883
[13,     1] loss: 1192.011
[14,     1] loss: 1154.300
[15,     1] loss: 1155.509
[16,     1] loss: 1102.834
[17,     1] loss: 1068.073
[18,     1] loss: 1078.593
[19,     1] loss: 1064.872
[20,     1] loss: 1106.198
[21,     1] loss: 1088.226
[22,     1] loss: 1069.120
[23,     1] loss: 1061.749
[24,     1] loss: 1056.155
[25,     1] loss: 1030.538
[26,     1] loss: 1025.928
[27,     1] loss: 1004.278
[28,     1] loss: 990.285
[29,     1] loss: 992.121
[30,     1] loss: 1021.735
[31,     1] loss: 1009.256
[32,     1] loss: 971.528
[33,     1] loss: 970.726
[34,     1] loss: 980.012
[35,     1] loss: 935.432
[36,     1] loss: 954.331
[37,     1] loss: 920.074
[38,     1] loss: 941.464
[39,     1] loss: 950.282
[40,     1] loss: 882.563
[41,     1] loss: 892.835
Early stopping applied (best metric=0.8680433034896851)
Finished Training
Total time taken: 6.843143939971924
{'Hydroxylation-K Validation Accuracy': 0.72975768321513, 'Hydroxylation-K Validation Sensitivity': 0.6162962962962962, 'Hydroxylation-K Validation Specificity': 0.7578947368421053, 'Hydroxylation-K Validation Precision': 0.3958304967343869, 'Hydroxylation-K AUC ROC': 0.7435282651072125, 'Hydroxylation-K AUC PR': 0.5051663383929632, 'Hydroxylation-K MCC': 0.3252395826083136, 'Hydroxylation-K F1': 0.4793897553387808, 'Validation Loss (Hydroxylation-K)': 0.4794690469900767, 'Hydroxylation-P Validation Accuracy': 0.7794997039067391, 'Hydroxylation-P Validation Sensitivity': 0.8013756613756614, 'Hydroxylation-P Validation Specificity': 0.7748416379869321, 'Hydroxylation-P Validation Precision': 0.44577314124708467, 'Hydroxylation-P AUC ROC': 0.8546163664894025, 'Hydroxylation-P AUC PR': 0.5798304677092861, 'Hydroxylation-P MCC': 0.47593360628609505, 'Hydroxylation-P F1': 0.5683174319858854, 'Validation Loss (Hydroxylation-P)': 0.37622182567914325, 'Validation Loss (total)': 0.8556908806165059, 'TimeToTrain': 8.842487208048503}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00880987111396247,
 'learning_rate_Hydroxylation-K': 0.004511508853639662,
 'learning_rate_Hydroxylation-P': 0.00839956344269223,
 'log_base': 1.9633030716109683,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2728532392,
 'sample_weights': [1.9891752638021247, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.171823183128738,
 'weight_decay_Hydroxylation-K': 5.505640036904076,
 'weight_decay_Hydroxylation-P': 5.579026476177239}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1431.135
[2,     1] loss: 1441.038
[3,     1] loss: 1438.786
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003892947901973315,
 'learning_rate_Hydroxylation-K': 0.0035972586044857808,
 'learning_rate_Hydroxylation-P': 0.007000721466966498,
 'log_base': 2.453050981982708,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2390571955,
 'sample_weights': [2.474611810264454, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6454326518138234,
 'weight_decay_Hydroxylation-K': 6.925948443977948,
 'weight_decay_Hydroxylation-P': 0.6818270610364368}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.628
[2,     1] loss: 1299.387
[3,     1] loss: 1302.044
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004547302499717642,
 'learning_rate_Hydroxylation-K': 0.005024341292081029,
 'learning_rate_Hydroxylation-P': 0.001216479081706287,
 'log_base': 1.3537048119028285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2433130827,
 'sample_weights': [1.860450899790306, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.251216598023642,
 'weight_decay_Hydroxylation-K': 4.0309779703702135,
 'weight_decay_Hydroxylation-P': 1.707852186519205}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2069.699
[2,     1] loss: 2068.937
[3,     1] loss: 2066.975
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007326149091903885,
 'learning_rate_Hydroxylation-K': 0.003741107221817166,
 'learning_rate_Hydroxylation-P': 0.007799305298050357,
 'log_base': 2.987981036708603,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1830943179,
 'sample_weights': [5.51253077349835, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.147019422015541,
 'weight_decay_Hydroxylation-K': 9.460768918714102,
 'weight_decay_Hydroxylation-P': 0.1785319278938673}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.683
[2,     1] loss: 1231.289
[3,     1] loss: 1228.413
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005272073220271133,
 'learning_rate_Hydroxylation-K': 0.004801632841488409,
 'learning_rate_Hydroxylation-P': 0.009704276574312174,
 'log_base': 2.623777417702005,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3912093300,
 'sample_weights': [1.5251656482713856, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.002816172167727,
 'weight_decay_Hydroxylation-K': 7.457548470887712,
 'weight_decay_Hydroxylation-P': 2.592957185336216}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.552
[2,     1] loss: 1274.535
[3,     1] loss: 1273.919
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00734476275784027,
 'learning_rate_Hydroxylation-K': 0.001495613465940923,
 'learning_rate_Hydroxylation-P': 0.005105243651359986,
 'log_base': 1.0398299906137847,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2138882070,
 'sample_weights': [1.730683303582699, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8197931184567464,
 'weight_decay_Hydroxylation-K': 7.327778910758659,
 'weight_decay_Hydroxylation-P': 5.65243741401636}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14049.148
[2,     1] loss: 14095.588
[3,     1] loss: 13894.479
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004543578271477925,
 'learning_rate_Hydroxylation-K': 0.0033647825729153317,
 'learning_rate_Hydroxylation-P': 0.00036639945894063495,
 'log_base': 1.6530078261871255,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1726593156,
 'sample_weights': [42.743512032755085, 5.343144835560084],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.504804753528106,
 'weight_decay_Hydroxylation-K': 0.32743136171994114,
 'weight_decay_Hydroxylation-P': 2.4826574211995007}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1609.989
[2,     1] loss: 1609.134
[3,     1] loss: 1608.339
[4,     1] loss: 1607.296
[5,     1] loss: 1612.103
[6,     1] loss: 1606.514
[7,     1] loss: 1603.679
[8,     1] loss: 1602.346
[9,     1] loss: 1596.919
[10,     1] loss: 1589.917
[11,     1] loss: 1577.498
[12,     1] loss: 1545.252
[13,     1] loss: 1517.615
[14,     1] loss: 1461.351
[15,     1] loss: 1437.621
[16,     1] loss: 1366.321
[17,     1] loss: 1410.350
[18,     1] loss: 1395.460
[19,     1] loss: 1344.991
[20,     1] loss: 1328.641
[21,     1] loss: 1322.430
[22,     1] loss: 1349.419
[23,     1] loss: 1302.923
[24,     1] loss: 1284.131
[25,     1] loss: 1317.441
[26,     1] loss: 1229.815
[27,     1] loss: 1218.071
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003301772372712024,
 'learning_rate_Hydroxylation-K': 0.007132396483958124,
 'learning_rate_Hydroxylation-P': 0.0041094712614178,
 'log_base': 1.5430411955433447,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2338179763,
 'sample_weights': [3.3216366810566558, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.065406514367538,
 'weight_decay_Hydroxylation-K': 5.27804281959127,
 'weight_decay_Hydroxylation-P': 3.5574929118302565}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1723.440
[2,     1] loss: 1716.140
[3,     1] loss: 1715.987
[4,     1] loss: 1721.504
[5,     1] loss: 1719.694
[6,     1] loss: 1717.241
[7,     1] loss: 1723.863
[8,     1] loss: 1713.589
[9,     1] loss: 1715.862
[10,     1] loss: 1720.219
[11,     1] loss: 1707.953
[12,     1] loss: 1716.559
[13,     1] loss: 1702.337
[14,     1] loss: 1701.913
[15,     1] loss: 1688.186
[16,     1] loss: 1669.252
[17,     1] loss: 1640.768
[18,     1] loss: 1624.551
[19,     1] loss: 1577.203
[20,     1] loss: 1551.017
[21,     1] loss: 1524.123
[22,     1] loss: 1465.513
[23,     1] loss: 1465.643
[24,     1] loss: 1463.577
[25,     1] loss: 1476.136
[26,     1] loss: 1454.645
[27,     1] loss: 1492.136
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008811924556704055,
 'learning_rate_Hydroxylation-K': 0.009946786103630843,
 'learning_rate_Hydroxylation-P': 0.0030992647867614936,
 'log_base': 2.0642487089468107,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 425435515,
 'sample_weights': [3.8488135075792465, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7675788388374967,
 'weight_decay_Hydroxylation-K': 6.525085389417744,
 'weight_decay_Hydroxylation-P': 1.2617922338877592}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.505
[2,     1] loss: 1400.680
[3,     1] loss: 1392.773
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005114098112028384,
 'learning_rate_Hydroxylation-K': 0.003293677381667989,
 'learning_rate_Hydroxylation-P': 0.006440005624666494,
 'log_base': 2.4884238931606246,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2100292731,
 'sample_weights': [2.3034225761669704, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.5893791709711946,
 'weight_decay_Hydroxylation-K': 8.0664896240266,
 'weight_decay_Hydroxylation-P': 2.7940576562703563}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.670
[2,     1] loss: 1291.684
[3,     1] loss: 1296.322
[4,     1] loss: 1294.767
[5,     1] loss: 1294.263
[6,     1] loss: 1294.214
[7,     1] loss: 1295.032
[8,     1] loss: 1291.949
[9,     1] loss: 1291.379
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003404195962440292,
 'learning_rate_Hydroxylation-K': 0.006236644975316958,
 'learning_rate_Hydroxylation-P': 0.007987230033214651,
 'log_base': 2.8827553623899425,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3371932368,
 'sample_weights': [1.8312334757429631, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6617977832450561,
 'weight_decay_Hydroxylation-K': 8.579655036093472,
 'weight_decay_Hydroxylation-P': 2.475134176288981}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.900
[2,     1] loss: 1238.175
[3,     1] loss: 1241.456
[4,     1] loss: 1241.269
[5,     1] loss: 1235.655
[6,     1] loss: 1231.700
[7,     1] loss: 1229.665
[8,     1] loss: 1210.477
[9,     1] loss: 1182.230
[10,     1] loss: 1164.313
[11,     1] loss: 1121.358
[12,     1] loss: 1101.572
[13,     1] loss: 1076.297
[14,     1] loss: 1078.366
[15,     1] loss: 1047.624
[16,     1] loss: 1057.341
[17,     1] loss: 1031.815
[18,     1] loss: 1012.731
[19,     1] loss: 1014.038
[20,     1] loss: 979.688
[21,     1] loss: 961.122
[22,     1] loss: 983.852
[23,     1] loss: 1002.671
[24,     1] loss: 950.553
[25,     1] loss: 960.158
[26,     1] loss: 959.654
[27,     1] loss: 967.655
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004183816683325808,
 'learning_rate_Hydroxylation-K': 0.0012205961257155716,
 'learning_rate_Hydroxylation-P': 0.006478933263704603,
 'log_base': 2.0501633412138927,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2015245346,
 'sample_weights': [1.5768109297926791, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4172725901966542,
 'weight_decay_Hydroxylation-K': 8.951093607065761,
 'weight_decay_Hydroxylation-P': 0.08527216419871164}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.944
[2,     1] loss: 1401.137
[3,     1] loss: 1398.036
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031832690232761565,
 'learning_rate_Hydroxylation-K': 0.0006963963095044944,
 'learning_rate_Hydroxylation-P': 0.005968772764879624,
 'log_base': 2.452357972210199,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2366838139,
 'sample_weights': [2.325390549220701, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.932622400338393,
 'weight_decay_Hydroxylation-K': 7.105109840269979,
 'weight_decay_Hydroxylation-P': 2.5654648502681687}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.249
[2,     1] loss: 1298.783
[3,     1] loss: 1301.352
[4,     1] loss: 1300.027
[5,     1] loss: 1293.576
[6,     1] loss: 1287.859
[7,     1] loss: 1288.798
[8,     1] loss: 1263.273
[9,     1] loss: 1242.988
[10,     1] loss: 1212.307
[11,     1] loss: 1166.568
[12,     1] loss: 1124.697
[13,     1] loss: 1117.423
[14,     1] loss: 1075.258
[15,     1] loss: 1095.049
[16,     1] loss: 1058.275
[17,     1] loss: 1025.897
[18,     1] loss: 992.864
[19,     1] loss: 1015.683
[20,     1] loss: 1052.587
[21,     1] loss: 1002.099
[22,     1] loss: 999.527
[23,     1] loss: 998.833
[24,     1] loss: 971.031
[25,     1] loss: 972.159
[26,     1] loss: 962.885
[27,     1] loss: 927.311
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016514825445590126,
 'learning_rate_Hydroxylation-K': 0.003356308205616225,
 'learning_rate_Hydroxylation-P': 0.007030656588143424,
 'log_base': 2.722294676941989,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 732879102,
 'sample_weights': [1.8610368972056996, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9633016391904552,
 'weight_decay_Hydroxylation-K': 9.308318961189919,
 'weight_decay_Hydroxylation-P': 1.1363730071919875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.507
[2,     1] loss: 1264.284
[3,     1] loss: 1260.288
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006688914111187347,
 'learning_rate_Hydroxylation-K': 0.0013848977303127603,
 'learning_rate_Hydroxylation-P': 0.004202728873923295,
 'log_base': 2.5487559952959296,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3834490888,
 'sample_weights': [1.6669840860476592, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.022547653627177,
 'weight_decay_Hydroxylation-K': 2.6272583074569535,
 'weight_decay_Hydroxylation-P': 1.4049464355599606}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.358
[2,     1] loss: 1287.539
[3,     1] loss: 1287.649
[4,     1] loss: 1284.701
[5,     1] loss: 1285.796
[6,     1] loss: 1283.398
[7,     1] loss: 1285.977
[8,     1] loss: 1284.197
[9,     1] loss: 1282.926
[10,     1] loss: 1284.450
[11,     1] loss: 1282.380
[12,     1] loss: 1280.134
[13,     1] loss: 1282.580
[14,     1] loss: 1276.796
[15,     1] loss: 1270.134
[16,     1] loss: 1264.902
[17,     1] loss: 1253.199
[18,     1] loss: 1231.726
[19,     1] loss: 1209.412
[20,     1] loss: 1167.037
[21,     1] loss: 1130.432
[22,     1] loss: 1104.448
[23,     1] loss: 1071.071
[24,     1] loss: 1058.593
[25,     1] loss: 1106.202
[26,     1] loss: 1085.294
[27,     1] loss: 1008.643
[28,     1] loss: 1070.717
[29,     1] loss: 1022.051
[30,     1] loss: 1018.519
[31,     1] loss: 1027.841
[32,     1] loss: 1014.090
[33,     1] loss: 992.033
[34,     1] loss: 998.265
[35,     1] loss: 959.891
[36,     1] loss: 950.512
[37,     1] loss: 1056.610
[38,     1] loss: 1092.068
[39,     1] loss: 939.894
[40,     1] loss: 1009.167
[41,     1] loss: 983.284
[42,     1] loss: 968.846
[43,     1] loss: 957.263
[44,     1] loss: 909.657
[45,     1] loss: 918.534
[46,     1] loss: 894.233
[47,     1] loss: 919.453
[48,     1] loss: 875.849
[49,     1] loss: 924.737
[50,     1] loss: 900.407
[51,     1] loss: 872.363
[52,     1] loss: 881.115
[53,     1] loss: 879.920
[54,     1] loss: 843.819
[55,     1] loss: 824.846
[56,     1] loss: 795.362
[57,     1] loss: 870.626
[58,     1] loss: 970.526
[59,     1] loss: 921.745
[60,     1] loss: 842.831
[61,     1] loss: 898.548
[62,     1] loss: 836.941
[63,     1] loss: 891.861
[64,     1] loss: 951.142
[65,     1] loss: 801.033
[66,     1] loss: 963.504
[67,     1] loss: 792.351
[68,     1] loss: 866.363
[69,     1] loss: 798.683
[70,     1] loss: 813.498
[71,     1] loss: 861.362
[72,     1] loss: 755.221
[73,     1] loss: 863.978
[74,     1] loss: 1043.920
[75,     1] loss: 778.175
[76,     1] loss: 907.512
[77,     1] loss: 781.880
[78,     1] loss: 853.928
[79,     1] loss: 811.449
[80,     1] loss: 847.146
[81,     1] loss: 868.539
[82,     1] loss: 738.700
[83,     1] loss: 889.526
[84,     1] loss: 868.103
[85,     1] loss: 735.367
[86,     1] loss: 1022.352
[87,     1] loss: 845.959
[88,     1] loss: 834.816
[89,     1] loss: 781.467
[90,     1] loss: 808.197
[91,     1] loss: 748.992
Early stopping applied (best metric=0.7564818859100342)
Finished Training
Total time taken: 13.385284662246704
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.574
[2,     1] loss: 1287.276
[3,     1] loss: 1293.290
[4,     1] loss: 1282.500
[5,     1] loss: 1285.189
[6,     1] loss: 1284.683
[7,     1] loss: 1283.510
[8,     1] loss: 1278.224
[9,     1] loss: 1278.171
[10,     1] loss: 1271.211
[11,     1] loss: 1265.630
[12,     1] loss: 1248.599
[13,     1] loss: 1223.659
[14,     1] loss: 1196.203
[15,     1] loss: 1156.579
[16,     1] loss: 1174.695
[17,     1] loss: 1137.471
[18,     1] loss: 1107.327
[19,     1] loss: 1109.239
[20,     1] loss: 1071.285
[21,     1] loss: 1055.469
[22,     1] loss: 1022.676
[23,     1] loss: 1088.492
[24,     1] loss: 1030.429
[25,     1] loss: 1078.987
[26,     1] loss: 976.435
[27,     1] loss: 1125.619
[28,     1] loss: 984.321
[29,     1] loss: 1014.084
[30,     1] loss: 1068.467
[31,     1] loss: 983.365
[32,     1] loss: 1025.496
[33,     1] loss: 944.151
[34,     1] loss: 933.153
[35,     1] loss: 963.604
[36,     1] loss: 890.319
[37,     1] loss: 964.833
[38,     1] loss: 933.160
[39,     1] loss: 931.652
[40,     1] loss: 905.124
[41,     1] loss: 951.968
[42,     1] loss: 884.054
[43,     1] loss: 931.115
[44,     1] loss: 904.352
[45,     1] loss: 917.069
[46,     1] loss: 896.382
[47,     1] loss: 806.269
[48,     1] loss: 896.162
[49,     1] loss: 929.887
[50,     1] loss: 851.019
[51,     1] loss: 836.426
[52,     1] loss: 859.255
[53,     1] loss: 780.345
[54,     1] loss: 803.303
[55,     1] loss: 865.452
[56,     1] loss: 755.641
[57,     1] loss: 749.470
Early stopping applied (best metric=0.8267379999160767)
Finished Training
Total time taken: 9.41819715499878
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1286.529
[2,     1] loss: 1287.276
[3,     1] loss: 1285.900
[4,     1] loss: 1287.597
[5,     1] loss: 1283.821
[6,     1] loss: 1282.181
[7,     1] loss: 1282.645
[8,     1] loss: 1281.029
[9,     1] loss: 1279.904
[10,     1] loss: 1273.929
[11,     1] loss: 1261.599
[12,     1] loss: 1241.576
[13,     1] loss: 1206.758
[14,     1] loss: 1190.545
[15,     1] loss: 1138.905
[16,     1] loss: 1122.560
[17,     1] loss: 1113.637
[18,     1] loss: 1056.862
[19,     1] loss: 1071.857
[20,     1] loss: 1120.366
[21,     1] loss: 1079.838
[22,     1] loss: 1052.720
[23,     1] loss: 1060.345
[24,     1] loss: 1052.947
[25,     1] loss: 1024.545
[26,     1] loss: 996.834
[27,     1] loss: 988.405
[28,     1] loss: 962.051
[29,     1] loss: 935.917
[30,     1] loss: 962.939
[31,     1] loss: 912.908
[32,     1] loss: 920.060
[33,     1] loss: 903.058
[34,     1] loss: 900.231
[35,     1] loss: 903.775
[36,     1] loss: 956.545
[37,     1] loss: 923.502
[38,     1] loss: 864.727
[39,     1] loss: 1012.148
[40,     1] loss: 833.588
[41,     1] loss: 959.346
[42,     1] loss: 812.184
[43,     1] loss: 951.725
[44,     1] loss: 829.658
[45,     1] loss: 966.637
[46,     1] loss: 815.892
[47,     1] loss: 889.802
[48,     1] loss: 833.396
[49,     1] loss: 822.538
[50,     1] loss: 819.991
[51,     1] loss: 749.967
[52,     1] loss: 810.394
[53,     1] loss: 1245.231
[54,     1] loss: 841.814
[55,     1] loss: 936.140
[56,     1] loss: 877.831
[57,     1] loss: 967.043
[58,     1] loss: 916.047
[59,     1] loss: 855.665
[60,     1] loss: 885.679
[61,     1] loss: 869.341
[62,     1] loss: 883.717
[63,     1] loss: 819.894
[64,     1] loss: 817.228
[65,     1] loss: 751.990
[66,     1] loss: 786.543
[67,     1] loss: 767.965
Early stopping applied (best metric=0.890679657459259)
Finished Training
Total time taken: 9.061188220977783
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.939
[2,     1] loss: 1289.785
[3,     1] loss: 1286.382
[4,     1] loss: 1285.626
[5,     1] loss: 1282.019
[6,     1] loss: 1285.514
[7,     1] loss: 1285.476
[8,     1] loss: 1282.724
[9,     1] loss: 1283.947
[10,     1] loss: 1281.379
[11,     1] loss: 1280.007
[12,     1] loss: 1273.771
[13,     1] loss: 1262.956
[14,     1] loss: 1244.848
[15,     1] loss: 1217.359
[16,     1] loss: 1180.803
[17,     1] loss: 1134.281
[18,     1] loss: 1086.491
[19,     1] loss: 1052.584
[20,     1] loss: 1026.563
[21,     1] loss: 1016.719
[22,     1] loss: 1085.304
[23,     1] loss: 1327.815
[24,     1] loss: 985.886
[25,     1] loss: 1140.632
[26,     1] loss: 1072.802
[27,     1] loss: 1040.038
[28,     1] loss: 1045.676
[29,     1] loss: 1085.385
[30,     1] loss: 1067.334
[31,     1] loss: 1051.258
[32,     1] loss: 994.544
[33,     1] loss: 1023.752
[34,     1] loss: 1047.839
[35,     1] loss: 977.939
[36,     1] loss: 996.786
[37,     1] loss: 966.871
[38,     1] loss: 923.799
[39,     1] loss: 881.252
[40,     1] loss: 897.770
[41,     1] loss: 919.262
[42,     1] loss: 922.450
[43,     1] loss: 936.554
[44,     1] loss: 886.029
[45,     1] loss: 858.689
[46,     1] loss: 889.619
[47,     1] loss: 895.148
[48,     1] loss: 842.721
[49,     1] loss: 899.088
[50,     1] loss: 846.329
[51,     1] loss: 802.513
[52,     1] loss: 844.106
[53,     1] loss: 932.157
[54,     1] loss: 844.037
[55,     1] loss: 763.947
[56,     1] loss: 853.336
[57,     1] loss: 791.396
[58,     1] loss: 732.898
[59,     1] loss: 765.544
[60,     1] loss: 882.699
[61,     1] loss: 833.511
[62,     1] loss: 716.008
[63,     1] loss: 813.077
[64,     1] loss: 785.426
Early stopping applied (best metric=0.8596058487892151)
Finished Training
Total time taken: 10.714226007461548
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1286.562
[2,     1] loss: 1284.502
[3,     1] loss: 1283.218
[4,     1] loss: 1288.467
[5,     1] loss: 1283.205
[6,     1] loss: 1279.891
[7,     1] loss: 1285.068
[8,     1] loss: 1279.276
[9,     1] loss: 1265.210
[10,     1] loss: 1248.790
[11,     1] loss: 1212.737
[12,     1] loss: 1181.260
[13,     1] loss: 1227.543
[14,     1] loss: 1092.094
[15,     1] loss: 1167.004
[16,     1] loss: 1048.526
[17,     1] loss: 1114.329
[18,     1] loss: 1054.511
[19,     1] loss: 1081.573
[20,     1] loss: 1056.680
[21,     1] loss: 1063.304
[22,     1] loss: 1034.307
[23,     1] loss: 1021.793
[24,     1] loss: 1012.678
[25,     1] loss: 1025.532
[26,     1] loss: 981.575
[27,     1] loss: 974.658
[28,     1] loss: 963.550
[29,     1] loss: 921.530
[30,     1] loss: 1007.273
[31,     1] loss: 924.368
[32,     1] loss: 957.763
[33,     1] loss: 1043.685
[34,     1] loss: 929.669
[35,     1] loss: 997.121
[36,     1] loss: 956.763
[37,     1] loss: 982.488
[38,     1] loss: 913.830
[39,     1] loss: 957.956
[40,     1] loss: 898.658
[41,     1] loss: 882.993
[42,     1] loss: 950.026
[43,     1] loss: 858.143
[44,     1] loss: 877.962
[45,     1] loss: 923.547
[46,     1] loss: 887.480
[47,     1] loss: 935.543
[48,     1] loss: 808.860
[49,     1] loss: 889.729
[50,     1] loss: 822.776
[51,     1] loss: 780.628
[52,     1] loss: 832.379
[53,     1] loss: 1089.923
[54,     1] loss: 1037.380
[55,     1] loss: 886.050
[56,     1] loss: 936.710
[57,     1] loss: 948.510
[58,     1] loss: 879.973
[59,     1] loss: 902.790
[60,     1] loss: 880.034
[61,     1] loss: 876.174
[62,     1] loss: 868.072
[63,     1] loss: 844.476
[64,     1] loss: 834.063
[65,     1] loss: 797.434
[66,     1] loss: 829.010
[67,     1] loss: 870.501
[68,     1] loss: 771.873
[69,     1] loss: 848.950
[70,     1] loss: 989.505
[71,     1] loss: 782.935
Early stopping applied (best metric=0.7585035562515259)
Finished Training
Total time taken: 11.75824785232544
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.740
[2,     1] loss: 1288.959
[3,     1] loss: 1294.417
[4,     1] loss: 1287.021
[5,     1] loss: 1281.610
[6,     1] loss: 1283.273
[7,     1] loss: 1280.043
[8,     1] loss: 1280.062
[9,     1] loss: 1279.283
[10,     1] loss: 1272.786
[11,     1] loss: 1265.582
[12,     1] loss: 1248.937
[13,     1] loss: 1224.332
[14,     1] loss: 1181.725
[15,     1] loss: 1134.985
[16,     1] loss: 1102.216
[17,     1] loss: 1100.413
[18,     1] loss: 1052.155
[19,     1] loss: 1055.062
[20,     1] loss: 1046.391
[21,     1] loss: 992.750
[22,     1] loss: 1045.864
[23,     1] loss: 1021.514
[24,     1] loss: 1026.872
[25,     1] loss: 1039.016
[26,     1] loss: 1022.662
[27,     1] loss: 1006.721
[28,     1] loss: 993.875
[29,     1] loss: 957.699
[30,     1] loss: 964.247
[31,     1] loss: 919.212
[32,     1] loss: 946.385
[33,     1] loss: 926.182
[34,     1] loss: 898.559
[35,     1] loss: 948.624
[36,     1] loss: 908.210
[37,     1] loss: 888.599
[38,     1] loss: 884.912
[39,     1] loss: 920.767
[40,     1] loss: 1327.272
[41,     1] loss: 995.611
[42,     1] loss: 1006.891
[43,     1] loss: 974.834
[44,     1] loss: 1012.640
[45,     1] loss: 1043.068
[46,     1] loss: 1033.265
[47,     1] loss: 990.764
[48,     1] loss: 1002.551
[49,     1] loss: 965.152
[50,     1] loss: 973.882
[51,     1] loss: 931.764
[52,     1] loss: 898.649
Early stopping applied (best metric=0.8331746459007263)
Finished Training
Total time taken: 7.841165542602539
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.299
[2,     1] loss: 1287.736
[3,     1] loss: 1283.740
[4,     1] loss: 1283.723
[5,     1] loss: 1284.164
[6,     1] loss: 1280.124
[7,     1] loss: 1270.930
[8,     1] loss: 1265.737
[9,     1] loss: 1238.899
[10,     1] loss: 1197.806
[11,     1] loss: 1150.829
[12,     1] loss: 1127.551
[13,     1] loss: 1181.170
[14,     1] loss: 1039.637
[15,     1] loss: 1072.473
[16,     1] loss: 1099.527
[17,     1] loss: 1092.346
[18,     1] loss: 1021.968
[19,     1] loss: 1080.629
[20,     1] loss: 1044.685
[21,     1] loss: 1025.052
[22,     1] loss: 1014.630
[23,     1] loss: 988.073
[24,     1] loss: 970.355
[25,     1] loss: 1001.762
[26,     1] loss: 921.676
[27,     1] loss: 999.270
[28,     1] loss: 926.167
[29,     1] loss: 906.158
[30,     1] loss: 963.210
[31,     1] loss: 932.584
[32,     1] loss: 924.501
[33,     1] loss: 936.844
[34,     1] loss: 849.742
[35,     1] loss: 900.353
[36,     1] loss: 1035.883
[37,     1] loss: 873.741
[38,     1] loss: 845.202
[39,     1] loss: 872.618
[40,     1] loss: 839.059
[41,     1] loss: 826.927
[42,     1] loss: 772.469
[43,     1] loss: 849.391
[44,     1] loss: 767.327
[45,     1] loss: 768.268
[46,     1] loss: 975.023
[47,     1] loss: 1276.191
[48,     1] loss: 870.426
[49,     1] loss: 1055.125
[50,     1] loss: 1020.739
[51,     1] loss: 995.534
[52,     1] loss: 980.325
[53,     1] loss: 968.226
[54,     1] loss: 952.694
[55,     1] loss: 919.750
[56,     1] loss: 922.107
[57,     1] loss: 890.352
[58,     1] loss: 870.153
[59,     1] loss: 881.722
[60,     1] loss: 899.668
[61,     1] loss: 923.388
[62,     1] loss: 958.487
[63,     1] loss: 853.864
Early stopping applied (best metric=0.8421341180801392)
Finished Training
Total time taken: 10.10421347618103
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.206
[2,     1] loss: 1285.918
[3,     1] loss: 1285.298
[4,     1] loss: 1283.766
[5,     1] loss: 1277.816
[6,     1] loss: 1276.855
[7,     1] loss: 1266.163
[8,     1] loss: 1235.556
[9,     1] loss: 1191.918
[10,     1] loss: 1153.176
[11,     1] loss: 1112.732
[12,     1] loss: 1083.134
[13,     1] loss: 1083.405
[14,     1] loss: 1035.047
[15,     1] loss: 1110.245
[16,     1] loss: 992.034
[17,     1] loss: 1020.094
[18,     1] loss: 981.031
[19,     1] loss: 1007.077
[20,     1] loss: 958.956
[21,     1] loss: 992.496
[22,     1] loss: 968.280
[23,     1] loss: 904.811
[24,     1] loss: 909.938
[25,     1] loss: 855.878
[26,     1] loss: 919.167
[27,     1] loss: 952.632
[28,     1] loss: 996.805
[29,     1] loss: 924.580
[30,     1] loss: 873.282
[31,     1] loss: 923.034
[32,     1] loss: 875.027
[33,     1] loss: 893.743
[34,     1] loss: 872.027
[35,     1] loss: 845.585
[36,     1] loss: 855.893
[37,     1] loss: 818.227
[38,     1] loss: 845.656
[39,     1] loss: 771.536
[40,     1] loss: 763.281
[41,     1] loss: 790.147
[42,     1] loss: 810.126
[43,     1] loss: 1373.818
[44,     1] loss: 1325.163
[45,     1] loss: 961.766
[46,     1] loss: 1010.193
[47,     1] loss: 1047.751
[48,     1] loss: 1071.354
[49,     1] loss: 1085.176
[50,     1] loss: 1025.121
[51,     1] loss: 983.994
[52,     1] loss: 990.026
[53,     1] loss: 1014.067
[54,     1] loss: 964.613
[55,     1] loss: 952.905
[56,     1] loss: 927.889
[57,     1] loss: 926.265
[58,     1] loss: 875.686
Early stopping applied (best metric=0.9719829559326172)
Finished Training
Total time taken: 7.905165433883667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.316
[2,     1] loss: 1282.149
[3,     1] loss: 1290.167
[4,     1] loss: 1283.683
[5,     1] loss: 1283.316
[6,     1] loss: 1276.142
[7,     1] loss: 1274.356
[8,     1] loss: 1263.297
[9,     1] loss: 1242.899
[10,     1] loss: 1209.829
[11,     1] loss: 1174.263
[12,     1] loss: 1124.494
[13,     1] loss: 1109.599
[14,     1] loss: 1124.061
[15,     1] loss: 1108.473
[16,     1] loss: 1064.036
[17,     1] loss: 1062.659
[18,     1] loss: 1061.743
[19,     1] loss: 1063.823
[20,     1] loss: 1095.451
[21,     1] loss: 1076.620
[22,     1] loss: 1060.545
[23,     1] loss: 1017.615
[24,     1] loss: 1047.266
[25,     1] loss: 1008.093
[26,     1] loss: 1001.113
[27,     1] loss: 977.184
[28,     1] loss: 926.298
[29,     1] loss: 927.002
[30,     1] loss: 943.413
[31,     1] loss: 911.075
[32,     1] loss: 975.982
[33,     1] loss: 999.064
[34,     1] loss: 957.806
[35,     1] loss: 916.628
[36,     1] loss: 971.901
[37,     1] loss: 943.346
[38,     1] loss: 928.617
[39,     1] loss: 888.916
[40,     1] loss: 974.025
[41,     1] loss: 851.724
[42,     1] loss: 880.331
[43,     1] loss: 909.568
[44,     1] loss: 804.121
[45,     1] loss: 990.825
[46,     1] loss: 854.954
[47,     1] loss: 842.800
[48,     1] loss: 870.533
[49,     1] loss: 814.749
[50,     1] loss: 856.654
[51,     1] loss: 794.228
[52,     1] loss: 779.741
[53,     1] loss: 824.692
[54,     1] loss: 904.769
[55,     1] loss: 858.344
[56,     1] loss: 748.294
[57,     1] loss: 791.601
[58,     1] loss: 768.585
[59,     1] loss: 729.679
[60,     1] loss: 741.038
[61,     1] loss: 707.597
[62,     1] loss: 697.738
[63,     1] loss: 744.512
Early stopping applied (best metric=0.7694162726402283)
Finished Training
Total time taken: 10.4262216091156
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1291.848
[2,     1] loss: 1296.031
[3,     1] loss: 1294.300
[4,     1] loss: 1288.448
[5,     1] loss: 1285.459
[6,     1] loss: 1285.619
[7,     1] loss: 1286.200
[8,     1] loss: 1286.885
[9,     1] loss: 1286.494
[10,     1] loss: 1285.258
[11,     1] loss: 1285.397
[12,     1] loss: 1284.877
[13,     1] loss: 1284.895
[14,     1] loss: 1285.003
[15,     1] loss: 1285.575
[16,     1] loss: 1286.163
[17,     1] loss: 1284.796
[18,     1] loss: 1285.294
[19,     1] loss: 1285.563
[20,     1] loss: 1283.702
[21,     1] loss: 1281.753
[22,     1] loss: 1279.506
[23,     1] loss: 1273.997
[24,     1] loss: 1261.145
[25,     1] loss: 1250.575
[26,     1] loss: 1239.506
[27,     1] loss: 1210.567
[28,     1] loss: 1199.147
[29,     1] loss: 1141.358
[30,     1] loss: 1121.317
[31,     1] loss: 1145.577
[32,     1] loss: 1115.328
[33,     1] loss: 1078.920
[34,     1] loss: 1027.921
[35,     1] loss: 1104.991
[36,     1] loss: 1029.569
[37,     1] loss: 1087.173
[38,     1] loss: 1035.752
[39,     1] loss: 991.894
[40,     1] loss: 1014.265
[41,     1] loss: 1003.911
[42,     1] loss: 1032.688
[43,     1] loss: 962.187
[44,     1] loss: 981.019
[45,     1] loss: 935.175
[46,     1] loss: 945.512
[47,     1] loss: 932.470
[48,     1] loss: 932.836
[49,     1] loss: 961.303
[50,     1] loss: 900.941
[51,     1] loss: 863.792
[52,     1] loss: 866.963
[53,     1] loss: 960.152
[54,     1] loss: 1314.541
[55,     1] loss: 901.346
[56,     1] loss: 1084.815
[57,     1] loss: 1016.054
[58,     1] loss: 1042.166
[59,     1] loss: 1012.667
[60,     1] loss: 977.822
[61,     1] loss: 975.751
[62,     1] loss: 958.620
[63,     1] loss: 962.370
[64,     1] loss: 922.774
[65,     1] loss: 885.331
[66,     1] loss: 871.126
[67,     1] loss: 893.512
[68,     1] loss: 903.845
[69,     1] loss: 882.116
[70,     1] loss: 897.791
[71,     1] loss: 892.557
[72,     1] loss: 881.909
[73,     1] loss: 983.022
[74,     1] loss: 943.602
[75,     1] loss: 839.829
[76,     1] loss: 936.235
[77,     1] loss: 867.168
[78,     1] loss: 827.162
[79,     1] loss: 824.826
[80,     1] loss: 851.947
[81,     1] loss: 818.472
[82,     1] loss: 861.058
[83,     1] loss: 1380.451
[84,     1] loss: 958.872
[85,     1] loss: 1002.922
[86,     1] loss: 1023.839
[87,     1] loss: 985.374
[88,     1] loss: 963.248
Early stopping applied (best metric=0.7819604873657227)
Finished Training
Total time taken: 14.77031397819519
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.680
[2,     1] loss: 1287.828
[3,     1] loss: 1285.461
[4,     1] loss: 1286.418
[5,     1] loss: 1281.600
[6,     1] loss: 1280.242
[7,     1] loss: 1274.298
[8,     1] loss: 1266.045
[9,     1] loss: 1245.691
[10,     1] loss: 1212.830
[11,     1] loss: 1177.547
[12,     1] loss: 1142.866
[13,     1] loss: 1153.843
[14,     1] loss: 1060.977
[15,     1] loss: 1142.057
[16,     1] loss: 1090.027
[17,     1] loss: 1094.781
[18,     1] loss: 1043.337
[19,     1] loss: 1046.451
[20,     1] loss: 1029.967
[21,     1] loss: 1040.031
[22,     1] loss: 1021.937
[23,     1] loss: 1017.684
[24,     1] loss: 1006.588
[25,     1] loss: 1016.849
[26,     1] loss: 1011.041
[27,     1] loss: 1004.195
[28,     1] loss: 1007.026
[29,     1] loss: 1036.791
[30,     1] loss: 1003.260
[31,     1] loss: 973.961
[32,     1] loss: 944.545
[33,     1] loss: 946.482
[34,     1] loss: 924.974
[35,     1] loss: 892.686
[36,     1] loss: 907.084
[37,     1] loss: 929.806
[38,     1] loss: 915.688
[39,     1] loss: 896.615
[40,     1] loss: 916.939
[41,     1] loss: 871.912
[42,     1] loss: 881.058
[43,     1] loss: 1044.722
[44,     1] loss: 1214.872
[45,     1] loss: 952.309
[46,     1] loss: 996.649
[47,     1] loss: 1035.589
[48,     1] loss: 1025.469
[49,     1] loss: 1030.296
[50,     1] loss: 983.260
[51,     1] loss: 989.544
[52,     1] loss: 958.339
[53,     1] loss: 958.238
[54,     1] loss: 948.782
[55,     1] loss: 856.795
[56,     1] loss: 890.801
[57,     1] loss: 895.976
[58,     1] loss: 907.121
[59,     1] loss: 889.403
[60,     1] loss: 851.464
Early stopping applied (best metric=0.8308447003364563)
Finished Training
Total time taken: 10.005210638046265
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.338
[2,     1] loss: 1291.121
[3,     1] loss: 1283.471
[4,     1] loss: 1281.715
[5,     1] loss: 1282.773
[6,     1] loss: 1278.161
[7,     1] loss: 1283.225
[8,     1] loss: 1276.973
[9,     1] loss: 1271.857
[10,     1] loss: 1258.106
[11,     1] loss: 1250.951
[12,     1] loss: 1215.640
[13,     1] loss: 1193.778
[14,     1] loss: 1207.983
[15,     1] loss: 1130.172
[16,     1] loss: 1172.177
[17,     1] loss: 1131.698
[18,     1] loss: 1095.855
[19,     1] loss: 1066.486
[20,     1] loss: 1021.981
[21,     1] loss: 1055.270
[22,     1] loss: 1031.710
[23,     1] loss: 1101.772
[24,     1] loss: 1016.050
[25,     1] loss: 990.346
[26,     1] loss: 982.620
[27,     1] loss: 1044.651
[28,     1] loss: 942.008
[29,     1] loss: 1013.786
[30,     1] loss: 1000.016
[31,     1] loss: 991.171
[32,     1] loss: 1015.631
[33,     1] loss: 948.953
[34,     1] loss: 952.910
[35,     1] loss: 945.532
[36,     1] loss: 983.178
[37,     1] loss: 870.621
[38,     1] loss: 906.889
[39,     1] loss: 852.291
[40,     1] loss: 928.749
[41,     1] loss: 911.234
[42,     1] loss: 913.036
[43,     1] loss: 822.589
[44,     1] loss: 813.339
[45,     1] loss: 880.161
[46,     1] loss: 1010.890
[47,     1] loss: 1184.189
[48,     1] loss: 874.561
[49,     1] loss: 1005.086
[50,     1] loss: 984.037
[51,     1] loss: 911.720
[52,     1] loss: 974.202
[53,     1] loss: 958.658
[54,     1] loss: 912.678
[55,     1] loss: 946.882
[56,     1] loss: 922.294
[57,     1] loss: 864.866
[58,     1] loss: 876.242
Early stopping applied (best metric=0.8720905780792236)
Finished Training
Total time taken: 7.8581671714782715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.339
[2,     1] loss: 1284.033
[3,     1] loss: 1285.661
[4,     1] loss: 1288.383
[5,     1] loss: 1280.751
[6,     1] loss: 1285.441
[7,     1] loss: 1280.509
[8,     1] loss: 1280.053
[9,     1] loss: 1275.664
[10,     1] loss: 1267.362
[11,     1] loss: 1253.703
[12,     1] loss: 1229.812
[13,     1] loss: 1191.568
[14,     1] loss: 1178.095
[15,     1] loss: 1126.882
[16,     1] loss: 1134.692
[17,     1] loss: 1126.081
[18,     1] loss: 1068.107
[19,     1] loss: 1130.082
[20,     1] loss: 1052.421
[21,     1] loss: 1036.392
[22,     1] loss: 1046.081
[23,     1] loss: 1011.049
[24,     1] loss: 1009.669
[25,     1] loss: 967.951
[26,     1] loss: 977.737
[27,     1] loss: 972.608
[28,     1] loss: 940.534
[29,     1] loss: 974.573
[30,     1] loss: 1135.924
[31,     1] loss: 979.441
[32,     1] loss: 1033.560
[33,     1] loss: 993.611
[34,     1] loss: 984.996
[35,     1] loss: 982.802
[36,     1] loss: 961.496
[37,     1] loss: 940.725
[38,     1] loss: 935.831
[39,     1] loss: 958.971
[40,     1] loss: 905.777
[41,     1] loss: 946.957
[42,     1] loss: 870.348
[43,     1] loss: 937.617
[44,     1] loss: 821.376
[45,     1] loss: 962.272
[46,     1] loss: 894.650
[47,     1] loss: 835.936
[48,     1] loss: 855.932
[49,     1] loss: 837.916
[50,     1] loss: 785.147
[51,     1] loss: 792.349
[52,     1] loss: 949.961
[53,     1] loss: 1536.366
[54,     1] loss: 777.858
[55,     1] loss: 1146.072
[56,     1] loss: 1052.749
[57,     1] loss: 1024.851
[58,     1] loss: 1048.514
[59,     1] loss: 1116.312
[60,     1] loss: 1073.240
[61,     1] loss: 1018.873
[62,     1] loss: 998.092
[63,     1] loss: 1003.565
[64,     1] loss: 1034.191
[65,     1] loss: 1014.731
[66,     1] loss: 978.394
[67,     1] loss: 971.234
[68,     1] loss: 916.951
[69,     1] loss: 930.488
Early stopping applied (best metric=0.7674234509468079)
Finished Training
Total time taken: 11.35423994064331
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.408
[2,     1] loss: 1288.150
[3,     1] loss: 1284.875
[4,     1] loss: 1284.160
[5,     1] loss: 1283.359
[6,     1] loss: 1283.462
[7,     1] loss: 1280.667
[8,     1] loss: 1280.845
[9,     1] loss: 1275.890
[10,     1] loss: 1268.939
[11,     1] loss: 1250.754
[12,     1] loss: 1231.781
[13,     1] loss: 1205.178
[14,     1] loss: 1174.479
[15,     1] loss: 1150.396
[16,     1] loss: 1104.040
[17,     1] loss: 1101.440
[18,     1] loss: 1050.185
[19,     1] loss: 1091.228
[20,     1] loss: 1095.612
[21,     1] loss: 1048.021
[22,     1] loss: 1065.755
[23,     1] loss: 1013.660
[24,     1] loss: 1074.357
[25,     1] loss: 1000.465
[26,     1] loss: 1014.566
[27,     1] loss: 1002.125
[28,     1] loss: 977.862
[29,     1] loss: 983.273
[30,     1] loss: 969.130
[31,     1] loss: 921.119
[32,     1] loss: 968.367
[33,     1] loss: 977.585
[34,     1] loss: 987.448
[35,     1] loss: 930.375
[36,     1] loss: 917.664
[37,     1] loss: 933.949
[38,     1] loss: 953.456
[39,     1] loss: 885.669
[40,     1] loss: 942.235
[41,     1] loss: 923.824
[42,     1] loss: 884.225
[43,     1] loss: 932.167
[44,     1] loss: 853.034
[45,     1] loss: 850.375
[46,     1] loss: 864.599
[47,     1] loss: 853.168
[48,     1] loss: 811.752
[49,     1] loss: 827.498
[50,     1] loss: 879.960
[51,     1] loss: 1598.000
[52,     1] loss: 862.055
[53,     1] loss: 996.760
[54,     1] loss: 1034.976
[55,     1] loss: 993.957
[56,     1] loss: 993.935
[57,     1] loss: 1047.440
[58,     1] loss: 1003.744
[59,     1] loss: 969.054
[60,     1] loss: 970.831
[61,     1] loss: 930.118
Early stopping applied (best metric=0.8217053413391113)
Finished Training
Total time taken: 8.227172136306763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1287.478
[2,     1] loss: 1286.336
[3,     1] loss: 1288.073
[4,     1] loss: 1286.570
[5,     1] loss: 1285.313
[6,     1] loss: 1285.929
[7,     1] loss: 1287.830
[8,     1] loss: 1283.036
[9,     1] loss: 1284.739
[10,     1] loss: 1283.046
[11,     1] loss: 1277.774
[12,     1] loss: 1268.876
[13,     1] loss: 1250.640
[14,     1] loss: 1227.047
[15,     1] loss: 1182.932
[16,     1] loss: 1147.490
[17,     1] loss: 1145.185
[18,     1] loss: 1089.810
[19,     1] loss: 1068.019
[20,     1] loss: 1098.510
[21,     1] loss: 1028.156
[22,     1] loss: 1023.206
[23,     1] loss: 994.220
[24,     1] loss: 1005.429
[25,     1] loss: 1011.964
[26,     1] loss: 989.253
[27,     1] loss: 1029.569
[28,     1] loss: 1001.257
[29,     1] loss: 950.887
[30,     1] loss: 922.327
[31,     1] loss: 939.413
[32,     1] loss: 901.704
[33,     1] loss: 1015.382
[34,     1] loss: 957.196
[35,     1] loss: 906.065
[36,     1] loss: 931.280
[37,     1] loss: 939.948
[38,     1] loss: 929.642
[39,     1] loss: 896.066
[40,     1] loss: 886.437
[41,     1] loss: 911.850
[42,     1] loss: 823.642
[43,     1] loss: 843.515
[44,     1] loss: 839.367
[45,     1] loss: 804.832
[46,     1] loss: 835.491
[47,     1] loss: 1006.483
[48,     1] loss: 1244.335
[49,     1] loss: 899.574
[50,     1] loss: 1040.600
[51,     1] loss: 1056.471
[52,     1] loss: 988.298
[53,     1] loss: 998.148
[54,     1] loss: 1008.681
[55,     1] loss: 956.796
[56,     1] loss: 913.197
[57,     1] loss: 1018.091
[58,     1] loss: 955.035
[59,     1] loss: 913.246
[60,     1] loss: 891.352
[61,     1] loss: 968.663
Early stopping applied (best metric=0.804141640663147)
Finished Training
Total time taken: 10.113214492797852
{'Hydroxylation-K Validation Accuracy': 0.7872340425531915, 'Hydroxylation-K Validation Sensitivity': 0.634074074074074, 'Hydroxylation-K Validation Specificity': 0.8263157894736842, 'Hydroxylation-K Validation Precision': 0.49523024361259654, 'Hydroxylation-K AUC ROC': 0.7965107212475634, 'Hydroxylation-K AUC PR': 0.6144667512782191, 'Hydroxylation-K MCC': 0.4250653449927731, 'Hydroxylation-K F1': 0.5481728416511025, 'Validation Loss (Hydroxylation-K)': 0.44687891205151875, 'Hydroxylation-P Validation Accuracy': 0.7942557738185879, 'Hydroxylation-P Validation Sensitivity': 0.7595238095238095, 'Hydroxylation-P Validation Specificity': 0.8017606863185196, 'Hydroxylation-P Validation Precision': 0.4601548885281792, 'Hydroxylation-P AUC ROC': 0.8441114942992842, 'Hydroxylation-P AUC PR': 0.5747580056715051, 'Hydroxylation-P MCC': 0.47315020258717083, 'Hydroxylation-P F1': 0.5695994518907993, 'Validation Loss (Hydroxylation-P)': 0.37891329725583395, 'Validation Loss (total)': 0.8257922093073528, 'TimeToTrain': 10.196148554484049}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005424743412782502,
 'learning_rate_Hydroxylation-K': 0.008688366903252538,
 'learning_rate_Hydroxylation-P': 0.00046411871870144484,
 'log_base': 1.1928724862596871,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3683987846,
 'sample_weights': [1.7856689864439415, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.601875272172879,
 'weight_decay_Hydroxylation-K': 4.126933490753077,
 'weight_decay_Hydroxylation-P': 3.3635108222408636}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3060.625
[2,     1] loss: 3074.653
[3,     1] loss: 3071.694
[4,     1] loss: 3070.902
[5,     1] loss: 3071.622
[6,     1] loss: 3057.265
[7,     1] loss: 3057.498
[8,     1] loss: 3021.682
[9,     1] loss: 3014.880
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00958521846165481,
 'learning_rate_Hydroxylation-K': 0.0005139780807979377,
 'learning_rate_Hydroxylation-P': 0.002976932370525571,
 'log_base': 1.027546986674897,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1891800300,
 'sample_weights': [9.465881686125334, 1.183280793737419],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.233982574823536,
 'weight_decay_Hydroxylation-K': 9.91196416163981,
 'weight_decay_Hydroxylation-P': 7.184857529703336}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19985.166
Exploding loss, terminate run (best metric=1.0992087125778198)
Finished Training
Total time taken: 0.21800470352172852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19929.877
Exploding loss, terminate run (best metric=1.091922640800476)
Finished Training
Total time taken: 0.19600582122802734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19972.988
Exploding loss, terminate run (best metric=1.0924066305160522)
Finished Training
Total time taken: 0.20800256729125977
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20138.266
Exploding loss, terminate run (best metric=1.0731267929077148)
Finished Training
Total time taken: 0.20900416374206543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20049.879
Exploding loss, terminate run (best metric=1.1458261013031006)
Finished Training
Total time taken: 0.20000433921813965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19963.590
Exploding loss, terminate run (best metric=1.0991771221160889)
Finished Training
Total time taken: 0.21900439262390137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19999.869
Exploding loss, terminate run (best metric=1.0931875705718994)
Finished Training
Total time taken: 0.1970064640045166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20016.213
Exploding loss, terminate run (best metric=1.0905743837356567)
Finished Training
Total time taken: 0.2140059471130371
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20057.451
Exploding loss, terminate run (best metric=1.0768420696258545)
Finished Training
Total time taken: 0.22400283813476562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19948.785
Exploding loss, terminate run (best metric=1.0750526189804077)
Finished Training
Total time taken: 0.20900249481201172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19897.654
Exploding loss, terminate run (best metric=1.0951366424560547)
Finished Training
Total time taken: 0.19800615310668945
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20097.145
Exploding loss, terminate run (best metric=1.172461748123169)
Finished Training
Total time taken: 0.21500492095947266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19988.863
Exploding loss, terminate run (best metric=1.0948643684387207)
Finished Training
Total time taken: 0.19800662994384766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19918.068
Exploding loss, terminate run (best metric=1.08951735496521)
Finished Training
Total time taken: 0.21200346946716309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20018.209
Exploding loss, terminate run (best metric=1.0778656005859375)
Finished Training
Total time taken: 0.2180042266845703
{'Hydroxylation-K Validation Accuracy': 0.37222222222222223, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.2771929824561403, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6135672514619883, 'Hydroxylation-K AUC PR': 0.3182003027445764, 'Hydroxylation-K MCC': 0.012926112238824687, 'Hydroxylation-K F1': 0.250846280156625, 'Validation Loss (Hydroxylation-K)': 0.562073798974355, 'Hydroxylation-P Validation Accuracy': 0.36269238448132923, 'Hydroxylation-P Validation Sensitivity': 0.7276190476190476, 'Hydroxylation-P Validation Specificity': 0.283739837398374, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5546158548461032, 'Hydroxylation-P AUC PR': 0.2531923712444527, 'Hydroxylation-P MCC': 0.010337726360815275, 'Hydroxylation-P F1': 0.22336422938251219, 'Validation Loss (Hydroxylation-P)': 0.5357375423113505, 'Validation Loss (total)': 1.0978113571802774, 'TimeToTrain': 0.20900460879007976}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006225261495715151,
 'learning_rate_Hydroxylation-K': 0.0014575285012643551,
 'learning_rate_Hydroxylation-P': 0.00461218133339137,
 'log_base': 2.241000671422899,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1168620419,
 'sample_weights': [61.47998912146488, 7.669030570796696],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.111810536866446,
 'weight_decay_Hydroxylation-K': 5.9770327794322,
 'weight_decay_Hydroxylation-P': 0.40754473797950197}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1350.666
[2,     1] loss: 1340.684
[3,     1] loss: 1351.185
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008026741099970891,
 'learning_rate_Hydroxylation-K': 0.009885182164507539,
 'learning_rate_Hydroxylation-P': 0.0009354250755488542,
 'log_base': 1.7512592062932626,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1380127472,
 'sample_weights': [2.0689014855242256, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5382869162754653,
 'weight_decay_Hydroxylation-K': 0.4963343501118489,
 'weight_decay_Hydroxylation-P': 2.9713219534112043}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1537.663
[2,     1] loss: 1535.305
[3,     1] loss: 1538.552
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009513998386428388,
 'learning_rate_Hydroxylation-K': 0.0011334669930160237,
 'learning_rate_Hydroxylation-P': 0.00586048723840432,
 'log_base': 1.578560729429456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2367925057,
 'sample_weights': [2.979365775949368, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.68777954988343,
 'weight_decay_Hydroxylation-K': 9.594621318833559,
 'weight_decay_Hydroxylation-P': 3.190707817397973}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1684.865
[2,     1] loss: 1684.535
[3,     1] loss: 1679.647
[4,     1] loss: 1678.995
[5,     1] loss: 1674.636
[6,     1] loss: 1685.353
[7,     1] loss: 1682.292
[8,     1] loss: 1676.187
[9,     1] loss: 1678.963
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005671954161910058,
 'learning_rate_Hydroxylation-K': 0.002075002424352407,
 'learning_rate_Hydroxylation-P': 0.0008433141900691123,
 'log_base': 1.0305760852198664,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1682456764,
 'sample_weights': [3.656941455671598, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.564106263117157,
 'weight_decay_Hydroxylation-K': 8.963481142486994,
 'weight_decay_Hydroxylation-P': 4.9725065026315605}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18070.160
Exploding loss, terminate run (best metric=1.1066404581069946)
Finished Training
Total time taken: 0.1990041732788086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18187.924
Exploding loss, terminate run (best metric=1.0919182300567627)
Finished Training
Total time taken: 0.21000361442565918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17863.363
Exploding loss, terminate run (best metric=1.109375238418579)
Finished Training
Total time taken: 0.2100050449371338
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17950.008
Exploding loss, terminate run (best metric=1.0736815929412842)
Finished Training
Total time taken: 0.22600579261779785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18115.045
Exploding loss, terminate run (best metric=1.0764458179473877)
Finished Training
Total time taken: 0.22100543975830078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18009.574
Exploding loss, terminate run (best metric=1.1100618839263916)
Finished Training
Total time taken: 0.20200443267822266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18107.008
Exploding loss, terminate run (best metric=1.0922439098358154)
Finished Training
Total time taken: 0.20500516891479492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17972.959
Exploding loss, terminate run (best metric=1.096623420715332)
Finished Training
Total time taken: 0.21700239181518555
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17889.311
Exploding loss, terminate run (best metric=1.1148991584777832)
Finished Training
Total time taken: 0.20200586318969727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17945.215
Exploding loss, terminate run (best metric=1.0805094242095947)
Finished Training
Total time taken: 0.21700286865234375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17998.732
Exploding loss, terminate run (best metric=1.0984678268432617)
Finished Training
Total time taken: 0.22000432014465332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17951.900
Exploding loss, terminate run (best metric=1.091365933418274)
Finished Training
Total time taken: 0.19800376892089844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17981.047
Exploding loss, terminate run (best metric=1.0942752361297607)
Finished Training
Total time taken: 0.22300457954406738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18013.979
Exploding loss, terminate run (best metric=1.0772082805633545)
Finished Training
Total time taken: 0.22300434112548828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18003.387
Exploding loss, terminate run (best metric=1.1088991165161133)
Finished Training
Total time taken: 0.19900274276733398
{'Hydroxylation-K Validation Accuracy': 0.5994680851063829, 'Hydroxylation-K Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-K Validation Specificity': 0.6666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6578752436647174, 'Hydroxylation-K AUC PR': 0.3505367029585178, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.11182266009852218, 'Validation Loss (Hydroxylation-K)': 0.5595197121302287, 'Hydroxylation-P Validation Accuracy': 0.6071568617498265, 'Hydroxylation-P Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-P Validation Specificity': 0.6666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.62370929318408, 'Hydroxylation-P AUC PR': 0.29616928779192137, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.0998006920753702, 'Validation Loss (Hydroxylation-P)': 0.535321315129598, 'Validation Loss (total)': 1.0948410352071127, 'TimeToTrain': 0.21147096951802571}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006501310932899272,
 'learning_rate_Hydroxylation-K': 0.0037145779674559246,
 'learning_rate_Hydroxylation-P': 0.0017356935929421012,
 'log_base': 1.3940872038580177,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3240367081,
 'sample_weights': [55.47128641842695, 6.9195033607416425],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.819015605666277,
 'weight_decay_Hydroxylation-K': 9.240714254228049,
 'weight_decay_Hydroxylation-P': 5.483479002890761}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1965.861
[2,     1] loss: 1966.906
[3,     1] loss: 1985.807
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068432289278885965,
 'learning_rate_Hydroxylation-K': 0.00271151420198802,
 'learning_rate_Hydroxylation-P': 0.0008979122013575008,
 'log_base': 1.159235484689448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 462987058,
 'sample_weights': [5.024812834000258, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.10300451478853,
 'weight_decay_Hydroxylation-K': 8.431666895249831,
 'weight_decay_Hydroxylation-P': 4.596150141746267}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3651.983
[2,     1] loss: 3680.782
[3,     1] loss: 3668.572
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003379405226369534,
 'learning_rate_Hydroxylation-K': 0.004832013786670589,
 'learning_rate_Hydroxylation-P': 0.004663813906554088,
 'log_base': 2.609128498499676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4093883901,
 'sample_weights': [11.298287626581722, 1.4123403602488538],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.489904734628533,
 'weight_decay_Hydroxylation-K': 7.948681552357103,
 'weight_decay_Hydroxylation-P': 5.191804972965316}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.363
[2,     1] loss: 1280.698
[3,     1] loss: 1272.682
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008123265171447463,
 'learning_rate_Hydroxylation-K': 0.0006223734367245594,
 'learning_rate_Hydroxylation-P': 0.0006896030723707283,
 'log_base': 1.6197862525149855,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 621499488,
 'sample_weights': [1.7407871194241256, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.942687690521509,
 'weight_decay_Hydroxylation-K': 8.562672667653493,
 'weight_decay_Hydroxylation-P': 8.101451079579023}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1646.340
[2,     1] loss: 1631.771
[3,     1] loss: 1645.024
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009206416125099282,
 'learning_rate_Hydroxylation-K': 0.0005720992648121169,
 'learning_rate_Hydroxylation-P': 0.004063925194047715,
 'log_base': 1.2015463677964309,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1342771273,
 'sample_weights': [3.4614622269316495, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.331170930853897,
 'weight_decay_Hydroxylation-K': 9.674975961205499,
 'weight_decay_Hydroxylation-P': 7.68657003220748}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2959.058
[2,     1] loss: 2976.451
[3,     1] loss: 2952.088
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003042497898673243,
 'learning_rate_Hydroxylation-K': 0.003021497733123867,
 'learning_rate_Hydroxylation-P': 0.0072232200420901255,
 'log_base': 2.2935955728279716,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1872205773,
 'sample_weights': [9.092363719408358, 1.1365892492212986],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.218094549915506,
 'weight_decay_Hydroxylation-K': 8.410626420645558,
 'weight_decay_Hydroxylation-P': 1.2104796904720725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.629
[2,     1] loss: 1331.246
[3,     1] loss: 1326.158
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054981456044881066,
 'learning_rate_Hydroxylation-K': 0.002003521260092034,
 'learning_rate_Hydroxylation-P': 0.002108371443963866,
 'log_base': 1.01120421106352,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3190201789,
 'sample_weights': [2.0110848177270157, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.572376578010415,
 'weight_decay_Hydroxylation-K': 7.204943211434629,
 'weight_decay_Hydroxylation-P': 6.92824684894393}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48929.074
Exploding loss, terminate run (best metric=1.097729206085205)
Finished Training
Total time taken: 0.22400474548339844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48896.617
Exploding loss, terminate run (best metric=1.0916874408721924)
Finished Training
Total time taken: 0.1990034580230713
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48652.047
Exploding loss, terminate run (best metric=1.0954748392105103)
Finished Training
Total time taken: 0.21300411224365234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48720.391
Exploding loss, terminate run (best metric=1.0898070335388184)
Finished Training
Total time taken: 0.2030038833618164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 48485.195
Exploding loss, terminate run (best metric=1.0848095417022705)
Finished Training
Total time taken: 0.20500636100769043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48688.578
Exploding loss, terminate run (best metric=1.096835732460022)
Finished Training
Total time taken: 0.22400331497192383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48545.547
Exploding loss, terminate run (best metric=1.0920450687408447)
Finished Training
Total time taken: 0.22000408172607422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48499.852
Exploding loss, terminate run (best metric=1.1135010719299316)
Finished Training
Total time taken: 0.1990041732788086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48813.875
Exploding loss, terminate run (best metric=1.0732908248901367)
Finished Training
Total time taken: 0.21700477600097656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 48795.184
Exploding loss, terminate run (best metric=1.072414517402649)
Finished Training
Total time taken: 0.2310047149658203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48625.852
Exploding loss, terminate run (best metric=1.0962960720062256)
Finished Training
Total time taken: 0.21500349044799805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48570.672
Exploding loss, terminate run (best metric=1.1009654998779297)
Finished Training
Total time taken: 0.19600415229797363
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48755.930
Exploding loss, terminate run (best metric=1.0927305221557617)
Finished Training
Total time taken: 0.22600436210632324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48525.039
Exploding loss, terminate run (best metric=1.0771780014038086)
Finished Training
Total time taken: 0.19800543785095215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 48585.742
Exploding loss, terminate run (best metric=1.0724952220916748)
Finished Training
Total time taken: 0.20900559425354004
{'Hydroxylation-K Validation Accuracy': 0.36648936170212765, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.2754385964912281, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6450487329434698, 'Hydroxylation-K AUC PR': 0.35137693436615897, 'Hydroxylation-K MCC': 0.011194341570991124, 'Hydroxylation-K F1': 0.24717473196175024, 'Validation Loss (Hydroxylation-K)': 0.5584972302118937, 'Hydroxylation-P Validation Accuracy': 0.3525461651692807, 'Hydroxylation-P Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-P Validation Specificity': 0.2707317073170732, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6135624466089826, 'Hydroxylation-P AUC PR': 0.32168182575198045, 'Hydroxylation-P MCC': 0.0070841857007037596, 'Hydroxylation-P F1': 0.22131223605999922, 'Validation Loss (Hydroxylation-P)': 0.5313201427459717, 'Validation Loss (total)': 1.0898173729578653, 'TimeToTrain': 0.21193777720133464}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007351027961436939,
 'learning_rate_Hydroxylation-K': 0.006099667013582993,
 'learning_rate_Hydroxylation-P': 0.0005002877410009436,
 'log_base': 2.90836745509028,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 249959540,
 'sample_weights': [149.94571972347768, 18.704269876293434],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.087370267787028,
 'weight_decay_Hydroxylation-K': 9.670555804973503,
 'weight_decay_Hydroxylation-P': 3.10509585905322}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.849
[2,     1] loss: 1246.773
[3,     1] loss: 1239.497
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003170677796688293,
 'learning_rate_Hydroxylation-K': 0.00040764504172792373,
 'learning_rate_Hydroxylation-P': 0.0012645035551991193,
 'log_base': 1.048537070568546,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2327745715,
 'sample_weights': [1.5637465299546833, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.619970903505912,
 'weight_decay_Hydroxylation-K': 8.608666093176588,
 'weight_decay_Hydroxylation-P': 3.070238217281393}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11441.522
[2,     1] loss: 11508.872
[3,     1] loss: 11404.378
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005046003816936063,
 'learning_rate_Hydroxylation-K': 0.0011087051032000685,
 'learning_rate_Hydroxylation-P': 0.0020701474658570748,
 'log_base': 1.049996189304445,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 677389083,
 'sample_weights': [35.22334658412585, 4.40308794111002],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.957408497160709,
 'weight_decay_Hydroxylation-K': 6.892371584633742,
 'weight_decay_Hydroxylation-P': 7.044297311186646}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11146.131
[2,     1] loss: 11117.826
[3,     1] loss: 11130.019
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002825399694498901,
 'learning_rate_Hydroxylation-K': 0.00859507032542467,
 'learning_rate_Hydroxylation-P': 0.00748805274027769,
 'log_base': 2.193800836197316,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3794068788,
 'sample_weights': [34.21934248566292, 4.277582594018456],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.781558358176583,
 'weight_decay_Hydroxylation-K': 4.065390055245522,
 'weight_decay_Hydroxylation-P': 6.814936781618064}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1352.953
[2,     1] loss: 1353.967
[3,     1] loss: 1358.397
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 3.296561062010638e-05,
 'learning_rate_Hydroxylation-K': 0.0015276221548571089,
 'learning_rate_Hydroxylation-P': 0.001947475690198062,
 'log_base': 1.9840225569146615,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3314062195,
 'sample_weights': [2.1249586799022686, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.3144648417424145,
 'weight_decay_Hydroxylation-K': 3.277496013192028,
 'weight_decay_Hydroxylation-P': 0.06129564581630134}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.560
[2,     1] loss: 1424.415
[3,     1] loss: 1426.141
[4,     1] loss: 1419.973
[5,     1] loss: 1426.196
[6,     1] loss: 1431.959
[7,     1] loss: 1424.103
[8,     1] loss: 1420.045
[9,     1] loss: 1421.176
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002887619918819905,
 'learning_rate_Hydroxylation-K': 0.0033819596654693974,
 'learning_rate_Hydroxylation-P': 0.00445256664509172,
 'log_base': 1.288859571564794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1776113226,
 'sample_weights': [2.436693726176028, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.26881710624813,
 'weight_decay_Hydroxylation-K': 7.387153743818798,
 'weight_decay_Hydroxylation-P': 4.214049767441868}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2289.598
[2,     1] loss: 2307.006
[3,     1] loss: 2310.145
[4,     1] loss: 2305.752
[5,     1] loss: 2290.766
[6,     1] loss: 2291.877
[7,     1] loss: 2294.993
[8,     1] loss: 2292.940
[9,     1] loss: 2285.596
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005925222247225727,
 'learning_rate_Hydroxylation-K': 0.005309340751930979,
 'learning_rate_Hydroxylation-P': 0.009899764985548664,
 'log_base': 2.098708961315805,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2599686133,
 'sample_weights': [6.578884733335283, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.571064686543192,
 'weight_decay_Hydroxylation-K': 4.808066851542686,
 'weight_decay_Hydroxylation-P': 5.627249072675998}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.938
[2,     1] loss: 1386.413
[3,     1] loss: 1388.251
[4,     1] loss: 1384.176
[5,     1] loss: 1379.502
[6,     1] loss: 1383.088
[7,     1] loss: 1382.868
[8,     1] loss: 1383.125
[9,     1] loss: 1378.960
[10,     1] loss: 1382.236
[11,     1] loss: 1378.904
[12,     1] loss: 1379.927
[13,     1] loss: 1379.302
[14,     1] loss: 1369.263
[15,     1] loss: 1363.359
[16,     1] loss: 1348.292
[17,     1] loss: 1317.633
[18,     1] loss: 1294.809
[19,     1] loss: 1259.113
[20,     1] loss: 1222.284
[21,     1] loss: 1217.261
[22,     1] loss: 1178.675
[23,     1] loss: 1121.349
[24,     1] loss: 1126.847
[25,     1] loss: 1141.863
[26,     1] loss: 1127.408
[27,     1] loss: 1133.580
[28,     1] loss: 1116.276
[29,     1] loss: 1084.769
[30,     1] loss: 1080.171
[31,     1] loss: 1141.501
[32,     1] loss: 1068.385
[33,     1] loss: 1088.190
[34,     1] loss: 1062.275
[35,     1] loss: 1007.718
[36,     1] loss: 1150.194
[37,     1] loss: 1016.427
[38,     1] loss: 1034.007
[39,     1] loss: 1001.594
[40,     1] loss: 996.618
[41,     1] loss: 987.151
[42,     1] loss: 940.007
[43,     1] loss: 918.608
[44,     1] loss: 957.394
[45,     1] loss: 955.981
[46,     1] loss: 884.416
[47,     1] loss: 916.296
[48,     1] loss: 908.532
[49,     1] loss: 878.276
[50,     1] loss: 919.207
[51,     1] loss: 871.415
[52,     1] loss: 808.794
[53,     1] loss: 844.852
[54,     1] loss: 948.154
[55,     1] loss: 1026.771
[56,     1] loss: 895.311
[57,     1] loss: 912.648
[58,     1] loss: 900.060
[59,     1] loss: 946.174
[60,     1] loss: 861.806
[61,     1] loss: 865.426
[62,     1] loss: 824.252
[63,     1] loss: 840.786
[64,     1] loss: 929.473
[65,     1] loss: 914.296
[66,     1] loss: 841.367
[67,     1] loss: 869.813
[68,     1] loss: 755.086
[69,     1] loss: 754.812
[70,     1] loss: 902.176
[71,     1] loss: 1174.255
[72,     1] loss: 870.506
Early stopping applied (best metric=0.7980517745018005)
Finished Training
Total time taken: 9.787206888198853
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1386.523
[2,     1] loss: 1386.726
[3,     1] loss: 1384.382
[4,     1] loss: 1382.944
[5,     1] loss: 1382.269
[6,     1] loss: 1381.954
[7,     1] loss: 1379.606
[8,     1] loss: 1375.954
[9,     1] loss: 1374.130
[10,     1] loss: 1363.290
[11,     1] loss: 1350.067
[12,     1] loss: 1328.115
[13,     1] loss: 1291.875
[14,     1] loss: 1249.672
[15,     1] loss: 1235.307
[16,     1] loss: 1206.697
[17,     1] loss: 1189.845
[18,     1] loss: 1195.143
[19,     1] loss: 1127.229
[20,     1] loss: 1171.727
[21,     1] loss: 1147.858
[22,     1] loss: 1101.923
[23,     1] loss: 1083.846
[24,     1] loss: 1100.770
[25,     1] loss: 1115.388
[26,     1] loss: 1050.371
[27,     1] loss: 1134.871
[28,     1] loss: 1014.987
[29,     1] loss: 1014.702
[30,     1] loss: 1050.924
[31,     1] loss: 998.521
[32,     1] loss: 984.726
[33,     1] loss: 956.446
[34,     1] loss: 988.639
[35,     1] loss: 1017.640
[36,     1] loss: 980.119
[37,     1] loss: 1038.268
[38,     1] loss: 942.596
[39,     1] loss: 1023.521
[40,     1] loss: 999.474
[41,     1] loss: 917.778
[42,     1] loss: 908.719
[43,     1] loss: 906.316
[44,     1] loss: 884.945
[45,     1] loss: 951.948
[46,     1] loss: 1235.869
[47,     1] loss: 918.161
[48,     1] loss: 1003.493
[49,     1] loss: 951.389
[50,     1] loss: 971.006
[51,     1] loss: 971.026
[52,     1] loss: 980.940
[53,     1] loss: 962.521
[54,     1] loss: 910.114
[55,     1] loss: 878.927
[56,     1] loss: 848.835
[57,     1] loss: 834.442
[58,     1] loss: 875.181
[59,     1] loss: 866.896
[60,     1] loss: 907.817
[61,     1] loss: 854.857
[62,     1] loss: 822.485
[63,     1] loss: 786.671
[64,     1] loss: 802.481
[65,     1] loss: 878.602
[66,     1] loss: 1167.299
[67,     1] loss: 1180.693
[68,     1] loss: 1064.189
[69,     1] loss: 968.573
[70,     1] loss: 1141.497
[71,     1] loss: 1083.990
[72,     1] loss: 957.949
[73,     1] loss: 1009.335
[74,     1] loss: 1039.985
[75,     1] loss: 967.593
[76,     1] loss: 954.779
[77,     1] loss: 1001.045
[78,     1] loss: 1009.616
[79,     1] loss: 938.957
Early stopping applied (best metric=0.776801347732544)
Finished Training
Total time taken: 13.036277055740356
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.483
[2,     1] loss: 1384.236
[3,     1] loss: 1382.374
[4,     1] loss: 1381.205
[5,     1] loss: 1384.508
[6,     1] loss: 1380.813
[7,     1] loss: 1377.385
[8,     1] loss: 1376.630
[9,     1] loss: 1377.313
[10,     1] loss: 1377.505
[11,     1] loss: 1368.310
[12,     1] loss: 1355.896
[13,     1] loss: 1333.552
[14,     1] loss: 1301.401
[15,     1] loss: 1271.585
[16,     1] loss: 1238.330
[17,     1] loss: 1183.050
[18,     1] loss: 1209.427
[19,     1] loss: 1204.356
[20,     1] loss: 1122.215
[21,     1] loss: 1216.085
[22,     1] loss: 1143.219
[23,     1] loss: 1138.669
[24,     1] loss: 1186.904
[25,     1] loss: 1068.652
[26,     1] loss: 1138.183
[27,     1] loss: 1167.961
[28,     1] loss: 1086.414
[29,     1] loss: 1070.322
[30,     1] loss: 1088.563
[31,     1] loss: 1045.950
[32,     1] loss: 989.755
[33,     1] loss: 1040.713
[34,     1] loss: 994.856
[35,     1] loss: 1008.211
[36,     1] loss: 952.322
[37,     1] loss: 1010.577
[38,     1] loss: 991.013
[39,     1] loss: 919.475
[40,     1] loss: 943.084
[41,     1] loss: 984.131
[42,     1] loss: 924.978
[43,     1] loss: 955.839
[44,     1] loss: 999.149
[45,     1] loss: 910.614
[46,     1] loss: 882.926
[47,     1] loss: 882.247
[48,     1] loss: 857.548
[49,     1] loss: 853.132
[50,     1] loss: 866.990
[51,     1] loss: 1055.520
[52,     1] loss: 2158.107
[53,     1] loss: 1554.400
[54,     1] loss: 1197.553
[55,     1] loss: 1289.974
[56,     1] loss: 1306.683
[57,     1] loss: 1296.837
[58,     1] loss: 1285.156
[59,     1] loss: 1309.243
[60,     1] loss: 1311.751
[61,     1] loss: 1251.476
[62,     1] loss: 1242.064
[63,     1] loss: 1270.862
[64,     1] loss: 1237.689
[65,     1] loss: 1220.878
[66,     1] loss: 1224.608
Early stopping applied (best metric=0.8613077998161316)
Finished Training
Total time taken: 10.964232206344604
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.481
[2,     1] loss: 1385.100
[3,     1] loss: 1379.171
[4,     1] loss: 1381.176
[5,     1] loss: 1381.597
[6,     1] loss: 1385.646
[7,     1] loss: 1385.907
[8,     1] loss: 1378.298
[9,     1] loss: 1382.121
[10,     1] loss: 1377.261
[11,     1] loss: 1370.428
[12,     1] loss: 1360.027
[13,     1] loss: 1350.341
[14,     1] loss: 1316.238
[15,     1] loss: 1267.351
[16,     1] loss: 1258.997
[17,     1] loss: 1219.687
[18,     1] loss: 1205.711
[19,     1] loss: 1244.954
[20,     1] loss: 1123.326
[21,     1] loss: 1158.690
[22,     1] loss: 1126.103
[23,     1] loss: 1160.557
[24,     1] loss: 1126.193
[25,     1] loss: 1120.130
[26,     1] loss: 1117.038
[27,     1] loss: 1098.153
[28,     1] loss: 1116.253
[29,     1] loss: 1094.931
[30,     1] loss: 1076.514
[31,     1] loss: 1049.998
[32,     1] loss: 1026.703
[33,     1] loss: 992.454
[34,     1] loss: 1050.337
[35,     1] loss: 1005.484
[36,     1] loss: 976.307
[37,     1] loss: 1052.487
[38,     1] loss: 1110.859
[39,     1] loss: 1009.136
[40,     1] loss: 1029.635
[41,     1] loss: 1018.395
[42,     1] loss: 1007.501
[43,     1] loss: 979.754
[44,     1] loss: 958.782
[45,     1] loss: 918.960
[46,     1] loss: 994.894
[47,     1] loss: 863.150
[48,     1] loss: 884.595
[49,     1] loss: 992.188
[50,     1] loss: 946.211
[51,     1] loss: 863.340
[52,     1] loss: 943.560
[53,     1] loss: 873.622
[54,     1] loss: 924.067
[55,     1] loss: 906.513
[56,     1] loss: 837.691
[57,     1] loss: 930.816
[58,     1] loss: 1211.505
[59,     1] loss: 838.147
[60,     1] loss: 984.855
[61,     1] loss: 940.132
[62,     1] loss: 960.353
[63,     1] loss: 939.687
[64,     1] loss: 855.472
[65,     1] loss: 915.686
[66,     1] loss: 885.201
[67,     1] loss: 841.934
[68,     1] loss: 999.772
[69,     1] loss: 1135.243
[70,     1] loss: 915.506
[71,     1] loss: 973.733
[72,     1] loss: 950.000
Early stopping applied (best metric=0.7593399882316589)
Finished Training
Total time taken: 9.77220869064331
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1384.881
[2,     1] loss: 1393.026
[3,     1] loss: 1383.395
[4,     1] loss: 1388.689
[5,     1] loss: 1382.013
[6,     1] loss: 1382.478
[7,     1] loss: 1382.721
[8,     1] loss: 1380.874
[9,     1] loss: 1375.536
[10,     1] loss: 1368.844
[11,     1] loss: 1364.007
[12,     1] loss: 1341.334
[13,     1] loss: 1292.433
[14,     1] loss: 1288.890
[15,     1] loss: 1240.127
[16,     1] loss: 1204.168
[17,     1] loss: 1147.856
[18,     1] loss: 1155.444
[19,     1] loss: 1201.762
[20,     1] loss: 1215.443
[21,     1] loss: 1149.089
[22,     1] loss: 1207.619
[23,     1] loss: 1133.755
[24,     1] loss: 1161.884
[25,     1] loss: 1162.018
[26,     1] loss: 1140.898
[27,     1] loss: 1150.468
[28,     1] loss: 1134.446
[29,     1] loss: 1092.233
[30,     1] loss: 1088.842
[31,     1] loss: 1081.747
[32,     1] loss: 1054.882
[33,     1] loss: 1058.592
[34,     1] loss: 1100.488
[35,     1] loss: 1015.581
[36,     1] loss: 988.411
[37,     1] loss: 937.705
[38,     1] loss: 1043.657
[39,     1] loss: 943.019
[40,     1] loss: 973.880
[41,     1] loss: 1099.327
[42,     1] loss: 1026.259
[43,     1] loss: 1004.648
[44,     1] loss: 1048.110
[45,     1] loss: 995.877
[46,     1] loss: 1018.407
[47,     1] loss: 956.747
[48,     1] loss: 1053.090
[49,     1] loss: 911.868
[50,     1] loss: 979.757
[51,     1] loss: 903.403
[52,     1] loss: 945.730
[53,     1] loss: 906.144
[54,     1] loss: 948.422
[55,     1] loss: 922.748
[56,     1] loss: 856.161
[57,     1] loss: 918.062
[58,     1] loss: 870.107
[59,     1] loss: 818.237
[60,     1] loss: 838.485
[61,     1] loss: 873.738
[62,     1] loss: 1141.658
[63,     1] loss: 1191.062
[64,     1] loss: 939.751
[65,     1] loss: 1024.061
[66,     1] loss: 1067.968
[67,     1] loss: 996.742
[68,     1] loss: 987.620
[69,     1] loss: 963.549
[70,     1] loss: 936.010
[71,     1] loss: 954.826
[72,     1] loss: 975.338
[73,     1] loss: 930.112
[74,     1] loss: 903.715
[75,     1] loss: 968.856
[76,     1] loss: 835.381
Early stopping applied (best metric=0.7576819062232971)
Finished Training
Total time taken: 11.422240495681763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.795
[2,     1] loss: 1383.921
[3,     1] loss: 1382.147
[4,     1] loss: 1384.412
[5,     1] loss: 1380.258
[6,     1] loss: 1382.559
[7,     1] loss: 1380.951
[8,     1] loss: 1380.219
[9,     1] loss: 1381.602
[10,     1] loss: 1378.186
[11,     1] loss: 1375.077
[12,     1] loss: 1374.958
[13,     1] loss: 1363.557
[14,     1] loss: 1345.114
[15,     1] loss: 1317.926
[16,     1] loss: 1252.719
[17,     1] loss: 1225.547
[18,     1] loss: 1181.914
[19,     1] loss: 1141.369
[20,     1] loss: 1147.291
[21,     1] loss: 1240.334
[22,     1] loss: 1119.462
[23,     1] loss: 1205.221
[24,     1] loss: 1101.339
[25,     1] loss: 1148.380
[26,     1] loss: 1155.110
[27,     1] loss: 1125.476
[28,     1] loss: 1106.359
[29,     1] loss: 1103.134
[30,     1] loss: 1085.340
[31,     1] loss: 1019.912
[32,     1] loss: 1071.656
[33,     1] loss: 1038.029
[34,     1] loss: 977.786
[35,     1] loss: 1052.924
[36,     1] loss: 958.770
[37,     1] loss: 1009.742
[38,     1] loss: 1011.926
[39,     1] loss: 971.301
[40,     1] loss: 1010.810
[41,     1] loss: 970.870
[42,     1] loss: 942.372
[43,     1] loss: 1065.588
[44,     1] loss: 1092.527
[45,     1] loss: 958.122
[46,     1] loss: 1080.782
[47,     1] loss: 965.233
[48,     1] loss: 994.053
[49,     1] loss: 970.973
[50,     1] loss: 937.577
[51,     1] loss: 974.824
[52,     1] loss: 883.478
[53,     1] loss: 996.977
[54,     1] loss: 894.296
[55,     1] loss: 981.702
[56,     1] loss: 921.898
[57,     1] loss: 919.006
[58,     1] loss: 989.269
[59,     1] loss: 876.305
[60,     1] loss: 942.381
[61,     1] loss: 949.753
[62,     1] loss: 937.583
[63,     1] loss: 786.162
[64,     1] loss: 909.334
Early stopping applied (best metric=0.8306030631065369)
Finished Training
Total time taken: 8.668182611465454
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.938
[2,     1] loss: 1384.616
[3,     1] loss: 1382.333
[4,     1] loss: 1382.497
[5,     1] loss: 1382.717
[6,     1] loss: 1376.884
[7,     1] loss: 1371.946
[8,     1] loss: 1368.502
[9,     1] loss: 1356.189
[10,     1] loss: 1333.927
[11,     1] loss: 1293.785
[12,     1] loss: 1258.208
[13,     1] loss: 1234.012
[14,     1] loss: 1191.969
[15,     1] loss: 1201.494
[16,     1] loss: 1189.879
[17,     1] loss: 1141.691
[18,     1] loss: 1169.604
[19,     1] loss: 1071.751
[20,     1] loss: 1062.807
[21,     1] loss: 1085.024
[22,     1] loss: 1119.379
[23,     1] loss: 1038.729
[24,     1] loss: 1099.295
[25,     1] loss: 1086.845
[26,     1] loss: 1052.056
[27,     1] loss: 1017.479
[28,     1] loss: 1005.409
[29,     1] loss: 969.657
[30,     1] loss: 1038.595
[31,     1] loss: 1070.677
[32,     1] loss: 921.522
[33,     1] loss: 1070.285
[34,     1] loss: 1066.170
[35,     1] loss: 976.821
[36,     1] loss: 1026.766
[37,     1] loss: 1007.144
[38,     1] loss: 964.816
[39,     1] loss: 958.384
[40,     1] loss: 900.319
[41,     1] loss: 918.009
[42,     1] loss: 967.100
[43,     1] loss: 879.090
[44,     1] loss: 946.990
[45,     1] loss: 965.880
[46,     1] loss: 1002.964
[47,     1] loss: 844.569
[48,     1] loss: 920.263
[49,     1] loss: 852.620
[50,     1] loss: 827.051
[51,     1] loss: 792.026
[52,     1] loss: 903.031
[53,     1] loss: 1245.230
[54,     1] loss: 1106.921
[55,     1] loss: 985.759
[56,     1] loss: 969.698
[57,     1] loss: 1072.635
[58,     1] loss: 1064.706
[59,     1] loss: 1020.111
[60,     1] loss: 1012.699
[61,     1] loss: 974.528
[62,     1] loss: 987.086
[63,     1] loss: 907.281
[64,     1] loss: 968.850
[65,     1] loss: 933.341
[66,     1] loss: 894.000
Early stopping applied (best metric=0.8578449487686157)
Finished Training
Total time taken: 10.986231327056885
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1382.969
[2,     1] loss: 1387.756
[3,     1] loss: 1380.380
[4,     1] loss: 1382.669
[5,     1] loss: 1381.056
[6,     1] loss: 1379.210
[7,     1] loss: 1373.480
[8,     1] loss: 1373.421
[9,     1] loss: 1355.539
[10,     1] loss: 1328.479
[11,     1] loss: 1299.686
[12,     1] loss: 1255.431
[13,     1] loss: 1184.718
[14,     1] loss: 1152.740
[15,     1] loss: 1190.588
[16,     1] loss: 1165.030
[17,     1] loss: 1067.355
[18,     1] loss: 1135.112
[19,     1] loss: 1076.418
[20,     1] loss: 1096.658
[21,     1] loss: 1087.406
[22,     1] loss: 1072.659
[23,     1] loss: 1041.753
[24,     1] loss: 1034.528
[25,     1] loss: 1040.002
[26,     1] loss: 1014.166
[27,     1] loss: 1009.263
[28,     1] loss: 992.052
[29,     1] loss: 977.830
[30,     1] loss: 1013.964
[31,     1] loss: 961.399
[32,     1] loss: 932.907
[33,     1] loss: 908.231
[34,     1] loss: 887.117
[35,     1] loss: 1018.864
[36,     1] loss: 1305.287
[37,     1] loss: 926.135
[38,     1] loss: 1047.664
[39,     1] loss: 1032.605
[40,     1] loss: 1015.910
[41,     1] loss: 1024.851
[42,     1] loss: 1012.870
[43,     1] loss: 944.782
[44,     1] loss: 947.283
[45,     1] loss: 1034.299
[46,     1] loss: 911.667
[47,     1] loss: 966.453
[48,     1] loss: 893.337
[49,     1] loss: 955.193
[50,     1] loss: 913.043
[51,     1] loss: 907.640
[52,     1] loss: 902.279
[53,     1] loss: 851.647
[54,     1] loss: 866.345
[55,     1] loss: 846.304
[56,     1] loss: 778.895
[57,     1] loss: 848.353
[58,     1] loss: 971.910
[59,     1] loss: 1024.521
[60,     1] loss: 819.779
[61,     1] loss: 902.074
[62,     1] loss: 837.339
[63,     1] loss: 898.915
[64,     1] loss: 845.768
[65,     1] loss: 879.078
[66,     1] loss: 783.931
[67,     1] loss: 895.853
[68,     1] loss: 890.373
[69,     1] loss: 761.734
Early stopping applied (best metric=0.9344737529754639)
Finished Training
Total time taken: 11.288238286972046
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.297
[2,     1] loss: 1386.271
[3,     1] loss: 1380.583
[4,     1] loss: 1384.085
[5,     1] loss: 1384.301
[6,     1] loss: 1384.915
[7,     1] loss: 1379.077
[8,     1] loss: 1379.145
[9,     1] loss: 1377.791
[10,     1] loss: 1374.334
[11,     1] loss: 1365.967
[12,     1] loss: 1363.893
[13,     1] loss: 1353.156
[14,     1] loss: 1313.278
[15,     1] loss: 1277.084
[16,     1] loss: 1261.208
[17,     1] loss: 1242.287
[18,     1] loss: 1191.732
[19,     1] loss: 1164.249
[20,     1] loss: 1124.758
[21,     1] loss: 1105.581
[22,     1] loss: 1167.611
[23,     1] loss: 1097.187
[24,     1] loss: 1097.365
[25,     1] loss: 1137.004
[26,     1] loss: 1082.891
[27,     1] loss: 1170.163
[28,     1] loss: 1049.520
[29,     1] loss: 1176.276
[30,     1] loss: 1069.724
[31,     1] loss: 1093.513
[32,     1] loss: 1154.112
[33,     1] loss: 1034.568
[34,     1] loss: 1147.044
[35,     1] loss: 1038.117
[36,     1] loss: 1074.502
[37,     1] loss: 1114.764
[38,     1] loss: 997.358
[39,     1] loss: 1080.009
[40,     1] loss: 1016.337
[41,     1] loss: 1002.039
[42,     1] loss: 975.789
[43,     1] loss: 964.666
[44,     1] loss: 974.406
[45,     1] loss: 908.529
[46,     1] loss: 976.979
[47,     1] loss: 963.031
[48,     1] loss: 959.354
[49,     1] loss: 919.414
[50,     1] loss: 934.065
[51,     1] loss: 998.945
[52,     1] loss: 923.102
[53,     1] loss: 900.130
[54,     1] loss: 879.538
[55,     1] loss: 888.085
[56,     1] loss: 841.776
[57,     1] loss: 872.543
[58,     1] loss: 1006.834
[59,     1] loss: 890.021
[60,     1] loss: 808.250
[61,     1] loss: 859.522
[62,     1] loss: 841.984
[63,     1] loss: 845.224
[64,     1] loss: 865.227
[65,     1] loss: 1524.124
[66,     1] loss: 1333.756
[67,     1] loss: 1143.770
[68,     1] loss: 1028.890
[69,     1] loss: 1115.793
[70,     1] loss: 1228.093
[71,     1] loss: 1216.409
[72,     1] loss: 1214.106
[73,     1] loss: 1173.359
[74,     1] loss: 1200.159
[75,     1] loss: 1163.677
Early stopping applied (best metric=0.8002409934997559)
Finished Training
Total time taken: 10.10521411895752
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1382.204
[2,     1] loss: 1389.491
[3,     1] loss: 1383.542
[4,     1] loss: 1382.945
[5,     1] loss: 1380.054
[6,     1] loss: 1374.645
[7,     1] loss: 1367.211
[8,     1] loss: 1337.039
[9,     1] loss: 1306.396
[10,     1] loss: 1268.629
[11,     1] loss: 1233.558
[12,     1] loss: 1211.788
[13,     1] loss: 1189.701
[14,     1] loss: 1182.487
[15,     1] loss: 1155.425
[16,     1] loss: 1143.755
[17,     1] loss: 1140.369
[18,     1] loss: 1159.812
[19,     1] loss: 1128.104
[20,     1] loss: 1114.082
[21,     1] loss: 1087.314
[22,     1] loss: 1090.437
[23,     1] loss: 1082.856
[24,     1] loss: 1064.089
[25,     1] loss: 1029.328
[26,     1] loss: 1012.499
[27,     1] loss: 1005.711
[28,     1] loss: 1006.930
[29,     1] loss: 985.410
[30,     1] loss: 979.141
[31,     1] loss: 948.637
[32,     1] loss: 904.067
[33,     1] loss: 1101.362
[34,     1] loss: 1524.979
[35,     1] loss: 975.151
[36,     1] loss: 1164.129
[37,     1] loss: 1129.796
[38,     1] loss: 1085.535
[39,     1] loss: 1075.254
[40,     1] loss: 1081.985
[41,     1] loss: 1117.297
[42,     1] loss: 1049.237
[43,     1] loss: 1063.246
[44,     1] loss: 1067.172
[45,     1] loss: 1017.705
[46,     1] loss: 1036.033
[47,     1] loss: 995.183
[48,     1] loss: 964.139
[49,     1] loss: 993.023
Early stopping applied (best metric=0.8273357152938843)
Finished Training
Total time taken: 8.136174440383911
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.880
[2,     1] loss: 1383.918
[3,     1] loss: 1383.452
[4,     1] loss: 1380.390
[5,     1] loss: 1382.718
[6,     1] loss: 1381.233
[7,     1] loss: 1377.911
[8,     1] loss: 1378.135
[9,     1] loss: 1364.861
[10,     1] loss: 1350.280
[11,     1] loss: 1316.358
[12,     1] loss: 1275.505
[13,     1] loss: 1260.472
[14,     1] loss: 1192.660
[15,     1] loss: 1176.992
[16,     1] loss: 1162.496
[17,     1] loss: 1121.980
[18,     1] loss: 1140.894
[19,     1] loss: 1108.270
[20,     1] loss: 1092.618
[21,     1] loss: 1099.918
[22,     1] loss: 1090.408
[23,     1] loss: 1062.370
[24,     1] loss: 1059.958
[25,     1] loss: 1053.394
[26,     1] loss: 1100.456
[27,     1] loss: 1056.764
[28,     1] loss: 1014.293
[29,     1] loss: 1015.036
[30,     1] loss: 949.599
[31,     1] loss: 954.909
[32,     1] loss: 950.292
[33,     1] loss: 1135.488
[34,     1] loss: 1468.320
[35,     1] loss: 1043.956
[36,     1] loss: 1038.773
[37,     1] loss: 1126.533
[38,     1] loss: 1148.703
[39,     1] loss: 1130.697
[40,     1] loss: 1102.712
[41,     1] loss: 1099.807
[42,     1] loss: 1091.604
Early stopping applied (best metric=0.9048284888267517)
Finished Training
Total time taken: 6.136128187179565
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.076
[2,     1] loss: 1390.787
[3,     1] loss: 1383.673
[4,     1] loss: 1381.338
[5,     1] loss: 1383.359
[6,     1] loss: 1380.419
[7,     1] loss: 1378.406
[8,     1] loss: 1378.304
[9,     1] loss: 1378.631
[10,     1] loss: 1371.347
[11,     1] loss: 1362.167
[12,     1] loss: 1339.503
[13,     1] loss: 1314.057
[14,     1] loss: 1290.120
[15,     1] loss: 1253.466
[16,     1] loss: 1196.460
[17,     1] loss: 1159.334
[18,     1] loss: 1152.134
[19,     1] loss: 1198.922
[20,     1] loss: 1231.588
[21,     1] loss: 1124.566
[22,     1] loss: 1123.652
[23,     1] loss: 1110.378
[24,     1] loss: 1140.305
[25,     1] loss: 1115.029
[26,     1] loss: 1113.704
[27,     1] loss: 1075.687
[28,     1] loss: 1075.699
[29,     1] loss: 1038.822
[30,     1] loss: 1033.352
[31,     1] loss: 1058.006
[32,     1] loss: 1004.902
[33,     1] loss: 1031.294
[34,     1] loss: 1068.915
[35,     1] loss: 1014.993
[36,     1] loss: 1004.566
[37,     1] loss: 967.222
[38,     1] loss: 1056.959
[39,     1] loss: 954.300
[40,     1] loss: 1061.034
[41,     1] loss: 956.075
[42,     1] loss: 1008.720
[43,     1] loss: 998.021
[44,     1] loss: 1002.740
[45,     1] loss: 944.113
[46,     1] loss: 969.934
[47,     1] loss: 933.921
[48,     1] loss: 931.171
[49,     1] loss: 929.875
[50,     1] loss: 917.421
[51,     1] loss: 909.231
[52,     1] loss: 922.882
[53,     1] loss: 911.982
[54,     1] loss: 969.452
[55,     1] loss: 921.847
[56,     1] loss: 821.518
[57,     1] loss: 914.658
[58,     1] loss: 893.096
[59,     1] loss: 826.861
[60,     1] loss: 827.044
[61,     1] loss: 874.364
[62,     1] loss: 878.460
[63,     1] loss: 839.177
[64,     1] loss: 793.880
[65,     1] loss: 745.608
[66,     1] loss: 948.078
[67,     1] loss: 2375.108
[68,     1] loss: 1286.003
[69,     1] loss: 1093.925
[70,     1] loss: 1302.764
[71,     1] loss: 1263.239
[72,     1] loss: 1279.980
[73,     1] loss: 1307.119
[74,     1] loss: 1296.743
[75,     1] loss: 1270.463
[76,     1] loss: 1219.389
Early stopping applied (best metric=0.7464451789855957)
Finished Training
Total time taken: 12.672266006469727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1379.518
[2,     1] loss: 1397.282
[3,     1] loss: 1383.648
[4,     1] loss: 1388.609
[5,     1] loss: 1383.418
[6,     1] loss: 1379.335
[7,     1] loss: 1381.196
[8,     1] loss: 1375.249
[9,     1] loss: 1367.470
[10,     1] loss: 1345.948
[11,     1] loss: 1324.832
[12,     1] loss: 1278.445
[13,     1] loss: 1241.216
[14,     1] loss: 1201.667
[15,     1] loss: 1247.448
[16,     1] loss: 1158.191
[17,     1] loss: 1162.586
[18,     1] loss: 1106.755
[19,     1] loss: 1164.854
[20,     1] loss: 1211.026
[21,     1] loss: 1091.584
[22,     1] loss: 1185.578
[23,     1] loss: 1111.347
[24,     1] loss: 1124.595
[25,     1] loss: 1131.163
[26,     1] loss: 1079.079
[27,     1] loss: 1068.786
[28,     1] loss: 1078.225
[29,     1] loss: 1072.150
[30,     1] loss: 1040.735
[31,     1] loss: 1068.973
[32,     1] loss: 1035.783
[33,     1] loss: 1010.587
[34,     1] loss: 1015.626
[35,     1] loss: 944.250
[36,     1] loss: 1019.797
[37,     1] loss: 1027.370
[38,     1] loss: 942.391
[39,     1] loss: 974.507
[40,     1] loss: 904.970
[41,     1] loss: 1062.203
[42,     1] loss: 949.966
[43,     1] loss: 941.382
[44,     1] loss: 934.731
[45,     1] loss: 880.733
[46,     1] loss: 933.489
[47,     1] loss: 844.079
[48,     1] loss: 868.319
[49,     1] loss: 1067.818
[50,     1] loss: 986.609
[51,     1] loss: 845.847
[52,     1] loss: 1014.693
[53,     1] loss: 940.119
[54,     1] loss: 889.425
[55,     1] loss: 941.582
[56,     1] loss: 865.875
[57,     1] loss: 932.794
[58,     1] loss: 852.590
[59,     1] loss: 823.763
[60,     1] loss: 858.763
[61,     1] loss: 765.897
[62,     1] loss: 783.235
[63,     1] loss: 788.771
[64,     1] loss: 811.964
[65,     1] loss: 886.567
[66,     1] loss: 868.765
[67,     1] loss: 775.635
[68,     1] loss: 731.299
[69,     1] loss: 863.350
[70,     1] loss: 829.286
[71,     1] loss: 705.850
[72,     1] loss: 744.506
[73,     1] loss: 1012.672
[74,     1] loss: 1224.781
[75,     1] loss: 902.317
[76,     1] loss: 1082.271
[77,     1] loss: 994.295
[78,     1] loss: 965.995
Early stopping applied (best metric=0.799871563911438)
Finished Training
Total time taken: 10.47822117805481
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1386.821
[2,     1] loss: 1388.103
[3,     1] loss: 1380.286
[4,     1] loss: 1385.946
[5,     1] loss: 1385.717
[6,     1] loss: 1382.442
[7,     1] loss: 1380.538
[8,     1] loss: 1377.861
[9,     1] loss: 1377.387
[10,     1] loss: 1378.654
[11,     1] loss: 1369.741
[12,     1] loss: 1361.231
[13,     1] loss: 1342.650
[14,     1] loss: 1314.424
[15,     1] loss: 1287.318
[16,     1] loss: 1240.025
[17,     1] loss: 1212.611
[18,     1] loss: 1215.992
[19,     1] loss: 1152.014
[20,     1] loss: 1167.817
[21,     1] loss: 1172.810
[22,     1] loss: 1172.136
[23,     1] loss: 1111.886
[24,     1] loss: 1120.167
[25,     1] loss: 1108.616
[26,     1] loss: 1102.650
[27,     1] loss: 1154.067
[28,     1] loss: 1114.721
[29,     1] loss: 1100.098
[30,     1] loss: 1115.074
[31,     1] loss: 1104.175
[32,     1] loss: 1015.394
[33,     1] loss: 1059.143
[34,     1] loss: 1034.321
[35,     1] loss: 1059.956
[36,     1] loss: 1000.998
[37,     1] loss: 1087.797
[38,     1] loss: 1017.105
[39,     1] loss: 1040.301
[40,     1] loss: 1014.515
[41,     1] loss: 1064.490
[42,     1] loss: 965.488
[43,     1] loss: 930.343
[44,     1] loss: 895.506
[45,     1] loss: 924.685
[46,     1] loss: 870.388
[47,     1] loss: 878.136
[48,     1] loss: 974.789
[49,     1] loss: 1209.040
[50,     1] loss: 1236.906
[51,     1] loss: 1094.656
[52,     1] loss: 1148.431
[53,     1] loss: 1136.303
[54,     1] loss: 1135.150
[55,     1] loss: 1077.043
[56,     1] loss: 1007.554
[57,     1] loss: 1052.701
[58,     1] loss: 1052.947
[59,     1] loss: 1030.453
[60,     1] loss: 1051.193
[61,     1] loss: 1004.893
[62,     1] loss: 978.165
[63,     1] loss: 948.461
[64,     1] loss: 985.377
[65,     1] loss: 913.056
[66,     1] loss: 925.060
[67,     1] loss: 905.683
[68,     1] loss: 1006.717
[69,     1] loss: 946.351
[70,     1] loss: 870.323
[71,     1] loss: 909.983
[72,     1] loss: 883.822
[73,     1] loss: 853.732
[74,     1] loss: 833.111
[75,     1] loss: 748.305
[76,     1] loss: 768.059
[77,     1] loss: 1298.932
[78,     1] loss: 2237.276
[79,     1] loss: 1210.655
[80,     1] loss: 1277.661
[81,     1] loss: 1338.541
[82,     1] loss: 1348.899
[83,     1] loss: 1340.143
[84,     1] loss: 1337.991
[85,     1] loss: 1353.391
[86,     1] loss: 1312.794
Early stopping applied (best metric=0.7530203461647034)
Finished Training
Total time taken: 13.455282926559448
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1386.187
[2,     1] loss: 1382.760
[3,     1] loss: 1395.063
[4,     1] loss: 1385.139
[5,     1] loss: 1383.062
[6,     1] loss: 1382.378
[7,     1] loss: 1376.417
[8,     1] loss: 1370.741
[9,     1] loss: 1359.503
[10,     1] loss: 1332.628
[11,     1] loss: 1298.194
[12,     1] loss: 1263.540
[13,     1] loss: 1234.090
[14,     1] loss: 1207.958
[15,     1] loss: 1169.206
[16,     1] loss: 1113.169
[17,     1] loss: 1156.191
[18,     1] loss: 1159.383
[19,     1] loss: 1217.959
[20,     1] loss: 1081.389
[21,     1] loss: 1148.060
[22,     1] loss: 1123.806
[23,     1] loss: 1128.638
[24,     1] loss: 1112.888
[25,     1] loss: 1102.161
[26,     1] loss: 1077.981
[27,     1] loss: 1045.451
[28,     1] loss: 1058.029
[29,     1] loss: 1047.911
[30,     1] loss: 1014.650
[31,     1] loss: 1002.808
[32,     1] loss: 996.347
[33,     1] loss: 987.193
[34,     1] loss: 962.358
[35,     1] loss: 973.561
[36,     1] loss: 1022.518
[37,     1] loss: 1010.822
[38,     1] loss: 961.110
[39,     1] loss: 912.529
[40,     1] loss: 918.497
[41,     1] loss: 998.320
[42,     1] loss: 961.438
[43,     1] loss: 916.828
[44,     1] loss: 906.553
[45,     1] loss: 912.752
[46,     1] loss: 852.820
[47,     1] loss: 886.549
[48,     1] loss: 1098.083
[49,     1] loss: 1744.777
[50,     1] loss: 1037.270
[51,     1] loss: 1060.429
[52,     1] loss: 1168.235
[53,     1] loss: 1175.523
[54,     1] loss: 1161.821
[55,     1] loss: 1174.742
[56,     1] loss: 1156.827
[57,     1] loss: 1145.698
[58,     1] loss: 1118.676
[59,     1] loss: 1096.827
[60,     1] loss: 1030.117
[61,     1] loss: 1074.577
[62,     1] loss: 1095.135
[63,     1] loss: 1071.200
[64,     1] loss: 1038.909
[65,     1] loss: 994.023
[66,     1] loss: 1027.779
Early stopping applied (best metric=0.8160696625709534)
Finished Training
Total time taken: 10.2802152633667
{'Hydroxylation-K Validation Accuracy': 0.7661052009456265, 'Hydroxylation-K Validation Sensitivity': 0.6888888888888889, 'Hydroxylation-K Validation Specificity': 0.7859649122807018, 'Hydroxylation-K Validation Precision': 0.4547192676604441, 'Hydroxylation-K AUC ROC': 0.8087134502923977, 'Hydroxylation-K AUC PR': 0.5912507012438623, 'Hydroxylation-K MCC': 0.4161413409680254, 'Hydroxylation-K F1': 0.5415295781305599, 'Validation Loss (Hydroxylation-K)': 0.4345328489939372, 'Hydroxylation-P Validation Accuracy': 0.7889493934318055, 'Hydroxylation-P Validation Sensitivity': 0.7613756613756614, 'Hydroxylation-P Validation Specificity': 0.7949099705720983, 'Hydroxylation-P Validation Precision': 0.45433623032126236, 'Hydroxylation-P AUC ROC': 0.8474682444677207, 'Hydroxylation-P AUC PR': 0.6083123152436551, 'Hydroxylation-P MCC': 0.4678395270088645, 'Hydroxylation-P F1': 0.5622703557373084, 'Validation Loss (Hydroxylation-P)': 0.38039491772651673, 'Validation Loss (total)': 0.8149277687072753, 'TimeToTrain': 10.479221312204997}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007717756204459587,
 'learning_rate_Hydroxylation-K': 0.0012875257455177802,
 'learning_rate_Hydroxylation-P': 0.006902377835683796,
 'log_base': 2.967590628721987,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1262152702,
 'sample_weights': [2.2536504943467657, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.196159017102588,
 'weight_decay_Hydroxylation-K': 3.584974072199002,
 'weight_decay_Hydroxylation-P': 0.06131361379850553}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.740
[2,     1] loss: 1231.140
[3,     1] loss: 1241.416
[4,     1] loss: 1234.742
[5,     1] loss: 1232.682
[6,     1] loss: 1232.425
[7,     1] loss: 1232.899
[8,     1] loss: 1230.944
[9,     1] loss: 1230.002
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007806341747292421,
 'learning_rate_Hydroxylation-K': 0.008021686543355541,
 'learning_rate_Hydroxylation-P': 0.003902268310593272,
 'log_base': 2.2216054386025315,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1876242392,
 'sample_weights': [1.5347667687950708, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7743154611327814,
 'weight_decay_Hydroxylation-K': 6.204826007127587,
 'weight_decay_Hydroxylation-P': 8.262042824704958}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.052
[2,     1] loss: 1364.998
[3,     1] loss: 1353.217
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008427694405808811,
 'learning_rate_Hydroxylation-K': 4.770739770003883e-05,
 'learning_rate_Hydroxylation-P': 0.0010137957698179907,
 'log_base': 1.0178798699653644,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 286571754,
 'sample_weights': [2.09143095059001, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.425371805436141,
 'weight_decay_Hydroxylation-K': 9.263857805456412,
 'weight_decay_Hydroxylation-P': 8.107585649410604}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30519.078
Exploding loss, terminate run (best metric=1.1191773414611816)
Finished Training
Total time taken: 0.21900510787963867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30511.955
Exploding loss, terminate run (best metric=1.1125187873840332)
Finished Training
Total time taken: 0.22000432014465332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30786.992
Exploding loss, terminate run (best metric=1.183994174003601)
Finished Training
Total time taken: 0.19700288772583008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30688.344
Exploding loss, terminate run (best metric=1.094565510749817)
Finished Training
Total time taken: 0.2180032730102539
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30614.943
Exploding loss, terminate run (best metric=1.074486494064331)
Finished Training
Total time taken: 0.2250044345855713
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30562.807
Exploding loss, terminate run (best metric=1.0960613489151)
Finished Training
Total time taken: 0.21900653839111328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30608.715
Exploding loss, terminate run (best metric=1.0933122634887695)
Finished Training
Total time taken: 0.2030026912689209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30576.445
Exploding loss, terminate run (best metric=1.0923266410827637)
Finished Training
Total time taken: 0.20800399780273438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30771.336
Exploding loss, terminate run (best metric=1.1212183237075806)
Finished Training
Total time taken: 0.2230055332183838
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30671.746
Exploding loss, terminate run (best metric=1.07151460647583)
Finished Training
Total time taken: 0.21300411224365234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30558.750
Exploding loss, terminate run (best metric=1.1329288482666016)
Finished Training
Total time taken: 0.23000407218933105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30501.699
Exploding loss, terminate run (best metric=1.0963120460510254)
Finished Training
Total time taken: 0.23500514030456543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30538.490
Exploding loss, terminate run (best metric=1.0971953868865967)
Finished Training
Total time taken: 0.21100497245788574
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30548.312
Exploding loss, terminate run (best metric=1.0808491706848145)
Finished Training
Total time taken: 0.2230055332183838
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30648.391
Exploding loss, terminate run (best metric=1.1116338968276978)
Finished Training
Total time taken: 0.23000669479370117
{'Hydroxylation-K Validation Accuracy': 0.5972222222222222, 'Hydroxylation-K Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-K Validation Specificity': 0.6666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5195906432748538, 'Hydroxylation-K AUC PR': 0.26690884199007003, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.11026272577996717, 'Validation Loss (Hydroxylation-K)': 0.5645841081937154, 'Hydroxylation-P Validation Accuracy': 0.6072752990541935, 'Hydroxylation-P Validation Sensitivity': 0.3333333333333333, 'Hydroxylation-P Validation Specificity': 0.6666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5916843435962843, 'Hydroxylation-P AUC PR': 0.27233595315371684, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.09988628443564067, 'Validation Loss (Hydroxylation-P)': 0.5406221985816956, 'Validation Loss (total)': 1.1052063226699829, 'TimeToTrain': 0.21827128728230794}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007129582939273488,
 'learning_rate_Hydroxylation-K': 0.006760091968102993,
 'learning_rate_Hydroxylation-P': 0.005853218446108805,
 'log_base': 2.4276079491262608,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1219390893,
 'sample_weights': [94.27211777278487, 11.759529621009918],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5856305203452656,
 'weight_decay_Hydroxylation-K': 5.967434015121073,
 'weight_decay_Hydroxylation-P': 6.175241816066214}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.767
[2,     1] loss: 1317.561
[3,     1] loss: 1308.006
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005692930444665767,
 'learning_rate_Hydroxylation-K': 0.000797454850924859,
 'learning_rate_Hydroxylation-P': 0.000781463867343289,
 'log_base': 1.2408249897205241,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3542279300,
 'sample_weights': [1.8823217052840442, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.163481930215928,
 'weight_decay_Hydroxylation-K': 6.351548459911754,
 'weight_decay_Hydroxylation-P': 6.3427806409619745}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2541.640
[2,     1] loss: 2542.691
[3,     1] loss: 2531.442
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009470219897201796,
 'learning_rate_Hydroxylation-K': 0.00018477735399194758,
 'learning_rate_Hydroxylation-P': 0.0017100353319826863,
 'log_base': 1.2796897586354978,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3790518643,
 'sample_weights': [7.736909991417672, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.21197133485364,
 'weight_decay_Hydroxylation-K': 7.963495499101075,
 'weight_decay_Hydroxylation-P': 9.50579320672974}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2340.773
[2,     1] loss: 2357.909
[3,     1] loss: 2342.305
[4,     1] loss: 2329.251
[5,     1] loss: 2330.042
[6,     1] loss: 2347.691
[7,     1] loss: 2336.890
[8,     1] loss: 2331.595
[9,     1] loss: 2340.490
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008696264707158145,
 'learning_rate_Hydroxylation-K': 0.0023592117409576958,
 'learning_rate_Hydroxylation-P': 0.0018956031899774357,
 'log_base': 1.077263293034504,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1761433592,
 'sample_weights': [6.769357323762159, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.950971638504204,
 'weight_decay_Hydroxylation-K': 8.991357665465452,
 'weight_decay_Hydroxylation-P': 7.310069520056063}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7306.477
[2,     1] loss: 7295.184
[3,     1] loss: 7271.658
[4,     1] loss: 7258.203
[5,     1] loss: 7264.927
[6,     1] loss: 7237.502
[7,     1] loss: 7276.763
[8,     1] loss: 7248.634
[9,     1] loss: 7191.062
[10,     1] loss: 7149.055
[11,     1] loss: 7063.487
[12,     1] loss: 6892.748
[13,     1] loss: 6881.150
[14,     1] loss: 6650.879
[15,     1] loss: 6374.893
[16,     1] loss: 6261.221
[17,     1] loss: 6076.167
[18,     1] loss: 6186.723
[19,     1] loss: 5831.668
[20,     1] loss: 5943.580
[21,     1] loss: 6259.999
[22,     1] loss: 6508.767
[23,     1] loss: 6056.532
[24,     1] loss: 5960.846
[25,     1] loss: 5993.047
[26,     1] loss: 5532.697
[27,     1] loss: 5653.368
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006817603390827676,
 'learning_rate_Hydroxylation-K': 0.0005437445017273594,
 'learning_rate_Hydroxylation-P': 0.00040771781805746805,
 'log_base': 1.118342055647104,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1238056004,
 'sample_weights': [22.431565069744167, 2.804053652963437],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.612515965902192,
 'weight_decay_Hydroxylation-K': 8.660881324668209,
 'weight_decay_Hydroxylation-P': 7.593148154211452}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4835.070
[2,     1] loss: 4861.787
[3,     1] loss: 4832.087
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00802236672553289,
 'learning_rate_Hydroxylation-K': 0.0016786021353892692,
 'learning_rate_Hydroxylation-P': 0.0010713307676822762,
 'log_base': 1.3841899905285469,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1079424127,
 'sample_weights': [14.926095036314527, 1.8658337561791232],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.708914342377565,
 'weight_decay_Hydroxylation-K': 9.949983670884814,
 'weight_decay_Hydroxylation-P': 6.7463985193939315}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1996.474
[2,     1] loss: 2001.439
[3,     1] loss: 1990.480
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005721637530868604,
 'learning_rate_Hydroxylation-K': 0.006311359383523369,
 'learning_rate_Hydroxylation-P': 0.006006638259778143,
 'log_base': 1.9563719363115526,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 181469133,
 'sample_weights': [5.134929209150273, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8192767492854696,
 'weight_decay_Hydroxylation-K': 6.110134313247155,
 'weight_decay_Hydroxylation-P': 4.603476645710497}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1438.264
[2,     1] loss: 1434.882
[3,     1] loss: 1434.975
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019179578709401324,
 'learning_rate_Hydroxylation-K': 0.004805982814215227,
 'learning_rate_Hydroxylation-P': 0.00553017983636456,
 'log_base': 1.5662894612035108,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1593668710,
 'sample_weights': [2.487652782658609, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.040859464843281,
 'weight_decay_Hydroxylation-K': 7.175764430991348,
 'weight_decay_Hydroxylation-P': 2.1859339629127437}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1688.014
[2,     1] loss: 1688.794
[3,     1] loss: 1686.647
[4,     1] loss: 1690.348
[5,     1] loss: 1699.059
[6,     1] loss: 1698.920
[7,     1] loss: 1691.821
[8,     1] loss: 1683.859
[9,     1] loss: 1683.508
[10,     1] loss: 1685.696
[11,     1] loss: 1679.240
[12,     1] loss: 1678.132
[13,     1] loss: 1649.412
[14,     1] loss: 1630.029
[15,     1] loss: 1617.209
[16,     1] loss: 1576.855
[17,     1] loss: 1534.945
[18,     1] loss: 1525.718
[19,     1] loss: 1474.008
[20,     1] loss: 1469.320
[21,     1] loss: 1397.313
[22,     1] loss: 1436.816
[23,     1] loss: 1470.401
[24,     1] loss: 1369.551
[25,     1] loss: 1469.028
[26,     1] loss: 1372.435
[27,     1] loss: 1413.259
[28,     1] loss: 1358.943
[29,     1] loss: 1322.727
[30,     1] loss: 1326.004
[31,     1] loss: 1348.818
[32,     1] loss: 1347.682
[33,     1] loss: 1307.210
[34,     1] loss: 1269.601
[35,     1] loss: 1305.634
[36,     1] loss: 1317.961
[37,     1] loss: 1215.569
[38,     1] loss: 1270.345
[39,     1] loss: 1177.353
[40,     1] loss: 1192.868
[41,     1] loss: 1231.835
[42,     1] loss: 1143.176
[43,     1] loss: 1170.054
[44,     1] loss: 1228.166
[45,     1] loss: 1237.293
[46,     1] loss: 1091.612
[47,     1] loss: 1177.724
[48,     1] loss: 1159.971
[49,     1] loss: 1100.160
[50,     1] loss: 1081.999
[51,     1] loss: 1108.508
[52,     1] loss: 1037.493
[53,     1] loss: 1086.721
[54,     1] loss: 1041.469
[55,     1] loss: 1023.586
[56,     1] loss: 1080.114
[57,     1] loss: 1050.445
[58,     1] loss: 1161.936
[59,     1] loss: 1053.176
[60,     1] loss: 994.761
[61,     1] loss: 999.128
[62,     1] loss: 936.368
[63,     1] loss: 968.033
[64,     1] loss: 951.255
[65,     1] loss: 1139.306
[66,     1] loss: 942.295
[67,     1] loss: 1090.228
[68,     1] loss: 933.467
[69,     1] loss: 1036.262
[70,     1] loss: 879.678
[71,     1] loss: 1002.220
[72,     1] loss: 891.095
Early stopping applied (best metric=0.785676121711731)
Finished Training
Total time taken: 12.017256021499634
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1697.979
[2,     1] loss: 1697.594
[3,     1] loss: 1690.642
[4,     1] loss: 1684.464
[5,     1] loss: 1694.321
[6,     1] loss: 1691.334
[7,     1] loss: 1690.049
[8,     1] loss: 1692.356
[9,     1] loss: 1689.337
[10,     1] loss: 1681.642
[11,     1] loss: 1680.250
[12,     1] loss: 1674.139
[13,     1] loss: 1665.428
[14,     1] loss: 1651.561
[15,     1] loss: 1635.955
[16,     1] loss: 1593.007
[17,     1] loss: 1570.373
[18,     1] loss: 1550.467
[19,     1] loss: 1536.237
[20,     1] loss: 1489.402
[21,     1] loss: 1485.861
[22,     1] loss: 1438.977
[23,     1] loss: 1455.451
[24,     1] loss: 1448.638
[25,     1] loss: 1459.984
[26,     1] loss: 1387.733
[27,     1] loss: 1389.611
[28,     1] loss: 1384.597
[29,     1] loss: 1424.960
[30,     1] loss: 1319.090
[31,     1] loss: 1298.614
[32,     1] loss: 1308.573
[33,     1] loss: 1309.326
[34,     1] loss: 1325.984
[35,     1] loss: 1285.052
[36,     1] loss: 1316.116
[37,     1] loss: 1284.194
[38,     1] loss: 1243.805
[39,     1] loss: 1254.291
[40,     1] loss: 1257.064
[41,     1] loss: 1203.772
[42,     1] loss: 1254.829
[43,     1] loss: 1197.679
[44,     1] loss: 1249.096
[45,     1] loss: 1218.646
[46,     1] loss: 1199.362
[47,     1] loss: 1212.976
[48,     1] loss: 1217.460
[49,     1] loss: 1130.205
[50,     1] loss: 1154.262
[51,     1] loss: 1205.242
[52,     1] loss: 1164.802
[53,     1] loss: 1160.573
[54,     1] loss: 1226.288
[55,     1] loss: 1150.014
[56,     1] loss: 1121.909
[57,     1] loss: 1055.492
[58,     1] loss: 1108.565
[59,     1] loss: 1043.122
[60,     1] loss: 1006.946
[61,     1] loss: 1036.252
[62,     1] loss: 959.374
[63,     1] loss: 1050.897
[64,     1] loss: 946.253
[65,     1] loss: 928.969
[66,     1] loss: 932.191
[67,     1] loss: 933.708
[68,     1] loss: 1012.070
Early stopping applied (best metric=0.6813690066337585)
Finished Training
Total time taken: 11.316240787506104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1686.690
[2,     1] loss: 1690.565
[3,     1] loss: 1691.121
[4,     1] loss: 1688.732
[5,     1] loss: 1688.256
[6,     1] loss: 1695.818
[7,     1] loss: 1682.174
[8,     1] loss: 1690.356
[9,     1] loss: 1686.319
[10,     1] loss: 1676.353
[11,     1] loss: 1681.062
[12,     1] loss: 1675.248
[13,     1] loss: 1653.917
[14,     1] loss: 1643.873
[15,     1] loss: 1616.610
[16,     1] loss: 1594.292
[17,     1] loss: 1585.832
[18,     1] loss: 1561.911
[19,     1] loss: 1528.048
[20,     1] loss: 1499.330
[21,     1] loss: 1493.247
[22,     1] loss: 1401.375
[23,     1] loss: 1369.879
[24,     1] loss: 1428.954
[25,     1] loss: 1394.572
[26,     1] loss: 1383.458
[27,     1] loss: 1342.292
[28,     1] loss: 1422.288
[29,     1] loss: 1419.275
[30,     1] loss: 1330.407
[31,     1] loss: 1366.221
[32,     1] loss: 1404.184
[33,     1] loss: 1356.385
[34,     1] loss: 1372.594
[35,     1] loss: 1380.772
[36,     1] loss: 1284.167
[37,     1] loss: 1387.766
[38,     1] loss: 1239.988
[39,     1] loss: 1256.453
[40,     1] loss: 1322.629
[41,     1] loss: 1271.985
[42,     1] loss: 1245.602
[43,     1] loss: 1213.700
[44,     1] loss: 1264.401
[45,     1] loss: 1260.365
[46,     1] loss: 1169.234
[47,     1] loss: 1212.016
[48,     1] loss: 1183.519
[49,     1] loss: 1142.351
[50,     1] loss: 1126.666
[51,     1] loss: 1133.472
[52,     1] loss: 1080.227
[53,     1] loss: 1021.814
[54,     1] loss: 1094.567
[55,     1] loss: 1010.924
[56,     1] loss: 1003.091
[57,     1] loss: 1053.956
[58,     1] loss: 939.993
[59,     1] loss: 1087.366
[60,     1] loss: 1132.146
[61,     1] loss: 991.721
[62,     1] loss: 1050.593
[63,     1] loss: 966.896
[64,     1] loss: 998.676
[65,     1] loss: 970.097
[66,     1] loss: 985.594
[67,     1] loss: 1017.332
[68,     1] loss: 975.092
[69,     1] loss: 981.167
[70,     1] loss: 1010.513
[71,     1] loss: 868.405
[72,     1] loss: 954.301
Early stopping applied (best metric=0.7434588670730591)
Finished Training
Total time taken: 9.704202890396118
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1684.628
[2,     1] loss: 1691.595
[3,     1] loss: 1687.441
[4,     1] loss: 1702.457
[5,     1] loss: 1698.374
[6,     1] loss: 1692.523
[7,     1] loss: 1687.739
[8,     1] loss: 1689.386
[9,     1] loss: 1687.080
[10,     1] loss: 1688.539
[11,     1] loss: 1683.399
[12,     1] loss: 1666.955
[13,     1] loss: 1661.095
[14,     1] loss: 1635.979
[15,     1] loss: 1620.713
[16,     1] loss: 1583.646
[17,     1] loss: 1579.411
[18,     1] loss: 1518.333
[19,     1] loss: 1520.232
[20,     1] loss: 1482.596
[21,     1] loss: 1455.022
[22,     1] loss: 1434.625
[23,     1] loss: 1411.225
[24,     1] loss: 1412.331
[25,     1] loss: 1439.989
[26,     1] loss: 1466.676
[27,     1] loss: 1347.529
[28,     1] loss: 1328.515
[29,     1] loss: 1337.109
[30,     1] loss: 1350.494
[31,     1] loss: 1356.687
[32,     1] loss: 1318.971
[33,     1] loss: 1315.284
[34,     1] loss: 1331.235
[35,     1] loss: 1235.749
[36,     1] loss: 1290.582
[37,     1] loss: 1311.353
[38,     1] loss: 1260.220
[39,     1] loss: 1249.651
[40,     1] loss: 1215.661
[41,     1] loss: 1213.324
[42,     1] loss: 1216.799
[43,     1] loss: 1264.720
[44,     1] loss: 1151.566
[45,     1] loss: 1139.791
[46,     1] loss: 1226.306
[47,     1] loss: 1200.730
[48,     1] loss: 1111.691
[49,     1] loss: 1084.412
[50,     1] loss: 1109.067
[51,     1] loss: 1077.247
[52,     1] loss: 1007.508
[53,     1] loss: 974.409
[54,     1] loss: 963.027
[55,     1] loss: 937.349
[56,     1] loss: 1009.272
[57,     1] loss: 923.728
[58,     1] loss: 933.869
Early stopping applied (best metric=0.804530143737793)
Finished Training
Total time taken: 8.395177364349365
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1713.764
[2,     1] loss: 1695.618
[3,     1] loss: 1698.186
[4,     1] loss: 1696.386
[5,     1] loss: 1693.586
[6,     1] loss: 1690.836
[7,     1] loss: 1695.267
[8,     1] loss: 1692.066
[9,     1] loss: 1685.760
[10,     1] loss: 1689.222
[11,     1] loss: 1685.751
[12,     1] loss: 1695.496
[13,     1] loss: 1688.376
[14,     1] loss: 1686.367
[15,     1] loss: 1686.756
[16,     1] loss: 1684.121
[17,     1] loss: 1677.718
[18,     1] loss: 1666.989
[19,     1] loss: 1658.397
[20,     1] loss: 1655.054
[21,     1] loss: 1621.855
[22,     1] loss: 1617.364
[23,     1] loss: 1591.486
[24,     1] loss: 1573.445
[25,     1] loss: 1558.595
[26,     1] loss: 1530.571
[27,     1] loss: 1500.650
[28,     1] loss: 1487.568
[29,     1] loss: 1425.898
[30,     1] loss: 1412.176
[31,     1] loss: 1441.837
[32,     1] loss: 1370.581
[33,     1] loss: 1375.027
[34,     1] loss: 1397.531
[35,     1] loss: 1405.413
[36,     1] loss: 1336.396
[37,     1] loss: 1394.214
[38,     1] loss: 1276.213
[39,     1] loss: 1343.966
[40,     1] loss: 1325.317
[41,     1] loss: 1226.401
[42,     1] loss: 1258.560
[43,     1] loss: 1238.921
[44,     1] loss: 1279.433
[45,     1] loss: 1185.084
[46,     1] loss: 1232.129
[47,     1] loss: 1222.407
[48,     1] loss: 1166.854
[49,     1] loss: 1203.255
[50,     1] loss: 1224.284
[51,     1] loss: 1220.081
[52,     1] loss: 1115.899
[53,     1] loss: 1172.972
[54,     1] loss: 1220.163
[55,     1] loss: 1196.013
[56,     1] loss: 1116.842
[57,     1] loss: 1185.978
[58,     1] loss: 1149.742
[59,     1] loss: 1189.033
[60,     1] loss: 1160.693
[61,     1] loss: 1099.491
[62,     1] loss: 1297.317
[63,     1] loss: 1060.499
[64,     1] loss: 1241.633
[65,     1] loss: 1083.546
[66,     1] loss: 1153.762
[67,     1] loss: 1215.495
[68,     1] loss: 1039.867
[69,     1] loss: 1220.172
[70,     1] loss: 1120.183
[71,     1] loss: 1065.440
[72,     1] loss: 1102.556
[73,     1] loss: 1010.417
[74,     1] loss: 1026.961
[75,     1] loss: 1105.643
[76,     1] loss: 1070.032
[77,     1] loss: 1024.309
[78,     1] loss: 1055.135
[79,     1] loss: 1028.558
[80,     1] loss: 1026.276
[81,     1] loss: 995.788
[82,     1] loss: 989.775
[83,     1] loss: 958.371
[84,     1] loss: 946.211
[85,     1] loss: 992.213
[86,     1] loss: 896.153
[87,     1] loss: 969.054
[88,     1] loss: 943.877
[89,     1] loss: 985.475
[90,     1] loss: 919.616
[91,     1] loss: 878.298
[92,     1] loss: 921.312
[93,     1] loss: 944.029
[94,     1] loss: 913.728
[95,     1] loss: 854.990
[96,     1] loss: 848.702
[97,     1] loss: 821.659
[98,     1] loss: 895.138
[99,     1] loss: 794.328
[100,     1] loss: 896.302
[101,     1] loss: 781.285
[102,     1] loss: 848.438
[103,     1] loss: 796.062
[104,     1] loss: 838.846
[105,     1] loss: 857.674
[106,     1] loss: 968.070
Early stopping applied (best metric=0.8278681039810181)
Finished Training
Total time taken: 14.227301120758057
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1692.591
[2,     1] loss: 1687.441
[3,     1] loss: 1704.046
[4,     1] loss: 1695.323
[5,     1] loss: 1682.551
[6,     1] loss: 1690.695
[7,     1] loss: 1691.473
[8,     1] loss: 1689.108
[9,     1] loss: 1681.700
[10,     1] loss: 1674.505
[11,     1] loss: 1658.810
[12,     1] loss: 1640.312
[13,     1] loss: 1619.502
[14,     1] loss: 1598.514
[15,     1] loss: 1586.390
[16,     1] loss: 1547.151
[17,     1] loss: 1507.165
[18,     1] loss: 1464.243
[19,     1] loss: 1436.382
[20,     1] loss: 1468.401
[21,     1] loss: 1431.853
[22,     1] loss: 1399.338
[23,     1] loss: 1443.458
[24,     1] loss: 1424.348
[25,     1] loss: 1376.045
[26,     1] loss: 1385.563
[27,     1] loss: 1391.561
[28,     1] loss: 1350.770
[29,     1] loss: 1355.846
[30,     1] loss: 1312.474
[31,     1] loss: 1316.447
[32,     1] loss: 1293.807
[33,     1] loss: 1339.059
[34,     1] loss: 1280.320
[35,     1] loss: 1299.920
[36,     1] loss: 1245.442
[37,     1] loss: 1259.019
[38,     1] loss: 1232.357
[39,     1] loss: 1284.499
[40,     1] loss: 1168.827
[41,     1] loss: 1193.378
[42,     1] loss: 1162.714
[43,     1] loss: 1137.587
[44,     1] loss: 1167.805
[45,     1] loss: 1174.067
[46,     1] loss: 1132.773
[47,     1] loss: 1161.130
[48,     1] loss: 1191.301
[49,     1] loss: 1133.698
[50,     1] loss: 1112.097
[51,     1] loss: 1119.964
[52,     1] loss: 1127.977
[53,     1] loss: 1040.220
[54,     1] loss: 1119.817
[55,     1] loss: 1366.501
[56,     1] loss: 1169.520
[57,     1] loss: 1078.577
[58,     1] loss: 1059.366
[59,     1] loss: 1140.998
[60,     1] loss: 1013.380
[61,     1] loss: 1078.687
[62,     1] loss: 972.402
[63,     1] loss: 1045.720
[64,     1] loss: 970.309
[65,     1] loss: 975.021
[66,     1] loss: 1070.794
[67,     1] loss: 980.595
[68,     1] loss: 908.599
[69,     1] loss: 984.020
[70,     1] loss: 907.944
Early stopping applied (best metric=0.7808078527450562)
Finished Training
Total time taken: 11.604246854782104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1702.944
[2,     1] loss: 1693.148
[3,     1] loss: 1697.234
[4,     1] loss: 1695.560
[5,     1] loss: 1690.727
[6,     1] loss: 1690.280
[7,     1] loss: 1689.840
[8,     1] loss: 1690.057
[9,     1] loss: 1690.982
[10,     1] loss: 1686.719
[11,     1] loss: 1687.754
[12,     1] loss: 1688.373
[13,     1] loss: 1682.630
[14,     1] loss: 1674.343
[15,     1] loss: 1671.984
[16,     1] loss: 1663.437
[17,     1] loss: 1647.996
[18,     1] loss: 1623.710
[19,     1] loss: 1608.586
[20,     1] loss: 1585.902
[21,     1] loss: 1573.674
[22,     1] loss: 1543.536
[23,     1] loss: 1525.018
[24,     1] loss: 1476.388
[25,     1] loss: 1403.248
[26,     1] loss: 1395.630
[27,     1] loss: 1404.077
[28,     1] loss: 1369.495
[29,     1] loss: 1352.301
[30,     1] loss: 1386.839
[31,     1] loss: 1365.705
[32,     1] loss: 1367.260
[33,     1] loss: 1348.588
[34,     1] loss: 1312.467
[35,     1] loss: 1348.421
[36,     1] loss: 1339.387
[37,     1] loss: 1256.311
[38,     1] loss: 1313.832
[39,     1] loss: 1266.338
[40,     1] loss: 1291.043
[41,     1] loss: 1265.100
[42,     1] loss: 1293.771
[43,     1] loss: 1240.875
[44,     1] loss: 1252.940
[45,     1] loss: 1211.910
[46,     1] loss: 1251.204
[47,     1] loss: 1239.509
[48,     1] loss: 1160.429
[49,     1] loss: 1204.406
[50,     1] loss: 1101.383
[51,     1] loss: 1139.649
[52,     1] loss: 1107.814
[53,     1] loss: 1183.248
[54,     1] loss: 1077.527
[55,     1] loss: 1225.382
[56,     1] loss: 1105.251
[57,     1] loss: 1096.902
[58,     1] loss: 1130.808
[59,     1] loss: 1104.970
[60,     1] loss: 1104.931
[61,     1] loss: 1067.083
[62,     1] loss: 1056.054
Early stopping applied (best metric=0.8349965810775757)
Finished Training
Total time taken: 10.265216827392578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1698.740
[2,     1] loss: 1691.706
[3,     1] loss: 1693.978
[4,     1] loss: 1690.629
[5,     1] loss: 1688.322
[6,     1] loss: 1692.459
[7,     1] loss: 1687.760
[8,     1] loss: 1682.703
[9,     1] loss: 1681.160
[10,     1] loss: 1679.327
[11,     1] loss: 1663.214
[12,     1] loss: 1654.530
[13,     1] loss: 1624.231
[14,     1] loss: 1592.531
[15,     1] loss: 1576.692
[16,     1] loss: 1520.949
[17,     1] loss: 1524.114
[18,     1] loss: 1505.575
[19,     1] loss: 1474.104
[20,     1] loss: 1450.155
[21,     1] loss: 1446.989
[22,     1] loss: 1462.844
[23,     1] loss: 1377.786
[24,     1] loss: 1352.430
[25,     1] loss: 1388.519
[26,     1] loss: 1354.159
[27,     1] loss: 1468.062
[28,     1] loss: 1377.991
[29,     1] loss: 1365.775
[30,     1] loss: 1313.959
[31,     1] loss: 1342.152
[32,     1] loss: 1302.917
[33,     1] loss: 1314.225
[34,     1] loss: 1317.182
[35,     1] loss: 1326.342
[36,     1] loss: 1288.189
[37,     1] loss: 1225.443
[38,     1] loss: 1280.405
[39,     1] loss: 1232.620
[40,     1] loss: 1242.312
[41,     1] loss: 1189.345
[42,     1] loss: 1178.604
[43,     1] loss: 1201.906
[44,     1] loss: 1130.000
[45,     1] loss: 1141.850
[46,     1] loss: 1129.273
[47,     1] loss: 1118.698
[48,     1] loss: 1116.628
[49,     1] loss: 1124.946
[50,     1] loss: 1172.023
[51,     1] loss: 1073.023
[52,     1] loss: 1083.791
[53,     1] loss: 1005.087
[54,     1] loss: 1084.841
[55,     1] loss: 1016.264
[56,     1] loss: 1057.722
[57,     1] loss: 1087.453
[58,     1] loss: 1034.983
[59,     1] loss: 1031.668
[60,     1] loss: 980.765
[61,     1] loss: 911.308
[62,     1] loss: 1012.468
[63,     1] loss: 1010.811
[64,     1] loss: 910.240
Early stopping applied (best metric=0.872734785079956)
Finished Training
Total time taken: 8.60818076133728
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1699.785
[2,     1] loss: 1694.216
[3,     1] loss: 1694.917
[4,     1] loss: 1688.385
[5,     1] loss: 1693.971
[6,     1] loss: 1688.926
[7,     1] loss: 1687.642
[8,     1] loss: 1688.590
[9,     1] loss: 1688.468
[10,     1] loss: 1687.347
[11,     1] loss: 1685.051
[12,     1] loss: 1684.865
[13,     1] loss: 1683.801
[14,     1] loss: 1670.049
[15,     1] loss: 1661.190
[16,     1] loss: 1646.614
[17,     1] loss: 1630.769
[18,     1] loss: 1611.270
[19,     1] loss: 1578.195
[20,     1] loss: 1549.827
[21,     1] loss: 1543.741
[22,     1] loss: 1535.490
[23,     1] loss: 1482.453
[24,     1] loss: 1446.830
[25,     1] loss: 1461.810
[26,     1] loss: 1398.135
[27,     1] loss: 1413.594
[28,     1] loss: 1421.695
[29,     1] loss: 1390.493
[30,     1] loss: 1437.932
[31,     1] loss: 1452.157
[32,     1] loss: 1451.556
[33,     1] loss: 1392.299
[34,     1] loss: 1332.842
[35,     1] loss: 1296.323
[36,     1] loss: 1327.910
[37,     1] loss: 1301.151
[38,     1] loss: 1309.843
[39,     1] loss: 1345.489
[40,     1] loss: 1300.925
[41,     1] loss: 1281.982
[42,     1] loss: 1292.365
[43,     1] loss: 1209.462
[44,     1] loss: 1209.312
[45,     1] loss: 1278.065
[46,     1] loss: 1184.137
[47,     1] loss: 1187.715
[48,     1] loss: 1241.950
[49,     1] loss: 1188.625
[50,     1] loss: 1164.693
[51,     1] loss: 1091.390
[52,     1] loss: 1113.467
[53,     1] loss: 1103.400
[54,     1] loss: 1104.359
[55,     1] loss: 1021.769
[56,     1] loss: 1162.300
[57,     1] loss: 1038.573
[58,     1] loss: 1121.356
[59,     1] loss: 1035.749
[60,     1] loss: 1018.722
[61,     1] loss: 1003.510
[62,     1] loss: 1004.019
[63,     1] loss: 1020.128
[64,     1] loss: 1065.719
[65,     1] loss: 973.326
[66,     1] loss: 984.991
[67,     1] loss: 919.404
[68,     1] loss: 950.151
[69,     1] loss: 908.019
Early stopping applied (best metric=0.8734095096588135)
Finished Training
Total time taken: 11.437239170074463
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1697.940
[2,     1] loss: 1694.860
[3,     1] loss: 1687.056
[4,     1] loss: 1692.393
[5,     1] loss: 1693.777
[6,     1] loss: 1688.594
[7,     1] loss: 1687.142
[8,     1] loss: 1676.332
[9,     1] loss: 1689.449
[10,     1] loss: 1683.396
[11,     1] loss: 1666.801
[12,     1] loss: 1658.187
[13,     1] loss: 1644.448
[14,     1] loss: 1612.694
[15,     1] loss: 1599.250
[16,     1] loss: 1582.368
[17,     1] loss: 1531.060
[18,     1] loss: 1513.440
[19,     1] loss: 1516.089
[20,     1] loss: 1524.649
[21,     1] loss: 1491.733
[22,     1] loss: 1450.326
[23,     1] loss: 1437.568
[24,     1] loss: 1464.814
[25,     1] loss: 1433.343
[26,     1] loss: 1398.470
[27,     1] loss: 1369.743
[28,     1] loss: 1348.408
[29,     1] loss: 1376.552
[30,     1] loss: 1303.691
[31,     1] loss: 1327.944
[32,     1] loss: 1286.594
[33,     1] loss: 1234.471
[34,     1] loss: 1290.141
[35,     1] loss: 1242.694
[36,     1] loss: 1232.985
[37,     1] loss: 1368.664
[38,     1] loss: 1232.513
[39,     1] loss: 1201.188
[40,     1] loss: 1237.490
[41,     1] loss: 1169.120
[42,     1] loss: 1251.480
[43,     1] loss: 1160.310
Early stopping applied (best metric=0.8819149732589722)
Finished Training
Total time taken: 5.819121837615967
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1696.323
[2,     1] loss: 1691.787
[3,     1] loss: 1688.674
[4,     1] loss: 1688.923
[5,     1] loss: 1693.626
[6,     1] loss: 1691.694
[7,     1] loss: 1686.833
[8,     1] loss: 1687.909
[9,     1] loss: 1692.112
[10,     1] loss: 1684.763
[11,     1] loss: 1679.525
[12,     1] loss: 1674.963
[13,     1] loss: 1669.790
[14,     1] loss: 1648.796
[15,     1] loss: 1620.493
[16,     1] loss: 1587.075
[17,     1] loss: 1560.161
[18,     1] loss: 1540.931
[19,     1] loss: 1492.017
[20,     1] loss: 1494.883
[21,     1] loss: 1440.106
[22,     1] loss: 1504.189
[23,     1] loss: 1353.065
[24,     1] loss: 1455.215
[25,     1] loss: 1386.012
[26,     1] loss: 1311.008
[27,     1] loss: 1353.587
[28,     1] loss: 1372.369
[29,     1] loss: 1321.408
[30,     1] loss: 1359.094
[31,     1] loss: 1330.426
[32,     1] loss: 1336.207
[33,     1] loss: 1292.432
[34,     1] loss: 1332.579
[35,     1] loss: 1357.025
[36,     1] loss: 1281.696
[37,     1] loss: 1277.910
[38,     1] loss: 1344.426
[39,     1] loss: 1235.361
[40,     1] loss: 1294.301
[41,     1] loss: 1332.899
[42,     1] loss: 1224.275
[43,     1] loss: 1292.936
[44,     1] loss: 1233.533
[45,     1] loss: 1271.677
[46,     1] loss: 1206.977
[47,     1] loss: 1210.201
[48,     1] loss: 1200.017
[49,     1] loss: 1227.271
[50,     1] loss: 1252.695
[51,     1] loss: 1144.618
[52,     1] loss: 1186.542
[53,     1] loss: 1106.951
[54,     1] loss: 1176.604
[55,     1] loss: 1129.850
[56,     1] loss: 1158.032
[57,     1] loss: 1097.684
[58,     1] loss: 1128.810
[59,     1] loss: 1090.692
[60,     1] loss: 1067.471
[61,     1] loss: 1010.108
[62,     1] loss: 1103.372
[63,     1] loss: 987.810
[64,     1] loss: 1079.462
[65,     1] loss: 991.849
[66,     1] loss: 1061.151
[67,     1] loss: 1032.231
[68,     1] loss: 1079.082
[69,     1] loss: 1007.124
[70,     1] loss: 989.586
[71,     1] loss: 990.324
[72,     1] loss: 954.132
[73,     1] loss: 958.223
[74,     1] loss: 1008.214
[75,     1] loss: 941.689
[76,     1] loss: 944.668
[77,     1] loss: 812.478
[78,     1] loss: 917.331
[79,     1] loss: 857.101
[80,     1] loss: 975.997
[81,     1] loss: 849.307
[82,     1] loss: 867.507
[83,     1] loss: 786.452
[84,     1] loss: 836.319
[85,     1] loss: 872.797
[86,     1] loss: 833.047
[87,     1] loss: 839.669
[88,     1] loss: 746.116
[89,     1] loss: 698.001
[90,     1] loss: 790.151
[91,     1] loss: 742.630
[92,     1] loss: 763.978
[93,     1] loss: 733.614
[94,     1] loss: 758.855
[95,     1] loss: 737.529
Early stopping applied (best metric=0.7897094488143921)
Finished Training
Total time taken: 13.980294466018677
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1687.947
[2,     1] loss: 1704.345
[3,     1] loss: 1693.449
[4,     1] loss: 1689.717
[5,     1] loss: 1698.874
[6,     1] loss: 1687.148
[7,     1] loss: 1689.443
[8,     1] loss: 1687.503
[9,     1] loss: 1689.199
[10,     1] loss: 1690.590
[11,     1] loss: 1690.241
[12,     1] loss: 1681.303
[13,     1] loss: 1676.348
[14,     1] loss: 1670.004
[15,     1] loss: 1659.136
[16,     1] loss: 1645.714
[17,     1] loss: 1617.394
[18,     1] loss: 1613.389
[19,     1] loss: 1578.737
[20,     1] loss: 1544.305
[21,     1] loss: 1553.831
[22,     1] loss: 1500.878
[23,     1] loss: 1512.241
[24,     1] loss: 1488.190
[25,     1] loss: 1450.870
[26,     1] loss: 1464.754
[27,     1] loss: 1428.001
[28,     1] loss: 1452.672
[29,     1] loss: 1430.999
[30,     1] loss: 1473.700
[31,     1] loss: 1391.818
[32,     1] loss: 1429.337
[33,     1] loss: 1356.328
[34,     1] loss: 1339.389
[35,     1] loss: 1344.717
[36,     1] loss: 1325.830
[37,     1] loss: 1275.008
[38,     1] loss: 1283.115
[39,     1] loss: 1331.243
[40,     1] loss: 1307.539
[41,     1] loss: 1283.670
[42,     1] loss: 1237.759
[43,     1] loss: 1228.770
[44,     1] loss: 1238.785
[45,     1] loss: 1205.640
[46,     1] loss: 1165.468
[47,     1] loss: 1175.888
[48,     1] loss: 1213.185
[49,     1] loss: 1100.023
[50,     1] loss: 1134.875
[51,     1] loss: 1121.885
[52,     1] loss: 1114.944
[53,     1] loss: 1171.695
[54,     1] loss: 1116.575
[55,     1] loss: 1065.547
[56,     1] loss: 1080.522
[57,     1] loss: 1046.268
[58,     1] loss: 1044.702
[59,     1] loss: 1033.046
[60,     1] loss: 996.272
[61,     1] loss: 1224.342
[62,     1] loss: 1179.703
[63,     1] loss: 918.643
[64,     1] loss: 1031.068
[65,     1] loss: 950.035
Early stopping applied (best metric=0.8868205547332764)
Finished Training
Total time taken: 10.778227090835571
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1697.426
[2,     1] loss: 1692.016
[3,     1] loss: 1690.889
[4,     1] loss: 1692.745
[5,     1] loss: 1687.369
[6,     1] loss: 1692.908
[7,     1] loss: 1687.527
[8,     1] loss: 1678.220
[9,     1] loss: 1680.958
[10,     1] loss: 1665.957
[11,     1] loss: 1651.184
[12,     1] loss: 1636.263
[13,     1] loss: 1617.044
[14,     1] loss: 1605.165
[15,     1] loss: 1559.412
[16,     1] loss: 1497.762
[17,     1] loss: 1551.484
[18,     1] loss: 1473.188
[19,     1] loss: 1467.422
[20,     1] loss: 1438.168
[21,     1] loss: 1502.353
[22,     1] loss: 1411.239
[23,     1] loss: 1450.088
[24,     1] loss: 1441.771
[25,     1] loss: 1404.813
[26,     1] loss: 1398.155
[27,     1] loss: 1364.835
[28,     1] loss: 1331.092
[29,     1] loss: 1409.700
[30,     1] loss: 1329.313
[31,     1] loss: 1321.130
[32,     1] loss: 1395.344
[33,     1] loss: 1277.373
[34,     1] loss: 1316.051
[35,     1] loss: 1284.573
[36,     1] loss: 1266.154
[37,     1] loss: 1287.207
[38,     1] loss: 1168.863
[39,     1] loss: 1326.040
[40,     1] loss: 1213.033
[41,     1] loss: 1334.792
[42,     1] loss: 1177.929
[43,     1] loss: 1201.846
[44,     1] loss: 1116.883
[45,     1] loss: 1223.308
[46,     1] loss: 1103.459
[47,     1] loss: 1203.310
[48,     1] loss: 1039.333
[49,     1] loss: 1066.367
[50,     1] loss: 1075.323
[51,     1] loss: 1037.966
[52,     1] loss: 1023.346
[53,     1] loss: 1083.161
[54,     1] loss: 1035.893
[55,     1] loss: 971.155
[56,     1] loss: 1138.637
[57,     1] loss: 908.091
[58,     1] loss: 1184.946
[59,     1] loss: 1151.559
[60,     1] loss: 927.291
Early stopping applied (best metric=0.7599015235900879)
Finished Training
Total time taken: 8.866186141967773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1689.269
[2,     1] loss: 1694.951
[3,     1] loss: 1695.497
[4,     1] loss: 1695.154
[5,     1] loss: 1692.859
[6,     1] loss: 1690.526
[7,     1] loss: 1686.985
[8,     1] loss: 1688.155
[9,     1] loss: 1687.559
[10,     1] loss: 1686.050
[11,     1] loss: 1677.437
[12,     1] loss: 1659.480
[13,     1] loss: 1653.214
[14,     1] loss: 1626.870
[15,     1] loss: 1602.903
[16,     1] loss: 1605.365
[17,     1] loss: 1566.459
[18,     1] loss: 1520.738
[19,     1] loss: 1520.297
[20,     1] loss: 1489.878
[21,     1] loss: 1449.078
[22,     1] loss: 1511.545
[23,     1] loss: 1454.104
[24,     1] loss: 1442.126
[25,     1] loss: 1408.505
[26,     1] loss: 1390.560
[27,     1] loss: 1424.691
[28,     1] loss: 1380.517
[29,     1] loss: 1423.357
[30,     1] loss: 1411.974
[31,     1] loss: 1354.121
[32,     1] loss: 1312.981
[33,     1] loss: 1373.027
[34,     1] loss: 1390.353
[35,     1] loss: 1326.909
[36,     1] loss: 1293.818
[37,     1] loss: 1291.624
[38,     1] loss: 1205.679
[39,     1] loss: 1267.578
[40,     1] loss: 1268.277
[41,     1] loss: 1200.521
[42,     1] loss: 1207.511
[43,     1] loss: 1242.472
[44,     1] loss: 1249.755
[45,     1] loss: 1170.899
[46,     1] loss: 1114.035
[47,     1] loss: 1124.865
[48,     1] loss: 1136.024
[49,     1] loss: 1100.198
[50,     1] loss: 1114.520
[51,     1] loss: 1214.112
[52,     1] loss: 1192.671
[53,     1] loss: 1163.006
[54,     1] loss: 1161.615
[55,     1] loss: 1267.892
[56,     1] loss: 1125.656
[57,     1] loss: 1180.171
Early stopping applied (best metric=0.7680213451385498)
Finished Training
Total time taken: 9.491199970245361
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1696.882
[2,     1] loss: 1696.377
[3,     1] loss: 1693.796
[4,     1] loss: 1691.184
[5,     1] loss: 1693.471
[6,     1] loss: 1696.592
[7,     1] loss: 1692.446
[8,     1] loss: 1695.871
[9,     1] loss: 1694.979
[10,     1] loss: 1686.654
[11,     1] loss: 1687.644
[12,     1] loss: 1686.910
[13,     1] loss: 1690.572
[14,     1] loss: 1680.720
[15,     1] loss: 1677.802
[16,     1] loss: 1676.097
[17,     1] loss: 1658.068
[18,     1] loss: 1644.921
[19,     1] loss: 1619.492
[20,     1] loss: 1585.049
[21,     1] loss: 1563.079
[22,     1] loss: 1535.011
[23,     1] loss: 1494.148
[24,     1] loss: 1497.399
[25,     1] loss: 1454.803
[26,     1] loss: 1435.100
[27,     1] loss: 1406.765
[28,     1] loss: 1421.106
[29,     1] loss: 1438.132
[30,     1] loss: 1430.787
[31,     1] loss: 1327.378
[32,     1] loss: 1358.325
[33,     1] loss: 1378.645
[34,     1] loss: 1338.335
[35,     1] loss: 1346.261
[36,     1] loss: 1370.749
[37,     1] loss: 1316.922
[38,     1] loss: 1311.831
[39,     1] loss: 1324.992
[40,     1] loss: 1274.719
[41,     1] loss: 1281.532
[42,     1] loss: 1236.567
[43,     1] loss: 1248.024
[44,     1] loss: 1238.195
[45,     1] loss: 1192.205
[46,     1] loss: 1200.594
[47,     1] loss: 1215.423
[48,     1] loss: 1194.012
[49,     1] loss: 1226.497
[50,     1] loss: 1192.824
[51,     1] loss: 1152.678
[52,     1] loss: 1121.788
[53,     1] loss: 1061.717
[54,     1] loss: 1030.010
[55,     1] loss: 1054.619
[56,     1] loss: 1006.601
[57,     1] loss: 1082.456
[58,     1] loss: 1083.876
[59,     1] loss: 1006.166
[60,     1] loss: 919.043
[61,     1] loss: 999.180
[62,     1] loss: 981.317
[63,     1] loss: 1014.699
[64,     1] loss: 1044.231
[65,     1] loss: 970.678
[66,     1] loss: 945.160
[67,     1] loss: 1062.541
[68,     1] loss: 945.433
[69,     1] loss: 970.836
Early stopping applied (best metric=0.7117602229118347)
Finished Training
Total time taken: 9.300195693969727
{'Hydroxylation-K Validation Accuracy': 0.7799645390070922, 'Hydroxylation-K Validation Sensitivity': 0.7007407407407408, 'Hydroxylation-K Validation Specificity': 0.8, 'Hydroxylation-K Validation Precision': 0.48399249443367093, 'Hydroxylation-K AUC ROC': 0.8355165692007798, 'Hydroxylation-K AUC PR': 0.6423598022923961, 'Hydroxylation-K MCC': 0.4454295200511868, 'Hydroxylation-K F1': 0.5648210501390227, 'Validation Loss (Hydroxylation-K)': 0.4093994736671448, 'Hydroxylation-P Validation Accuracy': 0.7752003959189889, 'Hydroxylation-P Validation Sensitivity': 0.772962962962963, 'Hydroxylation-P Validation Specificity': 0.7757419322659485, 'Hydroxylation-P Validation Precision': 0.43005002428068684, 'Hydroxylation-P AUC ROC': 0.8330157542592078, 'Hydroxylation-P AUC PR': 0.5796580260091789, 'Hydroxylation-P MCC': 0.4509144407992521, 'Hydroxylation-P F1': 0.5505522358229802, 'Validation Loss (Hydroxylation-P)': 0.39079912900924685, 'Validation Loss (total)': 0.8001986026763916, 'TimeToTrain': 10.387352466583252}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006921241411413722,
 'learning_rate_Hydroxylation-K': 0.005332370823168448,
 'learning_rate_Hydroxylation-P': 0.0052914544865706615,
 'log_base': 2.0439485188022886,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1836192250,
 'sample_weights': [3.7233038960190554, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.419562843627548,
 'weight_decay_Hydroxylation-K': 3.637720891529596,
 'weight_decay_Hydroxylation-P': 7.262675747941865}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.531
[2,     1] loss: 1400.588
[3,     1] loss: 1402.069
[4,     1] loss: 1396.282
[5,     1] loss: 1388.650
[6,     1] loss: 1386.259
[7,     1] loss: 1372.488
[8,     1] loss: 1343.767
[9,     1] loss: 1297.890
[10,     1] loss: 1255.068
[11,     1] loss: 1260.393
[12,     1] loss: 1214.546
[13,     1] loss: 1224.972
[14,     1] loss: 1163.888
[15,     1] loss: 1198.408
[16,     1] loss: 1171.942
[17,     1] loss: 1186.265
[18,     1] loss: 1134.345
[19,     1] loss: 1185.964
[20,     1] loss: 1140.095
[21,     1] loss: 1131.260
[22,     1] loss: 1170.526
[23,     1] loss: 1138.101
[24,     1] loss: 1132.700
[25,     1] loss: 1092.034
[26,     1] loss: 1063.796
[27,     1] loss: 1114.394
[28,     1] loss: 1027.489
[29,     1] loss: 1056.565
[30,     1] loss: 1056.566
[31,     1] loss: 1015.206
[32,     1] loss: 1023.227
[33,     1] loss: 1070.860
[34,     1] loss: 1013.078
[35,     1] loss: 1021.118
[36,     1] loss: 1018.944
[37,     1] loss: 950.184
[38,     1] loss: 1031.857
[39,     1] loss: 1033.026
[40,     1] loss: 992.781
[41,     1] loss: 984.125
[42,     1] loss: 995.925
[43,     1] loss: 1069.703
[44,     1] loss: 950.569
[45,     1] loss: 968.583
[46,     1] loss: 959.762
[47,     1] loss: 942.639
[48,     1] loss: 973.992
[49,     1] loss: 936.095
[50,     1] loss: 883.683
[51,     1] loss: 927.237
[52,     1] loss: 871.189
[53,     1] loss: 866.845
[54,     1] loss: 894.399
[55,     1] loss: 794.624
[56,     1] loss: 853.919
[57,     1] loss: 957.960
[58,     1] loss: 807.672
[59,     1] loss: 796.810
[60,     1] loss: 817.716
[61,     1] loss: 760.988
[62,     1] loss: 857.915
[63,     1] loss: 704.647
[64,     1] loss: 752.916
[65,     1] loss: 780.527
[66,     1] loss: 725.412
[67,     1] loss: 715.833
[68,     1] loss: 745.989
[69,     1] loss: 1043.898
[70,     1] loss: 700.687
[71,     1] loss: 762.207
[72,     1] loss: 708.004
[73,     1] loss: 796.778
[74,     1] loss: 681.571
[75,     1] loss: 775.983
[76,     1] loss: 629.318
[77,     1] loss: 710.595
[78,     1] loss: 631.602
[79,     1] loss: 691.149
[80,     1] loss: 613.313
[81,     1] loss: 614.055
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0071280067884366615,
 'learning_rate_Hydroxylation-K': 0.00043951349778658717,
 'learning_rate_Hydroxylation-P': 0.0013095629460579594,
 'log_base': 1.0571998396627904,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 931383501,
 'sample_weights': [2.335266069532836, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.497801810766445,
 'weight_decay_Hydroxylation-K': 9.494211242735796,
 'weight_decay_Hydroxylation-P': 8.790659224320907}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9812.352
[2,     1] loss: 9739.654
[3,     1] loss: 9761.412
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005044276176837324,
 'learning_rate_Hydroxylation-K': 0.00439305195838456,
 'learning_rate_Hydroxylation-P': 0.003444880193812912,
 'log_base': 1.0549282305007825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4212058950,
 'sample_weights': [30.013134404292625, 3.7517863288439366],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.681777728605723,
 'weight_decay_Hydroxylation-K': 5.669449865536118,
 'weight_decay_Hydroxylation-P': 8.987613773352555}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10120.598
[2,     1] loss: 10123.817
[3,     1] loss: 10246.586
[4,     1] loss: 10141.458
[5,     1] loss: 10132.829
[6,     1] loss: 10107.412
[7,     1] loss: 10120.938
[8,     1] loss: 10127.205
[9,     1] loss: 10104.547
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004553544928708839,
 'learning_rate_Hydroxylation-K': 0.003420399070035423,
 'learning_rate_Hydroxylation-P': 0.0008101222088652433,
 'log_base': 1.2219353508579214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 443481963,
 'sample_weights': [31.220454607871943, 3.90270716814415],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.272661909170196,
 'weight_decay_Hydroxylation-K': 7.641613176129204,
 'weight_decay_Hydroxylation-P': 4.9257646140972}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2707.561
[2,     1] loss: 2703.378
[3,     1] loss: 2696.671
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018362113970704784,
 'learning_rate_Hydroxylation-K': 0.0012250062448680794,
 'learning_rate_Hydroxylation-P': 0.0038374569990483947,
 'log_base': 2.3972649225670426,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2053271227,
 'sample_weights': [8.329060260691005, 1.0411726411923328],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.0236971288092995,
 'weight_decay_Hydroxylation-K': 5.950683547326639,
 'weight_decay_Hydroxylation-P': 0.6858641891150166}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1311.133
[2,     1] loss: 1310.627
[3,     1] loss: 1309.591
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00916914504429327,
 'learning_rate_Hydroxylation-K': 0.0005848648505869725,
 'learning_rate_Hydroxylation-P': 0.0057445941189524395,
 'log_base': 1.214850589129123,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2766075102,
 'sample_weights': [1.9094004153765034, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.935504389426489,
 'weight_decay_Hydroxylation-K': 8.74946429111554,
 'weight_decay_Hydroxylation-P': 5.945893173755587}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2776.543
[2,     1] loss: 2785.375
[3,     1] loss: 2793.336
[4,     1] loss: 2783.601
[5,     1] loss: 2765.894
[6,     1] loss: 2768.966
[7,     1] loss: 2750.762
[8,     1] loss: 2722.713
[9,     1] loss: 2677.786
[10,     1] loss: 2581.862
[11,     1] loss: 2527.218
[12,     1] loss: 2457.527
[13,     1] loss: 2310.996
[14,     1] loss: 2360.121
[15,     1] loss: 2305.982
[16,     1] loss: 2369.586
[17,     1] loss: 2315.094
[18,     1] loss: 2314.874
[19,     1] loss: 2249.219
[20,     1] loss: 2334.748
[21,     1] loss: 2129.488
[22,     1] loss: 2172.750
[23,     1] loss: 2081.882
[24,     1] loss: 2048.386
[25,     1] loss: 2070.739
[26,     1] loss: 2060.641
[27,     1] loss: 2337.562
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008652627513771109,
 'learning_rate_Hydroxylation-K': 0.0004912191193012114,
 'learning_rate_Hydroxylation-P': 0.004829507908054061,
 'log_base': 1.0955765910045736,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 768319288,
 'sample_weights': [8.577914575587254, 1.0722806289128035],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.855024137901694,
 'weight_decay_Hydroxylation-K': 8.634663314202536,
 'weight_decay_Hydroxylation-P': 9.014946363051784}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5940.439
[2,     1] loss: 5927.278
[3,     1] loss: 6048.301
[4,     1] loss: 5917.723
[5,     1] loss: 5950.831
[6,     1] loss: 5910.827
[7,     1] loss: 5917.360
[8,     1] loss: 5921.335
[9,     1] loss: 5912.326
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006280727465704061,
 'learning_rate_Hydroxylation-K': 0.0028680895074448117,
 'learning_rate_Hydroxylation-P': 0.002823800553120204,
 'log_base': 1.713710695891363,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2007577891,
 'sample_weights': [18.289095814738538, 2.2862250480189434],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.264440539038148,
 'weight_decay_Hydroxylation-K': 5.95619340487296,
 'weight_decay_Hydroxylation-P': 4.399119884767117}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1567.015
[2,     1] loss: 1561.483
[3,     1] loss: 1559.271
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00569863160287689,
 'learning_rate_Hydroxylation-K': 0.008246762663782893,
 'learning_rate_Hydroxylation-P': 0.00042667210486677597,
 'log_base': 2.876274990791045,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3501763015,
 'sample_weights': [3.0992462690391247, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.374996628307631,
 'weight_decay_Hydroxylation-K': 8.90093569559921,
 'weight_decay_Hydroxylation-P': 2.6781059407765486}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.570
[2,     1] loss: 1238.281
[3,     1] loss: 1241.321
[4,     1] loss: 1245.805
[5,     1] loss: 1241.424
[6,     1] loss: 1237.902
[7,     1] loss: 1238.748
[8,     1] loss: 1237.143
[9,     1] loss: 1232.514
[10,     1] loss: 1220.401
[11,     1] loss: 1203.052
[12,     1] loss: 1160.546
[13,     1] loss: 1119.984
[14,     1] loss: 1073.252
[15,     1] loss: 1035.463
[16,     1] loss: 995.378
[17,     1] loss: 1008.655
[18,     1] loss: 987.741
[19,     1] loss: 1027.261
[20,     1] loss: 915.690
[21,     1] loss: 967.724
[22,     1] loss: 929.920
[23,     1] loss: 919.344
[24,     1] loss: 926.654
[25,     1] loss: 947.428
[26,     1] loss: 897.346
[27,     1] loss: 933.906
[28,     1] loss: 876.631
[29,     1] loss: 895.397
[30,     1] loss: 886.100
[31,     1] loss: 866.205
[32,     1] loss: 821.198
[33,     1] loss: 898.779
[34,     1] loss: 832.386
[35,     1] loss: 790.090
[36,     1] loss: 797.405
[37,     1] loss: 833.838
[38,     1] loss: 783.281
[39,     1] loss: 838.907
[40,     1] loss: 914.438
[41,     1] loss: 774.609
[42,     1] loss: 790.849
[43,     1] loss: 781.844
Early stopping applied (best metric=1.019524097442627)
Finished Training
Total time taken: 5.835005760192871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.405
[2,     1] loss: 1244.128
[3,     1] loss: 1242.445
[4,     1] loss: 1241.830
[5,     1] loss: 1240.810
[6,     1] loss: 1241.144
[7,     1] loss: 1238.981
[8,     1] loss: 1240.561
[9,     1] loss: 1235.787
[10,     1] loss: 1240.270
[11,     1] loss: 1237.603
[12,     1] loss: 1228.415
[13,     1] loss: 1224.148
[14,     1] loss: 1211.683
[15,     1] loss: 1186.543
[16,     1] loss: 1172.508
[17,     1] loss: 1140.931
[18,     1] loss: 1120.224
[19,     1] loss: 1069.261
[20,     1] loss: 1014.632
[21,     1] loss: 1057.517
[22,     1] loss: 1059.382
[23,     1] loss: 1001.269
[24,     1] loss: 1008.181
[25,     1] loss: 995.539
[26,     1] loss: 970.462
[27,     1] loss: 996.301
[28,     1] loss: 998.597
[29,     1] loss: 965.759
[30,     1] loss: 969.535
[31,     1] loss: 955.724
[32,     1] loss: 969.306
[33,     1] loss: 922.333
[34,     1] loss: 919.112
[35,     1] loss: 866.412
[36,     1] loss: 884.951
[37,     1] loss: 892.079
[38,     1] loss: 862.878
[39,     1] loss: 877.338
[40,     1] loss: 883.999
[41,     1] loss: 934.366
[42,     1] loss: 890.678
[43,     1] loss: 791.779
[44,     1] loss: 835.230
[45,     1] loss: 807.118
[46,     1] loss: 834.609
[47,     1] loss: 802.865
[48,     1] loss: 777.394
[49,     1] loss: 787.301
[50,     1] loss: 765.503
[51,     1] loss: 705.680
[52,     1] loss: 762.075
[53,     1] loss: 874.527
[54,     1] loss: 1117.772
[55,     1] loss: 784.403
[56,     1] loss: 826.359
[57,     1] loss: 830.196
[58,     1] loss: 780.340
[59,     1] loss: 810.827
Early stopping applied (best metric=0.8081569671630859)
Finished Training
Total time taken: 9.95000958442688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.045
[2,     1] loss: 1244.702
[3,     1] loss: 1245.118
[4,     1] loss: 1244.573
[5,     1] loss: 1243.522
[6,     1] loss: 1238.137
[7,     1] loss: 1240.526
[8,     1] loss: 1238.906
[9,     1] loss: 1241.061
[10,     1] loss: 1239.914
[11,     1] loss: 1240.872
[12,     1] loss: 1235.203
[13,     1] loss: 1229.552
[14,     1] loss: 1218.791
[15,     1] loss: 1200.654
[16,     1] loss: 1161.275
[17,     1] loss: 1130.016
[18,     1] loss: 1084.935
[19,     1] loss: 1049.266
[20,     1] loss: 1076.331
[21,     1] loss: 1032.666
[22,     1] loss: 1040.063
[23,     1] loss: 1098.204
[24,     1] loss: 1014.068
[25,     1] loss: 1005.149
[26,     1] loss: 999.028
[27,     1] loss: 989.960
[28,     1] loss: 1002.967
[29,     1] loss: 985.317
[30,     1] loss: 961.809
[31,     1] loss: 966.965
[32,     1] loss: 971.624
[33,     1] loss: 941.018
[34,     1] loss: 947.647
[35,     1] loss: 923.962
[36,     1] loss: 920.891
[37,     1] loss: 866.931
[38,     1] loss: 869.218
[39,     1] loss: 941.166
[40,     1] loss: 946.515
[41,     1] loss: 901.516
[42,     1] loss: 848.586
[43,     1] loss: 832.913
[44,     1] loss: 851.132
[45,     1] loss: 837.391
[46,     1] loss: 856.026
[47,     1] loss: 977.056
[48,     1] loss: 832.067
[49,     1] loss: 855.963
[50,     1] loss: 867.863
[51,     1] loss: 858.382
[52,     1] loss: 778.227
[53,     1] loss: 845.565
[54,     1] loss: 747.611
[55,     1] loss: 818.329
[56,     1] loss: 697.159
[57,     1] loss: 768.023
Early stopping applied (best metric=0.8237216472625732)
Finished Training
Total time taken: 9.457011461257935
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.412
[2,     1] loss: 1242.458
[3,     1] loss: 1240.868
[4,     1] loss: 1238.448
[5,     1] loss: 1239.783
[6,     1] loss: 1235.351
[7,     1] loss: 1233.135
[8,     1] loss: 1222.004
[9,     1] loss: 1204.277
[10,     1] loss: 1149.750
[11,     1] loss: 1122.593
[12,     1] loss: 1113.991
[13,     1] loss: 1057.094
[14,     1] loss: 1031.927
[15,     1] loss: 1047.493
[16,     1] loss: 991.314
[17,     1] loss: 1071.323
[18,     1] loss: 972.609
[19,     1] loss: 1050.088
[20,     1] loss: 966.143
[21,     1] loss: 980.699
[22,     1] loss: 1011.800
[23,     1] loss: 938.087
[24,     1] loss: 998.220
[25,     1] loss: 934.294
[26,     1] loss: 913.596
[27,     1] loss: 911.971
[28,     1] loss: 878.355
[29,     1] loss: 942.982
[30,     1] loss: 901.165
[31,     1] loss: 914.948
[32,     1] loss: 900.289
[33,     1] loss: 843.681
[34,     1] loss: 889.073
[35,     1] loss: 899.127
[36,     1] loss: 886.710
[37,     1] loss: 858.532
[38,     1] loss: 826.407
[39,     1] loss: 834.563
[40,     1] loss: 792.703
[41,     1] loss: 767.482
[42,     1] loss: 762.789
[43,     1] loss: 804.425
[44,     1] loss: 1002.786
[45,     1] loss: 1038.588
[46,     1] loss: 816.916
[47,     1] loss: 910.459
[48,     1] loss: 914.094
[49,     1] loss: 821.587
[50,     1] loss: 904.483
[51,     1] loss: 828.807
[52,     1] loss: 795.222
[53,     1] loss: 822.377
[54,     1] loss: 865.124
[55,     1] loss: 772.172
[56,     1] loss: 763.916
[57,     1] loss: 762.057
[58,     1] loss: 768.513
[59,     1] loss: 701.567
[60,     1] loss: 746.979
[61,     1] loss: 752.031
[62,     1] loss: 733.357
[63,     1] loss: 697.988
[64,     1] loss: 674.944
[65,     1] loss: 723.238
[66,     1] loss: 814.565
[67,     1] loss: 897.566
[68,     1] loss: 639.165
[69,     1] loss: 747.936
[70,     1] loss: 681.208
[71,     1] loss: 682.381
[72,     1] loss: 624.155
[73,     1] loss: 695.114
[74,     1] loss: 625.140
[75,     1] loss: 642.748
[76,     1] loss: 642.311
[77,     1] loss: 602.660
[78,     1] loss: 596.316
[79,     1] loss: 573.010
[80,     1] loss: 504.486
[81,     1] loss: 535.460
[82,     1] loss: 532.408
[83,     1] loss: 608.852
Early stopping applied (best metric=0.8721956014633179)
Finished Training
Total time taken: 11.215009689331055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1244.649
[2,     1] loss: 1245.086
[3,     1] loss: 1244.684
[4,     1] loss: 1240.385
[5,     1] loss: 1243.658
[6,     1] loss: 1240.656
[7,     1] loss: 1236.107
[8,     1] loss: 1232.931
[9,     1] loss: 1216.333
[10,     1] loss: 1204.380
[11,     1] loss: 1164.769
[12,     1] loss: 1155.144
[13,     1] loss: 1140.211
[14,     1] loss: 1106.701
[15,     1] loss: 1104.698
[16,     1] loss: 1027.911
[17,     1] loss: 1020.043
[18,     1] loss: 1047.858
[19,     1] loss: 1061.282
[20,     1] loss: 1020.486
[21,     1] loss: 1047.442
[22,     1] loss: 1000.216
[23,     1] loss: 1037.105
[24,     1] loss: 951.363
[25,     1] loss: 981.752
[26,     1] loss: 1011.582
[27,     1] loss: 997.583
[28,     1] loss: 951.676
[29,     1] loss: 974.704
[30,     1] loss: 906.313
[31,     1] loss: 939.866
[32,     1] loss: 901.875
[33,     1] loss: 910.877
[34,     1] loss: 858.086
[35,     1] loss: 868.642
[36,     1] loss: 831.201
[37,     1] loss: 841.542
[38,     1] loss: 832.010
[39,     1] loss: 834.398
[40,     1] loss: 809.496
[41,     1] loss: 812.178
[42,     1] loss: 827.300
[43,     1] loss: 842.366
[44,     1] loss: 873.339
[45,     1] loss: 806.277
[46,     1] loss: 704.783
[47,     1] loss: 775.032
[48,     1] loss: 761.234
[49,     1] loss: 748.228
[50,     1] loss: 689.522
[51,     1] loss: 756.212
[52,     1] loss: 820.540
[53,     1] loss: 768.094
[54,     1] loss: 736.029
[55,     1] loss: 708.205
[56,     1] loss: 690.844
[57,     1] loss: 707.395
[58,     1] loss: 634.477
[59,     1] loss: 664.391
[60,     1] loss: 745.777
[61,     1] loss: 984.293
[62,     1] loss: 1157.802
[63,     1] loss: 864.556
[64,     1] loss: 763.457
[65,     1] loss: 924.472
[66,     1] loss: 920.300
[67,     1] loss: 847.848
[68,     1] loss: 840.042
[69,     1] loss: 870.229
[70,     1] loss: 832.675
[71,     1] loss: 807.125
[72,     1] loss: 746.103
[73,     1] loss: 813.461
[74,     1] loss: 776.408
[75,     1] loss: 754.660
[76,     1] loss: 709.927
[77,     1] loss: 644.183
[78,     1] loss: 650.298
Early stopping applied (best metric=0.757581353187561)
Finished Training
Total time taken: 12.980012893676758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.649
[2,     1] loss: 1241.048
[3,     1] loss: 1245.455
[4,     1] loss: 1239.551
[5,     1] loss: 1237.213
[6,     1] loss: 1234.142
[7,     1] loss: 1230.154
[8,     1] loss: 1215.733
[9,     1] loss: 1193.465
[10,     1] loss: 1158.998
[11,     1] loss: 1119.846
[12,     1] loss: 1076.826
[13,     1] loss: 1036.347
[14,     1] loss: 1003.870
[15,     1] loss: 1004.776
[16,     1] loss: 1029.260
[17,     1] loss: 957.185
[18,     1] loss: 1014.029
[19,     1] loss: 975.739
[20,     1] loss: 928.530
[21,     1] loss: 966.883
[22,     1] loss: 930.030
[23,     1] loss: 898.341
[24,     1] loss: 882.641
[25,     1] loss: 951.471
[26,     1] loss: 932.356
[27,     1] loss: 860.950
[28,     1] loss: 870.171
[29,     1] loss: 867.285
[30,     1] loss: 863.889
[31,     1] loss: 857.726
[32,     1] loss: 831.378
[33,     1] loss: 832.445
[34,     1] loss: 855.144
[35,     1] loss: 974.412
[36,     1] loss: 909.951
[37,     1] loss: 817.937
[38,     1] loss: 849.165
[39,     1] loss: 853.042
[40,     1] loss: 797.112
[41,     1] loss: 871.762
[42,     1] loss: 784.762
[43,     1] loss: 849.569
[44,     1] loss: 727.419
[45,     1] loss: 775.084
[46,     1] loss: 768.105
[47,     1] loss: 770.699
[48,     1] loss: 729.925
[49,     1] loss: 799.937
[50,     1] loss: 684.199
[51,     1] loss: 731.262
[52,     1] loss: 652.587
[53,     1] loss: 686.349
[54,     1] loss: 715.473
[55,     1] loss: 685.025
[56,     1] loss: 674.862
[57,     1] loss: 696.695
[58,     1] loss: 824.698
[59,     1] loss: 746.905
[60,     1] loss: 657.861
[61,     1] loss: 686.465
[62,     1] loss: 645.485
[63,     1] loss: 797.028
[64,     1] loss: 649.029
[65,     1] loss: 665.155
[66,     1] loss: 660.220
[67,     1] loss: 650.134
[68,     1] loss: 654.477
[69,     1] loss: 605.066
[70,     1] loss: 672.113
[71,     1] loss: 653.795
[72,     1] loss: 509.980
[73,     1] loss: 610.733
[74,     1] loss: 766.970
[75,     1] loss: 511.820
Early stopping applied (best metric=0.8920239210128784)
Finished Training
Total time taken: 10.213011026382446
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.412
[2,     1] loss: 1257.866
[3,     1] loss: 1245.271
[4,     1] loss: 1239.126
[5,     1] loss: 1240.355
[6,     1] loss: 1240.695
[7,     1] loss: 1240.204
[8,     1] loss: 1240.730
[9,     1] loss: 1238.819
[10,     1] loss: 1236.410
[11,     1] loss: 1234.613
[12,     1] loss: 1228.562
[13,     1] loss: 1212.213
[14,     1] loss: 1195.320
[15,     1] loss: 1166.906
[16,     1] loss: 1131.290
[17,     1] loss: 1087.992
[18,     1] loss: 1068.522
[19,     1] loss: 1012.847
[20,     1] loss: 1043.291
[21,     1] loss: 1040.176
[22,     1] loss: 1014.226
[23,     1] loss: 974.860
[24,     1] loss: 965.000
[25,     1] loss: 947.879
[26,     1] loss: 965.213
[27,     1] loss: 949.533
[28,     1] loss: 976.385
[29,     1] loss: 909.508
[30,     1] loss: 889.128
[31,     1] loss: 937.392
[32,     1] loss: 889.252
[33,     1] loss: 875.422
[34,     1] loss: 908.387
[35,     1] loss: 908.406
[36,     1] loss: 858.562
[37,     1] loss: 921.577
[38,     1] loss: 981.513
[39,     1] loss: 861.265
[40,     1] loss: 909.236
[41,     1] loss: 836.955
[42,     1] loss: 881.775
[43,     1] loss: 832.289
[44,     1] loss: 863.184
[45,     1] loss: 844.107
[46,     1] loss: 831.958
[47,     1] loss: 806.820
[48,     1] loss: 777.961
[49,     1] loss: 763.615
[50,     1] loss: 759.150
[51,     1] loss: 784.098
[52,     1] loss: 808.971
[53,     1] loss: 948.791
[54,     1] loss: 883.081
[55,     1] loss: 773.414
[56,     1] loss: 835.310
[57,     1] loss: 784.998
[58,     1] loss: 767.726
[59,     1] loss: 792.765
[60,     1] loss: 746.997
[61,     1] loss: 823.030
[62,     1] loss: 707.671
[63,     1] loss: 730.141
[64,     1] loss: 729.151
[65,     1] loss: 655.479
[66,     1] loss: 683.154
[67,     1] loss: 691.734
[68,     1] loss: 642.091
[69,     1] loss: 654.625
[70,     1] loss: 573.821
[71,     1] loss: 594.148
[72,     1] loss: 593.323
[73,     1] loss: 859.973
[74,     1] loss: 1323.014
[75,     1] loss: 594.931
[76,     1] loss: 950.675
[77,     1] loss: 764.866
[78,     1] loss: 763.276
[79,     1] loss: 857.004
[80,     1] loss: 851.667
[81,     1] loss: 780.140
[82,     1] loss: 737.103
[83,     1] loss: 804.585
[84,     1] loss: 704.405
Early stopping applied (best metric=0.8146193027496338)
Finished Training
Total time taken: 13.99401569366455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.525
[2,     1] loss: 1248.266
[3,     1] loss: 1246.609
[4,     1] loss: 1241.250
[5,     1] loss: 1239.343
[6,     1] loss: 1240.694
[7,     1] loss: 1238.200
[8,     1] loss: 1241.340
[9,     1] loss: 1242.832
[10,     1] loss: 1236.873
[11,     1] loss: 1233.803
[12,     1] loss: 1231.703
[13,     1] loss: 1226.248
[14,     1] loss: 1216.688
[15,     1] loss: 1192.146
[16,     1] loss: 1180.755
[17,     1] loss: 1152.944
[18,     1] loss: 1111.310
[19,     1] loss: 1080.320
[20,     1] loss: 1106.454
[21,     1] loss: 1029.720
[22,     1] loss: 1084.122
[23,     1] loss: 1024.151
[24,     1] loss: 1048.007
[25,     1] loss: 971.988
[26,     1] loss: 1010.081
[27,     1] loss: 976.360
[28,     1] loss: 970.441
[29,     1] loss: 968.030
[30,     1] loss: 944.868
[31,     1] loss: 921.120
[32,     1] loss: 942.296
[33,     1] loss: 916.514
[34,     1] loss: 905.300
[35,     1] loss: 911.290
[36,     1] loss: 916.427
[37,     1] loss: 977.679
[38,     1] loss: 863.709
[39,     1] loss: 952.195
[40,     1] loss: 872.569
[41,     1] loss: 923.996
[42,     1] loss: 858.830
[43,     1] loss: 921.122
[44,     1] loss: 876.865
[45,     1] loss: 881.797
[46,     1] loss: 862.741
[47,     1] loss: 865.599
[48,     1] loss: 788.754
[49,     1] loss: 848.701
[50,     1] loss: 811.916
[51,     1] loss: 839.118
[52,     1] loss: 768.205
[53,     1] loss: 847.003
[54,     1] loss: 775.880
Early stopping applied (best metric=0.8565089106559753)
Finished Training
Total time taken: 7.298007249832153
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.287
[2,     1] loss: 1250.590
[3,     1] loss: 1241.375
[4,     1] loss: 1242.226
[5,     1] loss: 1251.041
[6,     1] loss: 1241.043
[7,     1] loss: 1243.307
[8,     1] loss: 1236.731
[9,     1] loss: 1239.853
[10,     1] loss: 1239.562
[11,     1] loss: 1236.567
[12,     1] loss: 1233.820
[13,     1] loss: 1232.393
[14,     1] loss: 1223.452
[15,     1] loss: 1206.785
[16,     1] loss: 1183.792
[17,     1] loss: 1156.931
[18,     1] loss: 1129.551
[19,     1] loss: 1094.261
[20,     1] loss: 1075.292
[21,     1] loss: 1003.164
[22,     1] loss: 1083.040
[23,     1] loss: 1021.617
[24,     1] loss: 1056.436
[25,     1] loss: 956.241
[26,     1] loss: 1053.624
[27,     1] loss: 983.125
[28,     1] loss: 1026.318
[29,     1] loss: 1016.476
[30,     1] loss: 983.031
[31,     1] loss: 1001.827
[32,     1] loss: 971.988
[33,     1] loss: 933.210
[34,     1] loss: 925.663
[35,     1] loss: 944.219
[36,     1] loss: 905.005
[37,     1] loss: 934.164
[38,     1] loss: 871.864
[39,     1] loss: 942.810
[40,     1] loss: 880.915
[41,     1] loss: 883.953
[42,     1] loss: 847.412
[43,     1] loss: 886.871
[44,     1] loss: 778.159
[45,     1] loss: 815.743
[46,     1] loss: 766.422
[47,     1] loss: 752.514
[48,     1] loss: 764.874
[49,     1] loss: 775.237
[50,     1] loss: 793.280
[51,     1] loss: 744.635
[52,     1] loss: 801.196
[53,     1] loss: 933.866
[54,     1] loss: 810.491
[55,     1] loss: 761.751
[56,     1] loss: 754.277
[57,     1] loss: 770.411
[58,     1] loss: 776.568
[59,     1] loss: 741.635
[60,     1] loss: 715.475
[61,     1] loss: 678.870
[62,     1] loss: 698.882
[63,     1] loss: 684.859
[64,     1] loss: 785.197
[65,     1] loss: 746.061
[66,     1] loss: 668.570
[67,     1] loss: 654.232
[68,     1] loss: 660.321
[69,     1] loss: 668.535
[70,     1] loss: 682.238
[71,     1] loss: 567.214
[72,     1] loss: 734.907
[73,     1] loss: 762.442
[74,     1] loss: 560.733
Early stopping applied (best metric=0.8624223470687866)
Finished Training
Total time taken: 12.324011087417603
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1247.212
[2,     1] loss: 1246.034
[3,     1] loss: 1244.147
[4,     1] loss: 1240.623
[5,     1] loss: 1241.952
[6,     1] loss: 1236.354
[7,     1] loss: 1234.588
[8,     1] loss: 1239.523
[9,     1] loss: 1225.075
[10,     1] loss: 1213.076
[11,     1] loss: 1185.503
[12,     1] loss: 1156.984
[13,     1] loss: 1121.031
[14,     1] loss: 1099.833
[15,     1] loss: 1054.961
[16,     1] loss: 991.366
[17,     1] loss: 1108.183
[18,     1] loss: 1038.068
[19,     1] loss: 1038.248
[20,     1] loss: 1015.731
[21,     1] loss: 1084.574
[22,     1] loss: 967.128
[23,     1] loss: 993.521
[24,     1] loss: 1009.504
[25,     1] loss: 992.492
[26,     1] loss: 946.390
[27,     1] loss: 961.154
[28,     1] loss: 905.201
[29,     1] loss: 967.831
[30,     1] loss: 936.473
[31,     1] loss: 896.807
[32,     1] loss: 864.902
[33,     1] loss: 864.762
[34,     1] loss: 851.416
[35,     1] loss: 848.248
[36,     1] loss: 853.459
[37,     1] loss: 811.773
[38,     1] loss: 828.317
[39,     1] loss: 953.087
[40,     1] loss: 1037.329
[41,     1] loss: 811.284
[42,     1] loss: 911.920
[43,     1] loss: 806.060
[44,     1] loss: 925.797
[45,     1] loss: 860.540
[46,     1] loss: 801.193
[47,     1] loss: 844.626
[48,     1] loss: 779.967
[49,     1] loss: 850.063
[50,     1] loss: 721.135
[51,     1] loss: 795.173
[52,     1] loss: 800.551
[53,     1] loss: 752.015
[54,     1] loss: 707.634
[55,     1] loss: 674.837
[56,     1] loss: 727.831
[57,     1] loss: 722.998
[58,     1] loss: 685.952
[59,     1] loss: 756.145
[60,     1] loss: 708.942
[61,     1] loss: 688.298
[62,     1] loss: 670.424
[63,     1] loss: 682.678
[64,     1] loss: 624.190
[65,     1] loss: 881.359
[66,     1] loss: 986.894
[67,     1] loss: 648.880
[68,     1] loss: 783.568
[69,     1] loss: 728.672
[70,     1] loss: 706.650
[71,     1] loss: 738.940
[72,     1] loss: 648.735
[73,     1] loss: 713.436
[74,     1] loss: 637.033
[75,     1] loss: 768.925
[76,     1] loss: 661.217
[77,     1] loss: 597.781
Early stopping applied (best metric=0.8449550867080688)
Finished Training
Total time taken: 12.836012601852417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.997
[2,     1] loss: 1245.520
[3,     1] loss: 1241.838
[4,     1] loss: 1240.105
[5,     1] loss: 1237.975
[6,     1] loss: 1232.168
[7,     1] loss: 1228.455
[8,     1] loss: 1210.477
[9,     1] loss: 1186.408
[10,     1] loss: 1151.196
[11,     1] loss: 1128.499
[12,     1] loss: 1098.395
[13,     1] loss: 1065.106
[14,     1] loss: 1031.428
[15,     1] loss: 996.340
[16,     1] loss: 977.931
[17,     1] loss: 1027.135
[18,     1] loss: 973.821
[19,     1] loss: 1017.379
[20,     1] loss: 934.705
[21,     1] loss: 993.565
[22,     1] loss: 993.196
[23,     1] loss: 917.339
[24,     1] loss: 935.048
[25,     1] loss: 894.051
[26,     1] loss: 933.525
[27,     1] loss: 889.937
[28,     1] loss: 904.816
[29,     1] loss: 871.737
[30,     1] loss: 824.924
[31,     1] loss: 840.474
[32,     1] loss: 829.856
[33,     1] loss: 890.829
[34,     1] loss: 868.437
[35,     1] loss: 847.685
[36,     1] loss: 845.724
[37,     1] loss: 845.180
[38,     1] loss: 780.496
[39,     1] loss: 790.543
[40,     1] loss: 761.076
[41,     1] loss: 819.329
Early stopping applied (best metric=1.036603569984436)
Finished Training
Total time taken: 5.527005910873413
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.802
[2,     1] loss: 1237.765
[3,     1] loss: 1241.292
[4,     1] loss: 1241.260
[5,     1] loss: 1232.075
[6,     1] loss: 1228.466
[7,     1] loss: 1213.438
[8,     1] loss: 1204.925
[9,     1] loss: 1154.649
[10,     1] loss: 1143.820
[11,     1] loss: 1106.503
[12,     1] loss: 1056.144
[13,     1] loss: 1059.489
[14,     1] loss: 1039.298
[15,     1] loss: 1045.715
[16,     1] loss: 1007.235
[17,     1] loss: 1003.548
[18,     1] loss: 1051.508
[19,     1] loss: 979.495
[20,     1] loss: 976.582
[21,     1] loss: 961.830
[22,     1] loss: 966.796
[23,     1] loss: 1003.622
[24,     1] loss: 968.781
[25,     1] loss: 980.592
[26,     1] loss: 902.070
[27,     1] loss: 960.193
[28,     1] loss: 935.169
[29,     1] loss: 928.527
[30,     1] loss: 920.846
[31,     1] loss: 891.337
[32,     1] loss: 924.142
[33,     1] loss: 899.331
[34,     1] loss: 876.984
[35,     1] loss: 909.560
[36,     1] loss: 937.021
[37,     1] loss: 835.639
[38,     1] loss: 893.343
[39,     1] loss: 885.679
[40,     1] loss: 869.836
[41,     1] loss: 845.916
[42,     1] loss: 849.199
[43,     1] loss: 817.022
[44,     1] loss: 799.010
[45,     1] loss: 890.791
[46,     1] loss: 828.905
[47,     1] loss: 724.212
[48,     1] loss: 769.016
[49,     1] loss: 736.772
[50,     1] loss: 793.764
[51,     1] loss: 787.305
[52,     1] loss: 687.029
[53,     1] loss: 739.464
[54,     1] loss: 841.108
[55,     1] loss: 778.010
[56,     1] loss: 696.719
[57,     1] loss: 701.993
[58,     1] loss: 673.602
[59,     1] loss: 777.516
[60,     1] loss: 692.579
[61,     1] loss: 630.435
[62,     1] loss: 736.251
[63,     1] loss: 662.958
[64,     1] loss: 714.082
[65,     1] loss: 748.291
[66,     1] loss: 623.724
[67,     1] loss: 651.497
[68,     1] loss: 574.217
[69,     1] loss: 633.755
[70,     1] loss: 730.749
[71,     1] loss: 619.205
Early stopping applied (best metric=0.8257732391357422)
Finished Training
Total time taken: 11.832011938095093
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.743
[2,     1] loss: 1237.919
[3,     1] loss: 1240.400
[4,     1] loss: 1243.757
[5,     1] loss: 1239.374
[6,     1] loss: 1242.369
[7,     1] loss: 1243.361
[8,     1] loss: 1239.827
[9,     1] loss: 1238.361
[10,     1] loss: 1236.409
[11,     1] loss: 1234.473
[12,     1] loss: 1226.193
[13,     1] loss: 1214.092
[14,     1] loss: 1186.554
[15,     1] loss: 1151.139
[16,     1] loss: 1139.418
[17,     1] loss: 1108.998
[18,     1] loss: 1054.174
[19,     1] loss: 1045.927
[20,     1] loss: 1048.802
[21,     1] loss: 1083.651
[22,     1] loss: 974.013
[23,     1] loss: 1075.687
[24,     1] loss: 981.280
[25,     1] loss: 987.540
[26,     1] loss: 972.039
[27,     1] loss: 968.912
[28,     1] loss: 965.290
[29,     1] loss: 934.836
[30,     1] loss: 928.116
[31,     1] loss: 963.893
[32,     1] loss: 941.223
[33,     1] loss: 955.154
[34,     1] loss: 914.025
[35,     1] loss: 930.149
[36,     1] loss: 897.314
[37,     1] loss: 856.331
[38,     1] loss: 875.039
[39,     1] loss: 865.896
[40,     1] loss: 826.573
[41,     1] loss: 899.656
[42,     1] loss: 851.838
[43,     1] loss: 857.208
[44,     1] loss: 868.180
[45,     1] loss: 844.214
[46,     1] loss: 841.608
[47,     1] loss: 819.223
[48,     1] loss: 803.350
[49,     1] loss: 829.671
[50,     1] loss: 823.177
[51,     1] loss: 735.579
[52,     1] loss: 795.079
[53,     1] loss: 759.698
[54,     1] loss: 739.862
[55,     1] loss: 756.650
[56,     1] loss: 789.656
[57,     1] loss: 837.071
[58,     1] loss: 791.849
[59,     1] loss: 721.750
[60,     1] loss: 729.521
[61,     1] loss: 760.593
[62,     1] loss: 733.842
[63,     1] loss: 748.479
[64,     1] loss: 813.333
[65,     1] loss: 701.036
[66,     1] loss: 693.401
[67,     1] loss: 839.152
[68,     1] loss: 661.050
[69,     1] loss: 713.855
[70,     1] loss: 615.031
[71,     1] loss: 736.362
[72,     1] loss: 647.173
[73,     1] loss: 690.393
[74,     1] loss: 638.655
[75,     1] loss: 631.554
[76,     1] loss: 768.754
[77,     1] loss: 650.115
[78,     1] loss: 569.218
[79,     1] loss: 601.418
[80,     1] loss: 530.864
[81,     1] loss: 758.263
[82,     1] loss: 821.048
[83,     1] loss: 525.363
[84,     1] loss: 853.944
[85,     1] loss: 583.803
Early stopping applied (best metric=0.8231131434440613)
Finished Training
Total time taken: 14.029013872146606
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.570
[2,     1] loss: 1238.396
[3,     1] loss: 1242.547
[4,     1] loss: 1243.514
[5,     1] loss: 1239.755
[6,     1] loss: 1238.189
[7,     1] loss: 1220.032
[8,     1] loss: 1213.432
[9,     1] loss: 1179.354
[10,     1] loss: 1133.436
[11,     1] loss: 1103.280
[12,     1] loss: 1081.970
[13,     1] loss: 1036.699
[14,     1] loss: 1044.336
[15,     1] loss: 1042.864
[16,     1] loss: 989.675
[17,     1] loss: 1028.303
[18,     1] loss: 1026.808
[19,     1] loss: 946.151
[20,     1] loss: 1002.984
[21,     1] loss: 940.236
[22,     1] loss: 948.532
[23,     1] loss: 970.525
[24,     1] loss: 947.452
[25,     1] loss: 921.496
[26,     1] loss: 952.263
[27,     1] loss: 921.082
[28,     1] loss: 932.262
[29,     1] loss: 900.328
[30,     1] loss: 906.864
[31,     1] loss: 863.738
[32,     1] loss: 876.319
[33,     1] loss: 856.019
[34,     1] loss: 815.038
[35,     1] loss: 845.180
[36,     1] loss: 813.460
[37,     1] loss: 786.921
[38,     1] loss: 875.172
[39,     1] loss: 844.708
[40,     1] loss: 793.803
[41,     1] loss: 803.423
[42,     1] loss: 702.572
[43,     1] loss: 812.486
[44,     1] loss: 935.308
[45,     1] loss: 710.729
[46,     1] loss: 805.551
[47,     1] loss: 760.554
[48,     1] loss: 733.668
[49,     1] loss: 736.427
[50,     1] loss: 788.646
[51,     1] loss: 707.107
Early stopping applied (best metric=0.9234132766723633)
Finished Training
Total time taken: 7.746007442474365
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.896
[2,     1] loss: 1243.039
[3,     1] loss: 1249.643
[4,     1] loss: 1242.291
[5,     1] loss: 1236.804
[6,     1] loss: 1227.908
[7,     1] loss: 1213.691
[8,     1] loss: 1194.427
[9,     1] loss: 1152.780
[10,     1] loss: 1089.440
[11,     1] loss: 1052.395
[12,     1] loss: 1037.090
[13,     1] loss: 1042.921
[14,     1] loss: 960.512
[15,     1] loss: 1036.710
[16,     1] loss: 1009.196
[17,     1] loss: 994.822
[18,     1] loss: 981.953
[19,     1] loss: 969.350
[20,     1] loss: 925.214
[21,     1] loss: 923.818
[22,     1] loss: 937.083
[23,     1] loss: 883.368
[24,     1] loss: 903.296
[25,     1] loss: 972.870
[26,     1] loss: 909.191
[27,     1] loss: 892.865
[28,     1] loss: 884.813
[29,     1] loss: 881.887
[30,     1] loss: 860.061
[31,     1] loss: 817.074
[32,     1] loss: 871.118
[33,     1] loss: 770.897
[34,     1] loss: 785.387
[35,     1] loss: 846.182
[36,     1] loss: 788.027
[37,     1] loss: 740.751
Early stopping applied (best metric=0.9595832824707031)
Finished Training
Total time taken: 5.465005397796631
{'Hydroxylation-K Validation Accuracy': 0.7423167848699763, 'Hydroxylation-K Validation Sensitivity': 0.5918518518518519, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.4065223665223665, 'Hydroxylation-K AUC ROC': 0.74364522417154, 'Hydroxylation-K AUC PR': 0.5001663332642075, 'Hydroxylation-K MCC': 0.3291086537498117, 'Hydroxylation-K F1': 0.47637704324660846, 'Validation Loss (Hydroxylation-K)': 0.4866746703783671, 'Hydroxylation-P Validation Accuracy': 0.7802102600544811, 'Hydroxylation-P Validation Sensitivity': 0.7765608465608466, 'Hydroxylation-P Validation Specificity': 0.7810040401017507, 'Hydroxylation-P Validation Precision': 0.4388060639337544, 'Hydroxylation-P AUC ROC': 0.8307843184990424, 'Hydroxylation-P AUC PR': 0.5551541100851325, 'Hydroxylation-P MCC': 0.46075474295567703, 'Hydroxylation-P F1': 0.5578251389904685, 'Validation Loss (Hydroxylation-P)': 0.38800504604975383, 'Validation Loss (total)': 0.8746797164281209, 'TimeToTrain': 10.046743440628052}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035347286975971988,
 'learning_rate_Hydroxylation-K': 0.005850225798658033,
 'learning_rate_Hydroxylation-P': 0.0009093094745743838,
 'log_base': 2.9161528172498574,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3080687677,
 'sample_weights': [1.5813419615641606, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.421848395970086,
 'weight_decay_Hydroxylation-K': 5.5855583416906,
 'weight_decay_Hydroxylation-P': 8.623185736173227}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.083
[2,     1] loss: 1235.390
[3,     1] loss: 1234.874
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00511252006040703,
 'learning_rate_Hydroxylation-K': 0.005260512075811813,
 'learning_rate_Hydroxylation-P': 0.004111184467892772,
 'log_base': 2.6510433646335843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1778167460,
 'sample_weights': [1.559840605315577, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.342099567796738,
 'weight_decay_Hydroxylation-K': 9.307600956964619,
 'weight_decay_Hydroxylation-P': 6.709144294111906}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.865
[2,     1] loss: 1270.984
[3,     1] loss: 1270.971
[4,     1] loss: 1268.470
[5,     1] loss: 1267.026
[6,     1] loss: 1267.164
[7,     1] loss: 1264.588
[8,     1] loss: 1263.620
[9,     1] loss: 1265.261
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00565142245026126,
 'learning_rate_Hydroxylation-K': 0.003706247593062646,
 'learning_rate_Hydroxylation-P': 0.005073046159634581,
 'log_base': 2.279189937486297,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1497939918,
 'sample_weights': [1.7123314245783634, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.0714463254614386,
 'weight_decay_Hydroxylation-K': 5.550673075244942,
 'weight_decay_Hydroxylation-P': 3.560064207890476}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.660
[2,     1] loss: 1335.400
[3,     1] loss: 1334.921
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036773541561315294,
 'learning_rate_Hydroxylation-K': 0.008696307142693545,
 'learning_rate_Hydroxylation-P': 0.006991172946822565,
 'log_base': 1.0731835944800183,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 797143198,
 'sample_weights': [2.0264656921928705, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.515488303462947,
 'weight_decay_Hydroxylation-K': 4.253502698588099,
 'weight_decay_Hydroxylation-P': 0.6094935440608875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7712.655
[2,     1] loss: 7661.216
[3,     1] loss: 7654.733
[4,     1] loss: 7625.289
[5,     1] loss: 7635.894
[6,     1] loss: 7640.966
[7,     1] loss: 7644.483
[8,     1] loss: 7579.469
[9,     1] loss: 7626.774
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006080110592123153,
 'learning_rate_Hydroxylation-K': 0.002486279789951169,
 'learning_rate_Hydroxylation-P': 0.004518216823561947,
 'log_base': 1.2800819109469945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2188664562,
 'sample_weights': [23.636609304433563, 2.954689985192478],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.437310024290369,
 'weight_decay_Hydroxylation-K': 6.72744490416138,
 'weight_decay_Hydroxylation-P': 5.394331031159427}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2354.467
[2,     1] loss: 2337.555
[3,     1] loss: 2330.858
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004306421704422466,
 'learning_rate_Hydroxylation-K': 0.0023369784780977385,
 'learning_rate_Hydroxylation-P': 0.0006720529391939572,
 'log_base': 1.6136135178494022,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3198009085,
 'sample_weights': [6.760957550471734, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9199170585403018,
 'weight_decay_Hydroxylation-K': 0.7072703636442785,
 'weight_decay_Hydroxylation-P': 0.6926449269447819}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1655.558
[2,     1] loss: 1652.667
[3,     1] loss: 1646.069
[4,     1] loss: 1647.097
[5,     1] loss: 1645.628
[6,     1] loss: 1638.732
[7,     1] loss: 1642.918
[8,     1] loss: 1639.578
[9,     1] loss: 1642.618
[10,     1] loss: 1637.219
[11,     1] loss: 1629.269
[12,     1] loss: 1635.623
[13,     1] loss: 1619.692
[14,     1] loss: 1618.746
[15,     1] loss: 1606.957
[16,     1] loss: 1594.515
[17,     1] loss: 1560.714
[18,     1] loss: 1529.012
[19,     1] loss: 1511.314
[20,     1] loss: 1451.954
[21,     1] loss: 1435.907
[22,     1] loss: 1461.050
[23,     1] loss: 1390.394
[24,     1] loss: 1402.182
[25,     1] loss: 1393.058
[26,     1] loss: 1365.112
[27,     1] loss: 1412.601
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005180955981862851,
 'learning_rate_Hydroxylation-K': 0.0004830722256621441,
 'learning_rate_Hydroxylation-P': 0.004493150617615711,
 'log_base': 1.2946499708122898,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2082328468,
 'sample_weights': [3.489083779763471, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.4578581191000355,
 'weight_decay_Hydroxylation-K': 4.661772700683454,
 'weight_decay_Hydroxylation-P': 2.966585465454602}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2284.157
[2,     1] loss: 2276.101
[3,     1] loss: 2294.834
[4,     1] loss: 2264.985
[5,     1] loss: 2274.730
[6,     1] loss: 2268.009
[7,     1] loss: 2272.704
[8,     1] loss: 2269.521
[9,     1] loss: 2273.415
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002668118549374614,
 'learning_rate_Hydroxylation-K': 0.001311916456768255,
 'learning_rate_Hydroxylation-P': 0.0015663119930083378,
 'log_base': 1.083130005286834,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1188170643,
 'sample_weights': [6.464687045373927, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.583276191756228,
 'weight_decay_Hydroxylation-K': 6.803368386012018,
 'weight_decay_Hydroxylation-P': 9.364469175703256}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6799.411
[2,     1] loss: 6791.375
[3,     1] loss: 6765.050
[4,     1] loss: 6794.702
[5,     1] loss: 6772.583
[6,     1] loss: 6760.205
[7,     1] loss: 6736.509
[8,     1] loss: 6733.524
[9,     1] loss: 6712.794
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003625914539129368,
 'learning_rate_Hydroxylation-K': 0.0030432477105557364,
 'learning_rate_Hydroxylation-P': 0.002267139214751821,
 'log_base': 1.1517569181213474,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3981702172,
 'sample_weights': [20.905930654692003, 2.6133420043864106],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.221009986263526,
 'weight_decay_Hydroxylation-K': 8.814414980210458,
 'weight_decay_Hydroxylation-P': 6.334878586325822}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3848.953
[2,     1] loss: 3838.843
[3,     1] loss: 3837.449
[4,     1] loss: 3834.825
[5,     1] loss: 3830.807
[6,     1] loss: 3838.290
[7,     1] loss: 3814.974
[8,     1] loss: 3821.561
[9,     1] loss: 3812.583
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007525376541049084,
 'learning_rate_Hydroxylation-K': 0.002413483500690913,
 'learning_rate_Hydroxylation-P': 0.0065315560777814054,
 'log_base': 2.2334363192576174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2887003536,
 'sample_weights': [11.81584329946042, 1.4770373116490447],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.244339104003312,
 'weight_decay_Hydroxylation-K': 4.777002351988434,
 'weight_decay_Hydroxylation-P': 1.8463761931819342}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1350.959
[2,     1] loss: 1350.119
[3,     1] loss: 1346.803
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008567768311028963,
 'learning_rate_Hydroxylation-K': 0.005828341438365406,
 'learning_rate_Hydroxylation-P': 0.00552281026088991,
 'log_base': 1.6161258125575537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2534659762,
 'sample_weights': [2.0776070178712116, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9267970396257461,
 'weight_decay_Hydroxylation-K': 7.143808191922897,
 'weight_decay_Hydroxylation-P': 2.1719880684103288}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1643.953
[2,     1] loss: 1645.068
[3,     1] loss: 1647.584
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009475166089537608,
 'learning_rate_Hydroxylation-K': 0.0035156381561017493,
 'learning_rate_Hydroxylation-P': 0.009629359510888044,
 'log_base': 2.9483915306097264,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2022849117,
 'sample_weights': [3.4777760720602964, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3549045792248413,
 'weight_decay_Hydroxylation-K': 2.4198195358271475,
 'weight_decay_Hydroxylation-P': 2.508095768009245}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.807
[2,     1] loss: 1232.962
[3,     1] loss: 1240.294
[4,     1] loss: 1234.622
[5,     1] loss: 1244.543
[6,     1] loss: 1234.793
[7,     1] loss: 1233.499
[8,     1] loss: 1236.834
[9,     1] loss: 1235.075
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008176668684536956,
 'learning_rate_Hydroxylation-K': 0.008905258161590168,
 'learning_rate_Hydroxylation-P': 0.004684492353480266,
 'log_base': 2.760855113448796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3765123354,
 'sample_weights': [1.5439797000810684, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.592804565337852,
 'weight_decay_Hydroxylation-K': 8.719829659422698,
 'weight_decay_Hydroxylation-P': 4.161678779796227}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.987
[2,     1] loss: 1256.239
[3,     1] loss: 1253.524
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004059382105159164,
 'learning_rate_Hydroxylation-K': 0.006392779906002476,
 'learning_rate_Hydroxylation-P': 0.006690934284545564,
 'log_base': 2.6598219942326424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1954326496,
 'sample_weights': [1.6438962509598092, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.1886786685811,
 'weight_decay_Hydroxylation-K': 8.270805919482449,
 'weight_decay_Hydroxylation-P': 3.4821185623739126}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.536
[2,     1] loss: 1266.396
[3,     1] loss: 1267.015
[4,     1] loss: 1258.571
[5,     1] loss: 1262.336
[6,     1] loss: 1248.981
[7,     1] loss: 1223.143
[8,     1] loss: 1207.724
[9,     1] loss: 1158.491
[10,     1] loss: 1111.722
[11,     1] loss: 1093.748
[12,     1] loss: 1043.154
[13,     1] loss: 1076.922
[14,     1] loss: 1050.887
[15,     1] loss: 1051.035
[16,     1] loss: 998.114
[17,     1] loss: 1064.854
[18,     1] loss: 993.055
[19,     1] loss: 1002.135
[20,     1] loss: 967.183
[21,     1] loss: 965.852
[22,     1] loss: 924.765
[23,     1] loss: 869.799
[24,     1] loss: 971.052
[25,     1] loss: 977.524
[26,     1] loss: 1023.835
[27,     1] loss: 898.840
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006176585014379359,
 'learning_rate_Hydroxylation-K': 0.00048319203183871473,
 'learning_rate_Hydroxylation-P': 0.002746186163999033,
 'log_base': 1.1206978961515917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1800712465,
 'sample_weights': [1.706544794629521, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.372738629770266,
 'weight_decay_Hydroxylation-K': 4.541784952132927,
 'weight_decay_Hydroxylation-P': 3.8449079526993795}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4786.187
[2,     1] loss: 4775.971
[3,     1] loss: 4765.770
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009516552237422175,
 'learning_rate_Hydroxylation-K': 0.007973481729408139,
 'learning_rate_Hydroxylation-P': 0.0027689930631033233,
 'log_base': 1.0928374842106061,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1858871191,
 'sample_weights': [14.650456507257234, 1.8313776127090864],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.138758833328831,
 'weight_decay_Hydroxylation-K': 8.833055683925693,
 'weight_decay_Hydroxylation-P': 4.646959008496563}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6099.670
[2,     1] loss: 6109.063
[3,     1] loss: 6094.208
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009392596258296458,
 'learning_rate_Hydroxylation-K': 0.0051281966454133545,
 'learning_rate_Hydroxylation-P': 0.007889187084439704,
 'log_base': 2.946390157704531,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3303780973,
 'sample_weights': [18.804797988325536, 2.3506903030821427],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.6160080442858415,
 'weight_decay_Hydroxylation-K': 6.779452512895281,
 'weight_decay_Hydroxylation-P': 5.535246246044884}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.456
[2,     1] loss: 1238.699
[3,     1] loss: 1242.507
[4,     1] loss: 1231.094
[5,     1] loss: 1234.105
[6,     1] loss: 1232.070
[7,     1] loss: 1235.213
[8,     1] loss: 1230.393
[9,     1] loss: 1227.649
[10,     1] loss: 1225.068
[11,     1] loss: 1222.767
[12,     1] loss: 1213.133
[13,     1] loss: 1193.384
[14,     1] loss: 1158.011
[15,     1] loss: 1108.912
[16,     1] loss: 1141.855
[17,     1] loss: 1087.430
[18,     1] loss: 1067.174
[19,     1] loss: 1025.045
[20,     1] loss: 1034.875
[21,     1] loss: 1029.220
[22,     1] loss: 1016.802
[23,     1] loss: 998.964
[24,     1] loss: 977.933
[25,     1] loss: 968.559
[26,     1] loss: 963.752
[27,     1] loss: 950.696
[28,     1] loss: 937.631
[29,     1] loss: 918.801
[30,     1] loss: 975.372
[31,     1] loss: 974.368
[32,     1] loss: 912.638
[33,     1] loss: 903.890
[34,     1] loss: 838.794
[35,     1] loss: 877.699
[36,     1] loss: 933.147
[37,     1] loss: 978.915
[38,     1] loss: 905.291
[39,     1] loss: 826.524
[40,     1] loss: 863.933
[41,     1] loss: 825.029
[42,     1] loss: 822.158
[43,     1] loss: 876.417
[44,     1] loss: 871.838
[45,     1] loss: 775.724
[46,     1] loss: 782.298
[47,     1] loss: 741.499
[48,     1] loss: 752.098
[49,     1] loss: 842.480
[50,     1] loss: 1287.249
[51,     1] loss: 1016.042
[52,     1] loss: 880.476
[53,     1] loss: 867.409
[54,     1] loss: 967.803
[55,     1] loss: 930.009
[56,     1] loss: 850.542
[57,     1] loss: 876.064
[58,     1] loss: 881.696
[59,     1] loss: 794.915
[60,     1] loss: 843.854
[61,     1] loss: 794.036
[62,     1] loss: 758.734
[63,     1] loss: 752.220
[64,     1] loss: 792.840
[65,     1] loss: 738.167
Early stopping applied (best metric=0.801999032497406)
Finished Training
Total time taken: 10.782010793685913
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.948
[2,     1] loss: 1242.105
[3,     1] loss: 1235.193
[4,     1] loss: 1234.218
[5,     1] loss: 1228.158
[6,     1] loss: 1229.127
[7,     1] loss: 1231.838
[8,     1] loss: 1229.254
[9,     1] loss: 1212.028
[10,     1] loss: 1205.688
[11,     1] loss: 1175.470
[12,     1] loss: 1123.697
[13,     1] loss: 1117.142
[14,     1] loss: 1072.239
[15,     1] loss: 1060.272
[16,     1] loss: 1045.717
[17,     1] loss: 1018.524
[18,     1] loss: 989.886
[19,     1] loss: 976.181
[20,     1] loss: 997.556
[21,     1] loss: 1025.495
[22,     1] loss: 958.280
[23,     1] loss: 958.284
[24,     1] loss: 960.989
[25,     1] loss: 933.635
[26,     1] loss: 929.650
[27,     1] loss: 927.448
[28,     1] loss: 867.878
[29,     1] loss: 940.627
[30,     1] loss: 923.390
[31,     1] loss: 878.054
[32,     1] loss: 865.713
[33,     1] loss: 924.923
[34,     1] loss: 918.705
[35,     1] loss: 811.561
[36,     1] loss: 804.299
[37,     1] loss: 832.748
[38,     1] loss: 846.330
[39,     1] loss: 810.949
[40,     1] loss: 824.337
[41,     1] loss: 904.092
[42,     1] loss: 859.959
Early stopping applied (best metric=0.8990334272384644)
Finished Training
Total time taken: 5.675004482269287
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.647
[2,     1] loss: 1236.760
[3,     1] loss: 1235.341
[4,     1] loss: 1235.871
[5,     1] loss: 1234.235
[6,     1] loss: 1234.623
[7,     1] loss: 1230.656
[8,     1] loss: 1234.302
[9,     1] loss: 1230.283
[10,     1] loss: 1233.190
[11,     1] loss: 1232.007
[12,     1] loss: 1232.069
[13,     1] loss: 1232.899
[14,     1] loss: 1230.326
[15,     1] loss: 1221.682
[16,     1] loss: 1218.709
[17,     1] loss: 1200.146
[18,     1] loss: 1165.861
[19,     1] loss: 1123.003
[20,     1] loss: 1114.469
[21,     1] loss: 1084.158
[22,     1] loss: 1025.653
[23,     1] loss: 1115.729
[24,     1] loss: 1019.837
[25,     1] loss: 1036.755
[26,     1] loss: 982.489
[27,     1] loss: 984.791
[28,     1] loss: 983.983
[29,     1] loss: 970.898
[30,     1] loss: 984.364
[31,     1] loss: 940.533
[32,     1] loss: 936.823
[33,     1] loss: 904.669
[34,     1] loss: 911.660
[35,     1] loss: 943.765
[36,     1] loss: 896.004
[37,     1] loss: 891.692
[38,     1] loss: 851.702
[39,     1] loss: 867.637
[40,     1] loss: 866.999
[41,     1] loss: 819.053
[42,     1] loss: 878.632
[43,     1] loss: 816.796
[44,     1] loss: 1035.203
[45,     1] loss: 922.158
[46,     1] loss: 852.542
[47,     1] loss: 898.613
[48,     1] loss: 891.441
[49,     1] loss: 830.306
[50,     1] loss: 858.225
[51,     1] loss: 839.090
[52,     1] loss: 802.405
[53,     1] loss: 820.129
[54,     1] loss: 778.343
[55,     1] loss: 713.668
[56,     1] loss: 720.065
[57,     1] loss: 749.217
[58,     1] loss: 825.517
Early stopping applied (best metric=0.917884349822998)
Finished Training
Total time taken: 9.618010759353638
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.335
[2,     1] loss: 1242.509
[3,     1] loss: 1239.127
[4,     1] loss: 1238.529
[5,     1] loss: 1234.249
[6,     1] loss: 1233.301
[7,     1] loss: 1231.152
[8,     1] loss: 1237.210
[9,     1] loss: 1232.296
[10,     1] loss: 1234.240
[11,     1] loss: 1230.517
[12,     1] loss: 1229.043
[13,     1] loss: 1227.271
[14,     1] loss: 1223.172
[15,     1] loss: 1216.729
[16,     1] loss: 1200.963
[17,     1] loss: 1174.654
[18,     1] loss: 1140.843
[19,     1] loss: 1116.274
[20,     1] loss: 1094.924
[21,     1] loss: 1047.493
[22,     1] loss: 1016.205
[23,     1] loss: 954.037
[24,     1] loss: 986.191
[25,     1] loss: 1076.448
[26,     1] loss: 1009.667
[27,     1] loss: 1014.192
[28,     1] loss: 974.945
[29,     1] loss: 958.806
[30,     1] loss: 992.307
[31,     1] loss: 948.458
[32,     1] loss: 968.283
[33,     1] loss: 923.784
[34,     1] loss: 924.951
[35,     1] loss: 912.252
[36,     1] loss: 907.881
[37,     1] loss: 858.273
[38,     1] loss: 864.872
[39,     1] loss: 871.380
[40,     1] loss: 863.524
[41,     1] loss: 848.795
[42,     1] loss: 835.500
[43,     1] loss: 796.794
[44,     1] loss: 766.421
[45,     1] loss: 761.372
[46,     1] loss: 911.018
[47,     1] loss: 1526.268
[48,     1] loss: 852.131
[49,     1] loss: 1016.022
[50,     1] loss: 918.301
[51,     1] loss: 896.956
[52,     1] loss: 989.144
[53,     1] loss: 1002.147
[54,     1] loss: 978.676
[55,     1] loss: 901.871
[56,     1] loss: 939.149
[57,     1] loss: 986.481
[58,     1] loss: 882.814
[59,     1] loss: 863.197
[60,     1] loss: 882.514
[61,     1] loss: 824.761
[62,     1] loss: 862.339
[63,     1] loss: 831.010
Early stopping applied (best metric=0.8166114091873169)
Finished Training
Total time taken: 10.430010557174683
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.076
[2,     1] loss: 1251.531
[3,     1] loss: 1238.421
[4,     1] loss: 1236.858
[5,     1] loss: 1235.634
[6,     1] loss: 1239.394
[7,     1] loss: 1236.033
[8,     1] loss: 1234.907
[9,     1] loss: 1235.674
[10,     1] loss: 1235.574
[11,     1] loss: 1234.172
[12,     1] loss: 1234.665
[13,     1] loss: 1236.133
[14,     1] loss: 1234.014
[15,     1] loss: 1234.244
[16,     1] loss: 1233.269
[17,     1] loss: 1235.669
[18,     1] loss: 1234.748
[19,     1] loss: 1234.687
[20,     1] loss: 1233.729
[21,     1] loss: 1234.260
[22,     1] loss: 1232.224
[23,     1] loss: 1230.612
[24,     1] loss: 1223.824
[25,     1] loss: 1211.165
[26,     1] loss: 1196.089
[27,     1] loss: 1165.561
[28,     1] loss: 1128.508
[29,     1] loss: 1100.664
[30,     1] loss: 1073.402
[31,     1] loss: 1079.108
[32,     1] loss: 1059.219
[33,     1] loss: 1098.385
[34,     1] loss: 977.519
[35,     1] loss: 1101.729
[36,     1] loss: 1009.917
[37,     1] loss: 978.042
[38,     1] loss: 1038.275
[39,     1] loss: 1010.283
[40,     1] loss: 1011.852
[41,     1] loss: 1054.298
[42,     1] loss: 955.079
[43,     1] loss: 976.000
[44,     1] loss: 986.950
[45,     1] loss: 926.498
[46,     1] loss: 951.510
[47,     1] loss: 882.212
[48,     1] loss: 922.922
[49,     1] loss: 930.890
[50,     1] loss: 891.293
[51,     1] loss: 944.987
[52,     1] loss: 846.857
[53,     1] loss: 925.941
[54,     1] loss: 821.144
[55,     1] loss: 843.976
[56,     1] loss: 856.403
[57,     1] loss: 849.108
[58,     1] loss: 904.567
[59,     1] loss: 828.096
[60,     1] loss: 825.957
[61,     1] loss: 892.144
[62,     1] loss: 804.286
[63,     1] loss: 825.630
[64,     1] loss: 827.259
[65,     1] loss: 789.451
[66,     1] loss: 726.456
[67,     1] loss: 832.639
[68,     1] loss: 928.222
[69,     1] loss: 863.887
[70,     1] loss: 747.868
[71,     1] loss: 825.925
[72,     1] loss: 744.459
[73,     1] loss: 751.395
[74,     1] loss: 674.016
[75,     1] loss: 801.195
[76,     1] loss: 710.067
Early stopping applied (best metric=0.7624362111091614)
Finished Training
Total time taken: 10.203009605407715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.786
[2,     1] loss: 1256.280
[3,     1] loss: 1235.745
[4,     1] loss: 1234.328
[5,     1] loss: 1239.454
[6,     1] loss: 1232.457
[7,     1] loss: 1235.104
[8,     1] loss: 1234.244
[9,     1] loss: 1232.088
[10,     1] loss: 1233.177
[11,     1] loss: 1232.452
[12,     1] loss: 1233.934
[13,     1] loss: 1235.936
[14,     1] loss: 1230.953
[15,     1] loss: 1233.321
[16,     1] loss: 1233.120
[17,     1] loss: 1232.028
[18,     1] loss: 1231.576
[19,     1] loss: 1231.768
[20,     1] loss: 1225.442
[21,     1] loss: 1217.807
[22,     1] loss: 1208.774
[23,     1] loss: 1190.709
[24,     1] loss: 1159.666
[25,     1] loss: 1134.817
[26,     1] loss: 1099.679
[27,     1] loss: 1052.772
[28,     1] loss: 1058.865
[29,     1] loss: 1029.809
[30,     1] loss: 997.621
[31,     1] loss: 1078.235
[32,     1] loss: 1166.459
[33,     1] loss: 968.149
[34,     1] loss: 1003.420
[35,     1] loss: 990.196
[36,     1] loss: 998.490
[37,     1] loss: 1004.043
[38,     1] loss: 977.915
[39,     1] loss: 947.435
[40,     1] loss: 1002.428
[41,     1] loss: 934.258
[42,     1] loss: 926.297
[43,     1] loss: 931.346
[44,     1] loss: 918.290
[45,     1] loss: 906.582
[46,     1] loss: 870.603
[47,     1] loss: 825.257
[48,     1] loss: 889.995
[49,     1] loss: 865.563
[50,     1] loss: 798.501
[51,     1] loss: 868.772
[52,     1] loss: 875.825
[53,     1] loss: 830.690
[54,     1] loss: 850.149
[55,     1] loss: 964.318
[56,     1] loss: 1040.051
[57,     1] loss: 802.833
[58,     1] loss: 963.593
[59,     1] loss: 844.441
[60,     1] loss: 905.190
[61,     1] loss: 824.941
[62,     1] loss: 805.247
[63,     1] loss: 829.402
Early stopping applied (best metric=0.9074936509132385)
Finished Training
Total time taken: 10.525010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.236
[2,     1] loss: 1250.996
[3,     1] loss: 1236.774
[4,     1] loss: 1237.443
[5,     1] loss: 1233.553
[6,     1] loss: 1231.892
[7,     1] loss: 1236.067
[8,     1] loss: 1234.749
[9,     1] loss: 1234.840
[10,     1] loss: 1232.857
[11,     1] loss: 1234.788
[12,     1] loss: 1233.365
[13,     1] loss: 1232.430
[14,     1] loss: 1234.003
[15,     1] loss: 1234.784
[16,     1] loss: 1233.535
[17,     1] loss: 1231.701
[18,     1] loss: 1234.972
[19,     1] loss: 1233.741
[20,     1] loss: 1233.164
[21,     1] loss: 1233.073
[22,     1] loss: 1232.666
[23,     1] loss: 1231.541
[24,     1] loss: 1229.949
[25,     1] loss: 1226.732
[26,     1] loss: 1222.624
[27,     1] loss: 1210.079
[28,     1] loss: 1194.076
[29,     1] loss: 1167.703
[30,     1] loss: 1146.246
[31,     1] loss: 1116.047
[32,     1] loss: 1096.486
[33,     1] loss: 1050.514
[34,     1] loss: 1049.636
[35,     1] loss: 1064.708
[36,     1] loss: 1030.402
[37,     1] loss: 995.959
[38,     1] loss: 1057.990
[39,     1] loss: 988.760
[40,     1] loss: 974.390
[41,     1] loss: 931.986
[42,     1] loss: 906.730
[43,     1] loss: 923.557
[44,     1] loss: 899.850
[45,     1] loss: 924.499
[46,     1] loss: 915.536
[47,     1] loss: 1065.036
[48,     1] loss: 886.314
[49,     1] loss: 978.020
[50,     1] loss: 874.158
[51,     1] loss: 947.472
[52,     1] loss: 927.829
[53,     1] loss: 884.015
[54,     1] loss: 925.253
[55,     1] loss: 826.269
[56,     1] loss: 896.149
[57,     1] loss: 805.752
[58,     1] loss: 877.711
[59,     1] loss: 883.988
[60,     1] loss: 785.165
[61,     1] loss: 891.510
[62,     1] loss: 1068.007
[63,     1] loss: 804.051
[64,     1] loss: 954.009
[65,     1] loss: 839.547
[66,     1] loss: 928.399
[67,     1] loss: 852.901
[68,     1] loss: 852.871
[69,     1] loss: 873.965
[70,     1] loss: 835.805
[71,     1] loss: 835.086
[72,     1] loss: 808.855
[73,     1] loss: 800.580
[74,     1] loss: 800.494
[75,     1] loss: 705.165
[76,     1] loss: 882.837
[77,     1] loss: 898.123
[78,     1] loss: 737.712
[79,     1] loss: 913.473
[80,     1] loss: 730.006
[81,     1] loss: 866.049
[82,     1] loss: 757.108
[83,     1] loss: 872.695
[84,     1] loss: 708.051
[85,     1] loss: 809.794
[86,     1] loss: 671.923
[87,     1] loss: 821.265
[88,     1] loss: 926.323
[89,     1] loss: 671.020
[90,     1] loss: 778.343
[91,     1] loss: 712.652
[92,     1] loss: 702.021
[93,     1] loss: 755.941
[94,     1] loss: 684.351
[95,     1] loss: 641.738
[96,     1] loss: 759.975
Early stopping applied (best metric=0.8530992269515991)
Finished Training
Total time taken: 16.14801812171936
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.058
[2,     1] loss: 1244.641
[3,     1] loss: 1233.227
[4,     1] loss: 1237.007
[5,     1] loss: 1235.783
[6,     1] loss: 1233.321
[7,     1] loss: 1235.078
[8,     1] loss: 1233.679
[9,     1] loss: 1234.039
[10,     1] loss: 1231.775
[11,     1] loss: 1233.592
[12,     1] loss: 1231.009
[13,     1] loss: 1231.699
[14,     1] loss: 1231.317
[15,     1] loss: 1231.534
[16,     1] loss: 1230.976
[17,     1] loss: 1227.325
[18,     1] loss: 1225.114
[19,     1] loss: 1218.805
[20,     1] loss: 1206.673
[21,     1] loss: 1188.849
[22,     1] loss: 1171.388
[23,     1] loss: 1139.428
[24,     1] loss: 1117.603
[25,     1] loss: 1097.906
[26,     1] loss: 1072.967
[27,     1] loss: 1054.302
[28,     1] loss: 1058.199
[29,     1] loss: 1036.853
[30,     1] loss: 999.343
[31,     1] loss: 966.578
[32,     1] loss: 1001.234
[33,     1] loss: 990.670
[34,     1] loss: 971.889
[35,     1] loss: 983.995
[36,     1] loss: 985.971
[37,     1] loss: 942.653
[38,     1] loss: 939.387
[39,     1] loss: 956.648
[40,     1] loss: 884.908
[41,     1] loss: 953.622
[42,     1] loss: 1004.455
[43,     1] loss: 907.320
[44,     1] loss: 1008.070
[45,     1] loss: 892.723
[46,     1] loss: 1001.731
[47,     1] loss: 901.330
[48,     1] loss: 896.021
[49,     1] loss: 867.628
[50,     1] loss: 864.038
[51,     1] loss: 868.222
[52,     1] loss: 857.605
[53,     1] loss: 780.752
[54,     1] loss: 820.644
[55,     1] loss: 761.448
[56,     1] loss: 769.540
[57,     1] loss: 822.986
[58,     1] loss: 787.692
[59,     1] loss: 894.291
[60,     1] loss: 1113.033
[61,     1] loss: 833.798
[62,     1] loss: 888.378
[63,     1] loss: 844.271
[64,     1] loss: 884.244
[65,     1] loss: 863.355
[66,     1] loss: 801.345
[67,     1] loss: 839.856
[68,     1] loss: 792.328
[69,     1] loss: 786.532
[70,     1] loss: 772.161
[71,     1] loss: 735.077
[72,     1] loss: 704.656
[73,     1] loss: 692.403
[74,     1] loss: 635.868
[75,     1] loss: 657.792
[76,     1] loss: 645.340
Early stopping applied (best metric=0.8137342929840088)
Finished Training
Total time taken: 10.225009202957153
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.938
[2,     1] loss: 1240.042
[3,     1] loss: 1234.763
[4,     1] loss: 1238.123
[5,     1] loss: 1233.655
[6,     1] loss: 1231.647
[7,     1] loss: 1235.345
[8,     1] loss: 1229.470
[9,     1] loss: 1232.691
[10,     1] loss: 1234.237
[11,     1] loss: 1232.913
[12,     1] loss: 1235.487
[13,     1] loss: 1232.424
[14,     1] loss: 1230.167
[15,     1] loss: 1228.284
[16,     1] loss: 1230.146
[17,     1] loss: 1225.544
[18,     1] loss: 1215.815
[19,     1] loss: 1206.121
[20,     1] loss: 1187.604
[21,     1] loss: 1148.316
[22,     1] loss: 1125.024
[23,     1] loss: 1143.235
[24,     1] loss: 1092.556
[25,     1] loss: 1098.378
[26,     1] loss: 1047.566
[27,     1] loss: 1049.902
[28,     1] loss: 1006.225
[29,     1] loss: 1062.712
[30,     1] loss: 1002.674
[31,     1] loss: 1013.406
[32,     1] loss: 999.942
[33,     1] loss: 962.484
[34,     1] loss: 978.726
[35,     1] loss: 929.880
[36,     1] loss: 965.812
[37,     1] loss: 939.715
[38,     1] loss: 952.543
[39,     1] loss: 949.703
[40,     1] loss: 910.480
[41,     1] loss: 889.180
[42,     1] loss: 943.142
[43,     1] loss: 940.998
[44,     1] loss: 878.466
[45,     1] loss: 881.479
[46,     1] loss: 910.507
[47,     1] loss: 857.923
[48,     1] loss: 852.615
[49,     1] loss: 897.727
[50,     1] loss: 842.039
[51,     1] loss: 869.817
[52,     1] loss: 832.484
[53,     1] loss: 780.848
[54,     1] loss: 830.229
[55,     1] loss: 769.322
[56,     1] loss: 1035.828
[57,     1] loss: 1790.406
[58,     1] loss: 1630.604
[59,     1] loss: 1322.839
[60,     1] loss: 1227.826
[61,     1] loss: 1217.890
[62,     1] loss: 1229.675
[63,     1] loss: 1234.791
[64,     1] loss: 1233.009
[65,     1] loss: 1233.817
[66,     1] loss: 1232.401
[67,     1] loss: 1232.625
[68,     1] loss: 1234.301
[69,     1] loss: 1229.207
[70,     1] loss: 1227.763
[71,     1] loss: 1226.767
[72,     1] loss: 1223.671
[73,     1] loss: 1221.644
[74,     1] loss: 1207.467
[75,     1] loss: 1197.781
Early stopping applied (best metric=0.8388161063194275)
Finished Training
Total time taken: 10.925010681152344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.765
[2,     1] loss: 1234.588
[3,     1] loss: 1237.286
[4,     1] loss: 1237.705
[5,     1] loss: 1231.162
[6,     1] loss: 1230.923
[7,     1] loss: 1226.698
[8,     1] loss: 1220.943
[9,     1] loss: 1203.858
[10,     1] loss: 1174.065
[11,     1] loss: 1128.904
[12,     1] loss: 1054.785
[13,     1] loss: 1107.552
[14,     1] loss: 1165.913
[15,     1] loss: 1033.822
[16,     1] loss: 1048.808
[17,     1] loss: 1007.106
[18,     1] loss: 1034.573
[19,     1] loss: 1027.919
[20,     1] loss: 1002.188
[21,     1] loss: 1002.231
[22,     1] loss: 1037.691
[23,     1] loss: 982.499
[24,     1] loss: 970.806
[25,     1] loss: 957.953
[26,     1] loss: 966.137
[27,     1] loss: 977.487
[28,     1] loss: 971.348
[29,     1] loss: 929.205
[30,     1] loss: 1023.419
[31,     1] loss: 903.097
[32,     1] loss: 920.568
[33,     1] loss: 845.877
[34,     1] loss: 939.825
[35,     1] loss: 848.998
[36,     1] loss: 869.304
[37,     1] loss: 812.883
[38,     1] loss: 849.599
[39,     1] loss: 816.814
[40,     1] loss: 816.493
[41,     1] loss: 742.895
[42,     1] loss: 820.850
[43,     1] loss: 842.541
[44,     1] loss: 1379.585
[45,     1] loss: 881.486
[46,     1] loss: 994.646
[47,     1] loss: 954.563
[48,     1] loss: 967.410
[49,     1] loss: 1014.716
[50,     1] loss: 974.135
[51,     1] loss: 951.883
[52,     1] loss: 874.081
[53,     1] loss: 903.009
[54,     1] loss: 856.245
[55,     1] loss: 867.538
[56,     1] loss: 846.691
[57,     1] loss: 775.805
[58,     1] loss: 762.692
[59,     1] loss: 810.815
[60,     1] loss: 781.784
[61,     1] loss: 768.737
[62,     1] loss: 745.698
[63,     1] loss: 752.391
[64,     1] loss: 729.885
[65,     1] loss: 775.475
[66,     1] loss: 942.249
[67,     1] loss: 1142.118
[68,     1] loss: 731.025
[69,     1] loss: 955.666
[70,     1] loss: 833.367
[71,     1] loss: 837.748
[72,     1] loss: 881.454
[73,     1] loss: 809.906
[74,     1] loss: 806.549
[75,     1] loss: 829.539
[76,     1] loss: 725.637
[77,     1] loss: 797.370
[78,     1] loss: 737.252
[79,     1] loss: 722.537
[80,     1] loss: 708.749
[81,     1] loss: 680.008
[82,     1] loss: 694.882
[83,     1] loss: 785.254
[84,     1] loss: 1008.079
[85,     1] loss: 717.243
[86,     1] loss: 745.500
[87,     1] loss: 761.064
[88,     1] loss: 686.392
[89,     1] loss: 738.888
[90,     1] loss: 641.593
[91,     1] loss: 654.228
[92,     1] loss: 600.939
[93,     1] loss: 604.945
[94,     1] loss: 616.127
[95,     1] loss: 646.285
[96,     1] loss: 1567.866
[97,     1] loss: 1438.433
Early stopping applied (best metric=0.8137625455856323)
Finished Training
Total time taken: 16.041016101837158
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.120
[2,     1] loss: 1238.262
[3,     1] loss: 1238.363
[4,     1] loss: 1232.398
[5,     1] loss: 1234.480
[6,     1] loss: 1233.752
[7,     1] loss: 1231.976
[8,     1] loss: 1231.196
[9,     1] loss: 1227.876
[10,     1] loss: 1220.987
[11,     1] loss: 1216.514
[12,     1] loss: 1194.151
[13,     1] loss: 1174.683
[14,     1] loss: 1140.707
[15,     1] loss: 1079.101
[16,     1] loss: 1075.352
[17,     1] loss: 1079.799
[18,     1] loss: 993.747
[19,     1] loss: 1120.525
[20,     1] loss: 1022.316
[21,     1] loss: 1107.070
[22,     1] loss: 991.113
[23,     1] loss: 1010.316
[24,     1] loss: 1019.483
[25,     1] loss: 1006.086
[26,     1] loss: 968.809
[27,     1] loss: 971.417
[28,     1] loss: 976.287
[29,     1] loss: 961.377
[30,     1] loss: 934.895
[31,     1] loss: 956.118
[32,     1] loss: 911.541
[33,     1] loss: 915.646
[34,     1] loss: 947.492
[35,     1] loss: 891.081
[36,     1] loss: 951.898
[37,     1] loss: 917.213
[38,     1] loss: 911.950
[39,     1] loss: 883.842
[40,     1] loss: 906.103
[41,     1] loss: 866.044
[42,     1] loss: 842.351
[43,     1] loss: 809.132
[44,     1] loss: 815.544
[45,     1] loss: 785.981
[46,     1] loss: 765.710
[47,     1] loss: 941.561
[48,     1] loss: 1089.421
[49,     1] loss: 803.453
[50,     1] loss: 872.618
[51,     1] loss: 909.070
[52,     1] loss: 844.451
[53,     1] loss: 870.672
[54,     1] loss: 887.573
[55,     1] loss: 839.082
[56,     1] loss: 794.455
[57,     1] loss: 827.731
[58,     1] loss: 798.006
[59,     1] loss: 817.591
[60,     1] loss: 779.129
[61,     1] loss: 820.413
[62,     1] loss: 893.269
[63,     1] loss: 756.397
[64,     1] loss: 824.790
[65,     1] loss: 703.015
[66,     1] loss: 747.388
[67,     1] loss: 666.981
[68,     1] loss: 720.460
[69,     1] loss: 668.340
[70,     1] loss: 661.526
[71,     1] loss: 635.104
[72,     1] loss: 661.501
[73,     1] loss: 825.707
[74,     1] loss: 1617.892
[75,     1] loss: 689.457
[76,     1] loss: 1045.307
[77,     1] loss: 823.384
[78,     1] loss: 933.160
[79,     1] loss: 1036.535
[80,     1] loss: 927.217
[81,     1] loss: 890.612
[82,     1] loss: 934.278
[83,     1] loss: 866.138
[84,     1] loss: 830.071
Early stopping applied (best metric=0.8523207902908325)
Finished Training
Total time taken: 13.676013946533203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.659
[2,     1] loss: 1235.455
[3,     1] loss: 1234.164
[4,     1] loss: 1236.389
[5,     1] loss: 1236.549
[6,     1] loss: 1231.200
[7,     1] loss: 1230.829
[8,     1] loss: 1235.550
[9,     1] loss: 1229.967
[10,     1] loss: 1231.363
[11,     1] loss: 1225.643
[12,     1] loss: 1223.277
[13,     1] loss: 1214.360
[14,     1] loss: 1197.109
[15,     1] loss: 1170.187
[16,     1] loss: 1115.517
[17,     1] loss: 1100.581
[18,     1] loss: 1075.144
[19,     1] loss: 1088.091
[20,     1] loss: 1006.500
[21,     1] loss: 1042.388
[22,     1] loss: 988.980
[23,     1] loss: 1011.770
[24,     1] loss: 986.432
[25,     1] loss: 1046.531
[26,     1] loss: 1035.629
[27,     1] loss: 956.232
[28,     1] loss: 946.149
[29,     1] loss: 908.687
[30,     1] loss: 961.671
[31,     1] loss: 930.145
[32,     1] loss: 940.505
[33,     1] loss: 911.492
[34,     1] loss: 890.493
[35,     1] loss: 880.999
[36,     1] loss: 843.147
[37,     1] loss: 896.314
[38,     1] loss: 1071.925
[39,     1] loss: 937.665
[40,     1] loss: 825.345
[41,     1] loss: 917.252
[42,     1] loss: 906.639
[43,     1] loss: 897.318
[44,     1] loss: 850.915
[45,     1] loss: 867.482
[46,     1] loss: 851.505
[47,     1] loss: 824.757
[48,     1] loss: 807.307
[49,     1] loss: 788.291
[50,     1] loss: 788.846
[51,     1] loss: 1171.264
[52,     1] loss: 937.360
[53,     1] loss: 898.875
[54,     1] loss: 886.194
[55,     1] loss: 929.477
[56,     1] loss: 867.573
[57,     1] loss: 841.923
[58,     1] loss: 856.245
[59,     1] loss: 779.992
[60,     1] loss: 843.471
[61,     1] loss: 758.687
[62,     1] loss: 828.935
[63,     1] loss: 711.942
[64,     1] loss: 703.130
[65,     1] loss: 730.054
[66,     1] loss: 704.133
[67,     1] loss: 672.260
[68,     1] loss: 652.713
[69,     1] loss: 631.845
[70,     1] loss: 805.490
[71,     1] loss: 1766.876
[72,     1] loss: 691.181
[73,     1] loss: 1215.393
[74,     1] loss: 828.332
[75,     1] loss: 965.159
[76,     1] loss: 1031.995
[77,     1] loss: 1007.321
[78,     1] loss: 928.062
[79,     1] loss: 977.758
[80,     1] loss: 976.118
[81,     1] loss: 875.619
[82,     1] loss: 913.292
[83,     1] loss: 927.612
[84,     1] loss: 864.759
[85,     1] loss: 902.206
[86,     1] loss: 835.146
[87,     1] loss: 809.131
[88,     1] loss: 769.948
Early stopping applied (best metric=0.6920018196105957)
Finished Training
Total time taken: 13.2560133934021
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.384
[2,     1] loss: 1236.805
[3,     1] loss: 1230.461
[4,     1] loss: 1239.775
[5,     1] loss: 1235.176
[6,     1] loss: 1236.794
[7,     1] loss: 1232.704
[8,     1] loss: 1235.294
[9,     1] loss: 1232.422
[10,     1] loss: 1234.605
[11,     1] loss: 1232.711
[12,     1] loss: 1231.118
[13,     1] loss: 1229.111
[14,     1] loss: 1226.641
[15,     1] loss: 1223.248
[16,     1] loss: 1214.942
[17,     1] loss: 1194.797
[18,     1] loss: 1154.726
[19,     1] loss: 1106.095
[20,     1] loss: 1093.114
[21,     1] loss: 1072.305
[22,     1] loss: 1076.585
[23,     1] loss: 1061.369
[24,     1] loss: 1027.399
[25,     1] loss: 1023.276
[26,     1] loss: 1085.897
[27,     1] loss: 987.656
[28,     1] loss: 1019.859
[29,     1] loss: 959.585
[30,     1] loss: 1001.718
[31,     1] loss: 958.820
[32,     1] loss: 961.690
[33,     1] loss: 932.233
[34,     1] loss: 978.147
[35,     1] loss: 931.955
[36,     1] loss: 945.253
[37,     1] loss: 896.379
[38,     1] loss: 964.738
[39,     1] loss: 922.803
[40,     1] loss: 934.639
[41,     1] loss: 883.281
[42,     1] loss: 838.788
[43,     1] loss: 892.564
[44,     1] loss: 840.981
[45,     1] loss: 838.404
[46,     1] loss: 847.469
[47,     1] loss: 922.657
[48,     1] loss: 831.029
[49,     1] loss: 897.698
[50,     1] loss: 819.438
[51,     1] loss: 884.534
[52,     1] loss: 776.178
[53,     1] loss: 847.685
[54,     1] loss: 777.532
[55,     1] loss: 816.301
[56,     1] loss: 809.328
[57,     1] loss: 719.068
[58,     1] loss: 744.380
[59,     1] loss: 679.415
[60,     1] loss: 672.758
[61,     1] loss: 608.492
[62,     1] loss: 638.387
[63,     1] loss: 660.973
[64,     1] loss: 720.374
[65,     1] loss: 2554.545
[66,     1] loss: 1537.330
[67,     1] loss: 1045.496
[68,     1] loss: 1066.658
[69,     1] loss: 1193.974
[70,     1] loss: 1081.953
[71,     1] loss: 1115.592
[72,     1] loss: 1102.251
[73,     1] loss: 1046.243
[74,     1] loss: 1099.166
[75,     1] loss: 1045.336
[76,     1] loss: 1048.931
[77,     1] loss: 1080.612
Early stopping applied (best metric=0.8441582322120667)
Finished Training
Total time taken: 11.966010808944702
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.188
[2,     1] loss: 1236.769
[3,     1] loss: 1236.512
[4,     1] loss: 1239.666
[5,     1] loss: 1235.535
[6,     1] loss: 1232.797
[7,     1] loss: 1234.099
[8,     1] loss: 1234.803
[9,     1] loss: 1232.995
[10,     1] loss: 1233.325
[11,     1] loss: 1232.143
[12,     1] loss: 1234.380
[13,     1] loss: 1233.486
[14,     1] loss: 1232.582
[15,     1] loss: 1233.943
[16,     1] loss: 1233.996
[17,     1] loss: 1232.240
[18,     1] loss: 1233.009
[19,     1] loss: 1230.897
[20,     1] loss: 1230.515
[21,     1] loss: 1226.549
[22,     1] loss: 1224.369
[23,     1] loss: 1208.703
[24,     1] loss: 1188.662
[25,     1] loss: 1148.375
[26,     1] loss: 1130.089
[27,     1] loss: 1079.673
[28,     1] loss: 1007.513
[29,     1] loss: 1067.203
[30,     1] loss: 1005.678
[31,     1] loss: 1054.371
[32,     1] loss: 1039.066
[33,     1] loss: 1024.395
[34,     1] loss: 1006.307
[35,     1] loss: 1002.219
[36,     1] loss: 964.419
[37,     1] loss: 988.108
[38,     1] loss: 961.731
[39,     1] loss: 931.656
[40,     1] loss: 953.765
[41,     1] loss: 934.145
[42,     1] loss: 991.671
[43,     1] loss: 930.741
[44,     1] loss: 913.818
[45,     1] loss: 899.695
[46,     1] loss: 903.896
[47,     1] loss: 904.272
[48,     1] loss: 928.833
[49,     1] loss: 919.221
[50,     1] loss: 862.853
[51,     1] loss: 877.835
[52,     1] loss: 841.054
[53,     1] loss: 812.794
[54,     1] loss: 904.775
[55,     1] loss: 976.333
[56,     1] loss: 787.946
[57,     1] loss: 865.478
[58,     1] loss: 805.687
[59,     1] loss: 831.783
[60,     1] loss: 796.960
[61,     1] loss: 823.690
[62,     1] loss: 723.649
[63,     1] loss: 796.133
[64,     1] loss: 758.543
[65,     1] loss: 688.171
[66,     1] loss: 695.084
[67,     1] loss: 677.930
[68,     1] loss: 724.333
[69,     1] loss: 745.330
[70,     1] loss: 1356.207
[71,     1] loss: 741.021
[72,     1] loss: 1050.281
[73,     1] loss: 841.836
[74,     1] loss: 913.078
[75,     1] loss: 920.027
[76,     1] loss: 868.918
[77,     1] loss: 812.059
[78,     1] loss: 910.241
[79,     1] loss: 790.525
[80,     1] loss: 849.768
[81,     1] loss: 783.660
[82,     1] loss: 769.771
[83,     1] loss: 763.438
[84,     1] loss: 722.464
[85,     1] loss: 745.246
[86,     1] loss: 755.455
[87,     1] loss: 686.401
Early stopping applied (best metric=0.8074158430099487)
Finished Training
Total time taken: 11.740010976791382
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.215
[2,     1] loss: 1238.944
[3,     1] loss: 1239.479
[4,     1] loss: 1236.021
[5,     1] loss: 1236.811
[6,     1] loss: 1235.685
[7,     1] loss: 1235.323
[8,     1] loss: 1233.476
[9,     1] loss: 1230.404
[10,     1] loss: 1231.338
[11,     1] loss: 1227.055
[12,     1] loss: 1219.984
[13,     1] loss: 1199.225
[14,     1] loss: 1176.020
[15,     1] loss: 1135.893
[16,     1] loss: 1134.294
[17,     1] loss: 1103.262
[18,     1] loss: 1012.501
[19,     1] loss: 1097.570
[20,     1] loss: 1084.702
[21,     1] loss: 1016.879
[22,     1] loss: 1051.847
[23,     1] loss: 1027.657
[24,     1] loss: 1007.030
[25,     1] loss: 1012.311
[26,     1] loss: 1011.670
[27,     1] loss: 968.989
[28,     1] loss: 971.664
[29,     1] loss: 959.723
[30,     1] loss: 994.118
[31,     1] loss: 939.422
[32,     1] loss: 946.902
[33,     1] loss: 914.013
[34,     1] loss: 953.679
[35,     1] loss: 927.953
[36,     1] loss: 905.626
[37,     1] loss: 890.154
[38,     1] loss: 877.007
[39,     1] loss: 875.633
[40,     1] loss: 840.178
[41,     1] loss: 805.310
[42,     1] loss: 917.000
[43,     1] loss: 917.317
[44,     1] loss: 792.587
[45,     1] loss: 856.046
[46,     1] loss: 797.219
[47,     1] loss: 771.050
[48,     1] loss: 757.443
[49,     1] loss: 720.862
[50,     1] loss: 828.672
[51,     1] loss: 1153.483
[52,     1] loss: 734.506
[53,     1] loss: 865.927
[54,     1] loss: 772.114
[55,     1] loss: 842.980
[56,     1] loss: 763.555
[57,     1] loss: 786.111
[58,     1] loss: 755.093
[59,     1] loss: 773.817
[60,     1] loss: 706.694
Early stopping applied (best metric=0.7725388407707214)
Finished Training
Total time taken: 8.102007150650024
{'Hydroxylation-K Validation Accuracy': 0.7607269503546099, 'Hydroxylation-K Validation Sensitivity': 0.682962962962963, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.44673006098063306, 'Hydroxylation-K AUC ROC': 0.7939571150097466, 'Hydroxylation-K AUC PR': 0.5717311015508336, 'Hydroxylation-K MCC': 0.4054602155871554, 'Hydroxylation-K F1': 0.5331422145106356, 'Validation Loss (Hydroxylation-K)': 0.4482811431090037, 'Hydroxylation-P Validation Accuracy': 0.7889446051807861, 'Hydroxylation-P Validation Sensitivity': 0.7995767195767196, 'Hydroxylation-P Validation Specificity': 0.7867848770512245, 'Hydroxylation-P Validation Precision': 0.4579474899674505, 'Hydroxylation-P AUC ROC': 0.8462880367702303, 'Hydroxylation-P AUC PR': 0.5668124515141776, 'Hydroxylation-P MCC': 0.48777236701998694, 'Hydroxylation-P F1': 0.5778049264409808, 'Validation Loss (Hydroxylation-P)': 0.37793924609820045, 'Validation Loss (total)': 0.8262203852335612, 'TimeToTrain': 11.287477779388428}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00551791488226366,
 'learning_rate_Hydroxylation-K': 0.004717143899563011,
 'learning_rate_Hydroxylation-P': 0.006505527386848689,
 'log_base': 1.9923900945930275,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4134641822,
 'sample_weights': [1.5460959713841609, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.465407998724267,
 'weight_decay_Hydroxylation-K': 9.028474531760956,
 'weight_decay_Hydroxylation-P': 1.0889349701734277}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.398
[2,     1] loss: 1416.291
[3,     1] loss: 1420.178
[4,     1] loss: 1416.124
[5,     1] loss: 1418.669
[6,     1] loss: 1413.553
[7,     1] loss: 1410.661
[8,     1] loss: 1398.062
[9,     1] loss: 1386.772
[10,     1] loss: 1345.836
[11,     1] loss: 1310.604
[12,     1] loss: 1276.533
[13,     1] loss: 1244.092
[14,     1] loss: 1283.222
[15,     1] loss: 1245.837
[16,     1] loss: 1190.860
[17,     1] loss: 1207.246
[18,     1] loss: 1135.073
[19,     1] loss: 1174.842
[20,     1] loss: 1152.457
[21,     1] loss: 1125.121
[22,     1] loss: 1093.838
[23,     1] loss: 1117.501
[24,     1] loss: 1093.229
[25,     1] loss: 1103.849
[26,     1] loss: 1082.393
[27,     1] loss: 1111.694
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033166550652316833,
 'learning_rate_Hydroxylation-K': 0.000772417010058901,
 'learning_rate_Hydroxylation-P': 0.008809475552532734,
 'log_base': 2.4942050926291004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2366532661,
 'sample_weights': [2.421816995514653, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.2761201926788237,
 'weight_decay_Hydroxylation-K': 9.16968630185858,
 'weight_decay_Hydroxylation-P': 1.2192825946384824}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.802
[2,     1] loss: 1292.367
[3,     1] loss: 1292.432
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009143734241274163,
 'learning_rate_Hydroxylation-K': 0.0019546732316464218,
 'learning_rate_Hydroxylation-P': 0.0038422970829665505,
 'log_base': 1.0466649627256428,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4067536938,
 'sample_weights': [1.8265840283800945, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.290966836494094,
 'weight_decay_Hydroxylation-K': 9.3243510125697,
 'weight_decay_Hydroxylation-P': 3.8742176738211347}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11845.021
[2,     1] loss: 11891.180
[3,     1] loss: 12011.443
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006868125705346938,
 'learning_rate_Hydroxylation-K': 0.009654379034347094,
 'learning_rate_Hydroxylation-P': 0.008632908075718173,
 'log_base': 1.9610914587193793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 435862582,
 'sample_weights': [36.60346466095827, 4.575609346675368],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.794124745687249,
 'weight_decay_Hydroxylation-K': 8.955134129580124,
 'weight_decay_Hydroxylation-P': 9.870825187525284}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1435.335
[2,     1] loss: 1435.813
[3,     1] loss: 1430.181
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008659355846305348,
 'learning_rate_Hydroxylation-K': 0.002554034429814985,
 'learning_rate_Hydroxylation-P': 0.005896732946243681,
 'log_base': 2.197176094716329,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 166683513,
 'sample_weights': [2.4787530961275137, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7318116043329881,
 'weight_decay_Hydroxylation-K': 5.245804180869513,
 'weight_decay_Hydroxylation-P': 2.0265696064435663}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1355.676
[2,     1] loss: 1366.510
[3,     1] loss: 1352.706
[4,     1] loss: 1351.070
[5,     1] loss: 1357.619
[6,     1] loss: 1357.435
[7,     1] loss: 1355.201
[8,     1] loss: 1351.294
[9,     1] loss: 1357.397
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002549457302142641,
 'learning_rate_Hydroxylation-K': 0.0023563942726160672,
 'learning_rate_Hydroxylation-P': 0.006590070146280026,
 'log_base': 2.9403679859507146,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3480985664,
 'sample_weights': [2.120808601515968, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6584792214922897,
 'weight_decay_Hydroxylation-K': 9.362953210122155,
 'weight_decay_Hydroxylation-P': 4.155648104557439}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.450
[2,     1] loss: 1234.900
[3,     1] loss: 1236.332
[4,     1] loss: 1233.171
[5,     1] loss: 1234.402
[6,     1] loss: 1230.670
[7,     1] loss: 1228.254
[8,     1] loss: 1233.701
[9,     1] loss: 1228.752
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009713221643681276,
 'learning_rate_Hydroxylation-K': 0.0001487031165030002,
 'learning_rate_Hydroxylation-P': 0.003342710447400509,
 'log_base': 1.1667048838974556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3547295623,
 'sample_weights': [1.5478807379627215, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.479407821703877,
 'weight_decay_Hydroxylation-K': 9.675867411804566,
 'weight_decay_Hydroxylation-P': 5.552836970550913}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3521.439
[2,     1] loss: 3516.948
[3,     1] loss: 3506.471
[4,     1] loss: 3519.047
[5,     1] loss: 3505.678
[6,     1] loss: 3511.274
[7,     1] loss: 3516.502
[8,     1] loss: 3510.197
[9,     1] loss: 3504.134
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009175920782100143,
 'learning_rate_Hydroxylation-K': 0.0054254531290414895,
 'learning_rate_Hydroxylation-P': 0.006878535379925089,
 'log_base': 2.8079369699844765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4080207056,
 'sample_weights': [10.827642584728244, 1.3535074636250504],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.261044215262637,
 'weight_decay_Hydroxylation-K': 7.810007730713296,
 'weight_decay_Hydroxylation-P': 0.7293490125130413}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.826
[2,     1] loss: 1247.042
[3,     1] loss: 1250.271
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008675616949334844,
 'learning_rate_Hydroxylation-K': 0.0037740319728572723,
 'learning_rate_Hydroxylation-P': 0.0029583984576036504,
 'log_base': 1.9427746811387143,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1155963443,
 'sample_weights': [1.6169723316735714, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.2409155659020055,
 'weight_decay_Hydroxylation-K': 7.873753281186323,
 'weight_decay_Hydroxylation-P': 4.197761231165243}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1438.403
[2,     1] loss: 1440.075
[3,     1] loss: 1432.993
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008225578566195758,
 'learning_rate_Hydroxylation-K': 0.009768984978954199,
 'learning_rate_Hydroxylation-P': 0.0008789283473002526,
 'log_base': 1.276431182535377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3013925566,
 'sample_weights': [2.513777913443521, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.2059440472888183,
 'weight_decay_Hydroxylation-K': 8.016499067276863,
 'weight_decay_Hydroxylation-P': 2.8952281044225647}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2354.776
[2,     1] loss: 2345.753
[3,     1] loss: 2379.556
[4,     1] loss: 2359.028
[5,     1] loss: 2349.847
[6,     1] loss: 2343.415
[7,     1] loss: 2359.403
[8,     1] loss: 2346.380
[9,     1] loss: 2350.001
[10,     1] loss: 2349.583
[11,     1] loss: 2343.298
[12,     1] loss: 2344.132
[13,     1] loss: 2349.038
[14,     1] loss: 2341.056
[15,     1] loss: 2348.583
[16,     1] loss: 2343.167
[17,     1] loss: 2340.057
[18,     1] loss: 2342.255
[19,     1] loss: 2337.240
[20,     1] loss: 2341.911
[21,     1] loss: 2343.747
[22,     1] loss: 2343.890
[23,     1] loss: 2335.925
[24,     1] loss: 2335.019
[25,     1] loss: 2327.677
[26,     1] loss: 2330.469
[27,     1] loss: 2304.223
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008738969374489446,
 'learning_rate_Hydroxylation-K': 0.006087164849925837,
 'learning_rate_Hydroxylation-P': 0.009169558288227287,
 'log_base': 2.371986007069264,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3075765042,
 'sample_weights': [6.840072595869025, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.22821165328331,
 'weight_decay_Hydroxylation-K': 5.478378891038537,
 'weight_decay_Hydroxylation-P': 6.867083378122635}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.206
[2,     1] loss: 1321.073
[3,     1] loss: 1317.007
[4,     1] loss: 1313.272
[5,     1] loss: 1321.962
[6,     1] loss: 1310.099
[7,     1] loss: 1312.682
[8,     1] loss: 1310.572
[9,     1] loss: 1309.180
[10,     1] loss: 1301.978
[11,     1] loss: 1293.756
[12,     1] loss: 1254.316
[13,     1] loss: 1211.552
[14,     1] loss: 1195.825
[15,     1] loss: 1149.802
[16,     1] loss: 1136.920
[17,     1] loss: 1085.520
[18,     1] loss: 1127.865
[19,     1] loss: 1079.191
[20,     1] loss: 1056.537
[21,     1] loss: 1068.278
[22,     1] loss: 1011.827
[23,     1] loss: 1027.277
[24,     1] loss: 1005.509
[25,     1] loss: 959.375
[26,     1] loss: 975.290
[27,     1] loss: 970.467
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004278733953938076,
 'learning_rate_Hydroxylation-K': 0.0001813252246096386,
 'learning_rate_Hydroxylation-P': 0.0010780152220517892,
 'log_base': 2.4917118821787456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2526450280,
 'sample_weights': [1.9328352855647288, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0131771711289814,
 'weight_decay_Hydroxylation-K': 9.666731098482536,
 'weight_decay_Hydroxylation-P': 8.561776406645544}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.668
[2,     1] loss: 1295.062
[3,     1] loss: 1297.688
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006401756052025862,
 'learning_rate_Hydroxylation-K': 0.0016551534645624613,
 'learning_rate_Hydroxylation-P': 0.007716492989560325,
 'log_base': 1.1069190807180074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3126103656,
 'sample_weights': [1.8285849362291675, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0356186161890992,
 'weight_decay_Hydroxylation-K': 4.949643063529653,
 'weight_decay_Hydroxylation-P': 2.8913642260305443}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5317.946
[2,     1] loss: 5312.086
[3,     1] loss: 5291.175
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019336851683614486,
 'learning_rate_Hydroxylation-K': 0.0009420702663227604,
 'learning_rate_Hydroxylation-P': 0.0017647430761427029,
 'log_base': 2.2854508762514048,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2642941052,
 'sample_weights': [16.434672722204763, 2.0544132314741965],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.779787496214462,
 'weight_decay_Hydroxylation-K': 2.467689454595142,
 'weight_decay_Hydroxylation-P': 6.1400415429186195}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1336.387
[2,     1] loss: 1336.735
[3,     1] loss: 1331.102
[4,     1] loss: 1333.733
[5,     1] loss: 1332.275
[6,     1] loss: 1330.776
[7,     1] loss: 1328.458
[8,     1] loss: 1326.510
[9,     1] loss: 1319.301
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005872416685231613,
 'learning_rate_Hydroxylation-K': 0.002410516510969684,
 'learning_rate_Hydroxylation-P': 0.001236143744061858,
 'log_base': 2.7128542240619495,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2191216257,
 'sample_weights': [2.0197401664516335, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.211130124725457,
 'weight_decay_Hydroxylation-K': 9.390335501152272,
 'weight_decay_Hydroxylation-P': 5.565684848999976}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.275
[2,     1] loss: 1263.804
[3,     1] loss: 1266.320
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006239289015954595,
 'learning_rate_Hydroxylation-K': 0.00045071172075817614,
 'learning_rate_Hydroxylation-P': 0.0012709608514190663,
 'log_base': 1.216055247256029,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3311748759,
 'sample_weights': [1.6727865461335825, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.565631322644676,
 'weight_decay_Hydroxylation-K': 4.990876889319223,
 'weight_decay_Hydroxylation-P': 2.935499172273473}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2781.500
[2,     1] loss: 2775.983
[3,     1] loss: 2757.570
[4,     1] loss: 2750.347
[5,     1] loss: 2772.137
[6,     1] loss: 2773.561
[7,     1] loss: 2751.159
[8,     1] loss: 2749.160
[9,     1] loss: 2734.968
[10,     1] loss: 2738.125
[11,     1] loss: 2723.346
[12,     1] loss: 2664.409
[13,     1] loss: 2606.836
[14,     1] loss: 2581.041
[15,     1] loss: 2468.741
[16,     1] loss: 2378.921
[17,     1] loss: 2396.158
[18,     1] loss: 2565.646
[19,     1] loss: 2330.681
[20,     1] loss: 2410.758
[21,     1] loss: 2321.508
[22,     1] loss: 2339.134
[23,     1] loss: 2671.327
[24,     1] loss: 2267.958
[25,     1] loss: 2525.013
[26,     1] loss: 2269.347
[27,     1] loss: 2213.600
[28,     1] loss: 2323.240
[29,     1] loss: 2180.281
[30,     1] loss: 2091.938
[31,     1] loss: 2165.377
[32,     1] loss: 2096.560
[33,     1] loss: 2118.308
[34,     1] loss: 1985.054
[35,     1] loss: 1922.002
[36,     1] loss: 1955.516
[37,     1] loss: 1922.392
[38,     1] loss: 1782.647
[39,     1] loss: 2135.810
[40,     1] loss: 1770.982
[41,     1] loss: 1985.801
[42,     1] loss: 1867.864
[43,     1] loss: 1774.888
[44,     1] loss: 1701.916
[45,     1] loss: 1479.596
[46,     1] loss: 1899.583
[47,     1] loss: 2654.503
[48,     1] loss: 1625.370
[49,     1] loss: 2253.932
[50,     1] loss: 1723.315
[51,     1] loss: 1813.087
[52,     1] loss: 2125.635
[53,     1] loss: 1827.121
[54,     1] loss: 1824.115
[55,     1] loss: 2033.846
[56,     1] loss: 1747.086
[57,     1] loss: 1854.016
Early stopping applied (best metric=0.7057493925094604)
Finished Training
Total time taken: 9.493009328842163
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2774.333
[2,     1] loss: 2777.472
[3,     1] loss: 2770.697
[4,     1] loss: 2755.485
[5,     1] loss: 2763.637
[6,     1] loss: 2750.648
[7,     1] loss: 2742.610
[8,     1] loss: 2725.566
[9,     1] loss: 2721.273
[10,     1] loss: 2656.553
[11,     1] loss: 2634.863
[12,     1] loss: 2527.839
[13,     1] loss: 2474.551
[14,     1] loss: 2270.151
[15,     1] loss: 2531.349
[16,     1] loss: 2263.293
[17,     1] loss: 2261.896
[18,     1] loss: 2353.856
[19,     1] loss: 2235.906
[20,     1] loss: 2312.830
[21,     1] loss: 2215.438
[22,     1] loss: 2422.219
[23,     1] loss: 2079.926
[24,     1] loss: 2237.168
[25,     1] loss: 2204.506
[26,     1] loss: 2264.216
[27,     1] loss: 2137.897
[28,     1] loss: 2040.065
[29,     1] loss: 2050.083
[30,     1] loss: 1964.633
[31,     1] loss: 2046.956
[32,     1] loss: 1904.378
[33,     1] loss: 1978.033
[34,     1] loss: 1888.792
[35,     1] loss: 2331.291
[36,     1] loss: 2238.970
[37,     1] loss: 2176.660
[38,     1] loss: 1954.243
[39,     1] loss: 2206.541
[40,     1] loss: 2125.726
[41,     1] loss: 2072.804
[42,     1] loss: 2036.451
[43,     1] loss: 1978.319
[44,     1] loss: 1860.398
[45,     1] loss: 1876.664
[46,     1] loss: 1802.478
[47,     1] loss: 1717.793
[48,     1] loss: 1771.233
[49,     1] loss: 1613.666
[50,     1] loss: 1948.221
[51,     1] loss: 1778.804
[52,     1] loss: 1640.656
[53,     1] loss: 1494.891
[54,     1] loss: 1546.106
[55,     1] loss: 1534.298
[56,     1] loss: 1413.848
[57,     1] loss: 1508.526
[58,     1] loss: 1428.403
[59,     1] loss: 1476.574
[60,     1] loss: 2062.233
[61,     1] loss: 2627.891
[62,     1] loss: 1606.923
Early stopping applied (best metric=0.900442361831665)
Finished Training
Total time taken: 8.374007940292358
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2778.062
[2,     1] loss: 2764.368
[3,     1] loss: 2764.715
[4,     1] loss: 2782.546
[5,     1] loss: 2763.942
[6,     1] loss: 2763.485
[7,     1] loss: 2770.815
[8,     1] loss: 2762.354
[9,     1] loss: 2768.099
[10,     1] loss: 2761.041
[11,     1] loss: 2756.317
[12,     1] loss: 2760.136
[13,     1] loss: 2741.152
[14,     1] loss: 2736.261
[15,     1] loss: 2701.046
[16,     1] loss: 2645.367
[17,     1] loss: 2615.723
[18,     1] loss: 2522.115
[19,     1] loss: 2471.036
[20,     1] loss: 2449.235
[21,     1] loss: 2314.488
[22,     1] loss: 2664.864
[23,     1] loss: 2270.812
[24,     1] loss: 2455.614
[25,     1] loss: 2178.800
[26,     1] loss: 2139.235
[27,     1] loss: 2217.870
[28,     1] loss: 2106.054
[29,     1] loss: 2252.673
[30,     1] loss: 1994.094
[31,     1] loss: 2056.505
[32,     1] loss: 2042.389
[33,     1] loss: 1863.680
[34,     1] loss: 2000.529
[35,     1] loss: 1882.227
[36,     1] loss: 1681.960
[37,     1] loss: 1775.863
[38,     1] loss: 1870.673
[39,     1] loss: 2066.860
[40,     1] loss: 1735.887
[41,     1] loss: 2017.653
[42,     1] loss: 1700.863
[43,     1] loss: 1870.842
[44,     1] loss: 1595.422
Early stopping applied (best metric=1.033836841583252)
Finished Training
Total time taken: 7.37300705909729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2769.691
[2,     1] loss: 2791.233
[3,     1] loss: 2778.126
[4,     1] loss: 2774.934
[5,     1] loss: 2769.927
[6,     1] loss: 2764.234
[7,     1] loss: 2766.122
[8,     1] loss: 2764.781
[9,     1] loss: 2773.477
[10,     1] loss: 2762.996
[11,     1] loss: 2761.808
[12,     1] loss: 2757.010
[13,     1] loss: 2767.215
[14,     1] loss: 2760.587
[15,     1] loss: 2762.851
[16,     1] loss: 2752.738
[17,     1] loss: 2756.994
[18,     1] loss: 2743.738
[19,     1] loss: 2729.415
[20,     1] loss: 2707.420
[21,     1] loss: 2662.173
[22,     1] loss: 2587.651
[23,     1] loss: 2584.268
[24,     1] loss: 2466.455
[25,     1] loss: 2325.393
[26,     1] loss: 2290.442
[27,     1] loss: 2361.523
[28,     1] loss: 2157.727
[29,     1] loss: 2061.930
[30,     1] loss: 2488.630
[31,     1] loss: 2007.513
[32,     1] loss: 2141.311
[33,     1] loss: 2072.147
[34,     1] loss: 1872.555
[35,     1] loss: 1896.325
[36,     1] loss: 1932.561
[37,     1] loss: 2016.014
[38,     1] loss: 1995.349
[39,     1] loss: 1739.323
[40,     1] loss: 1783.445
[41,     1] loss: 1702.689
[42,     1] loss: 1635.315
[43,     1] loss: 1633.324
[44,     1] loss: 1500.954
Early stopping applied (best metric=1.0305910110473633)
Finished Training
Total time taken: 7.393008232116699
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2766.561
[2,     1] loss: 2854.425
[3,     1] loss: 2782.586
[4,     1] loss: 2766.245
[5,     1] loss: 2764.832
[6,     1] loss: 2767.989
[7,     1] loss: 2775.816
[8,     1] loss: 2761.400
[9,     1] loss: 2763.659
[10,     1] loss: 2773.185
[11,     1] loss: 2773.292
[12,     1] loss: 2765.504
[13,     1] loss: 2765.118
[14,     1] loss: 2761.524
[15,     1] loss: 2758.402
[16,     1] loss: 2757.933
[17,     1] loss: 2753.990
[18,     1] loss: 2739.310
[19,     1] loss: 2727.592
[20,     1] loss: 2684.421
[21,     1] loss: 2661.780
[22,     1] loss: 2587.330
[23,     1] loss: 2583.960
[24,     1] loss: 2546.892
[25,     1] loss: 2510.694
[26,     1] loss: 2496.649
[27,     1] loss: 2393.603
[28,     1] loss: 2385.163
[29,     1] loss: 2376.959
[30,     1] loss: 2335.179
[31,     1] loss: 2422.938
[32,     1] loss: 2223.955
[33,     1] loss: 2448.159
[34,     1] loss: 2203.502
[35,     1] loss: 2441.105
[36,     1] loss: 2245.765
[37,     1] loss: 2169.955
[38,     1] loss: 2182.682
[39,     1] loss: 2061.633
[40,     1] loss: 2171.790
[41,     1] loss: 1975.513
[42,     1] loss: 1982.079
[43,     1] loss: 1960.905
[44,     1] loss: 2118.698
[45,     1] loss: 1919.296
[46,     1] loss: 1951.952
[47,     1] loss: 1955.766
[48,     1] loss: 1830.010
[49,     1] loss: 1887.956
[50,     1] loss: 1707.419
[51,     1] loss: 1645.002
Early stopping applied (best metric=0.8870885372161865)
Finished Training
Total time taken: 6.905007600784302
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2780.301
[2,     1] loss: 2775.407
[3,     1] loss: 2766.796
[4,     1] loss: 2765.971
[5,     1] loss: 2771.590
[6,     1] loss: 2779.526
[7,     1] loss: 2760.703
[8,     1] loss: 2761.533
[9,     1] loss: 2774.817
[10,     1] loss: 2759.883
[11,     1] loss: 2755.923
[12,     1] loss: 2759.248
[13,     1] loss: 2743.771
[14,     1] loss: 2726.562
[15,     1] loss: 2717.400
[16,     1] loss: 2676.672
[17,     1] loss: 2646.850
[18,     1] loss: 2622.360
[19,     1] loss: 2556.255
[20,     1] loss: 2449.256
[21,     1] loss: 2431.500
[22,     1] loss: 2227.368
[23,     1] loss: 2393.378
[24,     1] loss: 2400.637
[25,     1] loss: 2312.305
[26,     1] loss: 2173.722
[27,     1] loss: 2396.215
[28,     1] loss: 2138.166
[29,     1] loss: 2247.437
[30,     1] loss: 2223.829
[31,     1] loss: 2103.088
[32,     1] loss: 2144.488
[33,     1] loss: 2147.195
[34,     1] loss: 1962.779
[35,     1] loss: 2063.115
[36,     1] loss: 1892.685
[37,     1] loss: 1818.587
[38,     1] loss: 1813.351
[39,     1] loss: 1988.164
[40,     1] loss: 1897.376
[41,     1] loss: 2515.242
[42,     1] loss: 2097.002
[43,     1] loss: 1979.072
[44,     1] loss: 1802.406
[45,     1] loss: 2041.425
[46,     1] loss: 2164.038
[47,     1] loss: 1999.822
[48,     1] loss: 1807.831
[49,     1] loss: 2031.987
[50,     1] loss: 1737.535
[51,     1] loss: 1820.776
[52,     1] loss: 1726.562
[53,     1] loss: 1807.324
[54,     1] loss: 1667.895
[55,     1] loss: 1685.568
[56,     1] loss: 1404.869
[57,     1] loss: 1635.292
[58,     1] loss: 1563.621
[59,     1] loss: 1488.823
[60,     1] loss: 1340.659
[61,     1] loss: 1620.711
[62,     1] loss: 1514.690
[63,     1] loss: 1365.760
[64,     1] loss: 1412.826
[65,     1] loss: 2210.113
[66,     1] loss: 2583.725
[67,     1] loss: 1450.916
[68,     1] loss: 1876.071
[69,     1] loss: 1796.580
Early stopping applied (best metric=0.8777148723602295)
Finished Training
Total time taken: 11.562012434005737
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2770.953
[2,     1] loss: 2755.658
[3,     1] loss: 2802.115
[4,     1] loss: 2764.242
[5,     1] loss: 2761.863
[6,     1] loss: 2756.995
[7,     1] loss: 2762.513
[8,     1] loss: 2760.032
[9,     1] loss: 2761.585
[10,     1] loss: 2732.538
[11,     1] loss: 2708.623
[12,     1] loss: 2700.245
[13,     1] loss: 2681.623
[14,     1] loss: 2600.811
[15,     1] loss: 2555.417
[16,     1] loss: 2529.812
[17,     1] loss: 2441.435
[18,     1] loss: 2251.797
[19,     1] loss: 2276.221
[20,     1] loss: 2183.173
[21,     1] loss: 2657.702
[22,     1] loss: 2477.487
[23,     1] loss: 2265.068
[24,     1] loss: 2128.017
[25,     1] loss: 2357.274
[26,     1] loss: 2249.231
[27,     1] loss: 2196.904
[28,     1] loss: 2199.896
[29,     1] loss: 2332.235
[30,     1] loss: 2100.512
[31,     1] loss: 2124.522
[32,     1] loss: 2116.855
[33,     1] loss: 1954.035
[34,     1] loss: 1961.588
[35,     1] loss: 1963.132
[36,     1] loss: 2127.085
[37,     1] loss: 1941.265
[38,     1] loss: 1896.411
[39,     1] loss: 1610.996
[40,     1] loss: 1737.293
[41,     1] loss: 1842.602
[42,     1] loss: 1816.510
[43,     1] loss: 1856.429
[44,     1] loss: 1694.423
[45,     1] loss: 1730.161
[46,     1] loss: 1735.720
[47,     1] loss: 1688.980
[48,     1] loss: 1487.025
[49,     1] loss: 1592.025
[50,     1] loss: 1894.913
[51,     1] loss: 3003.618
[52,     1] loss: 1521.649
[53,     1] loss: 2081.250
[54,     1] loss: 1756.791
[55,     1] loss: 1946.099
[56,     1] loss: 1933.543
[57,     1] loss: 1975.746
[58,     1] loss: 1872.785
[59,     1] loss: 1915.067
[60,     1] loss: 2112.983
[61,     1] loss: 1691.749
[62,     1] loss: 1937.347
[63,     1] loss: 1980.815
[64,     1] loss: 1620.776
[65,     1] loss: 1744.201
[66,     1] loss: 1436.414
Early stopping applied (best metric=0.8859630823135376)
Finished Training
Total time taken: 8.894008874893188
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2774.482
[2,     1] loss: 2764.695
[3,     1] loss: 2772.310
[4,     1] loss: 2767.284
[5,     1] loss: 2760.144
[6,     1] loss: 2760.246
[7,     1] loss: 2764.493
[8,     1] loss: 2769.844
[9,     1] loss: 2752.364
[10,     1] loss: 2748.374
[11,     1] loss: 2755.923
[12,     1] loss: 2747.052
[13,     1] loss: 2742.367
[14,     1] loss: 2732.585
[15,     1] loss: 2714.073
[16,     1] loss: 2679.842
[17,     1] loss: 2567.851
[18,     1] loss: 2539.462
[19,     1] loss: 2489.766
[20,     1] loss: 2515.094
[21,     1] loss: 2331.903
[22,     1] loss: 2431.325
[23,     1] loss: 2205.667
[24,     1] loss: 2374.304
[25,     1] loss: 2112.528
[26,     1] loss: 2388.076
[27,     1] loss: 2106.334
[28,     1] loss: 2143.271
[29,     1] loss: 2112.333
[30,     1] loss: 2157.201
[31,     1] loss: 2106.645
[32,     1] loss: 1989.645
[33,     1] loss: 1956.730
[34,     1] loss: 2076.426
[35,     1] loss: 1982.061
[36,     1] loss: 1836.696
[37,     1] loss: 1846.814
[38,     1] loss: 2031.523
[39,     1] loss: 2299.487
[40,     1] loss: 1597.114
[41,     1] loss: 2159.060
[42,     1] loss: 1745.020
[43,     1] loss: 1958.067
[44,     1] loss: 1987.919
[45,     1] loss: 1767.717
[46,     1] loss: 2065.035
[47,     1] loss: 1697.901
[48,     1] loss: 1937.249
Early stopping applied (best metric=0.9132473468780518)
Finished Training
Total time taken: 7.945007562637329
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2773.271
[2,     1] loss: 2773.581
[3,     1] loss: 2764.119
[4,     1] loss: 2758.143
[5,     1] loss: 2784.424
[6,     1] loss: 2764.858
[7,     1] loss: 2754.517
[8,     1] loss: 2750.126
[9,     1] loss: 2752.531
[10,     1] loss: 2732.360
[11,     1] loss: 2709.799
[12,     1] loss: 2680.726
[13,     1] loss: 2631.074
[14,     1] loss: 2581.670
[15,     1] loss: 2458.593
[16,     1] loss: 2484.984
[17,     1] loss: 2441.408
[18,     1] loss: 2362.195
[19,     1] loss: 2365.163
[20,     1] loss: 2397.755
[21,     1] loss: 2173.574
[22,     1] loss: 2268.619
[23,     1] loss: 2152.066
[24,     1] loss: 2076.589
[25,     1] loss: 2071.068
[26,     1] loss: 2123.611
[27,     1] loss: 1940.962
[28,     1] loss: 2048.931
[29,     1] loss: 1984.878
[30,     1] loss: 1962.001
[31,     1] loss: 1825.987
[32,     1] loss: 2006.852
[33,     1] loss: 1939.951
[34,     1] loss: 2221.836
[35,     1] loss: 1682.400
[36,     1] loss: 2018.702
[37,     1] loss: 1785.277
[38,     1] loss: 1921.843
[39,     1] loss: 1761.671
[40,     1] loss: 1759.899
[41,     1] loss: 1961.398
[42,     1] loss: 1614.582
[43,     1] loss: 1816.100
[44,     1] loss: 1455.592
Early stopping applied (best metric=0.9053652286529541)
Finished Training
Total time taken: 5.94500470161438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2773.783
[2,     1] loss: 2770.486
[3,     1] loss: 2771.565
[4,     1] loss: 2772.986
[5,     1] loss: 2770.650
[6,     1] loss: 2777.325
[7,     1] loss: 2767.072
[8,     1] loss: 2768.646
[9,     1] loss: 2765.724
[10,     1] loss: 2760.070
[11,     1] loss: 2765.937
[12,     1] loss: 2763.007
[13,     1] loss: 2758.592
[14,     1] loss: 2751.498
[15,     1] loss: 2730.822
[16,     1] loss: 2706.807
[17,     1] loss: 2656.017
[18,     1] loss: 2595.581
[19,     1] loss: 2540.454
[20,     1] loss: 2504.493
[21,     1] loss: 2444.667
[22,     1] loss: 2397.563
[23,     1] loss: 2404.665
[24,     1] loss: 2401.318
[25,     1] loss: 2319.258
[26,     1] loss: 2244.918
[27,     1] loss: 2188.112
[28,     1] loss: 2201.360
[29,     1] loss: 2151.371
[30,     1] loss: 2316.159
[31,     1] loss: 2226.732
[32,     1] loss: 2100.732
[33,     1] loss: 2046.246
[34,     1] loss: 1974.477
[35,     1] loss: 2224.073
[36,     1] loss: 2510.668
[37,     1] loss: 2000.447
[38,     1] loss: 2219.986
[39,     1] loss: 2063.510
[40,     1] loss: 2109.180
[41,     1] loss: 2349.441
[42,     1] loss: 2010.337
[43,     1] loss: 2044.186
[44,     1] loss: 2120.826
[45,     1] loss: 2020.152
[46,     1] loss: 1927.163
[47,     1] loss: 2178.082
[48,     1] loss: 1786.198
[49,     1] loss: 1918.452
[50,     1] loss: 1781.494
[51,     1] loss: 1998.227
[52,     1] loss: 1908.582
[53,     1] loss: 1611.442
[54,     1] loss: 2257.474
[55,     1] loss: 1767.188
[56,     1] loss: 1808.399
[57,     1] loss: 1710.764
[58,     1] loss: 1709.147
[59,     1] loss: 1740.459
[60,     1] loss: 1603.474
[61,     1] loss: 1608.359
[62,     1] loss: 1620.860
[63,     1] loss: 1512.656
[64,     1] loss: 1665.443
[65,     1] loss: 2022.879
[66,     1] loss: 1497.081
Early stopping applied (best metric=0.8984538912773132)
Finished Training
Total time taken: 10.965010643005371
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2769.492
[2,     1] loss: 2781.002
[3,     1] loss: 2768.749
[4,     1] loss: 2765.754
[5,     1] loss: 2767.258
[6,     1] loss: 2771.172
[7,     1] loss: 2767.674
[8,     1] loss: 2762.867
[9,     1] loss: 2765.457
[10,     1] loss: 2765.008
[11,     1] loss: 2763.910
[12,     1] loss: 2761.214
[13,     1] loss: 2761.186
[14,     1] loss: 2752.944
[15,     1] loss: 2749.930
[16,     1] loss: 2746.665
[17,     1] loss: 2740.151
[18,     1] loss: 2693.267
[19,     1] loss: 2639.282
[20,     1] loss: 2614.923
[21,     1] loss: 2488.740
[22,     1] loss: 2449.585
[23,     1] loss: 2458.913
[24,     1] loss: 2293.628
[25,     1] loss: 2205.939
[26,     1] loss: 2284.777
[27,     1] loss: 2166.344
[28,     1] loss: 2573.330
[29,     1] loss: 2338.107
[30,     1] loss: 2198.775
[31,     1] loss: 2214.350
[32,     1] loss: 2131.863
[33,     1] loss: 2104.838
[34,     1] loss: 2041.222
[35,     1] loss: 2014.164
[36,     1] loss: 2155.800
[37,     1] loss: 1786.311
[38,     1] loss: 1943.094
[39,     1] loss: 1766.502
[40,     1] loss: 1766.189
[41,     1] loss: 1763.670
[42,     1] loss: 1937.380
[43,     1] loss: 2037.506
[44,     1] loss: 1699.717
[45,     1] loss: 1840.683
[46,     1] loss: 1782.604
[47,     1] loss: 1600.632
[48,     1] loss: 1685.967
[49,     1] loss: 1696.710
[50,     1] loss: 1612.185
[51,     1] loss: 1698.480
[52,     1] loss: 1656.871
[53,     1] loss: 1528.505
[54,     1] loss: 1489.083
[55,     1] loss: 1535.076
[56,     1] loss: 1468.536
[57,     1] loss: 1411.035
[58,     1] loss: 1504.063
[59,     1] loss: 2380.834
[60,     1] loss: 2351.385
[61,     1] loss: 1537.197
[62,     1] loss: 1902.060
[63,     1] loss: 1996.010
[64,     1] loss: 1733.309
[65,     1] loss: 1752.744
[66,     1] loss: 1844.642
Early stopping applied (best metric=0.8635297417640686)
Finished Training
Total time taken: 10.810009956359863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2775.999
[2,     1] loss: 2773.524
[3,     1] loss: 2758.798
[4,     1] loss: 2777.103
[5,     1] loss: 2761.884
[6,     1] loss: 2763.868
[7,     1] loss: 2761.442
[8,     1] loss: 2751.928
[9,     1] loss: 2759.550
[10,     1] loss: 2736.457
[11,     1] loss: 2729.665
[12,     1] loss: 2695.534
[13,     1] loss: 2643.429
[14,     1] loss: 2594.703
[15,     1] loss: 2476.627
[16,     1] loss: 2345.427
[17,     1] loss: 2372.080
[18,     1] loss: 2305.863
[19,     1] loss: 2204.933
[20,     1] loss: 2104.999
[21,     1] loss: 2131.266
[22,     1] loss: 2041.161
[23,     1] loss: 2112.049
[24,     1] loss: 1995.520
[25,     1] loss: 2130.674
[26,     1] loss: 2150.853
[27,     1] loss: 2165.600
[28,     1] loss: 2001.574
[29,     1] loss: 1928.708
[30,     1] loss: 2187.408
[31,     1] loss: 2001.380
[32,     1] loss: 2127.217
[33,     1] loss: 1874.832
Early stopping applied (best metric=1.06390380859375)
Finished Training
Total time taken: 5.457004547119141
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2766.908
[2,     1] loss: 2762.559
[3,     1] loss: 2787.184
[4,     1] loss: 2771.579
[5,     1] loss: 2768.262
[6,     1] loss: 2756.218
[7,     1] loss: 2778.214
[8,     1] loss: 2761.349
[9,     1] loss: 2760.498
[10,     1] loss: 2767.149
[11,     1] loss: 2766.378
[12,     1] loss: 2767.605
[13,     1] loss: 2763.113
[14,     1] loss: 2762.131
[15,     1] loss: 2757.435
[16,     1] loss: 2749.939
[17,     1] loss: 2747.418
[18,     1] loss: 2772.683
[19,     1] loss: 2746.918
[20,     1] loss: 2735.239
[21,     1] loss: 2704.800
[22,     1] loss: 2644.982
[23,     1] loss: 2609.001
[24,     1] loss: 2604.378
[25,     1] loss: 2562.615
[26,     1] loss: 2451.492
[27,     1] loss: 2441.612
[28,     1] loss: 2306.933
[29,     1] loss: 2437.250
[30,     1] loss: 2379.469
[31,     1] loss: 2519.039
[32,     1] loss: 2295.805
[33,     1] loss: 2256.862
[34,     1] loss: 2347.677
[35,     1] loss: 2215.639
[36,     1] loss: 2282.179
[37,     1] loss: 2143.010
[38,     1] loss: 2043.962
[39,     1] loss: 2072.564
[40,     1] loss: 2122.149
[41,     1] loss: 2012.826
[42,     1] loss: 1801.394
[43,     1] loss: 2029.838
[44,     1] loss: 1799.468
[45,     1] loss: 1678.578
[46,     1] loss: 1972.484
[47,     1] loss: 3350.907
[48,     1] loss: 1801.989
[49,     1] loss: 2122.544
[50,     1] loss: 2155.676
[51,     1] loss: 2030.845
[52,     1] loss: 2170.544
[53,     1] loss: 2281.954
[54,     1] loss: 2192.642
[55,     1] loss: 2079.392
[56,     1] loss: 1949.409
[57,     1] loss: 2069.489
[58,     1] loss: 1963.534
[59,     1] loss: 1958.304
[60,     1] loss: 2063.820
[61,     1] loss: 1723.450
[62,     1] loss: 1818.128
[63,     1] loss: 1676.156
[64,     1] loss: 1783.779
[65,     1] loss: 1832.355
[66,     1] loss: 1587.797
[67,     1] loss: 1693.763
[68,     1] loss: 1550.332
[69,     1] loss: 1673.333
[70,     1] loss: 1689.133
[71,     1] loss: 1608.700
[72,     1] loss: 1751.741
[73,     1] loss: 1465.954
[74,     1] loss: 1512.405
[75,     1] loss: 1399.079
[76,     1] loss: 1530.425
[77,     1] loss: 1681.314
[78,     1] loss: 2328.747
Early stopping applied (best metric=0.7955406904220581)
Finished Training
Total time taken: 10.633009910583496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2778.061
[2,     1] loss: 2774.692
[3,     1] loss: 2766.584
[4,     1] loss: 2770.460
[5,     1] loss: 2769.308
[6,     1] loss: 2770.912
[7,     1] loss: 2771.337
[8,     1] loss: 2781.175
[9,     1] loss: 2765.384
[10,     1] loss: 2765.435
[11,     1] loss: 2781.894
[12,     1] loss: 2772.757
[13,     1] loss: 2760.737
[14,     1] loss: 2763.253
[15,     1] loss: 2768.973
[16,     1] loss: 2765.783
[17,     1] loss: 2763.991
[18,     1] loss: 2769.876
[19,     1] loss: 2765.719
[20,     1] loss: 2760.456
[21,     1] loss: 2758.309
[22,     1] loss: 2752.854
[23,     1] loss: 2756.838
[24,     1] loss: 2747.197
[25,     1] loss: 2731.627
[26,     1] loss: 2696.385
[27,     1] loss: 2671.091
[28,     1] loss: 2620.942
[29,     1] loss: 2585.028
[30,     1] loss: 2576.799
[31,     1] loss: 2435.429
[32,     1] loss: 2362.561
[33,     1] loss: 2400.231
[34,     1] loss: 2326.285
[35,     1] loss: 2398.269
[36,     1] loss: 2348.893
[37,     1] loss: 2395.666
[38,     1] loss: 2644.346
[39,     1] loss: 2267.404
[40,     1] loss: 2267.068
[41,     1] loss: 2265.304
[42,     1] loss: 2198.768
[43,     1] loss: 2262.800
[44,     1] loss: 2136.870
[45,     1] loss: 2289.482
[46,     1] loss: 2110.196
[47,     1] loss: 2453.715
[48,     1] loss: 2039.687
[49,     1] loss: 2329.167
[50,     1] loss: 1998.031
[51,     1] loss: 2254.113
[52,     1] loss: 2066.841
[53,     1] loss: 2032.715
[54,     1] loss: 2230.125
[55,     1] loss: 2072.960
[56,     1] loss: 1903.770
[57,     1] loss: 1982.347
[58,     1] loss: 1738.910
[59,     1] loss: 2165.824
[60,     1] loss: 1884.043
[61,     1] loss: 1789.878
[62,     1] loss: 1757.443
[63,     1] loss: 1991.212
[64,     1] loss: 2014.499
[65,     1] loss: 1825.956
[66,     1] loss: 1769.295
[67,     1] loss: 1659.578
[68,     1] loss: 1645.250
[69,     1] loss: 1532.919
[70,     1] loss: 1738.781
[71,     1] loss: 1685.654
[72,     1] loss: 1423.475
[73,     1] loss: 1513.333
[74,     1] loss: 1422.703
[75,     1] loss: 1376.757
[76,     1] loss: 1547.307
[77,     1] loss: 2860.243
[78,     1] loss: 1579.622
[79,     1] loss: 2238.886
[80,     1] loss: 1751.546
[81,     1] loss: 2073.389
[82,     1] loss: 1952.688
[83,     1] loss: 1976.300
[84,     1] loss: 1877.759
[85,     1] loss: 1737.608
[86,     1] loss: 1748.689
[87,     1] loss: 1595.757
[88,     1] loss: 1710.148
[89,     1] loss: 1747.337
[90,     1] loss: 1433.367
[91,     1] loss: 2059.912
[92,     1] loss: 2888.758
[93,     1] loss: 2621.281
[94,     1] loss: 1615.423
[95,     1] loss: 2676.747
[96,     1] loss: 2095.245
[97,     1] loss: 1988.186
Early stopping applied (best metric=0.7859594821929932)
Finished Training
Total time taken: 14.810014724731445
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2798.223
[2,     1] loss: 2766.938
[3,     1] loss: 2779.150
[4,     1] loss: 2776.716
[5,     1] loss: 2773.205
[6,     1] loss: 2771.646
[7,     1] loss: 2763.567
[8,     1] loss: 2766.782
[9,     1] loss: 2757.122
[10,     1] loss: 2752.498
[11,     1] loss: 2756.876
[12,     1] loss: 2749.481
[13,     1] loss: 2724.090
[14,     1] loss: 2704.228
[15,     1] loss: 2663.430
[16,     1] loss: 2623.321
[17,     1] loss: 2531.239
[18,     1] loss: 2493.812
[19,     1] loss: 2463.895
[20,     1] loss: 2440.209
[21,     1] loss: 2466.075
[22,     1] loss: 2432.217
[23,     1] loss: 2287.220
[24,     1] loss: 2321.896
[25,     1] loss: 2224.890
[26,     1] loss: 2343.062
[27,     1] loss: 2213.120
[28,     1] loss: 2253.926
[29,     1] loss: 2112.522
[30,     1] loss: 2237.272
[31,     1] loss: 2160.836
[32,     1] loss: 2225.352
[33,     1] loss: 1974.819
[34,     1] loss: 2196.816
[35,     1] loss: 1987.899
[36,     1] loss: 1854.027
[37,     1] loss: 2008.940
[38,     1] loss: 1824.883
[39,     1] loss: 2059.082
[40,     1] loss: 1739.682
[41,     1] loss: 2165.887
[42,     1] loss: 1664.528
[43,     1] loss: 1973.826
[44,     1] loss: 1779.074
[45,     1] loss: 1761.820
[46,     1] loss: 1730.596
[47,     1] loss: 1617.335
[48,     1] loss: 1911.300
[49,     1] loss: 1858.541
[50,     1] loss: 1716.953
[51,     1] loss: 1701.782
[52,     1] loss: 1633.906
[53,     1] loss: 1630.377
[54,     1] loss: 1489.053
[55,     1] loss: 1742.525
Early stopping applied (best metric=0.8590515851974487)
Finished Training
Total time taken: 9.227007865905762
{'Hydroxylation-K Validation Accuracy': 0.7044326241134752, 'Hydroxylation-K Validation Sensitivity': 0.7555555555555555, 'Hydroxylation-K Validation Specificity': 0.6912280701754386, 'Hydroxylation-K Validation Precision': 0.39979803312156253, 'Hydroxylation-K AUC ROC': 0.7797465886939571, 'Hydroxylation-K AUC PR': 0.566438220451091, 'Hydroxylation-K MCC': 0.37597898414583775, 'Hydroxylation-K F1': 0.5169034332946781, 'Validation Loss (Hydroxylation-K)': 0.44156602025032043, 'Hydroxylation-P Validation Accuracy': 0.7373374109605265, 'Hydroxylation-P Validation Sensitivity': 0.6855026455026455, 'Hydroxylation-P Validation Specificity': 0.7484637637787421, 'Hydroxylation-P Validation Precision': 0.38250572102733044, 'Hydroxylation-P AUC ROC': 0.7745574176600661, 'Hydroxylation-P AUC PR': 0.472946379054003, 'Hydroxylation-P MCC': 0.36025833200918184, 'Hydroxylation-P F1': 0.48556503063768425, 'Validation Loss (Hydroxylation-P)': 0.4521964967250824, 'Validation Loss (total)': 0.8937625249226888, 'TimeToTrain': 9.052408758799235}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007520213211818438,
 'learning_rate_Hydroxylation-K': 0.0002920292275125383,
 'learning_rate_Hydroxylation-P': 0.0011129631490777718,
 'log_base': 1.2816017019445587,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2788276418,
 'sample_weights': [8.540783243977636, 1.0653796256731023],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.5899284564190275,
 'weight_decay_Hydroxylation-K': 3.9855094147205805,
 'weight_decay_Hydroxylation-P': 3.179983932496499}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2338.037
[2,     1] loss: 2332.176
[3,     1] loss: 2333.404
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00770429544967694,
 'learning_rate_Hydroxylation-K': 0.008570544240509265,
 'learning_rate_Hydroxylation-P': 0.009117403347151882,
 'log_base': 2.2924751800781373,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3641561666,
 'sample_weights': [6.72862415828527, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.018561610111854,
 'weight_decay_Hydroxylation-K': 0.39602383389509477,
 'weight_decay_Hydroxylation-P': 1.89942342365985}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1339.001
[2,     1] loss: 1361.244
[3,     1] loss: 1333.556
[4,     1] loss: 1333.133
[5,     1] loss: 1333.198
[6,     1] loss: 1333.396
[7,     1] loss: 1334.513
[8,     1] loss: 1329.557
[9,     1] loss: 1331.613
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002006022476657898,
 'learning_rate_Hydroxylation-K': 0.0014367788190442295,
 'learning_rate_Hydroxylation-P': 0.006813854890702713,
 'log_base': 2.175416292332444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1161392625,
 'sample_weights': [2.012269234050829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.530020808398605,
 'weight_decay_Hydroxylation-K': 6.839961377879494,
 'weight_decay_Hydroxylation-P': 0.7249295406496343}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1363.554
[2,     1] loss: 1356.558
[3,     1] loss: 1358.291
[4,     1] loss: 1361.668
[5,     1] loss: 1361.044
[6,     1] loss: 1358.210
[7,     1] loss: 1358.020
[8,     1] loss: 1352.569
[9,     1] loss: 1349.415
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005572740667900009,
 'learning_rate_Hydroxylation-K': 0.00016286450126042594,
 'learning_rate_Hydroxylation-P': 0.00403439843733893,
 'log_base': 1.0251978936953445,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3511691164,
 'sample_weights': [2.1479671788755175, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.222106785401525,
 'weight_decay_Hydroxylation-K': 0.10131205873512927,
 'weight_decay_Hydroxylation-P': 0.2628115950530353}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21707.363
Exploding loss, terminate run (best metric=1.0998517274856567)
Finished Training
Total time taken: 0.19500160217285156
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21824.662
Exploding loss, terminate run (best metric=1.0894229412078857)
Finished Training
Total time taken: 0.21199965476989746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21954.105
Exploding loss, terminate run (best metric=1.0930143594741821)
Finished Training
Total time taken: 0.2330005168914795
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21793.049
Exploding loss, terminate run (best metric=1.0902886390686035)
Finished Training
Total time taken: 0.2240004539489746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21682.262
Exploding loss, terminate run (best metric=1.089321494102478)
Finished Training
Total time taken: 0.20200037956237793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21785.637
Exploding loss, terminate run (best metric=1.0953471660614014)
Finished Training
Total time taken: 0.22699952125549316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21855.807
Exploding loss, terminate run (best metric=1.0917773246765137)
Finished Training
Total time taken: 0.21900105476379395
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21754.162
Exploding loss, terminate run (best metric=1.1027673482894897)
Finished Training
Total time taken: 0.20200061798095703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21764.441
Exploding loss, terminate run (best metric=1.0782859325408936)
Finished Training
Total time taken: 0.21799874305725098
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21813.992
Exploding loss, terminate run (best metric=1.0837514400482178)
Finished Training
Total time taken: 0.22499895095825195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21752.105
Exploding loss, terminate run (best metric=1.0973002910614014)
Finished Training
Total time taken: 0.20199990272521973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21882.055
Exploding loss, terminate run (best metric=1.0935519933700562)
Finished Training
Total time taken: 0.20000004768371582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21745.957
Exploding loss, terminate run (best metric=1.1028116941452026)
Finished Training
Total time taken: 0.22799944877624512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21816.617
Exploding loss, terminate run (best metric=1.074366807937622)
Finished Training
Total time taken: 0.22100019454956055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21818.906
Exploding loss, terminate run (best metric=1.0772666931152344)
Finished Training
Total time taken: 0.20299935340881348
{'Hydroxylation-K Validation Accuracy': 0.6377659574468085, 'Hydroxylation-K Validation Sensitivity': 0.26666666666666666, 'Hydroxylation-K Validation Specificity': 0.7298245614035088, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6369590643274854, 'Hydroxylation-K AUC PR': 0.3430302626058183, 'Hydroxylation-K MCC': -0.006839855680567693, 'Hydroxylation-K F1': 0.09039408866995075, 'Validation Loss (Hydroxylation-K)': 0.5577513019243876, 'Hydroxylation-P Validation Accuracy': 0.648242288885505, 'Hydroxylation-P Validation Sensitivity': 0.2742857142857143, 'Hydroxylation-P Validation Specificity': 0.7284552845528456, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5738179238643102, 'Hydroxylation-P AUC PR': 0.2677395258729827, 'Hydroxylation-P MCC': 0.0038377728091091843, 'Hydroxylation-P F1': 0.09071115158004704, 'Validation Loss (Hydroxylation-P)': 0.532857092221578, 'Validation Loss (total)': 1.0906083901723227, 'TimeToTrain': 0.2140666961669922}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038627683563965277,
 'learning_rate_Hydroxylation-K': 0.0024371713042219845,
 'learning_rate_Hydroxylation-P': 0.006146387487062832,
 'log_base': 2.422246412094174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 59963560,
 'sample_weights': [67.13430434260952, 8.37435138993685],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.373547700650885,
 'weight_decay_Hydroxylation-K': 9.815349683612705,
 'weight_decay_Hydroxylation-P': 2.017240615041721}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.089
[2,     1] loss: 1307.783
[3,     1] loss: 1307.575
[4,     1] loss: 1303.209
[5,     1] loss: 1303.528
[6,     1] loss: 1302.639
[7,     1] loss: 1300.554
[8,     1] loss: 1299.930
[9,     1] loss: 1291.940
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004844465259701546,
 'learning_rate_Hydroxylation-K': 0.001556254346162136,
 'learning_rate_Hydroxylation-P': 0.0032975334615960696,
 'log_base': 1.010234051561328,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1952668739,
 'sample_weights': [1.8870259605176662, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.756521561094558,
 'weight_decay_Hydroxylation-K': 4.858789997477161,
 'weight_decay_Hydroxylation-P': 1.6701832523648585}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53294.441
Exploding loss, terminate run (best metric=1.0965521335601807)
Finished Training
Total time taken: 0.2070002555847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53428.195
Exploding loss, terminate run (best metric=1.0944678783416748)
Finished Training
Total time taken: 0.19700312614440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53103.180
Exploding loss, terminate run (best metric=1.1068754196166992)
Finished Training
Total time taken: 0.22099757194519043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53172.637
Exploding loss, terminate run (best metric=1.073689341545105)
Finished Training
Total time taken: 0.2110002040863037
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 53602.891
Exploding loss, terminate run (best metric=1.075507640838623)
Finished Training
Total time taken: 0.20099997520446777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53404.066
Exploding loss, terminate run (best metric=1.1050572395324707)
Finished Training
Total time taken: 0.22299742698669434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53255.602
Exploding loss, terminate run (best metric=1.093087911605835)
Finished Training
Total time taken: 0.22499990463256836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53110.270
Exploding loss, terminate run (best metric=1.0932292938232422)
Finished Training
Total time taken: 0.20099925994873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53225.672
Exploding loss, terminate run (best metric=1.0723369121551514)
Finished Training
Total time taken: 0.2090015411376953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 53278.625
Exploding loss, terminate run (best metric=1.0753523111343384)
Finished Training
Total time taken: 0.22099971771240234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53203.262
Exploding loss, terminate run (best metric=1.0955488681793213)
Finished Training
Total time taken: 0.19800114631652832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53155.562
Exploding loss, terminate run (best metric=1.091871976852417)
Finished Training
Total time taken: 0.20299816131591797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53834.738
Exploding loss, terminate run (best metric=1.0940625667572021)
Finished Training
Total time taken: 0.20199918746948242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 53237.816
Exploding loss, terminate run (best metric=1.0743697881698608)
Finished Training
Total time taken: 0.21000003814697266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 53148.953
Exploding loss, terminate run (best metric=1.0886036157608032)
Finished Training
Total time taken: 0.22700214385986328
{'Hydroxylation-K Validation Accuracy': 0.4082151300236406, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3456140350877193, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6326705653021443, 'Hydroxylation-K AUC PR': 0.307474493574858, 'Hydroxylation-K MCC': 0.018034197251558817, 'Hydroxylation-K F1': 0.2249798770082746, 'Validation Loss (Hydroxylation-K)': 0.5575663844744364, 'Hydroxylation-P Validation Accuracy': 0.40636407627362403, 'Hydroxylation-P Validation Sensitivity': 0.6628571428571428, 'Hydroxylation-P Validation Specificity': 0.35121951219512193, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5771961976886786, 'Hydroxylation-P AUC PR': 0.2683720246328885, 'Hydroxylation-P MCC': 0.01602740563414066, 'Hydroxylation-P F1': 0.20371379221948596, 'Validation Loss (Hydroxylation-P)': 0.5311411380767822, 'Validation Loss (total)': 1.0887075265248616, 'TimeToTrain': 0.21039997736612956}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004110591697967731,
 'learning_rate_Hydroxylation-K': 0.0027248251049460884,
 'learning_rate_Hydroxylation-P': 0.005857826019516871,
 'log_base': 2.4977068825962,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3405749602,
 'sample_weights': [164.08124783856405, 20.467539499435006],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.8322957903377075,
 'weight_decay_Hydroxylation-K': 8.974633413744364,
 'weight_decay_Hydroxylation-P': 0.45905092127697356}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.564
[2,     1] loss: 1290.588
[3,     1] loss: 1287.106
[4,     1] loss: 1293.071
[5,     1] loss: 1288.216
[6,     1] loss: 1284.042
[7,     1] loss: 1275.123
[8,     1] loss: 1258.259
[9,     1] loss: 1235.937
[10,     1] loss: 1193.917
[11,     1] loss: 1182.677
[12,     1] loss: 1142.332
[13,     1] loss: 1107.653
[14,     1] loss: 1115.659
[15,     1] loss: 1049.978
[16,     1] loss: 1079.765
[17,     1] loss: 1008.531
[18,     1] loss: 1078.826
[19,     1] loss: 1038.629
[20,     1] loss: 1007.763
[21,     1] loss: 977.922
[22,     1] loss: 1015.327
[23,     1] loss: 991.640
[24,     1] loss: 994.573
[25,     1] loss: 965.463
[26,     1] loss: 919.849
[27,     1] loss: 988.816
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066435212854646965,
 'learning_rate_Hydroxylation-K': 0.0025423959505656182,
 'learning_rate_Hydroxylation-P': 0.005095183376656535,
 'log_base': 1.28818605472608,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1279104849,
 'sample_weights': [1.8237844362136963, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.398299658012503,
 'weight_decay_Hydroxylation-K': 4.085428518501122,
 'weight_decay_Hydroxylation-P': 2.0431821496930307}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2309.813
[2,     1] loss: 2308.991
[3,     1] loss: 2284.946
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027953477550650588,
 'learning_rate_Hydroxylation-K': 0.008567193322084827,
 'learning_rate_Hydroxylation-P': 0.006156133477470627,
 'log_base': 2.635072308486848,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3763519829,
 'sample_weights': [6.59246426473659, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.002138580634067,
 'weight_decay_Hydroxylation-K': 2.672346955345211,
 'weight_decay_Hydroxylation-P': 3.522941052181727}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.548
[2,     1] loss: 1270.488
[3,     1] loss: 1268.979
[4,     1] loss: 1269.097
[5,     1] loss: 1265.454
[6,     1] loss: 1261.129
[7,     1] loss: 1251.372
[8,     1] loss: 1232.088
[9,     1] loss: 1208.205
[10,     1] loss: 1178.370
[11,     1] loss: 1126.476
[12,     1] loss: 1115.724
[13,     1] loss: 1135.195
[14,     1] loss: 1086.670
[15,     1] loss: 1069.147
[16,     1] loss: 1075.702
[17,     1] loss: 1018.356
[18,     1] loss: 1059.385
[19,     1] loss: 1023.876
[20,     1] loss: 1010.911
[21,     1] loss: 1002.672
[22,     1] loss: 1016.146
[23,     1] loss: 954.488
[24,     1] loss: 956.525
[25,     1] loss: 944.646
[26,     1] loss: 962.620
[27,     1] loss: 945.876
[28,     1] loss: 948.258
[29,     1] loss: 941.537
[30,     1] loss: 929.632
[31,     1] loss: 862.394
[32,     1] loss: 925.221
[33,     1] loss: 886.787
[34,     1] loss: 900.400
[35,     1] loss: 973.475
[36,     1] loss: 901.963
[37,     1] loss: 883.434
[38,     1] loss: 884.908
[39,     1] loss: 867.837
[40,     1] loss: 853.092
[41,     1] loss: 814.375
[42,     1] loss: 856.778
[43,     1] loss: 821.588
[44,     1] loss: 829.919
[45,     1] loss: 841.344
[46,     1] loss: 775.544
[47,     1] loss: 716.863
[48,     1] loss: 767.134
[49,     1] loss: 791.234
[50,     1] loss: 749.601
[51,     1] loss: 745.984
[52,     1] loss: 779.265
Early stopping applied (best metric=0.8304718732833862)
Finished Training
Total time taken: 7.046006679534912
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.154
[2,     1] loss: 1269.660
[3,     1] loss: 1270.161
[4,     1] loss: 1266.992
[5,     1] loss: 1272.832
[6,     1] loss: 1266.545
[7,     1] loss: 1258.950
[8,     1] loss: 1241.992
[9,     1] loss: 1216.223
[10,     1] loss: 1184.498
[11,     1] loss: 1167.106
[12,     1] loss: 1126.838
[13,     1] loss: 1115.030
[14,     1] loss: 1092.682
[15,     1] loss: 1050.967
[16,     1] loss: 1108.504
[17,     1] loss: 1018.226
[18,     1] loss: 1008.670
[19,     1] loss: 1062.157
[20,     1] loss: 1018.897
[21,     1] loss: 1036.932
[22,     1] loss: 1020.570
[23,     1] loss: 1007.953
[24,     1] loss: 975.153
[25,     1] loss: 1017.862
[26,     1] loss: 979.785
[27,     1] loss: 1027.739
[28,     1] loss: 998.664
[29,     1] loss: 966.153
[30,     1] loss: 949.417
[31,     1] loss: 960.354
[32,     1] loss: 913.198
[33,     1] loss: 919.577
[34,     1] loss: 939.405
[35,     1] loss: 919.121
[36,     1] loss: 896.081
[37,     1] loss: 911.625
[38,     1] loss: 913.027
[39,     1] loss: 902.177
[40,     1] loss: 878.939
[41,     1] loss: 833.763
[42,     1] loss: 878.651
[43,     1] loss: 967.107
[44,     1] loss: 891.427
[45,     1] loss: 879.761
[46,     1] loss: 902.561
[47,     1] loss: 803.472
[48,     1] loss: 831.060
[49,     1] loss: 826.929
[50,     1] loss: 804.554
[51,     1] loss: 847.426
[52,     1] loss: 838.970
[53,     1] loss: 786.535
[54,     1] loss: 782.383
[55,     1] loss: 873.614
[56,     1] loss: 843.190
[57,     1] loss: 793.695
[58,     1] loss: 807.754
[59,     1] loss: 770.005
[60,     1] loss: 737.849
[61,     1] loss: 733.120
[62,     1] loss: 733.595
[63,     1] loss: 755.131
[64,     1] loss: 753.011
[65,     1] loss: 710.507
[66,     1] loss: 746.274
[67,     1] loss: 957.299
[68,     1] loss: 875.410
[69,     1] loss: 746.765
[70,     1] loss: 813.484
[71,     1] loss: 783.760
[72,     1] loss: 780.122
[73,     1] loss: 803.696
[74,     1] loss: 763.416
[75,     1] loss: 739.274
[76,     1] loss: 762.163
[77,     1] loss: 677.104
[78,     1] loss: 722.128
[79,     1] loss: 661.154
[80,     1] loss: 650.221
[81,     1] loss: 647.640
[82,     1] loss: 631.014
[83,     1] loss: 650.610
[84,     1] loss: 684.849
[85,     1] loss: 590.287
[86,     1] loss: 618.883
[87,     1] loss: 783.932
[88,     1] loss: 664.682
[89,     1] loss: 551.885
[90,     1] loss: 690.794
[91,     1] loss: 569.172
[92,     1] loss: 609.288
[93,     1] loss: 655.615
[94,     1] loss: 584.878
[95,     1] loss: 744.823
[96,     1] loss: 871.539
[97,     1] loss: 542.820
[98,     1] loss: 697.503
[99,     1] loss: 573.368
Early stopping applied (best metric=0.6991640329360962)
Finished Training
Total time taken: 16.468016862869263
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.573
[2,     1] loss: 1269.309
[3,     1] loss: 1269.571
[4,     1] loss: 1270.081
[5,     1] loss: 1265.508
[6,     1] loss: 1265.104
[7,     1] loss: 1255.157
[8,     1] loss: 1243.512
[9,     1] loss: 1221.553
[10,     1] loss: 1194.461
[11,     1] loss: 1149.991
[12,     1] loss: 1120.003
[13,     1] loss: 1104.008
[14,     1] loss: 1099.517
[15,     1] loss: 1046.459
[16,     1] loss: 1062.958
[17,     1] loss: 1089.357
[18,     1] loss: 1051.971
[19,     1] loss: 1066.693
[20,     1] loss: 1060.817
[21,     1] loss: 1022.703
[22,     1] loss: 1024.044
[23,     1] loss: 1011.594
[24,     1] loss: 996.230
[25,     1] loss: 1019.278
[26,     1] loss: 994.657
[27,     1] loss: 955.666
[28,     1] loss: 974.258
[29,     1] loss: 957.696
[30,     1] loss: 976.910
[31,     1] loss: 950.296
[32,     1] loss: 984.438
[33,     1] loss: 970.290
[34,     1] loss: 905.493
[35,     1] loss: 944.402
[36,     1] loss: 873.880
[37,     1] loss: 882.503
[38,     1] loss: 885.030
[39,     1] loss: 928.201
[40,     1] loss: 913.118
[41,     1] loss: 925.435
[42,     1] loss: 899.650
[43,     1] loss: 853.470
[44,     1] loss: 963.757
[45,     1] loss: 809.373
[46,     1] loss: 895.135
[47,     1] loss: 826.442
[48,     1] loss: 823.275
[49,     1] loss: 833.700
[50,     1] loss: 923.349
[51,     1] loss: 827.827
[52,     1] loss: 839.399
[53,     1] loss: 864.517
[54,     1] loss: 768.539
[55,     1] loss: 875.920
[56,     1] loss: 715.154
[57,     1] loss: 822.692
[58,     1] loss: 757.485
[59,     1] loss: 769.212
[60,     1] loss: 756.528
[61,     1] loss: 717.648
[62,     1] loss: 796.971
Early stopping applied (best metric=0.7462950944900513)
Finished Training
Total time taken: 10.399012804031372
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.651
[2,     1] loss: 1272.192
[3,     1] loss: 1271.280
[4,     1] loss: 1270.102
[5,     1] loss: 1270.139
[6,     1] loss: 1266.328
[7,     1] loss: 1258.711
[8,     1] loss: 1253.695
[9,     1] loss: 1236.458
[10,     1] loss: 1223.321
[11,     1] loss: 1199.988
[12,     1] loss: 1167.900
[13,     1] loss: 1149.091
[14,     1] loss: 1124.767
[15,     1] loss: 1114.147
[16,     1] loss: 1090.320
[17,     1] loss: 1055.002
[18,     1] loss: 1059.877
[19,     1] loss: 1096.121
[20,     1] loss: 1027.888
[21,     1] loss: 1104.911
[22,     1] loss: 1030.316
[23,     1] loss: 1050.574
[24,     1] loss: 1022.514
[25,     1] loss: 1012.914
[26,     1] loss: 1006.391
[27,     1] loss: 989.706
[28,     1] loss: 963.956
[29,     1] loss: 939.112
[30,     1] loss: 946.014
[31,     1] loss: 980.288
[32,     1] loss: 914.617
[33,     1] loss: 943.528
[34,     1] loss: 901.780
[35,     1] loss: 898.518
[36,     1] loss: 986.082
[37,     1] loss: 990.277
[38,     1] loss: 919.451
[39,     1] loss: 987.777
[40,     1] loss: 890.822
[41,     1] loss: 939.179
[42,     1] loss: 905.861
[43,     1] loss: 884.807
[44,     1] loss: 889.278
[45,     1] loss: 836.541
[46,     1] loss: 917.772
[47,     1] loss: 868.834
[48,     1] loss: 888.237
[49,     1] loss: 817.260
[50,     1] loss: 934.326
[51,     1] loss: 858.203
[52,     1] loss: 816.816
[53,     1] loss: 814.566
[54,     1] loss: 781.148
[55,     1] loss: 775.834
[56,     1] loss: 733.005
[57,     1] loss: 812.650
[58,     1] loss: 823.857
[59,     1] loss: 738.285
[60,     1] loss: 770.373
[61,     1] loss: 886.831
[62,     1] loss: 753.845
[63,     1] loss: 942.698
[64,     1] loss: 774.764
[65,     1] loss: 865.850
[66,     1] loss: 769.503
[67,     1] loss: 826.084
[68,     1] loss: 833.331
[69,     1] loss: 750.942
[70,     1] loss: 835.755
Early stopping applied (best metric=0.7969580292701721)
Finished Training
Total time taken: 11.610013961791992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1275.233
[2,     1] loss: 1273.524
[3,     1] loss: 1270.564
[4,     1] loss: 1271.040
[5,     1] loss: 1267.626
[6,     1] loss: 1265.240
[7,     1] loss: 1251.198
[8,     1] loss: 1237.653
[9,     1] loss: 1205.886
[10,     1] loss: 1188.440
[11,     1] loss: 1168.966
[12,     1] loss: 1133.146
[13,     1] loss: 1094.265
[14,     1] loss: 1114.011
[15,     1] loss: 1075.057
[16,     1] loss: 1060.560
[17,     1] loss: 1066.314
[18,     1] loss: 1053.223
[19,     1] loss: 1019.401
[20,     1] loss: 1053.235
[21,     1] loss: 1011.138
[22,     1] loss: 988.161
[23,     1] loss: 977.330
[24,     1] loss: 1001.274
[25,     1] loss: 977.254
[26,     1] loss: 964.551
[27,     1] loss: 949.904
[28,     1] loss: 922.107
[29,     1] loss: 893.668
[30,     1] loss: 946.729
[31,     1] loss: 936.049
[32,     1] loss: 912.599
[33,     1] loss: 910.088
[34,     1] loss: 899.187
[35,     1] loss: 857.333
[36,     1] loss: 872.824
[37,     1] loss: 934.546
[38,     1] loss: 874.752
[39,     1] loss: 857.964
[40,     1] loss: 892.310
[41,     1] loss: 858.806
[42,     1] loss: 915.413
[43,     1] loss: 865.879
[44,     1] loss: 873.504
[45,     1] loss: 815.671
[46,     1] loss: 871.909
[47,     1] loss: 802.392
[48,     1] loss: 840.100
[49,     1] loss: 804.974
[50,     1] loss: 774.779
[51,     1] loss: 750.420
[52,     1] loss: 763.833
[53,     1] loss: 755.116
[54,     1] loss: 781.140
[55,     1] loss: 816.399
[56,     1] loss: 787.118
[57,     1] loss: 747.431
[58,     1] loss: 779.679
[59,     1] loss: 704.873
[60,     1] loss: 755.769
[61,     1] loss: 744.164
[62,     1] loss: 702.875
[63,     1] loss: 719.974
[64,     1] loss: 707.809
[65,     1] loss: 851.329
[66,     1] loss: 679.068
[67,     1] loss: 739.742
[68,     1] loss: 742.310
[69,     1] loss: 684.571
[70,     1] loss: 746.381
[71,     1] loss: 681.143
[72,     1] loss: 697.711
[73,     1] loss: 644.645
Early stopping applied (best metric=0.8420804142951965)
Finished Training
Total time taken: 9.816007137298584
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.762
[2,     1] loss: 1273.229
[3,     1] loss: 1269.914
[4,     1] loss: 1270.786
[5,     1] loss: 1270.508
[6,     1] loss: 1269.966
[7,     1] loss: 1268.559
[8,     1] loss: 1266.281
[9,     1] loss: 1264.145
[10,     1] loss: 1258.916
[11,     1] loss: 1249.473
[12,     1] loss: 1235.200
[13,     1] loss: 1221.409
[14,     1] loss: 1193.659
[15,     1] loss: 1167.329
[16,     1] loss: 1125.034
[17,     1] loss: 1122.011
[18,     1] loss: 1090.094
[19,     1] loss: 1103.488
[20,     1] loss: 1070.047
[21,     1] loss: 1051.349
[22,     1] loss: 1068.059
[23,     1] loss: 1034.211
[24,     1] loss: 1146.474
[25,     1] loss: 1031.337
[26,     1] loss: 1044.450
[27,     1] loss: 1021.433
[28,     1] loss: 1008.507
[29,     1] loss: 1034.675
[30,     1] loss: 1007.987
[31,     1] loss: 971.003
[32,     1] loss: 1003.526
[33,     1] loss: 959.350
[34,     1] loss: 1019.772
[35,     1] loss: 955.714
[36,     1] loss: 1007.130
[37,     1] loss: 1002.101
[38,     1] loss: 914.600
[39,     1] loss: 949.870
[40,     1] loss: 964.385
[41,     1] loss: 937.148
[42,     1] loss: 922.230
[43,     1] loss: 902.768
[44,     1] loss: 933.615
[45,     1] loss: 894.965
[46,     1] loss: 912.244
[47,     1] loss: 883.634
[48,     1] loss: 871.878
[49,     1] loss: 903.569
[50,     1] loss: 861.382
[51,     1] loss: 864.195
[52,     1] loss: 868.625
[53,     1] loss: 808.421
[54,     1] loss: 913.926
[55,     1] loss: 929.316
[56,     1] loss: 806.915
[57,     1] loss: 968.093
[58,     1] loss: 863.795
[59,     1] loss: 869.081
[60,     1] loss: 907.199
[61,     1] loss: 842.059
[62,     1] loss: 858.550
[63,     1] loss: 795.892
[64,     1] loss: 827.745
[65,     1] loss: 781.997
[66,     1] loss: 724.706
[67,     1] loss: 743.960
[68,     1] loss: 698.217
[69,     1] loss: 714.818
[70,     1] loss: 740.673
[71,     1] loss: 700.739
[72,     1] loss: 666.094
[73,     1] loss: 735.278
[74,     1] loss: 739.895
[75,     1] loss: 632.955
[76,     1] loss: 754.968
[77,     1] loss: 758.735
[78,     1] loss: 630.109
[79,     1] loss: 677.844
[80,     1] loss: 674.335
[81,     1] loss: 646.128
[82,     1] loss: 644.703
[83,     1] loss: 651.876
[84,     1] loss: 590.624
[85,     1] loss: 645.589
[86,     1] loss: 848.460
[87,     1] loss: 715.077
Early stopping applied (best metric=0.7464108467102051)
Finished Training
Total time taken: 14.413013219833374
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.123
[2,     1] loss: 1276.684
[3,     1] loss: 1269.209
[4,     1] loss: 1270.319
[5,     1] loss: 1266.848
[6,     1] loss: 1268.318
[7,     1] loss: 1267.274
[8,     1] loss: 1262.268
[9,     1] loss: 1251.779
[10,     1] loss: 1242.551
[11,     1] loss: 1220.663
[12,     1] loss: 1188.146
[13,     1] loss: 1177.925
[14,     1] loss: 1137.298
[15,     1] loss: 1119.430
[16,     1] loss: 1092.918
[17,     1] loss: 1100.971
[18,     1] loss: 1058.526
[19,     1] loss: 1090.253
[20,     1] loss: 1003.310
[21,     1] loss: 1056.411
[22,     1] loss: 1013.693
[23,     1] loss: 1020.237
[24,     1] loss: 962.698
[25,     1] loss: 1027.791
[26,     1] loss: 975.892
[27,     1] loss: 981.465
[28,     1] loss: 955.336
[29,     1] loss: 977.502
[30,     1] loss: 950.243
[31,     1] loss: 960.318
[32,     1] loss: 946.629
[33,     1] loss: 932.014
[34,     1] loss: 965.272
[35,     1] loss: 900.093
[36,     1] loss: 934.075
[37,     1] loss: 883.464
[38,     1] loss: 902.828
[39,     1] loss: 871.626
[40,     1] loss: 850.800
[41,     1] loss: 877.538
[42,     1] loss: 885.829
[43,     1] loss: 852.509
[44,     1] loss: 902.983
[45,     1] loss: 833.453
[46,     1] loss: 806.348
[47,     1] loss: 796.980
[48,     1] loss: 792.748
[49,     1] loss: 716.280
[50,     1] loss: 792.489
[51,     1] loss: 804.070
[52,     1] loss: 870.754
[53,     1] loss: 889.399
[54,     1] loss: 776.291
[55,     1] loss: 750.374
[56,     1] loss: 776.657
[57,     1] loss: 789.012
[58,     1] loss: 766.005
[59,     1] loss: 743.811
[60,     1] loss: 803.651
[61,     1] loss: 742.479
[62,     1] loss: 750.986
[63,     1] loss: 830.064
[64,     1] loss: 715.756
[65,     1] loss: 739.248
[66,     1] loss: 778.397
[67,     1] loss: 689.861
[68,     1] loss: 742.064
[69,     1] loss: 789.076
[70,     1] loss: 681.407
[71,     1] loss: 697.575
[72,     1] loss: 754.718
[73,     1] loss: 665.791
[74,     1] loss: 694.684
[75,     1] loss: 737.462
[76,     1] loss: 672.408
[77,     1] loss: 655.949
[78,     1] loss: 894.427
[79,     1] loss: 648.117
[80,     1] loss: 726.883
[81,     1] loss: 715.818
[82,     1] loss: 684.097
[83,     1] loss: 699.840
[84,     1] loss: 699.661
[85,     1] loss: 691.333
[86,     1] loss: 674.688
[87,     1] loss: 668.328
[88,     1] loss: 600.024
[89,     1] loss: 703.809
[90,     1] loss: 614.155
Early stopping applied (best metric=0.8104544878005981)
Finished Training
Total time taken: 12.979012966156006
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.647
[2,     1] loss: 1270.628
[3,     1] loss: 1272.958
[4,     1] loss: 1269.621
[5,     1] loss: 1271.399
[6,     1] loss: 1264.444
[7,     1] loss: 1259.152
[8,     1] loss: 1257.439
[9,     1] loss: 1246.297
[10,     1] loss: 1216.076
[11,     1] loss: 1196.924
[12,     1] loss: 1158.135
[13,     1] loss: 1126.654
[14,     1] loss: 1099.169
[15,     1] loss: 1081.787
[16,     1] loss: 1070.004
[17,     1] loss: 1041.527
[18,     1] loss: 1023.203
[19,     1] loss: 1041.203
[20,     1] loss: 1037.630
[21,     1] loss: 992.696
[22,     1] loss: 1035.545
[23,     1] loss: 1009.876
[24,     1] loss: 996.252
[25,     1] loss: 1021.853
[26,     1] loss: 986.634
[27,     1] loss: 944.839
[28,     1] loss: 983.108
[29,     1] loss: 930.989
[30,     1] loss: 964.139
[31,     1] loss: 912.758
[32,     1] loss: 943.508
[33,     1] loss: 899.554
[34,     1] loss: 918.615
[35,     1] loss: 871.370
[36,     1] loss: 863.225
[37,     1] loss: 859.783
[38,     1] loss: 845.008
[39,     1] loss: 810.425
[40,     1] loss: 790.363
[41,     1] loss: 826.506
[42,     1] loss: 762.629
[43,     1] loss: 789.282
[44,     1] loss: 773.322
[45,     1] loss: 780.193
[46,     1] loss: 784.212
[47,     1] loss: 852.152
[48,     1] loss: 826.239
[49,     1] loss: 716.965
[50,     1] loss: 743.018
[51,     1] loss: 684.211
[52,     1] loss: 693.596
[53,     1] loss: 708.148
[54,     1] loss: 693.880
Early stopping applied (best metric=0.9154284000396729)
Finished Training
Total time taken: 8.277007818222046
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.927
[2,     1] loss: 1278.769
[3,     1] loss: 1270.336
[4,     1] loss: 1272.413
[5,     1] loss: 1272.499
[6,     1] loss: 1267.318
[7,     1] loss: 1267.187
[8,     1] loss: 1266.249
[9,     1] loss: 1264.528
[10,     1] loss: 1260.026
[11,     1] loss: 1253.179
[12,     1] loss: 1240.175
[13,     1] loss: 1220.938
[14,     1] loss: 1197.149
[15,     1] loss: 1174.447
[16,     1] loss: 1127.719
[17,     1] loss: 1120.717
[18,     1] loss: 1090.773
[19,     1] loss: 1073.044
[20,     1] loss: 1066.991
[21,     1] loss: 1048.621
[22,     1] loss: 1053.998
[23,     1] loss: 1043.500
[24,     1] loss: 1025.819
[25,     1] loss: 993.107
[26,     1] loss: 936.378
[27,     1] loss: 979.238
[28,     1] loss: 1036.234
[29,     1] loss: 965.945
[30,     1] loss: 926.340
[31,     1] loss: 917.774
[32,     1] loss: 938.844
[33,     1] loss: 899.688
[34,     1] loss: 930.266
[35,     1] loss: 898.578
[36,     1] loss: 898.634
[37,     1] loss: 894.921
[38,     1] loss: 915.818
[39,     1] loss: 845.587
[40,     1] loss: 865.790
[41,     1] loss: 883.771
[42,     1] loss: 844.404
[43,     1] loss: 824.728
[44,     1] loss: 900.288
[45,     1] loss: 822.320
[46,     1] loss: 800.129
[47,     1] loss: 858.378
[48,     1] loss: 934.344
[49,     1] loss: 899.410
[50,     1] loss: 821.578
[51,     1] loss: 897.105
[52,     1] loss: 849.344
[53,     1] loss: 847.604
[54,     1] loss: 793.260
[55,     1] loss: 806.664
[56,     1] loss: 788.369
[57,     1] loss: 777.313
[58,     1] loss: 754.522
[59,     1] loss: 760.731
[60,     1] loss: 802.219
[61,     1] loss: 760.305
[62,     1] loss: 704.735
[63,     1] loss: 747.766
[64,     1] loss: 739.049
[65,     1] loss: 747.514
[66,     1] loss: 687.441
[67,     1] loss: 712.985
[68,     1] loss: 733.008
[69,     1] loss: 914.544
[70,     1] loss: 1197.554
[71,     1] loss: 707.370
[72,     1] loss: 920.163
[73,     1] loss: 783.285
[74,     1] loss: 803.940
[75,     1] loss: 834.521
[76,     1] loss: 859.471
[77,     1] loss: 785.110
Early stopping applied (best metric=0.7833881974220276)
Finished Training
Total time taken: 10.497009992599487
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1278.856
[2,     1] loss: 1270.962
[3,     1] loss: 1274.639
[4,     1] loss: 1275.508
[5,     1] loss: 1271.808
[6,     1] loss: 1271.856
[7,     1] loss: 1268.993
[8,     1] loss: 1266.702
[9,     1] loss: 1262.870
[10,     1] loss: 1250.567
[11,     1] loss: 1237.425
[12,     1] loss: 1208.688
[13,     1] loss: 1177.893
[14,     1] loss: 1148.557
[15,     1] loss: 1119.614
[16,     1] loss: 1100.797
[17,     1] loss: 1044.004
[18,     1] loss: 1056.604
[19,     1] loss: 1043.503
[20,     1] loss: 1023.927
[21,     1] loss: 1010.072
[22,     1] loss: 1010.357
[23,     1] loss: 1027.024
[24,     1] loss: 965.790
[25,     1] loss: 999.816
[26,     1] loss: 1017.169
[27,     1] loss: 997.040
[28,     1] loss: 938.088
[29,     1] loss: 966.358
[30,     1] loss: 966.206
[31,     1] loss: 932.333
[32,     1] loss: 931.809
[33,     1] loss: 956.830
[34,     1] loss: 912.130
[35,     1] loss: 939.062
[36,     1] loss: 878.212
[37,     1] loss: 954.246
[38,     1] loss: 906.435
[39,     1] loss: 857.224
[40,     1] loss: 846.702
[41,     1] loss: 859.421
[42,     1] loss: 880.818
[43,     1] loss: 867.022
[44,     1] loss: 801.518
[45,     1] loss: 910.878
[46,     1] loss: 952.088
[47,     1] loss: 852.146
[48,     1] loss: 895.016
[49,     1] loss: 813.380
[50,     1] loss: 898.447
[51,     1] loss: 808.006
[52,     1] loss: 827.272
[53,     1] loss: 836.497
[54,     1] loss: 804.654
[55,     1] loss: 851.356
[56,     1] loss: 759.610
[57,     1] loss: 749.800
[58,     1] loss: 747.842
[59,     1] loss: 742.467
[60,     1] loss: 747.136
[61,     1] loss: 718.580
[62,     1] loss: 710.545
[63,     1] loss: 686.819
[64,     1] loss: 733.823
[65,     1] loss: 755.692
[66,     1] loss: 691.596
[67,     1] loss: 686.793
[68,     1] loss: 701.865
Early stopping applied (best metric=0.8702230453491211)
Finished Training
Total time taken: 10.48701024055481
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.584
[2,     1] loss: 1276.640
[3,     1] loss: 1268.289
[4,     1] loss: 1272.841
[5,     1] loss: 1270.844
[6,     1] loss: 1268.647
[7,     1] loss: 1267.052
[8,     1] loss: 1269.887
[9,     1] loss: 1264.572
[10,     1] loss: 1260.263
[11,     1] loss: 1254.016
[12,     1] loss: 1238.239
[13,     1] loss: 1216.725
[14,     1] loss: 1196.442
[15,     1] loss: 1165.853
[16,     1] loss: 1149.786
[17,     1] loss: 1101.782
[18,     1] loss: 1090.013
[19,     1] loss: 1079.663
[20,     1] loss: 1026.262
[21,     1] loss: 1033.809
[22,     1] loss: 1024.878
[23,     1] loss: 1027.288
[24,     1] loss: 1034.131
[25,     1] loss: 998.152
[26,     1] loss: 976.702
[27,     1] loss: 999.778
[28,     1] loss: 977.720
[29,     1] loss: 957.849
[30,     1] loss: 940.847
[31,     1] loss: 940.149
[32,     1] loss: 947.010
[33,     1] loss: 901.301
[34,     1] loss: 902.204
[35,     1] loss: 873.089
[36,     1] loss: 930.179
[37,     1] loss: 849.246
[38,     1] loss: 893.710
[39,     1] loss: 917.788
[40,     1] loss: 860.161
[41,     1] loss: 916.223
[42,     1] loss: 835.422
[43,     1] loss: 935.476
[44,     1] loss: 841.689
[45,     1] loss: 926.833
[46,     1] loss: 798.032
[47,     1] loss: 895.849
[48,     1] loss: 836.462
[49,     1] loss: 855.577
[50,     1] loss: 852.480
[51,     1] loss: 834.827
[52,     1] loss: 829.869
[53,     1] loss: 820.143
[54,     1] loss: 821.310
[55,     1] loss: 798.817
[56,     1] loss: 814.278
[57,     1] loss: 770.686
[58,     1] loss: 787.989
[59,     1] loss: 737.090
[60,     1] loss: 757.241
[61,     1] loss: 802.317
[62,     1] loss: 724.465
[63,     1] loss: 684.707
[64,     1] loss: 812.121
[65,     1] loss: 719.789
[66,     1] loss: 721.930
[67,     1] loss: 749.285
[68,     1] loss: 705.307
[69,     1] loss: 664.665
[70,     1] loss: 759.663
[71,     1] loss: 685.571
[72,     1] loss: 705.065
[73,     1] loss: 631.670
[74,     1] loss: 620.043
[75,     1] loss: 625.906
[76,     1] loss: 616.817
[77,     1] loss: 754.193
[78,     1] loss: 934.177
[79,     1] loss: 1114.724
[80,     1] loss: 721.242
[81,     1] loss: 875.795
[82,     1] loss: 884.437
[83,     1] loss: 807.029
[84,     1] loss: 834.811
[85,     1] loss: 847.832
[86,     1] loss: 846.832
Early stopping applied (best metric=0.7957202196121216)
Finished Training
Total time taken: 13.676013469696045
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.427
[2,     1] loss: 1272.147
[3,     1] loss: 1275.591
[4,     1] loss: 1270.067
[5,     1] loss: 1267.740
[6,     1] loss: 1264.038
[7,     1] loss: 1259.545
[8,     1] loss: 1254.855
[9,     1] loss: 1229.671
[10,     1] loss: 1206.630
[11,     1] loss: 1182.252
[12,     1] loss: 1144.604
[13,     1] loss: 1119.866
[14,     1] loss: 1093.806
[15,     1] loss: 1077.110
[16,     1] loss: 1087.431
[17,     1] loss: 1079.955
[18,     1] loss: 1027.669
[19,     1] loss: 1012.742
[20,     1] loss: 1015.780
[21,     1] loss: 1005.860
[22,     1] loss: 1000.990
[23,     1] loss: 984.050
[24,     1] loss: 1013.168
[25,     1] loss: 981.111
[26,     1] loss: 969.939
[27,     1] loss: 1016.494
[28,     1] loss: 967.836
[29,     1] loss: 994.523
[30,     1] loss: 933.227
[31,     1] loss: 950.190
[32,     1] loss: 916.077
[33,     1] loss: 916.646
[34,     1] loss: 872.351
[35,     1] loss: 934.714
[36,     1] loss: 895.466
[37,     1] loss: 902.899
[38,     1] loss: 878.003
[39,     1] loss: 882.265
[40,     1] loss: 894.843
[41,     1] loss: 827.413
[42,     1] loss: 832.191
[43,     1] loss: 841.526
[44,     1] loss: 795.326
[45,     1] loss: 807.987
[46,     1] loss: 837.613
[47,     1] loss: 876.175
[48,     1] loss: 841.394
[49,     1] loss: 772.864
[50,     1] loss: 833.208
[51,     1] loss: 774.853
[52,     1] loss: 785.571
[53,     1] loss: 822.446
[54,     1] loss: 706.301
[55,     1] loss: 806.956
[56,     1] loss: 778.697
[57,     1] loss: 768.213
[58,     1] loss: 739.290
[59,     1] loss: 766.355
[60,     1] loss: 749.179
[61,     1] loss: 684.828
[62,     1] loss: 704.822
[63,     1] loss: 669.562
[64,     1] loss: 697.041
[65,     1] loss: 624.389
[66,     1] loss: 581.094
[67,     1] loss: 622.101
[68,     1] loss: 634.468
[69,     1] loss: 753.619
[70,     1] loss: 870.921
[71,     1] loss: 806.086
[72,     1] loss: 661.138
[73,     1] loss: 795.347
[74,     1] loss: 738.887
[75,     1] loss: 730.431
[76,     1] loss: 758.013
Early stopping applied (best metric=0.8191723227500916)
Finished Training
Total time taken: 10.24000883102417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.113
[2,     1] loss: 1272.304
[3,     1] loss: 1271.090
[4,     1] loss: 1270.328
[5,     1] loss: 1266.847
[6,     1] loss: 1265.607
[7,     1] loss: 1260.814
[8,     1] loss: 1251.920
[9,     1] loss: 1234.506
[10,     1] loss: 1199.366
[11,     1] loss: 1170.649
[12,     1] loss: 1125.443
[13,     1] loss: 1098.372
[14,     1] loss: 1074.390
[15,     1] loss: 1072.621
[16,     1] loss: 1103.438
[17,     1] loss: 1059.573
[18,     1] loss: 1084.740
[19,     1] loss: 1059.250
[20,     1] loss: 1061.090
[21,     1] loss: 1034.498
[22,     1] loss: 1022.630
[23,     1] loss: 1041.107
[24,     1] loss: 1028.325
[25,     1] loss: 1025.546
[26,     1] loss: 1030.784
[27,     1] loss: 977.053
[28,     1] loss: 979.603
[29,     1] loss: 992.172
[30,     1] loss: 943.603
[31,     1] loss: 912.055
[32,     1] loss: 946.325
[33,     1] loss: 957.185
[34,     1] loss: 939.632
[35,     1] loss: 978.311
[36,     1] loss: 909.321
[37,     1] loss: 916.513
[38,     1] loss: 880.910
[39,     1] loss: 885.279
[40,     1] loss: 886.261
[41,     1] loss: 866.352
[42,     1] loss: 845.761
[43,     1] loss: 820.929
[44,     1] loss: 877.729
[45,     1] loss: 812.641
[46,     1] loss: 841.971
[47,     1] loss: 780.843
[48,     1] loss: 778.080
[49,     1] loss: 801.800
[50,     1] loss: 768.004
[51,     1] loss: 749.884
[52,     1] loss: 755.071
[53,     1] loss: 763.789
[54,     1] loss: 846.107
[55,     1] loss: 938.037
[56,     1] loss: 747.505
[57,     1] loss: 898.635
[58,     1] loss: 758.794
[59,     1] loss: 829.617
[60,     1] loss: 763.244
[61,     1] loss: 802.720
[62,     1] loss: 791.459
[63,     1] loss: 758.002
[64,     1] loss: 727.688
[65,     1] loss: 708.699
[66,     1] loss: 728.536
[67,     1] loss: 694.448
[68,     1] loss: 670.449
[69,     1] loss: 762.409
[70,     1] loss: 694.182
[71,     1] loss: 718.516
[72,     1] loss: 688.050
[73,     1] loss: 674.999
[74,     1] loss: 703.844
[75,     1] loss: 630.237
[76,     1] loss: 618.841
[77,     1] loss: 782.748
[78,     1] loss: 591.859
[79,     1] loss: 658.858
Early stopping applied (best metric=0.9226852655410767)
Finished Training
Total time taken: 13.182012796401978
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.536
[2,     1] loss: 1270.563
[3,     1] loss: 1270.416
[4,     1] loss: 1269.791
[5,     1] loss: 1270.427
[6,     1] loss: 1263.192
[7,     1] loss: 1259.573
[8,     1] loss: 1253.235
[9,     1] loss: 1233.732
[10,     1] loss: 1204.394
[11,     1] loss: 1176.549
[12,     1] loss: 1123.968
[13,     1] loss: 1089.442
[14,     1] loss: 1080.794
[15,     1] loss: 1084.708
[16,     1] loss: 1015.036
[17,     1] loss: 1033.229
[18,     1] loss: 1048.346
[19,     1] loss: 1030.543
[20,     1] loss: 997.310
[21,     1] loss: 1012.260
[22,     1] loss: 957.732
[23,     1] loss: 959.794
[24,     1] loss: 1016.058
[25,     1] loss: 957.301
[26,     1] loss: 955.170
[27,     1] loss: 1014.492
[28,     1] loss: 946.591
[29,     1] loss: 984.732
[30,     1] loss: 957.612
[31,     1] loss: 943.822
[32,     1] loss: 942.544
[33,     1] loss: 923.992
[34,     1] loss: 915.064
[35,     1] loss: 928.328
[36,     1] loss: 883.500
[37,     1] loss: 892.039
[38,     1] loss: 854.781
[39,     1] loss: 884.658
[40,     1] loss: 853.292
[41,     1] loss: 871.315
[42,     1] loss: 860.125
[43,     1] loss: 816.288
[44,     1] loss: 834.634
[45,     1] loss: 779.857
[46,     1] loss: 807.837
[47,     1] loss: 821.885
[48,     1] loss: 803.853
[49,     1] loss: 740.144
[50,     1] loss: 812.652
[51,     1] loss: 802.132
[52,     1] loss: 772.692
[53,     1] loss: 754.137
[54,     1] loss: 678.028
[55,     1] loss: 835.362
[56,     1] loss: 948.964
[57,     1] loss: 697.341
[58,     1] loss: 845.639
[59,     1] loss: 733.022
[60,     1] loss: 806.581
[61,     1] loss: 713.827
[62,     1] loss: 724.155
[63,     1] loss: 743.881
[64,     1] loss: 725.882
[65,     1] loss: 768.963
[66,     1] loss: 700.093
[67,     1] loss: 748.076
[68,     1] loss: 694.127
[69,     1] loss: 685.299
[70,     1] loss: 671.311
[71,     1] loss: 736.737
[72,     1] loss: 713.651
[73,     1] loss: 624.521
[74,     1] loss: 647.084
[75,     1] loss: 622.465
[76,     1] loss: 583.317
[77,     1] loss: 602.960
[78,     1] loss: 589.518
[79,     1] loss: 880.771
[80,     1] loss: 983.759
[81,     1] loss: 623.899
[82,     1] loss: 824.085
[83,     1] loss: 853.995
[84,     1] loss: 729.398
[85,     1] loss: 832.355
[86,     1] loss: 880.082
[87,     1] loss: 820.122
[88,     1] loss: 760.045
Early stopping applied (best metric=0.9530787467956543)
Finished Training
Total time taken: 12.630012035369873
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1275.255
[2,     1] loss: 1273.616
[3,     1] loss: 1272.324
[4,     1] loss: 1272.268
[5,     1] loss: 1269.452
[6,     1] loss: 1264.914
[7,     1] loss: 1264.555
[8,     1] loss: 1250.752
[9,     1] loss: 1232.712
[10,     1] loss: 1201.412
[11,     1] loss: 1173.866
[12,     1] loss: 1142.686
[13,     1] loss: 1114.770
[14,     1] loss: 1092.751
[15,     1] loss: 1052.089
[16,     1] loss: 1060.996
[17,     1] loss: 1066.501
[18,     1] loss: 1106.651
[19,     1] loss: 1047.560
[20,     1] loss: 1101.250
[21,     1] loss: 1037.373
[22,     1] loss: 1020.978
[23,     1] loss: 1008.859
[24,     1] loss: 1015.717
[25,     1] loss: 1047.427
[26,     1] loss: 1002.410
[27,     1] loss: 998.716
[28,     1] loss: 962.959
[29,     1] loss: 963.984
[30,     1] loss: 941.105
[31,     1] loss: 938.306
[32,     1] loss: 962.155
[33,     1] loss: 934.414
[34,     1] loss: 904.828
[35,     1] loss: 913.999
[36,     1] loss: 916.821
[37,     1] loss: 900.542
[38,     1] loss: 902.184
[39,     1] loss: 822.710
[40,     1] loss: 912.797
[41,     1] loss: 861.423
[42,     1] loss: 944.881
[43,     1] loss: 843.948
[44,     1] loss: 926.358
[45,     1] loss: 839.548
[46,     1] loss: 904.724
[47,     1] loss: 858.277
[48,     1] loss: 874.555
[49,     1] loss: 846.326
[50,     1] loss: 839.923
[51,     1] loss: 845.880
[52,     1] loss: 810.680
[53,     1] loss: 854.373
Early stopping applied (best metric=0.8756102323532104)
Finished Training
Total time taken: 8.851008415222168
{'Hydroxylation-K Validation Accuracy': 0.7532801418439716, 'Hydroxylation-K Validation Sensitivity': 0.6718518518518518, 'Hydroxylation-K Validation Specificity': 0.7736842105263158, 'Hydroxylation-K Validation Precision': 0.43830665836857785, 'Hydroxylation-K AUC ROC': 0.7956335282651072, 'Hydroxylation-K AUC PR': 0.605159802628496, 'Hydroxylation-K MCC': 0.3899707177720703, 'Hydroxylation-K F1': 0.5243201605386829, 'Validation Loss (Hydroxylation-K)': 0.4413361569245656, 'Hydroxylation-P Validation Accuracy': 0.7943129621169822, 'Hydroxylation-P Validation Sensitivity': 0.7631216931216931, 'Hydroxylation-P Validation Specificity': 0.8009975559878298, 'Hydroxylation-P Validation Precision': 0.4596722063800118, 'Hydroxylation-P AUC ROC': 0.8367174687887691, 'Hydroxylation-P AUC PR': 0.5568990401803511, 'Hydroxylation-P MCC': 0.47470695252749123, 'Hydroxylation-P F1': 0.5702587390742146, 'Validation Loss (Hydroxylation-P)': 0.3858065923055013, 'Validation Loss (total)': 0.8271427472432454, 'TimeToTrain': 11.371411148707072}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005654242763592599,
 'learning_rate_Hydroxylation-K': 0.007752411180060243,
 'learning_rate_Hydroxylation-P': 0.005646943529068872,
 'log_base': 2.8111380346943142,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2471819217,
 'sample_weights': [1.7242885955906015, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.533404145308781,
 'weight_decay_Hydroxylation-K': 3.5242425512316546,
 'weight_decay_Hydroxylation-P': 3.680013571811809}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.094
[2,     1] loss: 1250.737
[3,     1] loss: 1251.556
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0053283744292836515,
 'learning_rate_Hydroxylation-K': 0.0009240979754329119,
 'learning_rate_Hydroxylation-P': 0.007765577523709027,
 'log_base': 2.5015273174672163,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1116028223,
 'sample_weights': [1.615189894583649, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.1626894315704,
 'weight_decay_Hydroxylation-K': 7.171730975977793,
 'weight_decay_Hydroxylation-P': 0.059076651792713264}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.924
[2,     1] loss: 1294.083
[3,     1] loss: 1292.841
[4,     1] loss: 1291.548
[5,     1] loss: 1286.032
[6,     1] loss: 1289.904
[7,     1] loss: 1289.403
[8,     1] loss: 1282.253
[9,     1] loss: 1273.948
[10,     1] loss: 1283.678
[11,     1] loss: 1251.373
[12,     1] loss: 1224.590
[13,     1] loss: 1196.912
[14,     1] loss: 1139.525
[15,     1] loss: 1110.733
[16,     1] loss: 1124.103
[17,     1] loss: 1121.440
[18,     1] loss: 1163.388
[19,     1] loss: 1096.367
[20,     1] loss: 1046.928
[21,     1] loss: 1091.415
[22,     1] loss: 1038.985
[23,     1] loss: 1055.251
[24,     1] loss: 1057.898
[25,     1] loss: 1062.178
[26,     1] loss: 1058.799
[27,     1] loss: 1029.685
[28,     1] loss: 1013.109
[29,     1] loss: 1007.903
[30,     1] loss: 978.663
[31,     1] loss: 975.743
[32,     1] loss: 980.095
[33,     1] loss: 994.615
[34,     1] loss: 1042.992
[35,     1] loss: 1012.490
[36,     1] loss: 942.760
[37,     1] loss: 985.436
[38,     1] loss: 920.487
[39,     1] loss: 887.122
[40,     1] loss: 895.597
[41,     1] loss: 861.402
[42,     1] loss: 902.265
[43,     1] loss: 924.461
Early stopping applied (best metric=0.9633150100708008)
Finished Training
Total time taken: 7.148006916046143
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.827
[2,     1] loss: 1292.914
[3,     1] loss: 1291.673
[4,     1] loss: 1288.738
[5,     1] loss: 1290.455
[6,     1] loss: 1285.076
[7,     1] loss: 1287.187
[8,     1] loss: 1270.693
[9,     1] loss: 1258.519
[10,     1] loss: 1230.369
[11,     1] loss: 1186.110
[12,     1] loss: 1149.599
[13,     1] loss: 1118.914
[14,     1] loss: 1070.465
[15,     1] loss: 1038.602
[16,     1] loss: 1082.393
[17,     1] loss: 1078.502
[18,     1] loss: 1039.635
[19,     1] loss: 1081.430
[20,     1] loss: 1008.950
[21,     1] loss: 1016.880
[22,     1] loss: 1025.652
[23,     1] loss: 1038.091
[24,     1] loss: 1000.759
[25,     1] loss: 1043.442
[26,     1] loss: 1000.438
[27,     1] loss: 968.883
[28,     1] loss: 964.837
[29,     1] loss: 953.725
[30,     1] loss: 943.802
[31,     1] loss: 984.742
[32,     1] loss: 952.433
[33,     1] loss: 932.554
[34,     1] loss: 927.945
[35,     1] loss: 902.358
[36,     1] loss: 904.632
[37,     1] loss: 929.190
[38,     1] loss: 854.604
[39,     1] loss: 897.468
[40,     1] loss: 934.137
[41,     1] loss: 888.883
[42,     1] loss: 876.040
[43,     1] loss: 883.479
[44,     1] loss: 871.601
[45,     1] loss: 899.084
[46,     1] loss: 834.191
[47,     1] loss: 833.789
[48,     1] loss: 820.653
[49,     1] loss: 798.116
[50,     1] loss: 823.590
[51,     1] loss: 806.140
[52,     1] loss: 790.531
[53,     1] loss: 759.686
[54,     1] loss: 799.064
[55,     1] loss: 772.543
[56,     1] loss: 737.922
[57,     1] loss: 669.703
[58,     1] loss: 755.822
[59,     1] loss: 874.587
[60,     1] loss: 752.171
[61,     1] loss: 681.063
[62,     1] loss: 759.258
[63,     1] loss: 683.418
[64,     1] loss: 742.266
[65,     1] loss: 698.532
[66,     1] loss: 778.120
[67,     1] loss: 638.633
[68,     1] loss: 749.346
[69,     1] loss: 653.409
[70,     1] loss: 750.809
[71,     1] loss: 650.140
[72,     1] loss: 748.835
[73,     1] loss: 665.775
Early stopping applied (best metric=0.7911511659622192)
Finished Training
Total time taken: 12.14401364326477
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.369
[2,     1] loss: 1292.740
[3,     1] loss: 1291.472
[4,     1] loss: 1288.572
[5,     1] loss: 1286.579
[6,     1] loss: 1279.677
[7,     1] loss: 1269.691
[8,     1] loss: 1264.867
[9,     1] loss: 1239.858
[10,     1] loss: 1204.255
[11,     1] loss: 1155.934
[12,     1] loss: 1130.955
[13,     1] loss: 1089.099
[14,     1] loss: 1090.555
[15,     1] loss: 1117.793
[16,     1] loss: 1051.672
[17,     1] loss: 1028.754
[18,     1] loss: 1091.498
[19,     1] loss: 1033.435
[20,     1] loss: 1091.988
[21,     1] loss: 1041.417
[22,     1] loss: 1056.683
[23,     1] loss: 1064.485
[24,     1] loss: 1004.986
[25,     1] loss: 1018.613
[26,     1] loss: 1012.742
[27,     1] loss: 988.087
[28,     1] loss: 970.127
[29,     1] loss: 989.347
[30,     1] loss: 987.708
[31,     1] loss: 950.933
[32,     1] loss: 915.246
[33,     1] loss: 944.096
[34,     1] loss: 874.339
[35,     1] loss: 944.183
[36,     1] loss: 906.413
[37,     1] loss: 869.121
[38,     1] loss: 925.241
[39,     1] loss: 881.460
[40,     1] loss: 928.808
[41,     1] loss: 893.542
[42,     1] loss: 847.228
[43,     1] loss: 857.896
[44,     1] loss: 846.857
[45,     1] loss: 836.469
[46,     1] loss: 821.098
[47,     1] loss: 841.684
[48,     1] loss: 853.820
[49,     1] loss: 767.405
[50,     1] loss: 803.271
[51,     1] loss: 770.775
Early stopping applied (best metric=0.7972537875175476)
Finished Training
Total time taken: 6.924007177352905
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.919
[2,     1] loss: 1291.797
[3,     1] loss: 1290.883
[4,     1] loss: 1297.296
[5,     1] loss: 1273.089
[6,     1] loss: 1268.810
[7,     1] loss: 1232.097
[8,     1] loss: 1209.557
[9,     1] loss: 1180.044
[10,     1] loss: 1136.062
[11,     1] loss: 1116.812
[12,     1] loss: 1055.282
[13,     1] loss: 1049.531
[14,     1] loss: 1044.061
[15,     1] loss: 1083.021
[16,     1] loss: 1085.426
[17,     1] loss: 1035.125
[18,     1] loss: 1042.561
[19,     1] loss: 1021.988
[20,     1] loss: 1037.328
[21,     1] loss: 979.166
[22,     1] loss: 955.377
[23,     1] loss: 989.408
[24,     1] loss: 914.617
[25,     1] loss: 1005.451
[26,     1] loss: 903.984
[27,     1] loss: 1007.890
[28,     1] loss: 928.961
[29,     1] loss: 963.500
[30,     1] loss: 977.519
[31,     1] loss: 935.332
[32,     1] loss: 953.830
[33,     1] loss: 953.949
[34,     1] loss: 882.652
[35,     1] loss: 909.659
[36,     1] loss: 968.070
[37,     1] loss: 896.526
[38,     1] loss: 872.284
[39,     1] loss: 882.096
[40,     1] loss: 853.892
[41,     1] loss: 889.833
[42,     1] loss: 846.088
[43,     1] loss: 848.538
[44,     1] loss: 840.141
[45,     1] loss: 853.701
[46,     1] loss: 766.692
[47,     1] loss: 857.660
[48,     1] loss: 817.715
[49,     1] loss: 799.032
Early stopping applied (best metric=0.9164962768554688)
Finished Training
Total time taken: 8.209009885787964
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1294.131
[2,     1] loss: 1293.564
[3,     1] loss: 1299.198
[4,     1] loss: 1291.556
[5,     1] loss: 1293.244
[6,     1] loss: 1292.651
[7,     1] loss: 1291.080
[8,     1] loss: 1283.710
[9,     1] loss: 1278.050
[10,     1] loss: 1266.447
[11,     1] loss: 1246.277
[12,     1] loss: 1230.447
[13,     1] loss: 1185.354
[14,     1] loss: 1145.978
[15,     1] loss: 1148.676
[16,     1] loss: 1113.098
[17,     1] loss: 1174.901
[18,     1] loss: 1123.815
[19,     1] loss: 1101.695
[20,     1] loss: 1068.501
[21,     1] loss: 1077.740
[22,     1] loss: 1118.222
[23,     1] loss: 1088.792
[24,     1] loss: 1086.272
[25,     1] loss: 1073.610
[26,     1] loss: 1055.900
[27,     1] loss: 1028.215
[28,     1] loss: 1065.834
[29,     1] loss: 1009.954
[30,     1] loss: 997.025
[31,     1] loss: 1031.523
[32,     1] loss: 1014.035
[33,     1] loss: 1032.165
[34,     1] loss: 973.662
[35,     1] loss: 999.910
[36,     1] loss: 998.361
[37,     1] loss: 981.599
[38,     1] loss: 1006.753
[39,     1] loss: 956.585
[40,     1] loss: 967.503
[41,     1] loss: 923.832
[42,     1] loss: 945.278
[43,     1] loss: 914.076
[44,     1] loss: 908.252
[45,     1] loss: 896.007
[46,     1] loss: 883.320
[47,     1] loss: 936.424
[48,     1] loss: 876.499
[49,     1] loss: 915.475
[50,     1] loss: 885.945
[51,     1] loss: 919.352
[52,     1] loss: 875.924
[53,     1] loss: 842.068
[54,     1] loss: 871.446
[55,     1] loss: 892.207
[56,     1] loss: 835.310
[57,     1] loss: 892.739
[58,     1] loss: 861.647
[59,     1] loss: 872.949
[60,     1] loss: 843.642
[61,     1] loss: 803.274
[62,     1] loss: 789.851
[63,     1] loss: 789.340
[64,     1] loss: 783.325
[65,     1] loss: 737.419
[66,     1] loss: 706.283
[67,     1] loss: 716.755
[68,     1] loss: 739.967
[69,     1] loss: 734.218
[70,     1] loss: 759.900
[71,     1] loss: 654.866
[72,     1] loss: 637.751
[73,     1] loss: 731.310
[74,     1] loss: 647.988
[75,     1] loss: 653.677
[76,     1] loss: 731.794
[77,     1] loss: 853.359
[78,     1] loss: 693.435
[79,     1] loss: 690.782
[80,     1] loss: 621.887
[81,     1] loss: 645.875
[82,     1] loss: 601.161
[83,     1] loss: 587.979
[84,     1] loss: 618.777
[85,     1] loss: 590.938
[86,     1] loss: 586.472
[87,     1] loss: 658.299
[88,     1] loss: 492.846
[89,     1] loss: 629.664
[90,     1] loss: 591.960
[91,     1] loss: 571.067
[92,     1] loss: 588.870
[93,     1] loss: 496.633
[94,     1] loss: 590.216
[95,     1] loss: 512.904
[96,     1] loss: 585.224
[97,     1] loss: 489.206
[98,     1] loss: 524.055
[99,     1] loss: 503.874
[100,     1] loss: 471.129
Early stopping applied (best metric=0.6448050141334534)
Finished Training
Total time taken: 13.526012182235718
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.507
[2,     1] loss: 1291.677
[3,     1] loss: 1296.896
[4,     1] loss: 1294.292
[5,     1] loss: 1292.028
[6,     1] loss: 1287.194
[7,     1] loss: 1289.179
[8,     1] loss: 1281.491
[9,     1] loss: 1284.056
[10,     1] loss: 1267.112
[11,     1] loss: 1243.729
[12,     1] loss: 1221.850
[13,     1] loss: 1191.765
[14,     1] loss: 1182.028
[15,     1] loss: 1138.583
[16,     1] loss: 1092.138
[17,     1] loss: 1085.996
[18,     1] loss: 1076.112
[19,     1] loss: 1150.412
[20,     1] loss: 1071.960
[21,     1] loss: 1036.978
[22,     1] loss: 1068.186
[23,     1] loss: 1054.438
[24,     1] loss: 1042.783
[25,     1] loss: 999.329
[26,     1] loss: 1050.099
[27,     1] loss: 1045.355
[28,     1] loss: 987.653
[29,     1] loss: 956.604
[30,     1] loss: 1036.740
[31,     1] loss: 1030.799
[32,     1] loss: 1025.016
[33,     1] loss: 983.959
[34,     1] loss: 969.958
[35,     1] loss: 1005.310
[36,     1] loss: 939.991
[37,     1] loss: 938.843
[38,     1] loss: 887.326
[39,     1] loss: 942.211
[40,     1] loss: 920.528
[41,     1] loss: 932.018
[42,     1] loss: 929.406
[43,     1] loss: 918.454
[44,     1] loss: 939.145
[45,     1] loss: 862.495
[46,     1] loss: 861.869
[47,     1] loss: 857.616
[48,     1] loss: 892.010
[49,     1] loss: 876.219
[50,     1] loss: 832.055
[51,     1] loss: 839.055
[52,     1] loss: 917.847
[53,     1] loss: 809.187
[54,     1] loss: 793.530
[55,     1] loss: 801.412
[56,     1] loss: 756.856
[57,     1] loss: 774.365
[58,     1] loss: 744.563
[59,     1] loss: 710.305
[60,     1] loss: 737.883
[61,     1] loss: 782.151
[62,     1] loss: 708.966
[63,     1] loss: 731.327
[64,     1] loss: 786.216
[65,     1] loss: 654.384
[66,     1] loss: 747.756
[67,     1] loss: 708.595
[68,     1] loss: 662.790
[69,     1] loss: 670.113
[70,     1] loss: 654.871
[71,     1] loss: 665.282
[72,     1] loss: 593.097
[73,     1] loss: 680.168
[74,     1] loss: 639.753
[75,     1] loss: 620.398
[76,     1] loss: 838.279
[77,     1] loss: 690.298
[78,     1] loss: 658.752
[79,     1] loss: 586.649
[80,     1] loss: 719.525
[81,     1] loss: 599.735
Early stopping applied (best metric=0.7257283329963684)
Finished Training
Total time taken: 13.555014610290527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.324
[2,     1] loss: 1299.255
[3,     1] loss: 1295.310
[4,     1] loss: 1293.023
[5,     1] loss: 1290.643
[6,     1] loss: 1291.290
[7,     1] loss: 1289.194
[8,     1] loss: 1288.632
[9,     1] loss: 1283.858
[10,     1] loss: 1285.572
[11,     1] loss: 1273.372
[12,     1] loss: 1260.772
[13,     1] loss: 1248.279
[14,     1] loss: 1210.541
[15,     1] loss: 1182.047
[16,     1] loss: 1146.662
[17,     1] loss: 1101.793
[18,     1] loss: 1123.490
[19,     1] loss: 1090.006
[20,     1] loss: 1075.321
[21,     1] loss: 1064.579
[22,     1] loss: 1064.604
[23,     1] loss: 1033.851
[24,     1] loss: 1079.882
[25,     1] loss: 1032.043
[26,     1] loss: 1013.679
[27,     1] loss: 1022.052
[28,     1] loss: 968.290
[29,     1] loss: 983.079
[30,     1] loss: 991.662
[31,     1] loss: 944.209
[32,     1] loss: 945.743
[33,     1] loss: 953.039
[34,     1] loss: 996.945
[35,     1] loss: 982.044
[36,     1] loss: 960.282
[37,     1] loss: 938.096
[38,     1] loss: 918.091
[39,     1] loss: 909.748
[40,     1] loss: 911.930
[41,     1] loss: 875.957
[42,     1] loss: 920.661
[43,     1] loss: 910.495
[44,     1] loss: 884.765
[45,     1] loss: 892.116
[46,     1] loss: 859.469
[47,     1] loss: 900.915
[48,     1] loss: 867.074
[49,     1] loss: 874.346
[50,     1] loss: 838.614
[51,     1] loss: 800.649
[52,     1] loss: 839.815
[53,     1] loss: 813.710
[54,     1] loss: 859.340
[55,     1] loss: 800.750
[56,     1] loss: 808.316
[57,     1] loss: 836.459
[58,     1] loss: 958.486
[59,     1] loss: 760.349
[60,     1] loss: 793.453
[61,     1] loss: 765.266
[62,     1] loss: 752.858
[63,     1] loss: 721.269
[64,     1] loss: 763.353
[65,     1] loss: 732.227
[66,     1] loss: 699.665
[67,     1] loss: 686.442
[68,     1] loss: 707.291
Early stopping applied (best metric=0.7985890507698059)
Finished Training
Total time taken: 9.167009353637695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.909
[2,     1] loss: 1306.817
[3,     1] loss: 1291.143
[4,     1] loss: 1293.317
[5,     1] loss: 1289.094
[6,     1] loss: 1294.637
[7,     1] loss: 1288.448
[8,     1] loss: 1286.564
[9,     1] loss: 1285.379
[10,     1] loss: 1282.071
[11,     1] loss: 1274.515
[12,     1] loss: 1257.137
[13,     1] loss: 1241.790
[14,     1] loss: 1211.708
[15,     1] loss: 1186.291
[16,     1] loss: 1138.259
[17,     1] loss: 1097.797
[18,     1] loss: 1074.878
[19,     1] loss: 1053.565
[20,     1] loss: 1081.454
[21,     1] loss: 1086.822
[22,     1] loss: 1040.984
[23,     1] loss: 1073.583
[24,     1] loss: 1057.060
[25,     1] loss: 1032.527
[26,     1] loss: 1031.902
[27,     1] loss: 1020.096
[28,     1] loss: 1012.832
[29,     1] loss: 1013.125
[30,     1] loss: 974.904
[31,     1] loss: 982.539
[32,     1] loss: 969.470
[33,     1] loss: 981.937
[34,     1] loss: 973.947
[35,     1] loss: 961.117
[36,     1] loss: 966.930
[37,     1] loss: 909.760
[38,     1] loss: 944.401
[39,     1] loss: 886.518
[40,     1] loss: 924.484
[41,     1] loss: 914.408
[42,     1] loss: 851.282
[43,     1] loss: 901.459
[44,     1] loss: 841.175
[45,     1] loss: 882.250
[46,     1] loss: 840.513
[47,     1] loss: 834.563
[48,     1] loss: 833.312
[49,     1] loss: 842.291
[50,     1] loss: 817.303
[51,     1] loss: 845.139
Early stopping applied (best metric=0.9101802110671997)
Finished Training
Total time taken: 8.54800820350647
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.323
[2,     1] loss: 1292.562
[3,     1] loss: 1288.610
[4,     1] loss: 1289.981
[5,     1] loss: 1284.874
[6,     1] loss: 1283.532
[7,     1] loss: 1283.633
[8,     1] loss: 1279.856
[9,     1] loss: 1274.709
[10,     1] loss: 1241.342
[11,     1] loss: 1221.513
[12,     1] loss: 1186.007
[13,     1] loss: 1169.443
[14,     1] loss: 1124.241
[15,     1] loss: 1108.164
[16,     1] loss: 1057.194
[17,     1] loss: 1106.409
[18,     1] loss: 1080.203
[19,     1] loss: 1048.416
[20,     1] loss: 1015.740
[21,     1] loss: 1002.386
[22,     1] loss: 1018.726
[23,     1] loss: 977.108
[24,     1] loss: 1008.158
[25,     1] loss: 1001.374
[26,     1] loss: 993.896
[27,     1] loss: 1001.579
[28,     1] loss: 939.332
[29,     1] loss: 983.163
[30,     1] loss: 919.814
[31,     1] loss: 893.252
[32,     1] loss: 894.827
[33,     1] loss: 890.515
[34,     1] loss: 947.669
[35,     1] loss: 953.338
[36,     1] loss: 871.305
[37,     1] loss: 916.109
[38,     1] loss: 878.380
[39,     1] loss: 869.597
[40,     1] loss: 848.301
[41,     1] loss: 883.834
[42,     1] loss: 898.972
[43,     1] loss: 803.496
[44,     1] loss: 770.569
[45,     1] loss: 793.428
[46,     1] loss: 854.642
[47,     1] loss: 882.322
[48,     1] loss: 814.257
[49,     1] loss: 862.802
[50,     1] loss: 751.419
[51,     1] loss: 828.187
[52,     1] loss: 748.018
[53,     1] loss: 790.471
[54,     1] loss: 719.606
[55,     1] loss: 750.355
[56,     1] loss: 738.334
[57,     1] loss: 766.020
Early stopping applied (best metric=0.9423538446426392)
Finished Training
Total time taken: 9.529009580612183
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1294.990
[2,     1] loss: 1295.821
[3,     1] loss: 1293.821
[4,     1] loss: 1288.904
[5,     1] loss: 1289.592
[6,     1] loss: 1285.580
[7,     1] loss: 1285.778
[8,     1] loss: 1272.660
[9,     1] loss: 1255.921
[10,     1] loss: 1232.085
[11,     1] loss: 1205.769
[12,     1] loss: 1170.693
[13,     1] loss: 1141.536
[14,     1] loss: 1157.145
[15,     1] loss: 1142.106
[16,     1] loss: 1109.686
[17,     1] loss: 1105.018
[18,     1] loss: 1032.368
[19,     1] loss: 1071.050
[20,     1] loss: 1065.810
[21,     1] loss: 1072.128
[22,     1] loss: 1056.267
[23,     1] loss: 1056.443
[24,     1] loss: 1033.334
[25,     1] loss: 1018.969
[26,     1] loss: 997.862
[27,     1] loss: 1022.722
[28,     1] loss: 1012.592
[29,     1] loss: 1000.613
[30,     1] loss: 931.814
[31,     1] loss: 974.663
[32,     1] loss: 984.873
[33,     1] loss: 942.260
[34,     1] loss: 929.384
[35,     1] loss: 965.147
[36,     1] loss: 956.072
[37,     1] loss: 916.669
[38,     1] loss: 931.502
[39,     1] loss: 927.910
[40,     1] loss: 906.420
[41,     1] loss: 961.083
[42,     1] loss: 930.832
[43,     1] loss: 859.414
[44,     1] loss: 941.592
[45,     1] loss: 882.382
[46,     1] loss: 891.830
[47,     1] loss: 867.992
[48,     1] loss: 833.355
[49,     1] loss: 829.196
[50,     1] loss: 823.306
[51,     1] loss: 860.524
[52,     1] loss: 1043.097
[53,     1] loss: 817.809
[54,     1] loss: 852.109
[55,     1] loss: 803.097
[56,     1] loss: 786.943
[57,     1] loss: 833.475
[58,     1] loss: 795.376
[59,     1] loss: 771.362
[60,     1] loss: 806.528
[61,     1] loss: 755.502
[62,     1] loss: 671.706
[63,     1] loss: 720.719
[64,     1] loss: 794.328
[65,     1] loss: 707.235
[66,     1] loss: 696.906
[67,     1] loss: 645.379
[68,     1] loss: 634.646
[69,     1] loss: 680.951
[70,     1] loss: 726.363
[71,     1] loss: 693.184
[72,     1] loss: 602.828
[73,     1] loss: 626.361
[74,     1] loss: 681.255
Early stopping applied (best metric=0.7532822489738464)
Finished Training
Total time taken: 12.368012428283691
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.007
[2,     1] loss: 1294.341
[3,     1] loss: 1291.673
[4,     1] loss: 1296.876
[5,     1] loss: 1290.885
[6,     1] loss: 1293.032
[7,     1] loss: 1289.869
[8,     1] loss: 1283.947
[9,     1] loss: 1283.452
[10,     1] loss: 1282.586
[11,     1] loss: 1266.268
[12,     1] loss: 1242.754
[13,     1] loss: 1193.711
[14,     1] loss: 1173.152
[15,     1] loss: 1142.219
[16,     1] loss: 1122.182
[17,     1] loss: 1067.966
[18,     1] loss: 1043.188
[19,     1] loss: 1052.834
[20,     1] loss: 1049.451
[21,     1] loss: 1064.730
[22,     1] loss: 1017.952
[23,     1] loss: 1028.016
[24,     1] loss: 1020.726
[25,     1] loss: 1064.377
[26,     1] loss: 1031.050
[27,     1] loss: 987.565
[28,     1] loss: 1028.429
[29,     1] loss: 984.708
[30,     1] loss: 953.164
[31,     1] loss: 971.125
[32,     1] loss: 982.500
[33,     1] loss: 951.812
[34,     1] loss: 987.019
[35,     1] loss: 937.695
[36,     1] loss: 924.278
[37,     1] loss: 929.574
[38,     1] loss: 898.427
[39,     1] loss: 898.669
[40,     1] loss: 926.878
[41,     1] loss: 917.526
[42,     1] loss: 898.174
[43,     1] loss: 889.115
[44,     1] loss: 840.167
[45,     1] loss: 887.064
[46,     1] loss: 850.208
[47,     1] loss: 795.766
[48,     1] loss: 806.421
[49,     1] loss: 830.807
[50,     1] loss: 817.798
[51,     1] loss: 808.494
[52,     1] loss: 785.660
[53,     1] loss: 849.843
[54,     1] loss: 885.458
[55,     1] loss: 795.608
Early stopping applied (best metric=0.9672722816467285)
Finished Training
Total time taken: 9.147010087966919
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.184
[2,     1] loss: 1294.864
[3,     1] loss: 1296.302
[4,     1] loss: 1296.398
[5,     1] loss: 1293.837
[6,     1] loss: 1294.675
[7,     1] loss: 1290.477
[8,     1] loss: 1293.168
[9,     1] loss: 1292.959
[10,     1] loss: 1293.014
[11,     1] loss: 1292.776
[12,     1] loss: 1291.742
[13,     1] loss: 1288.382
[14,     1] loss: 1291.790
[15,     1] loss: 1289.995
[16,     1] loss: 1291.583
[17,     1] loss: 1289.064
[18,     1] loss: 1288.575
[19,     1] loss: 1287.344
[20,     1] loss: 1286.635
[21,     1] loss: 1283.428
[22,     1] loss: 1275.618
[23,     1] loss: 1270.245
[24,     1] loss: 1263.096
[25,     1] loss: 1238.589
[26,     1] loss: 1221.949
[27,     1] loss: 1191.967
[28,     1] loss: 1164.507
[29,     1] loss: 1154.587
[30,     1] loss: 1131.032
[31,     1] loss: 1099.027
[32,     1] loss: 1039.184
[33,     1] loss: 1109.570
[34,     1] loss: 1057.083
[35,     1] loss: 1038.358
[36,     1] loss: 1065.764
[37,     1] loss: 1010.192
[38,     1] loss: 1031.542
[39,     1] loss: 1032.331
[40,     1] loss: 999.687
[41,     1] loss: 981.157
[42,     1] loss: 970.819
[43,     1] loss: 980.643
[44,     1] loss: 1003.999
[45,     1] loss: 977.527
[46,     1] loss: 1004.274
[47,     1] loss: 958.351
[48,     1] loss: 983.476
[49,     1] loss: 921.420
[50,     1] loss: 1013.115
[51,     1] loss: 920.731
[52,     1] loss: 858.884
[53,     1] loss: 921.529
[54,     1] loss: 872.689
[55,     1] loss: 910.093
[56,     1] loss: 882.411
[57,     1] loss: 880.335
[58,     1] loss: 809.992
[59,     1] loss: 898.935
Early stopping applied (best metric=0.8522429466247559)
Finished Training
Total time taken: 8.109007596969604
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.873
[2,     1] loss: 1297.093
[3,     1] loss: 1293.463
[4,     1] loss: 1295.133
[5,     1] loss: 1290.949
[6,     1] loss: 1288.639
[7,     1] loss: 1288.863
[8,     1] loss: 1284.017
[9,     1] loss: 1277.325
[10,     1] loss: 1281.065
[11,     1] loss: 1254.191
[12,     1] loss: 1235.150
[13,     1] loss: 1212.953
[14,     1] loss: 1190.734
[15,     1] loss: 1155.920
[16,     1] loss: 1140.483
[17,     1] loss: 1128.910
[18,     1] loss: 1142.573
[19,     1] loss: 1075.324
[20,     1] loss: 1052.291
[21,     1] loss: 1058.253
[22,     1] loss: 1052.386
[23,     1] loss: 989.214
[24,     1] loss: 1111.401
[25,     1] loss: 1056.934
[26,     1] loss: 1066.392
[27,     1] loss: 966.372
[28,     1] loss: 964.201
[29,     1] loss: 973.165
[30,     1] loss: 982.239
[31,     1] loss: 979.842
[32,     1] loss: 989.214
[33,     1] loss: 967.015
[34,     1] loss: 973.905
[35,     1] loss: 966.121
[36,     1] loss: 976.261
[37,     1] loss: 932.025
[38,     1] loss: 938.394
[39,     1] loss: 892.744
[40,     1] loss: 894.453
[41,     1] loss: 890.938
[42,     1] loss: 936.759
[43,     1] loss: 899.433
[44,     1] loss: 901.520
[45,     1] loss: 841.537
[46,     1] loss: 861.503
[47,     1] loss: 858.769
[48,     1] loss: 783.132
[49,     1] loss: 868.136
[50,     1] loss: 876.621
[51,     1] loss: 775.025
[52,     1] loss: 912.134
[53,     1] loss: 842.497
[54,     1] loss: 857.700
[55,     1] loss: 863.987
[56,     1] loss: 794.873
[57,     1] loss: 828.174
[58,     1] loss: 769.894
[59,     1] loss: 726.080
[60,     1] loss: 852.233
[61,     1] loss: 717.119
[62,     1] loss: 846.337
[63,     1] loss: 709.358
[64,     1] loss: 820.092
[65,     1] loss: 708.097
Early stopping applied (best metric=0.9199633598327637)
Finished Training
Total time taken: 9.826010704040527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.595
[2,     1] loss: 1292.018
[3,     1] loss: 1291.298
[4,     1] loss: 1284.748
[5,     1] loss: 1287.699
[6,     1] loss: 1280.442
[7,     1] loss: 1257.909
[8,     1] loss: 1227.047
[9,     1] loss: 1195.390
[10,     1] loss: 1125.197
[11,     1] loss: 1102.300
[12,     1] loss: 1085.238
[13,     1] loss: 1046.141
[14,     1] loss: 988.685
[15,     1] loss: 1033.965
[16,     1] loss: 977.910
[17,     1] loss: 1014.584
[18,     1] loss: 951.573
[19,     1] loss: 951.524
[20,     1] loss: 956.701
[21,     1] loss: 946.319
[22,     1] loss: 960.758
[23,     1] loss: 973.103
[24,     1] loss: 915.250
[25,     1] loss: 921.392
[26,     1] loss: 922.819
[27,     1] loss: 885.095
[28,     1] loss: 907.741
[29,     1] loss: 951.350
[30,     1] loss: 864.429
Early stopping applied (best metric=1.0513713359832764)
Finished Training
Total time taken: 4.5840065479278564
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1308.705
[2,     1] loss: 1290.325
[3,     1] loss: 1297.445
[4,     1] loss: 1291.539
[5,     1] loss: 1294.474
[6,     1] loss: 1295.606
[7,     1] loss: 1292.140
[8,     1] loss: 1298.825
[9,     1] loss: 1288.355
[10,     1] loss: 1286.682
[11,     1] loss: 1293.077
[12,     1] loss: 1286.279
[13,     1] loss: 1281.219
[14,     1] loss: 1269.114
[15,     1] loss: 1251.200
[16,     1] loss: 1234.893
[17,     1] loss: 1201.529
[18,     1] loss: 1182.354
[19,     1] loss: 1133.473
[20,     1] loss: 1130.129
[21,     1] loss: 1103.477
[22,     1] loss: 1094.635
[23,     1] loss: 1052.126
[24,     1] loss: 1062.382
[25,     1] loss: 1037.494
[26,     1] loss: 1079.493
[27,     1] loss: 1027.848
[28,     1] loss: 1005.643
[29,     1] loss: 1045.241
[30,     1] loss: 1038.884
[31,     1] loss: 1024.019
[32,     1] loss: 1018.020
[33,     1] loss: 1030.172
[34,     1] loss: 1056.370
[35,     1] loss: 990.741
[36,     1] loss: 967.987
[37,     1] loss: 961.500
[38,     1] loss: 921.323
[39,     1] loss: 920.439
[40,     1] loss: 935.285
[41,     1] loss: 955.111
[42,     1] loss: 935.243
[43,     1] loss: 918.064
[44,     1] loss: 914.684
[45,     1] loss: 881.534
[46,     1] loss: 875.626
[47,     1] loss: 884.061
[48,     1] loss: 906.100
[49,     1] loss: 823.500
[50,     1] loss: 888.181
[51,     1] loss: 871.430
[52,     1] loss: 821.329
[53,     1] loss: 891.851
[54,     1] loss: 837.258
[55,     1] loss: 865.511
[56,     1] loss: 797.703
[57,     1] loss: 843.024
[58,     1] loss: 791.600
[59,     1] loss: 826.141
[60,     1] loss: 812.965
[61,     1] loss: 709.155
[62,     1] loss: 780.955
[63,     1] loss: 760.981
[64,     1] loss: 749.320
[65,     1] loss: 674.579
[66,     1] loss: 717.944
[67,     1] loss: 733.037
[68,     1] loss: 701.044
[69,     1] loss: 683.521
[70,     1] loss: 705.277
[71,     1] loss: 738.491
[72,     1] loss: 797.271
[73,     1] loss: 712.590
[74,     1] loss: 670.085
[75,     1] loss: 794.258
[76,     1] loss: 689.747
[77,     1] loss: 658.865
Early stopping applied (best metric=0.8085894584655762)
Finished Training
Total time taken: 11.902010202407837
{'Hydroxylation-K Validation Accuracy': 0.7729609929078014, 'Hydroxylation-K Validation Sensitivity': 0.6244444444444445, 'Hydroxylation-K Validation Specificity': 0.8105263157894737, 'Hydroxylation-K Validation Precision': 0.47824397824397824, 'Hydroxylation-K AUC ROC': 0.778635477582846, 'Hydroxylation-K AUC PR': 0.5640111729681189, 'Hydroxylation-K MCC': 0.3998521105962238, 'Hydroxylation-K F1': 0.5265909310623269, 'Validation Loss (Hydroxylation-K)': 0.4617241720358531, 'Hydroxylation-P Validation Accuracy': 0.7662210547687934, 'Hydroxylation-P Validation Sensitivity': 0.7476719576719577, 'Hydroxylation-P Validation Specificity': 0.7701356676143448, 'Hydroxylation-P Validation Precision': 0.42439745920811234, 'Hydroxylation-P AUC ROC': 0.82363779177523, 'Hydroxylation-P AUC PR': 0.5401958048762011, 'Hydroxylation-P MCC': 0.4304366449218164, 'Hydroxylation-P F1': 0.5364398402723813, 'Validation Loss (Hydroxylation-P)': 0.3944487810134888, 'Validation Loss (total)': 0.8561729550361633, 'TimeToTrain': 9.645743274688721}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008129968975970197,
 'learning_rate_Hydroxylation-K': 0.006938731309742693,
 'learning_rate_Hydroxylation-P': 0.009981095069682139,
 'log_base': 2.963505488207679,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3020391495,
 'sample_weights': [1.8220949448859713, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8167449563483036,
 'weight_decay_Hydroxylation-K': 7.502046064087222,
 'weight_decay_Hydroxylation-P': 5.0945243940962675}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.777
[2,     1] loss: 1232.447
[3,     1] loss: 1238.194
[4,     1] loss: 1231.332
[5,     1] loss: 1228.294
[6,     1] loss: 1225.185
[7,     1] loss: 1230.391
[8,     1] loss: 1224.291
[9,     1] loss: 1213.128
[10,     1] loss: 1199.692
[11,     1] loss: 1180.675
[12,     1] loss: 1149.533
[13,     1] loss: 1092.629
[14,     1] loss: 1101.058
[15,     1] loss: 1052.686
[16,     1] loss: 1046.747
[17,     1] loss: 1041.876
[18,     1] loss: 1023.560
[19,     1] loss: 967.555
[20,     1] loss: 1000.839
[21,     1] loss: 998.649
[22,     1] loss: 969.078
[23,     1] loss: 994.572
[24,     1] loss: 956.134
[25,     1] loss: 949.553
[26,     1] loss: 978.193
[27,     1] loss: 904.113
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005142095823371362,
 'learning_rate_Hydroxylation-K': 0.0012836946845244147,
 'learning_rate_Hydroxylation-P': 0.0009029956195326965,
 'log_base': 1.0733338553029297,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2315503555,
 'sample_weights': [1.5367128707262563, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.584960920027054,
 'weight_decay_Hydroxylation-K': 4.944554401828181,
 'weight_decay_Hydroxylation-P': 1.2895659174749212}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7651.246
[2,     1] loss: 7674.484
[3,     1] loss: 7662.489
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005241276976902342,
 'learning_rate_Hydroxylation-K': 0.005547503887632026,
 'learning_rate_Hydroxylation-P': 0.008746804235487559,
 'log_base': 2.4762241372886207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3008447548,
 'sample_weights': [23.58984870881314, 2.9488446855641857],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.625997157213328,
 'weight_decay_Hydroxylation-K': 2.626886716587053,
 'weight_decay_Hydroxylation-P': 2.8515895960097577}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.599
[2,     1] loss: 1295.288
[3,     1] loss: 1299.170
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029514065107084405,
 'learning_rate_Hydroxylation-K': 0.0013464004701375248,
 'learning_rate_Hydroxylation-P': 0.009417717079049561,
 'log_base': 2.5698142096072845,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1023296426,
 'sample_weights': [1.841159079747648, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.989355746125133,
 'weight_decay_Hydroxylation-K': 8.598222566325218,
 'weight_decay_Hydroxylation-P': 2.115074882569653}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.414
[2,     1] loss: 1279.646
[3,     1] loss: 1277.653
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00033061236373106093,
 'learning_rate_Hydroxylation-K': 0.0077602774820665,
 'learning_rate_Hydroxylation-P': 0.005685233661562959,
 'log_base': 2.5287260585281377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2567382351,
 'sample_weights': [1.7687896889833732, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.554218436534919,
 'weight_decay_Hydroxylation-K': 3.6416052321632595,
 'weight_decay_Hydroxylation-P': 4.2500411860324}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.225
[2,     1] loss: 1290.806
[3,     1] loss: 1290.570
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005536859064476357,
 'learning_rate_Hydroxylation-K': 0.000889824185410298,
 'learning_rate_Hydroxylation-P': 0.0030547902497032164,
 'log_base': 1.114909572270392,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 155971898,
 'sample_weights': [1.7995203188378426, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.457833227903218,
 'weight_decay_Hydroxylation-K': 5.512360316463229,
 'weight_decay_Hydroxylation-P': 2.86459048483798}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4991.617
[2,     1] loss: 4966.605
[3,     1] loss: 5033.057
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007758585696797793,
 'learning_rate_Hydroxylation-K': 0.004050584208981902,
 'learning_rate_Hydroxylation-P': 0.004901831958192708,
 'log_base': 2.8329319536229147,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1650299425,
 'sample_weights': [15.347912947720411, 1.9185630263698352],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.193503483373622,
 'weight_decay_Hydroxylation-K': 9.619472092291373,
 'weight_decay_Hydroxylation-P': 6.334302161100686}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.990
[2,     1] loss: 1260.687
[3,     1] loss: 1246.695
[4,     1] loss: 1249.498
[5,     1] loss: 1246.137
[6,     1] loss: 1246.441
[7,     1] loss: 1246.547
[8,     1] loss: 1245.465
[9,     1] loss: 1244.886
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048352967084824535,
 'learning_rate_Hydroxylation-K': 0.0019646448405616695,
 'learning_rate_Hydroxylation-P': 0.0022544885197364074,
 'log_base': 1.0294915144741994,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3309747340,
 'sample_weights': [1.603210972880273, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.4672957470520505,
 'weight_decay_Hydroxylation-K': 2.3753981767410144,
 'weight_decay_Hydroxylation-P': 1.6366293949981203}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18658.625
Exploding loss, terminate run (best metric=1.0965967178344727)
Finished Training
Total time taken: 0.21300101280212402
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18652.533
Exploding loss, terminate run (best metric=1.0935871601104736)
Finished Training
Total time taken: 0.23600053787231445
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18562.215
Exploding loss, terminate run (best metric=1.0918498039245605)
Finished Training
Total time taken: 0.21399998664855957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18696.086
Exploding loss, terminate run (best metric=1.0731467008590698)
Finished Training
Total time taken: 0.19900012016296387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18691.648
Exploding loss, terminate run (best metric=1.097611427307129)
Finished Training
Total time taken: 0.22300004959106445
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18521.879
Exploding loss, terminate run (best metric=1.1074328422546387)
Finished Training
Total time taken: 0.19199872016906738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18702.031
Exploding loss, terminate run (best metric=1.0922060012817383)
Finished Training
Total time taken: 0.23600172996520996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18631.146
Exploding loss, terminate run (best metric=1.0981037616729736)
Finished Training
Total time taken: 0.24100065231323242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18714.131
Exploding loss, terminate run (best metric=1.0953634977340698)
Finished Training
Total time taken: 0.19700002670288086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18647.879
Exploding loss, terminate run (best metric=1.0739953517913818)
Finished Training
Total time taken: 0.21799993515014648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18658.906
Exploding loss, terminate run (best metric=1.0937132835388184)
Finished Training
Total time taken: 0.19999909400939941
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18722.281
Exploding loss, terminate run (best metric=1.0919160842895508)
Finished Training
Total time taken: 0.2089996337890625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18722.723
Exploding loss, terminate run (best metric=1.0968255996704102)
Finished Training
Total time taken: 0.2050001621246338
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18747.434
Exploding loss, terminate run (best metric=1.0772773027420044)
Finished Training
Total time taken: 0.20000243186950684
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18651.824
Exploding loss, terminate run (best metric=1.0754566192626953)
Finished Training
Total time taken: 0.2129991054534912
{'Hydroxylation-K Validation Accuracy': 0.4841903073286052, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.46842105263157896, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5730409356725146, 'Hydroxylation-K AUC PR': 0.29738145493695256, 'Hydroxylation-K MCC': 0.004988477453464162, 'Hydroxylation-K F1': 0.18119148445827213, 'Validation Loss (Hydroxylation-K)': 0.5583334008852641, 'Hydroxylation-P Validation Accuracy': 0.4861920376292236, 'Hydroxylation-P Validation Sensitivity': 0.5295238095238095, 'Hydroxylation-P Validation Specificity': 0.4760162601626016, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5928115907692819, 'Hydroxylation-P AUC PR': 0.2807495852334906, 'Hydroxylation-P MCC': 0.006363946870969531, 'Hydroxylation-P F1': 0.16192725286384, 'Validation Loss (Hydroxylation-P)': 0.5320054173469544, 'Validation Loss (total)': 1.0903388102849325, 'TimeToTrain': 0.21306687990824383}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017294990024680585,
 'learning_rate_Hydroxylation-K': 0.009260120028920673,
 'learning_rate_Hydroxylation-P': 0.006648187497718824,
 'log_base': 2.7199043837862855,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2539061280,
 'sample_weights': [57.480861720307075, 7.170178330673541],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.348656923242702,
 'weight_decay_Hydroxylation-K': 2.560340099815647,
 'weight_decay_Hydroxylation-P': 4.818369736521089}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.035
[2,     1] loss: 1263.110
[3,     1] loss: 1257.886
[4,     1] loss: 1258.917
[5,     1] loss: 1258.533
[6,     1] loss: 1253.779
[7,     1] loss: 1253.627
[8,     1] loss: 1249.810
[9,     1] loss: 1231.165
[10,     1] loss: 1218.076
[11,     1] loss: 1194.808
[12,     1] loss: 1184.954
[13,     1] loss: 1170.695
[14,     1] loss: 1138.480
[15,     1] loss: 1126.483
[16,     1] loss: 1094.277
[17,     1] loss: 1088.035
[18,     1] loss: 1050.932
[19,     1] loss: 1052.438
[20,     1] loss: 1068.608
[21,     1] loss: 1038.367
[22,     1] loss: 1036.788
[23,     1] loss: 1051.919
[24,     1] loss: 1031.552
[25,     1] loss: 1067.419
[26,     1] loss: 1012.350
[27,     1] loss: 989.819
[28,     1] loss: 1000.772
[29,     1] loss: 1021.882
[30,     1] loss: 987.620
[31,     1] loss: 1003.521
[32,     1] loss: 973.128
[33,     1] loss: 957.249
[34,     1] loss: 946.813
[35,     1] loss: 927.578
[36,     1] loss: 976.240
[37,     1] loss: 943.554
[38,     1] loss: 979.635
[39,     1] loss: 977.728
[40,     1] loss: 919.085
[41,     1] loss: 951.799
[42,     1] loss: 913.766
[43,     1] loss: 939.034
[44,     1] loss: 918.432
[45,     1] loss: 892.290
[46,     1] loss: 909.314
[47,     1] loss: 824.294
[48,     1] loss: 916.981
[49,     1] loss: 892.194
[50,     1] loss: 882.307
[51,     1] loss: 836.382
[52,     1] loss: 838.933
[53,     1] loss: 878.629
[54,     1] loss: 835.038
[55,     1] loss: 859.272
[56,     1] loss: 824.014
[57,     1] loss: 793.476
[58,     1] loss: 848.258
[59,     1] loss: 887.520
[60,     1] loss: 826.671
[61,     1] loss: 792.022
[62,     1] loss: 773.265
[63,     1] loss: 846.991
[64,     1] loss: 795.181
[65,     1] loss: 762.683
[66,     1] loss: 751.820
[67,     1] loss: 748.436
[68,     1] loss: 767.056
[69,     1] loss: 766.014
[70,     1] loss: 721.949
[71,     1] loss: 774.595
[72,     1] loss: 784.455
[73,     1] loss: 745.674
[74,     1] loss: 698.093
[75,     1] loss: 743.000
[76,     1] loss: 675.167
[77,     1] loss: 740.193
[78,     1] loss: 719.018
[79,     1] loss: 679.844
[80,     1] loss: 704.491
[81,     1] loss: 678.201
[82,     1] loss: 687.532
[83,     1] loss: 704.865
[84,     1] loss: 685.479
[85,     1] loss: 647.365
[86,     1] loss: 731.989
[87,     1] loss: 600.060
[88,     1] loss: 676.735
[89,     1] loss: 674.568
[90,     1] loss: 687.780
[91,     1] loss: 626.062
[92,     1] loss: 596.334
[93,     1] loss: 573.349
[94,     1] loss: 605.114
[95,     1] loss: 582.567
[96,     1] loss: 573.001
[97,     1] loss: 593.716
[98,     1] loss: 583.399
[99,     1] loss: 701.460
[100,     1] loss: 701.386
[101,     1] loss: 633.912
[102,     1] loss: 554.746
[103,     1] loss: 679.256
[104,     1] loss: 643.399
[105,     1] loss: 575.720
[106,     1] loss: 658.258
[107,     1] loss: 551.927
[108,     1] loss: 643.762
[109,     1] loss: 564.323
[110,     1] loss: 572.988
[111,     1] loss: 575.160
[112,     1] loss: 577.854
[113,     1] loss: 617.109
[114,     1] loss: 505.155
[115,     1] loss: 597.899
[116,     1] loss: 531.884
[117,     1] loss: 579.336
[118,     1] loss: 567.068
[119,     1] loss: 579.687
[120,     1] loss: 609.727
[121,     1] loss: 594.788
[122,     1] loss: 530.925
[123,     1] loss: 569.988
[124,     1] loss: 518.389
[125,     1] loss: 552.156
[126,     1] loss: 509.023
[127,     1] loss: 584.647
[128,     1] loss: 554.244
[129,     1] loss: 489.601
[130,     1] loss: 512.115
[131,     1] loss: 519.077
[132,     1] loss: 500.548
[133,     1] loss: 601.374
[134,     1] loss: 552.287
[135,     1] loss: 509.629
[136,     1] loss: 549.409
[137,     1] loss: 507.985
[138,     1] loss: 509.123
[139,     1] loss: 567.519
[140,     1] loss: 465.395
[141,     1] loss: 533.290
[142,     1] loss: 519.503
[143,     1] loss: 454.558
[144,     1] loss: 508.915
[145,     1] loss: 464.992
[146,     1] loss: 421.440
[147,     1] loss: 426.426
[148,     1] loss: 468.686
[149,     1] loss: 618.801
[150,     1] loss: 555.805
[151,     1] loss: 494.537
[152,     1] loss: 587.776
[153,     1] loss: 434.575
[154,     1] loss: 589.871
[155,     1] loss: 476.635
[156,     1] loss: 522.889
[157,     1] loss: 473.272
[158,     1] loss: 513.906
[159,     1] loss: 461.462
[160,     1] loss: 479.432
[161,     1] loss: 447.668
[162,     1] loss: 468.474
[163,     1] loss: 436.719
[164,     1] loss: 480.016
[165,     1] loss: 434.053
[166,     1] loss: 379.708
[167,     1] loss: 446.022
[168,     1] loss: 403.837
[169,     1] loss: 448.617
[170,     1] loss: 402.064
[171,     1] loss: 393.082
[172,     1] loss: 438.082
[173,     1] loss: 407.894
Early stopping applied (best metric=0.6762953996658325)
Finished Training
Total time taken: 23.398022413253784
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.562
[2,     1] loss: 1257.710
[3,     1] loss: 1263.321
[4,     1] loss: 1256.778
[5,     1] loss: 1257.798
[6,     1] loss: 1256.908
[7,     1] loss: 1256.574
[8,     1] loss: 1253.826
[9,     1] loss: 1249.202
[10,     1] loss: 1248.137
[11,     1] loss: 1239.547
[12,     1] loss: 1226.561
[13,     1] loss: 1211.138
[14,     1] loss: 1187.047
[15,     1] loss: 1163.151
[16,     1] loss: 1135.127
[17,     1] loss: 1105.362
[18,     1] loss: 1109.441
[19,     1] loss: 1091.490
[20,     1] loss: 1095.904
[21,     1] loss: 1042.088
[22,     1] loss: 1090.543
[23,     1] loss: 1065.105
[24,     1] loss: 1022.887
[25,     1] loss: 1063.525
[26,     1] loss: 992.278
[27,     1] loss: 1017.700
[28,     1] loss: 1063.639
[29,     1] loss: 973.300
[30,     1] loss: 1029.305
[31,     1] loss: 1026.092
[32,     1] loss: 993.962
[33,     1] loss: 965.569
[34,     1] loss: 966.742
[35,     1] loss: 948.991
[36,     1] loss: 959.141
[37,     1] loss: 961.387
[38,     1] loss: 932.234
[39,     1] loss: 893.182
[40,     1] loss: 926.845
[41,     1] loss: 919.438
[42,     1] loss: 895.450
[43,     1] loss: 927.191
[44,     1] loss: 913.040
[45,     1] loss: 856.032
[46,     1] loss: 898.070
[47,     1] loss: 889.741
[48,     1] loss: 898.217
[49,     1] loss: 840.291
[50,     1] loss: 865.908
[51,     1] loss: 838.917
[52,     1] loss: 866.780
[53,     1] loss: 836.254
[54,     1] loss: 785.313
[55,     1] loss: 866.898
[56,     1] loss: 805.773
[57,     1] loss: 786.442
[58,     1] loss: 788.782
[59,     1] loss: 786.635
[60,     1] loss: 803.590
[61,     1] loss: 794.129
[62,     1] loss: 763.464
[63,     1] loss: 749.907
[64,     1] loss: 777.733
[65,     1] loss: 716.017
[66,     1] loss: 745.563
[67,     1] loss: 745.624
[68,     1] loss: 700.044
[69,     1] loss: 699.677
[70,     1] loss: 727.318
[71,     1] loss: 628.533
[72,     1] loss: 668.916
[73,     1] loss: 666.646
[74,     1] loss: 672.474
[75,     1] loss: 588.079
[76,     1] loss: 598.389
[77,     1] loss: 649.196
[78,     1] loss: 739.287
[79,     1] loss: 874.249
[80,     1] loss: 605.431
[81,     1] loss: 761.637
[82,     1] loss: 660.407
[83,     1] loss: 705.074
[84,     1] loss: 611.298
[85,     1] loss: 680.117
[86,     1] loss: 632.998
[87,     1] loss: 713.750
[88,     1] loss: 618.115
[89,     1] loss: 631.112
[90,     1] loss: 579.088
[91,     1] loss: 639.152
[92,     1] loss: 617.906
[93,     1] loss: 517.054
[94,     1] loss: 574.212
Early stopping applied (best metric=0.8197648525238037)
Finished Training
Total time taken: 15.51001524925232
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.069
[2,     1] loss: 1257.637
[3,     1] loss: 1258.921
[4,     1] loss: 1259.954
[5,     1] loss: 1261.395
[6,     1] loss: 1258.216
[7,     1] loss: 1253.850
[8,     1] loss: 1252.384
[9,     1] loss: 1250.771
[10,     1] loss: 1239.812
[11,     1] loss: 1233.328
[12,     1] loss: 1210.102
[13,     1] loss: 1192.249
[14,     1] loss: 1169.047
[15,     1] loss: 1150.452
[16,     1] loss: 1130.174
[17,     1] loss: 1110.105
[18,     1] loss: 1082.181
[19,     1] loss: 1110.587
[20,     1] loss: 1098.752
[21,     1] loss: 1047.034
[22,     1] loss: 1042.943
[23,     1] loss: 1044.823
[24,     1] loss: 1028.799
[25,     1] loss: 1025.353
[26,     1] loss: 1019.921
[27,     1] loss: 1011.849
[28,     1] loss: 1000.454
[29,     1] loss: 1057.784
[30,     1] loss: 1002.992
[31,     1] loss: 986.801
[32,     1] loss: 991.640
[33,     1] loss: 962.790
[34,     1] loss: 1007.378
[35,     1] loss: 937.057
[36,     1] loss: 964.590
[37,     1] loss: 990.148
[38,     1] loss: 933.366
[39,     1] loss: 936.383
[40,     1] loss: 924.869
[41,     1] loss: 956.528
[42,     1] loss: 920.579
[43,     1] loss: 912.716
[44,     1] loss: 904.552
[45,     1] loss: 952.906
[46,     1] loss: 877.513
[47,     1] loss: 881.492
[48,     1] loss: 861.921
[49,     1] loss: 854.000
[50,     1] loss: 829.534
[51,     1] loss: 849.268
[52,     1] loss: 830.661
[53,     1] loss: 896.742
[54,     1] loss: 806.953
[55,     1] loss: 807.454
[56,     1] loss: 816.649
[57,     1] loss: 774.912
[58,     1] loss: 781.336
[59,     1] loss: 767.669
[60,     1] loss: 809.908
[61,     1] loss: 787.298
[62,     1] loss: 767.049
[63,     1] loss: 807.647
[64,     1] loss: 734.828
[65,     1] loss: 807.681
[66,     1] loss: 767.824
[67,     1] loss: 782.321
[68,     1] loss: 750.558
[69,     1] loss: 723.267
[70,     1] loss: 711.072
[71,     1] loss: 726.237
[72,     1] loss: 710.426
[73,     1] loss: 654.506
[74,     1] loss: 695.497
[75,     1] loss: 748.829
[76,     1] loss: 683.942
[77,     1] loss: 668.014
[78,     1] loss: 707.566
[79,     1] loss: 664.243
[80,     1] loss: 632.887
[81,     1] loss: 674.589
[82,     1] loss: 652.013
[83,     1] loss: 702.089
[84,     1] loss: 626.290
[85,     1] loss: 631.379
[86,     1] loss: 682.541
[87,     1] loss: 597.183
[88,     1] loss: 640.689
Early stopping applied (best metric=0.7887538075447083)
Finished Training
Total time taken: 11.923011779785156
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.043
[2,     1] loss: 1268.258
[3,     1] loss: 1260.514
[4,     1] loss: 1262.422
[5,     1] loss: 1258.419
[6,     1] loss: 1252.211
[7,     1] loss: 1254.301
[8,     1] loss: 1247.574
[9,     1] loss: 1233.788
[10,     1] loss: 1216.651
[11,     1] loss: 1191.067
[12,     1] loss: 1174.989
[13,     1] loss: 1154.113
[14,     1] loss: 1127.337
[15,     1] loss: 1119.633
[16,     1] loss: 1088.073
[17,     1] loss: 1070.757
[18,     1] loss: 1039.646
[19,     1] loss: 1036.695
[20,     1] loss: 1024.332
[21,     1] loss: 995.679
[22,     1] loss: 1047.518
[23,     1] loss: 1021.854
[24,     1] loss: 996.546
[25,     1] loss: 1012.134
[26,     1] loss: 951.075
[27,     1] loss: 986.789
[28,     1] loss: 1008.384
[29,     1] loss: 986.046
[30,     1] loss: 958.400
[31,     1] loss: 961.369
[32,     1] loss: 982.431
[33,     1] loss: 970.229
[34,     1] loss: 931.621
[35,     1] loss: 929.360
[36,     1] loss: 925.628
[37,     1] loss: 910.360
[38,     1] loss: 933.009
[39,     1] loss: 861.333
[40,     1] loss: 915.094
[41,     1] loss: 859.681
[42,     1] loss: 857.713
[43,     1] loss: 900.449
[44,     1] loss: 896.652
[45,     1] loss: 911.606
[46,     1] loss: 918.332
[47,     1] loss: 883.552
[48,     1] loss: 871.021
[49,     1] loss: 835.517
[50,     1] loss: 863.734
[51,     1] loss: 839.214
[52,     1] loss: 818.681
[53,     1] loss: 845.004
[54,     1] loss: 807.123
[55,     1] loss: 801.920
[56,     1] loss: 757.454
[57,     1] loss: 772.921
Early stopping applied (best metric=0.8209376335144043)
Finished Training
Total time taken: 7.772006034851074
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1262.919
[2,     1] loss: 1262.573
[3,     1] loss: 1262.552
[4,     1] loss: 1260.839
[5,     1] loss: 1260.417
[6,     1] loss: 1256.469
[7,     1] loss: 1258.647
[8,     1] loss: 1258.964
[9,     1] loss: 1252.787
[10,     1] loss: 1254.044
[11,     1] loss: 1245.405
[12,     1] loss: 1236.949
[13,     1] loss: 1221.890
[14,     1] loss: 1206.619
[15,     1] loss: 1194.615
[16,     1] loss: 1169.487
[17,     1] loss: 1144.452
[18,     1] loss: 1148.968
[19,     1] loss: 1107.652
[20,     1] loss: 1109.161
[21,     1] loss: 1111.347
[22,     1] loss: 1105.067
[23,     1] loss: 1028.668
[24,     1] loss: 1034.393
[25,     1] loss: 1063.728
[26,     1] loss: 1029.912
[27,     1] loss: 1004.924
[28,     1] loss: 1035.075
[29,     1] loss: 997.242
[30,     1] loss: 1017.478
[31,     1] loss: 979.927
[32,     1] loss: 1025.364
[33,     1] loss: 980.616
[34,     1] loss: 977.347
[35,     1] loss: 955.337
[36,     1] loss: 980.123
[37,     1] loss: 939.886
[38,     1] loss: 967.041
[39,     1] loss: 948.923
[40,     1] loss: 974.494
[41,     1] loss: 933.371
[42,     1] loss: 909.757
[43,     1] loss: 962.862
[44,     1] loss: 915.244
[45,     1] loss: 931.152
[46,     1] loss: 904.572
[47,     1] loss: 966.584
[48,     1] loss: 923.567
[49,     1] loss: 919.500
[50,     1] loss: 880.322
[51,     1] loss: 852.623
[52,     1] loss: 893.627
[53,     1] loss: 860.223
[54,     1] loss: 912.144
[55,     1] loss: 826.649
[56,     1] loss: 799.760
[57,     1] loss: 773.401
[58,     1] loss: 828.361
[59,     1] loss: 794.612
[60,     1] loss: 829.188
[61,     1] loss: 816.162
[62,     1] loss: 811.466
[63,     1] loss: 774.948
[64,     1] loss: 797.467
[65,     1] loss: 752.241
[66,     1] loss: 735.592
[67,     1] loss: 754.316
[68,     1] loss: 777.391
[69,     1] loss: 754.916
[70,     1] loss: 720.787
[71,     1] loss: 745.169
[72,     1] loss: 721.528
[73,     1] loss: 726.903
[74,     1] loss: 815.009
[75,     1] loss: 742.258
[76,     1] loss: 739.135
[77,     1] loss: 767.370
[78,     1] loss: 677.178
[79,     1] loss: 716.686
[80,     1] loss: 667.256
[81,     1] loss: 702.950
[82,     1] loss: 639.705
[83,     1] loss: 737.715
[84,     1] loss: 736.231
[85,     1] loss: 722.572
[86,     1] loss: 710.456
[87,     1] loss: 634.098
[88,     1] loss: 674.959
[89,     1] loss: 626.362
[90,     1] loss: 674.604
[91,     1] loss: 657.204
[92,     1] loss: 628.835
[93,     1] loss: 648.980
[94,     1] loss: 611.123
[95,     1] loss: 599.862
[96,     1] loss: 599.177
[97,     1] loss: 590.089
[98,     1] loss: 570.417
[99,     1] loss: 580.614
[100,     1] loss: 600.428
[101,     1] loss: 579.977
[102,     1] loss: 610.583
[103,     1] loss: 593.032
[104,     1] loss: 639.749
Early stopping applied (best metric=0.6874616146087646)
Finished Training
Total time taken: 14.840015172958374
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.158
[2,     1] loss: 1259.066
[3,     1] loss: 1259.121
[4,     1] loss: 1260.144
[5,     1] loss: 1259.352
[6,     1] loss: 1255.744
[7,     1] loss: 1256.349
[8,     1] loss: 1250.736
[9,     1] loss: 1238.303
[10,     1] loss: 1224.869
[11,     1] loss: 1208.629
[12,     1] loss: 1183.445
[13,     1] loss: 1151.581
[14,     1] loss: 1140.385
[15,     1] loss: 1100.088
[16,     1] loss: 1084.170
[17,     1] loss: 1063.805
[18,     1] loss: 1058.102
[19,     1] loss: 1055.911
[20,     1] loss: 1014.680
[21,     1] loss: 983.583
[22,     1] loss: 1003.333
[23,     1] loss: 1017.423
[24,     1] loss: 1028.247
[25,     1] loss: 1048.839
[26,     1] loss: 993.737
[27,     1] loss: 966.865
[28,     1] loss: 920.307
[29,     1] loss: 931.440
[30,     1] loss: 964.196
[31,     1] loss: 939.440
[32,     1] loss: 953.779
[33,     1] loss: 947.130
[34,     1] loss: 928.541
[35,     1] loss: 886.595
[36,     1] loss: 882.866
[37,     1] loss: 891.892
[38,     1] loss: 873.646
[39,     1] loss: 892.893
[40,     1] loss: 845.381
[41,     1] loss: 930.612
[42,     1] loss: 816.388
[43,     1] loss: 887.122
[44,     1] loss: 886.177
[45,     1] loss: 808.842
[46,     1] loss: 857.848
[47,     1] loss: 868.342
[48,     1] loss: 882.734
[49,     1] loss: 851.086
[50,     1] loss: 843.329
[51,     1] loss: 832.208
[52,     1] loss: 867.269
[53,     1] loss: 764.076
[54,     1] loss: 828.959
[55,     1] loss: 769.996
[56,     1] loss: 789.210
[57,     1] loss: 751.567
[58,     1] loss: 786.879
[59,     1] loss: 745.947
[60,     1] loss: 835.843
[61,     1] loss: 685.684
Early stopping applied (best metric=0.8528487086296082)
Finished Training
Total time taken: 10.104009628295898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.308
[2,     1] loss: 1257.432
[3,     1] loss: 1260.842
[4,     1] loss: 1258.560
[5,     1] loss: 1256.624
[6,     1] loss: 1253.995
[7,     1] loss: 1251.701
[8,     1] loss: 1243.303
[9,     1] loss: 1233.136
[10,     1] loss: 1220.736
[11,     1] loss: 1192.747
[12,     1] loss: 1170.062
[13,     1] loss: 1151.594
[14,     1] loss: 1124.238
[15,     1] loss: 1093.222
[16,     1] loss: 1084.386
[17,     1] loss: 1067.797
[18,     1] loss: 1034.725
[19,     1] loss: 1056.036
[20,     1] loss: 1040.233
[21,     1] loss: 1048.250
[22,     1] loss: 1069.334
[23,     1] loss: 999.164
[24,     1] loss: 1054.342
[25,     1] loss: 1036.065
[26,     1] loss: 992.763
[27,     1] loss: 992.133
[28,     1] loss: 984.785
[29,     1] loss: 1002.560
[30,     1] loss: 1016.785
[31,     1] loss: 950.581
[32,     1] loss: 958.473
[33,     1] loss: 978.519
[34,     1] loss: 909.155
[35,     1] loss: 921.606
[36,     1] loss: 901.635
[37,     1] loss: 985.535
[38,     1] loss: 908.576
[39,     1] loss: 919.429
[40,     1] loss: 886.586
[41,     1] loss: 894.463
[42,     1] loss: 912.752
[43,     1] loss: 873.832
[44,     1] loss: 937.084
[45,     1] loss: 864.326
[46,     1] loss: 842.742
[47,     1] loss: 895.462
[48,     1] loss: 809.542
[49,     1] loss: 866.491
[50,     1] loss: 806.588
[51,     1] loss: 800.569
[52,     1] loss: 836.021
[53,     1] loss: 768.096
[54,     1] loss: 778.441
[55,     1] loss: 796.146
[56,     1] loss: 820.003
[57,     1] loss: 923.005
[58,     1] loss: 849.695
[59,     1] loss: 745.274
[60,     1] loss: 870.380
[61,     1] loss: 750.933
[62,     1] loss: 777.068
[63,     1] loss: 756.196
[64,     1] loss: 737.455
[65,     1] loss: 747.477
[66,     1] loss: 777.651
[67,     1] loss: 704.040
[68,     1] loss: 748.639
[69,     1] loss: 690.849
[70,     1] loss: 712.260
[71,     1] loss: 681.074
[72,     1] loss: 678.371
[73,     1] loss: 651.516
[74,     1] loss: 673.745
[75,     1] loss: 716.844
[76,     1] loss: 633.940
[77,     1] loss: 700.301
[78,     1] loss: 782.256
Early stopping applied (best metric=0.7521108388900757)
Finished Training
Total time taken: 12.95701289176941
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.991
[2,     1] loss: 1262.550
[3,     1] loss: 1257.724
[4,     1] loss: 1259.006
[5,     1] loss: 1258.184
[6,     1] loss: 1257.165
[7,     1] loss: 1257.683
[8,     1] loss: 1253.329
[9,     1] loss: 1247.022
[10,     1] loss: 1245.649
[11,     1] loss: 1231.928
[12,     1] loss: 1212.139
[13,     1] loss: 1199.297
[14,     1] loss: 1183.771
[15,     1] loss: 1163.713
[16,     1] loss: 1148.772
[17,     1] loss: 1120.270
[18,     1] loss: 1107.632
[19,     1] loss: 1107.059
[20,     1] loss: 1077.134
[21,     1] loss: 1037.432
[22,     1] loss: 1049.154
[23,     1] loss: 1028.921
[24,     1] loss: 1062.569
[25,     1] loss: 1018.795
[26,     1] loss: 1014.446
[27,     1] loss: 1009.186
[28,     1] loss: 989.699
[29,     1] loss: 1022.544
[30,     1] loss: 972.780
[31,     1] loss: 950.343
[32,     1] loss: 1001.166
[33,     1] loss: 991.707
[34,     1] loss: 952.873
[35,     1] loss: 949.034
[36,     1] loss: 948.656
[37,     1] loss: 911.356
[38,     1] loss: 917.432
[39,     1] loss: 922.487
[40,     1] loss: 904.334
[41,     1] loss: 891.435
[42,     1] loss: 936.753
[43,     1] loss: 897.820
[44,     1] loss: 901.969
[45,     1] loss: 873.532
[46,     1] loss: 836.749
[47,     1] loss: 872.099
[48,     1] loss: 856.562
[49,     1] loss: 903.113
[50,     1] loss: 819.591
[51,     1] loss: 891.944
[52,     1] loss: 838.541
[53,     1] loss: 889.015
[54,     1] loss: 849.579
[55,     1] loss: 833.574
[56,     1] loss: 832.402
[57,     1] loss: 820.888
[58,     1] loss: 809.365
[59,     1] loss: 803.850
[60,     1] loss: 806.461
[61,     1] loss: 798.042
[62,     1] loss: 801.844
[63,     1] loss: 751.930
[64,     1] loss: 755.266
[65,     1] loss: 723.778
[66,     1] loss: 738.669
[67,     1] loss: 747.027
[68,     1] loss: 717.505
[69,     1] loss: 715.012
[70,     1] loss: 758.997
[71,     1] loss: 707.707
[72,     1] loss: 667.693
[73,     1] loss: 634.886
[74,     1] loss: 751.482
[75,     1] loss: 741.857
[76,     1] loss: 657.568
[77,     1] loss: 644.127
[78,     1] loss: 644.175
[79,     1] loss: 647.752
[80,     1] loss: 647.077
[81,     1] loss: 649.149
[82,     1] loss: 696.745
[83,     1] loss: 689.800
[84,     1] loss: 638.013
[85,     1] loss: 610.042
[86,     1] loss: 571.431
[87,     1] loss: 620.556
[88,     1] loss: 666.034
[89,     1] loss: 617.925
[90,     1] loss: 563.941
[91,     1] loss: 610.060
[92,     1] loss: 561.706
Early stopping applied (best metric=0.6490077376365662)
Finished Training
Total time taken: 15.25001335144043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.669
[2,     1] loss: 1259.741
[3,     1] loss: 1264.119
[4,     1] loss: 1260.357
[5,     1] loss: 1260.106
[6,     1] loss: 1259.819
[7,     1] loss: 1258.590
[8,     1] loss: 1256.216
[9,     1] loss: 1254.357
[10,     1] loss: 1252.464
[11,     1] loss: 1249.456
[12,     1] loss: 1240.290
[13,     1] loss: 1230.611
[14,     1] loss: 1207.979
[15,     1] loss: 1192.267
[16,     1] loss: 1162.451
[17,     1] loss: 1146.126
[18,     1] loss: 1128.145
[19,     1] loss: 1104.861
[20,     1] loss: 1079.752
[21,     1] loss: 1042.750
[22,     1] loss: 1051.186
[23,     1] loss: 1022.608
[24,     1] loss: 1019.966
[25,     1] loss: 1007.038
[26,     1] loss: 1019.845
[27,     1] loss: 1043.344
[28,     1] loss: 1014.729
[29,     1] loss: 1010.159
[30,     1] loss: 979.272
[31,     1] loss: 1012.975
[32,     1] loss: 994.849
[33,     1] loss: 954.152
[34,     1] loss: 987.457
[35,     1] loss: 937.180
[36,     1] loss: 994.860
[37,     1] loss: 932.970
[38,     1] loss: 962.452
[39,     1] loss: 958.536
[40,     1] loss: 907.404
[41,     1] loss: 936.347
[42,     1] loss: 915.175
[43,     1] loss: 897.547
[44,     1] loss: 962.542
[45,     1] loss: 906.511
[46,     1] loss: 916.134
[47,     1] loss: 908.769
[48,     1] loss: 872.981
[49,     1] loss: 895.804
[50,     1] loss: 884.926
[51,     1] loss: 887.308
[52,     1] loss: 893.133
[53,     1] loss: 860.048
[54,     1] loss: 826.373
[55,     1] loss: 826.048
[56,     1] loss: 830.887
[57,     1] loss: 826.121
[58,     1] loss: 842.672
[59,     1] loss: 808.548
[60,     1] loss: 805.704
[61,     1] loss: 760.945
[62,     1] loss: 769.721
[63,     1] loss: 792.681
[64,     1] loss: 840.233
[65,     1] loss: 851.464
[66,     1] loss: 797.273
[67,     1] loss: 856.208
[68,     1] loss: 780.455
[69,     1] loss: 885.203
[70,     1] loss: 809.476
[71,     1] loss: 862.259
[72,     1] loss: 764.559
[73,     1] loss: 827.314
[74,     1] loss: 807.212
[75,     1] loss: 724.345
[76,     1] loss: 731.794
[77,     1] loss: 743.939
[78,     1] loss: 779.086
[79,     1] loss: 708.096
[80,     1] loss: 678.599
[81,     1] loss: 682.728
[82,     1] loss: 718.554
[83,     1] loss: 668.449
[84,     1] loss: 690.642
[85,     1] loss: 615.688
[86,     1] loss: 692.450
[87,     1] loss: 641.734
[88,     1] loss: 639.535
[89,     1] loss: 634.749
[90,     1] loss: 642.459
[91,     1] loss: 668.792
[92,     1] loss: 622.897
[93,     1] loss: 635.370
[94,     1] loss: 706.220
[95,     1] loss: 668.602
[96,     1] loss: 584.777
[97,     1] loss: 605.055
[98,     1] loss: 582.788
[99,     1] loss: 618.169
[100,     1] loss: 579.042
[101,     1] loss: 611.466
[102,     1] loss: 619.808
[103,     1] loss: 538.808
[104,     1] loss: 698.025
Early stopping applied (best metric=0.6488215923309326)
Finished Training
Total time taken: 13.97101378440857
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1262.043
[2,     1] loss: 1262.422
[3,     1] loss: 1261.144
[4,     1] loss: 1260.314
[5,     1] loss: 1259.172
[6,     1] loss: 1257.613
[7,     1] loss: 1255.002
[8,     1] loss: 1251.344
[9,     1] loss: 1243.286
[10,     1] loss: 1235.334
[11,     1] loss: 1220.885
[12,     1] loss: 1207.677
[13,     1] loss: 1186.960
[14,     1] loss: 1157.700
[15,     1] loss: 1139.246
[16,     1] loss: 1105.075
[17,     1] loss: 1097.992
[18,     1] loss: 1077.506
[19,     1] loss: 1055.477
[20,     1] loss: 1047.354
[21,     1] loss: 1049.927
[22,     1] loss: 1046.967
[23,     1] loss: 1050.420
[24,     1] loss: 1065.208
[25,     1] loss: 1031.804
[26,     1] loss: 1064.977
[27,     1] loss: 982.811
[28,     1] loss: 1006.045
[29,     1] loss: 964.889
[30,     1] loss: 965.313
[31,     1] loss: 980.305
[32,     1] loss: 964.484
[33,     1] loss: 1007.263
[34,     1] loss: 939.236
[35,     1] loss: 1021.162
[36,     1] loss: 926.740
[37,     1] loss: 941.590
[38,     1] loss: 885.924
[39,     1] loss: 910.811
[40,     1] loss: 930.319
[41,     1] loss: 897.891
[42,     1] loss: 892.144
[43,     1] loss: 966.793
[44,     1] loss: 887.275
[45,     1] loss: 941.707
[46,     1] loss: 891.188
[47,     1] loss: 877.876
[48,     1] loss: 900.628
[49,     1] loss: 857.925
[50,     1] loss: 896.120
[51,     1] loss: 846.255
[52,     1] loss: 883.025
[53,     1] loss: 818.533
[54,     1] loss: 914.875
[55,     1] loss: 832.364
[56,     1] loss: 850.452
[57,     1] loss: 869.741
[58,     1] loss: 800.406
[59,     1] loss: 832.996
[60,     1] loss: 805.559
[61,     1] loss: 828.032
[62,     1] loss: 770.634
[63,     1] loss: 754.096
[64,     1] loss: 765.118
[65,     1] loss: 746.200
[66,     1] loss: 732.544
[67,     1] loss: 769.920
[68,     1] loss: 701.896
[69,     1] loss: 746.273
[70,     1] loss: 707.052
[71,     1] loss: 701.945
[72,     1] loss: 789.277
[73,     1] loss: 707.996
[74,     1] loss: 672.939
[75,     1] loss: 757.061
[76,     1] loss: 702.653
[77,     1] loss: 684.291
[78,     1] loss: 696.604
[79,     1] loss: 720.700
[80,     1] loss: 634.239
[81,     1] loss: 608.708
[82,     1] loss: 621.784
[83,     1] loss: 631.137
[84,     1] loss: 619.262
[85,     1] loss: 643.462
[86,     1] loss: 772.644
[87,     1] loss: 828.479
[88,     1] loss: 624.180
[89,     1] loss: 727.345
[90,     1] loss: 691.462
[91,     1] loss: 732.096
[92,     1] loss: 670.395
[93,     1] loss: 688.673
[94,     1] loss: 640.786
[95,     1] loss: 654.018
[96,     1] loss: 665.514
[97,     1] loss: 665.152
[98,     1] loss: 621.500
[99,     1] loss: 593.223
[100,     1] loss: 566.937
[101,     1] loss: 549.288
[102,     1] loss: 536.523
[103,     1] loss: 550.405
[104,     1] loss: 545.029
[105,     1] loss: 602.621
[106,     1] loss: 538.850
[107,     1] loss: 537.728
[108,     1] loss: 678.067
[109,     1] loss: 647.067
[110,     1] loss: 535.322
[111,     1] loss: 604.157
[112,     1] loss: 506.304
[113,     1] loss: 596.947
[114,     1] loss: 537.148
[115,     1] loss: 533.073
[116,     1] loss: 499.279
[117,     1] loss: 544.115
[118,     1] loss: 492.012
[119,     1] loss: 541.597
[120,     1] loss: 545.331
[121,     1] loss: 470.357
[122,     1] loss: 547.823
[123,     1] loss: 465.133
[124,     1] loss: 449.928
[125,     1] loss: 570.835
Early stopping applied (best metric=0.6786372661590576)
Finished Training
Total time taken: 19.92401957511902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.336
[2,     1] loss: 1261.623
[3,     1] loss: 1260.816
[4,     1] loss: 1259.406
[5,     1] loss: 1258.311
[6,     1] loss: 1260.486
[7,     1] loss: 1257.088
[8,     1] loss: 1258.737
[9,     1] loss: 1255.756
[10,     1] loss: 1252.778
[11,     1] loss: 1253.329
[12,     1] loss: 1243.435
[13,     1] loss: 1231.399
[14,     1] loss: 1214.915
[15,     1] loss: 1199.887
[16,     1] loss: 1182.232
[17,     1] loss: 1156.354
[18,     1] loss: 1135.770
[19,     1] loss: 1118.318
[20,     1] loss: 1084.331
[21,     1] loss: 1075.824
[22,     1] loss: 1070.292
[23,     1] loss: 1052.577
[24,     1] loss: 1013.363
[25,     1] loss: 1021.192
[26,     1] loss: 1022.308
[27,     1] loss: 1033.731
[28,     1] loss: 962.172
[29,     1] loss: 990.374
[30,     1] loss: 967.320
[31,     1] loss: 959.059
[32,     1] loss: 980.907
[33,     1] loss: 936.783
[34,     1] loss: 960.000
[35,     1] loss: 935.351
[36,     1] loss: 910.970
[37,     1] loss: 917.646
[38,     1] loss: 875.869
[39,     1] loss: 878.595
[40,     1] loss: 857.802
[41,     1] loss: 886.915
[42,     1] loss: 877.997
[43,     1] loss: 868.712
[44,     1] loss: 831.121
[45,     1] loss: 861.381
[46,     1] loss: 818.246
[47,     1] loss: 831.582
[48,     1] loss: 810.698
[49,     1] loss: 881.584
[50,     1] loss: 861.706
[51,     1] loss: 915.159
[52,     1] loss: 822.189
[53,     1] loss: 791.946
[54,     1] loss: 820.161
[55,     1] loss: 767.011
[56,     1] loss: 816.520
[57,     1] loss: 764.472
[58,     1] loss: 780.555
[59,     1] loss: 754.828
[60,     1] loss: 829.688
[61,     1] loss: 745.976
[62,     1] loss: 850.118
[63,     1] loss: 739.905
[64,     1] loss: 763.194
[65,     1] loss: 737.225
[66,     1] loss: 753.393
[67,     1] loss: 705.561
[68,     1] loss: 731.626
[69,     1] loss: 735.236
[70,     1] loss: 694.690
[71,     1] loss: 696.878
[72,     1] loss: 675.986
[73,     1] loss: 663.108
Early stopping applied (best metric=0.8889943361282349)
Finished Training
Total time taken: 9.901009559631348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.382
[2,     1] loss: 1260.417
[3,     1] loss: 1258.894
[4,     1] loss: 1255.101
[5,     1] loss: 1260.754
[6,     1] loss: 1260.005
[7,     1] loss: 1258.117
[8,     1] loss: 1256.473
[9,     1] loss: 1258.739
[10,     1] loss: 1255.352
[11,     1] loss: 1248.835
[12,     1] loss: 1246.170
[13,     1] loss: 1235.422
[14,     1] loss: 1226.869
[15,     1] loss: 1208.081
[16,     1] loss: 1192.315
[17,     1] loss: 1169.109
[18,     1] loss: 1150.734
[19,     1] loss: 1129.535
[20,     1] loss: 1115.655
[21,     1] loss: 1115.445
[22,     1] loss: 1108.269
[23,     1] loss: 1077.565
[24,     1] loss: 1080.684
[25,     1] loss: 1052.155
[26,     1] loss: 1067.111
[27,     1] loss: 1067.897
[28,     1] loss: 1024.048
[29,     1] loss: 1049.802
[30,     1] loss: 1039.827
[31,     1] loss: 1001.109
[32,     1] loss: 986.152
[33,     1] loss: 1016.920
[34,     1] loss: 986.106
[35,     1] loss: 941.547
[36,     1] loss: 1012.259
[37,     1] loss: 962.244
[38,     1] loss: 984.615
[39,     1] loss: 954.077
[40,     1] loss: 933.960
[41,     1] loss: 970.306
[42,     1] loss: 919.778
[43,     1] loss: 918.997
[44,     1] loss: 917.869
[45,     1] loss: 896.560
[46,     1] loss: 946.958
[47,     1] loss: 871.610
[48,     1] loss: 860.124
[49,     1] loss: 834.711
[50,     1] loss: 866.866
[51,     1] loss: 837.537
[52,     1] loss: 849.821
[53,     1] loss: 869.545
[54,     1] loss: 890.795
[55,     1] loss: 854.230
[56,     1] loss: 879.780
[57,     1] loss: 779.189
[58,     1] loss: 850.459
[59,     1] loss: 794.617
[60,     1] loss: 875.804
[61,     1] loss: 817.871
[62,     1] loss: 827.341
[63,     1] loss: 785.095
[64,     1] loss: 802.656
[65,     1] loss: 730.241
[66,     1] loss: 772.965
[67,     1] loss: 764.612
[68,     1] loss: 799.192
[69,     1] loss: 754.700
Early stopping applied (best metric=0.7883207201957703)
Finished Training
Total time taken: 11.21501111984253
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.543
[2,     1] loss: 1259.180
[3,     1] loss: 1256.947
[4,     1] loss: 1254.266
[5,     1] loss: 1258.871
[6,     1] loss: 1252.771
[7,     1] loss: 1251.199
[8,     1] loss: 1247.102
[9,     1] loss: 1232.901
[10,     1] loss: 1227.083
[11,     1] loss: 1203.789
[12,     1] loss: 1195.930
[13,     1] loss: 1171.850
[14,     1] loss: 1146.084
[15,     1] loss: 1141.237
[16,     1] loss: 1136.628
[17,     1] loss: 1099.166
[18,     1] loss: 1081.594
[19,     1] loss: 1058.726
[20,     1] loss: 1035.697
[21,     1] loss: 1048.994
[22,     1] loss: 1014.639
[23,     1] loss: 1050.854
[24,     1] loss: 1018.365
[25,     1] loss: 1019.265
[26,     1] loss: 987.063
[27,     1] loss: 1005.073
[28,     1] loss: 1011.462
[29,     1] loss: 1002.690
[30,     1] loss: 973.985
[31,     1] loss: 1006.547
[32,     1] loss: 963.143
[33,     1] loss: 965.278
[34,     1] loss: 954.956
[35,     1] loss: 967.429
[36,     1] loss: 955.794
[37,     1] loss: 930.600
[38,     1] loss: 951.172
[39,     1] loss: 878.115
[40,     1] loss: 910.207
[41,     1] loss: 949.497
[42,     1] loss: 907.468
[43,     1] loss: 933.566
[44,     1] loss: 875.757
[45,     1] loss: 912.243
[46,     1] loss: 907.647
[47,     1] loss: 896.513
[48,     1] loss: 866.729
[49,     1] loss: 889.373
[50,     1] loss: 863.477
[51,     1] loss: 849.344
[52,     1] loss: 814.220
[53,     1] loss: 819.972
[54,     1] loss: 836.344
[55,     1] loss: 799.707
[56,     1] loss: 793.075
[57,     1] loss: 734.896
[58,     1] loss: 824.617
[59,     1] loss: 768.589
[60,     1] loss: 863.462
[61,     1] loss: 882.170
[62,     1] loss: 793.070
[63,     1] loss: 855.443
[64,     1] loss: 771.368
[65,     1] loss: 810.708
[66,     1] loss: 757.211
[67,     1] loss: 840.492
[68,     1] loss: 724.455
[69,     1] loss: 779.738
[70,     1] loss: 803.499
[71,     1] loss: 800.677
[72,     1] loss: 751.978
[73,     1] loss: 729.832
[74,     1] loss: 773.002
[75,     1] loss: 706.922
[76,     1] loss: 737.334
[77,     1] loss: 672.505
[78,     1] loss: 732.193
Early stopping applied (best metric=0.7232891321182251)
Finished Training
Total time taken: 12.929012060165405
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.656
[2,     1] loss: 1257.628
[3,     1] loss: 1258.003
[4,     1] loss: 1254.828
[5,     1] loss: 1256.342
[6,     1] loss: 1251.706
[7,     1] loss: 1248.690
[8,     1] loss: 1235.659
[9,     1] loss: 1222.242
[10,     1] loss: 1203.326
[11,     1] loss: 1183.020
[12,     1] loss: 1148.575
[13,     1] loss: 1128.199
[14,     1] loss: 1072.672
[15,     1] loss: 1105.611
[16,     1] loss: 1046.635
[17,     1] loss: 1029.396
[18,     1] loss: 1037.587
[19,     1] loss: 1008.024
[20,     1] loss: 1028.686
[21,     1] loss: 1021.134
[22,     1] loss: 1011.090
[23,     1] loss: 999.144
[24,     1] loss: 1002.044
[25,     1] loss: 1014.509
[26,     1] loss: 1020.993
[27,     1] loss: 1020.852
[28,     1] loss: 959.713
[29,     1] loss: 987.536
[30,     1] loss: 930.299
[31,     1] loss: 933.949
[32,     1] loss: 956.988
[33,     1] loss: 925.228
[34,     1] loss: 940.785
[35,     1] loss: 903.872
[36,     1] loss: 949.657
[37,     1] loss: 875.448
[38,     1] loss: 979.980
[39,     1] loss: 909.014
[40,     1] loss: 898.314
[41,     1] loss: 880.474
[42,     1] loss: 925.380
[43,     1] loss: 921.287
[44,     1] loss: 842.914
[45,     1] loss: 913.324
[46,     1] loss: 891.732
[47,     1] loss: 880.404
[48,     1] loss: 868.779
[49,     1] loss: 821.861
[50,     1] loss: 878.790
[51,     1] loss: 845.322
[52,     1] loss: 817.721
[53,     1] loss: 794.040
[54,     1] loss: 842.676
[55,     1] loss: 784.805
[56,     1] loss: 821.938
[57,     1] loss: 773.287
[58,     1] loss: 753.021
[59,     1] loss: 799.762
[60,     1] loss: 768.151
[61,     1] loss: 768.220
[62,     1] loss: 727.202
[63,     1] loss: 744.094
Early stopping applied (best metric=0.9120054244995117)
Finished Training
Total time taken: 10.546010255813599
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1262.438
[2,     1] loss: 1260.155
[3,     1] loss: 1261.674
[4,     1] loss: 1260.701
[5,     1] loss: 1259.034
[6,     1] loss: 1261.117
[7,     1] loss: 1251.609
[8,     1] loss: 1251.982
[9,     1] loss: 1245.181
[10,     1] loss: 1228.292
[11,     1] loss: 1218.339
[12,     1] loss: 1192.778
[13,     1] loss: 1183.752
[14,     1] loss: 1152.463
[15,     1] loss: 1138.442
[16,     1] loss: 1120.126
[17,     1] loss: 1075.983
[18,     1] loss: 1053.066
[19,     1] loss: 1036.821
[20,     1] loss: 1032.788
[21,     1] loss: 1039.777
[22,     1] loss: 1082.334
[23,     1] loss: 1051.992
[24,     1] loss: 1039.072
[25,     1] loss: 1018.620
[26,     1] loss: 1000.033
[27,     1] loss: 1005.675
[28,     1] loss: 1005.859
[29,     1] loss: 991.636
[30,     1] loss: 964.439
[31,     1] loss: 1009.482
[32,     1] loss: 972.035
[33,     1] loss: 990.636
[34,     1] loss: 968.902
[35,     1] loss: 913.860
[36,     1] loss: 996.646
[37,     1] loss: 917.002
[38,     1] loss: 928.558
[39,     1] loss: 890.423
[40,     1] loss: 891.657
[41,     1] loss: 918.442
[42,     1] loss: 927.046
[43,     1] loss: 933.252
[44,     1] loss: 915.885
[45,     1] loss: 873.170
[46,     1] loss: 826.730
[47,     1] loss: 800.794
[48,     1] loss: 925.302
[49,     1] loss: 861.110
[50,     1] loss: 856.194
[51,     1] loss: 894.869
[52,     1] loss: 853.658
[53,     1] loss: 809.271
[54,     1] loss: 851.691
[55,     1] loss: 785.767
[56,     1] loss: 801.494
[57,     1] loss: 760.852
[58,     1] loss: 765.680
[59,     1] loss: 744.692
[60,     1] loss: 784.062
[61,     1] loss: 746.063
[62,     1] loss: 699.375
[63,     1] loss: 724.657
Early stopping applied (best metric=0.8131141662597656)
Finished Training
Total time taken: 10.559009790420532
{'Hydroxylation-K Validation Accuracy': 0.7842494089834515, 'Hydroxylation-K Validation Sensitivity': 0.7222222222222222, 'Hydroxylation-K Validation Specificity': 0.8, 'Hydroxylation-K Validation Precision': 0.482134847723083, 'Hydroxylation-K AUC ROC': 0.8530409356725146, 'Hydroxylation-K AUC PR': 0.6973854348150514, 'Hydroxylation-K MCC': 0.4579653756677537, 'Hydroxylation-K F1': 0.5723418629566608, 'Validation Loss (Hydroxylation-K)': 0.3859238177537918, 'Hydroxylation-P Validation Accuracy': 0.7842860260900462, 'Hydroxylation-P Validation Sensitivity': 0.7862962962962963, 'Hydroxylation-P Validation Specificity': 0.7838894707965485, 'Hydroxylation-P Validation Precision': 0.44219797497147495, 'Hydroxylation-P AUC ROC': 0.8423061534953649, 'Hydroxylation-P AUC PR': 0.5638714639885354, 'Hydroxylation-P MCC': 0.46949331452208454, 'Hydroxylation-P F1': 0.5645978972738602, 'Validation Loss (Hydroxylation-P)': 0.3807670672734578, 'Validation Loss (total)': 0.7666908820470174, 'TimeToTrain': 13.386612844467162}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009456522982655651,
 'learning_rate_Hydroxylation-K': 0.008261586086389713,
 'learning_rate_Hydroxylation-P': 0.0065015528706390425,
 'log_base': 2.7207127453745614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 82445401,
 'sample_weights': [1.6696851919897937, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.443251384720574,
 'weight_decay_Hydroxylation-K': 8.660816784937397,
 'weight_decay_Hydroxylation-P': 2.7719133287333904}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.908
[2,     1] loss: 1266.118
[3,     1] loss: 1259.225
[4,     1] loss: 1258.011
[5,     1] loss: 1258.217
[6,     1] loss: 1253.164
[7,     1] loss: 1248.940
[8,     1] loss: 1241.831
[9,     1] loss: 1234.494
[10,     1] loss: 1183.471
[11,     1] loss: 1132.617
[12,     1] loss: 1188.738
[13,     1] loss: 1177.483
[14,     1] loss: 1045.017
[15,     1] loss: 1066.392
[16,     1] loss: 1057.986
[17,     1] loss: 1054.077
[18,     1] loss: 1051.339
[19,     1] loss: 1012.093
[20,     1] loss: 1055.490
[21,     1] loss: 1011.250
[22,     1] loss: 1068.688
[23,     1] loss: 959.791
[24,     1] loss: 1029.083
[25,     1] loss: 978.696
[26,     1] loss: 1040.017
[27,     1] loss: 962.549
[28,     1] loss: 943.247
[29,     1] loss: 969.403
[30,     1] loss: 934.669
[31,     1] loss: 996.160
[32,     1] loss: 914.773
[33,     1] loss: 951.985
[34,     1] loss: 970.648
[35,     1] loss: 886.889
[36,     1] loss: 892.507
[37,     1] loss: 840.683
[38,     1] loss: 872.043
[39,     1] loss: 859.308
[40,     1] loss: 843.178
[41,     1] loss: 1007.004
[42,     1] loss: 1231.017
[43,     1] loss: 890.821
[44,     1] loss: 1035.801
[45,     1] loss: 1020.043
[46,     1] loss: 956.215
[47,     1] loss: 972.712
[48,     1] loss: 981.676
[49,     1] loss: 916.179
[50,     1] loss: 897.954
[51,     1] loss: 898.922
[52,     1] loss: 900.795
[53,     1] loss: 834.742
[54,     1] loss: 894.829
[55,     1] loss: 812.138
Early stopping applied (best metric=0.816551685333252)
Finished Training
Total time taken: 7.475006818771362
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.784
[2,     1] loss: 1299.309
[3,     1] loss: 1258.930
[4,     1] loss: 1256.369
[5,     1] loss: 1260.099
[6,     1] loss: 1261.479
[7,     1] loss: 1259.483
[8,     1] loss: 1259.422
[9,     1] loss: 1258.083
[10,     1] loss: 1261.530
[11,     1] loss: 1260.123
[12,     1] loss: 1259.101
[13,     1] loss: 1259.715
[14,     1] loss: 1259.491
[15,     1] loss: 1259.719
[16,     1] loss: 1259.150
[17,     1] loss: 1258.826
[18,     1] loss: 1258.647
[19,     1] loss: 1260.516
[20,     1] loss: 1259.011
[21,     1] loss: 1257.762
[22,     1] loss: 1257.577
[23,     1] loss: 1255.861
[24,     1] loss: 1253.343
[25,     1] loss: 1249.601
[26,     1] loss: 1242.446
[27,     1] loss: 1236.106
[28,     1] loss: 1222.717
[29,     1] loss: 1198.164
[30,     1] loss: 1193.164
[31,     1] loss: 1152.971
[32,     1] loss: 1144.962
[33,     1] loss: 1087.659
[34,     1] loss: 1044.249
[35,     1] loss: 1034.772
[36,     1] loss: 1082.197
[37,     1] loss: 1075.842
[38,     1] loss: 1039.849
[39,     1] loss: 1024.435
[40,     1] loss: 1027.063
[41,     1] loss: 1015.181
[42,     1] loss: 985.211
[43,     1] loss: 987.781
[44,     1] loss: 985.060
[45,     1] loss: 987.074
[46,     1] loss: 964.236
[47,     1] loss: 950.925
[48,     1] loss: 902.611
[49,     1] loss: 913.136
[50,     1] loss: 869.305
[51,     1] loss: 849.016
[52,     1] loss: 909.344
[53,     1] loss: 1345.094
[54,     1] loss: 1039.599
[55,     1] loss: 959.281
[56,     1] loss: 1037.761
[57,     1] loss: 1039.470
[58,     1] loss: 1032.575
[59,     1] loss: 982.899
[60,     1] loss: 996.527
[61,     1] loss: 968.930
[62,     1] loss: 979.625
[63,     1] loss: 950.041
[64,     1] loss: 978.596
[65,     1] loss: 901.536
[66,     1] loss: 926.601
Early stopping applied (best metric=0.8465146422386169)
Finished Training
Total time taken: 10.963011026382446
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.964
[2,     1] loss: 1265.691
[3,     1] loss: 1261.888
[4,     1] loss: 1262.326
[5,     1] loss: 1260.595
[6,     1] loss: 1259.625
[7,     1] loss: 1258.192
[8,     1] loss: 1254.640
[9,     1] loss: 1255.269
[10,     1] loss: 1245.581
[11,     1] loss: 1234.600
[12,     1] loss: 1207.355
[13,     1] loss: 1165.460
[14,     1] loss: 1128.734
[15,     1] loss: 1221.392
[16,     1] loss: 1089.039
[17,     1] loss: 1104.073
[18,     1] loss: 1036.980
[19,     1] loss: 1066.956
[20,     1] loss: 1055.238
[21,     1] loss: 1007.910
[22,     1] loss: 1055.342
[23,     1] loss: 972.045
[24,     1] loss: 1064.461
[25,     1] loss: 1006.940
[26,     1] loss: 1007.694
[27,     1] loss: 968.707
[28,     1] loss: 978.800
[29,     1] loss: 947.313
[30,     1] loss: 973.936
[31,     1] loss: 927.821
[32,     1] loss: 901.003
[33,     1] loss: 980.022
[34,     1] loss: 972.020
[35,     1] loss: 889.055
[36,     1] loss: 949.246
[37,     1] loss: 890.719
[38,     1] loss: 935.968
[39,     1] loss: 926.378
[40,     1] loss: 921.719
[41,     1] loss: 923.959
[42,     1] loss: 837.669
[43,     1] loss: 872.413
[44,     1] loss: 816.091
[45,     1] loss: 817.751
[46,     1] loss: 888.788
[47,     1] loss: 1068.040
[48,     1] loss: 937.822
[49,     1] loss: 895.520
[50,     1] loss: 884.492
[51,     1] loss: 958.420
[52,     1] loss: 836.690
[53,     1] loss: 923.504
[54,     1] loss: 836.679
[55,     1] loss: 841.109
[56,     1] loss: 842.147
[57,     1] loss: 818.447
[58,     1] loss: 826.878
[59,     1] loss: 894.435
[60,     1] loss: 790.959
[61,     1] loss: 747.789
[62,     1] loss: 740.560
[63,     1] loss: 758.540
[64,     1] loss: 1052.954
[65,     1] loss: 1513.168
[66,     1] loss: 975.235
[67,     1] loss: 1084.370
[68,     1] loss: 1113.154
[69,     1] loss: 1072.746
[70,     1] loss: 1075.915
[71,     1] loss: 1038.724
[72,     1] loss: 1022.873
[73,     1] loss: 979.953
[74,     1] loss: 982.250
[75,     1] loss: 962.729
[76,     1] loss: 919.635
[77,     1] loss: 906.597
[78,     1] loss: 940.855
[79,     1] loss: 995.780
[80,     1] loss: 917.427
Early stopping applied (best metric=0.8217591047286987)
Finished Training
Total time taken: 10.820010900497437
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.467
[2,     1] loss: 1273.001
[3,     1] loss: 1261.084
[4,     1] loss: 1259.023
[5,     1] loss: 1263.473
[6,     1] loss: 1260.335
[7,     1] loss: 1258.639
[8,     1] loss: 1258.715
[9,     1] loss: 1259.090
[10,     1] loss: 1260.491
[11,     1] loss: 1259.803
[12,     1] loss: 1258.199
[13,     1] loss: 1258.724
[14,     1] loss: 1259.740
[15,     1] loss: 1259.856
[16,     1] loss: 1259.806
[17,     1] loss: 1260.142
[18,     1] loss: 1258.292
[19,     1] loss: 1260.506
[20,     1] loss: 1259.948
[21,     1] loss: 1259.898
[22,     1] loss: 1260.397
[23,     1] loss: 1259.981
[24,     1] loss: 1258.593
[25,     1] loss: 1258.490
[26,     1] loss: 1258.085
[27,     1] loss: 1257.105
[28,     1] loss: 1256.821
[29,     1] loss: 1252.685
[30,     1] loss: 1248.679
[31,     1] loss: 1239.322
[32,     1] loss: 1226.133
[33,     1] loss: 1227.142
[34,     1] loss: 1189.913
[35,     1] loss: 1202.094
[36,     1] loss: 1148.807
[37,     1] loss: 1093.681
[38,     1] loss: 1093.140
[39,     1] loss: 1063.389
[40,     1] loss: 1171.289
[41,     1] loss: 1129.832
[42,     1] loss: 1094.497
[43,     1] loss: 1021.167
[44,     1] loss: 1130.590
[45,     1] loss: 1063.309
[46,     1] loss: 1035.967
[47,     1] loss: 1070.144
[48,     1] loss: 1034.272
[49,     1] loss: 997.867
[50,     1] loss: 1028.911
[51,     1] loss: 960.284
[52,     1] loss: 948.759
[53,     1] loss: 941.379
[54,     1] loss: 957.653
[55,     1] loss: 931.196
[56,     1] loss: 912.911
[57,     1] loss: 922.917
[58,     1] loss: 973.997
[59,     1] loss: 944.213
[60,     1] loss: 906.761
[61,     1] loss: 897.350
[62,     1] loss: 984.817
[63,     1] loss: 943.086
[64,     1] loss: 903.735
[65,     1] loss: 985.762
[66,     1] loss: 967.703
[67,     1] loss: 904.984
[68,     1] loss: 920.848
[69,     1] loss: 909.795
[70,     1] loss: 904.580
[71,     1] loss: 863.727
[72,     1] loss: 1022.758
[73,     1] loss: 915.116
[74,     1] loss: 846.074
[75,     1] loss: 917.662
[76,     1] loss: 882.021
[77,     1] loss: 854.745
[78,     1] loss: 792.430
[79,     1] loss: 837.802
[80,     1] loss: 815.918
Early stopping applied (best metric=0.7724360227584839)
Finished Training
Total time taken: 13.323012828826904
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1259.280
[2,     1] loss: 1262.620
[3,     1] loss: 1262.047
[4,     1] loss: 1258.578
[5,     1] loss: 1259.662
[6,     1] loss: 1260.808
[7,     1] loss: 1243.712
[8,     1] loss: 1243.426
[9,     1] loss: 1213.069
[10,     1] loss: 1168.319
[11,     1] loss: 1122.372
[12,     1] loss: 1080.896
[13,     1] loss: 1094.877
[14,     1] loss: 1003.725
[15,     1] loss: 1034.166
[16,     1] loss: 1098.914
[17,     1] loss: 1102.519
[18,     1] loss: 1047.972
[19,     1] loss: 1050.536
[20,     1] loss: 1021.467
[21,     1] loss: 975.929
[22,     1] loss: 997.565
[23,     1] loss: 940.232
[24,     1] loss: 980.235
[25,     1] loss: 972.539
[26,     1] loss: 899.209
[27,     1] loss: 959.593
[28,     1] loss: 893.634
[29,     1] loss: 905.300
[30,     1] loss: 986.002
[31,     1] loss: 844.267
[32,     1] loss: 949.156
[33,     1] loss: 941.974
[34,     1] loss: 851.825
[35,     1] loss: 915.539
[36,     1] loss: 849.191
[37,     1] loss: 949.499
[38,     1] loss: 827.680
[39,     1] loss: 900.023
[40,     1] loss: 862.370
[41,     1] loss: 814.763
[42,     1] loss: 874.560
[43,     1] loss: 821.256
[44,     1] loss: 818.139
[45,     1] loss: 762.345
[46,     1] loss: 801.363
[47,     1] loss: 805.646
[48,     1] loss: 964.246
Early stopping applied (best metric=0.8859245777130127)
Finished Training
Total time taken: 7.995009422302246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.328
[2,     1] loss: 1263.064
[3,     1] loss: 1259.674
[4,     1] loss: 1261.541
[5,     1] loss: 1263.275
[6,     1] loss: 1256.002
[7,     1] loss: 1259.212
[8,     1] loss: 1260.711
[9,     1] loss: 1257.593
[10,     1] loss: 1253.849
[11,     1] loss: 1252.043
[12,     1] loss: 1246.012
[13,     1] loss: 1225.938
[14,     1] loss: 1198.043
[15,     1] loss: 1161.474
[16,     1] loss: 1123.634
[17,     1] loss: 1110.477
[18,     1] loss: 1124.771
[19,     1] loss: 1060.674
[20,     1] loss: 1048.190
[21,     1] loss: 985.701
[22,     1] loss: 1019.752
[23,     1] loss: 1010.180
[24,     1] loss: 966.279
[25,     1] loss: 1007.236
[26,     1] loss: 943.152
[27,     1] loss: 964.919
[28,     1] loss: 975.482
[29,     1] loss: 958.109
[30,     1] loss: 953.274
[31,     1] loss: 966.931
[32,     1] loss: 1085.324
[33,     1] loss: 952.162
[34,     1] loss: 959.819
[35,     1] loss: 917.252
[36,     1] loss: 963.903
[37,     1] loss: 868.334
[38,     1] loss: 876.382
[39,     1] loss: 900.691
[40,     1] loss: 896.866
[41,     1] loss: 848.812
[42,     1] loss: 850.681
[43,     1] loss: 854.547
[44,     1] loss: 840.612
[45,     1] loss: 851.604
[46,     1] loss: 870.929
[47,     1] loss: 894.335
[48,     1] loss: 758.318
[49,     1] loss: 870.998
[50,     1] loss: 874.107
[51,     1] loss: 798.930
[52,     1] loss: 865.046
Early stopping applied (best metric=0.938613772392273)
Finished Training
Total time taken: 8.729007482528687
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.600
[2,     1] loss: 1262.162
[3,     1] loss: 1264.149
[4,     1] loss: 1260.375
[5,     1] loss: 1260.720
[6,     1] loss: 1253.446
[7,     1] loss: 1250.153
[8,     1] loss: 1240.495
[9,     1] loss: 1216.230
[10,     1] loss: 1175.904
[11,     1] loss: 1129.064
[12,     1] loss: 1148.134
[13,     1] loss: 1112.778
[14,     1] loss: 1025.456
[15,     1] loss: 1081.379
[16,     1] loss: 1023.011
[17,     1] loss: 1055.333
[18,     1] loss: 1010.642
[19,     1] loss: 1037.282
[20,     1] loss: 1038.082
[21,     1] loss: 987.843
[22,     1] loss: 1018.831
[23,     1] loss: 931.376
[24,     1] loss: 948.718
[25,     1] loss: 926.132
[26,     1] loss: 886.704
[27,     1] loss: 949.375
[28,     1] loss: 934.807
[29,     1] loss: 895.127
[30,     1] loss: 905.963
[31,     1] loss: 904.860
[32,     1] loss: 1110.510
[33,     1] loss: 1123.829
[34,     1] loss: 980.898
[35,     1] loss: 1012.764
[36,     1] loss: 1030.351
[37,     1] loss: 955.401
[38,     1] loss: 948.320
[39,     1] loss: 954.606
[40,     1] loss: 890.218
[41,     1] loss: 911.876
[42,     1] loss: 798.562
[43,     1] loss: 835.442
[44,     1] loss: 886.311
[45,     1] loss: 857.840
[46,     1] loss: 860.521
[47,     1] loss: 855.478
[48,     1] loss: 859.889
[49,     1] loss: 845.899
Early stopping applied (best metric=0.9260036945343018)
Finished Training
Total time taken: 6.620008707046509
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.164
[2,     1] loss: 1264.738
[3,     1] loss: 1258.791
[4,     1] loss: 1263.296
[5,     1] loss: 1257.390
[6,     1] loss: 1258.178
[7,     1] loss: 1258.007
[8,     1] loss: 1255.954
[9,     1] loss: 1252.230
[10,     1] loss: 1246.551
[11,     1] loss: 1230.484
[12,     1] loss: 1203.210
[13,     1] loss: 1156.521
[14,     1] loss: 1125.537
[15,     1] loss: 1149.998
[16,     1] loss: 1221.421
[17,     1] loss: 1070.672
[18,     1] loss: 1128.450
[19,     1] loss: 1066.379
[20,     1] loss: 1098.463
[21,     1] loss: 1094.175
[22,     1] loss: 1060.722
[23,     1] loss: 1056.335
[24,     1] loss: 1050.812
[25,     1] loss: 1029.860
[26,     1] loss: 1015.318
[27,     1] loss: 984.650
[28,     1] loss: 960.882
[29,     1] loss: 993.498
[30,     1] loss: 998.441
[31,     1] loss: 935.810
[32,     1] loss: 946.387
[33,     1] loss: 940.924
[34,     1] loss: 941.817
[35,     1] loss: 1024.681
[36,     1] loss: 1009.082
[37,     1] loss: 909.427
[38,     1] loss: 961.693
[39,     1] loss: 906.459
[40,     1] loss: 913.017
[41,     1] loss: 895.886
[42,     1] loss: 894.378
[43,     1] loss: 840.680
[44,     1] loss: 910.520
[45,     1] loss: 1039.024
[46,     1] loss: 901.661
[47,     1] loss: 928.363
[48,     1] loss: 927.706
[49,     1] loss: 920.245
[50,     1] loss: 836.815
[51,     1] loss: 907.670
[52,     1] loss: 874.305
[53,     1] loss: 791.202
[54,     1] loss: 785.736
[55,     1] loss: 747.989
[56,     1] loss: 738.774
[57,     1] loss: 874.757
[58,     1] loss: 1415.176
[59,     1] loss: 971.519
[60,     1] loss: 1023.956
Early stopping applied (best metric=0.7590091228485107)
Finished Training
Total time taken: 10.053009510040283
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.440
[2,     1] loss: 1266.045
[3,     1] loss: 1258.562
[4,     1] loss: 1264.554
[5,     1] loss: 1261.375
[6,     1] loss: 1261.944
[7,     1] loss: 1261.452
[8,     1] loss: 1261.507
[9,     1] loss: 1260.270
[10,     1] loss: 1260.834
[11,     1] loss: 1258.683
[12,     1] loss: 1259.838
[13,     1] loss: 1257.301
[14,     1] loss: 1258.627
[15,     1] loss: 1255.309
[16,     1] loss: 1253.944
[17,     1] loss: 1247.769
[18,     1] loss: 1237.141
[19,     1] loss: 1217.469
[20,     1] loss: 1186.595
[21,     1] loss: 1154.302
[22,     1] loss: 1121.762
[23,     1] loss: 1097.559
[24,     1] loss: 1085.850
[25,     1] loss: 1142.681
[26,     1] loss: 1084.200
[27,     1] loss: 1083.560
[28,     1] loss: 1057.546
[29,     1] loss: 1071.542
[30,     1] loss: 1016.115
[31,     1] loss: 1088.492
[32,     1] loss: 979.980
[33,     1] loss: 1041.683
[34,     1] loss: 1012.931
[35,     1] loss: 974.133
[36,     1] loss: 1015.273
[37,     1] loss: 943.993
[38,     1] loss: 990.953
[39,     1] loss: 946.445
[40,     1] loss: 993.113
[41,     1] loss: 943.400
[42,     1] loss: 971.705
[43,     1] loss: 930.729
[44,     1] loss: 986.155
[45,     1] loss: 913.917
[46,     1] loss: 944.983
[47,     1] loss: 898.410
[48,     1] loss: 976.957
[49,     1] loss: 909.538
[50,     1] loss: 841.181
[51,     1] loss: 919.945
[52,     1] loss: 959.447
[53,     1] loss: 856.507
[54,     1] loss: 871.504
[55,     1] loss: 933.072
[56,     1] loss: 836.382
[57,     1] loss: 910.623
[58,     1] loss: 885.987
[59,     1] loss: 867.145
[60,     1] loss: 848.750
[61,     1] loss: 765.600
[62,     1] loss: 791.658
[63,     1] loss: 1022.569
[64,     1] loss: 1380.167
[65,     1] loss: 1023.253
[66,     1] loss: 1090.551
[67,     1] loss: 1126.214
[68,     1] loss: 1029.863
[69,     1] loss: 1012.361
[70,     1] loss: 1037.284
[71,     1] loss: 994.770
[72,     1] loss: 1001.168
Early stopping applied (best metric=0.7635397911071777)
Finished Training
Total time taken: 11.808011770248413
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1263.976
[2,     1] loss: 1260.890
[3,     1] loss: 1260.027
[4,     1] loss: 1264.890
[5,     1] loss: 1257.054
[6,     1] loss: 1262.746
[7,     1] loss: 1250.600
[8,     1] loss: 1239.606
[9,     1] loss: 1227.269
[10,     1] loss: 1183.923
[11,     1] loss: 1118.293
[12,     1] loss: 1144.168
[13,     1] loss: 1055.346
[14,     1] loss: 1029.441
[15,     1] loss: 993.129
[16,     1] loss: 980.016
[17,     1] loss: 1015.461
[18,     1] loss: 963.433
[19,     1] loss: 928.761
[20,     1] loss: 958.417
[21,     1] loss: 934.298
[22,     1] loss: 960.857
[23,     1] loss: 1020.642
[24,     1] loss: 861.038
[25,     1] loss: 903.146
[26,     1] loss: 934.000
[27,     1] loss: 932.341
[28,     1] loss: 926.769
[29,     1] loss: 895.161
[30,     1] loss: 887.191
[31,     1] loss: 853.930
[32,     1] loss: 858.475
[33,     1] loss: 928.516
[34,     1] loss: 918.883
[35,     1] loss: 805.991
[36,     1] loss: 799.478
[37,     1] loss: 832.865
[38,     1] loss: 873.722
[39,     1] loss: 944.633
[40,     1] loss: 796.865
[41,     1] loss: 911.240
[42,     1] loss: 845.378
[43,     1] loss: 874.900
[44,     1] loss: 800.745
[45,     1] loss: 786.414
[46,     1] loss: 837.310
[47,     1] loss: 759.359
[48,     1] loss: 781.667
[49,     1] loss: 861.872
[50,     1] loss: 862.218
[51,     1] loss: 748.388
[52,     1] loss: 807.314
[53,     1] loss: 841.942
[54,     1] loss: 736.061
[55,     1] loss: 921.970
[56,     1] loss: 963.082
[57,     1] loss: 831.444
[58,     1] loss: 898.438
[59,     1] loss: 881.801
[60,     1] loss: 773.105
[61,     1] loss: 881.312
[62,     1] loss: 786.264
[63,     1] loss: 873.658
[64,     1] loss: 796.383
[65,     1] loss: 824.971
[66,     1] loss: 736.543
[67,     1] loss: 739.849
[68,     1] loss: 726.474
[69,     1] loss: 686.160
Early stopping applied (best metric=0.9035168290138245)
Finished Training
Total time taken: 10.041008949279785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.330
[2,     1] loss: 1259.996
[3,     1] loss: 1260.490
[4,     1] loss: 1262.109
[5,     1] loss: 1263.406
[6,     1] loss: 1258.441
[7,     1] loss: 1259.585
[8,     1] loss: 1260.269
[9,     1] loss: 1259.121
[10,     1] loss: 1255.907
[11,     1] loss: 1254.661
[12,     1] loss: 1251.096
[13,     1] loss: 1242.383
[14,     1] loss: 1228.115
[15,     1] loss: 1196.170
[16,     1] loss: 1166.213
[17,     1] loss: 1129.982
[18,     1] loss: 1098.283
[19,     1] loss: 1107.637
[20,     1] loss: 1099.647
[21,     1] loss: 1032.823
[22,     1] loss: 1063.723
[23,     1] loss: 1002.279
[24,     1] loss: 1058.374
[25,     1] loss: 960.586
[26,     1] loss: 1013.772
[27,     1] loss: 983.757
[28,     1] loss: 1029.804
[29,     1] loss: 966.026
[30,     1] loss: 960.869
[31,     1] loss: 970.788
[32,     1] loss: 981.158
[33,     1] loss: 937.053
[34,     1] loss: 876.872
[35,     1] loss: 925.530
[36,     1] loss: 964.013
[37,     1] loss: 1010.336
[38,     1] loss: 924.177
[39,     1] loss: 959.272
[40,     1] loss: 905.629
[41,     1] loss: 899.098
[42,     1] loss: 892.541
[43,     1] loss: 848.069
[44,     1] loss: 767.765
[45,     1] loss: 815.508
[46,     1] loss: 960.790
[47,     1] loss: 1145.855
[48,     1] loss: 863.615
[49,     1] loss: 946.677
[50,     1] loss: 990.480
[51,     1] loss: 931.338
[52,     1] loss: 951.595
[53,     1] loss: 939.892
[54,     1] loss: 876.874
[55,     1] loss: 869.372
[56,     1] loss: 793.110
[57,     1] loss: 799.448
[58,     1] loss: 782.813
[59,     1] loss: 731.800
[60,     1] loss: 812.629
[61,     1] loss: 1174.361
[62,     1] loss: 880.390
[63,     1] loss: 944.307
[64,     1] loss: 918.582
[65,     1] loss: 958.875
[66,     1] loss: 782.190
[67,     1] loss: 904.650
Early stopping applied (best metric=0.8662964105606079)
Finished Training
Total time taken: 10.187010049819946
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.574
[2,     1] loss: 1262.853
[3,     1] loss: 1263.963
[4,     1] loss: 1258.351
[5,     1] loss: 1260.177
[6,     1] loss: 1257.767
[7,     1] loss: 1256.667
[8,     1] loss: 1254.418
[9,     1] loss: 1252.563
[10,     1] loss: 1245.735
[11,     1] loss: 1229.670
[12,     1] loss: 1200.922
[13,     1] loss: 1172.305
[14,     1] loss: 1123.958
[15,     1] loss: 1094.102
[16,     1] loss: 1116.630
[17,     1] loss: 1134.319
[18,     1] loss: 1018.998
[19,     1] loss: 1080.424
[20,     1] loss: 1017.422
[21,     1] loss: 991.091
[22,     1] loss: 999.973
[23,     1] loss: 1028.372
[24,     1] loss: 965.125
[25,     1] loss: 990.095
[26,     1] loss: 954.947
[27,     1] loss: 904.914
[28,     1] loss: 947.811
[29,     1] loss: 985.400
[30,     1] loss: 976.739
[31,     1] loss: 914.903
[32,     1] loss: 940.271
[33,     1] loss: 922.188
[34,     1] loss: 935.872
[35,     1] loss: 887.095
[36,     1] loss: 886.296
[37,     1] loss: 868.856
[38,     1] loss: 853.613
[39,     1] loss: 1150.360
[40,     1] loss: 1301.866
[41,     1] loss: 940.845
[42,     1] loss: 964.132
[43,     1] loss: 1075.316
[44,     1] loss: 1082.025
[45,     1] loss: 1002.765
[46,     1] loss: 987.719
[47,     1] loss: 1006.808
[48,     1] loss: 965.006
[49,     1] loss: 930.570
[50,     1] loss: 981.237
[51,     1] loss: 919.070
[52,     1] loss: 908.482
[53,     1] loss: 928.411
[54,     1] loss: 924.514
[55,     1] loss: 896.956
[56,     1] loss: 928.838
[57,     1] loss: 864.043
[58,     1] loss: 899.931
[59,     1] loss: 847.362
[60,     1] loss: 910.222
[61,     1] loss: 934.955
[62,     1] loss: 787.456
[63,     1] loss: 868.885
[64,     1] loss: 819.060
[65,     1] loss: 859.950
[66,     1] loss: 1020.660
[67,     1] loss: 824.564
[68,     1] loss: 818.700
[69,     1] loss: 864.546
[70,     1] loss: 787.628
[71,     1] loss: 888.355
[72,     1] loss: 1075.285
[73,     1] loss: 818.128
[74,     1] loss: 961.959
[75,     1] loss: 850.716
[76,     1] loss: 915.554
[77,     1] loss: 814.993
[78,     1] loss: 966.297
[79,     1] loss: 807.549
[80,     1] loss: 857.076
[81,     1] loss: 807.469
[82,     1] loss: 813.563
[83,     1] loss: 780.231
[84,     1] loss: 829.468
[85,     1] loss: 826.154
[86,     1] loss: 734.060
[87,     1] loss: 713.428
[88,     1] loss: 796.635
[89,     1] loss: 1092.290
[90,     1] loss: 1112.542
[91,     1] loss: 856.228
[92,     1] loss: 960.678
[93,     1] loss: 946.281
[94,     1] loss: 888.635
[95,     1] loss: 942.604
[96,     1] loss: 891.861
[97,     1] loss: 911.643
[98,     1] loss: 837.984
[99,     1] loss: 872.616
[100,     1] loss: 824.612
[101,     1] loss: 806.658
[102,     1] loss: 788.990
[103,     1] loss: 724.803
[104,     1] loss: 813.199
[105,     1] loss: 721.624
[106,     1] loss: 778.751
Early stopping applied (best metric=0.8112229108810425)
Finished Training
Total time taken: 16.157017946243286
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.837
[2,     1] loss: 1262.032
[3,     1] loss: 1264.833
[4,     1] loss: 1256.837
[5,     1] loss: 1263.280
[6,     1] loss: 1263.475
[7,     1] loss: 1256.050
[8,     1] loss: 1260.284
[9,     1] loss: 1258.410
[10,     1] loss: 1257.383
[11,     1] loss: 1255.271
[12,     1] loss: 1250.261
[13,     1] loss: 1243.332
[14,     1] loss: 1223.635
[15,     1] loss: 1186.793
[16,     1] loss: 1155.069
[17,     1] loss: 1162.406
[18,     1] loss: 1119.331
[19,     1] loss: 1074.804
[20,     1] loss: 1047.293
[21,     1] loss: 1050.608
[22,     1] loss: 1024.702
[23,     1] loss: 1136.052
[24,     1] loss: 968.497
[25,     1] loss: 1052.964
[26,     1] loss: 1021.616
[27,     1] loss: 1039.282
[28,     1] loss: 1019.520
[29,     1] loss: 1004.156
[30,     1] loss: 1001.896
[31,     1] loss: 988.533
[32,     1] loss: 970.753
[33,     1] loss: 953.088
[34,     1] loss: 937.494
[35,     1] loss: 957.581
[36,     1] loss: 931.231
[37,     1] loss: 897.525
[38,     1] loss: 946.507
[39,     1] loss: 859.465
[40,     1] loss: 918.276
[41,     1] loss: 893.128
[42,     1] loss: 848.306
[43,     1] loss: 887.843
[44,     1] loss: 970.372
[45,     1] loss: 929.889
[46,     1] loss: 816.649
[47,     1] loss: 860.133
[48,     1] loss: 783.076
[49,     1] loss: 825.442
[50,     1] loss: 797.410
[51,     1] loss: 732.469
[52,     1] loss: 905.440
[53,     1] loss: 1502.231
[54,     1] loss: 903.336
[55,     1] loss: 997.419
[56,     1] loss: 1021.322
[57,     1] loss: 995.598
[58,     1] loss: 988.668
[59,     1] loss: 976.333
[60,     1] loss: 935.217
[61,     1] loss: 980.535
[62,     1] loss: 982.546
[63,     1] loss: 898.987
[64,     1] loss: 988.679
[65,     1] loss: 843.628
[66,     1] loss: 915.209
[67,     1] loss: 857.978
Early stopping applied (best metric=0.8348195552825928)
Finished Training
Total time taken: 9.901009559631348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.887
[2,     1] loss: 1270.677
[3,     1] loss: 1263.435
[4,     1] loss: 1260.600
[5,     1] loss: 1261.141
[6,     1] loss: 1259.093
[7,     1] loss: 1257.947
[8,     1] loss: 1257.965
[9,     1] loss: 1260.425
[10,     1] loss: 1259.147
[11,     1] loss: 1257.954
[12,     1] loss: 1256.868
[13,     1] loss: 1256.064
[14,     1] loss: 1253.984
[15,     1] loss: 1247.484
[16,     1] loss: 1234.637
[17,     1] loss: 1207.815
[18,     1] loss: 1179.738
[19,     1] loss: 1174.791
[20,     1] loss: 1111.915
[21,     1] loss: 1093.354
[22,     1] loss: 1076.632
[23,     1] loss: 1078.338
[24,     1] loss: 1031.298
[25,     1] loss: 1002.951
[26,     1] loss: 1048.223
[27,     1] loss: 1025.971
[28,     1] loss: 999.407
[29,     1] loss: 986.716
[30,     1] loss: 990.076
[31,     1] loss: 986.661
[32,     1] loss: 951.742
[33,     1] loss: 909.966
[34,     1] loss: 985.329
[35,     1] loss: 1408.760
[36,     1] loss: 919.313
[37,     1] loss: 1093.825
[38,     1] loss: 992.423
[39,     1] loss: 1056.387
[40,     1] loss: 1074.339
[41,     1] loss: 1019.997
[42,     1] loss: 1020.556
[43,     1] loss: 1001.341
[44,     1] loss: 961.273
[45,     1] loss: 968.243
[46,     1] loss: 963.864
[47,     1] loss: 896.387
[48,     1] loss: 953.570
[49,     1] loss: 891.663
[50,     1] loss: 960.078
[51,     1] loss: 869.135
[52,     1] loss: 917.135
[53,     1] loss: 889.089
[54,     1] loss: 878.981
[55,     1] loss: 834.091
[56,     1] loss: 820.984
[57,     1] loss: 789.279
[58,     1] loss: 809.626
[59,     1] loss: 1459.546
[60,     1] loss: 1428.327
[61,     1] loss: 1028.942
[62,     1] loss: 1136.187
[63,     1] loss: 1147.805
[64,     1] loss: 1145.624
[65,     1] loss: 1121.619
[66,     1] loss: 1092.500
[67,     1] loss: 1086.727
[68,     1] loss: 1076.833
[69,     1] loss: 1034.487
[70,     1] loss: 1036.962
[71,     1] loss: 1007.598
[72,     1] loss: 988.045
[73,     1] loss: 950.359
[74,     1] loss: 939.185
[75,     1] loss: 943.778
[76,     1] loss: 999.969
[77,     1] loss: 888.446
Early stopping applied (best metric=0.7142072916030884)
Finished Training
Total time taken: 12.933012962341309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1261.961
[2,     1] loss: 1267.840
[3,     1] loss: 1269.659
[4,     1] loss: 1262.020
[5,     1] loss: 1257.918
[6,     1] loss: 1266.180
[7,     1] loss: 1262.324
[8,     1] loss: 1262.824
[9,     1] loss: 1262.220
[10,     1] loss: 1261.567
[11,     1] loss: 1260.504
[12,     1] loss: 1259.680
[13,     1] loss: 1259.607
[14,     1] loss: 1258.163
[15,     1] loss: 1255.469
[16,     1] loss: 1251.499
[17,     1] loss: 1247.791
[18,     1] loss: 1250.699
[19,     1] loss: 1214.017
[20,     1] loss: 1244.509
[21,     1] loss: 1152.418
[22,     1] loss: 1180.567
[23,     1] loss: 1101.966
[24,     1] loss: 1101.151
[25,     1] loss: 1067.386
[26,     1] loss: 1100.900
[27,     1] loss: 1013.879
[28,     1] loss: 1014.531
[29,     1] loss: 1027.968
[30,     1] loss: 1027.426
[31,     1] loss: 1034.441
[32,     1] loss: 975.828
[33,     1] loss: 968.789
[34,     1] loss: 934.657
[35,     1] loss: 933.498
[36,     1] loss: 926.666
[37,     1] loss: 913.491
[38,     1] loss: 875.219
[39,     1] loss: 895.007
[40,     1] loss: 876.257
[41,     1] loss: 826.867
[42,     1] loss: 1032.612
[43,     1] loss: 1327.920
[44,     1] loss: 934.039
[45,     1] loss: 1051.261
[46,     1] loss: 1076.872
[47,     1] loss: 1061.951
[48,     1] loss: 1029.878
[49,     1] loss: 992.137
[50,     1] loss: 953.691
[51,     1] loss: 976.657
[52,     1] loss: 929.047
[53,     1] loss: 917.246
[54,     1] loss: 857.073
[55,     1] loss: 904.970
[56,     1] loss: 896.535
[57,     1] loss: 863.251
[58,     1] loss: 940.176
[59,     1] loss: 1067.990
[60,     1] loss: 866.215
[61,     1] loss: 993.753
[62,     1] loss: 903.473
[63,     1] loss: 962.225
[64,     1] loss: 882.801
[65,     1] loss: 897.978
[66,     1] loss: 862.886
[67,     1] loss: 886.820
[68,     1] loss: 815.649
[69,     1] loss: 815.730
[70,     1] loss: 968.085
[71,     1] loss: 897.031
[72,     1] loss: 806.659
[73,     1] loss: 810.514
[74,     1] loss: 796.704
[75,     1] loss: 765.996
[76,     1] loss: 880.514
[77,     1] loss: 1032.108
[78,     1] loss: 920.066
[79,     1] loss: 930.253
[80,     1] loss: 938.775
[81,     1] loss: 957.908
[82,     1] loss: 787.989
[83,     1] loss: 877.773
Early stopping applied (best metric=0.8038821816444397)
Finished Training
Total time taken: 13.09801459312439
{'Hydroxylation-K Validation Accuracy': 0.7466312056737588, 'Hydroxylation-K Validation Sensitivity': 0.6977777777777777, 'Hydroxylation-K Validation Specificity': 0.7596491228070176, 'Hydroxylation-K Validation Precision': 0.43341515553589854, 'Hydroxylation-K AUC ROC': 0.7997270955165692, 'Hydroxylation-K AUC PR': 0.5803690422655299, 'Hydroxylation-K MCC': 0.394796726751891, 'Hydroxylation-K F1': 0.5293857444670103, 'Validation Loss (Hydroxylation-K)': 0.4505927364031474, 'Hydroxylation-P Validation Accuracy': 0.7802016310508773, 'Hydroxylation-P Validation Sensitivity': 0.8008465608465608, 'Hydroxylation-P Validation Specificity': 0.7756646216768917, 'Hydroxylation-P Validation Precision': 0.44038427201617175, 'Hydroxylation-P AUC ROC': 0.8485680440064699, 'Hydroxylation-P AUC PR': 0.5885818046647026, 'Hydroxylation-P MCC': 0.47313066697846906, 'Hydroxylation-P F1': 0.5652845098905105, 'Validation Loss (Hydroxylation-P)': 0.38036043047904966, 'Validation Loss (total)': 0.8309531728426616, 'TimeToTrain': 10.67354416847229}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003230687552922461,
 'learning_rate_Hydroxylation-K': 0.0017783299077217256,
 'learning_rate_Hydroxylation-P': 0.003420408576891588,
 'log_base': 1.2300966131184756,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 947281342,
 'sample_weights': [1.669189474704591, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.152396397912863,
 'weight_decay_Hydroxylation-K': 1.4164539305694523,
 'weight_decay_Hydroxylation-P': 0.974105239857228}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2618.090
[2,     1] loss: 2613.914
[3,     1] loss: 2614.372
[4,     1] loss: 2614.239
[5,     1] loss: 2595.199
[6,     1] loss: 2599.186
[7,     1] loss: 2589.356
[8,     1] loss: 2579.358
[9,     1] loss: 2530.953
[10,     1] loss: 2475.441
[11,     1] loss: 2430.285
[12,     1] loss: 2389.144
[13,     1] loss: 2269.296
[14,     1] loss: 2277.035
[15,     1] loss: 2108.147
[16,     1] loss: 2129.941
[17,     1] loss: 2195.916
[18,     1] loss: 2133.977
[19,     1] loss: 2087.010
[20,     1] loss: 2104.386
[21,     1] loss: 2128.381
[22,     1] loss: 1994.116
[23,     1] loss: 2082.032
[24,     1] loss: 1942.017
[25,     1] loss: 1964.574
[26,     1] loss: 2110.836
[27,     1] loss: 1832.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021445532193298104,
 'learning_rate_Hydroxylation-K': 0.006503573456882468,
 'learning_rate_Hydroxylation-P': 0.005645872552844672,
 'log_base': 2.665297481174254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3744013890,
 'sample_weights': [8.06133213864627, 1.0077053366914461],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.06605268258368,
 'weight_decay_Hydroxylation-K': 2.3442673131500165,
 'weight_decay_Hydroxylation-P': 2.7475621202198894}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.422
[2,     1] loss: 1267.084
[3,     1] loss: 1265.731
[4,     1] loss: 1265.459
[5,     1] loss: 1260.802
[6,     1] loss: 1257.388
[7,     1] loss: 1246.655
[8,     1] loss: 1226.611
[9,     1] loss: 1209.329
[10,     1] loss: 1182.397
[11,     1] loss: 1161.786
[12,     1] loss: 1136.386
[13,     1] loss: 1093.315
[14,     1] loss: 1071.886
[15,     1] loss: 1055.984
[16,     1] loss: 1037.637
[17,     1] loss: 1056.130
[18,     1] loss: 1001.624
[19,     1] loss: 988.979
[20,     1] loss: 1053.342
[21,     1] loss: 996.871
[22,     1] loss: 969.387
[23,     1] loss: 1011.974
[24,     1] loss: 1025.433
[25,     1] loss: 987.298
[26,     1] loss: 1009.035
[27,     1] loss: 980.582
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037093589613481507,
 'learning_rate_Hydroxylation-K': 0.008235846044180409,
 'learning_rate_Hydroxylation-P': 0.0075373327941176856,
 'log_base': 2.7456930525367294,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3110422259,
 'sample_weights': [1.7029648584459025, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.936676341698032,
 'weight_decay_Hydroxylation-K': 2.3559664614463554,
 'weight_decay_Hydroxylation-P': 6.448289623635621}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.560
[2,     1] loss: 1255.895
[3,     1] loss: 1259.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027563074697625105,
 'learning_rate_Hydroxylation-K': 0.008311776617174192,
 'learning_rate_Hydroxylation-P': 0.006659436352030426,
 'log_base': 2.325197664948206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2023914011,
 'sample_weights': [1.6528591500191854, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.444415116205931,
 'weight_decay_Hydroxylation-K': 1.6096758717943902,
 'weight_decay_Hydroxylation-P': 2.8491249710293634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.846
[2,     1] loss: 1322.434
[3,     1] loss: 1328.885
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004577121191126854,
 'learning_rate_Hydroxylation-K': 0.000974917049771609,
 'learning_rate_Hydroxylation-P': 0.002004871126542043,
 'log_base': 1.1659411095591967,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 364233991,
 'sample_weights': [1.9784701955322943, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.495083916180417,
 'weight_decay_Hydroxylation-K': 0.13214994573130623,
 'weight_decay_Hydroxylation-P': 3.350488324118472}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3535.734
[2,     1] loss: 3530.303
[3,     1] loss: 3529.244
[4,     1] loss: 3522.808
[5,     1] loss: 3531.559
[6,     1] loss: 3510.530
[7,     1] loss: 3508.214
[8,     1] loss: 3481.692
[9,     1] loss: 3495.295
[10,     1] loss: 3469.317
[11,     1] loss: 3452.229
[12,     1] loss: 3390.812
[13,     1] loss: 3361.533
[14,     1] loss: 3259.291
[15,     1] loss: 3214.856
[16,     1] loss: 3163.916
[17,     1] loss: 2979.732
[18,     1] loss: 3090.048
[19,     1] loss: 2897.371
[20,     1] loss: 2990.509
[21,     1] loss: 2948.097
[22,     1] loss: 2957.261
[23,     1] loss: 2965.010
[24,     1] loss: 2773.279
[25,     1] loss: 2798.218
[26,     1] loss: 2661.498
[27,     1] loss: 2671.963
[28,     1] loss: 2721.279
[29,     1] loss: 2560.946
[30,     1] loss: 2648.185
[31,     1] loss: 2673.629
[32,     1] loss: 2555.136
[33,     1] loss: 2574.159
[34,     1] loss: 2369.026
[35,     1] loss: 2511.187
[36,     1] loss: 2397.436
[37,     1] loss: 2373.986
[38,     1] loss: 2300.801
[39,     1] loss: 2404.928
[40,     1] loss: 2652.318
[41,     1] loss: 2479.155
[42,     1] loss: 2225.628
[43,     1] loss: 2271.703
[44,     1] loss: 2153.902
[45,     1] loss: 2202.985
[46,     1] loss: 2029.443
[47,     1] loss: 2342.836
[48,     1] loss: 2311.987
[49,     1] loss: 2996.148
[50,     1] loss: 1953.296
[51,     1] loss: 2317.700
[52,     1] loss: 2277.702
[53,     1] loss: 2206.391
[54,     1] loss: 2199.929
[55,     1] loss: 2006.485
[56,     1] loss: 2101.214
[57,     1] loss: 1835.726
[58,     1] loss: 1870.147
[59,     1] loss: 2580.362
[60,     1] loss: 2339.814
[61,     1] loss: 1932.670
[62,     1] loss: 2034.587
[63,     1] loss: 1752.834
[64,     1] loss: 2207.007
[65,     1] loss: 1691.545
[66,     1] loss: 1912.190
[67,     1] loss: 1900.989
[68,     1] loss: 1889.013
[69,     1] loss: 1846.428
Early stopping applied (best metric=0.7175149917602539)
Finished Training
Total time taken: 9.374008417129517
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3530.270
[2,     1] loss: 3523.495
[3,     1] loss: 3542.795
[4,     1] loss: 3524.790
[5,     1] loss: 3533.833
[6,     1] loss: 3527.357
[7,     1] loss: 3530.805
[8,     1] loss: 3538.451
[9,     1] loss: 3516.317
[10,     1] loss: 3516.746
[11,     1] loss: 3520.322
[12,     1] loss: 3513.002
[13,     1] loss: 3521.801
[14,     1] loss: 3503.504
[15,     1] loss: 3513.358
[16,     1] loss: 3490.049
[17,     1] loss: 3485.581
[18,     1] loss: 3454.615
[19,     1] loss: 3410.261
[20,     1] loss: 3340.047
[21,     1] loss: 3257.724
[22,     1] loss: 3270.679
[23,     1] loss: 3135.834
[24,     1] loss: 3030.725
[25,     1] loss: 3013.570
[26,     1] loss: 2949.570
[27,     1] loss: 2882.055
[28,     1] loss: 2879.595
[29,     1] loss: 2859.836
[30,     1] loss: 2918.447
[31,     1] loss: 2774.524
[32,     1] loss: 2918.763
[33,     1] loss: 2839.305
[34,     1] loss: 2783.857
[35,     1] loss: 2751.964
[36,     1] loss: 2534.497
[37,     1] loss: 2667.264
[38,     1] loss: 2477.639
[39,     1] loss: 2463.918
[40,     1] loss: 2323.661
[41,     1] loss: 2426.718
[42,     1] loss: 2550.731
[43,     1] loss: 2786.185
[44,     1] loss: 2272.624
[45,     1] loss: 2663.114
[46,     1] loss: 2366.561
[47,     1] loss: 2427.561
[48,     1] loss: 2498.025
[49,     1] loss: 2392.147
[50,     1] loss: 2260.151
[51,     1] loss: 2497.583
[52,     1] loss: 2177.925
[53,     1] loss: 2206.843
[54,     1] loss: 2239.263
[55,     1] loss: 2180.762
[56,     1] loss: 2114.097
[57,     1] loss: 2204.414
Early stopping applied (best metric=0.8168078660964966)
Finished Training
Total time taken: 9.580008029937744
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3550.282
[2,     1] loss: 3556.836
[3,     1] loss: 3532.830
[4,     1] loss: 3536.391
[5,     1] loss: 3534.617
[6,     1] loss: 3528.716
[7,     1] loss: 3514.059
[8,     1] loss: 3510.639
[9,     1] loss: 3559.708
[10,     1] loss: 3528.812
[11,     1] loss: 3522.410
[12,     1] loss: 3530.774
[13,     1] loss: 3523.865
[14,     1] loss: 3527.043
[15,     1] loss: 3518.655
[16,     1] loss: 3522.424
[17,     1] loss: 3528.954
[18,     1] loss: 3532.321
[19,     1] loss: 3522.742
[20,     1] loss: 3518.203
[21,     1] loss: 3520.699
[22,     1] loss: 3521.906
[23,     1] loss: 3520.285
[24,     1] loss: 3512.784
[25,     1] loss: 3507.834
[26,     1] loss: 3494.283
[27,     1] loss: 3482.250
[28,     1] loss: 3453.890
[29,     1] loss: 3401.214
[30,     1] loss: 3395.273
[31,     1] loss: 3344.526
[32,     1] loss: 3302.958
[33,     1] loss: 3191.698
[34,     1] loss: 3280.295
[35,     1] loss: 3119.191
[36,     1] loss: 2999.501
[37,     1] loss: 3323.053
[38,     1] loss: 2955.564
[39,     1] loss: 2978.828
[40,     1] loss: 3030.068
[41,     1] loss: 3010.647
[42,     1] loss: 2931.602
[43,     1] loss: 2956.962
[44,     1] loss: 2721.482
[45,     1] loss: 2790.378
[46,     1] loss: 2766.108
[47,     1] loss: 2952.381
[48,     1] loss: 2571.249
[49,     1] loss: 3067.813
[50,     1] loss: 3006.873
[51,     1] loss: 2570.892
[52,     1] loss: 3098.681
[53,     1] loss: 2689.187
[54,     1] loss: 2895.068
[55,     1] loss: 2818.622
[56,     1] loss: 2560.131
[57,     1] loss: 2594.416
[58,     1] loss: 2660.130
[59,     1] loss: 2443.700
[60,     1] loss: 2381.286
[61,     1] loss: 2410.190
[62,     1] loss: 2381.143
[63,     1] loss: 2242.598
[64,     1] loss: 2409.918
[65,     1] loss: 2296.624
[66,     1] loss: 2223.236
[67,     1] loss: 2651.378
[68,     1] loss: 2498.532
[69,     1] loss: 2541.990
[70,     1] loss: 2194.451
[71,     1] loss: 2710.139
[72,     1] loss: 2358.274
[73,     1] loss: 2424.168
[74,     1] loss: 2421.676
[75,     1] loss: 2398.254
[76,     1] loss: 2333.563
[77,     1] loss: 2202.129
[78,     1] loss: 2470.702
[79,     1] loss: 2068.177
[80,     1] loss: 2572.999
[81,     1] loss: 1949.184
Early stopping applied (best metric=0.6732849478721619)
Finished Training
Total time taken: 10.984010457992554
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3536.160
[2,     1] loss: 3525.148
[3,     1] loss: 3544.244
[4,     1] loss: 3508.577
[5,     1] loss: 3515.662
[6,     1] loss: 3529.470
[7,     1] loss: 3521.823
[8,     1] loss: 3515.482
[9,     1] loss: 3491.983
[10,     1] loss: 3505.791
[11,     1] loss: 3475.354
[12,     1] loss: 3412.779
[13,     1] loss: 3428.923
[14,     1] loss: 3292.984
[15,     1] loss: 3279.520
[16,     1] loss: 3260.052
[17,     1] loss: 3216.163
[18,     1] loss: 3022.593
[19,     1] loss: 2902.034
[20,     1] loss: 3096.871
[21,     1] loss: 2890.513
[22,     1] loss: 2864.803
[23,     1] loss: 3061.111
[24,     1] loss: 2817.059
[25,     1] loss: 2918.661
[26,     1] loss: 2896.347
[27,     1] loss: 2849.130
[28,     1] loss: 2851.163
[29,     1] loss: 2711.307
[30,     1] loss: 2740.718
[31,     1] loss: 2749.884
[32,     1] loss: 2671.740
[33,     1] loss: 2552.609
[34,     1] loss: 2500.271
[35,     1] loss: 2620.472
[36,     1] loss: 2462.201
[37,     1] loss: 2457.183
[38,     1] loss: 2780.105
[39,     1] loss: 2384.292
[40,     1] loss: 2502.953
[41,     1] loss: 2434.916
[42,     1] loss: 2493.567
[43,     1] loss: 2402.116
[44,     1] loss: 2331.880
[45,     1] loss: 2380.462
[46,     1] loss: 2295.138
[47,     1] loss: 2164.699
[48,     1] loss: 2236.200
[49,     1] loss: 2088.631
[50,     1] loss: 2574.397
[51,     1] loss: 2344.802
[52,     1] loss: 2236.320
[53,     1] loss: 2856.978
[54,     1] loss: 1981.360
[55,     1] loss: 2179.972
[56,     1] loss: 2233.814
[57,     1] loss: 2109.284
[58,     1] loss: 2170.509
[59,     1] loss: 2100.702
[60,     1] loss: 2085.839
[61,     1] loss: 2174.396
[62,     1] loss: 2015.741
[63,     1] loss: 2169.419
[64,     1] loss: 1921.771
[65,     1] loss: 1853.339
[66,     1] loss: 1815.756
[67,     1] loss: 1729.945
[68,     1] loss: 1640.212
[69,     1] loss: 1613.570
[70,     1] loss: 1584.099
[71,     1] loss: 1719.238
[72,     1] loss: 2828.142
[73,     1] loss: 2755.941
[74,     1] loss: 1669.195
[75,     1] loss: 1914.728
[76,     1] loss: 2069.101
[77,     1] loss: 2075.531
Early stopping applied (best metric=0.8302971124649048)
Finished Training
Total time taken: 12.829014778137207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3521.230
[2,     1] loss: 3536.749
[3,     1] loss: 3512.276
[4,     1] loss: 3495.548
[5,     1] loss: 3514.926
[6,     1] loss: 3508.509
[7,     1] loss: 3530.412
[8,     1] loss: 3520.285
[9,     1] loss: 3483.681
[10,     1] loss: 3502.041
[11,     1] loss: 3486.289
[12,     1] loss: 3422.530
[13,     1] loss: 3419.859
[14,     1] loss: 3301.359
[15,     1] loss: 3141.036
[16,     1] loss: 3135.268
[17,     1] loss: 3031.779
[18,     1] loss: 3042.412
[19,     1] loss: 2895.551
[20,     1] loss: 2736.658
[21,     1] loss: 2980.978
[22,     1] loss: 3095.584
[23,     1] loss: 3265.667
[24,     1] loss: 2708.522
[25,     1] loss: 2849.673
[26,     1] loss: 2885.875
[27,     1] loss: 2855.782
[28,     1] loss: 2877.931
[29,     1] loss: 2653.778
[30,     1] loss: 2844.728
[31,     1] loss: 2824.973
[32,     1] loss: 2501.859
[33,     1] loss: 2611.654
[34,     1] loss: 2554.823
[35,     1] loss: 2361.946
[36,     1] loss: 2313.875
[37,     1] loss: 2452.817
[38,     1] loss: 2721.089
[39,     1] loss: 2236.552
[40,     1] loss: 2670.751
[41,     1] loss: 2410.775
[42,     1] loss: 2389.404
[43,     1] loss: 2171.930
[44,     1] loss: 2408.485
[45,     1] loss: 2201.155
[46,     1] loss: 2236.499
[47,     1] loss: 2263.710
[48,     1] loss: 2143.383
[49,     1] loss: 2243.381
[50,     1] loss: 2241.301
[51,     1] loss: 2109.752
[52,     1] loss: 2040.011
[53,     1] loss: 1877.231
[54,     1] loss: 1992.121
[55,     1] loss: 1986.644
[56,     1] loss: 2080.533
[57,     1] loss: 1892.376
[58,     1] loss: 2022.755
[59,     1] loss: 1810.714
[60,     1] loss: 2144.357
[61,     1] loss: 2239.327
[62,     1] loss: 1908.507
[63,     1] loss: 2234.600
Early stopping applied (best metric=0.8195825815200806)
Finished Training
Total time taken: 10.490010261535645
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3537.236
[2,     1] loss: 3523.021
[3,     1] loss: 3530.758
[4,     1] loss: 3531.406
[5,     1] loss: 3528.452
[6,     1] loss: 3518.808
[7,     1] loss: 3522.853
[8,     1] loss: 3506.619
[9,     1] loss: 3486.606
[10,     1] loss: 3494.650
[11,     1] loss: 3451.338
[12,     1] loss: 3395.281
[13,     1] loss: 3385.209
[14,     1] loss: 3306.480
[15,     1] loss: 3264.117
[16,     1] loss: 3173.859
[17,     1] loss: 3057.634
[18,     1] loss: 3061.901
[19,     1] loss: 2947.189
[20,     1] loss: 2917.667
[21,     1] loss: 2896.363
[22,     1] loss: 2832.190
[23,     1] loss: 2950.946
[24,     1] loss: 2781.996
[25,     1] loss: 2838.975
[26,     1] loss: 2609.165
[27,     1] loss: 2613.234
[28,     1] loss: 2954.465
[29,     1] loss: 3097.319
[30,     1] loss: 2567.591
[31,     1] loss: 2691.441
[32,     1] loss: 2625.771
[33,     1] loss: 2549.401
[34,     1] loss: 2620.960
[35,     1] loss: 2528.699
[36,     1] loss: 2524.578
[37,     1] loss: 2690.882
[38,     1] loss: 2512.387
[39,     1] loss: 2571.298
[40,     1] loss: 2387.758
[41,     1] loss: 2425.218
[42,     1] loss: 2276.955
[43,     1] loss: 2285.069
[44,     1] loss: 2379.304
[45,     1] loss: 2053.660
[46,     1] loss: 2052.540
[47,     1] loss: 2098.562
[48,     1] loss: 2276.038
[49,     1] loss: 1993.319
[50,     1] loss: 1961.736
[51,     1] loss: 2229.070
[52,     1] loss: 2139.506
[53,     1] loss: 2073.939
[54,     1] loss: 2157.516
[55,     1] loss: 1997.456
[56,     1] loss: 1951.470
[57,     1] loss: 1693.301
[58,     1] loss: 1861.895
[59,     1] loss: 1801.008
Early stopping applied (best metric=0.8567862510681152)
Finished Training
Total time taken: 9.721009731292725
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3549.614
[2,     1] loss: 3531.339
[3,     1] loss: 3535.855
[4,     1] loss: 3530.777
[5,     1] loss: 3525.075
[6,     1] loss: 3516.360
[7,     1] loss: 3520.927
[8,     1] loss: 3508.086
[9,     1] loss: 3511.797
[10,     1] loss: 3480.501
[11,     1] loss: 3485.174
[12,     1] loss: 3439.612
[13,     1] loss: 3410.249
[14,     1] loss: 3365.466
[15,     1] loss: 3297.019
[16,     1] loss: 3239.118
[17,     1] loss: 3199.996
[18,     1] loss: 2996.906
[19,     1] loss: 3130.688
[20,     1] loss: 3065.252
[21,     1] loss: 2932.708
[22,     1] loss: 2981.807
[23,     1] loss: 3023.849
[24,     1] loss: 2930.934
[25,     1] loss: 2936.385
[26,     1] loss: 2705.373
[27,     1] loss: 2806.061
[28,     1] loss: 2818.103
[29,     1] loss: 2717.074
[30,     1] loss: 2597.442
[31,     1] loss: 2675.508
[32,     1] loss: 2689.813
[33,     1] loss: 2752.919
[34,     1] loss: 2544.110
[35,     1] loss: 2559.115
[36,     1] loss: 2418.619
[37,     1] loss: 2472.028
[38,     1] loss: 2470.433
[39,     1] loss: 2335.368
[40,     1] loss: 2331.193
[41,     1] loss: 2731.113
[42,     1] loss: 2820.418
[43,     1] loss: 2338.547
[44,     1] loss: 2871.213
[45,     1] loss: 2402.106
[46,     1] loss: 2586.542
[47,     1] loss: 2566.411
[48,     1] loss: 2384.698
[49,     1] loss: 2332.706
[50,     1] loss: 2399.624
Early stopping applied (best metric=0.8488717675209045)
Finished Training
Total time taken: 8.363008260726929
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3529.980
[2,     1] loss: 3538.298
[3,     1] loss: 3510.190
[4,     1] loss: 3520.860
[5,     1] loss: 3523.402
[6,     1] loss: 3542.860
[7,     1] loss: 3540.994
[8,     1] loss: 3517.836
[9,     1] loss: 3522.159
[10,     1] loss: 3521.298
[11,     1] loss: 3525.521
[12,     1] loss: 3529.584
[13,     1] loss: 3523.176
[14,     1] loss: 3520.434
[15,     1] loss: 3512.653
[16,     1] loss: 3497.777
[17,     1] loss: 3470.987
[18,     1] loss: 3474.145
[19,     1] loss: 3413.052
[20,     1] loss: 3369.793
[21,     1] loss: 3348.440
[22,     1] loss: 3259.568
[23,     1] loss: 3152.462
[24,     1] loss: 3240.292
[25,     1] loss: 3178.354
[26,     1] loss: 3160.760
[27,     1] loss: 2970.857
[28,     1] loss: 3089.261
[29,     1] loss: 2998.346
[30,     1] loss: 2816.646
[31,     1] loss: 2828.732
[32,     1] loss: 2705.259
[33,     1] loss: 2845.084
[34,     1] loss: 2847.375
[35,     1] loss: 2667.207
[36,     1] loss: 2657.109
[37,     1] loss: 2877.392
[38,     1] loss: 2713.750
[39,     1] loss: 2617.237
[40,     1] loss: 2665.792
[41,     1] loss: 2457.556
[42,     1] loss: 2396.893
[43,     1] loss: 2474.067
[44,     1] loss: 2523.617
[45,     1] loss: 2422.272
[46,     1] loss: 2447.077
[47,     1] loss: 2385.868
[48,     1] loss: 2362.911
[49,     1] loss: 2515.751
[50,     1] loss: 2430.591
[51,     1] loss: 2414.640
[52,     1] loss: 2229.754
[53,     1] loss: 2175.290
[54,     1] loss: 2319.457
[55,     1] loss: 2143.176
[56,     1] loss: 2413.310
[57,     1] loss: 2672.561
[58,     1] loss: 2329.164
[59,     1] loss: 2084.259
[60,     1] loss: 1986.189
Early stopping applied (best metric=0.873038649559021)
Finished Training
Total time taken: 8.129010677337646
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3535.582
[2,     1] loss: 3525.074
[3,     1] loss: 3527.220
[4,     1] loss: 3529.460
[5,     1] loss: 3509.851
[6,     1] loss: 3528.633
[7,     1] loss: 3510.809
[8,     1] loss: 3516.977
[9,     1] loss: 3505.149
[10,     1] loss: 3475.456
[11,     1] loss: 3479.081
[12,     1] loss: 3383.512
[13,     1] loss: 3364.465
[14,     1] loss: 3214.702
[15,     1] loss: 3162.269
[16,     1] loss: 3159.414
[17,     1] loss: 3068.366
[18,     1] loss: 2883.061
[19,     1] loss: 2816.240
[20,     1] loss: 2743.477
[21,     1] loss: 3175.623
[22,     1] loss: 2886.090
[23,     1] loss: 2835.786
[24,     1] loss: 3021.534
[25,     1] loss: 2765.376
[26,     1] loss: 2707.969
[27,     1] loss: 2777.418
[28,     1] loss: 2645.258
[29,     1] loss: 2594.625
[30,     1] loss: 2744.414
[31,     1] loss: 2661.430
[32,     1] loss: 2695.678
[33,     1] loss: 2423.222
[34,     1] loss: 2456.135
[35,     1] loss: 2508.251
[36,     1] loss: 2639.277
[37,     1] loss: 2466.654
[38,     1] loss: 2441.992
[39,     1] loss: 2372.170
[40,     1] loss: 2169.112
[41,     1] loss: 2549.707
[42,     1] loss: 2236.848
[43,     1] loss: 2662.236
[44,     1] loss: 2286.782
[45,     1] loss: 2236.604
[46,     1] loss: 2092.623
[47,     1] loss: 2524.583
[48,     1] loss: 2043.262
[49,     1] loss: 2501.723
[50,     1] loss: 2392.145
[51,     1] loss: 2235.964
[52,     1] loss: 2408.876
[53,     1] loss: 2094.062
[54,     1] loss: 2310.547
[55,     1] loss: 1972.647
[56,     1] loss: 1995.957
[57,     1] loss: 2244.632
Early stopping applied (best metric=0.8548282980918884)
Finished Training
Total time taken: 8.569006204605103
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3559.727
[2,     1] loss: 3545.145
[3,     1] loss: 3530.979
[4,     1] loss: 3536.833
[5,     1] loss: 3526.412
[6,     1] loss: 3541.917
[7,     1] loss: 3520.407
[8,     1] loss: 3527.787
[9,     1] loss: 3520.117
[10,     1] loss: 3519.294
[11,     1] loss: 3507.481
[12,     1] loss: 3506.260
[13,     1] loss: 3504.831
[14,     1] loss: 3506.740
[15,     1] loss: 3480.908
[16,     1] loss: 3445.949
[17,     1] loss: 3466.590
[18,     1] loss: 3340.711
[19,     1] loss: 3287.487
[20,     1] loss: 3197.753
[21,     1] loss: 3088.733
[22,     1] loss: 3114.064
[23,     1] loss: 2949.289
[24,     1] loss: 3038.580
[25,     1] loss: 2870.868
[26,     1] loss: 2859.010
[27,     1] loss: 3028.374
[28,     1] loss: 2869.720
[29,     1] loss: 2699.705
[30,     1] loss: 2881.842
[31,     1] loss: 2684.177
[32,     1] loss: 2868.687
[33,     1] loss: 2524.216
[34,     1] loss: 2778.024
[35,     1] loss: 2646.630
[36,     1] loss: 2404.531
[37,     1] loss: 2685.581
[38,     1] loss: 2779.488
[39,     1] loss: 2339.421
[40,     1] loss: 2777.430
[41,     1] loss: 2459.938
[42,     1] loss: 2560.575
[43,     1] loss: 2186.519
[44,     1] loss: 2445.337
[45,     1] loss: 2317.497
[46,     1] loss: 2516.536
[47,     1] loss: 2005.849
[48,     1] loss: 2373.433
[49,     1] loss: 2541.021
[50,     1] loss: 2180.611
[51,     1] loss: 2230.920
[52,     1] loss: 2321.420
[53,     1] loss: 2419.163
[54,     1] loss: 2162.395
[55,     1] loss: 2684.587
[56,     1] loss: 2037.755
[57,     1] loss: 2320.385
[58,     1] loss: 2108.789
[59,     1] loss: 2175.330
[60,     1] loss: 1990.089
[61,     1] loss: 1877.464
Early stopping applied (best metric=0.8261620998382568)
Finished Training
Total time taken: 9.212009191513062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3527.280
[2,     1] loss: 3542.046
[3,     1] loss: 3523.051
[4,     1] loss: 3518.257
[5,     1] loss: 3514.342
[6,     1] loss: 3552.435
[7,     1] loss: 3511.986
[8,     1] loss: 3509.115
[9,     1] loss: 3530.321
[10,     1] loss: 3526.392
[11,     1] loss: 3500.240
[12,     1] loss: 3481.615
[13,     1] loss: 3493.522
[14,     1] loss: 3455.756
[15,     1] loss: 3370.837
[16,     1] loss: 3344.549
[17,     1] loss: 3222.782
[18,     1] loss: 3186.179
[19,     1] loss: 3083.232
[20,     1] loss: 3179.881
[21,     1] loss: 3015.902
[22,     1] loss: 3002.336
[23,     1] loss: 2976.918
[24,     1] loss: 2989.765
[25,     1] loss: 2950.271
[26,     1] loss: 2807.072
[27,     1] loss: 2863.613
[28,     1] loss: 2857.446
[29,     1] loss: 2698.452
[30,     1] loss: 2591.939
[31,     1] loss: 2604.994
[32,     1] loss: 2537.310
[33,     1] loss: 2633.200
[34,     1] loss: 2640.962
[35,     1] loss: 2434.843
[36,     1] loss: 2256.309
[37,     1] loss: 2736.278
[38,     1] loss: 2171.095
[39,     1] loss: 2510.758
[40,     1] loss: 2337.328
[41,     1] loss: 2422.526
[42,     1] loss: 2251.408
[43,     1] loss: 2480.383
[44,     1] loss: 2082.481
[45,     1] loss: 2265.604
[46,     1] loss: 2025.361
[47,     1] loss: 2272.392
[48,     1] loss: 2005.689
[49,     1] loss: 1964.420
[50,     1] loss: 2055.668
[51,     1] loss: 2048.867
[52,     1] loss: 1914.788
[53,     1] loss: 1830.701
[54,     1] loss: 1985.476
[55,     1] loss: 1845.532
[56,     1] loss: 1840.583
[57,     1] loss: 1946.680
[58,     1] loss: 1868.890
[59,     1] loss: 1732.260
[60,     1] loss: 1702.292
[61,     1] loss: 1710.308
Early stopping applied (best metric=0.9760383367538452)
Finished Training
Total time taken: 8.73900818824768
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3526.637
[2,     1] loss: 3525.609
[3,     1] loss: 3592.109
[4,     1] loss: 3521.307
[5,     1] loss: 3493.351
[6,     1] loss: 3549.250
[7,     1] loss: 3534.072
[8,     1] loss: 3548.490
[9,     1] loss: 3562.369
[10,     1] loss: 3537.512
[11,     1] loss: 3520.128
[12,     1] loss: 3541.696
[13,     1] loss: 3523.593
[14,     1] loss: 3530.177
[15,     1] loss: 3522.879
[16,     1] loss: 3519.032
[17,     1] loss: 3520.175
[18,     1] loss: 3524.200
[19,     1] loss: 3523.650
[20,     1] loss: 3515.666
[21,     1] loss: 3520.071
[22,     1] loss: 3511.706
[23,     1] loss: 3516.864
[24,     1] loss: 3504.036
[25,     1] loss: 3494.033
[26,     1] loss: 3480.558
[27,     1] loss: 3455.926
[28,     1] loss: 3424.509
[29,     1] loss: 3371.568
[30,     1] loss: 3320.892
[31,     1] loss: 3266.813
[32,     1] loss: 3209.798
[33,     1] loss: 3185.650
[34,     1] loss: 3185.009
[35,     1] loss: 3128.382
[36,     1] loss: 3014.963
[37,     1] loss: 3035.670
[38,     1] loss: 3026.515
[39,     1] loss: 3007.399
[40,     1] loss: 2988.083
[41,     1] loss: 2947.123
[42,     1] loss: 2677.133
[43,     1] loss: 2799.831
[44,     1] loss: 2804.825
[45,     1] loss: 2875.964
[46,     1] loss: 2632.743
[47,     1] loss: 2659.466
[48,     1] loss: 2798.494
[49,     1] loss: 2494.204
[50,     1] loss: 2794.740
[51,     1] loss: 2501.615
[52,     1] loss: 2568.815
[53,     1] loss: 2501.351
[54,     1] loss: 2452.952
[55,     1] loss: 2462.152
[56,     1] loss: 2385.161
[57,     1] loss: 2301.727
[58,     1] loss: 2333.762
[59,     1] loss: 2366.608
[60,     1] loss: 2179.117
[61,     1] loss: 2224.268
[62,     1] loss: 2364.768
[63,     1] loss: 2119.188
Early stopping applied (best metric=0.9072244167327881)
Finished Training
Total time taken: 8.943008422851562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3538.796
[2,     1] loss: 3505.636
[3,     1] loss: 3529.351
[4,     1] loss: 3500.797
[5,     1] loss: 3506.708
[6,     1] loss: 3507.163
[7,     1] loss: 3501.939
[8,     1] loss: 3450.384
[9,     1] loss: 3399.452
[10,     1] loss: 3325.516
[11,     1] loss: 3218.503
[12,     1] loss: 3174.795
[13,     1] loss: 3116.740
[14,     1] loss: 3018.164
[15,     1] loss: 2946.186
[16,     1] loss: 2931.679
[17,     1] loss: 2912.885
[18,     1] loss: 2966.085
[19,     1] loss: 2841.662
[20,     1] loss: 2983.436
[21,     1] loss: 2863.794
[22,     1] loss: 2727.203
[23,     1] loss: 2735.288
[24,     1] loss: 2725.387
[25,     1] loss: 2622.833
[26,     1] loss: 2628.872
[27,     1] loss: 2442.073
[28,     1] loss: 2379.232
[29,     1] loss: 2327.784
[30,     1] loss: 2363.112
[31,     1] loss: 2681.744
[32,     1] loss: 2212.930
[33,     1] loss: 2510.519
[34,     1] loss: 2457.243
[35,     1] loss: 2418.131
[36,     1] loss: 2245.963
[37,     1] loss: 2658.339
[38,     1] loss: 2237.086
[39,     1] loss: 2166.295
[40,     1] loss: 2107.955
[41,     1] loss: 2227.635
Early stopping applied (best metric=0.9497592449188232)
Finished Training
Total time taken: 6.2520060539245605
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3524.615
[2,     1] loss: 3528.268
[3,     1] loss: 3523.999
[4,     1] loss: 3521.938
[5,     1] loss: 3535.390
[6,     1] loss: 3516.191
[7,     1] loss: 3509.125
[8,     1] loss: 3512.010
[9,     1] loss: 3509.912
[10,     1] loss: 3491.539
[11,     1] loss: 3473.810
[12,     1] loss: 3475.417
[13,     1] loss: 3449.101
[14,     1] loss: 3387.309
[15,     1] loss: 3266.699
[16,     1] loss: 3243.180
[17,     1] loss: 3124.765
[18,     1] loss: 3200.084
[19,     1] loss: 3140.525
[20,     1] loss: 3070.720
[21,     1] loss: 3060.998
[22,     1] loss: 3005.465
[23,     1] loss: 2979.163
[24,     1] loss: 2949.426
[25,     1] loss: 2881.085
[26,     1] loss: 2953.757
[27,     1] loss: 2784.505
[28,     1] loss: 2619.414
[29,     1] loss: 2755.892
[30,     1] loss: 2877.095
[31,     1] loss: 2507.275
[32,     1] loss: 2544.521
[33,     1] loss: 2523.661
[34,     1] loss: 2713.424
[35,     1] loss: 2507.970
[36,     1] loss: 2519.924
[37,     1] loss: 2369.724
[38,     1] loss: 2682.835
[39,     1] loss: 2263.868
[40,     1] loss: 2700.855
[41,     1] loss: 2312.126
[42,     1] loss: 2336.672
[43,     1] loss: 2293.587
[44,     1] loss: 2516.418
[45,     1] loss: 2196.869
[46,     1] loss: 2535.297
[47,     1] loss: 2206.858
[48,     1] loss: 2415.576
[49,     1] loss: 2075.728
[50,     1] loss: 2629.861
[51,     1] loss: 2140.140
[52,     1] loss: 2328.764
[53,     1] loss: 2163.437
[54,     1] loss: 2385.391
[55,     1] loss: 2109.747
Early stopping applied (best metric=0.7921895980834961)
Finished Training
Total time taken: 7.892010688781738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3580.898
[2,     1] loss: 3538.347
[3,     1] loss: 3546.837
[4,     1] loss: 3518.339
[5,     1] loss: 3527.614
[6,     1] loss: 3552.233
[7,     1] loss: 3528.554
[8,     1] loss: 3534.739
[9,     1] loss: 3531.363
[10,     1] loss: 3527.083
[11,     1] loss: 3519.569
[12,     1] loss: 3526.438
[13,     1] loss: 3526.008
[14,     1] loss: 3523.444
[15,     1] loss: 3518.505
[16,     1] loss: 3507.888
[17,     1] loss: 3498.225
[18,     1] loss: 3483.618
[19,     1] loss: 3465.917
[20,     1] loss: 3430.522
[21,     1] loss: 3352.375
[22,     1] loss: 3348.957
[23,     1] loss: 3308.207
[24,     1] loss: 3297.393
[25,     1] loss: 3232.297
[26,     1] loss: 3215.341
[27,     1] loss: 3073.090
[28,     1] loss: 3022.118
[29,     1] loss: 3053.389
[30,     1] loss: 2948.968
[31,     1] loss: 3036.762
[32,     1] loss: 2857.236
[33,     1] loss: 2946.657
[34,     1] loss: 2891.196
[35,     1] loss: 2974.546
[36,     1] loss: 3034.154
[37,     1] loss: 2790.569
[38,     1] loss: 2767.096
[39,     1] loss: 2684.086
[40,     1] loss: 2659.330
[41,     1] loss: 2797.035
[42,     1] loss: 2657.011
[43,     1] loss: 2663.054
[44,     1] loss: 2537.906
[45,     1] loss: 2670.384
[46,     1] loss: 2286.043
[47,     1] loss: 2595.515
[48,     1] loss: 2618.817
[49,     1] loss: 2302.379
[50,     1] loss: 2528.197
[51,     1] loss: 2351.252
[52,     1] loss: 2168.344
[53,     1] loss: 2047.939
[54,     1] loss: 2153.657
[55,     1] loss: 2210.051
[56,     1] loss: 2136.729
[57,     1] loss: 2278.613
[58,     1] loss: 3230.646
[59,     1] loss: 2674.265
[60,     1] loss: 2511.019
[61,     1] loss: 2234.796
[62,     1] loss: 2523.169
[63,     1] loss: 2493.312
[64,     1] loss: 2383.455
[65,     1] loss: 2422.441
[66,     1] loss: 2398.717
[67,     1] loss: 2388.233
[68,     1] loss: 2311.893
Early stopping applied (best metric=0.7191838026046753)
Finished Training
Total time taken: 9.59801173210144
{'Hydroxylation-K Validation Accuracy': 0.7772754137115839, 'Hydroxylation-K Validation Sensitivity': 0.7577777777777778, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.4689853719265484, 'Hydroxylation-K AUC ROC': 0.8470760233918129, 'Hydroxylation-K AUC PR': 0.6505399268961874, 'Hydroxylation-K MCC': 0.4626765999671831, 'Hydroxylation-K F1': 0.5783571576035345, 'Validation Loss (Hydroxylation-K)': 0.4000117580095927, 'Hydroxylation-P Validation Accuracy': 0.7540560208449656, 'Hydroxylation-P Validation Sensitivity': 0.704973544973545, 'Hydroxylation-P Validation Specificity': 0.7647289141603072, 'Hydroxylation-P Validation Precision': 0.3973784635359536, 'Hydroxylation-P AUC ROC': 0.7907913607692898, 'Hydroxylation-P AUC PR': 0.5396098364924957, 'Hydroxylation-P MCC': 0.3881577066796387, 'Hydroxylation-P F1': 0.5045220047867447, 'Validation Loss (Hydroxylation-P)': 0.4307595690091451, 'Validation Loss (total)': 0.8307713309923808, 'TimeToTrain': 9.245009406407673}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008797184264253737,
 'learning_rate_Hydroxylation-K': 0.007674565130505511,
 'learning_rate_Hydroxylation-P': 0.004468335995982647,
 'log_base': 2.5749014241006956,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 827550006,
 'sample_weights': [10.881892705072946, 1.3574102568309878],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.022715710992065,
 'weight_decay_Hydroxylation-K': 8.125663335505545,
 'weight_decay_Hydroxylation-P': 1.684394230124002}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.260
[2,     1] loss: 1280.268
[3,     1] loss: 1281.499
[4,     1] loss: 1279.754
[5,     1] loss: 1278.986
[6,     1] loss: 1280.879
[7,     1] loss: 1281.457
[8,     1] loss: 1278.983
[9,     1] loss: 1276.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037141114434782927,
 'learning_rate_Hydroxylation-K': 0.003741509129715241,
 'learning_rate_Hydroxylation-P': 0.004249240911531819,
 'log_base': 2.8754308882163815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2634441295,
 'sample_weights': [1.7650912323265684, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.5597286508037875,
 'weight_decay_Hydroxylation-K': 6.716112470792947,
 'weight_decay_Hydroxylation-P': 4.7322068543322215}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.154
[2,     1] loss: 1242.473
[3,     1] loss: 1241.855
[4,     1] loss: 1239.739
[5,     1] loss: 1240.936
[6,     1] loss: 1238.336
[7,     1] loss: 1232.357
[8,     1] loss: 1218.709
[9,     1] loss: 1200.839
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005870059839421055,
 'learning_rate_Hydroxylation-K': 0.0016632356996122075,
 'learning_rate_Hydroxylation-P': 0.003550614996822353,
 'log_base': 1.276369240020292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1760702673,
 'sample_weights': [1.5806089160499293, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.817342718664079,
 'weight_decay_Hydroxylation-K': 0.1847414987545396,
 'weight_decay_Hydroxylation-P': 6.0669590181870365}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2354.909
[2,     1] loss: 2358.405
[3,     1] loss: 2340.250
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00520767910275136,
 'learning_rate_Hydroxylation-K': 0.0043938524615464,
 'learning_rate_Hydroxylation-P': 0.001981559809523134,
 'log_base': 1.384970851818304,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2396718333,
 'sample_weights': [6.8414329065791355, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.928607052918277,
 'weight_decay_Hydroxylation-K': 0.9610285643805665,
 'weight_decay_Hydroxylation-P': 2.7879523144518834}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1992.573
[2,     1] loss: 1987.041
[3,     1] loss: 2004.329
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00539353456031358,
 'learning_rate_Hydroxylation-K': 0.0006720706526747145,
 'learning_rate_Hydroxylation-P': 0.0009649656846905338,
 'log_base': 1.4213204513326023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 578618227,
 'sample_weights': [5.126037191851611, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.043392885136654,
 'weight_decay_Hydroxylation-K': 0.9825462723219556,
 'weight_decay_Hydroxylation-P': 4.553080511353883}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1925.259
[2,     1] loss: 1916.234
[3,     1] loss: 1907.638
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009967468030876319,
 'learning_rate_Hydroxylation-K': 0.007769846759863629,
 'learning_rate_Hydroxylation-P': 0.008103822491962124,
 'log_base': 2.0172454052301294,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1039989879,
 'sample_weights': [4.748316365072999, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.339031768558406,
 'weight_decay_Hydroxylation-K': 2.3934382981482223,
 'weight_decay_Hydroxylation-P': 4.774338963758852}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.127
[2,     1] loss: 1443.863
[3,     1] loss: 1416.179
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009885683342427776,
 'learning_rate_Hydroxylation-K': 0.009280082965561019,
 'learning_rate_Hydroxylation-P': 0.007843914615339603,
 'log_base': 2.3869539278836593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2349013470,
 'sample_weights': [2.379029257189588, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.796892505124167,
 'weight_decay_Hydroxylation-K': 9.573497562121926,
 'weight_decay_Hydroxylation-P': 4.145367729412664}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1313.867
[2,     1] loss: 1318.189
[3,     1] loss: 1317.691
[4,     1] loss: 1313.789
[5,     1] loss: 1314.562
[6,     1] loss: 1310.505
[7,     1] loss: 1313.805
[8,     1] loss: 1311.936
[9,     1] loss: 1311.224
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004866121083240691,
 'learning_rate_Hydroxylation-K': 0.004123430067322356,
 'learning_rate_Hydroxylation-P': 0.0031123964522180255,
 'log_base': 1.1418341577056692,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3897673304,
 'sample_weights': [1.918860367641813, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.826053716415724,
 'weight_decay_Hydroxylation-K': 0.5150752850697944,
 'weight_decay_Hydroxylation-P': 2.0412969580790588}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4077.733
[2,     1] loss: 4096.307
[3,     1] loss: 4086.781
[4,     1] loss: 4088.848
[5,     1] loss: 4070.739
[6,     1] loss: 4076.675
[7,     1] loss: 4073.867
[8,     1] loss: 4043.610
[9,     1] loss: 4060.531
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006032907917740468,
 'learning_rate_Hydroxylation-K': 0.004793807771768575,
 'learning_rate_Hydroxylation-P': 0.00320911730636489,
 'log_base': 2.7604410794831815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1878460418,
 'sample_weights': [12.586663204416825, 1.5733935116533537],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.09993117686904,
 'weight_decay_Hydroxylation-K': 9.219420216006611,
 'weight_decay_Hydroxylation-P': 1.4283441527784073}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.057
[2,     1] loss: 1257.257
[3,     1] loss: 1254.009
[4,     1] loss: 1257.383
[5,     1] loss: 1253.742
[6,     1] loss: 1252.400
[7,     1] loss: 1251.276
[8,     1] loss: 1247.966
[9,     1] loss: 1240.306
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007699192602019755,
 'learning_rate_Hydroxylation-K': 0.0010400943495052469,
 'learning_rate_Hydroxylation-P': 0.004814684595377696,
 'log_base': 1.8629674872012265,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 528678963,
 'sample_weights': [1.6441390607756012, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.428523737235692,
 'weight_decay_Hydroxylation-K': 1.8365317978764475,
 'weight_decay_Hydroxylation-P': 0.34777883245741714}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1472.519
[2,     1] loss: 1474.740
[3,     1] loss: 1481.549
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007364338759835337,
 'learning_rate_Hydroxylation-K': 0.009798071056795612,
 'learning_rate_Hydroxylation-P': 0.007341528938818677,
 'log_base': 2.3172399567096034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1346410189,
 'sample_weights': [2.683256073372598, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.287273654542288,
 'weight_decay_Hydroxylation-K': 3.823547131726634,
 'weight_decay_Hydroxylation-P': 5.6286147745526485}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.778
[2,     1] loss: 1330.393
[3,     1] loss: 1330.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00950954939662389,
 'learning_rate_Hydroxylation-K': 0.009774650107705892,
 'learning_rate_Hydroxylation-P': 0.006485958852300608,
 'log_base': 2.415866176388935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3991795177,
 'sample_weights': [1.9865412041687365, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.85402395523877,
 'weight_decay_Hydroxylation-K': 9.404709686458949,
 'weight_decay_Hydroxylation-P': 4.001934497642531}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1310.003
[2,     1] loss: 1307.505
[3,     1] loss: 1309.626
[4,     1] loss: 1309.274
[5,     1] loss: 1306.272
[6,     1] loss: 1306.139
[7,     1] loss: 1310.144
[8,     1] loss: 1307.410
[9,     1] loss: 1308.131
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005629422026102852,
 'learning_rate_Hydroxylation-K': 0.0033206341937876897,
 'learning_rate_Hydroxylation-P': 0.0024257399837565594,
 'log_base': 1.3216716359666514,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3294845884,
 'sample_weights': [1.8926684633635231, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6858232828800315,
 'weight_decay_Hydroxylation-K': 2.315020898370772,
 'weight_decay_Hydroxylation-P': 1.2340465745183335}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2183.850
[2,     1] loss: 2169.560
[3,     1] loss: 2173.707
[4,     1] loss: 2176.434
[5,     1] loss: 2158.880
[6,     1] loss: 2167.040
[7,     1] loss: 2162.375
[8,     1] loss: 2167.001
[9,     1] loss: 2144.643
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005131414522214,
 'learning_rate_Hydroxylation-K': 0.0035585837296521457,
 'learning_rate_Hydroxylation-P': 0.0032890554865174877,
 'log_base': 1.303302066893078,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2724018286,
 'sample_weights': [5.985870032022454, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.477978971744152,
 'weight_decay_Hydroxylation-K': 6.087814762248945,
 'weight_decay_Hydroxylation-P': 1.3090885819538696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2239.561
[2,     1] loss: 2241.262
[3,     1] loss: 2241.228
[4,     1] loss: 2224.001
[5,     1] loss: 2239.408
[6,     1] loss: 2228.753
[7,     1] loss: 2225.014
[8,     1] loss: 2225.575
[9,     1] loss: 2201.632
[10,     1] loss: 2161.152
[11,     1] loss: 2136.228
[12,     1] loss: 2086.312
[13,     1] loss: 2011.829
[14,     1] loss: 1971.417
[15,     1] loss: 1958.785
[16,     1] loss: 1969.934
[17,     1] loss: 1821.111
[18,     1] loss: 1872.583
[19,     1] loss: 1865.639
[20,     1] loss: 1787.838
[21,     1] loss: 1839.689
[22,     1] loss: 1835.342
[23,     1] loss: 1811.526
[24,     1] loss: 1672.022
[25,     1] loss: 1705.160
[26,     1] loss: 1723.641
[27,     1] loss: 1631.055
[28,     1] loss: 1593.083
[29,     1] loss: 1667.125
[30,     1] loss: 1510.042
[31,     1] loss: 1544.929
[32,     1] loss: 1546.755
[33,     1] loss: 1596.114
[34,     1] loss: 1629.355
[35,     1] loss: 1516.868
[36,     1] loss: 1378.875
[37,     1] loss: 1517.139
[38,     1] loss: 1733.289
[39,     1] loss: 1562.643
[40,     1] loss: 1508.198
[41,     1] loss: 1484.842
[42,     1] loss: 1473.779
[43,     1] loss: 1411.280
[44,     1] loss: 1374.377
[45,     1] loss: 1421.212
[46,     1] loss: 1265.960
[47,     1] loss: 1252.057
Early stopping applied (best metric=0.8340215682983398)
Finished Training
Total time taken: 7.8660078048706055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2236.037
[2,     1] loss: 2219.822
[3,     1] loss: 2245.907
[4,     1] loss: 2241.010
[5,     1] loss: 2241.485
[6,     1] loss: 2241.473
[7,     1] loss: 2234.247
[8,     1] loss: 2228.444
[9,     1] loss: 2227.394
[10,     1] loss: 2220.024
[11,     1] loss: 2214.770
[12,     1] loss: 2206.778
[13,     1] loss: 2184.161
[14,     1] loss: 2143.438
[15,     1] loss: 2095.034
[16,     1] loss: 2038.534
[17,     1] loss: 1942.880
[18,     1] loss: 1953.581
[19,     1] loss: 1921.423
[20,     1] loss: 2044.244
[21,     1] loss: 1801.873
[22,     1] loss: 1778.165
[23,     1] loss: 1957.421
[24,     1] loss: 1831.332
[25,     1] loss: 1917.516
[26,     1] loss: 1783.613
[27,     1] loss: 1799.963
[28,     1] loss: 1842.400
[29,     1] loss: 1698.960
[30,     1] loss: 1826.216
[31,     1] loss: 1695.968
[32,     1] loss: 1577.393
[33,     1] loss: 1578.060
[34,     1] loss: 1743.116
[35,     1] loss: 1619.588
[36,     1] loss: 1678.305
[37,     1] loss: 1655.316
[38,     1] loss: 1738.436
[39,     1] loss: 1473.498
[40,     1] loss: 1590.807
[41,     1] loss: 1524.059
[42,     1] loss: 1555.950
[43,     1] loss: 1665.595
[44,     1] loss: 1393.464
[45,     1] loss: 1641.903
[46,     1] loss: 1508.716
[47,     1] loss: 1455.269
[48,     1] loss: 1436.174
[49,     1] loss: 1376.311
[50,     1] loss: 1460.604
[51,     1] loss: 1477.242
[52,     1] loss: 1515.283
Early stopping applied (best metric=0.8001355528831482)
Finished Training
Total time taken: 8.45200800895691
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2253.926
[2,     1] loss: 2229.030
[3,     1] loss: 2252.056
[4,     1] loss: 2225.381
[5,     1] loss: 2246.053
[6,     1] loss: 2237.243
[7,     1] loss: 2232.771
[8,     1] loss: 2235.192
[9,     1] loss: 2242.448
[10,     1] loss: 2233.519
[11,     1] loss: 2234.033
[12,     1] loss: 2241.110
[13,     1] loss: 2238.150
[14,     1] loss: 2233.783
[15,     1] loss: 2229.402
[16,     1] loss: 2233.633
[17,     1] loss: 2233.231
[18,     1] loss: 2229.008
[19,     1] loss: 2236.556
[20,     1] loss: 2226.498
[21,     1] loss: 2221.440
[22,     1] loss: 2237.618
[23,     1] loss: 2237.384
[24,     1] loss: 2212.640
[25,     1] loss: 2230.343
[26,     1] loss: 2243.949
[27,     1] loss: 2210.419
[28,     1] loss: 2204.613
[29,     1] loss: 2188.486
[30,     1] loss: 2157.998
[31,     1] loss: 2215.196
[32,     1] loss: 2147.754
[33,     1] loss: 2124.109
[34,     1] loss: 2109.115
[35,     1] loss: 2026.549
[36,     1] loss: 2058.406
[37,     1] loss: 1982.956
[38,     1] loss: 1964.195
[39,     1] loss: 1884.805
[40,     1] loss: 1850.385
[41,     1] loss: 1895.528
[42,     1] loss: 1742.647
[43,     1] loss: 1799.147
[44,     1] loss: 1990.108
[45,     1] loss: 1717.843
[46,     1] loss: 2095.401
[47,     1] loss: 1656.623
[48,     1] loss: 1863.187
[49,     1] loss: 1671.236
[50,     1] loss: 1753.277
[51,     1] loss: 1625.575
[52,     1] loss: 1669.140
[53,     1] loss: 1701.878
[54,     1] loss: 1565.400
[55,     1] loss: 1546.672
[56,     1] loss: 1549.763
[57,     1] loss: 1468.135
[58,     1] loss: 1516.780
[59,     1] loss: 1463.624
[60,     1] loss: 1388.955
[61,     1] loss: 1336.754
[62,     1] loss: 1539.443
[63,     1] loss: 1402.183
[64,     1] loss: 1551.388
[65,     1] loss: 1308.329
[66,     1] loss: 1407.198
[67,     1] loss: 1368.271
[68,     1] loss: 1427.666
[69,     1] loss: 1396.562
[70,     1] loss: 1278.249
[71,     1] loss: 1266.622
[72,     1] loss: 1266.803
[73,     1] loss: 1170.161
[74,     1] loss: 1205.509
[75,     1] loss: 1201.871
[76,     1] loss: 1093.535
[77,     1] loss: 1108.269
[78,     1] loss: 1427.215
[79,     1] loss: 1263.495
[80,     1] loss: 1197.033
[81,     1] loss: 1195.731
[82,     1] loss: 1075.449
[83,     1] loss: 1245.773
[84,     1] loss: 1188.597
[85,     1] loss: 1202.168
[86,     1] loss: 1219.888
[87,     1] loss: 1114.099
[88,     1] loss: 1224.173
[89,     1] loss: 970.129
[90,     1] loss: 1164.181
[91,     1] loss: 1042.250
[92,     1] loss: 1081.301
[93,     1] loss: 934.417
[94,     1] loss: 1201.437
[95,     1] loss: 1200.152
[96,     1] loss: 1347.169
[97,     1] loss: 940.322
[98,     1] loss: 1169.810
[99,     1] loss: 1001.031
[100,     1] loss: 1252.610
[101,     1] loss: 958.358
[102,     1] loss: 1066.335
[103,     1] loss: 925.354
[104,     1] loss: 1016.854
[105,     1] loss: 1013.363
[106,     1] loss: 929.489
Early stopping applied (best metric=0.8706904053688049)
Finished Training
Total time taken: 17.637016773223877
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2242.468
[2,     1] loss: 2246.457
[3,     1] loss: 2236.852
[4,     1] loss: 2252.951
[5,     1] loss: 2234.201
[6,     1] loss: 2231.401
[7,     1] loss: 2227.698
[8,     1] loss: 2230.968
[9,     1] loss: 2226.440
[10,     1] loss: 2231.820
[11,     1] loss: 2223.533
[12,     1] loss: 2219.364
[13,     1] loss: 2198.667
[14,     1] loss: 2193.645
[15,     1] loss: 2145.903
[16,     1] loss: 2096.645
[17,     1] loss: 2038.445
[18,     1] loss: 2077.488
[19,     1] loss: 1924.635
[20,     1] loss: 1897.094
[21,     1] loss: 1939.891
[22,     1] loss: 1913.836
[23,     1] loss: 1920.158
[24,     1] loss: 2072.881
[25,     1] loss: 1830.068
[26,     1] loss: 1852.145
[27,     1] loss: 1886.853
[28,     1] loss: 1883.528
[29,     1] loss: 1779.820
[30,     1] loss: 1768.171
[31,     1] loss: 1819.908
[32,     1] loss: 1799.669
[33,     1] loss: 1731.493
[34,     1] loss: 1840.404
[35,     1] loss: 1560.304
[36,     1] loss: 1755.423
[37,     1] loss: 1572.214
[38,     1] loss: 1569.097
[39,     1] loss: 1603.321
[40,     1] loss: 1661.839
[41,     1] loss: 1504.494
[42,     1] loss: 1555.589
[43,     1] loss: 1437.625
[44,     1] loss: 1486.465
[45,     1] loss: 1453.265
[46,     1] loss: 1625.492
[47,     1] loss: 1605.994
[48,     1] loss: 1423.161
[49,     1] loss: 1333.275
[50,     1] loss: 1837.747
[51,     1] loss: 1366.365
[52,     1] loss: 1569.811
[53,     1] loss: 1334.669
[54,     1] loss: 1375.597
[55,     1] loss: 1504.998
[56,     1] loss: 1417.091
[57,     1] loss: 1721.636
[58,     1] loss: 1327.189
[59,     1] loss: 1485.973
[60,     1] loss: 1404.759
[61,     1] loss: 1268.587
[62,     1] loss: 1298.081
[63,     1] loss: 1318.352
[64,     1] loss: 1308.852
[65,     1] loss: 1439.775
[66,     1] loss: 1167.146
Early stopping applied (best metric=0.8428944945335388)
Finished Training
Total time taken: 9.009006977081299
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2227.884
[2,     1] loss: 2266.219
[3,     1] loss: 2232.127
[4,     1] loss: 2233.584
[5,     1] loss: 2240.914
[6,     1] loss: 2240.890
[7,     1] loss: 2238.426
[8,     1] loss: 2239.889
[9,     1] loss: 2238.274
[10,     1] loss: 2236.092
[11,     1] loss: 2240.126
[12,     1] loss: 2226.131
[13,     1] loss: 2224.640
[14,     1] loss: 2217.414
[15,     1] loss: 2205.714
[16,     1] loss: 2184.110
[17,     1] loss: 2173.468
[18,     1] loss: 2113.250
[19,     1] loss: 2068.359
[20,     1] loss: 1988.607
[21,     1] loss: 1930.444
[22,     1] loss: 1963.423
[23,     1] loss: 1892.996
[24,     1] loss: 1862.056
[25,     1] loss: 1888.902
[26,     1] loss: 1812.039
[27,     1] loss: 1863.107
[28,     1] loss: 1790.771
[29,     1] loss: 1861.875
[30,     1] loss: 1905.807
[31,     1] loss: 1721.133
[32,     1] loss: 1797.759
[33,     1] loss: 1659.398
[34,     1] loss: 1572.758
[35,     1] loss: 1549.188
[36,     1] loss: 1604.397
[37,     1] loss: 1558.321
[38,     1] loss: 1577.636
[39,     1] loss: 1732.108
[40,     1] loss: 1504.980
[41,     1] loss: 1535.979
[42,     1] loss: 1402.460
[43,     1] loss: 1449.010
[44,     1] loss: 1364.918
[45,     1] loss: 1464.052
[46,     1] loss: 1431.622
[47,     1] loss: 1522.203
[48,     1] loss: 1326.717
[49,     1] loss: 1547.149
[50,     1] loss: 1458.668
Early stopping applied (best metric=0.9919074177742004)
Finished Training
Total time taken: 6.835006475448608
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2244.573
[2,     1] loss: 2233.226
[3,     1] loss: 2225.310
[4,     1] loss: 2243.998
[5,     1] loss: 2231.878
[6,     1] loss: 2227.739
[7,     1] loss: 2225.781
[8,     1] loss: 2223.391
[9,     1] loss: 2234.424
[10,     1] loss: 2240.430
[11,     1] loss: 2230.233
[12,     1] loss: 2222.249
[13,     1] loss: 2213.900
[14,     1] loss: 2224.488
[15,     1] loss: 2198.825
[16,     1] loss: 2180.614
[17,     1] loss: 2144.066
[18,     1] loss: 2087.729
[19,     1] loss: 2087.758
[20,     1] loss: 2021.026
[21,     1] loss: 2028.793
[22,     1] loss: 2009.442
[23,     1] loss: 1964.675
[24,     1] loss: 1935.733
[25,     1] loss: 1808.605
[26,     1] loss: 1981.746
[27,     1] loss: 1987.242
[28,     1] loss: 2089.861
[29,     1] loss: 1910.605
[30,     1] loss: 1996.660
[31,     1] loss: 1958.010
[32,     1] loss: 1911.449
[33,     1] loss: 1937.568
[34,     1] loss: 1893.483
[35,     1] loss: 1874.273
[36,     1] loss: 1697.552
[37,     1] loss: 1787.295
[38,     1] loss: 1745.108
[39,     1] loss: 1829.725
[40,     1] loss: 1845.880
[41,     1] loss: 1595.208
[42,     1] loss: 1781.867
[43,     1] loss: 1674.813
[44,     1] loss: 1578.723
[45,     1] loss: 1528.255
[46,     1] loss: 1638.870
[47,     1] loss: 1549.993
[48,     1] loss: 1600.542
[49,     1] loss: 1613.397
[50,     1] loss: 1420.834
[51,     1] loss: 1620.977
[52,     1] loss: 1338.038
[53,     1] loss: 1616.302
[54,     1] loss: 1387.116
[55,     1] loss: 1516.645
[56,     1] loss: 1282.515
[57,     1] loss: 1525.567
[58,     1] loss: 1271.625
[59,     1] loss: 1298.406
[60,     1] loss: 1396.191
[61,     1] loss: 1244.438
[62,     1] loss: 1172.534
[63,     1] loss: 1174.579
[64,     1] loss: 1250.443
Early stopping applied (best metric=0.6735648512840271)
Finished Training
Total time taken: 10.671010494232178
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2238.863
[2,     1] loss: 2240.590
[3,     1] loss: 2232.580
[4,     1] loss: 2243.811
[5,     1] loss: 2255.026
[6,     1] loss: 2236.517
[7,     1] loss: 2236.548
[8,     1] loss: 2235.596
[9,     1] loss: 2236.276
[10,     1] loss: 2226.828
[11,     1] loss: 2234.136
[12,     1] loss: 2230.711
[13,     1] loss: 2238.676
[14,     1] loss: 2238.952
[15,     1] loss: 2228.007
[16,     1] loss: 2225.485
[17,     1] loss: 2223.970
[18,     1] loss: 2214.224
[19,     1] loss: 2201.207
[20,     1] loss: 2177.675
[21,     1] loss: 2155.843
[22,     1] loss: 2108.822
[23,     1] loss: 2078.596
[24,     1] loss: 2020.770
[25,     1] loss: 2018.643
[26,     1] loss: 1973.250
[27,     1] loss: 1901.957
[28,     1] loss: 1834.452
[29,     1] loss: 1972.286
[30,     1] loss: 1826.271
[31,     1] loss: 1902.822
[32,     1] loss: 1786.693
[33,     1] loss: 1799.617
[34,     1] loss: 1790.995
[35,     1] loss: 1879.250
[36,     1] loss: 1923.540
[37,     1] loss: 1781.777
[38,     1] loss: 1865.825
[39,     1] loss: 1562.543
[40,     1] loss: 1831.088
[41,     1] loss: 1600.276
[42,     1] loss: 1853.221
[43,     1] loss: 1683.738
[44,     1] loss: 1698.128
[45,     1] loss: 1673.315
[46,     1] loss: 1611.179
[47,     1] loss: 1603.050
[48,     1] loss: 1644.685
[49,     1] loss: 1541.665
[50,     1] loss: 1647.105
[51,     1] loss: 1567.863
[52,     1] loss: 1548.865
[53,     1] loss: 1434.106
[54,     1] loss: 1435.495
[55,     1] loss: 1499.286
[56,     1] loss: 1667.912
[57,     1] loss: 1390.649
[58,     1] loss: 1331.966
[59,     1] loss: 1211.900
[60,     1] loss: 1332.224
[61,     1] loss: 1365.360
[62,     1] loss: 1317.108
[63,     1] loss: 1253.320
[64,     1] loss: 1140.204
[65,     1] loss: 1335.316
[66,     1] loss: 1913.890
[67,     1] loss: 1871.820
[68,     1] loss: 1229.318
[69,     1] loss: 1530.530
[70,     1] loss: 1402.020
[71,     1] loss: 1216.893
[72,     1] loss: 1424.911
[73,     1] loss: 1474.153
[74,     1] loss: 1340.622
[75,     1] loss: 1372.191
[76,     1] loss: 1603.531
[77,     1] loss: 1225.801
[78,     1] loss: 1366.726
Early stopping applied (best metric=0.7152539491653442)
Finished Training
Total time taken: 12.917015552520752
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2247.500
[2,     1] loss: 2228.871
[3,     1] loss: 2230.416
[4,     1] loss: 2263.131
[5,     1] loss: 2248.201
[6,     1] loss: 2241.501
[7,     1] loss: 2238.299
[8,     1] loss: 2233.977
[9,     1] loss: 2228.458
[10,     1] loss: 2236.977
[11,     1] loss: 2227.114
[12,     1] loss: 2222.878
[13,     1] loss: 2207.536
[14,     1] loss: 2197.393
[15,     1] loss: 2177.353
[16,     1] loss: 2162.545
[17,     1] loss: 2124.487
[18,     1] loss: 2080.390
[19,     1] loss: 2029.586
[20,     1] loss: 1948.825
[21,     1] loss: 1937.248
[22,     1] loss: 1911.006
[23,     1] loss: 1910.603
[24,     1] loss: 1802.941
[25,     1] loss: 1869.192
[26,     1] loss: 1835.149
[27,     1] loss: 1862.865
[28,     1] loss: 1876.655
[29,     1] loss: 1854.461
[30,     1] loss: 1832.016
[31,     1] loss: 1743.673
[32,     1] loss: 1685.501
[33,     1] loss: 1691.714
[34,     1] loss: 1603.518
[35,     1] loss: 1673.848
[36,     1] loss: 1712.209
[37,     1] loss: 1596.085
[38,     1] loss: 1490.251
[39,     1] loss: 1564.234
[40,     1] loss: 1422.434
[41,     1] loss: 1435.375
[42,     1] loss: 1516.422
[43,     1] loss: 1347.040
[44,     1] loss: 1396.211
[45,     1] loss: 1618.129
[46,     1] loss: 1231.761
Early stopping applied (best metric=0.8804060816764832)
Finished Training
Total time taken: 7.721007823944092
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2225.207
[2,     1] loss: 2238.783
[3,     1] loss: 2238.000
[4,     1] loss: 2229.651
[5,     1] loss: 2226.564
[6,     1] loss: 2229.596
[7,     1] loss: 2226.843
[8,     1] loss: 2212.895
[9,     1] loss: 2204.713
[10,     1] loss: 2173.870
[11,     1] loss: 2135.441
[12,     1] loss: 2085.040
[13,     1] loss: 2038.980
[14,     1] loss: 1907.206
[15,     1] loss: 1917.716
[16,     1] loss: 1823.610
[17,     1] loss: 1958.910
[18,     1] loss: 1806.102
[19,     1] loss: 1848.160
[20,     1] loss: 1742.118
[21,     1] loss: 1752.173
[22,     1] loss: 1779.519
[23,     1] loss: 1773.736
[24,     1] loss: 1727.322
[25,     1] loss: 1780.145
[26,     1] loss: 1734.899
[27,     1] loss: 1681.691
[28,     1] loss: 1529.766
[29,     1] loss: 1814.176
[30,     1] loss: 1582.171
[31,     1] loss: 1870.387
[32,     1] loss: 1505.760
[33,     1] loss: 1800.567
[34,     1] loss: 1551.826
[35,     1] loss: 1584.755
[36,     1] loss: 1508.750
[37,     1] loss: 1497.261
[38,     1] loss: 1470.904
[39,     1] loss: 1507.210
[40,     1] loss: 1450.000
[41,     1] loss: 1397.084
[42,     1] loss: 1364.432
[43,     1] loss: 1411.701
[44,     1] loss: 1297.508
Early stopping applied (best metric=0.9904392957687378)
Finished Training
Total time taken: 5.989006996154785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2243.432
[2,     1] loss: 2249.187
[3,     1] loss: 2237.705
[4,     1] loss: 2243.578
[5,     1] loss: 2238.393
[6,     1] loss: 2240.846
[7,     1] loss: 2238.834
[8,     1] loss: 2227.098
[9,     1] loss: 2224.866
[10,     1] loss: 2243.571
[11,     1] loss: 2235.094
[12,     1] loss: 2226.694
[13,     1] loss: 2238.615
[14,     1] loss: 2223.930
[15,     1] loss: 2209.312
[16,     1] loss: 2194.438
[17,     1] loss: 2157.207
[18,     1] loss: 2158.767
[19,     1] loss: 2139.858
[20,     1] loss: 2018.638
[21,     1] loss: 2019.147
[22,     1] loss: 1998.509
[23,     1] loss: 1979.246
[24,     1] loss: 1945.829
[25,     1] loss: 1914.009
[26,     1] loss: 1867.695
[27,     1] loss: 1859.427
[28,     1] loss: 1779.742
[29,     1] loss: 1865.362
[30,     1] loss: 1861.296
[31,     1] loss: 1790.129
[32,     1] loss: 1805.638
[33,     1] loss: 1666.350
[34,     1] loss: 1823.813
[35,     1] loss: 1743.903
[36,     1] loss: 1603.563
[37,     1] loss: 1631.919
[38,     1] loss: 1603.403
[39,     1] loss: 1497.840
[40,     1] loss: 1459.696
[41,     1] loss: 1479.904
[42,     1] loss: 1437.240
[43,     1] loss: 1635.876
[44,     1] loss: 1394.880
[45,     1] loss: 1608.233
[46,     1] loss: 1713.709
[47,     1] loss: 1400.170
[48,     1] loss: 1461.039
[49,     1] loss: 1436.151
[50,     1] loss: 1473.064
[51,     1] loss: 1466.073
[52,     1] loss: 1522.394
Early stopping applied (best metric=0.8652970790863037)
Finished Training
Total time taken: 8.669008016586304
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2234.810
[2,     1] loss: 2239.073
[3,     1] loss: 2243.593
[4,     1] loss: 2237.414
[5,     1] loss: 2237.847
[6,     1] loss: 2233.450
[7,     1] loss: 2236.848
[8,     1] loss: 2235.826
[9,     1] loss: 2232.448
[10,     1] loss: 2235.194
[11,     1] loss: 2233.326
[12,     1] loss: 2230.152
[13,     1] loss: 2234.012
[14,     1] loss: 2221.995
[15,     1] loss: 2227.794
[16,     1] loss: 2214.462
[17,     1] loss: 2199.229
[18,     1] loss: 2178.244
[19,     1] loss: 2143.911
[20,     1] loss: 2107.180
[21,     1] loss: 2016.977
[22,     1] loss: 2025.444
[23,     1] loss: 1894.897
[24,     1] loss: 1887.282
[25,     1] loss: 1906.325
[26,     1] loss: 1886.287
[27,     1] loss: 1876.701
[28,     1] loss: 1862.439
[29,     1] loss: 1850.924
[30,     1] loss: 1760.456
[31,     1] loss: 1688.129
[32,     1] loss: 1631.809
[33,     1] loss: 1670.450
[34,     1] loss: 1609.955
[35,     1] loss: 1653.634
[36,     1] loss: 1606.969
[37,     1] loss: 1755.182
[38,     1] loss: 1472.374
[39,     1] loss: 1550.493
[40,     1] loss: 1464.685
[41,     1] loss: 1531.412
[42,     1] loss: 1530.986
[43,     1] loss: 1290.496
[44,     1] loss: 1393.233
[45,     1] loss: 1406.481
[46,     1] loss: 1463.323
Early stopping applied (best metric=0.867499828338623)
Finished Training
Total time taken: 6.2600061893463135
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2234.599
[2,     1] loss: 2233.500
[3,     1] loss: 2225.829
[4,     1] loss: 2234.129
[5,     1] loss: 2233.221
[6,     1] loss: 2221.132
[7,     1] loss: 2252.702
[8,     1] loss: 2228.157
[9,     1] loss: 2226.429
[10,     1] loss: 2230.748
[11,     1] loss: 2243.586
[12,     1] loss: 2213.121
[13,     1] loss: 2201.566
[14,     1] loss: 2184.485
[15,     1] loss: 2161.189
[16,     1] loss: 2114.255
[17,     1] loss: 2115.449
[18,     1] loss: 1972.985
[19,     1] loss: 1911.345
[20,     1] loss: 1868.570
[21,     1] loss: 1929.841
[22,     1] loss: 1895.543
[23,     1] loss: 1789.213
[24,     1] loss: 1879.293
[25,     1] loss: 1933.034
[26,     1] loss: 1839.385
[27,     1] loss: 1778.850
[28,     1] loss: 1816.873
[29,     1] loss: 1813.225
[30,     1] loss: 1712.548
[31,     1] loss: 1632.025
[32,     1] loss: 1729.464
[33,     1] loss: 1687.670
[34,     1] loss: 1774.572
[35,     1] loss: 1685.643
[36,     1] loss: 1618.583
[37,     1] loss: 1605.611
[38,     1] loss: 1606.403
[39,     1] loss: 1645.819
[40,     1] loss: 1671.296
[41,     1] loss: 1515.534
[42,     1] loss: 1632.218
[43,     1] loss: 1492.294
[44,     1] loss: 1492.522
[45,     1] loss: 1448.353
[46,     1] loss: 1440.176
[47,     1] loss: 1392.663
[48,     1] loss: 1507.401
[49,     1] loss: 1465.818
[50,     1] loss: 1383.124
[51,     1] loss: 1250.360
[52,     1] loss: 1380.610
[53,     1] loss: 1282.343
[54,     1] loss: 1337.984
[55,     1] loss: 1330.856
[56,     1] loss: 1276.841
[57,     1] loss: 1340.778
[58,     1] loss: 1341.906
[59,     1] loss: 1253.143
Early stopping applied (best metric=0.7892408967018127)
Finished Training
Total time taken: 9.89100980758667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2239.614
[2,     1] loss: 2243.351
[3,     1] loss: 2239.391
[4,     1] loss: 2228.442
[5,     1] loss: 2234.579
[6,     1] loss: 2237.468
[7,     1] loss: 2254.823
[8,     1] loss: 2237.962
[9,     1] loss: 2234.960
[10,     1] loss: 2240.750
[11,     1] loss: 2228.542
[12,     1] loss: 2247.633
[13,     1] loss: 2245.215
[14,     1] loss: 2228.072
[15,     1] loss: 2231.558
[16,     1] loss: 2230.443
[17,     1] loss: 2230.804
[18,     1] loss: 2218.317
[19,     1] loss: 2217.128
[20,     1] loss: 2197.152
[21,     1] loss: 2186.440
[22,     1] loss: 2133.272
[23,     1] loss: 2100.601
[24,     1] loss: 2089.484
[25,     1] loss: 2013.117
[26,     1] loss: 1969.019
[27,     1] loss: 1927.575
[28,     1] loss: 1941.825
[29,     1] loss: 1859.896
[30,     1] loss: 1930.561
[31,     1] loss: 1942.147
[32,     1] loss: 1807.037
[33,     1] loss: 1760.676
[34,     1] loss: 1813.153
[35,     1] loss: 1753.814
[36,     1] loss: 1731.349
[37,     1] loss: 1726.304
[38,     1] loss: 1737.277
[39,     1] loss: 1604.285
[40,     1] loss: 1666.183
[41,     1] loss: 1618.488
[42,     1] loss: 1673.065
[43,     1] loss: 1622.669
[44,     1] loss: 1452.638
[45,     1] loss: 1566.665
[46,     1] loss: 1509.888
[47,     1] loss: 1413.483
[48,     1] loss: 1494.506
[49,     1] loss: 1562.895
[50,     1] loss: 1500.259
[51,     1] loss: 1380.508
[52,     1] loss: 1436.283
[53,     1] loss: 1332.069
[54,     1] loss: 1582.391
[55,     1] loss: 1697.866
[56,     1] loss: 1473.451
[57,     1] loss: 1477.155
[58,     1] loss: 1362.564
[59,     1] loss: 1331.949
[60,     1] loss: 1377.308
[61,     1] loss: 1301.512
[62,     1] loss: 1260.682
[63,     1] loss: 1276.572
Early stopping applied (best metric=0.8875731229782104)
Finished Training
Total time taken: 10.536010265350342
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2255.227
[2,     1] loss: 2228.765
[3,     1] loss: 2265.896
[4,     1] loss: 2236.938
[5,     1] loss: 2237.546
[6,     1] loss: 2240.923
[7,     1] loss: 2236.377
[8,     1] loss: 2237.421
[9,     1] loss: 2237.315
[10,     1] loss: 2225.893
[11,     1] loss: 2232.100
[12,     1] loss: 2238.566
[13,     1] loss: 2233.342
[14,     1] loss: 2226.269
[15,     1] loss: 2227.135
[16,     1] loss: 2213.120
[17,     1] loss: 2209.044
[18,     1] loss: 2184.246
[19,     1] loss: 2165.571
[20,     1] loss: 2123.051
[21,     1] loss: 2095.526
[22,     1] loss: 2017.479
[23,     1] loss: 1979.661
[24,     1] loss: 1918.047
[25,     1] loss: 1839.667
[26,     1] loss: 1951.088
[27,     1] loss: 1958.804
[28,     1] loss: 1764.790
[29,     1] loss: 1852.732
[30,     1] loss: 1765.296
[31,     1] loss: 1738.927
[32,     1] loss: 1815.180
[33,     1] loss: 1737.021
[34,     1] loss: 1649.138
[35,     1] loss: 1771.887
[36,     1] loss: 1671.010
[37,     1] loss: 1639.453
[38,     1] loss: 1641.530
[39,     1] loss: 1591.235
[40,     1] loss: 1525.875
[41,     1] loss: 1553.215
[42,     1] loss: 1447.987
[43,     1] loss: 1587.289
[44,     1] loss: 1457.697
[45,     1] loss: 1497.070
[46,     1] loss: 1818.824
[47,     1] loss: 1410.291
[48,     1] loss: 1644.890
[49,     1] loss: 1403.318
[50,     1] loss: 1636.648
[51,     1] loss: 1337.303
[52,     1] loss: 1404.928
[53,     1] loss: 1363.679
[54,     1] loss: 1415.333
[55,     1] loss: 1390.271
[56,     1] loss: 1436.325
[57,     1] loss: 1340.081
[58,     1] loss: 1263.897
[59,     1] loss: 1341.831
[60,     1] loss: 1135.904
[61,     1] loss: 1220.215
[62,     1] loss: 1293.122
Early stopping applied (best metric=0.7864612936973572)
Finished Training
Total time taken: 10.29000997543335
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2246.510
[2,     1] loss: 2241.058
[3,     1] loss: 2242.876
[4,     1] loss: 2239.165
[5,     1] loss: 2243.500
[6,     1] loss: 2236.054
[7,     1] loss: 2236.769
[8,     1] loss: 2234.287
[9,     1] loss: 2234.054
[10,     1] loss: 2226.571
[11,     1] loss: 2243.119
[12,     1] loss: 2230.276
[13,     1] loss: 2229.334
[14,     1] loss: 2217.878
[15,     1] loss: 2199.852
[16,     1] loss: 2177.298
[17,     1] loss: 2143.960
[18,     1] loss: 2098.583
[19,     1] loss: 2067.655
[20,     1] loss: 1969.154
[21,     1] loss: 1980.500
[22,     1] loss: 1882.385
[23,     1] loss: 1818.536
[24,     1] loss: 1878.604
[25,     1] loss: 1797.639
[26,     1] loss: 1925.017
[27,     1] loss: 1802.950
[28,     1] loss: 1867.614
[29,     1] loss: 1740.472
[30,     1] loss: 1782.368
[31,     1] loss: 1667.751
[32,     1] loss: 1751.411
[33,     1] loss: 1804.521
[34,     1] loss: 1730.498
[35,     1] loss: 1796.393
[36,     1] loss: 1663.948
[37,     1] loss: 1741.062
[38,     1] loss: 1726.163
[39,     1] loss: 1724.779
[40,     1] loss: 1569.219
[41,     1] loss: 1700.158
[42,     1] loss: 1541.578
[43,     1] loss: 1632.048
[44,     1] loss: 1649.236
[45,     1] loss: 1544.853
[46,     1] loss: 1802.594
[47,     1] loss: 1459.020
[48,     1] loss: 1583.571
[49,     1] loss: 1433.205
[50,     1] loss: 1529.539
[51,     1] loss: 1586.405
[52,     1] loss: 1362.522
[53,     1] loss: 1509.567
[54,     1] loss: 1253.246
Early stopping applied (best metric=0.7710385322570801)
Finished Training
Total time taken: 7.954008102416992
{'Hydroxylation-K Validation Accuracy': 0.7473699763593381, 'Hydroxylation-K Validation Sensitivity': 0.7696296296296297, 'Hydroxylation-K Validation Specificity': 0.7421052631578947, 'Hydroxylation-K Validation Precision': 0.44636354477995344, 'Hydroxylation-K AUC ROC': 0.8265107212475634, 'Hydroxylation-K AUC PR': 0.6355027433329385, 'Hydroxylation-K MCC': 0.43708584176104304, 'Hydroxylation-K F1': 0.5577322465515258, 'Validation Loss (Hydroxylation-K)': 0.41314337650934857, 'Hydroxylation-P Validation Accuracy': 0.7600879819975297, 'Hydroxylation-P Validation Sensitivity': 0.7255026455026455, 'Hydroxylation-P Validation Specificity': 0.7675719487256222, 'Hydroxylation-P Validation Precision': 0.4137800744214459, 'Hydroxylation-P AUC ROC': 0.8081099183983368, 'Hydroxylation-P AUC PR': 0.5482497261325512, 'Hydroxylation-P MCC': 0.4104998574651159, 'Hydroxylation-P F1': 0.5201328188584448, 'Validation Loss (Hydroxylation-P)': 0.42461824814478555, 'Validation Loss (total)': 0.8377616246541341, 'TimeToTrain': 9.379809284210205}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005143229980677245,
 'learning_rate_Hydroxylation-K': 0.00840893318582948,
 'learning_rate_Hydroxylation-P': 0.0026369084610710918,
 'log_base': 1.9969029196886636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1861447627,
 'sample_weights': [6.306812491153947, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6383133166349275,
 'weight_decay_Hydroxylation-K': 2.512422490471384,
 'weight_decay_Hydroxylation-P': 5.062593882647623}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1420.784
[2,     1] loss: 1418.440
[3,     1] loss: 1415.742
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012707826553565154,
 'learning_rate_Hydroxylation-K': 0.0025325565044328354,
 'learning_rate_Hydroxylation-P': 0.0022623296353094687,
 'log_base': 1.1669814158245606,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3003926520,
 'sample_weights': [2.413894341478524, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.376800205282175,
 'weight_decay_Hydroxylation-K': 0.2843193047920933,
 'weight_decay_Hydroxylation-P': 6.96509759601645}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3495.488
[2,     1] loss: 3514.892
[3,     1] loss: 3516.515
[4,     1] loss: 3514.851
[5,     1] loss: 3502.179
[6,     1] loss: 3506.693
[7,     1] loss: 3498.242
[8,     1] loss: 3499.956
[9,     1] loss: 3520.146
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008788123982476897,
 'learning_rate_Hydroxylation-K': 0.0006560453617226363,
 'learning_rate_Hydroxylation-P': 0.007892336845984197,
 'log_base': 1.1836930869675384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3942110095,
 'sample_weights': [10.811025227363812, 1.3514302139335796],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.958222879819669,
 'weight_decay_Hydroxylation-K': 2.7121313975007717,
 'weight_decay_Hydroxylation-P': 9.59161559691061}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3224.851
[2,     1] loss: 3226.587
[3,     1] loss: 3208.984
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004845591068120781,
 'learning_rate_Hydroxylation-K': 0.00637458716104254,
 'learning_rate_Hydroxylation-P': 0.00018570306205346106,
 'log_base': 2.0866085521313282,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2448026829,
 'sample_weights': [9.8994913302133, 1.237484087296503],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5869584861032582,
 'weight_decay_Hydroxylation-K': 2.938268404712199,
 'weight_decay_Hydroxylation-P': 9.716334500964424}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1383.862
[2,     1] loss: 1394.943
[3,     1] loss: 1397.014
[4,     1] loss: 1388.943
[5,     1] loss: 1385.001
[6,     1] loss: 1384.251
[7,     1] loss: 1383.180
[8,     1] loss: 1386.297
[9,     1] loss: 1380.027
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007366986455385565,
 'learning_rate_Hydroxylation-K': 0.004489586123952223,
 'learning_rate_Hydroxylation-P': 0.003520874379435311,
 'log_base': 1.7361867821564108,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1728200694,
 'sample_weights': [2.2696835574351284, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.231276489124061,
 'weight_decay_Hydroxylation-K': 7.522318407041071,
 'weight_decay_Hydroxylation-P': 0.3633758179474247}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1549.040
[2,     1] loss: 1554.201
[3,     1] loss: 1546.321
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004406842050207625,
 'learning_rate_Hydroxylation-K': 0.0018026601273857332,
 'learning_rate_Hydroxylation-P': 0.0027813070280540402,
 'log_base': 1.2697781727557749,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2416319917,
 'sample_weights': [3.0260463385092105, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.717474178630389,
 'weight_decay_Hydroxylation-K': 5.221457885999851,
 'weight_decay_Hydroxylation-P': 1.3272115832477491}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2377.293
[2,     1] loss: 2407.546
[3,     1] loss: 2378.287
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002301641183911032,
 'learning_rate_Hydroxylation-K': 0.006661634289730934,
 'learning_rate_Hydroxylation-P': 0.0013188956038466234,
 'log_base': 2.289176379743166,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2975260941,
 'sample_weights': [6.989732220514444, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.297345771664391,
 'weight_decay_Hydroxylation-K': 8.242234511466227,
 'weight_decay_Hydroxylation-P': 7.782276390258956}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.320
[2,     1] loss: 1330.253
[3,     1] loss: 1332.474
[4,     1] loss: 1332.568
[5,     1] loss: 1325.987
[6,     1] loss: 1317.573
[7,     1] loss: 1314.993
[8,     1] loss: 1294.620
[9,     1] loss: 1274.032
[10,     1] loss: 1259.147
[11,     1] loss: 1244.006
[12,     1] loss: 1183.780
[13,     1] loss: 1162.310
[14,     1] loss: 1137.031
[15,     1] loss: 1142.050
[16,     1] loss: 1121.156
[17,     1] loss: 1063.972
[18,     1] loss: 1106.592
[19,     1] loss: 1127.163
[20,     1] loss: 1077.319
[21,     1] loss: 1109.048
[22,     1] loss: 1073.627
[23,     1] loss: 1061.997
[24,     1] loss: 1072.252
[25,     1] loss: 1076.857
[26,     1] loss: 1057.871
[27,     1] loss: 1020.900
[28,     1] loss: 997.327
[29,     1] loss: 970.084
[30,     1] loss: 964.538
[31,     1] loss: 964.907
[32,     1] loss: 968.574
[33,     1] loss: 933.486
[34,     1] loss: 947.122
[35,     1] loss: 939.207
[36,     1] loss: 956.067
[37,     1] loss: 929.129
[38,     1] loss: 919.606
[39,     1] loss: 957.477
[40,     1] loss: 878.888
[41,     1] loss: 899.714
[42,     1] loss: 844.629
[43,     1] loss: 847.165
[44,     1] loss: 848.649
[45,     1] loss: 866.686
[46,     1] loss: 821.843
[47,     1] loss: 863.494
[48,     1] loss: 820.346
[49,     1] loss: 859.116
[50,     1] loss: 809.924
[51,     1] loss: 809.387
[52,     1] loss: 840.844
[53,     1] loss: 811.436
[54,     1] loss: 763.880
[55,     1] loss: 801.035
[56,     1] loss: 783.049
[57,     1] loss: 793.707
[58,     1] loss: 702.074
[59,     1] loss: 715.982
[60,     1] loss: 774.007
[61,     1] loss: 705.137
[62,     1] loss: 656.108
[63,     1] loss: 713.591
[64,     1] loss: 674.617
[65,     1] loss: 745.910
[66,     1] loss: 757.362
[67,     1] loss: 667.520
[68,     1] loss: 715.092
[69,     1] loss: 790.892
[70,     1] loss: 605.961
[71,     1] loss: 678.899
[72,     1] loss: 604.893
[73,     1] loss: 705.785
[74,     1] loss: 615.820
[75,     1] loss: 603.084
[76,     1] loss: 584.014
[77,     1] loss: 573.701
[78,     1] loss: 598.398
[79,     1] loss: 736.239
[80,     1] loss: 618.633
[81,     1] loss: 571.649
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005612834929824654,
 'learning_rate_Hydroxylation-K': 0.0004806471562072755,
 'learning_rate_Hydroxylation-P': 0.0037113119987920743,
 'log_base': 1.2299302510030765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2767369984,
 'sample_weights': [2.01576803350752, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.524730370418714,
 'weight_decay_Hydroxylation-K': 0.6539282771937986,
 'weight_decay_Hydroxylation-P': 3.331996655133931}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2615.045
[2,     1] loss: 2604.171
[3,     1] loss: 2643.857
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030543270104636967,
 'learning_rate_Hydroxylation-K': 0.009362156576653383,
 'learning_rate_Hydroxylation-P': 0.0040272396769159054,
 'log_base': 2.6707793502898522,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1340861034,
 'sample_weights': [8.066600436339243, 1.0083638992725656],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.674330329496382,
 'weight_decay_Hydroxylation-K': 4.086992726450564,
 'weight_decay_Hydroxylation-P': 5.672266597717959}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.743
[2,     1] loss: 1267.716
[3,     1] loss: 1265.989
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011438566213803454,
 'learning_rate_Hydroxylation-K': 0.006632616307942272,
 'learning_rate_Hydroxylation-P': 0.0027831331790059504,
 'log_base': 1.8011610164875087,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 296521598,
 'sample_weights': [1.699403077639694, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.148410060060193,
 'weight_decay_Hydroxylation-K': 8.594781671221742,
 'weight_decay_Hydroxylation-P': 8.898618091772676}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1510.125
[2,     1] loss: 1502.258
[3,     1] loss: 1503.951
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014230963683768232,
 'learning_rate_Hydroxylation-K': 0.005651497628198929,
 'learning_rate_Hydroxylation-P': 0.0023454402566863847,
 'log_base': 2.331402671582211,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 27429587,
 'sample_weights': [2.8371071968588097, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.913437442528458,
 'weight_decay_Hydroxylation-K': 9.932636062955435,
 'weight_decay_Hydroxylation-P': 8.871015848366715}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.928
[2,     1] loss: 1329.581
[3,     1] loss: 1325.453
[4,     1] loss: 1322.478
[5,     1] loss: 1325.245
[6,     1] loss: 1319.257
[7,     1] loss: 1324.289
[8,     1] loss: 1318.089
[9,     1] loss: 1315.804
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004714299919101987,
 'learning_rate_Hydroxylation-K': 0.004599450519333012,
 'learning_rate_Hydroxylation-P': 0.0038401122627215304,
 'log_base': 1.1542269277035833,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3325564545,
 'sample_weights': [1.9722411510494797, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7772542750542355,
 'weight_decay_Hydroxylation-K': 4.906683738372911,
 'weight_decay_Hydroxylation-P': 2.1190341733980507}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3787.677
[2,     1] loss: 3790.584
[3,     1] loss: 3781.036
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003638536884007948,
 'learning_rate_Hydroxylation-K': 0.0015484051308452577,
 'learning_rate_Hydroxylation-P': 0.0019130733270304816,
 'log_base': 1.0331476595604265,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2872469438,
 'sample_weights': [11.639363556796466, 1.454976493977505],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.0311843863838135,
 'weight_decay_Hydroxylation-K': 1.3299223947803165,
 'weight_decay_Hydroxylation-P': 2.1752432050347723}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16599.600
Exploding loss, terminate run (best metric=1.0979316234588623)
Finished Training
Total time taken: 0.19800066947937012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16668.314
Exploding loss, terminate run (best metric=1.095513105392456)
Finished Training
Total time taken: 0.20400118827819824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16660.105
Exploding loss, terminate run (best metric=1.0908150672912598)
Finished Training
Total time taken: 0.2240009307861328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16642.332
Exploding loss, terminate run (best metric=1.071913480758667)
Finished Training
Total time taken: 0.20200204849243164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16650.418
Exploding loss, terminate run (best metric=1.0745770931243896)
Finished Training
Total time taken: 0.18899893760681152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16639.295
Exploding loss, terminate run (best metric=1.0951493978500366)
Finished Training
Total time taken: 0.2219998836517334
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16601.721
Exploding loss, terminate run (best metric=1.097861886024475)
Finished Training
Total time taken: 0.19900107383728027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16635.664
Exploding loss, terminate run (best metric=1.0959281921386719)
Finished Training
Total time taken: 0.2220003604888916
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16493.389
Exploding loss, terminate run (best metric=1.1080095767974854)
Finished Training
Total time taken: 0.269000768661499
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16664.816
Exploding loss, terminate run (best metric=1.0740177631378174)
Finished Training
Total time taken: 0.20199918746948242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16639.709
Exploding loss, terminate run (best metric=1.1020326614379883)
Finished Training
Total time taken: 0.20599937438964844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16657.549
Exploding loss, terminate run (best metric=1.09334397315979)
Finished Training
Total time taken: 0.22400212287902832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16657.338
Exploding loss, terminate run (best metric=1.097800374031067)
Finished Training
Total time taken: 0.2010021209716797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16778.777
Exploding loss, terminate run (best metric=1.0762755870819092)
Finished Training
Total time taken: 0.21499991416931152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16626.312
Exploding loss, terminate run (best metric=1.0754601955413818)
Finished Training
Total time taken: 0.20099997520446777
{'Hydroxylation-K Validation Accuracy': 0.6184988179669031, 'Hydroxylation-K Validation Sensitivity': 0.31851851851851853, 'Hydroxylation-K Validation Specificity': 0.6964912280701754, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6172709551656921, 'Hydroxylation-K AUC PR': 0.34354147143224256, 'Hydroxylation-K MCC': 0.012034570542255367, 'Hydroxylation-K F1': 0.11405937957662096, 'Validation Loss (Hydroxylation-K)': 0.5568432768185934, 'Hydroxylation-P Validation Accuracy': 0.6216277346327598, 'Hydroxylation-P Validation Sensitivity': 0.32, 'Hydroxylation-P Validation Specificity': 0.6862985685071574, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5870872473550911, 'Hydroxylation-P AUC PR': 0.29108995556261186, 'Hydroxylation-P MCC': 0.013977277793546754, 'Hydroxylation-P F1': 0.10429632199472892, 'Validation Loss (Hydroxylation-P)': 0.5329320589701335, 'Validation Loss (total)': 1.0897753318150838, 'TimeToTrain': 0.21186723709106445}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008099351385748785,
 'learning_rate_Hydroxylation-K': 0.008922586854910158,
 'learning_rate_Hydroxylation-P': 0.0054676788696434,
 'log_base': 2.763866710281351,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2283089251,
 'sample_weights': [51.23199226704019, 6.390692654849148],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.31845943930896,
 'weight_decay_Hydroxylation-K': 2.5842216346343516,
 'weight_decay_Hydroxylation-P': 4.15127868866276}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.941
[2,     1] loss: 1259.540
[3,     1] loss: 1256.010
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007182865062506107,
 'learning_rate_Hydroxylation-K': 0.008468109181558917,
 'learning_rate_Hydroxylation-P': 0.009693965344647,
 'log_base': 1.686865169778938,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3055757563,
 'sample_weights': [1.6421333510829124, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.632838015951151,
 'weight_decay_Hydroxylation-K': 9.536608040387808,
 'weight_decay_Hydroxylation-P': 5.02736535337676}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1576.917
[2,     1] loss: 1587.718
[3,     1] loss: 1578.210
[4,     1] loss: 1587.424
[5,     1] loss: 1577.927
[6,     1] loss: 1568.096
[7,     1] loss: 1570.898
[8,     1] loss: 1551.382
[9,     1] loss: 1529.778
[10,     1] loss: 1496.618
[11,     1] loss: 1455.562
[12,     1] loss: 1388.936
[13,     1] loss: 1423.759
[14,     1] loss: 1447.555
[15,     1] loss: 1332.733
[16,     1] loss: 1378.742
[17,     1] loss: 1289.495
[18,     1] loss: 1343.298
[19,     1] loss: 1297.142
[20,     1] loss: 1319.018
[21,     1] loss: 1257.111
[22,     1] loss: 1242.383
[23,     1] loss: 1263.923
[24,     1] loss: 1250.919
[25,     1] loss: 1297.270
[26,     1] loss: 1290.573
[27,     1] loss: 1233.903
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003113862014978123,
 'learning_rate_Hydroxylation-K': 0.0006947949412721945,
 'learning_rate_Hydroxylation-P': 0.0009348719267027179,
 'log_base': 1.1668710002473832,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 662149321,
 'sample_weights': [3.192834076387232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.312962469419666,
 'weight_decay_Hydroxylation-K': 4.194169380282748,
 'weight_decay_Hydroxylation-P': 3.442549065156661}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3534.594
[2,     1] loss: 3512.791
[3,     1] loss: 3518.610
[4,     1] loss: 3507.854
[5,     1] loss: 3513.398
[6,     1] loss: 3498.346
[7,     1] loss: 3497.488
[8,     1] loss: 3486.758
[9,     1] loss: 3496.714
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022516848870267926,
 'learning_rate_Hydroxylation-K': 0.0023858360031193734,
 'learning_rate_Hydroxylation-P': 0.008257712546657602,
 'log_base': 2.058878103868786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3684340243,
 'sample_weights': [10.817653727599511, 1.3522588083825977],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.038455631156134,
 'weight_decay_Hydroxylation-K': 9.302784366633613,
 'weight_decay_Hydroxylation-P': 0.14308962016341134}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.802
[2,     1] loss: 1392.245
[3,     1] loss: 1394.609
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00962412331229927,
 'learning_rate_Hydroxylation-K': 0.0014237538154996516,
 'learning_rate_Hydroxylation-P': 0.00924361994287827,
 'log_base': 2.9943233400369156,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1429182154,
 'sample_weights': [2.311731910245278, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.531447033556067,
 'weight_decay_Hydroxylation-K': 7.453966792266497,
 'weight_decay_Hydroxylation-P': 4.287871328550581}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.425
[2,     1] loss: 1225.974
[3,     1] loss: 1230.424
[4,     1] loss: 1228.576
[5,     1] loss: 1222.362
[6,     1] loss: 1222.104
[7,     1] loss: 1208.293
[8,     1] loss: 1196.912
[9,     1] loss: 1187.650
[10,     1] loss: 1146.037
[11,     1] loss: 1061.361
[12,     1] loss: 1088.780
[13,     1] loss: 1060.195
[14,     1] loss: 984.587
[15,     1] loss: 998.262
[16,     1] loss: 985.815
[17,     1] loss: 1000.524
[18,     1] loss: 951.669
[19,     1] loss: 996.383
[20,     1] loss: 985.953
[21,     1] loss: 951.522
[22,     1] loss: 934.839
[23,     1] loss: 924.897
[24,     1] loss: 893.535
[25,     1] loss: 923.643
[26,     1] loss: 864.522
[27,     1] loss: 906.895
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001559848467797022,
 'learning_rate_Hydroxylation-K': 0.0007705098085430233,
 'learning_rate_Hydroxylation-P': 0.002703229687788162,
 'log_base': 1.4279228592826056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1357638580,
 'sample_weights': [1.522216948232911, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.181200592771846,
 'weight_decay_Hydroxylation-K': 0.4563108000977791,
 'weight_decay_Hydroxylation-P': 1.8127957938947015}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1902.382
[2,     1] loss: 1895.294
[3,     1] loss: 1894.778
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008542517264996186,
 'learning_rate_Hydroxylation-K': 0.009304401033276101,
 'learning_rate_Hydroxylation-P': 0.004656981148768104,
 'log_base': 2.986329188143661,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 810324589,
 'sample_weights': [4.686539778019497, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.776374267398554,
 'weight_decay_Hydroxylation-K': 5.40401314803988,
 'weight_decay_Hydroxylation-P': 4.144420733363219}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.386
[2,     1] loss: 1234.172
[3,     1] loss: 1235.683
[4,     1] loss: 1229.863
[5,     1] loss: 1230.263
[6,     1] loss: 1229.686
[7,     1] loss: 1230.224
[8,     1] loss: 1228.828
[9,     1] loss: 1229.794
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035484810505737657,
 'learning_rate_Hydroxylation-K': 0.00043964604825280434,
 'learning_rate_Hydroxylation-P': 0.000846853007423144,
 'log_base': 1.21406452950739,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 505837947,
 'sample_weights': [1.5259365416603161, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.821150312979817,
 'weight_decay_Hydroxylation-K': 1.1726495706498006,
 'weight_decay_Hydroxylation-P': 2.2473548742474545}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2804.031
[2,     1] loss: 2793.433
[3,     1] loss: 2792.119
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036148152151215937,
 'learning_rate_Hydroxylation-K': 0.0006482933373304972,
 'learning_rate_Hydroxylation-P': 0.0014471780172594802,
 'log_base': 1.28352955674107,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3463202758,
 'sample_weights': [8.606537348275554, 1.0758586133318588],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.226442767055875,
 'weight_decay_Hydroxylation-K': 0.8296870602407068,
 'weight_decay_Hydroxylation-P': 2.6703382251643872}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2319.385
[2,     1] loss: 2336.913
[3,     1] loss: 2311.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007595954502279106,
 'learning_rate_Hydroxylation-K': 0.005997682216316116,
 'learning_rate_Hydroxylation-P': 0.0022696137600151023,
 'log_base': 1.1382773313456265,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3535895196,
 'sample_weights': [6.688105731775907, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6536704072389,
 'weight_decay_Hydroxylation-K': 9.417293684504273,
 'weight_decay_Hydroxylation-P': 8.016664309072816}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4201.530
[2,     1] loss: 4227.274
[3,     1] loss: 4175.783
[4,     1] loss: 4184.965
[5,     1] loss: 4185.648
[6,     1] loss: 4180.567
[7,     1] loss: 4177.084
[8,     1] loss: 4178.786
[9,     1] loss: 4184.836
[10,     1] loss: 4184.102
[11,     1] loss: 4176.536
[12,     1] loss: 4175.297
[13,     1] loss: 4172.868
[14,     1] loss: 4172.969
[15,     1] loss: 4176.705
[16,     1] loss: 4170.779
[17,     1] loss: 4161.719
[18,     1] loss: 4149.353
[19,     1] loss: 4123.703
[20,     1] loss: 4078.482
[21,     1] loss: 3989.743
[22,     1] loss: 3940.327
[23,     1] loss: 3853.759
[24,     1] loss: 3713.146
[25,     1] loss: 3705.612
[26,     1] loss: 3587.037
[27,     1] loss: 3368.819
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027656239469234485,
 'learning_rate_Hydroxylation-K': 0.006642005592446567,
 'learning_rate_Hydroxylation-P': 0.0014373444510492927,
 'log_base': 2.2832342617417374,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 910443960,
 'sample_weights': [12.889859642507956, 1.611294526457785],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.687335250100677,
 'weight_decay_Hydroxylation-K': 9.85173019916782,
 'weight_decay_Hydroxylation-P': 5.352556863955474}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.928
[2,     1] loss: 1333.284
[3,     1] loss: 1335.143
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009729077996930355,
 'learning_rate_Hydroxylation-K': 0.005204908142205694,
 'learning_rate_Hydroxylation-P': 0.007049358684597395,
 'log_base': 1.7372132393448618,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4214634089,
 'sample_weights': [2.022114044951329, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.598433724272966,
 'weight_decay_Hydroxylation-K': 4.643314214503703,
 'weight_decay_Hydroxylation-P': 0.022113695895774832}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1548.914
[2,     1] loss: 1559.924
[3,     1] loss: 1540.734
[4,     1] loss: 1545.183
[5,     1] loss: 1544.211
[6,     1] loss: 1539.753
[7,     1] loss: 1533.523
[8,     1] loss: 1545.225
[9,     1] loss: 1541.385
[10,     1] loss: 1541.388
[11,     1] loss: 1535.410
[12,     1] loss: 1540.540
[13,     1] loss: 1531.736
[14,     1] loss: 1529.153
[15,     1] loss: 1529.862
[16,     1] loss: 1509.168
[17,     1] loss: 1510.693
[18,     1] loss: 1496.846
[19,     1] loss: 1471.848
[20,     1] loss: 1472.000
[21,     1] loss: 1457.051
[22,     1] loss: 1442.714
[23,     1] loss: 1412.191
[24,     1] loss: 1393.761
[25,     1] loss: 1401.035
[26,     1] loss: 1372.324
[27,     1] loss: 1351.931
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005843977874611515,
 'learning_rate_Hydroxylation-K': 0.0037290922573611457,
 'learning_rate_Hydroxylation-P': 0.0037835856766941017,
 'log_base': 1.1587355965348722,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2812912915,
 'sample_weights': [3.02280793813829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.210440673133338,
 'weight_decay_Hydroxylation-K': 5.686134776611793,
 'weight_decay_Hydroxylation-P': 0.15189266448753092}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3663.582
[2,     1] loss: 3672.668
[3,     1] loss: 3679.602
[4,     1] loss: 3666.936
[5,     1] loss: 3658.070
[6,     1] loss: 3671.213
[7,     1] loss: 3653.938
[8,     1] loss: 3671.370
[9,     1] loss: 3612.835
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007474975459103536,
 'learning_rate_Hydroxylation-K': 0.0021831681227523223,
 'learning_rate_Hydroxylation-P': 8.353270614133322e-05,
 'log_base': 1.684951968254157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4237386579,
 'sample_weights': [11.33136401007723, 1.4164750674651816],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.75069445078902,
 'weight_decay_Hydroxylation-K': 5.324823751397092,
 'weight_decay_Hydroxylation-P': 3.6464747305928356}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1579.272
[2,     1] loss: 1598.131
[3,     1] loss: 1575.629
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014326980217900995,
 'learning_rate_Hydroxylation-K': 0.007936842079124762,
 'learning_rate_Hydroxylation-P': 0.006933727232207413,
 'log_base': 2.587806864545253,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3201057991,
 'sample_weights': [3.199778743163014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.643809612580206,
 'weight_decay_Hydroxylation-K': 2.280259709514053,
 'weight_decay_Hydroxylation-P': 4.0811809516046615}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.527
[2,     1] loss: 1276.973
[3,     1] loss: 1280.417
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009001746117076239,
 'learning_rate_Hydroxylation-K': 0.007566583765596638,
 'learning_rate_Hydroxylation-P': 0.004617583185643611,
 'log_base': 2.4484640580018784,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1934975012,
 'sample_weights': [1.7558101372707076, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.456574911661793,
 'weight_decay_Hydroxylation-K': 6.791014380003334,
 'weight_decay_Hydroxylation-P': 2.195312297599643}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1308.666
[2,     1] loss: 1305.359
[3,     1] loss: 1307.191
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004834897048766946,
 'learning_rate_Hydroxylation-K': 0.0003073896588274127,
 'learning_rate_Hydroxylation-P': 0.0017570752920494695,
 'log_base': 1.0753597846449168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 453614323,
 'sample_weights': [1.864339496503629, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.079642429261997,
 'weight_decay_Hydroxylation-K': 1.0962511912074482,
 'weight_decay_Hydroxylation-P': 0.8541532100941177}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7448.646
[2,     1] loss: 7452.842
[3,     1] loss: 7424.135
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009729489811541567,
 'learning_rate_Hydroxylation-K': 0.00036544797631982325,
 'learning_rate_Hydroxylation-P': 0.007791766449758633,
 'log_base': 1.0372949020074376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 228852023,
 'sample_weights': [22.97758595343704, 2.872308892789678],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.535106976029704,
 'weight_decay_Hydroxylation-K': 9.30129057953758,
 'weight_decay_Hydroxylation-P': 5.381439415458276}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14823.027
[2,     1] loss: 14800.400
[3,     1] loss: 14909.854
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00368890491960206,
 'learning_rate_Hydroxylation-K': 0.0021369138632248465,
 'learning_rate_Hydroxylation-P': 0.0019910378269745407,
 'log_base': 1.2640334911466717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3291105755,
 'sample_weights': [45.592934604654936, 5.699336378447527],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.737472633295789,
 'weight_decay_Hydroxylation-K': 0.1356556300047355,
 'weight_decay_Hydroxylation-P': 4.139880569369739}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2418.440
[2,     1] loss: 2410.930
[3,     1] loss: 2408.673
[4,     1] loss: 2406.911
[5,     1] loss: 2408.376
[6,     1] loss: 2401.122
[7,     1] loss: 2412.534
[8,     1] loss: 2407.942
[9,     1] loss: 2409.590
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038120080979627924,
 'learning_rate_Hydroxylation-K': 0.009106301340212217,
 'learning_rate_Hydroxylation-P': 0.0057086500804736364,
 'log_base': 2.8390199693617095,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3938035829,
 'sample_weights': [7.125000566536825, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.356088117317189,
 'weight_decay_Hydroxylation-K': 3.9138421064278246,
 'weight_decay_Hydroxylation-P': 1.5888456385706862}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.194
[2,     1] loss: 1246.506
[3,     1] loss: 1243.829
[4,     1] loss: 1238.895
[5,     1] loss: 1245.385
[6,     1] loss: 1232.962
[7,     1] loss: 1230.032
[8,     1] loss: 1205.431
[9,     1] loss: 1210.138
[10,     1] loss: 1157.447
[11,     1] loss: 1154.989
[12,     1] loss: 1102.127
[13,     1] loss: 1113.657
[14,     1] loss: 1090.363
[15,     1] loss: 1083.814
[16,     1] loss: 1071.742
[17,     1] loss: 1023.730
[18,     1] loss: 984.624
[19,     1] loss: 982.427
[20,     1] loss: 1026.896
[21,     1] loss: 1008.474
[22,     1] loss: 993.591
[23,     1] loss: 971.018
[24,     1] loss: 983.269
[25,     1] loss: 1026.496
[26,     1] loss: 966.361
[27,     1] loss: 1018.611
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003387180185341703,
 'learning_rate_Hydroxylation-K': 0.009148303485072057,
 'learning_rate_Hydroxylation-P': 0.007415695790360351,
 'log_base': 2.9225856839440376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 439134933,
 'sample_weights': [1.5999126838359474, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.88699845386106,
 'weight_decay_Hydroxylation-K': 5.4414168983594955,
 'weight_decay_Hydroxylation-P': 4.678167276238537}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.568
[2,     1] loss: 1239.314
[3,     1] loss: 1233.292
[4,     1] loss: 1235.740
[5,     1] loss: 1235.183
[6,     1] loss: 1224.456
[7,     1] loss: 1215.284
[8,     1] loss: 1192.902
[9,     1] loss: 1157.133
[10,     1] loss: 1127.635
[11,     1] loss: 1105.125
[12,     1] loss: 1066.446
[13,     1] loss: 1060.310
[14,     1] loss: 1013.026
[15,     1] loss: 1003.348
[16,     1] loss: 1068.828
[17,     1] loss: 1030.166
[18,     1] loss: 993.604
[19,     1] loss: 1027.419
[20,     1] loss: 979.914
[21,     1] loss: 963.718
[22,     1] loss: 990.090
[23,     1] loss: 998.619
[24,     1] loss: 965.472
[25,     1] loss: 965.733
[26,     1] loss: 951.541
[27,     1] loss: 942.549
[28,     1] loss: 933.387
[29,     1] loss: 886.497
[30,     1] loss: 932.613
[31,     1] loss: 861.062
[32,     1] loss: 867.183
[33,     1] loss: 912.567
[34,     1] loss: 870.933
[35,     1] loss: 874.838
[36,     1] loss: 899.476
[37,     1] loss: 865.731
[38,     1] loss: 876.355
[39,     1] loss: 887.060
[40,     1] loss: 773.613
[41,     1] loss: 827.069
[42,     1] loss: 829.587
[43,     1] loss: 784.188
[44,     1] loss: 783.429
[45,     1] loss: 802.457
[46,     1] loss: 774.572
[47,     1] loss: 745.730
[48,     1] loss: 745.094
[49,     1] loss: 780.423
[50,     1] loss: 807.265
[51,     1] loss: 747.851
[52,     1] loss: 767.780
[53,     1] loss: 764.524
[54,     1] loss: 773.887
[55,     1] loss: 869.570
[56,     1] loss: 752.216
[57,     1] loss: 716.577
[58,     1] loss: 753.731
[59,     1] loss: 664.067
[60,     1] loss: 761.435
[61,     1] loss: 723.989
[62,     1] loss: 732.945
[63,     1] loss: 752.071
[64,     1] loss: 645.848
[65,     1] loss: 761.626
[66,     1] loss: 641.667
[67,     1] loss: 710.117
[68,     1] loss: 656.237
[69,     1] loss: 618.618
[70,     1] loss: 623.649
[71,     1] loss: 647.774
[72,     1] loss: 599.545
[73,     1] loss: 698.854
[74,     1] loss: 805.584
[75,     1] loss: 619.092
[76,     1] loss: 638.398
[77,     1] loss: 755.413
[78,     1] loss: 633.431
[79,     1] loss: 699.323
[80,     1] loss: 650.187
[81,     1] loss: 675.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061445871424002626,
 'learning_rate_Hydroxylation-K': 0.008538380890920886,
 'learning_rate_Hydroxylation-P': 0.004634088853618786,
 'log_base': 1.5551881766934441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2354849853,
 'sample_weights': [1.5566357289503148, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.78721435836194,
 'weight_decay_Hydroxylation-K': 4.22697016971247,
 'weight_decay_Hydroxylation-P': 1.1548651522066122}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1704.065
[2,     1] loss: 1709.019
[3,     1] loss: 1708.111
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00899300563814534,
 'learning_rate_Hydroxylation-K': 0.001997971570892169,
 'learning_rate_Hydroxylation-P': 0.0036101444453000738,
 'log_base': 1.369529629280731,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3021439281,
 'sample_weights': [3.780471425509135, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.706974538741573,
 'weight_decay_Hydroxylation-K': 3.9945296011869225,
 'weight_decay_Hydroxylation-P': 4.982952450585801}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2033.094
[2,     1] loss: 2023.669
[3,     1] loss: 2026.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025633994049295036,
 'learning_rate_Hydroxylation-K': 0.0018628828945406956,
 'learning_rate_Hydroxylation-P': 0.00041650332902532297,
 'log_base': 2.098436768355565,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3776475415,
 'sample_weights': [5.308796528850034, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.958483081464124,
 'weight_decay_Hydroxylation-K': 9.337667502163306,
 'weight_decay_Hydroxylation-P': 9.782349567044141}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.892
[2,     1] loss: 1380.620
[3,     1] loss: 1382.139
[4,     1] loss: 1382.780
[5,     1] loss: 1378.549
[6,     1] loss: 1373.341
[7,     1] loss: 1376.000
[8,     1] loss: 1363.994
[9,     1] loss: 1350.443
[10,     1] loss: 1333.740
[11,     1] loss: 1303.750
[12,     1] loss: 1275.907
[13,     1] loss: 1228.883
[14,     1] loss: 1182.828
[15,     1] loss: 1194.995
[16,     1] loss: 1183.701
[17,     1] loss: 1195.385
[18,     1] loss: 1109.797
[19,     1] loss: 1196.080
[20,     1] loss: 1140.278
[21,     1] loss: 1110.452
[22,     1] loss: 1119.017
[23,     1] loss: 1078.603
[24,     1] loss: 1091.800
[25,     1] loss: 1101.602
[26,     1] loss: 1143.997
[27,     1] loss: 1102.273
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005471767904913137,
 'learning_rate_Hydroxylation-K': 0.00945018406206265,
 'learning_rate_Hydroxylation-P': 0.0007683964926948798,
 'log_base': 1.5109190702203052,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3174157079,
 'sample_weights': [2.252374060886568, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.33398288548217475,
 'weight_decay_Hydroxylation-K': 0.9652842857490309,
 'weight_decay_Hydroxylation-P': 2.149575104381742}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1780.322
[2,     1] loss: 1768.322
[3,     1] loss: 1768.466
[4,     1] loss: 1758.922
[5,     1] loss: 1761.196
[6,     1] loss: 1771.811
[7,     1] loss: 1761.861
[8,     1] loss: 1762.661
[9,     1] loss: 1763.676
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00808584434901211,
 'learning_rate_Hydroxylation-K': 0.009767895972223935,
 'learning_rate_Hydroxylation-P': 0.002419405306434779,
 'log_base': 1.4256732471141642,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1001408921,
 'sample_weights': [4.0449959928445205, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3798430171965737,
 'weight_decay_Hydroxylation-K': 3.593170430937201,
 'weight_decay_Hydroxylation-P': 6.076750459963779}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1896.419
[2,     1] loss: 1896.367
[3,     1] loss: 1903.177
[4,     1] loss: 1907.713
[5,     1] loss: 1899.954
[6,     1] loss: 1908.054
[7,     1] loss: 1899.142
[8,     1] loss: 1904.462
[9,     1] loss: 1897.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009732715315022927,
 'learning_rate_Hydroxylation-K': 0.0024299135672987996,
 'learning_rate_Hydroxylation-P': 0.004909034428378962,
 'log_base': 1.0111773399615651,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2806477134,
 'sample_weights': [4.7073753167344385, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.32770459424052,
 'weight_decay_Hydroxylation-K': 7.895360060851571,
 'weight_decay_Hydroxylation-P': 8.45509582659838}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48719.246
Exploding loss, terminate run (best metric=1.1122758388519287)
Finished Training
Total time taken: 0.22300267219543457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48673.246
Exploding loss, terminate run (best metric=1.0924550294876099)
Finished Training
Total time taken: 0.19999933242797852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48759.617
Exploding loss, terminate run (best metric=1.1205637454986572)
Finished Training
Total time taken: 0.20800089836120605
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49006.859
Exploding loss, terminate run (best metric=1.1250979900360107)
Finished Training
Total time taken: 0.24000000953674316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 48928.508
Exploding loss, terminate run (best metric=1.0736544132232666)
Finished Training
Total time taken: 0.1979994773864746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48687.191
Exploding loss, terminate run (best metric=1.099719762802124)
Finished Training
Total time taken: 0.20800018310546875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48793.434
Exploding loss, terminate run (best metric=1.0935447216033936)
Finished Training
Total time taken: 0.2239990234375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49000.418
Exploding loss, terminate run (best metric=1.0955777168273926)
Finished Training
Total time taken: 0.21500182151794434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48619.871
Exploding loss, terminate run (best metric=1.0792887210845947)
Finished Training
Total time taken: 0.1999974250793457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49036.047
Exploding loss, terminate run (best metric=1.090014100074768)
Finished Training
Total time taken: 0.21800017356872559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48727.660
Exploding loss, terminate run (best metric=1.1027655601501465)
Finished Training
Total time taken: 0.21599984169006348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48755.656
Exploding loss, terminate run (best metric=1.164215326309204)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48717.617
Exploding loss, terminate run (best metric=1.093576431274414)
Finished Training
Total time taken: 0.2279987335205078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 48770.289
Exploding loss, terminate run (best metric=1.0864102840423584)
Finished Training
Total time taken: 0.2369999885559082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 48842.133
Exploding loss, terminate run (best metric=1.0780621767044067)
Finished Training
Total time taken: 0.20099949836730957
{'Hydroxylation-K Validation Accuracy': 0.6383569739952718, 'Hydroxylation-K Validation Sensitivity': 0.26666666666666666, 'Hydroxylation-K Validation Specificity': 0.7333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5418128654970761, 'Hydroxylation-K AUC PR': 0.2911736643462842, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.08883415435139573, 'Validation Loss (Hydroxylation-K)': 0.5618282357851664, 'Hydroxylation-P Validation Accuracy': 0.6509223558871801, 'Hydroxylation-P Validation Sensitivity': 0.26666666666666666, 'Hydroxylation-P Validation Specificity': 0.7333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5464140702501095, 'Hydroxylation-P AUC PR': 0.22424013314325658, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.08025363524017777, 'Validation Loss (Hydroxylation-P)': 0.5386532107988994, 'Validation Loss (total)': 1.1004814545313517, 'TimeToTrain': 0.21466660499572754}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005223375911137705,
 'learning_rate_Hydroxylation-K': 0.003507284565197748,
 'learning_rate_Hydroxylation-P': 0.005812518410733926,
 'log_base': 2.678221795545627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1546949483,
 'sample_weights': [150.30419886470068, 18.7489866619064],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.6063106672322425,
 'weight_decay_Hydroxylation-K': 8.460412337440294,
 'weight_decay_Hydroxylation-P': 2.7434455954167545}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.338
[2,     1] loss: 1268.900
[3,     1] loss: 1264.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007468486462396505,
 'learning_rate_Hydroxylation-K': 0.0015638941111963288,
 'learning_rate_Hydroxylation-P': 0.004895174657515372,
 'log_base': 1.1928005829814432,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 512947054,
 'sample_weights': [1.6946028048152604, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.763677774912912,
 'weight_decay_Hydroxylation-K': 6.40780269036193,
 'weight_decay_Hydroxylation-P': 5.484991859552756}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3095.377
[2,     1] loss: 3075.711
[3,     1] loss: 3077.269
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006940408125802887,
 'learning_rate_Hydroxylation-K': 0.008730728396216626,
 'learning_rate_Hydroxylation-P': 0.0022428949716704295,
 'log_base': 2.676088359376023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1874299072,
 'sample_weights': [9.469118119949055, 1.1836853635504376],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.45726584968724,
 'weight_decay_Hydroxylation-K': 8.800580712740476,
 'weight_decay_Hydroxylation-P': 9.962337369369912}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.268
[2,     1] loss: 1265.481
[3,     1] loss: 1264.422
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 7.164257076863958e-05,
 'learning_rate_Hydroxylation-K': 0.00962028808629536,
 'learning_rate_Hydroxylation-P': 0.00523293096212157,
 'log_base': 1.8431315900766405,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2239704731,
 'sample_weights': [1.6959747027579224, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.84381994384234,
 'weight_decay_Hydroxylation-K': 4.22530928327055,
 'weight_decay_Hydroxylation-P': 1.2043546021433285}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1486.014
[2,     1] loss: 1483.828
[3,     1] loss: 1485.495
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004828055293425463,
 'learning_rate_Hydroxylation-K': 0.002533573284118549,
 'learning_rate_Hydroxylation-P': 0.003398298399665308,
 'log_base': 1.4898773346282623,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1977028171,
 'sample_weights': [2.730230200419775, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8398866190425514,
 'weight_decay_Hydroxylation-K': 7.709508337412516,
 'weight_decay_Hydroxylation-P': 2.895794813782296}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1792.980
[2,     1] loss: 1789.884
[3,     1] loss: 1797.812
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006448614094632171,
 'learning_rate_Hydroxylation-K': 0.004802115839743686,
 'learning_rate_Hydroxylation-P': 0.0019038276440534172,
 'log_base': 1.2941462527016454,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 457652375,
 'sample_weights': [4.187281532548171, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.5537765058596555,
 'weight_decay_Hydroxylation-K': 7.939724803553309,
 'weight_decay_Hydroxylation-P': 0.9132687144008629}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2268.734
[2,     1] loss: 2308.810
[3,     1] loss: 2275.712
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003043823436323594,
 'learning_rate_Hydroxylation-K': 0.003790688696668299,
 'learning_rate_Hydroxylation-P': 0.004109163027122386,
 'log_base': 1.025933656141513,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 268908745,
 'sample_weights': [6.4744436347339995, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.05959198110781,
 'weight_decay_Hydroxylation-K': 4.565653819238447,
 'weight_decay_Hydroxylation-P': 2.014563354849967}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21187.842
Exploding loss, terminate run (best metric=1.0983381271362305)
Finished Training
Total time taken: 0.2129993438720703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21326.387
Exploding loss, terminate run (best metric=1.0961393117904663)
Finished Training
Total time taken: 0.20400094985961914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21142.707
Exploding loss, terminate run (best metric=1.092578649520874)
Finished Training
Total time taken: 0.22999882698059082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21191.324
Exploding loss, terminate run (best metric=1.0759207010269165)
Finished Training
Total time taken: 0.22800016403198242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21228.629
Exploding loss, terminate run (best metric=1.0735820531845093)
Finished Training
Total time taken: 0.1979978084564209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21180.070
Exploding loss, terminate run (best metric=1.0954697132110596)
Finished Training
Total time taken: 0.2200005054473877
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21113.852
Exploding loss, terminate run (best metric=1.0917720794677734)
Finished Training
Total time taken: 0.20499849319458008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21158.750
Exploding loss, terminate run (best metric=1.0888538360595703)
Finished Training
Total time taken: 0.21000003814697266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21119.258
Exploding loss, terminate run (best metric=1.0716454982757568)
Finished Training
Total time taken: 0.22799992561340332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21178.297
Exploding loss, terminate run (best metric=1.0861937999725342)
Finished Training
Total time taken: 0.21900033950805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21205.977
Exploding loss, terminate run (best metric=1.0996828079223633)
Finished Training
Total time taken: 0.20099925994873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21178.496
Exploding loss, terminate run (best metric=1.0895583629608154)
Finished Training
Total time taken: 0.22499966621398926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21208.805
Exploding loss, terminate run (best metric=1.0925720930099487)
Finished Training
Total time taken: 0.22600245475769043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21077.375
Exploding loss, terminate run (best metric=1.0919907093048096)
Finished Training
Total time taken: 0.20199894905090332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21333.453
Exploding loss, terminate run (best metric=1.0802291631698608)
Finished Training
Total time taken: 0.21499991416931152
{'Hydroxylation-K Validation Accuracy': 0.5633569739952718, 'Hydroxylation-K Validation Sensitivity': 0.4066666666666667, 'Hydroxylation-K Validation Specificity': 0.6017543859649123, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6714035087719299, 'Hydroxylation-K AUC PR': 0.36810038701967224, 'Hydroxylation-K MCC': 0.023944691776627978, 'Hydroxylation-K F1': 0.14733568508523137, 'Validation Loss (Hydroxylation-K)': 0.556803019841512, 'Hydroxylation-P Validation Accuracy': 0.5678302285839975, 'Hydroxylation-P Validation Sensitivity': 0.4111111111111111, 'Hydroxylation-P Validation Specificity': 0.6016260162601627, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6036702756817226, 'Hydroxylation-P AUC PR': 0.2916082394913827, 'Hydroxylation-P MCC': 0.02943481685142572, 'Hydroxylation-P F1': 0.13953412930328052, 'Validation Loss (Hydroxylation-P)': 0.5314987540245056, 'Validation Loss (total)': 1.0883017937342325, 'TimeToTrain': 0.21493310928344728}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00042971431171540597,
 'learning_rate_Hydroxylation-K': 0.0045814487626853525,
 'learning_rate_Hydroxylation-P': 0.005556024941398587,
 'log_base': 1.1877734046977821,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2720957099,
 'sample_weights': [65.2531416281711, 8.139694641101572],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.575965945004143,
 'weight_decay_Hydroxylation-K': 4.1204083081099565,
 'weight_decay_Hydroxylation-P': 0.9316191833869161}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3150.307
[2,     1] loss: 3192.314
[3,     1] loss: 3157.971
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009230848854669184,
 'learning_rate_Hydroxylation-K': 0.005424494670308487,
 'learning_rate_Hydroxylation-P': 0.005608732597116391,
 'log_base': 2.957169795024537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3526353094,
 'sample_weights': [9.701526190643731, 1.2127374915487907],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0271202474848726,
 'weight_decay_Hydroxylation-K': 6.4603768987783,
 'weight_decay_Hydroxylation-P': 8.027504401491612}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.121
[2,     1] loss: 1234.760
[3,     1] loss: 1236.849
[4,     1] loss: 1235.354
[5,     1] loss: 1231.150
[6,     1] loss: 1224.191
[7,     1] loss: 1219.712
[8,     1] loss: 1211.524
[9,     1] loss: 1204.126
[10,     1] loss: 1176.389
[11,     1] loss: 1131.074
[12,     1] loss: 1099.007
[13,     1] loss: 1023.807
[14,     1] loss: 1047.067
[15,     1] loss: 1043.129
[16,     1] loss: 1063.762
[17,     1] loss: 1027.857
[18,     1] loss: 1016.020
[19,     1] loss: 1005.377
[20,     1] loss: 1008.640
[21,     1] loss: 999.471
[22,     1] loss: 957.885
[23,     1] loss: 988.209
[24,     1] loss: 957.004
[25,     1] loss: 923.929
[26,     1] loss: 990.576
[27,     1] loss: 930.021
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002671992824816909,
 'learning_rate_Hydroxylation-K': 0.006235414214278985,
 'learning_rate_Hydroxylation-P': 0.003029366391685707,
 'log_base': 2.9339290016364017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4176176970,
 'sample_weights': [1.539746226142128, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.257775907643102,
 'weight_decay_Hydroxylation-K': 8.524053591472555,
 'weight_decay_Hydroxylation-P': 0.3243568946653781}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.471
[2,     1] loss: 1234.781
[3,     1] loss: 1234.723
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018466897390379854,
 'learning_rate_Hydroxylation-K': 0.00043377210075272395,
 'learning_rate_Hydroxylation-P': 0.009482616040031207,
 'log_base': 1.71460554631492,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 970592528,
 'sample_weights': [1.551033408962541, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.418423483085234,
 'weight_decay_Hydroxylation-K': 5.5424717817659195,
 'weight_decay_Hydroxylation-P': 7.898593374388801}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1563.360
[2,     1] loss: 1559.716
[3,     1] loss: 1562.045
[4,     1] loss: 1564.616
[5,     1] loss: 1551.266
[6,     1] loss: 1555.184
[7,     1] loss: 1550.894
[8,     1] loss: 1558.904
[9,     1] loss: 1543.788
[10,     1] loss: 1532.166
[11,     1] loss: 1513.347
[12,     1] loss: 1491.145
[13,     1] loss: 1457.923
[14,     1] loss: 1442.218
[15,     1] loss: 1420.510
[16,     1] loss: 1390.408
[17,     1] loss: 1364.356
[18,     1] loss: 1354.168
[19,     1] loss: 1326.177
[20,     1] loss: 1346.264
[21,     1] loss: 1330.990
[22,     1] loss: 1313.986
[23,     1] loss: 1336.183
[24,     1] loss: 1240.751
[25,     1] loss: 1281.020
[26,     1] loss: 1348.466
[27,     1] loss: 1270.093
[28,     1] loss: 1282.371
[29,     1] loss: 1208.531
[30,     1] loss: 1247.275
[31,     1] loss: 1268.052
[32,     1] loss: 1257.250
[33,     1] loss: 1272.000
[34,     1] loss: 1206.489
[35,     1] loss: 1267.702
[36,     1] loss: 1279.965
[37,     1] loss: 1207.748
[38,     1] loss: 1222.001
[39,     1] loss: 1187.582
[40,     1] loss: 1141.375
[41,     1] loss: 1126.880
[42,     1] loss: 1166.798
[43,     1] loss: 1199.974
[44,     1] loss: 1158.557
[45,     1] loss: 1105.968
[46,     1] loss: 1121.773
[47,     1] loss: 1088.722
[48,     1] loss: 1118.770
[49,     1] loss: 1046.146
[50,     1] loss: 1037.076
[51,     1] loss: 995.015
[52,     1] loss: 1086.680
[53,     1] loss: 1041.338
[54,     1] loss: 998.704
[55,     1] loss: 947.565
[56,     1] loss: 1100.743
[57,     1] loss: 1034.948
[58,     1] loss: 1036.571
[59,     1] loss: 978.980
[60,     1] loss: 953.658
[61,     1] loss: 960.026
[62,     1] loss: 934.280
Early stopping applied (best metric=0.8482149243354797)
Finished Training
Total time taken: 8.532011032104492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.243
[2,     1] loss: 1558.262
[3,     1] loss: 1561.063
[4,     1] loss: 1564.533
[5,     1] loss: 1561.119
[6,     1] loss: 1558.503
[7,     1] loss: 1554.644
[8,     1] loss: 1554.177
[9,     1] loss: 1551.256
[10,     1] loss: 1548.201
[11,     1] loss: 1535.961
[12,     1] loss: 1524.420
[13,     1] loss: 1505.188
[14,     1] loss: 1484.667
[15,     1] loss: 1460.886
[16,     1] loss: 1430.008
[17,     1] loss: 1390.533
[18,     1] loss: 1366.251
[19,     1] loss: 1347.375
[20,     1] loss: 1329.956
[21,     1] loss: 1306.504
[22,     1] loss: 1263.983
[23,     1] loss: 1283.047
[24,     1] loss: 1280.984
[25,     1] loss: 1280.968
[26,     1] loss: 1271.281
[27,     1] loss: 1221.283
[28,     1] loss: 1300.028
[29,     1] loss: 1251.271
[30,     1] loss: 1281.328
[31,     1] loss: 1266.203
[32,     1] loss: 1267.847
[33,     1] loss: 1250.004
[34,     1] loss: 1225.842
[35,     1] loss: 1198.188
[36,     1] loss: 1194.508
[37,     1] loss: 1181.202
[38,     1] loss: 1145.722
[39,     1] loss: 1140.027
[40,     1] loss: 1200.167
[41,     1] loss: 1188.137
[42,     1] loss: 1153.034
[43,     1] loss: 1097.863
[44,     1] loss: 1116.490
[45,     1] loss: 1129.871
[46,     1] loss: 1169.714
[47,     1] loss: 1116.722
[48,     1] loss: 1081.500
[49,     1] loss: 1081.611
[50,     1] loss: 1042.030
[51,     1] loss: 1060.434
[52,     1] loss: 1096.410
[53,     1] loss: 1081.982
[54,     1] loss: 997.734
[55,     1] loss: 1007.824
[56,     1] loss: 1048.761
[57,     1] loss: 1004.086
[58,     1] loss: 1022.609
[59,     1] loss: 1042.174
[60,     1] loss: 976.041
[61,     1] loss: 892.234
[62,     1] loss: 982.507
[63,     1] loss: 945.655
[64,     1] loss: 881.047
Early stopping applied (best metric=0.8540465831756592)
Finished Training
Total time taken: 10.723009586334229
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1564.917
[2,     1] loss: 1560.701
[3,     1] loss: 1560.892
[4,     1] loss: 1565.330
[5,     1] loss: 1557.264
[6,     1] loss: 1561.385
[7,     1] loss: 1557.825
[8,     1] loss: 1555.733
[9,     1] loss: 1552.107
[10,     1] loss: 1548.661
[11,     1] loss: 1543.244
[12,     1] loss: 1534.411
[13,     1] loss: 1512.536
[14,     1] loss: 1489.939
[15,     1] loss: 1480.715
[16,     1] loss: 1452.359
[17,     1] loss: 1410.796
[18,     1] loss: 1389.971
[19,     1] loss: 1357.001
[20,     1] loss: 1325.207
[21,     1] loss: 1343.051
[22,     1] loss: 1332.349
[23,     1] loss: 1266.975
[24,     1] loss: 1351.294
[25,     1] loss: 1324.123
[26,     1] loss: 1255.753
[27,     1] loss: 1302.975
[28,     1] loss: 1266.522
[29,     1] loss: 1276.168
[30,     1] loss: 1295.952
[31,     1] loss: 1253.466
[32,     1] loss: 1266.830
[33,     1] loss: 1238.036
[34,     1] loss: 1271.507
[35,     1] loss: 1241.604
[36,     1] loss: 1201.283
[37,     1] loss: 1168.758
[38,     1] loss: 1198.772
[39,     1] loss: 1183.617
[40,     1] loss: 1154.500
[41,     1] loss: 1156.214
[42,     1] loss: 1171.020
[43,     1] loss: 1134.049
[44,     1] loss: 1148.071
[45,     1] loss: 1115.397
[46,     1] loss: 1091.897
[47,     1] loss: 1082.390
[48,     1] loss: 1141.209
[49,     1] loss: 1095.896
[50,     1] loss: 1109.586
[51,     1] loss: 1080.041
[52,     1] loss: 1000.348
[53,     1] loss: 1015.850
[54,     1] loss: 977.560
[55,     1] loss: 1024.453
[56,     1] loss: 1082.149
[57,     1] loss: 1003.613
[58,     1] loss: 1038.428
[59,     1] loss: 1001.076
[60,     1] loss: 1044.833
[61,     1] loss: 913.170
Early stopping applied (best metric=0.8130056262016296)
Finished Training
Total time taken: 10.17701005935669
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1562.706
[2,     1] loss: 1562.311
[3,     1] loss: 1562.271
[4,     1] loss: 1563.577
[5,     1] loss: 1560.477
[6,     1] loss: 1563.966
[7,     1] loss: 1556.574
[8,     1] loss: 1563.054
[9,     1] loss: 1561.442
[10,     1] loss: 1555.151
[11,     1] loss: 1548.789
[12,     1] loss: 1541.445
[13,     1] loss: 1539.795
[14,     1] loss: 1526.585
[15,     1] loss: 1509.496
[16,     1] loss: 1492.502
[17,     1] loss: 1459.432
[18,     1] loss: 1426.997
[19,     1] loss: 1400.451
[20,     1] loss: 1393.899
[21,     1] loss: 1354.886
[22,     1] loss: 1319.323
[23,     1] loss: 1304.949
[24,     1] loss: 1311.334
[25,     1] loss: 1327.820
[26,     1] loss: 1270.860
[27,     1] loss: 1355.447
[28,     1] loss: 1266.137
[29,     1] loss: 1241.612
[30,     1] loss: 1270.253
[31,     1] loss: 1259.214
[32,     1] loss: 1256.675
[33,     1] loss: 1211.512
[34,     1] loss: 1171.420
[35,     1] loss: 1188.961
[36,     1] loss: 1163.838
[37,     1] loss: 1211.853
[38,     1] loss: 1179.925
[39,     1] loss: 1151.484
[40,     1] loss: 1152.714
Early stopping applied (best metric=0.9991140365600586)
Finished Training
Total time taken: 6.749006748199463
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1559.864
[2,     1] loss: 1560.326
[3,     1] loss: 1560.314
[4,     1] loss: 1557.739
[5,     1] loss: 1559.382
[6,     1] loss: 1562.663
[7,     1] loss: 1557.610
[8,     1] loss: 1558.164
[9,     1] loss: 1556.653
[10,     1] loss: 1545.328
[11,     1] loss: 1545.638
[12,     1] loss: 1536.740
[13,     1] loss: 1509.668
[14,     1] loss: 1502.384
[15,     1] loss: 1467.649
[16,     1] loss: 1447.976
[17,     1] loss: 1444.600
[18,     1] loss: 1388.451
[19,     1] loss: 1361.612
[20,     1] loss: 1312.203
[21,     1] loss: 1357.037
[22,     1] loss: 1309.427
[23,     1] loss: 1359.869
[24,     1] loss: 1284.384
[25,     1] loss: 1295.877
[26,     1] loss: 1319.164
[27,     1] loss: 1284.932
[28,     1] loss: 1236.081
[29,     1] loss: 1294.581
[30,     1] loss: 1254.687
[31,     1] loss: 1239.689
[32,     1] loss: 1211.714
[33,     1] loss: 1235.306
[34,     1] loss: 1218.129
[35,     1] loss: 1236.817
[36,     1] loss: 1184.260
[37,     1] loss: 1146.329
[38,     1] loss: 1179.938
[39,     1] loss: 1198.595
[40,     1] loss: 1137.512
[41,     1] loss: 1121.151
[42,     1] loss: 1108.829
[43,     1] loss: 1096.558
[44,     1] loss: 1150.483
[45,     1] loss: 1031.995
[46,     1] loss: 1020.740
[47,     1] loss: 1098.482
[48,     1] loss: 1125.229
[49,     1] loss: 1068.584
[50,     1] loss: 1051.702
[51,     1] loss: 1026.873
[52,     1] loss: 1019.869
[53,     1] loss: 1010.587
Early stopping applied (best metric=0.8974177837371826)
Finished Training
Total time taken: 7.162006855010986
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.258
[2,     1] loss: 1563.087
[3,     1] loss: 1562.950
[4,     1] loss: 1560.237
[5,     1] loss: 1564.877
[6,     1] loss: 1558.735
[7,     1] loss: 1559.074
[8,     1] loss: 1551.952
[9,     1] loss: 1549.349
[10,     1] loss: 1551.406
[11,     1] loss: 1543.101
[12,     1] loss: 1530.960
[13,     1] loss: 1518.259
[14,     1] loss: 1501.048
[15,     1] loss: 1480.612
[16,     1] loss: 1466.398
[17,     1] loss: 1430.126
[18,     1] loss: 1406.968
[19,     1] loss: 1371.420
[20,     1] loss: 1367.134
[21,     1] loss: 1354.065
[22,     1] loss: 1285.578
[23,     1] loss: 1287.712
[24,     1] loss: 1313.115
[25,     1] loss: 1247.381
[26,     1] loss: 1246.379
[27,     1] loss: 1234.617
[28,     1] loss: 1329.264
[29,     1] loss: 1228.145
[30,     1] loss: 1311.411
[31,     1] loss: 1147.063
[32,     1] loss: 1233.947
[33,     1] loss: 1211.365
[34,     1] loss: 1249.203
[35,     1] loss: 1253.965
[36,     1] loss: 1222.499
[37,     1] loss: 1195.243
[38,     1] loss: 1233.899
[39,     1] loss: 1170.639
[40,     1] loss: 1181.861
[41,     1] loss: 1112.751
[42,     1] loss: 1139.404
[43,     1] loss: 1159.195
[44,     1] loss: 1120.962
[45,     1] loss: 1113.476
[46,     1] loss: 1071.438
[47,     1] loss: 1091.319
[48,     1] loss: 1124.216
[49,     1] loss: 1060.076
[50,     1] loss: 1086.095
[51,     1] loss: 1063.262
[52,     1] loss: 1127.541
[53,     1] loss: 1050.296
[54,     1] loss: 1045.934
[55,     1] loss: 1032.796
[56,     1] loss: 971.936
[57,     1] loss: 1032.841
Early stopping applied (best metric=0.9552980661392212)
Finished Training
Total time taken: 9.391011238098145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1562.901
[2,     1] loss: 1559.104
[3,     1] loss: 1562.099
[4,     1] loss: 1561.191
[5,     1] loss: 1557.671
[6,     1] loss: 1553.723
[7,     1] loss: 1550.954
[8,     1] loss: 1554.855
[9,     1] loss: 1555.248
[10,     1] loss: 1538.786
[11,     1] loss: 1533.245
[12,     1] loss: 1509.498
[13,     1] loss: 1498.732
[14,     1] loss: 1476.905
[15,     1] loss: 1438.927
[16,     1] loss: 1426.277
[17,     1] loss: 1372.480
[18,     1] loss: 1374.542
[19,     1] loss: 1323.965
[20,     1] loss: 1339.490
[21,     1] loss: 1299.634
[22,     1] loss: 1302.510
[23,     1] loss: 1307.777
[24,     1] loss: 1212.800
[25,     1] loss: 1292.389
[26,     1] loss: 1234.673
[27,     1] loss: 1237.249
[28,     1] loss: 1262.573
[29,     1] loss: 1247.899
[30,     1] loss: 1220.641
[31,     1] loss: 1199.573
[32,     1] loss: 1199.346
[33,     1] loss: 1162.761
[34,     1] loss: 1169.354
[35,     1] loss: 1179.570
[36,     1] loss: 1184.077
[37,     1] loss: 1118.487
[38,     1] loss: 1113.577
[39,     1] loss: 1093.551
[40,     1] loss: 1106.944
[41,     1] loss: 1061.463
[42,     1] loss: 1093.965
[43,     1] loss: 1075.601
[44,     1] loss: 989.332
Early stopping applied (best metric=0.8237523436546326)
Finished Training
Total time taken: 5.977004766464233
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.674
[2,     1] loss: 1553.313
[3,     1] loss: 1559.877
[4,     1] loss: 1554.104
[5,     1] loss: 1561.677
[6,     1] loss: 1556.612
[7,     1] loss: 1552.834
[8,     1] loss: 1557.723
[9,     1] loss: 1548.328
[10,     1] loss: 1543.552
[11,     1] loss: 1533.109
[12,     1] loss: 1525.318
[13,     1] loss: 1495.712
[14,     1] loss: 1465.915
[15,     1] loss: 1431.212
[16,     1] loss: 1401.581
[17,     1] loss: 1386.092
[18,     1] loss: 1328.740
[19,     1] loss: 1333.636
[20,     1] loss: 1343.170
[21,     1] loss: 1324.376
[22,     1] loss: 1334.663
[23,     1] loss: 1270.132
[24,     1] loss: 1288.678
[25,     1] loss: 1228.665
[26,     1] loss: 1303.172
[27,     1] loss: 1326.910
[28,     1] loss: 1308.623
[29,     1] loss: 1289.681
[30,     1] loss: 1273.671
[31,     1] loss: 1219.912
[32,     1] loss: 1258.997
[33,     1] loss: 1233.538
[34,     1] loss: 1222.230
[35,     1] loss: 1233.419
[36,     1] loss: 1189.985
[37,     1] loss: 1227.578
[38,     1] loss: 1255.698
[39,     1] loss: 1232.604
[40,     1] loss: 1178.308
[41,     1] loss: 1182.440
[42,     1] loss: 1192.251
[43,     1] loss: 1153.512
[44,     1] loss: 1131.517
[45,     1] loss: 1182.422
[46,     1] loss: 1064.233
[47,     1] loss: 1141.642
[48,     1] loss: 1135.153
[49,     1] loss: 1091.431
[50,     1] loss: 1096.161
[51,     1] loss: 1066.640
[52,     1] loss: 1114.080
[53,     1] loss: 1071.916
[54,     1] loss: 994.600
[55,     1] loss: 1038.584
[56,     1] loss: 968.937
[57,     1] loss: 958.083
[58,     1] loss: 1025.758
[59,     1] loss: 968.528
[60,     1] loss: 1087.061
[61,     1] loss: 968.910
[62,     1] loss: 1046.708
[63,     1] loss: 972.202
Early stopping applied (best metric=0.8946918249130249)
Finished Training
Total time taken: 10.531010150909424
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1558.214
[2,     1] loss: 1567.059
[3,     1] loss: 1560.494
[4,     1] loss: 1555.218
[5,     1] loss: 1557.147
[6,     1] loss: 1561.684
[7,     1] loss: 1559.609
[8,     1] loss: 1566.586
[9,     1] loss: 1555.021
[10,     1] loss: 1549.846
[11,     1] loss: 1547.591
[12,     1] loss: 1542.623
[13,     1] loss: 1533.053
[14,     1] loss: 1506.670
[15,     1] loss: 1482.230
[16,     1] loss: 1473.115
[17,     1] loss: 1428.588
[18,     1] loss: 1381.271
[19,     1] loss: 1408.345
[20,     1] loss: 1348.589
[21,     1] loss: 1344.304
[22,     1] loss: 1317.575
[23,     1] loss: 1318.317
[24,     1] loss: 1309.660
[25,     1] loss: 1245.468
[26,     1] loss: 1289.442
[27,     1] loss: 1300.141
[28,     1] loss: 1261.702
[29,     1] loss: 1307.891
[30,     1] loss: 1215.150
[31,     1] loss: 1247.094
[32,     1] loss: 1235.195
[33,     1] loss: 1216.151
[34,     1] loss: 1248.885
[35,     1] loss: 1229.679
[36,     1] loss: 1204.324
[37,     1] loss: 1133.553
[38,     1] loss: 1155.773
[39,     1] loss: 1127.060
Early stopping applied (best metric=0.9980454444885254)
Finished Training
Total time taken: 5.337003946304321
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1565.752
[2,     1] loss: 1561.619
[3,     1] loss: 1565.389
[4,     1] loss: 1561.069
[5,     1] loss: 1563.957
[6,     1] loss: 1560.837
[7,     1] loss: 1556.112
[8,     1] loss: 1562.623
[9,     1] loss: 1559.777
[10,     1] loss: 1557.193
[11,     1] loss: 1557.530
[12,     1] loss: 1548.658
[13,     1] loss: 1555.724
[14,     1] loss: 1546.251
[15,     1] loss: 1535.529
[16,     1] loss: 1530.148
[17,     1] loss: 1531.180
[18,     1] loss: 1507.955
[19,     1] loss: 1484.746
[20,     1] loss: 1469.735
[21,     1] loss: 1437.503
[22,     1] loss: 1402.378
[23,     1] loss: 1427.610
[24,     1] loss: 1387.653
[25,     1] loss: 1348.638
[26,     1] loss: 1393.384
[27,     1] loss: 1307.271
[28,     1] loss: 1370.050
[29,     1] loss: 1297.092
[30,     1] loss: 1335.989
[31,     1] loss: 1259.979
[32,     1] loss: 1282.044
[33,     1] loss: 1334.182
[34,     1] loss: 1236.299
[35,     1] loss: 1238.330
[36,     1] loss: 1238.285
[37,     1] loss: 1236.920
[38,     1] loss: 1182.997
[39,     1] loss: 1248.909
[40,     1] loss: 1201.901
[41,     1] loss: 1155.908
[42,     1] loss: 1174.153
[43,     1] loss: 1194.925
Early stopping applied (best metric=0.7989993095397949)
Finished Training
Total time taken: 7.154006719589233
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1567.891
[2,     1] loss: 1563.007
[3,     1] loss: 1560.214
[4,     1] loss: 1560.721
[5,     1] loss: 1564.372
[6,     1] loss: 1561.474
[7,     1] loss: 1562.062
[8,     1] loss: 1560.817
[9,     1] loss: 1556.849
[10,     1] loss: 1557.142
[11,     1] loss: 1559.275
[12,     1] loss: 1554.751
[13,     1] loss: 1548.500
[14,     1] loss: 1546.772
[15,     1] loss: 1540.821
[16,     1] loss: 1520.202
[17,     1] loss: 1525.456
[18,     1] loss: 1509.646
[19,     1] loss: 1488.882
[20,     1] loss: 1470.107
[21,     1] loss: 1435.704
[22,     1] loss: 1416.387
[23,     1] loss: 1406.828
[24,     1] loss: 1355.840
[25,     1] loss: 1375.339
[26,     1] loss: 1342.893
[27,     1] loss: 1329.559
[28,     1] loss: 1349.675
[29,     1] loss: 1319.804
[30,     1] loss: 1302.188
[31,     1] loss: 1370.421
[32,     1] loss: 1245.038
[33,     1] loss: 1279.114
[34,     1] loss: 1236.537
[35,     1] loss: 1297.598
[36,     1] loss: 1219.712
[37,     1] loss: 1281.728
[38,     1] loss: 1240.497
[39,     1] loss: 1223.385
[40,     1] loss: 1208.662
[41,     1] loss: 1186.875
[42,     1] loss: 1140.253
[43,     1] loss: 1221.400
[44,     1] loss: 1141.843
[45,     1] loss: 1165.483
[46,     1] loss: 1137.772
[47,     1] loss: 1121.491
[48,     1] loss: 1128.815
[49,     1] loss: 1088.648
[50,     1] loss: 1057.290
[51,     1] loss: 1081.072
[52,     1] loss: 1085.664
[53,     1] loss: 1066.151
[54,     1] loss: 1023.454
[55,     1] loss: 1100.454
[56,     1] loss: 1096.152
[57,     1] loss: 1067.253
[58,     1] loss: 1045.711
[59,     1] loss: 1070.372
[60,     1] loss: 928.301
[61,     1] loss: 982.003
[62,     1] loss: 983.937
[63,     1] loss: 985.025
[64,     1] loss: 998.136
[65,     1] loss: 949.705
[66,     1] loss: 896.899
[67,     1] loss: 1017.952
Early stopping applied (best metric=0.8023940324783325)
Finished Training
Total time taken: 9.395009517669678
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1565.944
[2,     1] loss: 1563.913
[3,     1] loss: 1564.545
[4,     1] loss: 1561.668
[5,     1] loss: 1557.571
[6,     1] loss: 1559.680
[7,     1] loss: 1558.212
[8,     1] loss: 1555.106
[9,     1] loss: 1556.746
[10,     1] loss: 1539.674
[11,     1] loss: 1530.741
[12,     1] loss: 1516.466
[13,     1] loss: 1496.709
[14,     1] loss: 1493.047
[15,     1] loss: 1444.503
[16,     1] loss: 1427.174
[17,     1] loss: 1404.923
[18,     1] loss: 1381.659
[19,     1] loss: 1357.273
[20,     1] loss: 1367.054
[21,     1] loss: 1282.689
[22,     1] loss: 1303.022
[23,     1] loss: 1297.909
[24,     1] loss: 1281.159
[25,     1] loss: 1306.360
[26,     1] loss: 1314.833
[27,     1] loss: 1271.312
[28,     1] loss: 1235.984
[29,     1] loss: 1224.572
[30,     1] loss: 1213.695
[31,     1] loss: 1218.096
[32,     1] loss: 1211.279
[33,     1] loss: 1193.193
[34,     1] loss: 1188.779
[35,     1] loss: 1114.931
[36,     1] loss: 1180.292
[37,     1] loss: 1168.809
[38,     1] loss: 1104.563
[39,     1] loss: 1151.229
[40,     1] loss: 1066.645
[41,     1] loss: 1094.263
[42,     1] loss: 1072.703
[43,     1] loss: 1094.081
Early stopping applied (best metric=0.996310830116272)
Finished Training
Total time taken: 6.011005640029907
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1565.543
[2,     1] loss: 1570.506
[3,     1] loss: 1559.388
[4,     1] loss: 1560.464
[5,     1] loss: 1563.303
[6,     1] loss: 1558.472
[7,     1] loss: 1559.945
[8,     1] loss: 1566.371
[9,     1] loss: 1554.792
[10,     1] loss: 1560.339
[11,     1] loss: 1562.114
[12,     1] loss: 1557.173
[13,     1] loss: 1555.964
[14,     1] loss: 1547.834
[15,     1] loss: 1550.865
[16,     1] loss: 1542.749
[17,     1] loss: 1532.745
[18,     1] loss: 1526.586
[19,     1] loss: 1505.995
[20,     1] loss: 1499.888
[21,     1] loss: 1475.073
[22,     1] loss: 1447.846
[23,     1] loss: 1430.534
[24,     1] loss: 1374.389
[25,     1] loss: 1367.109
[26,     1] loss: 1350.733
[27,     1] loss: 1313.051
[28,     1] loss: 1331.062
[29,     1] loss: 1286.506
[30,     1] loss: 1330.392
[31,     1] loss: 1325.697
[32,     1] loss: 1283.904
[33,     1] loss: 1270.701
[34,     1] loss: 1273.316
[35,     1] loss: 1245.587
[36,     1] loss: 1218.219
[37,     1] loss: 1256.411
[38,     1] loss: 1287.865
[39,     1] loss: 1179.818
[40,     1] loss: 1204.941
[41,     1] loss: 1182.618
[42,     1] loss: 1206.292
[43,     1] loss: 1127.123
[44,     1] loss: 1089.242
[45,     1] loss: 1126.166
[46,     1] loss: 1128.781
[47,     1] loss: 1164.333
[48,     1] loss: 1075.438
[49,     1] loss: 1005.348
[50,     1] loss: 1088.117
Early stopping applied (best metric=0.8402379155158997)
Finished Training
Total time taken: 7.764007329940796
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1561.032
[2,     1] loss: 1555.741
[3,     1] loss: 1566.305
[4,     1] loss: 1565.040
[5,     1] loss: 1560.520
[6,     1] loss: 1558.745
[7,     1] loss: 1559.988
[8,     1] loss: 1551.510
[9,     1] loss: 1551.625
[10,     1] loss: 1552.568
[11,     1] loss: 1552.137
[12,     1] loss: 1545.720
[13,     1] loss: 1532.144
[14,     1] loss: 1510.092
[15,     1] loss: 1499.676
[16,     1] loss: 1473.157
[17,     1] loss: 1462.905
[18,     1] loss: 1420.908
[19,     1] loss: 1416.920
[20,     1] loss: 1401.804
[21,     1] loss: 1328.388
[22,     1] loss: 1349.402
[23,     1] loss: 1339.745
[24,     1] loss: 1302.807
[25,     1] loss: 1273.740
[26,     1] loss: 1332.548
[27,     1] loss: 1235.310
[28,     1] loss: 1276.879
[29,     1] loss: 1236.677
[30,     1] loss: 1288.616
[31,     1] loss: 1279.249
[32,     1] loss: 1244.416
[33,     1] loss: 1305.809
[34,     1] loss: 1260.067
[35,     1] loss: 1247.862
[36,     1] loss: 1157.525
[37,     1] loss: 1189.248
[38,     1] loss: 1183.075
[39,     1] loss: 1138.371
[40,     1] loss: 1142.928
Early stopping applied (best metric=0.9154964685440063)
Finished Training
Total time taken: 6.612009525299072
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1573.673
[2,     1] loss: 1561.434
[3,     1] loss: 1568.204
[4,     1] loss: 1558.618
[5,     1] loss: 1562.022
[6,     1] loss: 1562.930
[7,     1] loss: 1559.982
[8,     1] loss: 1554.665
[9,     1] loss: 1554.843
[10,     1] loss: 1557.011
[11,     1] loss: 1557.396
[12,     1] loss: 1548.473
[13,     1] loss: 1544.021
[14,     1] loss: 1535.294
[15,     1] loss: 1527.338
[16,     1] loss: 1516.012
[17,     1] loss: 1491.150
[18,     1] loss: 1481.741
[19,     1] loss: 1465.647
[20,     1] loss: 1439.877
[21,     1] loss: 1406.529
[22,     1] loss: 1394.085
[23,     1] loss: 1383.395
[24,     1] loss: 1363.251
[25,     1] loss: 1407.092
[26,     1] loss: 1338.944
[27,     1] loss: 1364.991
[28,     1] loss: 1291.623
[29,     1] loss: 1296.091
[30,     1] loss: 1323.626
[31,     1] loss: 1227.383
[32,     1] loss: 1309.098
[33,     1] loss: 1229.072
[34,     1] loss: 1271.354
[35,     1] loss: 1252.829
[36,     1] loss: 1256.808
[37,     1] loss: 1242.334
[38,     1] loss: 1217.905
[39,     1] loss: 1247.330
[40,     1] loss: 1167.450
[41,     1] loss: 1306.008
[42,     1] loss: 1181.740
[43,     1] loss: 1200.774
[44,     1] loss: 1187.574
[45,     1] loss: 1172.924
[46,     1] loss: 1117.132
[47,     1] loss: 1152.853
[48,     1] loss: 1178.954
[49,     1] loss: 1156.686
[50,     1] loss: 1062.095
[51,     1] loss: 1104.043
[52,     1] loss: 1096.338
[53,     1] loss: 1142.605
[54,     1] loss: 1030.291
[55,     1] loss: 1083.311
[56,     1] loss: 1087.368
[57,     1] loss: 1100.984
Early stopping applied (best metric=0.8054580688476562)
Finished Training
Total time taken: 7.734009265899658
{'Hydroxylation-K Validation Accuracy': 0.7419326241134752, 'Hydroxylation-K Validation Sensitivity': 0.6703703703703704, 'Hydroxylation-K Validation Specificity': 0.7596491228070176, 'Hydroxylation-K Validation Precision': 0.42097920139715805, 'Hydroxylation-K AUC ROC': 0.7444444444444445, 'Hydroxylation-K AUC PR': 0.5122268299003546, 'Hydroxylation-K MCC': 0.3723432407517375, 'Hydroxylation-K F1': 0.5139635646342293, 'Validation Loss (Hydroxylation-K)': 0.472223687171936, 'Hydroxylation-P Validation Accuracy': 0.7648135120044668, 'Hydroxylation-P Validation Sensitivity': 0.7482539682539683, 'Hydroxylation-P Validation Specificity': 0.7684248590952167, 'Hydroxylation-P Validation Precision': 0.41568578199120554, 'Hydroxylation-P AUC ROC': 0.8148486248374023, 'Hydroxylation-P AUC PR': 0.5169869792110141, 'Hydroxylation-P MCC': 0.425350946037256, 'Hydroxylation-P F1': 0.5308584965126216, 'Validation Loss (Hydroxylation-P)': 0.41060853004455566, 'Validation Loss (total)': 0.8828322172164917, 'TimeToTrain': 7.949941492080688}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002724355253872863,
 'learning_rate_Hydroxylation-K': 0.0032961232138931243,
 'learning_rate_Hydroxylation-P': 0.004722269839690383,
 'log_base': 1.025748600460345,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3541074283,
 'sample_weights': [3.098542380797097, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.554535509394704,
 'weight_decay_Hydroxylation-K': 3.2233040904072467,
 'weight_decay_Hydroxylation-P': 1.3454849235264301}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21283.984
Exploding loss, terminate run (best metric=1.0960670709609985)
Finished Training
Total time taken: 0.2200007438659668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21351.023
Exploding loss, terminate run (best metric=1.0921872854232788)
Finished Training
Total time taken: 0.2069988250732422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21320.141
Exploding loss, terminate run (best metric=1.0957162380218506)
Finished Training
Total time taken: 0.22900080680847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21269.672
Exploding loss, terminate run (best metric=1.0728249549865723)
Finished Training
Total time taken: 0.23799967765808105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21304.199
Exploding loss, terminate run (best metric=1.073686957359314)
Finished Training
Total time taken: 0.2330005168914795
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21253.572
Exploding loss, terminate run (best metric=1.097334623336792)
Finished Training
Total time taken: 0.21799898147583008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21229.168
Exploding loss, terminate run (best metric=1.0915992259979248)
Finished Training
Total time taken: 0.2129974365234375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21547.947
Exploding loss, terminate run (best metric=1.0995087623596191)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21541.217
Exploding loss, terminate run (best metric=1.0728284120559692)
Finished Training
Total time taken: 0.21499919891357422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21417.221
Exploding loss, terminate run (best metric=1.0754823684692383)
Finished Training
Total time taken: 0.22800183296203613
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21261.305
Exploding loss, terminate run (best metric=1.0998430252075195)
Finished Training
Total time taken: 0.20399832725524902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21452.164
Exploding loss, terminate run (best metric=1.0926792621612549)
Finished Training
Total time taken: 0.2030012607574463
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21325.191
Exploding loss, terminate run (best metric=1.0951495170593262)
Finished Training
Total time taken: 0.21399998664855957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21336.172
Exploding loss, terminate run (best metric=1.0791367292404175)
Finished Training
Total time taken: 0.19999980926513672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21272.451
Exploding loss, terminate run (best metric=1.073146104812622)
Finished Training
Total time taken: 0.21699976921081543
{'Hydroxylation-K Validation Accuracy': 0.47972813238770684, 'Hydroxylation-K Validation Sensitivity': 0.5674074074074074, 'Hydroxylation-K Validation Specificity': 0.45964912280701753, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6358869395711502, 'Hydroxylation-K AUC PR': 0.38148378327454807, 'Hydroxylation-K MCC': 0.04160976314228322, 'Hydroxylation-K F1': 0.22063127166575444, 'Validation Loss (Hydroxylation-K)': 0.5563907464345296, 'Hydroxylation-P Validation Accuracy': 0.47872588531885013, 'Hydroxylation-P Validation Sensitivity': 0.5466666666666666, 'Hydroxylation-P Validation Specificity': 0.4638211382113821, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5903592923527833, 'Hydroxylation-P AUC PR': 0.2924899395034956, 'Hydroxylation-P MCC': 0.015613154705829338, 'Hydroxylation-P F1': 0.17955488952797458, 'Validation Loss (Hydroxylation-P)': 0.5307552933692932, 'Validation Loss (total)': 1.0871460358301799, 'TimeToTrain': 0.21619981129964191}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018376604681083573,
 'learning_rate_Hydroxylation-K': 0.003402018174528858,
 'learning_rate_Hydroxylation-P': 0.005804280601674459,
 'log_base': 1.269028087231162,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 776262988,
 'sample_weights': [65.71616432045592, 8.197452217718727],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.223228896814383,
 'weight_decay_Hydroxylation-K': 3.4717503633143965,
 'weight_decay_Hydroxylation-P': 4.552259394323375}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2383.636
[2,     1] loss: 2387.008
[3,     1] loss: 2398.443
[4,     1] loss: 2378.688
[5,     1] loss: 2382.344
[6,     1] loss: 2381.271
[7,     1] loss: 2384.758
[8,     1] loss: 2381.930
[9,     1] loss: 2385.734
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007976656424527735,
 'learning_rate_Hydroxylation-K': 0.0034957746096185363,
 'learning_rate_Hydroxylation-P': 0.009872175031615589,
 'log_base': 2.809869759557188,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2208272458,
 'sample_weights': [7.007067723671892, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8960891713381276,
 'weight_decay_Hydroxylation-K': 1.5400553395833747,
 'weight_decay_Hydroxylation-P': 2.2345491468632757}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.761
[2,     1] loss: 1252.575
[3,     1] loss: 1248.466
[4,     1] loss: 1254.196
[5,     1] loss: 1246.938
[6,     1] loss: 1250.608
[7,     1] loss: 1244.924
[8,     1] loss: 1246.540
[9,     1] loss: 1241.575
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013260527209564212,
 'learning_rate_Hydroxylation-K': 0.0038975139076749756,
 'learning_rate_Hydroxylation-P': 0.004505760400754966,
 'log_base': 1.0947630176876646,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3754800915,
 'sample_weights': [1.6158953904790134, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.339122480587973,
 'weight_decay_Hydroxylation-K': 0.6760867188812996,
 'weight_decay_Hydroxylation-P': 0.5039994291738961}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5978.909
[2,     1] loss: 5983.002
[3,     1] loss: 5977.190
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009306416293314001,
 'learning_rate_Hydroxylation-K': 0.00844449306159677,
 'learning_rate_Hydroxylation-P': 0.005361791860655435,
 'log_base': 2.129710030766411,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3238521140,
 'sample_weights': [18.43915997727047, 2.3049837909697026],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7194828238957696,
 'weight_decay_Hydroxylation-K': 5.246147323316065,
 'weight_decay_Hydroxylation-P': 4.266640689178372}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1381.885
[2,     1] loss: 1372.565
[3,     1] loss: 1385.347
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006436295827539309,
 'learning_rate_Hydroxylation-K': 0.003316188328684639,
 'learning_rate_Hydroxylation-P': 0.0025331018621923525,
 'log_base': 1.0424812506065873,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3920539835,
 'sample_weights': [2.208299508900914, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.360127953197036,
 'weight_decay_Hydroxylation-K': 6.91455099840759,
 'weight_decay_Hydroxylation-P': 6.841315527379826}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13012.498
[2,     1] loss: 13033.788
[3,     1] loss: 13043.771
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035274200451597543,
 'learning_rate_Hydroxylation-K': 0.0038136755968686737,
 'learning_rate_Hydroxylation-P': 0.006792699610293803,
 'log_base': 1.1142815103187658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3627528057,
 'sample_weights': [40.127286005400315, 5.016103983699192],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.320304201678578,
 'weight_decay_Hydroxylation-K': 0.5912022919373783,
 'weight_decay_Hydroxylation-P': 2.3606382750912074}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5022.158
[2,     1] loss: 4999.140
[3,     1] loss: 5012.772
[4,     1] loss: 4990.785
[5,     1] loss: 4978.931
[6,     1] loss: 4942.947
[7,     1] loss: 4943.374
[8,     1] loss: 4866.555
[9,     1] loss: 4822.810
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009029316270682559,
 'learning_rate_Hydroxylation-K': 0.006821649945092575,
 'learning_rate_Hydroxylation-P': 0.004949199232400625,
 'log_base': 2.1417818601857683,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2907074917,
 'sample_weights': [15.42783523047974, 1.9285536965806602],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.1297724687769826,
 'weight_decay_Hydroxylation-K': 7.427824502037919,
 'weight_decay_Hydroxylation-P': 0.6615609140402938}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1376.364
[2,     1] loss: 1372.539
[3,     1] loss: 1371.283
[4,     1] loss: 1367.841
[5,     1] loss: 1368.955
[6,     1] loss: 1365.555
[7,     1] loss: 1367.808
[8,     1] loss: 1360.373
[9,     1] loss: 1357.727
[10,     1] loss: 1348.036
[11,     1] loss: 1337.596
[12,     1] loss: 1297.599
[13,     1] loss: 1272.917
[14,     1] loss: 1221.336
[15,     1] loss: 1211.432
[16,     1] loss: 1214.429
[17,     1] loss: 1153.371
[18,     1] loss: 1166.604
[19,     1] loss: 1118.015
[20,     1] loss: 1121.744
[21,     1] loss: 1080.433
[22,     1] loss: 1120.529
[23,     1] loss: 1135.170
[24,     1] loss: 1108.509
[25,     1] loss: 1087.756
[26,     1] loss: 1074.170
[27,     1] loss: 1110.992
[28,     1] loss: 1047.927
[29,     1] loss: 1069.552
[30,     1] loss: 1060.561
[31,     1] loss: 1067.969
[32,     1] loss: 998.081
[33,     1] loss: 1061.754
[34,     1] loss: 1026.610
[35,     1] loss: 1028.473
[36,     1] loss: 1018.589
[37,     1] loss: 990.155
[38,     1] loss: 1001.495
[39,     1] loss: 956.346
[40,     1] loss: 1008.529
[41,     1] loss: 997.723
[42,     1] loss: 949.839
[43,     1] loss: 957.257
[44,     1] loss: 951.182
[45,     1] loss: 920.600
[46,     1] loss: 924.495
[47,     1] loss: 956.508
[48,     1] loss: 961.411
[49,     1] loss: 1196.571
[50,     1] loss: 1281.970
[51,     1] loss: 1061.037
[52,     1] loss: 976.845
[53,     1] loss: 1173.471
[54,     1] loss: 1088.757
[55,     1] loss: 1025.318
[56,     1] loss: 1027.608
[57,     1] loss: 1077.835
Early stopping applied (best metric=0.7061985731124878)
Finished Training
Total time taken: 9.5380117893219
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.695
[2,     1] loss: 1393.540
[3,     1] loss: 1382.475
[4,     1] loss: 1368.260
[5,     1] loss: 1368.249
[6,     1] loss: 1369.214
[7,     1] loss: 1372.213
[8,     1] loss: 1367.272
[9,     1] loss: 1374.267
[10,     1] loss: 1369.411
[11,     1] loss: 1370.247
[12,     1] loss: 1366.978
[13,     1] loss: 1366.810
[14,     1] loss: 1381.215
[15,     1] loss: 1373.635
[16,     1] loss: 1373.585
[17,     1] loss: 1370.198
[18,     1] loss: 1369.297
[19,     1] loss: 1366.977
[20,     1] loss: 1366.970
[21,     1] loss: 1369.298
[22,     1] loss: 1369.968
[23,     1] loss: 1370.376
[24,     1] loss: 1370.453
[25,     1] loss: 1366.797
[26,     1] loss: 1366.294
[27,     1] loss: 1363.839
[28,     1] loss: 1363.375
[29,     1] loss: 1358.063
[30,     1] loss: 1354.261
[31,     1] loss: 1340.309
[32,     1] loss: 1322.942
[33,     1] loss: 1305.858
[34,     1] loss: 1287.246
[35,     1] loss: 1256.450
[36,     1] loss: 1223.960
[37,     1] loss: 1192.798
[38,     1] loss: 1158.717
[39,     1] loss: 1163.559
[40,     1] loss: 1097.457
[41,     1] loss: 1112.713
[42,     1] loss: 1159.132
[43,     1] loss: 1090.997
[44,     1] loss: 1145.010
[45,     1] loss: 1083.794
[46,     1] loss: 1077.575
[47,     1] loss: 1103.174
[48,     1] loss: 1045.949
[49,     1] loss: 1041.165
[50,     1] loss: 1037.651
[51,     1] loss: 999.578
[52,     1] loss: 961.584
[53,     1] loss: 1045.337
[54,     1] loss: 1078.410
[55,     1] loss: 974.071
[56,     1] loss: 954.800
[57,     1] loss: 978.242
[58,     1] loss: 939.081
[59,     1] loss: 924.954
[60,     1] loss: 925.340
[61,     1] loss: 1006.989
[62,     1] loss: 1003.261
[63,     1] loss: 1059.843
[64,     1] loss: 886.737
[65,     1] loss: 930.257
[66,     1] loss: 877.721
[67,     1] loss: 896.096
[68,     1] loss: 843.660
[69,     1] loss: 932.262
[70,     1] loss: 904.174
[71,     1] loss: 806.110
[72,     1] loss: 781.898
Early stopping applied (best metric=0.8018831014633179)
Finished Training
Total time taken: 11.796009063720703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1371.453
[2,     1] loss: 1369.035
[3,     1] loss: 1369.214
[4,     1] loss: 1377.113
[5,     1] loss: 1370.847
[6,     1] loss: 1369.037
[7,     1] loss: 1370.711
[8,     1] loss: 1370.126
[9,     1] loss: 1372.611
[10,     1] loss: 1370.656
[11,     1] loss: 1367.832
[12,     1] loss: 1366.617
[13,     1] loss: 1369.257
[14,     1] loss: 1366.429
[15,     1] loss: 1367.248
[16,     1] loss: 1367.963
[17,     1] loss: 1365.591
[18,     1] loss: 1362.216
[19,     1] loss: 1361.299
[20,     1] loss: 1351.763
[21,     1] loss: 1344.211
[22,     1] loss: 1331.473
[23,     1] loss: 1306.950
[24,     1] loss: 1272.840
[25,     1] loss: 1268.275
[26,     1] loss: 1180.135
[27,     1] loss: 1153.209
[28,     1] loss: 1188.312
[29,     1] loss: 1135.946
[30,     1] loss: 1211.029
[31,     1] loss: 1139.442
[32,     1] loss: 1142.285
[33,     1] loss: 1174.558
[34,     1] loss: 1128.426
[35,     1] loss: 1080.469
[36,     1] loss: 1155.292
[37,     1] loss: 1092.706
[38,     1] loss: 1093.713
[39,     1] loss: 1018.513
[40,     1] loss: 1062.932
[41,     1] loss: 1077.891
[42,     1] loss: 1051.467
[43,     1] loss: 1079.447
[44,     1] loss: 992.941
[45,     1] loss: 1057.534
[46,     1] loss: 1033.048
[47,     1] loss: 1051.848
[48,     1] loss: 974.913
[49,     1] loss: 973.586
[50,     1] loss: 986.157
[51,     1] loss: 953.562
[52,     1] loss: 1009.547
[53,     1] loss: 967.936
[54,     1] loss: 917.029
[55,     1] loss: 930.720
[56,     1] loss: 930.464
[57,     1] loss: 866.009
[58,     1] loss: 862.480
[59,     1] loss: 912.989
[60,     1] loss: 897.527
[61,     1] loss: 895.514
[62,     1] loss: 1121.155
[63,     1] loss: 865.702
[64,     1] loss: 1044.764
[65,     1] loss: 908.247
[66,     1] loss: 995.188
[67,     1] loss: 931.512
[68,     1] loss: 874.610
[69,     1] loss: 1033.459
[70,     1] loss: 847.706
[71,     1] loss: 949.193
[72,     1] loss: 832.375
[73,     1] loss: 819.265
[74,     1] loss: 871.275
[75,     1] loss: 811.149
[76,     1] loss: 749.138
[77,     1] loss: 748.213
[78,     1] loss: 645.776
Early stopping applied (best metric=0.6699399352073669)
Finished Training
Total time taken: 13.070010662078857
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1370.570
[2,     1] loss: 1370.875
[3,     1] loss: 1378.972
[4,     1] loss: 1369.309
[5,     1] loss: 1366.737
[6,     1] loss: 1372.458
[7,     1] loss: 1370.314
[8,     1] loss: 1369.801
[9,     1] loss: 1365.930
[10,     1] loss: 1371.755
[11,     1] loss: 1372.110
[12,     1] loss: 1371.111
[13,     1] loss: 1370.286
[14,     1] loss: 1368.180
[15,     1] loss: 1369.845
[16,     1] loss: 1366.636
[17,     1] loss: 1366.314
[18,     1] loss: 1364.337
[19,     1] loss: 1360.868
[20,     1] loss: 1354.406
[21,     1] loss: 1341.866
[22,     1] loss: 1320.529
[23,     1] loss: 1293.975
[24,     1] loss: 1246.064
[25,     1] loss: 1195.701
[26,     1] loss: 1218.832
[27,     1] loss: 1215.365
[28,     1] loss: 1135.902
[29,     1] loss: 1151.780
[30,     1] loss: 1108.268
[31,     1] loss: 1190.750
[32,     1] loss: 1057.148
[33,     1] loss: 1169.241
[34,     1] loss: 1093.297
[35,     1] loss: 1092.236
[36,     1] loss: 1109.714
[37,     1] loss: 1079.951
[38,     1] loss: 1068.255
[39,     1] loss: 1009.845
[40,     1] loss: 1007.451
[41,     1] loss: 1008.009
[42,     1] loss: 1054.674
[43,     1] loss: 1017.535
[44,     1] loss: 1047.400
[45,     1] loss: 924.836
[46,     1] loss: 975.694
[47,     1] loss: 965.346
[48,     1] loss: 949.241
[49,     1] loss: 938.539
[50,     1] loss: 945.494
[51,     1] loss: 945.212
[52,     1] loss: 908.439
[53,     1] loss: 871.951
[54,     1] loss: 929.993
[55,     1] loss: 874.103
[56,     1] loss: 846.076
[57,     1] loss: 825.203
[58,     1] loss: 828.473
[59,     1] loss: 821.040
[60,     1] loss: 833.379
[61,     1] loss: 1071.628
[62,     1] loss: 2038.320
[63,     1] loss: 896.283
[64,     1] loss: 1182.134
[65,     1] loss: 1189.247
[66,     1] loss: 1141.603
[67,     1] loss: 1122.765
[68,     1] loss: 1049.338
[69,     1] loss: 1043.430
[70,     1] loss: 1042.758
[71,     1] loss: 985.421
[72,     1] loss: 995.989
[73,     1] loss: 959.423
[74,     1] loss: 955.599
[75,     1] loss: 932.945
[76,     1] loss: 940.026
[77,     1] loss: 895.612
[78,     1] loss: 906.346
[79,     1] loss: 910.053
Early stopping applied (best metric=0.7667489051818848)
Finished Training
Total time taken: 13.177012920379639
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1378.472
[2,     1] loss: 1379.851
[3,     1] loss: 1372.183
[4,     1] loss: 1370.800
[5,     1] loss: 1375.531
[6,     1] loss: 1370.677
[7,     1] loss: 1367.176
[8,     1] loss: 1371.661
[9,     1] loss: 1373.386
[10,     1] loss: 1371.969
[11,     1] loss: 1370.119
[12,     1] loss: 1371.004
[13,     1] loss: 1371.396
[14,     1] loss: 1369.228
[15,     1] loss: 1372.214
[16,     1] loss: 1371.253
[17,     1] loss: 1370.047
[18,     1] loss: 1366.635
[19,     1] loss: 1369.950
[20,     1] loss: 1369.909
[21,     1] loss: 1364.349
[22,     1] loss: 1360.622
[23,     1] loss: 1355.450
[24,     1] loss: 1352.224
[25,     1] loss: 1338.757
[26,     1] loss: 1312.105
[27,     1] loss: 1293.297
[28,     1] loss: 1257.592
[29,     1] loss: 1226.234
[30,     1] loss: 1190.431
[31,     1] loss: 1166.724
[32,     1] loss: 1107.493
[33,     1] loss: 1138.296
[34,     1] loss: 1133.514
[35,     1] loss: 1216.579
[36,     1] loss: 1113.879
[37,     1] loss: 1101.562
[38,     1] loss: 1096.031
[39,     1] loss: 1109.896
[40,     1] loss: 1076.007
[41,     1] loss: 1024.639
[42,     1] loss: 1039.405
[43,     1] loss: 1081.915
[44,     1] loss: 1072.906
[45,     1] loss: 1026.537
[46,     1] loss: 1065.673
[47,     1] loss: 995.331
[48,     1] loss: 967.015
[49,     1] loss: 1011.287
[50,     1] loss: 959.816
[51,     1] loss: 989.590
[52,     1] loss: 973.690
[53,     1] loss: 931.845
[54,     1] loss: 949.367
[55,     1] loss: 857.784
[56,     1] loss: 848.390
[57,     1] loss: 847.620
[58,     1] loss: 867.096
[59,     1] loss: 1068.327
[60,     1] loss: 1051.516
[61,     1] loss: 818.862
[62,     1] loss: 911.292
[63,     1] loss: 899.893
[64,     1] loss: 911.734
Early stopping applied (best metric=0.8509737849235535)
Finished Training
Total time taken: 8.719008922576904
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.911
[2,     1] loss: 1382.957
[3,     1] loss: 1366.945
[4,     1] loss: 1382.215
[5,     1] loss: 1371.783
[6,     1] loss: 1369.903
[7,     1] loss: 1372.069
[8,     1] loss: 1371.275
[9,     1] loss: 1370.841
[10,     1] loss: 1369.860
[11,     1] loss: 1369.986
[12,     1] loss: 1368.746
[13,     1] loss: 1365.466
[14,     1] loss: 1365.848
[15,     1] loss: 1364.627
[16,     1] loss: 1355.768
[17,     1] loss: 1350.440
[18,     1] loss: 1340.995
[19,     1] loss: 1306.991
[20,     1] loss: 1276.235
[21,     1] loss: 1247.813
[22,     1] loss: 1229.208
[23,     1] loss: 1233.070
[24,     1] loss: 1204.949
[25,     1] loss: 1178.226
[26,     1] loss: 1161.244
[27,     1] loss: 1164.885
[28,     1] loss: 1157.223
[29,     1] loss: 1126.254
[30,     1] loss: 1132.233
[31,     1] loss: 1138.238
[32,     1] loss: 1113.809
[33,     1] loss: 1117.274
[34,     1] loss: 1066.543
[35,     1] loss: 1059.855
[36,     1] loss: 1076.504
[37,     1] loss: 1040.700
[38,     1] loss: 1047.686
[39,     1] loss: 988.043
[40,     1] loss: 1000.593
[41,     1] loss: 957.516
[42,     1] loss: 1005.024
[43,     1] loss: 946.872
[44,     1] loss: 929.320
[45,     1] loss: 936.198
[46,     1] loss: 1044.124
[47,     1] loss: 1164.407
[48,     1] loss: 937.008
[49,     1] loss: 1086.726
[50,     1] loss: 959.155
[51,     1] loss: 976.401
[52,     1] loss: 943.459
[53,     1] loss: 931.310
[54,     1] loss: 967.419
[55,     1] loss: 864.104
[56,     1] loss: 833.534
[57,     1] loss: 844.682
[58,     1] loss: 850.214
[59,     1] loss: 818.946
Early stopping applied (best metric=0.7784817218780518)
Finished Training
Total time taken: 9.929008722305298
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1372.799
[2,     1] loss: 1371.205
[3,     1] loss: 1367.889
[4,     1] loss: 1365.703
[5,     1] loss: 1375.593
[6,     1] loss: 1374.331
[7,     1] loss: 1373.826
[8,     1] loss: 1369.813
[9,     1] loss: 1370.661
[10,     1] loss: 1369.594
[11,     1] loss: 1366.046
[12,     1] loss: 1364.128
[13,     1] loss: 1361.313
[14,     1] loss: 1362.251
[15,     1] loss: 1356.148
[16,     1] loss: 1342.563
[17,     1] loss: 1325.006
[18,     1] loss: 1292.089
[19,     1] loss: 1281.777
[20,     1] loss: 1249.133
[21,     1] loss: 1195.621
[22,     1] loss: 1160.516
[23,     1] loss: 1134.985
[24,     1] loss: 1203.862
[25,     1] loss: 1165.017
[26,     1] loss: 1154.682
[27,     1] loss: 1127.764
[28,     1] loss: 1131.388
[29,     1] loss: 1120.937
[30,     1] loss: 1146.988
[31,     1] loss: 1089.713
[32,     1] loss: 1080.177
[33,     1] loss: 1097.395
[34,     1] loss: 1073.152
[35,     1] loss: 1077.995
[36,     1] loss: 1071.191
[37,     1] loss: 1099.623
[38,     1] loss: 1055.378
[39,     1] loss: 1040.097
[40,     1] loss: 985.333
[41,     1] loss: 984.686
[42,     1] loss: 945.757
[43,     1] loss: 1001.698
[44,     1] loss: 1121.666
[45,     1] loss: 1083.721
[46,     1] loss: 956.674
[47,     1] loss: 1023.127
[48,     1] loss: 965.630
[49,     1] loss: 903.039
[50,     1] loss: 933.498
[51,     1] loss: 937.344
[52,     1] loss: 922.057
[53,     1] loss: 922.395
[54,     1] loss: 810.841
[55,     1] loss: 1022.367
[56,     1] loss: 901.151
[57,     1] loss: 861.763
[58,     1] loss: 967.597
[59,     1] loss: 812.870
[60,     1] loss: 861.038
[61,     1] loss: 783.997
[62,     1] loss: 846.208
[63,     1] loss: 889.766
[64,     1] loss: 769.028
[65,     1] loss: 780.742
Early stopping applied (best metric=0.8876816034317017)
Finished Training
Total time taken: 8.825007677078247
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1372.583
[2,     1] loss: 1374.171
[3,     1] loss: 1364.661
[4,     1] loss: 1372.321
[5,     1] loss: 1372.584
[6,     1] loss: 1370.701
[7,     1] loss: 1367.476
[8,     1] loss: 1366.501
[9,     1] loss: 1363.351
[10,     1] loss: 1361.121
[11,     1] loss: 1352.922
[12,     1] loss: 1339.777
[13,     1] loss: 1314.815
[14,     1] loss: 1289.718
[15,     1] loss: 1235.511
[16,     1] loss: 1216.713
[17,     1] loss: 1163.885
[18,     1] loss: 1164.652
[19,     1] loss: 1165.680
[20,     1] loss: 1098.046
[21,     1] loss: 1093.944
[22,     1] loss: 1121.931
[23,     1] loss: 1079.473
[24,     1] loss: 1114.986
[25,     1] loss: 1107.909
[26,     1] loss: 1141.496
[27,     1] loss: 1074.482
[28,     1] loss: 1175.257
[29,     1] loss: 1015.913
[30,     1] loss: 1106.184
[31,     1] loss: 1052.334
[32,     1] loss: 1083.597
[33,     1] loss: 1027.673
[34,     1] loss: 1050.653
[35,     1] loss: 1072.345
[36,     1] loss: 1029.725
[37,     1] loss: 1007.287
[38,     1] loss: 1045.805
[39,     1] loss: 999.662
[40,     1] loss: 983.923
[41,     1] loss: 989.894
[42,     1] loss: 1024.465
[43,     1] loss: 961.228
[44,     1] loss: 933.178
[45,     1] loss: 946.666
[46,     1] loss: 907.332
[47,     1] loss: 909.979
[48,     1] loss: 896.527
[49,     1] loss: 932.620
[50,     1] loss: 861.121
[51,     1] loss: 855.483
[52,     1] loss: 1073.171
[53,     1] loss: 1022.266
[54,     1] loss: 859.858
[55,     1] loss: 957.152
[56,     1] loss: 841.003
[57,     1] loss: 916.307
[58,     1] loss: 813.599
[59,     1] loss: 856.716
[60,     1] loss: 804.936
[61,     1] loss: 936.328
[62,     1] loss: 803.131
[63,     1] loss: 815.028
[64,     1] loss: 800.188
[65,     1] loss: 720.721
[66,     1] loss: 846.882
Early stopping applied (best metric=0.7398440837860107)
Finished Training
Total time taken: 8.971008777618408
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1370.767
[2,     1] loss: 1372.800
[3,     1] loss: 1368.552
[4,     1] loss: 1370.785
[5,     1] loss: 1367.863
[6,     1] loss: 1369.995
[7,     1] loss: 1368.314
[8,     1] loss: 1372.187
[9,     1] loss: 1367.605
[10,     1] loss: 1367.501
[11,     1] loss: 1367.893
[12,     1] loss: 1362.867
[13,     1] loss: 1363.380
[14,     1] loss: 1355.379
[15,     1] loss: 1349.260
[16,     1] loss: 1337.121
[17,     1] loss: 1303.000
[18,     1] loss: 1258.757
[19,     1] loss: 1230.319
[20,     1] loss: 1171.792
[21,     1] loss: 1126.016
[22,     1] loss: 1163.414
[23,     1] loss: 1153.321
[24,     1] loss: 1106.031
[25,     1] loss: 1138.760
[26,     1] loss: 1086.247
[27,     1] loss: 1121.274
[28,     1] loss: 1076.414
[29,     1] loss: 1087.566
[30,     1] loss: 1078.235
[31,     1] loss: 1010.946
[32,     1] loss: 1043.058
[33,     1] loss: 1057.434
[34,     1] loss: 1042.669
[35,     1] loss: 1111.991
[36,     1] loss: 999.771
[37,     1] loss: 1079.329
[38,     1] loss: 975.933
[39,     1] loss: 1014.852
[40,     1] loss: 995.368
[41,     1] loss: 1015.927
[42,     1] loss: 909.560
[43,     1] loss: 971.867
[44,     1] loss: 938.154
[45,     1] loss: 976.854
[46,     1] loss: 938.893
[47,     1] loss: 931.616
[48,     1] loss: 948.481
[49,     1] loss: 890.730
[50,     1] loss: 923.644
[51,     1] loss: 870.141
[52,     1] loss: 843.209
[53,     1] loss: 913.968
[54,     1] loss: 868.390
[55,     1] loss: 759.185
[56,     1] loss: 795.078
[57,     1] loss: 844.101
[58,     1] loss: 814.059
[59,     1] loss: 1115.534
[60,     1] loss: 1456.098
[61,     1] loss: 1039.644
[62,     1] loss: 915.148
[63,     1] loss: 1077.207
[64,     1] loss: 1070.906
[65,     1] loss: 1051.033
[66,     1] loss: 1033.916
[67,     1] loss: 1016.378
[68,     1] loss: 1041.796
[69,     1] loss: 978.593
[70,     1] loss: 940.623
[71,     1] loss: 985.360
[72,     1] loss: 922.723
[73,     1] loss: 903.582
[74,     1] loss: 857.371
[75,     1] loss: 854.159
[76,     1] loss: 803.933
[77,     1] loss: 818.394
Early stopping applied (best metric=0.775098979473114)
Finished Training
Total time taken: 12.840015649795532
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1372.416
[2,     1] loss: 1382.475
[3,     1] loss: 1386.666
[4,     1] loss: 1377.616
[5,     1] loss: 1374.657
[6,     1] loss: 1374.813
[7,     1] loss: 1373.094
[8,     1] loss: 1370.694
[9,     1] loss: 1372.274
[10,     1] loss: 1368.080
[11,     1] loss: 1370.934
[12,     1] loss: 1372.467
[13,     1] loss: 1370.058
[14,     1] loss: 1370.682
[15,     1] loss: 1368.920
[16,     1] loss: 1375.926
[17,     1] loss: 1367.867
[18,     1] loss: 1368.384
[19,     1] loss: 1370.662
[20,     1] loss: 1369.974
[21,     1] loss: 1368.190
[22,     1] loss: 1366.737
[23,     1] loss: 1368.212
[24,     1] loss: 1367.973
[25,     1] loss: 1363.020
[26,     1] loss: 1360.421
[27,     1] loss: 1348.340
[28,     1] loss: 1332.934
[29,     1] loss: 1311.703
[30,     1] loss: 1287.331
[31,     1] loss: 1231.013
[32,     1] loss: 1192.029
[33,     1] loss: 1219.050
[34,     1] loss: 1214.971
[35,     1] loss: 1161.183
[36,     1] loss: 1181.496
[37,     1] loss: 1164.381
[38,     1] loss: 1092.941
[39,     1] loss: 1151.185
[40,     1] loss: 1090.072
[41,     1] loss: 1068.490
[42,     1] loss: 1081.509
[43,     1] loss: 1032.373
[44,     1] loss: 1059.019
[45,     1] loss: 1070.217
[46,     1] loss: 1128.914
[47,     1] loss: 1000.640
[48,     1] loss: 1056.372
[49,     1] loss: 1050.783
[50,     1] loss: 1019.874
[51,     1] loss: 986.819
[52,     1] loss: 1045.422
[53,     1] loss: 1090.856
[54,     1] loss: 980.433
[55,     1] loss: 994.599
[56,     1] loss: 951.356
[57,     1] loss: 978.334
[58,     1] loss: 976.179
[59,     1] loss: 965.430
[60,     1] loss: 876.762
[61,     1] loss: 934.537
[62,     1] loss: 813.053
[63,     1] loss: 892.721
[64,     1] loss: 923.290
[65,     1] loss: 803.289
[66,     1] loss: 805.783
[67,     1] loss: 979.384
[68,     1] loss: 851.904
[69,     1] loss: 832.128
[70,     1] loss: 756.137
[71,     1] loss: 786.340
[72,     1] loss: 751.331
[73,     1] loss: 730.785
[74,     1] loss: 733.592
Early stopping applied (best metric=0.8023004531860352)
Finished Training
Total time taken: 11.012011051177979
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1369.394
[2,     1] loss: 1376.456
[3,     1] loss: 1376.048
[4,     1] loss: 1369.119
[5,     1] loss: 1366.065
[6,     1] loss: 1363.112
[7,     1] loss: 1354.464
[8,     1] loss: 1354.404
[9,     1] loss: 1327.539
[10,     1] loss: 1305.714
[11,     1] loss: 1283.648
[12,     1] loss: 1204.694
[13,     1] loss: 1199.546
[14,     1] loss: 1191.483
[15,     1] loss: 1142.796
[16,     1] loss: 1133.639
[17,     1] loss: 1144.367
[18,     1] loss: 1151.276
[19,     1] loss: 1123.196
[20,     1] loss: 1150.386
[21,     1] loss: 1139.572
[22,     1] loss: 1118.076
[23,     1] loss: 1107.782
[24,     1] loss: 1035.812
[25,     1] loss: 1079.531
[26,     1] loss: 1068.319
[27,     1] loss: 1033.908
[28,     1] loss: 1088.880
[29,     1] loss: 1015.619
[30,     1] loss: 1080.710
[31,     1] loss: 1029.248
[32,     1] loss: 976.017
[33,     1] loss: 986.333
[34,     1] loss: 978.381
[35,     1] loss: 971.668
[36,     1] loss: 969.348
[37,     1] loss: 953.974
[38,     1] loss: 1000.920
[39,     1] loss: 947.967
[40,     1] loss: 937.084
[41,     1] loss: 955.048
[42,     1] loss: 915.612
[43,     1] loss: 920.287
Early stopping applied (best metric=0.9206233024597168)
Finished Training
Total time taken: 6.4590065479278564
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.135
[2,     1] loss: 1378.089
[3,     1] loss: 1374.422
[4,     1] loss: 1368.640
[5,     1] loss: 1375.534
[6,     1] loss: 1370.399
[7,     1] loss: 1372.927
[8,     1] loss: 1360.871
[9,     1] loss: 1365.129
[10,     1] loss: 1367.314
[11,     1] loss: 1368.341
[12,     1] loss: 1364.257
[13,     1] loss: 1359.755
[14,     1] loss: 1354.487
[15,     1] loss: 1347.708
[16,     1] loss: 1331.951
[17,     1] loss: 1300.905
[18,     1] loss: 1263.419
[19,     1] loss: 1240.905
[20,     1] loss: 1192.528
[21,     1] loss: 1209.170
[22,     1] loss: 1141.924
[23,     1] loss: 1196.292
[24,     1] loss: 1139.855
[25,     1] loss: 1133.990
[26,     1] loss: 1092.473
[27,     1] loss: 1134.703
[28,     1] loss: 1114.881
[29,     1] loss: 1124.233
[30,     1] loss: 1092.311
[31,     1] loss: 1042.278
[32,     1] loss: 1065.520
[33,     1] loss: 1046.989
[34,     1] loss: 1069.687
[35,     1] loss: 1028.544
[36,     1] loss: 1028.729
[37,     1] loss: 1038.237
[38,     1] loss: 1011.471
[39,     1] loss: 1038.317
[40,     1] loss: 1018.422
[41,     1] loss: 990.336
[42,     1] loss: 997.738
[43,     1] loss: 991.612
[44,     1] loss: 914.097
[45,     1] loss: 957.643
[46,     1] loss: 935.444
[47,     1] loss: 938.889
[48,     1] loss: 929.120
[49,     1] loss: 898.050
[50,     1] loss: 897.623
[51,     1] loss: 980.012
[52,     1] loss: 1000.865
[53,     1] loss: 823.845
[54,     1] loss: 968.283
[55,     1] loss: 892.092
[56,     1] loss: 1024.591
[57,     1] loss: 870.781
[58,     1] loss: 908.504
[59,     1] loss: 814.893
[60,     1] loss: 880.686
[61,     1] loss: 835.764
[62,     1] loss: 840.209
[63,     1] loss: 753.394
[64,     1] loss: 721.617
[65,     1] loss: 758.330
[66,     1] loss: 779.020
[67,     1] loss: 1113.949
[68,     1] loss: 997.068
[69,     1] loss: 724.042
[70,     1] loss: 887.038
[71,     1] loss: 898.199
[72,     1] loss: 831.423
[73,     1] loss: 835.077
[74,     1] loss: 773.237
[75,     1] loss: 807.362
[76,     1] loss: 805.164
[77,     1] loss: 727.602
[78,     1] loss: 733.101
Early stopping applied (best metric=0.8542578220367432)
Finished Training
Total time taken: 11.228012800216675
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1370.345
[2,     1] loss: 1373.785
[3,     1] loss: 1368.911
[4,     1] loss: 1371.776
[5,     1] loss: 1364.010
[6,     1] loss: 1363.556
[7,     1] loss: 1367.507
[8,     1] loss: 1353.213
[9,     1] loss: 1339.890
[10,     1] loss: 1307.915
[11,     1] loss: 1283.371
[12,     1] loss: 1197.217
[13,     1] loss: 1181.830
[14,     1] loss: 1163.633
[15,     1] loss: 1136.429
[16,     1] loss: 1150.179
[17,     1] loss: 1123.928
[18,     1] loss: 1136.008
[19,     1] loss: 1123.812
[20,     1] loss: 1112.266
[21,     1] loss: 1070.318
[22,     1] loss: 1106.047
[23,     1] loss: 1058.070
[24,     1] loss: 1072.656
[25,     1] loss: 1055.684
[26,     1] loss: 1044.415
[27,     1] loss: 1007.519
[28,     1] loss: 1060.111
[29,     1] loss: 1032.698
[30,     1] loss: 1013.943
[31,     1] loss: 1035.737
[32,     1] loss: 1000.288
[33,     1] loss: 992.099
[34,     1] loss: 1051.619
[35,     1] loss: 1020.428
[36,     1] loss: 960.263
[37,     1] loss: 945.017
[38,     1] loss: 1037.766
[39,     1] loss: 907.840
[40,     1] loss: 961.944
[41,     1] loss: 903.555
[42,     1] loss: 955.271
[43,     1] loss: 916.280
[44,     1] loss: 869.699
[45,     1] loss: 925.122
[46,     1] loss: 843.822
[47,     1] loss: 808.103
[48,     1] loss: 810.394
[49,     1] loss: 827.987
[50,     1] loss: 798.878
[51,     1] loss: 821.126
[52,     1] loss: 1166.290
Early stopping applied (best metric=0.7770470380783081)
Finished Training
Total time taken: 7.117008686065674
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.927
[2,     1] loss: 1372.364
[3,     1] loss: 1372.135
[4,     1] loss: 1374.359
[5,     1] loss: 1370.444
[6,     1] loss: 1366.230
[7,     1] loss: 1372.261
[8,     1] loss: 1371.874
[9,     1] loss: 1368.729
[10,     1] loss: 1368.496
[11,     1] loss: 1367.176
[12,     1] loss: 1367.906
[13,     1] loss: 1367.022
[14,     1] loss: 1361.729
[15,     1] loss: 1357.136
[16,     1] loss: 1346.849
[17,     1] loss: 1343.853
[18,     1] loss: 1324.031
[19,     1] loss: 1282.590
[20,     1] loss: 1271.405
[21,     1] loss: 1219.882
[22,     1] loss: 1239.966
[23,     1] loss: 1225.802
[24,     1] loss: 1183.167
[25,     1] loss: 1123.896
[26,     1] loss: 1137.550
[27,     1] loss: 1132.391
[28,     1] loss: 1135.295
[29,     1] loss: 1128.831
[30,     1] loss: 1170.905
[31,     1] loss: 1128.950
[32,     1] loss: 1143.249
[33,     1] loss: 1142.414
[34,     1] loss: 1069.459
[35,     1] loss: 1089.167
[36,     1] loss: 1032.544
[37,     1] loss: 1085.497
[38,     1] loss: 1103.099
[39,     1] loss: 1010.018
[40,     1] loss: 1086.342
[41,     1] loss: 1028.802
[42,     1] loss: 1140.861
[43,     1] loss: 1008.102
[44,     1] loss: 1052.338
[45,     1] loss: 1055.940
[46,     1] loss: 971.240
[47,     1] loss: 981.166
[48,     1] loss: 1002.839
[49,     1] loss: 1037.637
[50,     1] loss: 1008.211
[51,     1] loss: 956.442
[52,     1] loss: 978.831
[53,     1] loss: 942.548
[54,     1] loss: 983.723
[55,     1] loss: 881.426
[56,     1] loss: 895.354
[57,     1] loss: 897.509
[58,     1] loss: 882.080
[59,     1] loss: 887.665
[60,     1] loss: 894.895
Early stopping applied (best metric=0.7694476246833801)
Finished Training
Total time taken: 9.830010175704956
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1372.278
[2,     1] loss: 1370.120
[3,     1] loss: 1369.781
[4,     1] loss: 1376.573
[5,     1] loss: 1371.988
[6,     1] loss: 1369.888
[7,     1] loss: 1374.639
[8,     1] loss: 1362.803
[9,     1] loss: 1365.740
[10,     1] loss: 1360.763
[11,     1] loss: 1349.749
[12,     1] loss: 1339.510
[13,     1] loss: 1301.634
[14,     1] loss: 1291.465
[15,     1] loss: 1262.703
[16,     1] loss: 1217.522
[17,     1] loss: 1210.419
[18,     1] loss: 1179.171
[19,     1] loss: 1163.609
[20,     1] loss: 1137.214
[21,     1] loss: 1098.021
[22,     1] loss: 1148.531
[23,     1] loss: 1126.542
[24,     1] loss: 1096.455
[25,     1] loss: 1102.015
[26,     1] loss: 1114.395
[27,     1] loss: 1086.922
[28,     1] loss: 1089.389
[29,     1] loss: 1104.998
[30,     1] loss: 1101.874
[31,     1] loss: 998.184
[32,     1] loss: 1058.524
[33,     1] loss: 1052.176
[34,     1] loss: 1059.839
[35,     1] loss: 988.099
[36,     1] loss: 968.448
[37,     1] loss: 993.946
[38,     1] loss: 1046.109
[39,     1] loss: 943.229
[40,     1] loss: 991.899
[41,     1] loss: 972.244
[42,     1] loss: 1012.925
[43,     1] loss: 906.123
[44,     1] loss: 923.202
[45,     1] loss: 878.474
[46,     1] loss: 890.474
[47,     1] loss: 889.413
[48,     1] loss: 1005.968
[49,     1] loss: 1022.993
[50,     1] loss: 894.065
[51,     1] loss: 903.263
[52,     1] loss: 921.073
[53,     1] loss: 930.142
[54,     1] loss: 796.400
[55,     1] loss: 931.891
[56,     1] loss: 840.039
[57,     1] loss: 889.139
[58,     1] loss: 822.970
[59,     1] loss: 824.071
[60,     1] loss: 811.321
[61,     1] loss: 800.658
[62,     1] loss: 743.916
[63,     1] loss: 679.856
[64,     1] loss: 684.356
[65,     1] loss: 818.806
[66,     1] loss: 912.343
[67,     1] loss: 1395.536
[68,     1] loss: 691.437
[69,     1] loss: 1004.225
[70,     1] loss: 831.326
[71,     1] loss: 860.973
[72,     1] loss: 952.951
[73,     1] loss: 828.641
[74,     1] loss: 806.932
Early stopping applied (best metric=0.7804258465766907)
Finished Training
Total time taken: 12.310013771057129
{'Hydroxylation-K Validation Accuracy': 0.7956560283687943, 'Hydroxylation-K Validation Sensitivity': 0.7074074074074074, 'Hydroxylation-K Validation Specificity': 0.8175438596491228, 'Hydroxylation-K Validation Precision': 0.5104912407853585, 'Hydroxylation-K AUC ROC': 0.8224756335282651, 'Hydroxylation-K AUC PR': 0.6435980717844427, 'Hydroxylation-K MCC': 0.4730249061523224, 'Hydroxylation-K F1': 0.5868314046804647, 'Validation Loss (Hydroxylation-K)': 0.41377147237459816, 'Hydroxylation-P Validation Accuracy': 0.7959912525590918, 'Hydroxylation-P Validation Sensitivity': 0.7837566137566138, 'Hydroxylation-P Validation Specificity': 0.7985410743677989, 'Hydroxylation-P Validation Precision': 0.46197644485903744, 'Hydroxylation-P AUC ROC': 0.8431435799750927, 'Hydroxylation-P AUC PR': 0.576215158000154, 'Hydroxylation-P MCC': 0.48668887792705745, 'Hydroxylation-P F1': 0.5779037539150436, 'Validation Loss (Hydroxylation-P)': 0.378292053937912, 'Validation Loss (total)': 0.7920635183652242, 'TimeToTrain': 10.32141048113505}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032767298298839944,
 'learning_rate_Hydroxylation-K': 0.008275077593369047,
 'learning_rate_Hydroxylation-P': 0.004602896979108779,
 'log_base': 2.5440593237801306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 656988086,
 'sample_weights': [2.1935371632539966, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.238092532951811,
 'weight_decay_Hydroxylation-K': 5.650813845497285,
 'weight_decay_Hydroxylation-P': 2.324390879112869}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1291.593
[2,     1] loss: 1287.056
[3,     1] loss: 1284.059
[4,     1] loss: 1288.470
[5,     1] loss: 1279.304
[6,     1] loss: 1278.610
[7,     1] loss: 1275.059
[8,     1] loss: 1264.829
[9,     1] loss: 1240.561
[10,     1] loss: 1223.907
[11,     1] loss: 1178.602
[12,     1] loss: 1170.207
[13,     1] loss: 1129.171
[14,     1] loss: 1102.855
[15,     1] loss: 1098.132
[16,     1] loss: 1078.893
[17,     1] loss: 1095.370
[18,     1] loss: 1069.642
[19,     1] loss: 1066.450
[20,     1] loss: 1032.037
[21,     1] loss: 1087.307
[22,     1] loss: 1045.653
[23,     1] loss: 1029.293
[24,     1] loss: 1049.123
[25,     1] loss: 998.343
[26,     1] loss: 991.743
[27,     1] loss: 1037.916
[28,     1] loss: 951.721
[29,     1] loss: 986.887
[30,     1] loss: 981.907
[31,     1] loss: 951.639
[32,     1] loss: 923.537
[33,     1] loss: 949.241
[34,     1] loss: 933.634
[35,     1] loss: 924.883
[36,     1] loss: 910.100
[37,     1] loss: 904.488
[38,     1] loss: 938.173
[39,     1] loss: 895.783
[40,     1] loss: 936.390
[41,     1] loss: 906.478
[42,     1] loss: 939.341
[43,     1] loss: 910.302
[44,     1] loss: 888.256
[45,     1] loss: 836.629
[46,     1] loss: 855.626
[47,     1] loss: 797.381
[48,     1] loss: 826.979
[49,     1] loss: 852.592
[50,     1] loss: 775.236
[51,     1] loss: 810.792
[52,     1] loss: 790.414
[53,     1] loss: 816.088
[54,     1] loss: 833.052
[55,     1] loss: 761.448
[56,     1] loss: 739.054
[57,     1] loss: 759.616
[58,     1] loss: 761.935
[59,     1] loss: 722.620
[60,     1] loss: 750.010
[61,     1] loss: 759.870
[62,     1] loss: 747.651
[63,     1] loss: 663.249
[64,     1] loss: 783.379
[65,     1] loss: 636.361
[66,     1] loss: 686.791
[67,     1] loss: 710.941
[68,     1] loss: 636.469
[69,     1] loss: 698.955
[70,     1] loss: 678.264
[71,     1] loss: 650.166
[72,     1] loss: 650.682
[73,     1] loss: 649.962
[74,     1] loss: 617.583
[75,     1] loss: 638.582
[76,     1] loss: 680.493
[77,     1] loss: 620.740
Early stopping applied (best metric=0.7912411689758301)
Finished Training
Total time taken: 10.422009229660034
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.426
[2,     1] loss: 1283.538
[3,     1] loss: 1282.217
[4,     1] loss: 1282.940
[5,     1] loss: 1276.439
[6,     1] loss: 1273.667
[7,     1] loss: 1253.189
[8,     1] loss: 1228.024
[9,     1] loss: 1192.635
[10,     1] loss: 1174.322
[11,     1] loss: 1128.173
[12,     1] loss: 1101.120
[13,     1] loss: 1105.445
[14,     1] loss: 1086.742
[15,     1] loss: 1085.504
[16,     1] loss: 1047.927
[17,     1] loss: 1061.834
[18,     1] loss: 1030.157
[19,     1] loss: 1009.480
[20,     1] loss: 1015.254
[21,     1] loss: 1006.710
[22,     1] loss: 1014.348
[23,     1] loss: 981.147
[24,     1] loss: 947.797
[25,     1] loss: 985.884
[26,     1] loss: 1017.767
[27,     1] loss: 938.492
[28,     1] loss: 954.995
[29,     1] loss: 959.430
[30,     1] loss: 974.188
[31,     1] loss: 894.688
[32,     1] loss: 937.275
[33,     1] loss: 868.856
[34,     1] loss: 883.471
[35,     1] loss: 885.635
[36,     1] loss: 926.936
[37,     1] loss: 924.206
[38,     1] loss: 902.231
[39,     1] loss: 861.040
[40,     1] loss: 898.039
[41,     1] loss: 873.536
[42,     1] loss: 857.708
[43,     1] loss: 850.709
[44,     1] loss: 813.765
[45,     1] loss: 771.922
[46,     1] loss: 816.016
[47,     1] loss: 746.547
[48,     1] loss: 762.730
[49,     1] loss: 816.988
[50,     1] loss: 780.424
[51,     1] loss: 832.702
[52,     1] loss: 835.361
[53,     1] loss: 826.305
[54,     1] loss: 708.515
[55,     1] loss: 769.571
[56,     1] loss: 730.781
[57,     1] loss: 791.786
[58,     1] loss: 735.437
[59,     1] loss: 741.730
Early stopping applied (best metric=0.9378104209899902)
Finished Training
Total time taken: 9.863009691238403
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.882
[2,     1] loss: 1283.759
[3,     1] loss: 1280.768
[4,     1] loss: 1284.417
[5,     1] loss: 1280.355
[6,     1] loss: 1277.687
[7,     1] loss: 1256.702
[8,     1] loss: 1245.269
[9,     1] loss: 1211.367
[10,     1] loss: 1169.729
[11,     1] loss: 1141.746
[12,     1] loss: 1103.691
[13,     1] loss: 1135.652
[14,     1] loss: 1115.522
[15,     1] loss: 1051.060
[16,     1] loss: 1047.802
[17,     1] loss: 1055.094
[18,     1] loss: 1074.657
[19,     1] loss: 1044.208
[20,     1] loss: 1047.635
[21,     1] loss: 1037.947
[22,     1] loss: 1049.997
[23,     1] loss: 1022.476
[24,     1] loss: 1036.195
[25,     1] loss: 993.090
[26,     1] loss: 1024.049
[27,     1] loss: 989.050
[28,     1] loss: 996.485
[29,     1] loss: 981.280
[30,     1] loss: 960.454
[31,     1] loss: 965.976
[32,     1] loss: 903.149
[33,     1] loss: 916.777
[34,     1] loss: 940.927
[35,     1] loss: 937.802
[36,     1] loss: 900.786
[37,     1] loss: 935.953
[38,     1] loss: 850.277
[39,     1] loss: 889.544
[40,     1] loss: 894.034
[41,     1] loss: 841.798
[42,     1] loss: 869.371
[43,     1] loss: 829.231
[44,     1] loss: 859.799
[45,     1] loss: 855.602
[46,     1] loss: 865.713
[47,     1] loss: 820.203
[48,     1] loss: 773.156
[49,     1] loss: 786.579
[50,     1] loss: 769.727
[51,     1] loss: 786.894
[52,     1] loss: 939.786
[53,     1] loss: 964.867
[54,     1] loss: 775.200
[55,     1] loss: 777.882
Early stopping applied (best metric=0.8007127642631531)
Finished Training
Total time taken: 9.138007640838623
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.467
[2,     1] loss: 1282.573
[3,     1] loss: 1280.990
[4,     1] loss: 1287.292
[5,     1] loss: 1278.092
[6,     1] loss: 1286.639
[7,     1] loss: 1277.435
[8,     1] loss: 1267.958
[9,     1] loss: 1264.705
[10,     1] loss: 1229.134
[11,     1] loss: 1208.927
[12,     1] loss: 1150.758
[13,     1] loss: 1138.450
[14,     1] loss: 1103.802
[15,     1] loss: 1100.138
[16,     1] loss: 1117.800
[17,     1] loss: 1035.661
[18,     1] loss: 1062.272
[19,     1] loss: 1030.391
[20,     1] loss: 985.662
[21,     1] loss: 1023.518
[22,     1] loss: 1002.831
[23,     1] loss: 1023.947
[24,     1] loss: 983.780
[25,     1] loss: 964.462
[26,     1] loss: 977.183
[27,     1] loss: 954.839
[28,     1] loss: 952.866
[29,     1] loss: 955.404
[30,     1] loss: 956.298
[31,     1] loss: 980.700
[32,     1] loss: 938.933
[33,     1] loss: 937.615
[34,     1] loss: 969.927
[35,     1] loss: 926.906
[36,     1] loss: 919.764
[37,     1] loss: 882.677
[38,     1] loss: 935.032
[39,     1] loss: 874.395
[40,     1] loss: 875.544
[41,     1] loss: 848.957
[42,     1] loss: 858.690
[43,     1] loss: 799.951
[44,     1] loss: 803.073
[45,     1] loss: 814.303
[46,     1] loss: 834.066
[47,     1] loss: 822.045
[48,     1] loss: 767.985
[49,     1] loss: 779.338
[50,     1] loss: 819.115
[51,     1] loss: 840.476
[52,     1] loss: 784.131
[53,     1] loss: 737.183
[54,     1] loss: 736.342
[55,     1] loss: 751.247
[56,     1] loss: 670.211
[57,     1] loss: 712.331
[58,     1] loss: 861.583
[59,     1] loss: 896.579
[60,     1] loss: 671.767
[61,     1] loss: 791.571
[62,     1] loss: 679.506
[63,     1] loss: 812.682
[64,     1] loss: 707.039
[65,     1] loss: 793.618
[66,     1] loss: 695.892
[67,     1] loss: 708.688
[68,     1] loss: 624.777
[69,     1] loss: 683.611
[70,     1] loss: 632.211
[71,     1] loss: 689.507
[72,     1] loss: 593.952
[73,     1] loss: 761.198
[74,     1] loss: 603.434
[75,     1] loss: 725.068
[76,     1] loss: 589.194
[77,     1] loss: 616.224
[78,     1] loss: 576.768
[79,     1] loss: 586.435
[80,     1] loss: 591.974
[81,     1] loss: 578.601
[82,     1] loss: 591.152
[83,     1] loss: 491.731
[84,     1] loss: 549.600
[85,     1] loss: 515.188
[86,     1] loss: 487.290
[87,     1] loss: 528.419
[88,     1] loss: 592.198
[89,     1] loss: 763.512
[90,     1] loss: 515.901
[91,     1] loss: 569.374
[92,     1] loss: 563.202
[93,     1] loss: 531.530
[94,     1] loss: 600.729
[95,     1] loss: 548.243
[96,     1] loss: 491.860
[97,     1] loss: 533.058
[98,     1] loss: 482.228
[99,     1] loss: 484.876
[100,     1] loss: 486.462
[101,     1] loss: 576.218
[102,     1] loss: 472.896
[103,     1] loss: 432.712
[104,     1] loss: 487.792
[105,     1] loss: 589.883
[106,     1] loss: 396.477
[107,     1] loss: 514.424
[108,     1] loss: 538.109
Early stopping applied (best metric=0.7844336032867432)
Finished Training
Total time taken: 17.910016775131226
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1286.257
[2,     1] loss: 1291.261
[3,     1] loss: 1287.019
[4,     1] loss: 1284.978
[5,     1] loss: 1282.128
[6,     1] loss: 1277.468
[7,     1] loss: 1266.837
[8,     1] loss: 1256.676
[9,     1] loss: 1239.067
[10,     1] loss: 1208.623
[11,     1] loss: 1163.607
[12,     1] loss: 1150.982
[13,     1] loss: 1102.364
[14,     1] loss: 1099.404
[15,     1] loss: 1121.835
[16,     1] loss: 1085.718
[17,     1] loss: 1091.828
[18,     1] loss: 1111.780
[19,     1] loss: 1061.567
[20,     1] loss: 1014.872
[21,     1] loss: 1058.215
[22,     1] loss: 1083.979
[23,     1] loss: 1059.879
[24,     1] loss: 1012.629
[25,     1] loss: 1038.679
[26,     1] loss: 990.488
[27,     1] loss: 994.684
[28,     1] loss: 1011.731
[29,     1] loss: 931.108
[30,     1] loss: 958.301
[31,     1] loss: 946.539
[32,     1] loss: 968.059
[33,     1] loss: 965.732
[34,     1] loss: 942.685
[35,     1] loss: 912.308
[36,     1] loss: 908.831
[37,     1] loss: 862.273
[38,     1] loss: 884.156
[39,     1] loss: 861.940
[40,     1] loss: 824.973
[41,     1] loss: 866.236
[42,     1] loss: 892.487
[43,     1] loss: 921.255
[44,     1] loss: 806.165
[45,     1] loss: 863.715
[46,     1] loss: 828.552
[47,     1] loss: 752.085
[48,     1] loss: 837.139
[49,     1] loss: 791.112
[50,     1] loss: 714.882
[51,     1] loss: 770.965
[52,     1] loss: 771.174
[53,     1] loss: 746.044
[54,     1] loss: 795.342
[55,     1] loss: 742.792
[56,     1] loss: 691.007
[57,     1] loss: 712.457
[58,     1] loss: 716.888
[59,     1] loss: 716.135
Early stopping applied (best metric=0.7407833933830261)
Finished Training
Total time taken: 7.95900559425354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.032
[2,     1] loss: 1286.639
[3,     1] loss: 1288.773
[4,     1] loss: 1282.449
[5,     1] loss: 1282.345
[6,     1] loss: 1283.943
[7,     1] loss: 1281.289
[8,     1] loss: 1273.603
[9,     1] loss: 1264.155
[10,     1] loss: 1252.698
[11,     1] loss: 1223.408
[12,     1] loss: 1208.658
[13,     1] loss: 1170.304
[14,     1] loss: 1126.271
[15,     1] loss: 1133.532
[16,     1] loss: 1094.510
[17,     1] loss: 1127.523
[18,     1] loss: 1075.551
[19,     1] loss: 1105.471
[20,     1] loss: 1047.707
[21,     1] loss: 1065.724
[22,     1] loss: 1073.143
[23,     1] loss: 1085.120
[24,     1] loss: 1015.208
[25,     1] loss: 1038.541
[26,     1] loss: 1027.004
[27,     1] loss: 1031.565
[28,     1] loss: 993.963
[29,     1] loss: 1029.564
[30,     1] loss: 1014.676
[31,     1] loss: 963.236
[32,     1] loss: 972.974
[33,     1] loss: 960.273
[34,     1] loss: 925.942
[35,     1] loss: 968.471
[36,     1] loss: 929.357
[37,     1] loss: 903.035
[38,     1] loss: 879.862
[39,     1] loss: 881.617
[40,     1] loss: 905.922
[41,     1] loss: 920.621
[42,     1] loss: 880.974
[43,     1] loss: 932.590
[44,     1] loss: 845.501
[45,     1] loss: 830.472
[46,     1] loss: 893.760
[47,     1] loss: 810.069
[48,     1] loss: 828.622
[49,     1] loss: 824.749
[50,     1] loss: 891.427
[51,     1] loss: 854.512
[52,     1] loss: 774.585
[53,     1] loss: 855.272
[54,     1] loss: 806.639
[55,     1] loss: 772.640
[56,     1] loss: 810.368
[57,     1] loss: 782.552
[58,     1] loss: 832.499
[59,     1] loss: 755.190
[60,     1] loss: 798.102
[61,     1] loss: 717.772
[62,     1] loss: 752.842
[63,     1] loss: 712.801
[64,     1] loss: 678.427
[65,     1] loss: 704.112
[66,     1] loss: 658.473
[67,     1] loss: 634.973
[68,     1] loss: 593.980
[69,     1] loss: 628.801
[70,     1] loss: 789.612
[71,     1] loss: 1333.119
[72,     1] loss: 619.345
[73,     1] loss: 977.612
[74,     1] loss: 641.861
[75,     1] loss: 848.892
[76,     1] loss: 847.886
[77,     1] loss: 785.170
[78,     1] loss: 745.662
[79,     1] loss: 832.937
[80,     1] loss: 792.441
[81,     1] loss: 689.260
[82,     1] loss: 712.094
Early stopping applied (best metric=0.767714262008667)
Finished Training
Total time taken: 12.478012323379517
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.855
[2,     1] loss: 1285.232
[3,     1] loss: 1282.612
[4,     1] loss: 1282.392
[5,     1] loss: 1278.868
[6,     1] loss: 1267.815
[7,     1] loss: 1265.936
[8,     1] loss: 1243.635
[9,     1] loss: 1224.210
[10,     1] loss: 1183.009
[11,     1] loss: 1170.839
[12,     1] loss: 1131.050
[13,     1] loss: 1106.639
[14,     1] loss: 1094.901
[15,     1] loss: 1083.225
[16,     1] loss: 1097.047
[17,     1] loss: 1058.274
[18,     1] loss: 1048.854
[19,     1] loss: 1051.625
[20,     1] loss: 1015.666
[21,     1] loss: 1061.402
[22,     1] loss: 1013.751
[23,     1] loss: 984.172
[24,     1] loss: 996.105
[25,     1] loss: 972.966
[26,     1] loss: 991.625
[27,     1] loss: 1002.318
[28,     1] loss: 939.015
[29,     1] loss: 929.781
[30,     1] loss: 897.912
[31,     1] loss: 917.398
[32,     1] loss: 831.578
[33,     1] loss: 886.740
[34,     1] loss: 850.625
[35,     1] loss: 826.859
[36,     1] loss: 869.304
[37,     1] loss: 818.293
[38,     1] loss: 799.118
[39,     1] loss: 781.881
[40,     1] loss: 802.188
[41,     1] loss: 739.245
[42,     1] loss: 758.999
[43,     1] loss: 875.010
[44,     1] loss: 1038.338
[45,     1] loss: 769.448
[46,     1] loss: 936.801
[47,     1] loss: 790.531
[48,     1] loss: 951.172
[49,     1] loss: 859.672
[50,     1] loss: 827.307
[51,     1] loss: 888.133
[52,     1] loss: 871.326
[53,     1] loss: 761.426
[54,     1] loss: 879.258
[55,     1] loss: 785.051
[56,     1] loss: 745.690
[57,     1] loss: 807.932
[58,     1] loss: 738.848
[59,     1] loss: 768.790
[60,     1] loss: 730.156
[61,     1] loss: 740.912
Early stopping applied (best metric=0.7592657804489136)
Finished Training
Total time taken: 10.14700984954834
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.113
[2,     1] loss: 1289.952
[3,     1] loss: 1285.713
[4,     1] loss: 1282.503
[5,     1] loss: 1279.040
[6,     1] loss: 1273.664
[7,     1] loss: 1261.581
[8,     1] loss: 1242.927
[9,     1] loss: 1199.665
[10,     1] loss: 1176.499
[11,     1] loss: 1132.308
[12,     1] loss: 1069.952
[13,     1] loss: 1033.754
[14,     1] loss: 1098.703
[15,     1] loss: 1042.398
[16,     1] loss: 1057.751
[17,     1] loss: 1084.833
[18,     1] loss: 1028.859
[19,     1] loss: 1000.960
[20,     1] loss: 1006.403
[21,     1] loss: 1006.577
[22,     1] loss: 1017.552
[23,     1] loss: 1011.243
[24,     1] loss: 951.007
[25,     1] loss: 960.805
[26,     1] loss: 944.678
[27,     1] loss: 949.098
[28,     1] loss: 891.770
[29,     1] loss: 916.670
[30,     1] loss: 928.005
[31,     1] loss: 896.476
[32,     1] loss: 897.865
[33,     1] loss: 930.548
[34,     1] loss: 828.756
[35,     1] loss: 846.559
[36,     1] loss: 789.112
[37,     1] loss: 754.129
[38,     1] loss: 779.625
[39,     1] loss: 812.604
[40,     1] loss: 779.779
[41,     1] loss: 768.593
[42,     1] loss: 736.466
[43,     1] loss: 707.694
[44,     1] loss: 758.404
[45,     1] loss: 712.559
[46,     1] loss: 722.929
[47,     1] loss: 819.331
[48,     1] loss: 748.741
[49,     1] loss: 701.360
[50,     1] loss: 753.228
[51,     1] loss: 649.140
[52,     1] loss: 710.708
Early stopping applied (best metric=0.9393688440322876)
Finished Training
Total time taken: 8.667008638381958
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.659
[2,     1] loss: 1291.438
[3,     1] loss: 1286.047
[4,     1] loss: 1283.558
[5,     1] loss: 1283.856
[6,     1] loss: 1284.898
[7,     1] loss: 1290.512
[8,     1] loss: 1283.106
[9,     1] loss: 1282.706
[10,     1] loss: 1279.129
[11,     1] loss: 1279.162
[12,     1] loss: 1270.155
[13,     1] loss: 1263.947
[14,     1] loss: 1246.352
[15,     1] loss: 1231.695
[16,     1] loss: 1214.558
[17,     1] loss: 1180.935
[18,     1] loss: 1153.543
[19,     1] loss: 1137.721
[20,     1] loss: 1109.359
[21,     1] loss: 1092.419
[22,     1] loss: 1066.198
[23,     1] loss: 1033.025
[24,     1] loss: 1060.065
[25,     1] loss: 1034.706
[26,     1] loss: 1046.753
[27,     1] loss: 1020.361
[28,     1] loss: 1017.121
[29,     1] loss: 1041.020
[30,     1] loss: 981.024
[31,     1] loss: 997.824
[32,     1] loss: 999.155
[33,     1] loss: 973.449
[34,     1] loss: 977.808
[35,     1] loss: 933.987
[36,     1] loss: 934.768
[37,     1] loss: 948.878
[38,     1] loss: 947.542
[39,     1] loss: 904.516
[40,     1] loss: 946.219
[41,     1] loss: 910.258
[42,     1] loss: 892.479
[43,     1] loss: 892.810
[44,     1] loss: 893.341
[45,     1] loss: 886.234
[46,     1] loss: 855.852
[47,     1] loss: 937.208
[48,     1] loss: 911.867
[49,     1] loss: 798.847
[50,     1] loss: 855.793
[51,     1] loss: 827.059
[52,     1] loss: 836.148
[53,     1] loss: 766.134
[54,     1] loss: 812.982
[55,     1] loss: 768.224
[56,     1] loss: 757.294
[57,     1] loss: 753.757
[58,     1] loss: 733.314
[59,     1] loss: 786.724
[60,     1] loss: 780.340
[61,     1] loss: 677.590
[62,     1] loss: 739.053
[63,     1] loss: 749.337
[64,     1] loss: 697.391
[65,     1] loss: 667.214
[66,     1] loss: 733.233
[67,     1] loss: 754.942
[68,     1] loss: 716.802
[69,     1] loss: 671.371
[70,     1] loss: 693.762
[71,     1] loss: 641.062
[72,     1] loss: 638.509
[73,     1] loss: 634.805
[74,     1] loss: 667.243
[75,     1] loss: 753.361
[76,     1] loss: 590.058
[77,     1] loss: 722.052
[78,     1] loss: 749.665
[79,     1] loss: 595.320
[80,     1] loss: 896.581
[81,     1] loss: 618.467
Early stopping applied (best metric=0.7617716789245605)
Finished Training
Total time taken: 10.949013471603394
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1299.114
[2,     1] loss: 1287.348
[3,     1] loss: 1288.522
[4,     1] loss: 1287.057
[5,     1] loss: 1292.039
[6,     1] loss: 1284.956
[7,     1] loss: 1285.185
[8,     1] loss: 1284.361
[9,     1] loss: 1282.695
[10,     1] loss: 1286.340
[11,     1] loss: 1285.210
[12,     1] loss: 1285.112
[13,     1] loss: 1283.488
[14,     1] loss: 1281.875
[15,     1] loss: 1279.403
[16,     1] loss: 1268.000
[17,     1] loss: 1267.155
[18,     1] loss: 1242.902
[19,     1] loss: 1225.661
[20,     1] loss: 1198.441
[21,     1] loss: 1170.952
[22,     1] loss: 1148.806
[23,     1] loss: 1140.056
[24,     1] loss: 1122.841
[25,     1] loss: 1094.752
[26,     1] loss: 1099.314
[27,     1] loss: 1052.734
[28,     1] loss: 1127.499
[29,     1] loss: 1057.590
[30,     1] loss: 1057.787
[31,     1] loss: 999.819
[32,     1] loss: 1031.710
[33,     1] loss: 987.728
[34,     1] loss: 1011.083
[35,     1] loss: 1008.442
[36,     1] loss: 990.211
[37,     1] loss: 1034.261
[38,     1] loss: 999.782
[39,     1] loss: 945.063
[40,     1] loss: 941.064
[41,     1] loss: 926.094
[42,     1] loss: 944.362
[43,     1] loss: 919.548
[44,     1] loss: 863.437
[45,     1] loss: 926.173
[46,     1] loss: 912.683
[47,     1] loss: 894.872
[48,     1] loss: 907.955
[49,     1] loss: 875.450
[50,     1] loss: 883.198
[51,     1] loss: 871.776
[52,     1] loss: 864.893
[53,     1] loss: 839.127
[54,     1] loss: 800.235
[55,     1] loss: 820.032
[56,     1] loss: 852.934
[57,     1] loss: 850.632
[58,     1] loss: 824.490
[59,     1] loss: 818.742
[60,     1] loss: 769.068
[61,     1] loss: 758.411
[62,     1] loss: 714.995
[63,     1] loss: 752.859
[64,     1] loss: 741.034
[65,     1] loss: 719.557
[66,     1] loss: 704.177
[67,     1] loss: 863.026
[68,     1] loss: 1259.656
[69,     1] loss: 753.310
[70,     1] loss: 960.617
[71,     1] loss: 807.362
[72,     1] loss: 807.879
[73,     1] loss: 876.127
[74,     1] loss: 862.391
Early stopping applied (best metric=0.6948350071907043)
Finished Training
Total time taken: 12.3290114402771
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.120
[2,     1] loss: 1287.278
[3,     1] loss: 1285.597
[4,     1] loss: 1286.645
[5,     1] loss: 1284.126
[6,     1] loss: 1282.938
[7,     1] loss: 1283.181
[8,     1] loss: 1280.978
[9,     1] loss: 1282.047
[10,     1] loss: 1276.771
[11,     1] loss: 1269.680
[12,     1] loss: 1263.696
[13,     1] loss: 1248.079
[14,     1] loss: 1214.782
[15,     1] loss: 1193.872
[16,     1] loss: 1141.978
[17,     1] loss: 1113.697
[18,     1] loss: 1073.497
[19,     1] loss: 1059.210
[20,     1] loss: 1020.987
[21,     1] loss: 1008.249
[22,     1] loss: 936.195
[23,     1] loss: 1052.497
[24,     1] loss: 1020.590
[25,     1] loss: 977.577
[26,     1] loss: 1015.005
[27,     1] loss: 982.747
[28,     1] loss: 1003.440
[29,     1] loss: 948.059
[30,     1] loss: 949.335
[31,     1] loss: 933.296
[32,     1] loss: 919.812
[33,     1] loss: 874.954
[34,     1] loss: 869.751
[35,     1] loss: 899.583
[36,     1] loss: 871.093
[37,     1] loss: 903.775
[38,     1] loss: 890.336
[39,     1] loss: 863.350
[40,     1] loss: 909.417
[41,     1] loss: 833.004
[42,     1] loss: 832.339
[43,     1] loss: 928.274
[44,     1] loss: 860.952
[45,     1] loss: 869.721
[46,     1] loss: 860.424
[47,     1] loss: 892.568
[48,     1] loss: 801.030
[49,     1] loss: 827.447
[50,     1] loss: 787.551
[51,     1] loss: 826.014
[52,     1] loss: 799.240
[53,     1] loss: 755.123
[54,     1] loss: 769.461
[55,     1] loss: 805.242
[56,     1] loss: 712.297
[57,     1] loss: 705.229
[58,     1] loss: 789.281
[59,     1] loss: 670.994
[60,     1] loss: 729.919
[61,     1] loss: 707.393
[62,     1] loss: 681.802
[63,     1] loss: 624.273
[64,     1] loss: 657.118
[65,     1] loss: 617.775
[66,     1] loss: 633.042
[67,     1] loss: 705.458
[68,     1] loss: 1039.799
[69,     1] loss: 701.734
[70,     1] loss: 725.709
[71,     1] loss: 713.559
[72,     1] loss: 817.805
[73,     1] loss: 696.783
[74,     1] loss: 708.392
[75,     1] loss: 733.784
[76,     1] loss: 643.834
[77,     1] loss: 660.698
[78,     1] loss: 704.246
[79,     1] loss: 589.460
[80,     1] loss: 726.834
[81,     1] loss: 611.549
[82,     1] loss: 713.636
[83,     1] loss: 587.007
[84,     1] loss: 718.855
[85,     1] loss: 589.901
[86,     1] loss: 639.902
[87,     1] loss: 582.301
[88,     1] loss: 585.953
[89,     1] loss: 537.414
[90,     1] loss: 603.830
[91,     1] loss: 510.834
[92,     1] loss: 639.614
[93,     1] loss: 568.723
[94,     1] loss: 516.698
[95,     1] loss: 589.918
[96,     1] loss: 423.484
Early stopping applied (best metric=0.8622076511383057)
Finished Training
Total time taken: 13.589013814926147
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.478
[2,     1] loss: 1288.327
[3,     1] loss: 1284.966
[4,     1] loss: 1284.774
[5,     1] loss: 1284.298
[6,     1] loss: 1285.547
[7,     1] loss: 1278.205
[8,     1] loss: 1273.401
[9,     1] loss: 1265.052
[10,     1] loss: 1252.051
[11,     1] loss: 1233.483
[12,     1] loss: 1206.979
[13,     1] loss: 1174.552
[14,     1] loss: 1147.730
[15,     1] loss: 1099.334
[16,     1] loss: 1078.020
[17,     1] loss: 1081.609
[18,     1] loss: 1108.355
[19,     1] loss: 1052.027
[20,     1] loss: 1031.363
[21,     1] loss: 1050.057
[22,     1] loss: 1060.204
[23,     1] loss: 1052.568
[24,     1] loss: 992.791
[25,     1] loss: 1043.795
[26,     1] loss: 969.671
[27,     1] loss: 1029.935
[28,     1] loss: 1014.560
[29,     1] loss: 976.299
[30,     1] loss: 967.281
[31,     1] loss: 920.371
[32,     1] loss: 975.128
[33,     1] loss: 943.003
[34,     1] loss: 889.152
[35,     1] loss: 972.613
[36,     1] loss: 961.670
[37,     1] loss: 921.194
[38,     1] loss: 879.267
[39,     1] loss: 874.573
[40,     1] loss: 870.422
[41,     1] loss: 873.065
[42,     1] loss: 936.473
[43,     1] loss: 854.465
[44,     1] loss: 872.333
[45,     1] loss: 851.633
[46,     1] loss: 826.106
[47,     1] loss: 837.887
[48,     1] loss: 807.283
[49,     1] loss: 829.850
[50,     1] loss: 791.788
[51,     1] loss: 870.794
[52,     1] loss: 972.637
[53,     1] loss: 755.945
[54,     1] loss: 965.523
[55,     1] loss: 759.747
[56,     1] loss: 924.332
[57,     1] loss: 787.239
[58,     1] loss: 800.968
[59,     1] loss: 793.909
[60,     1] loss: 707.318
[61,     1] loss: 756.218
[62,     1] loss: 756.443
[63,     1] loss: 732.284
[64,     1] loss: 716.007
[65,     1] loss: 768.240
[66,     1] loss: 697.455
[67,     1] loss: 736.087
[68,     1] loss: 719.663
[69,     1] loss: 705.000
[70,     1] loss: 737.654
[71,     1] loss: 655.384
[72,     1] loss: 762.765
Early stopping applied (best metric=0.829189658164978)
Finished Training
Total time taken: 10.596010446548462
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.811
[2,     1] loss: 1285.088
[3,     1] loss: 1281.710
[4,     1] loss: 1282.296
[5,     1] loss: 1282.154
[6,     1] loss: 1280.577
[7,     1] loss: 1277.138
[8,     1] loss: 1266.477
[9,     1] loss: 1259.246
[10,     1] loss: 1231.948
[11,     1] loss: 1201.900
[12,     1] loss: 1173.146
[13,     1] loss: 1121.200
[14,     1] loss: 1083.775
[15,     1] loss: 1052.980
[16,     1] loss: 1062.823
[17,     1] loss: 1059.183
[18,     1] loss: 1100.320
[19,     1] loss: 1080.917
[20,     1] loss: 989.690
[21,     1] loss: 1095.078
[22,     1] loss: 1040.160
[23,     1] loss: 1038.667
[24,     1] loss: 1052.173
[25,     1] loss: 1002.222
[26,     1] loss: 969.248
[27,     1] loss: 964.948
[28,     1] loss: 983.481
[29,     1] loss: 939.163
[30,     1] loss: 995.689
[31,     1] loss: 992.391
[32,     1] loss: 997.083
[33,     1] loss: 1009.001
[34,     1] loss: 946.167
[35,     1] loss: 973.809
[36,     1] loss: 883.184
[37,     1] loss: 926.183
[38,     1] loss: 945.361
[39,     1] loss: 889.879
[40,     1] loss: 928.116
[41,     1] loss: 878.565
[42,     1] loss: 859.790
[43,     1] loss: 884.899
[44,     1] loss: 902.857
[45,     1] loss: 836.510
[46,     1] loss: 903.230
[47,     1] loss: 883.487
[48,     1] loss: 815.305
[49,     1] loss: 817.288
[50,     1] loss: 840.027
[51,     1] loss: 792.832
[52,     1] loss: 831.282
[53,     1] loss: 818.819
[54,     1] loss: 796.122
[55,     1] loss: 820.710
[56,     1] loss: 742.374
[57,     1] loss: 764.114
[58,     1] loss: 836.481
[59,     1] loss: 818.254
[60,     1] loss: 757.207
[61,     1] loss: 772.679
[62,     1] loss: 801.727
[63,     1] loss: 687.557
[64,     1] loss: 757.829
[65,     1] loss: 734.005
[66,     1] loss: 710.899
[67,     1] loss: 650.230
[68,     1] loss: 654.364
[69,     1] loss: 682.265
[70,     1] loss: 684.963
[71,     1] loss: 649.631
[72,     1] loss: 682.946
[73,     1] loss: 825.977
[74,     1] loss: 643.395
[75,     1] loss: 675.903
[76,     1] loss: 718.628
[77,     1] loss: 624.632
[78,     1] loss: 730.039
[79,     1] loss: 608.464
[80,     1] loss: 635.286
[81,     1] loss: 646.369
[82,     1] loss: 645.530
[83,     1] loss: 630.505
[84,     1] loss: 563.604
[85,     1] loss: 634.053
[86,     1] loss: 562.076
Early stopping applied (best metric=0.6976844072341919)
Finished Training
Total time taken: 12.486011981964111
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.525
[2,     1] loss: 1285.613
[3,     1] loss: 1283.795
[4,     1] loss: 1285.068
[5,     1] loss: 1281.631
[6,     1] loss: 1283.358
[7,     1] loss: 1280.832
[8,     1] loss: 1275.943
[9,     1] loss: 1264.679
[10,     1] loss: 1257.475
[11,     1] loss: 1233.571
[12,     1] loss: 1218.409
[13,     1] loss: 1188.471
[14,     1] loss: 1148.224
[15,     1] loss: 1108.966
[16,     1] loss: 1098.283
[17,     1] loss: 1113.895
[18,     1] loss: 1075.046
[19,     1] loss: 1075.171
[20,     1] loss: 1064.012
[21,     1] loss: 1028.143
[22,     1] loss: 1031.874
[23,     1] loss: 1003.078
[24,     1] loss: 1046.751
[25,     1] loss: 1013.218
[26,     1] loss: 1019.891
[27,     1] loss: 1003.541
[28,     1] loss: 980.576
[29,     1] loss: 965.485
[30,     1] loss: 996.073
[31,     1] loss: 970.507
[32,     1] loss: 973.589
[33,     1] loss: 910.142
[34,     1] loss: 930.795
[35,     1] loss: 976.125
[36,     1] loss: 900.866
[37,     1] loss: 899.413
[38,     1] loss: 962.136
[39,     1] loss: 932.213
[40,     1] loss: 850.403
[41,     1] loss: 901.813
[42,     1] loss: 854.279
[43,     1] loss: 875.206
[44,     1] loss: 888.427
[45,     1] loss: 785.086
[46,     1] loss: 824.965
[47,     1] loss: 826.184
[48,     1] loss: 811.200
[49,     1] loss: 761.287
[50,     1] loss: 776.484
[51,     1] loss: 935.207
[52,     1] loss: 1113.021
[53,     1] loss: 777.832
[54,     1] loss: 955.658
[55,     1] loss: 826.179
[56,     1] loss: 874.622
[57,     1] loss: 858.674
[58,     1] loss: 876.789
[59,     1] loss: 780.301
Early stopping applied (best metric=0.7079441547393799)
Finished Training
Total time taken: 8.891007423400879
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1301.389
[2,     1] loss: 1292.632
[3,     1] loss: 1286.053
[4,     1] loss: 1285.741
[5,     1] loss: 1285.748
[6,     1] loss: 1284.824
[7,     1] loss: 1285.000
[8,     1] loss: 1279.252
[9,     1] loss: 1272.411
[10,     1] loss: 1258.241
[11,     1] loss: 1243.218
[12,     1] loss: 1216.526
[13,     1] loss: 1199.420
[14,     1] loss: 1160.427
[15,     1] loss: 1133.354
[16,     1] loss: 1113.112
[17,     1] loss: 1073.380
[18,     1] loss: 1072.069
[19,     1] loss: 1076.727
[20,     1] loss: 1047.874
[21,     1] loss: 1031.437
[22,     1] loss: 1014.665
[23,     1] loss: 1023.920
[24,     1] loss: 1028.929
[25,     1] loss: 1006.155
[26,     1] loss: 1007.283
[27,     1] loss: 970.080
[28,     1] loss: 989.392
[29,     1] loss: 1006.825
[30,     1] loss: 997.143
[31,     1] loss: 936.858
[32,     1] loss: 941.721
[33,     1] loss: 902.294
[34,     1] loss: 877.269
[35,     1] loss: 942.982
[36,     1] loss: 942.540
[37,     1] loss: 852.985
[38,     1] loss: 929.465
[39,     1] loss: 838.899
[40,     1] loss: 858.917
[41,     1] loss: 853.044
[42,     1] loss: 866.334
[43,     1] loss: 836.671
[44,     1] loss: 871.936
[45,     1] loss: 810.472
[46,     1] loss: 818.522
[47,     1] loss: 782.437
[48,     1] loss: 869.931
[49,     1] loss: 761.038
[50,     1] loss: 813.973
[51,     1] loss: 748.609
[52,     1] loss: 709.496
[53,     1] loss: 786.997
[54,     1] loss: 719.700
[55,     1] loss: 721.896
Early stopping applied (best metric=0.851006031036377)
Finished Training
Total time taken: 8.580007791519165
{'Hydroxylation-K Validation Accuracy': 0.7631205673758865, 'Hydroxylation-K Validation Sensitivity': 0.697037037037037, 'Hydroxylation-K Validation Specificity': 0.7807017543859649, 'Hydroxylation-K Validation Precision': 0.45337234746677474, 'Hydroxylation-K AUC ROC': 0.8246588693957115, 'Hydroxylation-K AUC PR': 0.6305334728713919, 'Hydroxylation-K MCC': 0.41729836200619896, 'Hydroxylation-K F1': 0.5415539776775474, 'Validation Loss (Hydroxylation-K)': 0.42373905976613363, 'Hydroxylation-P Validation Accuracy': 0.7731834593844644, 'Hydroxylation-P Validation Sensitivity': 0.8030158730158731, 'Hydroxylation-P Validation Specificity': 0.7667564467055713, 'Hydroxylation-P Validation Precision': 0.43739675400185063, 'Hydroxylation-P AUC ROC': 0.8486534082767062, 'Hydroxylation-P AUC PR': 0.5954476211397232, 'Hydroxylation-P MCC': 0.46785427450233585, 'Hydroxylation-P F1': 0.5618435120228086, 'Validation Loss (Hydroxylation-P)': 0.3713255345821381, 'Validation Loss (total)': 0.7950645883878072, 'TimeToTrain': 10.933610407511393}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034336489506926796,
 'learning_rate_Hydroxylation-K': 0.004853185933044974,
 'learning_rate_Hydroxylation-P': 0.005099677256771182,
 'log_base': 1.395648634890728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3268864818,
 'sample_weights': [1.7891961664126341, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.653404889561731,
 'weight_decay_Hydroxylation-K': 6.415606549650376,
 'weight_decay_Hydroxylation-P': 4.248779612809627}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1966.770
[2,     1] loss: 1964.109
[3,     1] loss: 1956.140
[4,     1] loss: 1956.743
[5,     1] loss: 1949.908
[6,     1] loss: 1941.771
[7,     1] loss: 1943.306
[8,     1] loss: 1909.103
[9,     1] loss: 1885.739
[10,     1] loss: 1828.148
[11,     1] loss: 1830.526
[12,     1] loss: 1763.092
[13,     1] loss: 1783.920
[14,     1] loss: 1692.789
[15,     1] loss: 1690.130
[16,     1] loss: 1665.170
[17,     1] loss: 1694.131
[18,     1] loss: 1680.334
[19,     1] loss: 1578.369
[20,     1] loss: 1642.198
[21,     1] loss: 1545.268
[22,     1] loss: 1548.561
[23,     1] loss: 1511.510
[24,     1] loss: 1491.251
[25,     1] loss: 1477.458
[26,     1] loss: 1568.609
[27,     1] loss: 1353.873
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026991574345583743,
 'learning_rate_Hydroxylation-K': 0.009720794241964075,
 'learning_rate_Hydroxylation-P': 0.005114805447473794,
 'log_base': 2.7320269189827844,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1576854413,
 'sample_weights': [5.007939647452712, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1653515847868245,
 'weight_decay_Hydroxylation-K': 4.7146357481827845,
 'weight_decay_Hydroxylation-P': 5.461731764367911}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.715
[2,     1] loss: 1261.271
[3,     1] loss: 1260.884
[4,     1] loss: 1253.684
[5,     1] loss: 1256.083
[6,     1] loss: 1251.288
[7,     1] loss: 1251.833
[8,     1] loss: 1243.833
[9,     1] loss: 1233.239
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001830284246411954,
 'learning_rate_Hydroxylation-K': 0.009810974665928292,
 'learning_rate_Hydroxylation-P': 0.0042287093164935135,
 'log_base': 2.0030129150454017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2673905116,
 'sample_weights': [1.661065075907131, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.936848747362189,
 'weight_decay_Hydroxylation-K': 4.847899295260954,
 'weight_decay_Hydroxylation-P': 1.8340292725636267}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.262
[2,     1] loss: 1415.879
[3,     1] loss: 1417.036
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003396482205737087,
 'learning_rate_Hydroxylation-K': 0.003606005356067912,
 'learning_rate_Hydroxylation-P': 0.0026788269885028226,
 'log_base': 1.859853119834979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2380875315,
 'sample_weights': [2.4032780945518972, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.595641373737652,
 'weight_decay_Hydroxylation-K': 7.084841388131959,
 'weight_decay_Hydroxylation-P': 0.6503366173286952}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1477.105
[2,     1] loss: 1484.561
[3,     1] loss: 1482.816
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002778057760420718,
 'learning_rate_Hydroxylation-K': 0.005744846604506188,
 'learning_rate_Hydroxylation-P': 0.00161484203546907,
 'log_base': 1.0343528203470016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1092110998,
 'sample_weights': [2.69049126271597, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.9288438188482315,
 'weight_decay_Hydroxylation-K': 2.6034407621970796,
 'weight_decay_Hydroxylation-P': 0.3595236720363628}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16005.936
Exploding loss, terminate run (best metric=1.0959737300872803)
Finished Training
Total time taken: 0.19999957084655762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16048.229
Exploding loss, terminate run (best metric=1.0925989151000977)
Finished Training
Total time taken: 0.20799970626831055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16319.537
Exploding loss, terminate run (best metric=1.096190333366394)
Finished Training
Total time taken: 0.22100043296813965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15928.728
Exploding loss, terminate run (best metric=1.0962936878204346)
Finished Training
Total time taken: 0.20600199699401855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16166.225
Exploding loss, terminate run (best metric=1.0744984149932861)
Finished Training
Total time taken: 0.20799875259399414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16021.831
Exploding loss, terminate run (best metric=1.095050573348999)
Finished Training
Total time taken: 0.2350001335144043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16032.246
Exploding loss, terminate run (best metric=1.0931161642074585)
Finished Training
Total time taken: 0.19800019264221191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16060.662
Exploding loss, terminate run (best metric=1.0914421081542969)
Finished Training
Total time taken: 0.22299909591674805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16062.627
Exploding loss, terminate run (best metric=1.0722770690917969)
Finished Training
Total time taken: 0.23100018501281738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16082.783
Exploding loss, terminate run (best metric=1.0716543197631836)
Finished Training
Total time taken: 0.21099853515625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16119.684
Exploding loss, terminate run (best metric=1.095597267150879)
Finished Training
Total time taken: 0.20000004768371582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16037.055
Exploding loss, terminate run (best metric=1.092442274093628)
Finished Training
Total time taken: 0.22499990463256836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16044.143
Exploding loss, terminate run (best metric=1.09261155128479)
Finished Training
Total time taken: 0.20600032806396484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16045.416
Exploding loss, terminate run (best metric=1.0724854469299316)
Finished Training
Total time taken: 0.2030026912689209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16022.859
Exploding loss, terminate run (best metric=1.0762989521026611)
Finished Training
Total time taken: 0.22699832916259766
{'Hydroxylation-K Validation Accuracy': 0.4352541371158392, 'Hydroxylation-K Validation Sensitivity': 0.64, 'Hydroxylation-K Validation Specificity': 0.3842105263157895, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6437037037037037, 'Hydroxylation-K AUC PR': 0.3357281454868801, 'Hydroxylation-K MCC': 0.021212576891892952, 'Hydroxylation-K F1': 0.2326568144499179, 'Validation Loss (Hydroxylation-K)': 0.556842041015625, 'Hydroxylation-P Validation Accuracy': 0.43284313825017345, 'Hydroxylation-P Validation Sensitivity': 0.6203703703703703, 'Hydroxylation-P Validation Specificity': 0.39227642276422764, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.605575033627929, 'Hydroxylation-P AUC PR': 0.31394521714146983, 'Hydroxylation-P MCC': 0.013607191635043857, 'Hydroxylation-P F1': 0.2026725126455977, 'Validation Loss (Hydroxylation-P)': 0.5303933501243592, 'Validation Loss (total)': 1.0872353871663412, 'TimeToTrain': 0.2134666601816813}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027139680167991994,
 'learning_rate_Hydroxylation-K': 0.00016453346770345747,
 'learning_rate_Hydroxylation-P': 0.006998247726840531,
 'log_base': 2.3396684209475977,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2952741583,
 'sample_weights': [49.46366249242035, 6.170110717626995],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9007394172605396,
 'weight_decay_Hydroxylation-K': 8.679592963275098,
 'weight_decay_Hydroxylation-P': 4.938163826352299}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.825
[2,     1] loss: 1320.816
[3,     1] loss: 1324.951
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009286850108040044,
 'learning_rate_Hydroxylation-K': 0.0030313734807713767,
 'learning_rate_Hydroxylation-P': 0.002687386754549229,
 'log_base': 1.1162604017939197,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1000742896,
 'sample_weights': [1.9640294603210864, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.380213402584579,
 'weight_decay_Hydroxylation-K': 2.897510582522539,
 'weight_decay_Hydroxylation-P': 1.6811520425166382}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4928.325
[2,     1] loss: 4931.603
[3,     1] loss: 4932.802
[4,     1] loss: 4924.930
[5,     1] loss: 4917.777
[6,     1] loss: 4926.232
[7,     1] loss: 4921.783
[8,     1] loss: 4925.890
[9,     1] loss: 4926.595
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029671760038410913,
 'learning_rate_Hydroxylation-K': 0.0015298351156087066,
 'learning_rate_Hydroxylation-P': 0.004815955408303842,
 'log_base': 1.018409382482818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 149214210,
 'sample_weights': [15.178940041095146, 1.8974405993523151],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.282689096737455,
 'weight_decay_Hydroxylation-K': 3.467490403462446,
 'weight_decay_Hydroxylation-P': 2.2300415612179867}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29778.547
Exploding loss, terminate run (best metric=1.096221923828125)
Finished Training
Total time taken: 0.21800017356872559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29797.402
Exploding loss, terminate run (best metric=1.0914876461029053)
Finished Training
Total time taken: 0.23200297355651855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29696.084
Exploding loss, terminate run (best metric=1.0916759967803955)
Finished Training
Total time taken: 0.21900033950805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29783.951
Exploding loss, terminate run (best metric=1.0854341983795166)
Finished Training
Total time taken: 0.20400118827819824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29760.871
Exploding loss, terminate run (best metric=1.072035312652588)
Finished Training
Total time taken: 0.21499919891357422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29816.586
Exploding loss, terminate run (best metric=1.1007378101348877)
Finished Training
Total time taken: 0.2219984531402588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29711.547
Exploding loss, terminate run (best metric=1.094442367553711)
Finished Training
Total time taken: 0.19899868965148926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29794.746
Exploding loss, terminate run (best metric=1.0904860496520996)
Finished Training
Total time taken: 0.21899867057800293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29770.230
Exploding loss, terminate run (best metric=1.0745669603347778)
Finished Training
Total time taken: 0.21899986267089844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29608.270
Exploding loss, terminate run (best metric=1.0722100734710693)
Finished Training
Total time taken: 0.20100021362304688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29550.729
Exploding loss, terminate run (best metric=1.1043956279754639)
Finished Training
Total time taken: 0.2370011806488037
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29817.051
Exploding loss, terminate run (best metric=1.097214937210083)
Finished Training
Total time taken: 0.2389969825744629
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29663.002
Exploding loss, terminate run (best metric=1.0911785364151)
Finished Training
Total time taken: 0.22299933433532715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30054.887
Exploding loss, terminate run (best metric=1.0754847526550293)
Finished Training
Total time taken: 0.20300006866455078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29796.922
Exploding loss, terminate run (best metric=1.072890281677246)
Finished Training
Total time taken: 0.2239990234375
{'Hydroxylation-K Validation Accuracy': 0.4736111111111111, 'Hydroxylation-K Validation Sensitivity': 0.5733333333333334, 'Hydroxylation-K Validation Specificity': 0.4473684210526316, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6525925925925926, 'Hydroxylation-K AUC PR': 0.389956635630048, 'Hydroxylation-K MCC': 0.02191954629732649, 'Hydroxylation-K F1': 0.20567423709295632, 'Validation Loss (Hydroxylation-K)': 0.5567031621932983, 'Hydroxylation-P Validation Accuracy': 0.4721022790721283, 'Hydroxylation-P Validation Sensitivity': 0.5582010582010583, 'Hydroxylation-P Validation Specificity': 0.4532520325203252, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6186552866112944, 'Hydroxylation-P AUC PR': 0.2957367857976302, 'Hydroxylation-P MCC': 0.011012468632075697, 'Hydroxylation-P F1': 0.18102858805313055, 'Validation Loss (Hydroxylation-P)': 0.5306610067685446, 'Validation Loss (total)': 1.0873641649881998, 'TimeToTrain': 0.21826642354329426}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008875096642212909,
 'learning_rate_Hydroxylation-K': 0.009958013310599147,
 'learning_rate_Hydroxylation-P': 0.006204941248104957,
 'log_base': 1.511707495692812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1998171130,
 'sample_weights': [91.58443487713305, 11.424267325328007],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5170597414835574,
 'weight_decay_Hydroxylation-K': 4.821443066501354,
 'weight_decay_Hydroxylation-P': 0.27290432164545336}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1763.840
[2,     1] loss: 1759.084
[3,     1] loss: 1762.894
[4,     1] loss: 1761.648
[5,     1] loss: 1763.544
[6,     1] loss: 1763.077
[7,     1] loss: 1755.140
[8,     1] loss: 1761.604
[9,     1] loss: 1764.625
[10,     1] loss: 1754.002
[11,     1] loss: 1756.234
[12,     1] loss: 1753.118
[13,     1] loss: 1761.789
[14,     1] loss: 1756.488
[15,     1] loss: 1746.827
[16,     1] loss: 1741.836
[17,     1] loss: 1733.208
[18,     1] loss: 1728.282
[19,     1] loss: 1723.297
[20,     1] loss: 1675.986
[21,     1] loss: 1679.882
[22,     1] loss: 1685.609
[23,     1] loss: 1650.621
[24,     1] loss: 1640.062
[25,     1] loss: 1561.228
[26,     1] loss: 1575.942
[27,     1] loss: 1529.852
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003803123338326302,
 'learning_rate_Hydroxylation-K': 0.0041824279139883866,
 'learning_rate_Hydroxylation-P': 0.003306777613537331,
 'log_base': 1.9886858799714724,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3084362506,
 'sample_weights': [4.039889507303527, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.065867796109113,
 'weight_decay_Hydroxylation-K': 9.122989504450775,
 'weight_decay_Hydroxylation-P': 6.619748997701974}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1431.069
[2,     1] loss: 1416.788
[3,     1] loss: 1421.725
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003334944945048701,
 'learning_rate_Hydroxylation-K': 0.007640045316452526,
 'learning_rate_Hydroxylation-P': 0.008305254439268422,
 'log_base': 2.355588302333924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4148314067,
 'sample_weights': [2.4283725707277832, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.886083246996853,
 'weight_decay_Hydroxylation-K': 3.8205574981820263,
 'weight_decay_Hydroxylation-P': 2.7783916803745896}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1319.306
[2,     1] loss: 1318.484
[3,     1] loss: 1319.644
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004315340853573135,
 'learning_rate_Hydroxylation-K': 0.0004957643131177912,
 'learning_rate_Hydroxylation-P': 0.004112457522746057,
 'log_base': 1.8978471031078157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1650417733,
 'sample_weights': [1.948484647511082, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.180501525360529,
 'weight_decay_Hydroxylation-K': 2.0913944323697935,
 'weight_decay_Hydroxylation-P': 0.2916097196654908}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.746
[2,     1] loss: 1460.017
[3,     1] loss: 1457.457
[4,     1] loss: 1457.042
[5,     1] loss: 1454.997
[6,     1] loss: 1456.007
[7,     1] loss: 1457.254
[8,     1] loss: 1452.603
[9,     1] loss: 1448.953
[10,     1] loss: 1446.140
[11,     1] loss: 1436.145
[12,     1] loss: 1431.595
[13,     1] loss: 1408.714
[14,     1] loss: 1381.023
[15,     1] loss: 1360.953
[16,     1] loss: 1322.136
[17,     1] loss: 1294.581
[18,     1] loss: 1251.370
[19,     1] loss: 1212.012
[20,     1] loss: 1227.002
[21,     1] loss: 1261.930
[22,     1] loss: 1234.450
[23,     1] loss: 1175.835
[24,     1] loss: 1202.909
[25,     1] loss: 1180.024
[26,     1] loss: 1180.618
[27,     1] loss: 1161.979
[28,     1] loss: 1137.771
[29,     1] loss: 1232.391
[30,     1] loss: 1136.078
[31,     1] loss: 1124.944
[32,     1] loss: 1087.789
[33,     1] loss: 1103.318
[34,     1] loss: 1048.472
[35,     1] loss: 1052.509
[36,     1] loss: 1076.325
[37,     1] loss: 1082.231
[38,     1] loss: 997.946
[39,     1] loss: 1021.293
[40,     1] loss: 994.211
[41,     1] loss: 1029.228
[42,     1] loss: 1064.626
[43,     1] loss: 1092.495
[44,     1] loss: 970.019
[45,     1] loss: 1038.307
[46,     1] loss: 1032.635
[47,     1] loss: 1085.708
[48,     1] loss: 995.749
[49,     1] loss: 959.262
[50,     1] loss: 949.712
[51,     1] loss: 957.354
[52,     1] loss: 935.219
[53,     1] loss: 932.512
[54,     1] loss: 871.902
[55,     1] loss: 812.255
[56,     1] loss: 936.654
[57,     1] loss: 1158.823
[58,     1] loss: 865.458
[59,     1] loss: 1076.355
[60,     1] loss: 877.148
[61,     1] loss: 1023.593
[62,     1] loss: 1002.765
[63,     1] loss: 925.751
Early stopping applied (best metric=0.7051060795783997)
Finished Training
Total time taken: 8.523008108139038
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1456.733
[2,     1] loss: 1453.697
[3,     1] loss: 1462.718
[4,     1] loss: 1455.748
[5,     1] loss: 1449.882
[6,     1] loss: 1451.252
[7,     1] loss: 1445.170
[8,     1] loss: 1428.375
[9,     1] loss: 1403.782
[10,     1] loss: 1374.101
[11,     1] loss: 1324.157
[12,     1] loss: 1291.190
[13,     1] loss: 1232.598
[14,     1] loss: 1250.765
[15,     1] loss: 1166.493
[16,     1] loss: 1196.314
[17,     1] loss: 1213.076
[18,     1] loss: 1149.934
[19,     1] loss: 1220.485
[20,     1] loss: 1126.140
[21,     1] loss: 1169.156
[22,     1] loss: 1168.806
[23,     1] loss: 1151.804
[24,     1] loss: 1116.823
[25,     1] loss: 1102.747
[26,     1] loss: 1094.500
[27,     1] loss: 1095.834
[28,     1] loss: 1068.924
[29,     1] loss: 1031.240
[30,     1] loss: 1064.621
[31,     1] loss: 1003.068
[32,     1] loss: 1061.282
[33,     1] loss: 1060.368
[34,     1] loss: 957.996
[35,     1] loss: 1049.455
[36,     1] loss: 991.732
[37,     1] loss: 936.229
[38,     1] loss: 975.190
[39,     1] loss: 959.728
[40,     1] loss: 991.704
[41,     1] loss: 917.661
[42,     1] loss: 991.788
[43,     1] loss: 924.496
[44,     1] loss: 875.747
[45,     1] loss: 894.022
[46,     1] loss: 863.672
[47,     1] loss: 843.554
[48,     1] loss: 845.375
[49,     1] loss: 846.129
[50,     1] loss: 801.932
[51,     1] loss: 799.720
[52,     1] loss: 788.510
[53,     1] loss: 827.960
Early stopping applied (best metric=0.9084429144859314)
Finished Training
Total time taken: 8.89900827407837
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1457.960
[2,     1] loss: 1457.441
[3,     1] loss: 1457.952
[4,     1] loss: 1459.836
[5,     1] loss: 1454.368
[6,     1] loss: 1457.813
[7,     1] loss: 1450.708
[8,     1] loss: 1446.005
[9,     1] loss: 1433.990
[10,     1] loss: 1416.472
[11,     1] loss: 1377.050
[12,     1] loss: 1332.035
[13,     1] loss: 1292.796
[14,     1] loss: 1245.727
[15,     1] loss: 1240.771
[16,     1] loss: 1221.356
[17,     1] loss: 1194.634
[18,     1] loss: 1179.447
[19,     1] loss: 1201.304
[20,     1] loss: 1155.050
[21,     1] loss: 1145.596
[22,     1] loss: 1112.454
[23,     1] loss: 1152.028
[24,     1] loss: 1132.429
[25,     1] loss: 1105.903
[26,     1] loss: 1047.500
[27,     1] loss: 1101.747
[28,     1] loss: 1110.202
[29,     1] loss: 1083.155
[30,     1] loss: 1056.491
[31,     1] loss: 1053.334
[32,     1] loss: 998.809
[33,     1] loss: 1052.586
[34,     1] loss: 992.579
[35,     1] loss: 996.143
[36,     1] loss: 944.902
[37,     1] loss: 939.628
[38,     1] loss: 1018.248
[39,     1] loss: 1064.139
[40,     1] loss: 926.698
[41,     1] loss: 1008.495
[42,     1] loss: 983.061
[43,     1] loss: 1014.513
[44,     1] loss: 873.328
[45,     1] loss: 1021.823
[46,     1] loss: 900.662
[47,     1] loss: 945.634
[48,     1] loss: 880.653
[49,     1] loss: 905.901
[50,     1] loss: 853.231
[51,     1] loss: 783.469
[52,     1] loss: 827.760
Early stopping applied (best metric=0.8670753836631775)
Finished Training
Total time taken: 8.480008363723755
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1458.576
[2,     1] loss: 1456.534
[3,     1] loss: 1459.725
[4,     1] loss: 1459.875
[5,     1] loss: 1450.489
[6,     1] loss: 1446.559
[7,     1] loss: 1445.807
[8,     1] loss: 1424.757
[9,     1] loss: 1399.673
[10,     1] loss: 1367.652
[11,     1] loss: 1332.105
[12,     1] loss: 1270.647
[13,     1] loss: 1261.373
[14,     1] loss: 1270.630
[15,     1] loss: 1288.439
[16,     1] loss: 1230.604
[17,     1] loss: 1225.896
[18,     1] loss: 1200.160
[19,     1] loss: 1221.743
[20,     1] loss: 1154.815
[21,     1] loss: 1181.091
[22,     1] loss: 1156.565
[23,     1] loss: 1156.088
[24,     1] loss: 1162.147
[25,     1] loss: 1109.322
[26,     1] loss: 1113.184
[27,     1] loss: 1164.150
[28,     1] loss: 1173.915
[29,     1] loss: 1059.391
[30,     1] loss: 1161.931
[31,     1] loss: 1111.074
[32,     1] loss: 1089.888
[33,     1] loss: 1075.937
[34,     1] loss: 1118.174
[35,     1] loss: 1013.399
[36,     1] loss: 1111.805
[37,     1] loss: 1045.902
[38,     1] loss: 987.071
[39,     1] loss: 1030.355
[40,     1] loss: 988.132
[41,     1] loss: 1016.966
[42,     1] loss: 974.302
[43,     1] loss: 940.332
[44,     1] loss: 1062.634
[45,     1] loss: 990.781
[46,     1] loss: 1049.988
[47,     1] loss: 1016.379
[48,     1] loss: 960.810
[49,     1] loss: 1003.019
[50,     1] loss: 959.343
[51,     1] loss: 918.017
[52,     1] loss: 886.044
[53,     1] loss: 890.116
[54,     1] loss: 818.325
[55,     1] loss: 816.910
[56,     1] loss: 911.569
[57,     1] loss: 1145.430
[58,     1] loss: 983.077
[59,     1] loss: 873.160
[60,     1] loss: 991.007
[61,     1] loss: 902.712
[62,     1] loss: 984.320
[63,     1] loss: 871.910
[64,     1] loss: 825.279
[65,     1] loss: 856.163
[66,     1] loss: 825.999
[67,     1] loss: 954.119
[68,     1] loss: 909.738
[69,     1] loss: 786.655
[70,     1] loss: 828.302
[71,     1] loss: 772.175
[72,     1] loss: 774.227
[73,     1] loss: 742.969
[74,     1] loss: 922.020
[75,     1] loss: 814.300
[76,     1] loss: 746.112
[77,     1] loss: 769.333
[78,     1] loss: 708.813
[79,     1] loss: 652.883
[80,     1] loss: 717.501
[81,     1] loss: 740.797
[82,     1] loss: 730.633
[83,     1] loss: 701.861
[84,     1] loss: 621.241
[85,     1] loss: 613.565
[86,     1] loss: 652.010
[87,     1] loss: 596.947
[88,     1] loss: 619.937
Early stopping applied (best metric=0.7306368350982666)
Finished Training
Total time taken: 14.565016746520996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1458.883
[2,     1] loss: 1460.001
[3,     1] loss: 1458.146
[4,     1] loss: 1458.514
[5,     1] loss: 1452.128
[6,     1] loss: 1456.917
[7,     1] loss: 1441.583
[8,     1] loss: 1431.457
[9,     1] loss: 1403.333
[10,     1] loss: 1367.188
[11,     1] loss: 1308.915
[12,     1] loss: 1303.031
[13,     1] loss: 1251.689
[14,     1] loss: 1231.173
[15,     1] loss: 1262.241
[16,     1] loss: 1198.526
[17,     1] loss: 1278.698
[18,     1] loss: 1197.043
[19,     1] loss: 1266.939
[20,     1] loss: 1147.740
[21,     1] loss: 1248.542
[22,     1] loss: 1190.427
[23,     1] loss: 1116.024
[24,     1] loss: 1138.583
[25,     1] loss: 1111.635
[26,     1] loss: 1122.215
[27,     1] loss: 1163.701
[28,     1] loss: 1087.006
[29,     1] loss: 1076.065
[30,     1] loss: 1019.527
[31,     1] loss: 1025.144
[32,     1] loss: 1089.890
[33,     1] loss: 1042.753
[34,     1] loss: 1019.902
[35,     1] loss: 1035.086
[36,     1] loss: 1062.929
[37,     1] loss: 943.136
[38,     1] loss: 953.590
[39,     1] loss: 1034.498
[40,     1] loss: 992.878
[41,     1] loss: 984.181
[42,     1] loss: 961.035
[43,     1] loss: 1053.692
[44,     1] loss: 877.052
[45,     1] loss: 985.680
[46,     1] loss: 913.494
[47,     1] loss: 981.254
[48,     1] loss: 1011.214
[49,     1] loss: 843.494
[50,     1] loss: 949.521
[51,     1] loss: 888.186
[52,     1] loss: 877.658
[53,     1] loss: 892.347
[54,     1] loss: 820.362
[55,     1] loss: 919.998
[56,     1] loss: 835.009
[57,     1] loss: 902.964
[58,     1] loss: 804.147
[59,     1] loss: 811.866
[60,     1] loss: 810.085
[61,     1] loss: 803.302
[62,     1] loss: 759.794
[63,     1] loss: 833.715
[64,     1] loss: 931.697
[65,     1] loss: 836.560
[66,     1] loss: 726.838
[67,     1] loss: 830.247
[68,     1] loss: 733.180
Early stopping applied (best metric=0.7847919464111328)
Finished Training
Total time taken: 11.334009885787964
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1456.920
[2,     1] loss: 1458.120
[3,     1] loss: 1459.234
[4,     1] loss: 1458.884
[5,     1] loss: 1452.548
[6,     1] loss: 1461.272
[7,     1] loss: 1458.109
[8,     1] loss: 1446.654
[9,     1] loss: 1443.552
[10,     1] loss: 1443.102
[11,     1] loss: 1416.249
[12,     1] loss: 1406.418
[13,     1] loss: 1379.855
[14,     1] loss: 1339.650
[15,     1] loss: 1319.133
[16,     1] loss: 1283.910
[17,     1] loss: 1243.349
[18,     1] loss: 1256.813
[19,     1] loss: 1224.355
[20,     1] loss: 1177.224
[21,     1] loss: 1236.452
[22,     1] loss: 1243.455
[23,     1] loss: 1151.302
[24,     1] loss: 1202.256
[25,     1] loss: 1154.931
[26,     1] loss: 1180.610
[27,     1] loss: 1159.091
[28,     1] loss: 1162.232
[29,     1] loss: 1100.018
[30,     1] loss: 1153.004
[31,     1] loss: 1143.122
[32,     1] loss: 1121.055
[33,     1] loss: 1109.360
[34,     1] loss: 1051.065
[35,     1] loss: 1018.825
[36,     1] loss: 1062.805
[37,     1] loss: 1039.334
[38,     1] loss: 994.186
[39,     1] loss: 1244.642
[40,     1] loss: 1198.968
[41,     1] loss: 989.578
[42,     1] loss: 1009.506
[43,     1] loss: 1060.527
[44,     1] loss: 1032.646
[45,     1] loss: 1033.440
[46,     1] loss: 1035.466
[47,     1] loss: 988.167
[48,     1] loss: 1013.252
[49,     1] loss: 1004.949
[50,     1] loss: 976.087
[51,     1] loss: 1003.869
[52,     1] loss: 922.275
[53,     1] loss: 935.774
[54,     1] loss: 935.241
[55,     1] loss: 947.116
Early stopping applied (best metric=0.9365161061286926)
Finished Training
Total time taken: 7.514007806777954
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1461.192
[2,     1] loss: 1454.140
[3,     1] loss: 1458.606
[4,     1] loss: 1462.128
[5,     1] loss: 1453.956
[6,     1] loss: 1456.543
[7,     1] loss: 1453.229
[8,     1] loss: 1449.556
[9,     1] loss: 1444.677
[10,     1] loss: 1434.810
[11,     1] loss: 1405.537
[12,     1] loss: 1377.472
[13,     1] loss: 1340.270
[14,     1] loss: 1311.826
[15,     1] loss: 1266.275
[16,     1] loss: 1259.610
[17,     1] loss: 1236.207
[18,     1] loss: 1272.613
[19,     1] loss: 1280.683
[20,     1] loss: 1201.409
[21,     1] loss: 1164.799
[22,     1] loss: 1188.887
[23,     1] loss: 1216.809
[24,     1] loss: 1196.885
[25,     1] loss: 1191.203
[26,     1] loss: 1167.068
[27,     1] loss: 1149.094
[28,     1] loss: 1084.191
[29,     1] loss: 1101.557
[30,     1] loss: 1108.006
[31,     1] loss: 1184.643
[32,     1] loss: 1111.431
[33,     1] loss: 1044.934
[34,     1] loss: 1082.812
[35,     1] loss: 1079.716
[36,     1] loss: 1049.918
[37,     1] loss: 1004.668
[38,     1] loss: 1011.931
[39,     1] loss: 1054.780
[40,     1] loss: 995.900
[41,     1] loss: 1024.734
[42,     1] loss: 1126.973
[43,     1] loss: 934.057
[44,     1] loss: 992.787
[45,     1] loss: 956.926
[46,     1] loss: 943.656
[47,     1] loss: 923.171
[48,     1] loss: 863.360
[49,     1] loss: 876.800
[50,     1] loss: 874.523
[51,     1] loss: 1100.105
[52,     1] loss: 1167.922
[53,     1] loss: 844.382
[54,     1] loss: 963.242
[55,     1] loss: 864.696
[56,     1] loss: 936.910
[57,     1] loss: 863.931
[58,     1] loss: 845.872
[59,     1] loss: 895.841
[60,     1] loss: 802.852
[61,     1] loss: 797.542
[62,     1] loss: 864.730
[63,     1] loss: 802.369
[64,     1] loss: 786.938
Early stopping applied (best metric=0.741680383682251)
Finished Training
Total time taken: 10.686010599136353
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1460.370
[2,     1] loss: 1456.990
[3,     1] loss: 1458.821
[4,     1] loss: 1459.059
[5,     1] loss: 1456.830
[6,     1] loss: 1456.945
[7,     1] loss: 1449.862
[8,     1] loss: 1449.967
[9,     1] loss: 1444.688
[10,     1] loss: 1434.388
[11,     1] loss: 1413.042
[12,     1] loss: 1386.415
[13,     1] loss: 1352.059
[14,     1] loss: 1332.876
[15,     1] loss: 1292.588
[16,     1] loss: 1252.601
[17,     1] loss: 1226.439
[18,     1] loss: 1180.560
[19,     1] loss: 1225.908
[20,     1] loss: 1283.714
[21,     1] loss: 1186.767
[22,     1] loss: 1207.885
[23,     1] loss: 1158.473
[24,     1] loss: 1160.448
[25,     1] loss: 1160.782
[26,     1] loss: 1137.072
[27,     1] loss: 1178.086
[28,     1] loss: 1123.905
[29,     1] loss: 1078.538
[30,     1] loss: 1075.516
[31,     1] loss: 1084.720
[32,     1] loss: 1070.801
[33,     1] loss: 1042.635
[34,     1] loss: 1084.423
[35,     1] loss: 1154.664
[36,     1] loss: 1037.936
[37,     1] loss: 1068.370
[38,     1] loss: 985.759
[39,     1] loss: 1011.920
[40,     1] loss: 1052.256
[41,     1] loss: 1004.885
[42,     1] loss: 1047.502
[43,     1] loss: 948.468
[44,     1] loss: 981.102
[45,     1] loss: 930.180
[46,     1] loss: 977.247
[47,     1] loss: 1043.306
[48,     1] loss: 918.414
[49,     1] loss: 871.740
[50,     1] loss: 959.246
[51,     1] loss: 862.151
[52,     1] loss: 842.199
[53,     1] loss: 935.151
[54,     1] loss: 793.288
[55,     1] loss: 804.230
[56,     1] loss: 880.868
[57,     1] loss: 807.917
[58,     1] loss: 825.967
[59,     1] loss: 960.964
[60,     1] loss: 1266.808
[61,     1] loss: 937.187
[62,     1] loss: 991.161
[63,     1] loss: 910.489
[64,     1] loss: 943.079
[65,     1] loss: 993.656
[66,     1] loss: 912.594
[67,     1] loss: 883.266
[68,     1] loss: 910.262
[69,     1] loss: 848.029
[70,     1] loss: 825.641
[71,     1] loss: 828.757
[72,     1] loss: 779.462
Early stopping applied (best metric=0.836431622505188)
Finished Training
Total time taken: 9.815009593963623
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1459.031
[2,     1] loss: 1456.456
[3,     1] loss: 1453.488
[4,     1] loss: 1458.899
[5,     1] loss: 1458.023
[6,     1] loss: 1449.639
[7,     1] loss: 1449.213
[8,     1] loss: 1433.389
[9,     1] loss: 1418.154
[10,     1] loss: 1383.474
[11,     1] loss: 1335.591
[12,     1] loss: 1307.232
[13,     1] loss: 1280.807
[14,     1] loss: 1243.258
[15,     1] loss: 1227.023
[16,     1] loss: 1179.321
[17,     1] loss: 1199.506
[18,     1] loss: 1205.394
[19,     1] loss: 1184.050
[20,     1] loss: 1149.889
[21,     1] loss: 1202.354
[22,     1] loss: 1093.890
[23,     1] loss: 1169.586
[24,     1] loss: 1133.387
[25,     1] loss: 1103.774
[26,     1] loss: 1119.045
[27,     1] loss: 1042.977
[28,     1] loss: 1102.941
[29,     1] loss: 1052.826
[30,     1] loss: 1055.339
[31,     1] loss: 1030.650
[32,     1] loss: 1079.711
[33,     1] loss: 1016.090
[34,     1] loss: 1078.713
[35,     1] loss: 1032.897
[36,     1] loss: 953.242
[37,     1] loss: 982.481
[38,     1] loss: 955.568
[39,     1] loss: 948.562
[40,     1] loss: 868.005
[41,     1] loss: 972.196
[42,     1] loss: 966.072
[43,     1] loss: 947.183
[44,     1] loss: 918.703
[45,     1] loss: 866.556
[46,     1] loss: 870.008
[47,     1] loss: 824.517
[48,     1] loss: 988.025
[49,     1] loss: 1001.975
[50,     1] loss: 873.825
[51,     1] loss: 1029.005
[52,     1] loss: 933.192
[53,     1] loss: 867.344
[54,     1] loss: 865.934
[55,     1] loss: 849.829
Early stopping applied (best metric=0.8735859990119934)
Finished Training
Total time taken: 9.185009002685547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1458.054
[2,     1] loss: 1460.694
[3,     1] loss: 1461.496
[4,     1] loss: 1459.000
[5,     1] loss: 1454.594
[6,     1] loss: 1455.048
[7,     1] loss: 1444.015
[8,     1] loss: 1438.372
[9,     1] loss: 1421.995
[10,     1] loss: 1390.066
[11,     1] loss: 1370.576
[12,     1] loss: 1328.579
[13,     1] loss: 1278.109
[14,     1] loss: 1286.109
[15,     1] loss: 1269.788
[16,     1] loss: 1270.109
[17,     1] loss: 1266.213
[18,     1] loss: 1189.585
[19,     1] loss: 1208.202
[20,     1] loss: 1173.391
[21,     1] loss: 1155.958
[22,     1] loss: 1175.150
[23,     1] loss: 1148.524
[24,     1] loss: 1223.559
[25,     1] loss: 1121.005
[26,     1] loss: 1150.388
[27,     1] loss: 1123.764
[28,     1] loss: 1105.111
[29,     1] loss: 1083.877
[30,     1] loss: 1076.787
[31,     1] loss: 1056.110
[32,     1] loss: 1085.778
[33,     1] loss: 1059.192
[34,     1] loss: 1116.097
[35,     1] loss: 1169.306
[36,     1] loss: 1068.257
[37,     1] loss: 1141.347
[38,     1] loss: 1065.055
[39,     1] loss: 1097.959
[40,     1] loss: 1005.326
[41,     1] loss: 1088.525
[42,     1] loss: 1009.927
[43,     1] loss: 968.992
[44,     1] loss: 1009.589
[45,     1] loss: 994.793
[46,     1] loss: 965.386
[47,     1] loss: 984.434
[48,     1] loss: 919.162
[49,     1] loss: 948.275
[50,     1] loss: 944.233
[51,     1] loss: 915.874
[52,     1] loss: 1001.612
[53,     1] loss: 842.800
[54,     1] loss: 954.253
[55,     1] loss: 941.170
[56,     1] loss: 870.430
[57,     1] loss: 993.614
[58,     1] loss: 905.599
[59,     1] loss: 958.562
[60,     1] loss: 856.619
[61,     1] loss: 869.986
[62,     1] loss: 844.856
[63,     1] loss: 933.024
[64,     1] loss: 878.027
[65,     1] loss: 807.545
Early stopping applied (best metric=0.7740437388420105)
Finished Training
Total time taken: 10.780010461807251
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1455.783
[2,     1] loss: 1459.938
[3,     1] loss: 1464.918
[4,     1] loss: 1456.860
[5,     1] loss: 1455.701
[6,     1] loss: 1453.388
[7,     1] loss: 1451.681
[8,     1] loss: 1450.156
[9,     1] loss: 1441.259
[10,     1] loss: 1433.568
[11,     1] loss: 1411.284
[12,     1] loss: 1375.123
[13,     1] loss: 1355.368
[14,     1] loss: 1308.524
[15,     1] loss: 1298.706
[16,     1] loss: 1267.417
[17,     1] loss: 1252.038
[18,     1] loss: 1219.839
[19,     1] loss: 1211.798
[20,     1] loss: 1251.489
[21,     1] loss: 1238.079
[22,     1] loss: 1178.986
[23,     1] loss: 1206.946
[24,     1] loss: 1199.889
[25,     1] loss: 1146.851
[26,     1] loss: 1139.934
[27,     1] loss: 1130.250
[28,     1] loss: 1128.221
[29,     1] loss: 1161.098
[30,     1] loss: 1064.255
[31,     1] loss: 1108.292
[32,     1] loss: 1086.433
[33,     1] loss: 1074.966
[34,     1] loss: 1125.281
[35,     1] loss: 1059.866
[36,     1] loss: 1121.556
[37,     1] loss: 1048.919
[38,     1] loss: 1020.080
[39,     1] loss: 1096.289
[40,     1] loss: 975.069
[41,     1] loss: 1028.768
[42,     1] loss: 1047.466
[43,     1] loss: 985.325
[44,     1] loss: 1021.609
[45,     1] loss: 912.443
[46,     1] loss: 952.682
[47,     1] loss: 1015.475
[48,     1] loss: 928.178
[49,     1] loss: 846.680
[50,     1] loss: 957.141
[51,     1] loss: 879.603
[52,     1] loss: 852.386
[53,     1] loss: 858.783
[54,     1] loss: 1021.583
[55,     1] loss: 1156.510
[56,     1] loss: 838.630
[57,     1] loss: 993.469
[58,     1] loss: 858.023
[59,     1] loss: 980.948
[60,     1] loss: 843.758
[61,     1] loss: 900.869
[62,     1] loss: 827.076
[63,     1] loss: 882.666
[64,     1] loss: 789.171
[65,     1] loss: 892.915
[66,     1] loss: 726.588
[67,     1] loss: 715.353
[68,     1] loss: 753.392
[69,     1] loss: 712.846
[70,     1] loss: 690.911
[71,     1] loss: 646.629
[72,     1] loss: 708.783
[73,     1] loss: 808.557
[74,     1] loss: 1108.403
[75,     1] loss: 685.240
Early stopping applied (best metric=0.6635545492172241)
Finished Training
Total time taken: 10.174012899398804
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1462.622
[2,     1] loss: 1455.838
[3,     1] loss: 1460.806
[4,     1] loss: 1455.842
[5,     1] loss: 1457.919
[6,     1] loss: 1451.704
[7,     1] loss: 1447.748
[8,     1] loss: 1445.348
[9,     1] loss: 1434.466
[10,     1] loss: 1417.816
[11,     1] loss: 1389.681
[12,     1] loss: 1360.392
[13,     1] loss: 1310.406
[14,     1] loss: 1282.812
[15,     1] loss: 1252.952
[16,     1] loss: 1251.138
[17,     1] loss: 1224.420
[18,     1] loss: 1244.883
[19,     1] loss: 1183.966
[20,     1] loss: 1131.089
[21,     1] loss: 1218.198
[22,     1] loss: 1142.592
[23,     1] loss: 1193.675
[24,     1] loss: 1110.064
[25,     1] loss: 1120.871
[26,     1] loss: 1093.096
[27,     1] loss: 1081.622
[28,     1] loss: 1124.489
[29,     1] loss: 1054.322
[30,     1] loss: 1091.121
[31,     1] loss: 1085.180
[32,     1] loss: 1041.523
[33,     1] loss: 1061.627
[34,     1] loss: 1047.871
[35,     1] loss: 1069.223
[36,     1] loss: 1087.330
[37,     1] loss: 968.326
[38,     1] loss: 979.826
[39,     1] loss: 999.298
[40,     1] loss: 1007.194
[41,     1] loss: 999.632
[42,     1] loss: 1002.267
[43,     1] loss: 916.625
[44,     1] loss: 903.443
[45,     1] loss: 997.411
[46,     1] loss: 1030.878
[47,     1] loss: 886.004
[48,     1] loss: 956.414
[49,     1] loss: 879.966
[50,     1] loss: 968.474
[51,     1] loss: 874.261
[52,     1] loss: 971.109
[53,     1] loss: 903.894
[54,     1] loss: 977.032
[55,     1] loss: 839.064
[56,     1] loss: 888.391
[57,     1] loss: 904.461
[58,     1] loss: 849.745
[59,     1] loss: 803.936
[60,     1] loss: 797.861
[61,     1] loss: 837.551
[62,     1] loss: 816.391
Early stopping applied (best metric=0.8821762800216675)
Finished Training
Total time taken: 8.73500680923462
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1455.294
[2,     1] loss: 1459.837
[3,     1] loss: 1456.314
[4,     1] loss: 1456.580
[5,     1] loss: 1455.697
[6,     1] loss: 1453.887
[7,     1] loss: 1452.367
[8,     1] loss: 1450.514
[9,     1] loss: 1444.167
[10,     1] loss: 1426.304
[11,     1] loss: 1418.523
[12,     1] loss: 1401.928
[13,     1] loss: 1347.568
[14,     1] loss: 1332.478
[15,     1] loss: 1309.543
[16,     1] loss: 1269.867
[17,     1] loss: 1206.480
[18,     1] loss: 1220.055
[19,     1] loss: 1212.242
[20,     1] loss: 1210.310
[21,     1] loss: 1159.270
[22,     1] loss: 1200.206
[23,     1] loss: 1135.866
[24,     1] loss: 1151.579
[25,     1] loss: 1097.200
[26,     1] loss: 1161.873
[27,     1] loss: 1101.057
[28,     1] loss: 1134.572
[29,     1] loss: 1112.188
[30,     1] loss: 1077.893
[31,     1] loss: 1108.066
[32,     1] loss: 1111.636
[33,     1] loss: 1114.756
[34,     1] loss: 1032.604
[35,     1] loss: 1149.972
[36,     1] loss: 1039.323
[37,     1] loss: 1064.223
[38,     1] loss: 1049.699
[39,     1] loss: 1079.833
[40,     1] loss: 1004.769
[41,     1] loss: 987.864
[42,     1] loss: 1021.849
[43,     1] loss: 1019.362
[44,     1] loss: 942.962
[45,     1] loss: 993.115
[46,     1] loss: 950.762
[47,     1] loss: 1003.270
[48,     1] loss: 1079.453
[49,     1] loss: 898.842
[50,     1] loss: 924.760
[51,     1] loss: 979.526
[52,     1] loss: 940.177
[53,     1] loss: 899.848
[54,     1] loss: 964.810
[55,     1] loss: 890.075
[56,     1] loss: 810.163
[57,     1] loss: 872.576
Early stopping applied (best metric=0.8108348846435547)
Finished Training
Total time taken: 9.36801266670227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1453.144
[2,     1] loss: 1455.334
[3,     1] loss: 1464.881
[4,     1] loss: 1452.293
[5,     1] loss: 1450.844
[6,     1] loss: 1453.570
[7,     1] loss: 1442.031
[8,     1] loss: 1430.501
[9,     1] loss: 1415.990
[10,     1] loss: 1382.223
[11,     1] loss: 1345.495
[12,     1] loss: 1307.041
[13,     1] loss: 1269.124
[14,     1] loss: 1250.941
[15,     1] loss: 1243.568
[16,     1] loss: 1213.674
[17,     1] loss: 1284.344
[18,     1] loss: 1194.504
[19,     1] loss: 1176.527
[20,     1] loss: 1177.437
[21,     1] loss: 1180.562
[22,     1] loss: 1198.957
[23,     1] loss: 1178.970
[24,     1] loss: 1151.265
[25,     1] loss: 1136.125
[26,     1] loss: 1160.569
[27,     1] loss: 1081.771
[28,     1] loss: 1060.981
[29,     1] loss: 1087.975
[30,     1] loss: 1091.060
[31,     1] loss: 1091.010
[32,     1] loss: 1099.031
[33,     1] loss: 1054.255
[34,     1] loss: 1053.982
[35,     1] loss: 1062.796
[36,     1] loss: 1026.210
[37,     1] loss: 1022.094
[38,     1] loss: 1045.082
[39,     1] loss: 1091.034
[40,     1] loss: 1320.677
[41,     1] loss: 993.711
[42,     1] loss: 1157.766
[43,     1] loss: 1037.589
[44,     1] loss: 1017.455
[45,     1] loss: 1089.435
[46,     1] loss: 1072.889
[47,     1] loss: 1047.999
[48,     1] loss: 1058.853
[49,     1] loss: 936.923
[50,     1] loss: 1003.153
[51,     1] loss: 965.849
[52,     1] loss: 987.716
[53,     1] loss: 964.163
[54,     1] loss: 855.452
[55,     1] loss: 926.992
[56,     1] loss: 885.932
[57,     1] loss: 882.720
[58,     1] loss: 850.490
[59,     1] loss: 829.943
[60,     1] loss: 825.679
Early stopping applied (best metric=0.7739806175231934)
Finished Training
Total time taken: 8.7690110206604
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1465.984
[2,     1] loss: 1457.714
[3,     1] loss: 1461.640
[4,     1] loss: 1460.637
[5,     1] loss: 1456.074
[6,     1] loss: 1454.495
[7,     1] loss: 1454.474
[8,     1] loss: 1453.948
[9,     1] loss: 1448.321
[10,     1] loss: 1436.583
[11,     1] loss: 1429.504
[12,     1] loss: 1406.114
[13,     1] loss: 1374.259
[14,     1] loss: 1334.080
[15,     1] loss: 1310.929
[16,     1] loss: 1264.324
[17,     1] loss: 1267.723
[18,     1] loss: 1246.233
[19,     1] loss: 1256.539
[20,     1] loss: 1207.230
[21,     1] loss: 1232.078
[22,     1] loss: 1211.069
[23,     1] loss: 1228.330
[24,     1] loss: 1215.901
[25,     1] loss: 1180.396
[26,     1] loss: 1177.906
[27,     1] loss: 1204.337
[28,     1] loss: 1121.094
[29,     1] loss: 1121.622
[30,     1] loss: 1121.326
[31,     1] loss: 1143.459
[32,     1] loss: 1072.525
[33,     1] loss: 1202.630
[34,     1] loss: 1060.103
[35,     1] loss: 1032.386
[36,     1] loss: 1067.403
[37,     1] loss: 1018.720
[38,     1] loss: 990.403
[39,     1] loss: 997.012
[40,     1] loss: 1033.642
[41,     1] loss: 1039.360
[42,     1] loss: 985.808
[43,     1] loss: 1057.577
[44,     1] loss: 971.305
[45,     1] loss: 984.107
[46,     1] loss: 1100.767
[47,     1] loss: 944.025
[48,     1] loss: 971.974
[49,     1] loss: 978.881
[50,     1] loss: 898.359
[51,     1] loss: 949.033
[52,     1] loss: 916.922
[53,     1] loss: 964.223
[54,     1] loss: 835.090
[55,     1] loss: 895.923
[56,     1] loss: 1015.177
[57,     1] loss: 880.806
[58,     1] loss: 912.484
[59,     1] loss: 836.183
[60,     1] loss: 945.506
[61,     1] loss: 860.389
[62,     1] loss: 881.023
[63,     1] loss: 814.421
[64,     1] loss: 840.986
[65,     1] loss: 814.390
[66,     1] loss: 780.941
[67,     1] loss: 715.433
[68,     1] loss: 699.250
[69,     1] loss: 762.939
[70,     1] loss: 915.010
[71,     1] loss: 1236.510
[72,     1] loss: 801.013
Early stopping applied (best metric=0.7743053436279297)
Finished Training
Total time taken: 12.075010776519775
{'Hydroxylation-K Validation Accuracy': 0.7883569739952718, 'Hydroxylation-K Validation Sensitivity': 0.6555555555555556, 'Hydroxylation-K Validation Specificity': 0.8228070175438597, 'Hydroxylation-K Validation Precision': 0.49255091399828244, 'Hydroxylation-K AUC ROC': 0.8091228070175439, 'Hydroxylation-K AUC PR': 0.611160996327569, 'Hydroxylation-K MCC': 0.43492105181291935, 'Hydroxylation-K F1': 0.5546314908923605, 'Validation Loss (Hydroxylation-K)': 0.4313578188419342, 'Hydroxylation-P Validation Accuracy': 0.7885486692722874, 'Hydroxylation-P Validation Sensitivity': 0.7766666666666667, 'Hydroxylation-P Validation Specificity': 0.7911591600578582, 'Hydroxylation-P Validation Precision': 0.4538498851482752, 'Hydroxylation-P AUC ROC': 0.8467855737569938, 'Hydroxylation-P AUC PR': 0.605333451039378, 'Hydroxylation-P MCC': 0.4742979611773858, 'Hydroxylation-P F1': 0.5686386340547329, 'Validation Loss (Hydroxylation-P)': 0.372853022813797, 'Validation Loss (total)': 0.8042108456293742, 'TimeToTrain': 9.926810201009115}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052638983527727515,
 'learning_rate_Hydroxylation-K': 0.008736427106579304,
 'learning_rate_Hydroxylation-P': 0.0057837086142530775,
 'log_base': 2.79245119958372,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1287466469,
 'sample_weights': [2.6075058877034283, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.95001807974068,
 'weight_decay_Hydroxylation-K': 4.893814456799516,
 'weight_decay_Hydroxylation-P': 1.136135421765394}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.795
[2,     1] loss: 1253.275
[3,     1] loss: 1252.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003013473656593777,
 'learning_rate_Hydroxylation-K': 0.004697434153012947,
 'learning_rate_Hydroxylation-P': 0.0036945139540993975,
 'log_base': 1.2230953503696111,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1778959650,
 'sample_weights': [1.625680200134897, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.872794998970097,
 'weight_decay_Hydroxylation-K': 4.327467734750981,
 'weight_decay_Hydroxylation-P': 2.103481158086356}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2709.271
[2,     1] loss: 2686.804
[3,     1] loss: 2680.648
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00947073430381753,
 'learning_rate_Hydroxylation-K': 0.0002658022322773889,
 'learning_rate_Hydroxylation-P': 0.00375451163603492,
 'log_base': 1.0455507161571027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4105748167,
 'sample_weights': [8.289816303320158, 1.0362669575416237],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.225326448987621,
 'weight_decay_Hydroxylation-K': 5.575550722258346,
 'weight_decay_Hydroxylation-P': 3.126473918070342}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12201.537
[2,     1] loss: 12139.364
[3,     1] loss: 12158.566
[4,     1] loss: 12160.721
[5,     1] loss: 12152.090
[6,     1] loss: 12204.146
[7,     1] loss: 12098.436
[8,     1] loss: 12129.769
[9,     1] loss: 12152.668
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032963724403650384,
 'learning_rate_Hydroxylation-K': 0.008107125539257708,
 'learning_rate_Hydroxylation-P': 0.0032882159558878703,
 'log_base': 1.0955687104628922,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3522547015,
 'sample_weights': [37.47873120998706, 4.685021880151743],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.521715600189932,
 'weight_decay_Hydroxylation-K': 2.768536857930499,
 'weight_decay_Hydroxylation-P': 1.24969958476363}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5934.490
[2,     1] loss: 5946.808
[3,     1] loss: 5905.750
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016086193371128519,
 'learning_rate_Hydroxylation-K': 0.005492516064305064,
 'learning_rate_Hydroxylation-P': 0.002057753906421248,
 'log_base': 1.506038444518255,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1130324982,
 'sample_weights': [18.2905371398366, 2.286405220596905],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.795883052288753,
 'weight_decay_Hydroxylation-K': 0.5789736255977509,
 'weight_decay_Hydroxylation-P': 0.21355772322709868}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1765.896
[2,     1] loss: 1763.712
[3,     1] loss: 1771.343
[4,     1] loss: 1766.015
[5,     1] loss: 1762.509
[6,     1] loss: 1759.630
[7,     1] loss: 1759.470
[8,     1] loss: 1756.574
[9,     1] loss: 1764.070
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002252457564218632,
 'learning_rate_Hydroxylation-K': 0.0008436624628824452,
 'learning_rate_Hydroxylation-P': 0.007466666090757748,
 'log_base': 1.0828899329640909,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 870883168,
 'sample_weights': [4.076956912417212, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.559010049818183,
 'weight_decay_Hydroxylation-K': 2.214302703782378,
 'weight_decay_Hydroxylation-P': 0.9255151496500567}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6793.998
[2,     1] loss: 6828.931
[3,     1] loss: 6818.008
[4,     1] loss: 6764.119
[5,     1] loss: 6841.609
[6,     1] loss: 6772.866
[7,     1] loss: 6768.195
[8,     1] loss: 6814.044
[9,     1] loss: 6782.488
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002323020414584486,
 'learning_rate_Hydroxylation-K': 0.0007358409449638952,
 'learning_rate_Hydroxylation-P': 0.00261026106960899,
 'log_base': 1.1270685143760237,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2939791798,
 'sample_weights': [20.96412546590958, 2.6206166360258347],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7269828055510885,
 'weight_decay_Hydroxylation-K': 3.92411675505368,
 'weight_decay_Hydroxylation-P': 1.5430960464419567}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4565.129
[2,     1] loss: 4528.216
[3,     1] loss: 4519.593
[4,     1] loss: 4525.426
[5,     1] loss: 4535.178
[6,     1] loss: 4531.583
[7,     1] loss: 4530.574
[8,     1] loss: 4520.999
[9,     1] loss: 4499.957
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008210705255827129,
 'learning_rate_Hydroxylation-K': 0.0040104901525343534,
 'learning_rate_Hydroxylation-P': 0.006865595422237238,
 'log_base': 1.6519787157058197,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4178076185,
 'sample_weights': [13.95621780136174, 1.7445944313643051],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7901146913175374,
 'weight_decay_Hydroxylation-K': 4.548935960367581,
 'weight_decay_Hydroxylation-P': 1.9954441042216442}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1629.987
[2,     1] loss: 1607.885
[3,     1] loss: 1632.435
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004303984112822615,
 'learning_rate_Hydroxylation-K': 0.003440908558649268,
 'learning_rate_Hydroxylation-P': 0.0010190643178016896,
 'log_base': 1.116277508908794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3610631208,
 'sample_weights': [3.325757593632126, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.588341028982407,
 'weight_decay_Hydroxylation-K': 1.8115433063821222,
 'weight_decay_Hydroxylation-P': 1.4388222430016033}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4932.263
[2,     1] loss: 4921.541
[3,     1] loss: 4928.022
[4,     1] loss: 4911.844
[5,     1] loss: 4915.204
[6,     1] loss: 4910.347
[7,     1] loss: 4930.935
[8,     1] loss: 4900.355
[9,     1] loss: 4895.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005755236073889616,
 'learning_rate_Hydroxylation-K': 0.0014655023064558468,
 'learning_rate_Hydroxylation-P': 0.006698475709119925,
 'log_base': 1.9028413002469016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 584058083,
 'sample_weights': [15.176825292858986, 1.897176245639221],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.1382980729593495,
 'weight_decay_Hydroxylation-K': 6.330400690280147,
 'weight_decay_Hydroxylation-P': 1.1271249482583392}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1457.485
[2,     1] loss: 1469.531
[3,     1] loss: 1461.191
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004311049962065706,
 'learning_rate_Hydroxylation-K': 0.0036360292445927122,
 'learning_rate_Hydroxylation-P': 0.0022376066127830323,
 'log_base': 2.4712664146672263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 205595309,
 'sample_weights': [2.5949294216203684, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.214273355474225,
 'weight_decay_Hydroxylation-K': 4.161049338184569,
 'weight_decay_Hydroxylation-P': 4.24628160867767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1308.446
[2,     1] loss: 1298.521
[3,     1] loss: 1296.644
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006357308317827732,
 'learning_rate_Hydroxylation-K': 0.007953753583377896,
 'learning_rate_Hydroxylation-P': 0.0008980598076305295,
 'log_base': 1.251335528301845,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3691660583,
 'sample_weights': [1.845237569363496, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.958960340925222,
 'weight_decay_Hydroxylation-K': 2.349599106715493,
 'weight_decay_Hydroxylation-P': 0.8520114848138726}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2475.683
[2,     1] loss: 2474.029
[3,     1] loss: 2489.437
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004301722393191837,
 'learning_rate_Hydroxylation-K': 0.007521923477652153,
 'learning_rate_Hydroxylation-P': 0.004240722842388362,
 'log_base': 2.497867283383165,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1205295706,
 'sample_weights': [7.445844058911777, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.992958439972648,
 'weight_decay_Hydroxylation-K': 6.577829935064685,
 'weight_decay_Hydroxylation-P': 2.734240112590702}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.718
[2,     1] loss: 1292.043
[3,     1] loss: 1292.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017621347176870635,
 'learning_rate_Hydroxylation-K': 0.0015887340857674076,
 'learning_rate_Hydroxylation-P': 0.003729882710838935,
 'log_base': 1.039706498082277,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3452976243,
 'sample_weights': [1.8236564992646866, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.404457860923154,
 'weight_decay_Hydroxylation-K': 4.029308260445051,
 'weight_decay_Hydroxylation-P': 2.8678164989637764}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13898.375
[2,     1] loss: 14013.618
[3,     1] loss: 13936.615
[4,     1] loss: 13930.059
[5,     1] loss: 13913.012
[6,     1] loss: 13878.261
[7,     1] loss: 13941.325
[8,     1] loss: 13968.068
[9,     1] loss: 13901.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006433971013368882,
 'learning_rate_Hydroxylation-K': 4.848221468737484e-05,
 'learning_rate_Hydroxylation-P': 0.0025081817289236844,
 'log_base': 1.408771988708885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3017623366,
 'sample_weights': [42.87388740967573, 5.359442385498047],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.424687197384955,
 'weight_decay_Hydroxylation-K': 3.448442582496828,
 'weight_decay_Hydroxylation-P': 2.8081683751764315}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1933.135
[2,     1] loss: 1940.049
[3,     1] loss: 1935.667
[4,     1] loss: 1939.156
[5,     1] loss: 1939.811
[6,     1] loss: 1937.781
[7,     1] loss: 1930.630
[8,     1] loss: 1933.320
[9,     1] loss: 1936.315
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028429401576392246,
 'learning_rate_Hydroxylation-K': 0.0025585617886339307,
 'learning_rate_Hydroxylation-P': 0.006011907953159241,
 'log_base': 1.0137098735128043,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 584761312,
 'sample_weights': [4.871180456568161, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.5815352942359775,
 'weight_decay_Hydroxylation-K': 5.411386675052039,
 'weight_decay_Hydroxylation-P': 1.940259408900316}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39895.543
Exploding loss, terminate run (best metric=1.0955955982208252)
Finished Training
Total time taken: 0.21300005912780762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39648.699
Exploding loss, terminate run (best metric=1.091748595237732)
Finished Training
Total time taken: 0.23200130462646484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39903.969
Exploding loss, terminate run (best metric=1.0911331176757812)
Finished Training
Total time taken: 0.21899890899658203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39803.949
Exploding loss, terminate run (best metric=1.0708388090133667)
Finished Training
Total time taken: 0.1979994773864746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40205.379
Exploding loss, terminate run (best metric=1.074251413345337)
Finished Training
Total time taken: 0.2219986915588379
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39600.684
Exploding loss, terminate run (best metric=1.111959457397461)
Finished Training
Total time taken: 0.2199997901916504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40052.141
Exploding loss, terminate run (best metric=1.0924687385559082)
Finished Training
Total time taken: 0.1999983787536621
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39807.617
Exploding loss, terminate run (best metric=1.0925822257995605)
Finished Training
Total time taken: 0.2240002155303955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39656.383
Exploding loss, terminate run (best metric=1.0726749897003174)
Finished Training
Total time taken: 0.2200000286102295
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40084.660
Exploding loss, terminate run (best metric=1.0752562284469604)
Finished Training
Total time taken: 0.20200252532958984
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39842.922
Exploding loss, terminate run (best metric=1.1025164127349854)
Finished Training
Total time taken: 0.23400068283081055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40029.363
Exploding loss, terminate run (best metric=1.0956926345825195)
Finished Training
Total time taken: 0.2259986400604248
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40164.637
Exploding loss, terminate run (best metric=1.0914291143417358)
Finished Training
Total time taken: 0.19900012016296387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 39944.672
Exploding loss, terminate run (best metric=1.072587251663208)
Finished Training
Total time taken: 0.21999883651733398
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40018.941
Exploding loss, terminate run (best metric=1.0743014812469482)
Finished Training
Total time taken: 0.24000024795532227
{'Hydroxylation-K Validation Accuracy': 0.400531914893617, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6553216374269006, 'Hydroxylation-K AUC PR': 0.3745680611271313, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22364532019704436, 'Validation Loss (Hydroxylation-K)': 0.5566224972407023, 'Hydroxylation-P Validation Accuracy': 0.3916258734751197, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6583066487106589, 'Hydroxylation-P AUC PR': 0.36785983394756105, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.19991175489829743, 'Validation Loss (Hydroxylation-P)': 0.5303798913955688, 'Validation Loss (total)': 1.087002404530843, 'TimeToTrain': 0.21793319384256998}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002168033219944032,
 'learning_rate_Hydroxylation-K': 0.006395976695194379,
 'learning_rate_Hydroxylation-P': 0.000765971090885413,
 'log_base': 1.0413571091559508,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1264866421,
 'sample_weights': [122.69317887549863, 15.304780515907916],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.034546305852407,
 'weight_decay_Hydroxylation-K': 1.2677044514685623,
 'weight_decay_Hydroxylation-P': 2.0846098357786484}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13396.590
[2,     1] loss: 13361.581
[3,     1] loss: 13336.113
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005059939633017516,
 'learning_rate_Hydroxylation-K': 0.0036908447741432152,
 'learning_rate_Hydroxylation-P': 0.007429695609869287,
 'log_base': 1.2296202075079337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3114484609,
 'sample_weights': [41.19561777860847, 5.1496505999039535],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.509137255807293,
 'weight_decay_Hydroxylation-K': 3.859639269983173,
 'weight_decay_Hydroxylation-P': 1.4369107341066476}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2617.506
[2,     1] loss: 2616.082
[3,     1] loss: 2622.028
[4,     1] loss: 2620.107
[5,     1] loss: 2623.083
[6,     1] loss: 2624.823
[7,     1] loss: 2621.143
[8,     1] loss: 2619.164
[9,     1] loss: 2613.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004913787896351713,
 'learning_rate_Hydroxylation-K': 0.0003355478839331505,
 'learning_rate_Hydroxylation-P': 0.003603908589598419,
 'log_base': 1.1345160798835945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3822632556,
 'sample_weights': [8.076439091050522, 1.0095937784895366],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.804018062401997,
 'weight_decay_Hydroxylation-K': 4.133330951309331,
 'weight_decay_Hydroxylation-P': 0.4777258640590185}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4310.930
[2,     1] loss: 4291.104
[3,     1] loss: 4304.295
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037608206989915606,
 'learning_rate_Hydroxylation-K': 0.0017943210652982596,
 'learning_rate_Hydroxylation-P': 0.005158282915756544,
 'log_base': 1.2651806532477117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 392164992,
 'sample_weights': [13.227901362146037, 1.6535513692531063],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.006474921562582,
 'weight_decay_Hydroxylation-K': 3.4033326773361248,
 'weight_decay_Hydroxylation-P': 3.3760305694190933}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2412.050
[2,     1] loss: 2406.084
[3,     1] loss: 2403.141
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028188445872771926,
 'learning_rate_Hydroxylation-K': 0.0038449039752683615,
 'learning_rate_Hydroxylation-P': 0.006017783115108738,
 'log_base': 1.1162680461902672,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 113281764,
 'sample_weights': [7.0975223051973595, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.739824581059437,
 'weight_decay_Hydroxylation-K': 6.542611151556881,
 'weight_decay_Hydroxylation-P': 2.3898099414719036}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4913.990
[2,     1] loss: 4913.790
[3,     1] loss: 4917.153
[4,     1] loss: 4958.476
[5,     1] loss: 4935.418
[6,     1] loss: 4898.719
[7,     1] loss: 4909.637
[8,     1] loss: 4892.120
[9,     1] loss: 4891.962
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004088894978072612,
 'learning_rate_Hydroxylation-K': 0.001916678660998146,
 'learning_rate_Hydroxylation-P': 0.0005896080261325287,
 'log_base': 1.2168012158942636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3965778008,
 'sample_weights': [15.177994978864783, 1.8973224620225708],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.495974571750496,
 'weight_decay_Hydroxylation-K': 9.276567241566234,
 'weight_decay_Hydroxylation-P': 4.167974742461102}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2767.620
[2,     1] loss: 2749.091
[3,     1] loss: 2776.860
[4,     1] loss: 2754.948
[5,     1] loss: 2756.892
[6,     1] loss: 2753.386
[7,     1] loss: 2767.285
[8,     1] loss: 2761.850
[9,     1] loss: 2757.010
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00291146279894488,
 'learning_rate_Hydroxylation-K': 0.0037768473001421362,
 'learning_rate_Hydroxylation-P': 0.00530703516254989,
 'log_base': 1.2565339265583617,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2018753622,
 'sample_weights': [8.507780473851108, 1.0635135284648813],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.754196404429457,
 'weight_decay_Hydroxylation-K': 3.3403576009832676,
 'weight_decay_Hydroxylation-P': 1.6146286422051028}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2440.386
[2,     1] loss: 2458.179
[3,     1] loss: 2456.489
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005174128965461732,
 'learning_rate_Hydroxylation-K': 0.008035241677228177,
 'learning_rate_Hydroxylation-P': 0.007642459624588689,
 'log_base': 2.5537179024800714,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3715125937,
 'sample_weights': [7.31066958178387, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.8721491252915223,
 'weight_decay_Hydroxylation-K': 9.054914191775495,
 'weight_decay_Hydroxylation-P': 0.38437761985055197}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.602
[2,     1] loss: 1290.696
[3,     1] loss: 1285.590
[4,     1] loss: 1284.000
[5,     1] loss: 1286.382
[6,     1] loss: 1284.763
[7,     1] loss: 1282.413
[8,     1] loss: 1279.967
[9,     1] loss: 1278.237
[10,     1] loss: 1277.328
[11,     1] loss: 1269.000
[12,     1] loss: 1259.590
[13,     1] loss: 1247.291
[14,     1] loss: 1221.081
[15,     1] loss: 1190.334
[16,     1] loss: 1150.751
[17,     1] loss: 1101.104
[18,     1] loss: 1083.440
[19,     1] loss: 1090.271
[20,     1] loss: 1047.632
[21,     1] loss: 1021.088
[22,     1] loss: 1060.230
[23,     1] loss: 1056.217
[24,     1] loss: 1023.998
[25,     1] loss: 1021.749
[26,     1] loss: 1026.516
[27,     1] loss: 1039.163
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014443720628876868,
 'learning_rate_Hydroxylation-K': 0.008682525433718538,
 'learning_rate_Hydroxylation-P': 0.003202236557605021,
 'log_base': 2.6829165160485138,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1997459618,
 'sample_weights': [1.7806438230203996, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.949135216841992,
 'weight_decay_Hydroxylation-K': 4.160265536316641,
 'weight_decay_Hydroxylation-P': 0.9936298660386307}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.750
[2,     1] loss: 1263.649
[3,     1] loss: 1266.223
[4,     1] loss: 1264.268
[5,     1] loss: 1265.845
[6,     1] loss: 1262.128
[7,     1] loss: 1260.333
[8,     1] loss: 1261.118
[9,     1] loss: 1256.306
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004743135493660882,
 'learning_rate_Hydroxylation-K': 0.002559533727534459,
 'learning_rate_Hydroxylation-P': 0.002684476857506652,
 'log_base': 2.136003198866665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1875076969,
 'sample_weights': [1.691595512473445, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.82357101521424,
 'weight_decay_Hydroxylation-K': 3.7544641621803923,
 'weight_decay_Hydroxylation-P': 0.19204644632590004}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1375.617
[2,     1] loss: 1370.665
[3,     1] loss: 1373.640
[4,     1] loss: 1371.401
[5,     1] loss: 1367.609
[6,     1] loss: 1369.564
[7,     1] loss: 1364.591
[8,     1] loss: 1363.656
[9,     1] loss: 1347.837
[10,     1] loss: 1332.272
[11,     1] loss: 1308.888
[12,     1] loss: 1262.115
[13,     1] loss: 1248.796
[14,     1] loss: 1218.219
[15,     1] loss: 1175.192
[16,     1] loss: 1181.123
[17,     1] loss: 1138.653
[18,     1] loss: 1176.124
[19,     1] loss: 1149.495
[20,     1] loss: 1170.074
[21,     1] loss: 1124.680
[22,     1] loss: 1118.426
[23,     1] loss: 1166.881
[24,     1] loss: 1112.543
[25,     1] loss: 1113.854
[26,     1] loss: 1105.441
[27,     1] loss: 1090.247
[28,     1] loss: 1074.839
[29,     1] loss: 1098.586
[30,     1] loss: 1035.896
[31,     1] loss: 1009.097
[32,     1] loss: 1011.173
[33,     1] loss: 1029.783
[34,     1] loss: 1073.390
[35,     1] loss: 995.583
[36,     1] loss: 1122.359
[37,     1] loss: 1027.535
[38,     1] loss: 1062.115
[39,     1] loss: 995.071
[40,     1] loss: 1023.672
[41,     1] loss: 1027.913
[42,     1] loss: 1024.855
[43,     1] loss: 1026.091
[44,     1] loss: 960.969
[45,     1] loss: 970.697
[46,     1] loss: 956.769
[47,     1] loss: 892.806
[48,     1] loss: 886.881
[49,     1] loss: 850.536
[50,     1] loss: 989.743
[51,     1] loss: 1058.584
[52,     1] loss: 890.095
[53,     1] loss: 966.388
[54,     1] loss: 857.845
[55,     1] loss: 946.199
[56,     1] loss: 866.671
[57,     1] loss: 931.130
[58,     1] loss: 811.278
[59,     1] loss: 809.335
[60,     1] loss: 837.672
[61,     1] loss: 745.194
[62,     1] loss: 806.296
[63,     1] loss: 852.164
[64,     1] loss: 903.568
[65,     1] loss: 768.961
[66,     1] loss: 976.200
[67,     1] loss: 761.886
[68,     1] loss: 901.679
[69,     1] loss: 788.302
[70,     1] loss: 841.116
[71,     1] loss: 758.122
[72,     1] loss: 704.123
[73,     1] loss: 676.705
[74,     1] loss: 604.742
[75,     1] loss: 581.933
[76,     1] loss: 613.324
[77,     1] loss: 643.005
[78,     1] loss: 919.385
[79,     1] loss: 1939.207
[80,     1] loss: 1032.747
[81,     1] loss: 953.197
[82,     1] loss: 1280.246
[83,     1] loss: 1201.045
[84,     1] loss: 1189.808
[85,     1] loss: 1204.624
[86,     1] loss: 1182.834
[87,     1] loss: 1142.911
[88,     1] loss: 1107.371
[89,     1] loss: 1128.586
[90,     1] loss: 1108.301
[91,     1] loss: 1104.002
[92,     1] loss: 1110.921
[93,     1] loss: 1114.786
Early stopping applied (best metric=0.591445803642273)
Finished Training
Total time taken: 12.729010343551636
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1372.340
[2,     1] loss: 1376.775
[3,     1] loss: 1374.129
[4,     1] loss: 1369.172
[5,     1] loss: 1369.393
[6,     1] loss: 1370.575
[7,     1] loss: 1371.242
[8,     1] loss: 1363.201
[9,     1] loss: 1364.386
[10,     1] loss: 1356.332
[11,     1] loss: 1342.443
[12,     1] loss: 1320.170
[13,     1] loss: 1296.759
[14,     1] loss: 1244.258
[15,     1] loss: 1220.106
[16,     1] loss: 1167.677
[17,     1] loss: 1140.893
[18,     1] loss: 1125.356
[19,     1] loss: 1126.544
[20,     1] loss: 1123.017
[21,     1] loss: 1096.749
[22,     1] loss: 1121.831
[23,     1] loss: 1068.748
[24,     1] loss: 1083.113
[25,     1] loss: 1075.016
[26,     1] loss: 1022.829
[27,     1] loss: 1042.150
[28,     1] loss: 1020.473
[29,     1] loss: 994.874
[30,     1] loss: 1066.717
[31,     1] loss: 988.941
[32,     1] loss: 969.784
[33,     1] loss: 967.297
[34,     1] loss: 1052.771
[35,     1] loss: 982.129
[36,     1] loss: 916.934
[37,     1] loss: 913.409
[38,     1] loss: 902.182
[39,     1] loss: 897.986
[40,     1] loss: 857.578
[41,     1] loss: 898.650
[42,     1] loss: 884.516
[43,     1] loss: 1109.643
[44,     1] loss: 1037.751
[45,     1] loss: 832.178
[46,     1] loss: 970.759
[47,     1] loss: 943.477
[48,     1] loss: 882.839
[49,     1] loss: 916.845
[50,     1] loss: 867.304
[51,     1] loss: 887.240
[52,     1] loss: 877.363
[53,     1] loss: 846.613
[54,     1] loss: 796.549
[55,     1] loss: 820.952
Early stopping applied (best metric=0.9418119192123413)
Finished Training
Total time taken: 9.16501235961914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1369.515
[2,     1] loss: 1377.423
[3,     1] loss: 1371.362
[4,     1] loss: 1372.646
[5,     1] loss: 1369.745
[6,     1] loss: 1369.969
[7,     1] loss: 1362.353
[8,     1] loss: 1356.438
[9,     1] loss: 1339.984
[10,     1] loss: 1317.919
[11,     1] loss: 1274.628
[12,     1] loss: 1249.039
[13,     1] loss: 1191.416
[14,     1] loss: 1175.442
[15,     1] loss: 1153.288
[16,     1] loss: 1123.569
[17,     1] loss: 1142.204
[18,     1] loss: 1190.495
[19,     1] loss: 1134.257
[20,     1] loss: 1135.839
[21,     1] loss: 1128.086
[22,     1] loss: 1096.761
[23,     1] loss: 1077.467
[24,     1] loss: 1078.152
[25,     1] loss: 1094.065
[26,     1] loss: 1038.399
[27,     1] loss: 1012.776
[28,     1] loss: 1007.995
[29,     1] loss: 1007.397
[30,     1] loss: 1001.146
[31,     1] loss: 985.062
[32,     1] loss: 959.670
[33,     1] loss: 913.311
[34,     1] loss: 1013.554
[35,     1] loss: 964.099
[36,     1] loss: 874.163
[37,     1] loss: 945.925
[38,     1] loss: 904.826
[39,     1] loss: 938.422
[40,     1] loss: 839.438
[41,     1] loss: 850.255
[42,     1] loss: 797.918
[43,     1] loss: 796.644
[44,     1] loss: 789.547
[45,     1] loss: 811.412
[46,     1] loss: 1309.727
[47,     1] loss: 1485.088
Early stopping applied (best metric=0.8791791200637817)
Finished Training
Total time taken: 6.426006317138672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1368.539
[2,     1] loss: 1374.592
[3,     1] loss: 1365.358
[4,     1] loss: 1364.218
[5,     1] loss: 1363.870
[6,     1] loss: 1354.752
[7,     1] loss: 1325.798
[8,     1] loss: 1303.862
[9,     1] loss: 1252.436
[10,     1] loss: 1221.013
[11,     1] loss: 1190.694
[12,     1] loss: 1184.555
[13,     1] loss: 1182.440
[14,     1] loss: 1195.186
[15,     1] loss: 1146.512
[16,     1] loss: 1175.983
[17,     1] loss: 1113.067
[18,     1] loss: 1137.120
[19,     1] loss: 1088.803
[20,     1] loss: 1117.194
[21,     1] loss: 1102.241
[22,     1] loss: 1127.438
[23,     1] loss: 1088.651
[24,     1] loss: 1060.530
[25,     1] loss: 1036.494
[26,     1] loss: 1003.715
[27,     1] loss: 1022.846
[28,     1] loss: 1025.543
[29,     1] loss: 1042.445
[30,     1] loss: 1006.128
[31,     1] loss: 1053.435
[32,     1] loss: 1184.357
[33,     1] loss: 1010.167
[34,     1] loss: 1063.317
[35,     1] loss: 994.677
[36,     1] loss: 1111.043
[37,     1] loss: 948.389
[38,     1] loss: 1025.836
[39,     1] loss: 965.010
[40,     1] loss: 935.206
[41,     1] loss: 932.943
[42,     1] loss: 948.844
[43,     1] loss: 940.425
[44,     1] loss: 951.282
[45,     1] loss: 891.486
[46,     1] loss: 860.802
[47,     1] loss: 865.239
[48,     1] loss: 848.133
[49,     1] loss: 813.056
[50,     1] loss: 880.960
[51,     1] loss: 1367.372
[52,     1] loss: 1079.642
[53,     1] loss: 935.081
[54,     1] loss: 950.843
[55,     1] loss: 1008.592
[56,     1] loss: 1004.462
[57,     1] loss: 993.623
[58,     1] loss: 988.145
[59,     1] loss: 995.997
[60,     1] loss: 962.355
[61,     1] loss: 925.807
[62,     1] loss: 906.651
Early stopping applied (best metric=0.7626780271530151)
Finished Training
Total time taken: 10.344010353088379
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1373.282
[2,     1] loss: 1372.432
[3,     1] loss: 1378.866
[4,     1] loss: 1373.757
[5,     1] loss: 1370.618
[6,     1] loss: 1367.151
[7,     1] loss: 1360.670
[8,     1] loss: 1348.684
[9,     1] loss: 1329.530
[10,     1] loss: 1288.241
[11,     1] loss: 1260.027
[12,     1] loss: 1247.937
[13,     1] loss: 1189.792
[14,     1] loss: 1172.106
[15,     1] loss: 1130.318
[16,     1] loss: 1091.900
[17,     1] loss: 1134.006
[18,     1] loss: 1132.477
[19,     1] loss: 1210.528
[20,     1] loss: 1093.109
[21,     1] loss: 1167.512
[22,     1] loss: 1057.607
[23,     1] loss: 1098.832
[24,     1] loss: 1096.800
[25,     1] loss: 1087.889
[26,     1] loss: 1034.660
[27,     1] loss: 1045.537
[28,     1] loss: 1080.650
[29,     1] loss: 1031.605
[30,     1] loss: 1005.923
[31,     1] loss: 1008.243
[32,     1] loss: 1007.857
[33,     1] loss: 1028.819
[34,     1] loss: 981.757
[35,     1] loss: 974.265
[36,     1] loss: 956.800
[37,     1] loss: 986.061
[38,     1] loss: 931.087
[39,     1] loss: 933.396
[40,     1] loss: 904.065
[41,     1] loss: 877.262
[42,     1] loss: 841.598
[43,     1] loss: 880.485
[44,     1] loss: 1008.200
[45,     1] loss: 1722.570
[46,     1] loss: 886.294
[47,     1] loss: 1143.289
[48,     1] loss: 1121.940
[49,     1] loss: 1078.557
[50,     1] loss: 1097.770
[51,     1] loss: 1115.139
[52,     1] loss: 1118.954
[53,     1] loss: 1119.604
[54,     1] loss: 1069.494
[55,     1] loss: 1002.600
[56,     1] loss: 1017.473
[57,     1] loss: 994.819
[58,     1] loss: 982.876
[59,     1] loss: 1058.688
[60,     1] loss: 977.789
[61,     1] loss: 936.896
Early stopping applied (best metric=0.8215163946151733)
Finished Training
Total time taken: 8.270008563995361
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1372.012
[2,     1] loss: 1376.644
[3,     1] loss: 1373.455
[4,     1] loss: 1376.654
[5,     1] loss: 1371.200
[6,     1] loss: 1368.710
[7,     1] loss: 1370.052
[8,     1] loss: 1371.242
[9,     1] loss: 1371.525
[10,     1] loss: 1371.787
[11,     1] loss: 1369.904
[12,     1] loss: 1366.705
[13,     1] loss: 1367.298
[14,     1] loss: 1361.385
[15,     1] loss: 1354.399
[16,     1] loss: 1339.442
[17,     1] loss: 1326.913
[18,     1] loss: 1292.049
[19,     1] loss: 1278.249
[20,     1] loss: 1256.619
[21,     1] loss: 1230.623
[22,     1] loss: 1200.602
[23,     1] loss: 1205.192
[24,     1] loss: 1141.595
[25,     1] loss: 1157.729
[26,     1] loss: 1099.199
[27,     1] loss: 1140.862
[28,     1] loss: 1111.478
[29,     1] loss: 1053.947
[30,     1] loss: 1095.662
[31,     1] loss: 1098.577
[32,     1] loss: 1056.371
[33,     1] loss: 1041.247
[34,     1] loss: 1054.153
[35,     1] loss: 1022.692
[36,     1] loss: 1033.729
[37,     1] loss: 1012.526
[38,     1] loss: 992.641
[39,     1] loss: 1080.064
[40,     1] loss: 1030.847
[41,     1] loss: 961.166
[42,     1] loss: 1049.501
[43,     1] loss: 952.299
[44,     1] loss: 975.509
[45,     1] loss: 977.004
[46,     1] loss: 923.033
[47,     1] loss: 894.105
[48,     1] loss: 876.933
[49,     1] loss: 874.255
[50,     1] loss: 980.626
[51,     1] loss: 1448.769
[52,     1] loss: 902.382
[53,     1] loss: 1184.808
[54,     1] loss: 1008.024
[55,     1] loss: 1038.938
[56,     1] loss: 1058.605
[57,     1] loss: 1050.376
[58,     1] loss: 994.992
[59,     1] loss: 954.714
[60,     1] loss: 1013.628
[61,     1] loss: 1028.179
[62,     1] loss: 1038.309
[63,     1] loss: 1019.850
[64,     1] loss: 941.190
[65,     1] loss: 930.306
[66,     1] loss: 928.412
[67,     1] loss: 890.230
[68,     1] loss: 893.968
Early stopping applied (best metric=0.7432894110679626)
Finished Training
Total time taken: 11.424010992050171
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1377.179
[2,     1] loss: 1372.156
[3,     1] loss: 1370.991
[4,     1] loss: 1379.901
[5,     1] loss: 1372.568
[6,     1] loss: 1371.750
[7,     1] loss: 1368.710
[8,     1] loss: 1366.417
[9,     1] loss: 1370.855
[10,     1] loss: 1365.932
[11,     1] loss: 1365.750
[12,     1] loss: 1358.361
[13,     1] loss: 1345.525
[14,     1] loss: 1326.562
[15,     1] loss: 1286.151
[16,     1] loss: 1246.190
[17,     1] loss: 1199.975
[18,     1] loss: 1201.878
[19,     1] loss: 1182.881
[20,     1] loss: 1155.951
[21,     1] loss: 1174.346
[22,     1] loss: 1174.219
[23,     1] loss: 1081.127
[24,     1] loss: 1126.190
[25,     1] loss: 1086.530
[26,     1] loss: 1047.762
[27,     1] loss: 1075.194
[28,     1] loss: 1081.669
[29,     1] loss: 1074.103
[30,     1] loss: 1062.220
[31,     1] loss: 1043.326
[32,     1] loss: 1054.719
[33,     1] loss: 1009.035
[34,     1] loss: 1020.594
[35,     1] loss: 983.034
[36,     1] loss: 969.673
[37,     1] loss: 947.010
[38,     1] loss: 949.031
[39,     1] loss: 1102.899
[40,     1] loss: 1247.199
[41,     1] loss: 946.538
[42,     1] loss: 1058.786
[43,     1] loss: 1053.983
[44,     1] loss: 1056.980
[45,     1] loss: 1057.418
[46,     1] loss: 1044.254
[47,     1] loss: 1058.618
[48,     1] loss: 974.386
[49,     1] loss: 992.976
[50,     1] loss: 987.939
[51,     1] loss: 971.249
[52,     1] loss: 988.350
[53,     1] loss: 946.234
[54,     1] loss: 945.557
[55,     1] loss: 904.376
[56,     1] loss: 914.940
[57,     1] loss: 924.708
[58,     1] loss: 842.568
[59,     1] loss: 912.571
[60,     1] loss: 884.279
[61,     1] loss: 912.472
[62,     1] loss: 873.621
[63,     1] loss: 812.671
[64,     1] loss: 843.031
[65,     1] loss: 1204.969
[66,     1] loss: 1065.946
[67,     1] loss: 885.340
[68,     1] loss: 962.723
[69,     1] loss: 1006.901
[70,     1] loss: 909.858
[71,     1] loss: 920.661
[72,     1] loss: 943.381
[73,     1] loss: 851.033
[74,     1] loss: 888.916
[75,     1] loss: 821.502
[76,     1] loss: 915.866
[77,     1] loss: 825.613
[78,     1] loss: 812.192
[79,     1] loss: 813.753
[80,     1] loss: 748.599
[81,     1] loss: 811.989
[82,     1] loss: 710.651
[83,     1] loss: 755.149
[84,     1] loss: 765.871
[85,     1] loss: 700.476
[86,     1] loss: 658.091
[87,     1] loss: 638.028
[88,     1] loss: 631.142
[89,     1] loss: 673.261
[90,     1] loss: 1031.010
[91,     1] loss: 1996.732
[92,     1] loss: 1320.039
[93,     1] loss: 1278.374
[94,     1] loss: 1343.915
[95,     1] loss: 1358.820
[96,     1] loss: 1364.910
Early stopping applied (best metric=0.7786223888397217)
Finished Training
Total time taken: 16.106016397476196
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.888
[2,     1] loss: 1373.905
[3,     1] loss: 1372.987
[4,     1] loss: 1369.164
[5,     1] loss: 1369.683
[6,     1] loss: 1363.467
[7,     1] loss: 1361.526
[8,     1] loss: 1354.505
[9,     1] loss: 1340.200
[10,     1] loss: 1313.159
[11,     1] loss: 1271.341
[12,     1] loss: 1232.782
[13,     1] loss: 1231.934
[14,     1] loss: 1207.969
[15,     1] loss: 1128.571
[16,     1] loss: 1133.854
[17,     1] loss: 1121.239
[18,     1] loss: 1119.540
[19,     1] loss: 1130.785
[20,     1] loss: 1155.003
[21,     1] loss: 1094.054
[22,     1] loss: 1080.601
[23,     1] loss: 1120.691
[24,     1] loss: 1056.693
[25,     1] loss: 1051.132
[26,     1] loss: 1054.957
[27,     1] loss: 1011.181
[28,     1] loss: 994.574
[29,     1] loss: 1107.966
[30,     1] loss: 989.836
[31,     1] loss: 1044.940
[32,     1] loss: 965.908
[33,     1] loss: 964.356
[34,     1] loss: 940.252
[35,     1] loss: 954.692
[36,     1] loss: 919.843
[37,     1] loss: 941.038
[38,     1] loss: 934.095
[39,     1] loss: 897.722
[40,     1] loss: 853.892
[41,     1] loss: 855.961
[42,     1] loss: 1045.693
[43,     1] loss: 1326.033
[44,     1] loss: 885.802
[45,     1] loss: 1044.268
[46,     1] loss: 994.927
[47,     1] loss: 986.222
[48,     1] loss: 1020.626
[49,     1] loss: 1019.021
[50,     1] loss: 996.194
[51,     1] loss: 923.089
[52,     1] loss: 991.986
[53,     1] loss: 919.124
[54,     1] loss: 869.511
[55,     1] loss: 935.838
[56,     1] loss: 836.071
[57,     1] loss: 910.396
[58,     1] loss: 833.373
Early stopping applied (best metric=0.8135613203048706)
Finished Training
Total time taken: 9.714009046554565
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1376.172
[2,     1] loss: 1374.181
[3,     1] loss: 1375.970
[4,     1] loss: 1371.066
[5,     1] loss: 1368.532
[6,     1] loss: 1367.419
[7,     1] loss: 1363.359
[8,     1] loss: 1357.069
[9,     1] loss: 1344.730
[10,     1] loss: 1308.029
[11,     1] loss: 1282.482
[12,     1] loss: 1268.271
[13,     1] loss: 1238.255
[14,     1] loss: 1214.281
[15,     1] loss: 1206.955
[16,     1] loss: 1173.613
[17,     1] loss: 1147.553
[18,     1] loss: 1115.599
[19,     1] loss: 1155.523
[20,     1] loss: 1100.545
[21,     1] loss: 1093.339
[22,     1] loss: 1131.437
[23,     1] loss: 1085.230
[24,     1] loss: 1124.785
[25,     1] loss: 1052.578
[26,     1] loss: 1125.981
[27,     1] loss: 1069.891
[28,     1] loss: 1141.187
[29,     1] loss: 1031.941
[30,     1] loss: 1098.592
[31,     1] loss: 1036.558
[32,     1] loss: 1036.457
[33,     1] loss: 1021.845
[34,     1] loss: 1025.129
[35,     1] loss: 996.992
[36,     1] loss: 987.818
[37,     1] loss: 1056.815
[38,     1] loss: 1023.630
[39,     1] loss: 951.857
[40,     1] loss: 976.080
[41,     1] loss: 1004.702
[42,     1] loss: 957.835
[43,     1] loss: 986.506
[44,     1] loss: 945.641
[45,     1] loss: 947.046
[46,     1] loss: 905.834
[47,     1] loss: 959.473
[48,     1] loss: 905.975
[49,     1] loss: 853.982
[50,     1] loss: 880.466
[51,     1] loss: 936.342
[52,     1] loss: 957.593
[53,     1] loss: 856.313
[54,     1] loss: 802.615
[55,     1] loss: 885.138
[56,     1] loss: 952.228
[57,     1] loss: 936.715
[58,     1] loss: 823.284
[59,     1] loss: 991.615
[60,     1] loss: 830.419
[61,     1] loss: 828.310
[62,     1] loss: 846.062
[63,     1] loss: 792.392
[64,     1] loss: 840.769
[65,     1] loss: 799.671
[66,     1] loss: 879.684
[67,     1] loss: 1014.611
[68,     1] loss: 779.098
[69,     1] loss: 906.612
[70,     1] loss: 814.555
[71,     1] loss: 894.269
[72,     1] loss: 859.428
[73,     1] loss: 789.400
[74,     1] loss: 823.458
[75,     1] loss: 742.822
[76,     1] loss: 741.457
[77,     1] loss: 672.088
[78,     1] loss: 657.831
[79,     1] loss: 646.797
[80,     1] loss: 667.681
[81,     1] loss: 877.877
[82,     1] loss: 1386.336
Early stopping applied (best metric=0.708772599697113)
Finished Training
Total time taken: 12.691012620925903
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1374.429
[2,     1] loss: 1371.910
[3,     1] loss: 1372.663
[4,     1] loss: 1373.362
[5,     1] loss: 1370.392
[6,     1] loss: 1369.513
[7,     1] loss: 1357.379
[8,     1] loss: 1338.309
[9,     1] loss: 1307.088
[10,     1] loss: 1273.489
[11,     1] loss: 1228.547
[12,     1] loss: 1195.288
[13,     1] loss: 1157.527
[14,     1] loss: 1182.279
[15,     1] loss: 1167.748
[16,     1] loss: 1167.371
[17,     1] loss: 1152.836
[18,     1] loss: 1097.159
[19,     1] loss: 1174.395
[20,     1] loss: 1116.734
[21,     1] loss: 1142.196
[22,     1] loss: 1121.176
[23,     1] loss: 1108.878
[24,     1] loss: 1140.196
[25,     1] loss: 1076.538
[26,     1] loss: 1039.157
[27,     1] loss: 1087.088
[28,     1] loss: 1023.064
[29,     1] loss: 1052.693
[30,     1] loss: 1000.147
[31,     1] loss: 1029.555
[32,     1] loss: 1034.946
[33,     1] loss: 1011.666
[34,     1] loss: 960.067
[35,     1] loss: 1032.872
[36,     1] loss: 987.438
[37,     1] loss: 977.039
[38,     1] loss: 1037.287
[39,     1] loss: 950.632
[40,     1] loss: 1011.469
[41,     1] loss: 916.846
[42,     1] loss: 942.034
[43,     1] loss: 936.602
[44,     1] loss: 892.750
[45,     1] loss: 925.812
[46,     1] loss: 1023.368
[47,     1] loss: 997.990
[48,     1] loss: 870.845
[49,     1] loss: 956.872
[50,     1] loss: 905.836
[51,     1] loss: 928.420
[52,     1] loss: 874.233
[53,     1] loss: 953.175
[54,     1] loss: 897.009
[55,     1] loss: 830.775
[56,     1] loss: 872.339
[57,     1] loss: 798.093
[58,     1] loss: 827.031
[59,     1] loss: 810.152
[60,     1] loss: 778.411
[61,     1] loss: 832.679
[62,     1] loss: 922.595
[63,     1] loss: 801.767
[64,     1] loss: 716.437
[65,     1] loss: 887.977
[66,     1] loss: 745.900
[67,     1] loss: 837.147
[68,     1] loss: 678.866
[69,     1] loss: 875.474
[70,     1] loss: 922.214
[71,     1] loss: 696.089
[72,     1] loss: 875.195
[73,     1] loss: 725.951
[74,     1] loss: 766.736
Early stopping applied (best metric=0.7509703636169434)
Finished Training
Total time taken: 10.753010272979736
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1383.419
[2,     1] loss: 1373.343
[3,     1] loss: 1370.440
[4,     1] loss: 1374.603
[5,     1] loss: 1371.600
[6,     1] loss: 1367.869
[7,     1] loss: 1370.967
[8,     1] loss: 1368.919
[9,     1] loss: 1366.749
[10,     1] loss: 1359.270
[11,     1] loss: 1351.820
[12,     1] loss: 1339.962
[13,     1] loss: 1310.820
[14,     1] loss: 1286.688
[15,     1] loss: 1241.730
[16,     1] loss: 1240.837
[17,     1] loss: 1173.225
[18,     1] loss: 1147.610
[19,     1] loss: 1129.406
[20,     1] loss: 1143.516
[21,     1] loss: 1101.133
[22,     1] loss: 1150.401
[23,     1] loss: 1056.759
[24,     1] loss: 1093.542
[25,     1] loss: 1043.492
[26,     1] loss: 1080.355
[27,     1] loss: 1062.314
[28,     1] loss: 1030.076
[29,     1] loss: 1001.198
[30,     1] loss: 1029.293
[31,     1] loss: 1069.417
[32,     1] loss: 1102.852
[33,     1] loss: 1011.727
[34,     1] loss: 1042.224
[35,     1] loss: 952.398
[36,     1] loss: 962.178
[37,     1] loss: 946.672
[38,     1] loss: 978.793
[39,     1] loss: 867.436
[40,     1] loss: 922.078
[41,     1] loss: 891.161
[42,     1] loss: 891.675
[43,     1] loss: 946.020
[44,     1] loss: 1026.083
[45,     1] loss: 1056.634
[46,     1] loss: 865.424
[47,     1] loss: 1012.839
[48,     1] loss: 915.063
[49,     1] loss: 961.125
[50,     1] loss: 948.870
[51,     1] loss: 893.312
[52,     1] loss: 978.652
[53,     1] loss: 816.753
[54,     1] loss: 934.766
Early stopping applied (best metric=0.894477128982544)
Finished Training
Total time taken: 9.100008726119995
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1374.703
[2,     1] loss: 1374.881
[3,     1] loss: 1370.918
[4,     1] loss: 1374.036
[5,     1] loss: 1370.172
[6,     1] loss: 1372.217
[7,     1] loss: 1366.492
[8,     1] loss: 1364.836
[9,     1] loss: 1357.391
[10,     1] loss: 1341.480
[11,     1] loss: 1319.546
[12,     1] loss: 1289.805
[13,     1] loss: 1243.177
[14,     1] loss: 1213.564
[15,     1] loss: 1181.964
[16,     1] loss: 1173.837
[17,     1] loss: 1148.450
[18,     1] loss: 1141.656
[19,     1] loss: 1185.147
[20,     1] loss: 1181.626
[21,     1] loss: 1139.667
[22,     1] loss: 1114.172
[23,     1] loss: 1105.014
[24,     1] loss: 1116.898
[25,     1] loss: 1093.444
[26,     1] loss: 1125.201
[27,     1] loss: 1093.227
[28,     1] loss: 1073.254
[29,     1] loss: 1028.293
[30,     1] loss: 1045.087
[31,     1] loss: 1037.433
[32,     1] loss: 1042.205
[33,     1] loss: 1034.352
[34,     1] loss: 1004.174
[35,     1] loss: 1077.412
[36,     1] loss: 1061.895
[37,     1] loss: 995.042
[38,     1] loss: 1126.434
[39,     1] loss: 962.204
[40,     1] loss: 1016.648
[41,     1] loss: 988.172
[42,     1] loss: 1042.619
[43,     1] loss: 933.980
[44,     1] loss: 960.068
[45,     1] loss: 891.091
[46,     1] loss: 930.024
[47,     1] loss: 973.560
[48,     1] loss: 870.050
[49,     1] loss: 889.552
[50,     1] loss: 954.402
[51,     1] loss: 889.204
[52,     1] loss: 908.823
[53,     1] loss: 850.011
[54,     1] loss: 925.062
[55,     1] loss: 884.879
[56,     1] loss: 775.875
[57,     1] loss: 767.628
[58,     1] loss: 733.362
[59,     1] loss: 833.605
[60,     1] loss: 1344.526
[61,     1] loss: 1212.941
[62,     1] loss: 943.793
[63,     1] loss: 945.949
[64,     1] loss: 1078.620
[65,     1] loss: 1100.132
[66,     1] loss: 1048.896
[67,     1] loss: 1039.952
[68,     1] loss: 1056.181
[69,     1] loss: 1054.309
[70,     1] loss: 1069.089
[71,     1] loss: 968.639
[72,     1] loss: 961.271
[73,     1] loss: 1033.544
[74,     1] loss: 920.255
[75,     1] loss: 953.576
[76,     1] loss: 995.465
Early stopping applied (best metric=0.6881524324417114)
Finished Training
Total time taken: 10.27800989151001
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1376.012
[2,     1] loss: 1371.564
[3,     1] loss: 1368.438
[4,     1] loss: 1379.425
[5,     1] loss: 1370.638
[6,     1] loss: 1370.079
[7,     1] loss: 1373.224
[8,     1] loss: 1369.669
[9,     1] loss: 1367.078
[10,     1] loss: 1365.498
[11,     1] loss: 1355.881
[12,     1] loss: 1344.924
[13,     1] loss: 1322.923
[14,     1] loss: 1296.244
[15,     1] loss: 1252.348
[16,     1] loss: 1245.792
[17,     1] loss: 1193.564
[18,     1] loss: 1160.816
[19,     1] loss: 1155.418
[20,     1] loss: 1153.004
[21,     1] loss: 1140.595
[22,     1] loss: 1150.152
[23,     1] loss: 1094.268
[24,     1] loss: 1141.194
[25,     1] loss: 1088.089
[26,     1] loss: 1112.561
[27,     1] loss: 1055.869
[28,     1] loss: 1143.727
[29,     1] loss: 1057.399
[30,     1] loss: 1115.119
[31,     1] loss: 1093.596
[32,     1] loss: 1012.157
[33,     1] loss: 994.147
[34,     1] loss: 981.962
[35,     1] loss: 1026.942
[36,     1] loss: 1017.456
[37,     1] loss: 1016.935
[38,     1] loss: 1015.301
[39,     1] loss: 975.476
[40,     1] loss: 994.263
[41,     1] loss: 924.353
[42,     1] loss: 996.726
[43,     1] loss: 933.077
[44,     1] loss: 920.351
[45,     1] loss: 929.445
[46,     1] loss: 911.795
[47,     1] loss: 919.282
[48,     1] loss: 936.799
[49,     1] loss: 1060.417
[50,     1] loss: 1129.367
[51,     1] loss: 944.513
[52,     1] loss: 944.465
[53,     1] loss: 993.680
[54,     1] loss: 909.505
[55,     1] loss: 960.228
[56,     1] loss: 874.955
[57,     1] loss: 925.920
[58,     1] loss: 871.864
[59,     1] loss: 814.125
[60,     1] loss: 845.332
[61,     1] loss: 854.729
[62,     1] loss: 795.490
[63,     1] loss: 788.879
[64,     1] loss: 883.763
[65,     1] loss: 960.915
[66,     1] loss: 795.727
[67,     1] loss: 867.792
[68,     1] loss: 777.040
[69,     1] loss: 794.991
[70,     1] loss: 779.945
[71,     1] loss: 702.618
[72,     1] loss: 789.043
[73,     1] loss: 810.208
Early stopping applied (best metric=0.8572320938110352)
Finished Training
Total time taken: 12.186011552810669
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1369.985
[2,     1] loss: 1370.015
[3,     1] loss: 1376.342
[4,     1] loss: 1373.364
[5,     1] loss: 1374.602
[6,     1] loss: 1370.264
[7,     1] loss: 1370.234
[8,     1] loss: 1369.960
[9,     1] loss: 1367.069
[10,     1] loss: 1368.028
[11,     1] loss: 1363.359
[12,     1] loss: 1356.349
[13,     1] loss: 1347.854
[14,     1] loss: 1329.568
[15,     1] loss: 1306.607
[16,     1] loss: 1258.940
[17,     1] loss: 1224.208
[18,     1] loss: 1210.802
[19,     1] loss: 1182.909
[20,     1] loss: 1244.483
[21,     1] loss: 1182.533
[22,     1] loss: 1142.027
[23,     1] loss: 1170.719
[24,     1] loss: 1151.848
[25,     1] loss: 1129.043
[26,     1] loss: 1147.483
[27,     1] loss: 1140.858
[28,     1] loss: 1070.287
[29,     1] loss: 1068.455
[30,     1] loss: 1081.573
[31,     1] loss: 1066.948
[32,     1] loss: 1059.797
[33,     1] loss: 1017.821
[34,     1] loss: 1053.705
[35,     1] loss: 1018.583
[36,     1] loss: 1008.964
[37,     1] loss: 956.685
[38,     1] loss: 978.009
[39,     1] loss: 929.654
[40,     1] loss: 906.630
[41,     1] loss: 917.886
[42,     1] loss: 975.644
[43,     1] loss: 924.005
[44,     1] loss: 898.662
[45,     1] loss: 954.777
[46,     1] loss: 932.831
[47,     1] loss: 856.270
[48,     1] loss: 888.865
[49,     1] loss: 805.639
[50,     1] loss: 826.356
[51,     1] loss: 820.680
[52,     1] loss: 1184.077
Early stopping applied (best metric=0.8332154750823975)
Finished Training
Total time taken: 7.079007148742676
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1380.438
[2,     1] loss: 1374.415
[3,     1] loss: 1373.083
[4,     1] loss: 1374.038
[5,     1] loss: 1379.929
[6,     1] loss: 1370.305
[7,     1] loss: 1368.141
[8,     1] loss: 1369.027
[9,     1] loss: 1368.327
[10,     1] loss: 1360.502
[11,     1] loss: 1340.415
[12,     1] loss: 1311.020
[13,     1] loss: 1284.851
[14,     1] loss: 1267.021
[15,     1] loss: 1230.370
[16,     1] loss: 1192.988
[17,     1] loss: 1162.481
[18,     1] loss: 1156.686
[19,     1] loss: 1157.292
[20,     1] loss: 1167.911
[21,     1] loss: 1132.977
[22,     1] loss: 1079.716
[23,     1] loss: 1105.978
[24,     1] loss: 1070.304
[25,     1] loss: 1073.181
[26,     1] loss: 999.105
[27,     1] loss: 1037.219
[28,     1] loss: 957.046
[29,     1] loss: 989.298
[30,     1] loss: 978.617
[31,     1] loss: 922.812
[32,     1] loss: 951.861
[33,     1] loss: 969.718
[34,     1] loss: 995.667
[35,     1] loss: 926.317
[36,     1] loss: 910.168
[37,     1] loss: 953.609
[38,     1] loss: 874.868
[39,     1] loss: 935.616
[40,     1] loss: 984.948
[41,     1] loss: 847.125
[42,     1] loss: 955.278
[43,     1] loss: 952.273
[44,     1] loss: 851.397
[45,     1] loss: 948.637
[46,     1] loss: 853.413
[47,     1] loss: 895.327
[48,     1] loss: 815.176
[49,     1] loss: 863.663
[50,     1] loss: 863.761
[51,     1] loss: 866.423
[52,     1] loss: 931.468
[53,     1] loss: 771.896
[54,     1] loss: 889.348
[55,     1] loss: 764.798
[56,     1] loss: 828.189
[57,     1] loss: 783.356
[58,     1] loss: 797.880
[59,     1] loss: 843.818
[60,     1] loss: 648.674
[61,     1] loss: 723.239
Early stopping applied (best metric=0.913402795791626)
Finished Training
Total time taken: 10.23801326751709
{'Hydroxylation-K Validation Accuracy': 0.7659869976359338, 'Hydroxylation-K Validation Sensitivity': 0.6866666666666666, 'Hydroxylation-K Validation Specificity': 0.7859649122807018, 'Hydroxylation-K Validation Precision': 0.45755901505901503, 'Hydroxylation-K AUC ROC': 0.8064912280701755, 'Hydroxylation-K AUC PR': 0.6298510579088656, 'Hydroxylation-K MCC': 0.4157098669128081, 'Hydroxylation-K F1': 0.544948009949667, 'Validation Loss (Hydroxylation-K)': 0.42640889088312783, 'Hydroxylation-P Validation Accuracy': 0.7979626922491244, 'Hydroxylation-P Validation Sensitivity': 0.7820634920634921, 'Hydroxylation-P Validation Specificity': 0.8013716394832661, 'Hydroxylation-P Validation Precision': 0.46499431102202177, 'Hydroxylation-P AUC ROC': 0.8559229807606286, 'Hydroxylation-P AUC PR': 0.5938178571400519, 'Hydroxylation-P MCC': 0.4886946789101386, 'Hydroxylation-P F1': 0.5806800977350726, 'Validation Loss (Hydroxylation-P)': 0.37214625875155133, 'Validation Loss (total)': 0.7985551516215007, 'TimeToTrain': 10.43354385693868}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028097812323458556,
 'learning_rate_Hydroxylation-K': 0.004745058283116178,
 'learning_rate_Hydroxylation-P': 0.004456115038413043,
 'log_base': 2.8590320288320608,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 355844345,
 'sample_weights': [2.201345852595532, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.393951398823843,
 'weight_decay_Hydroxylation-K': 9.2381325403904,
 'weight_decay_Hydroxylation-P': 7.808630710275917}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.162
[2,     1] loss: 1244.954
[3,     1] loss: 1244.029
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004826936999599982,
 'learning_rate_Hydroxylation-K': 0.005375727554442311,
 'learning_rate_Hydroxylation-P': 0.00030075617400989005,
 'log_base': 1.2285437096980383,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3627419391,
 'sample_weights': [1.5892146402529126, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.090555048391373,
 'weight_decay_Hydroxylation-K': 9.993224063586238,
 'weight_decay_Hydroxylation-P': 9.126243150266204}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2631.744
[2,     1] loss: 2634.199
[3,     1] loss: 2636.906
[4,     1] loss: 2626.625
[5,     1] loss: 2627.319
[6,     1] loss: 2615.719
[7,     1] loss: 2626.544
[8,     1] loss: 2621.170
[9,     1] loss: 2610.394
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003494665017647765,
 'learning_rate_Hydroxylation-K': 0.0029728866381002568,
 'learning_rate_Hydroxylation-P': 0.005254099548140502,
 'log_base': 1.276423594052555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 897001054,
 'sample_weights': [8.110806330940028, 1.0138898489712356],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.941218791424124,
 'weight_decay_Hydroxylation-K': 7.375546717948737,
 'weight_decay_Hydroxylation-P': 1.20567106685515}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2359.572
[2,     1] loss: 2349.775
[3,     1] loss: 2353.389
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003271988161987778,
 'learning_rate_Hydroxylation-K': 0.003264186777712036,
 'learning_rate_Hydroxylation-P': 0.0037383847713479486,
 'log_base': 1.0467789724647674,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 946302254,
 'sample_weights': [6.840239212829742, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.021822950295107,
 'weight_decay_Hydroxylation-K': 2.9802419286333803,
 'weight_decay_Hydroxylation-P': 2.1670879120949764}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11910.270
[2,     1] loss: 11870.262
[3,     1] loss: 11832.852
[4,     1] loss: 11823.135
[5,     1] loss: 11847.334
[6,     1] loss: 11814.232
[7,     1] loss: 11842.822
[8,     1] loss: 11791.684
[9,     1] loss: 11758.576
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064969168103575165,
 'learning_rate_Hydroxylation-K': 0.004924494311427417,
 'learning_rate_Hydroxylation-P': 0.002214619063956771,
 'log_base': 2.4192992516957075,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4059468590,
 'sample_weights': [36.51625844403264, 4.564708149617956],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.4688051649738405,
 'weight_decay_Hydroxylation-K': 3.587266908865708,
 'weight_decay_Hydroxylation-P': 0.9579401504460487}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1308.334
[2,     1] loss: 1310.352
[3,     1] loss: 1307.138
[4,     1] loss: 1308.826
[5,     1] loss: 1308.804
[6,     1] loss: 1306.862
[7,     1] loss: 1307.575
[8,     1] loss: 1305.113
[9,     1] loss: 1302.112
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00790740919629963,
 'learning_rate_Hydroxylation-K': 0.00599475229432965,
 'learning_rate_Hydroxylation-P': 0.006668663476522861,
 'log_base': 2.780200606569616,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3010563876,
 'sample_weights': [1.8896263114260343, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.973775796635435,
 'weight_decay_Hydroxylation-K': 6.631238100862699,
 'weight_decay_Hydroxylation-P': 0.2715650304534918}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.497
[2,     1] loss: 1260.421
[3,     1] loss: 1251.131
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017452465782137232,
 'learning_rate_Hydroxylation-K': 0.0011875578127792912,
 'learning_rate_Hydroxylation-P': 0.006576376798648655,
 'log_base': 1.5704409684264575,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3610211549,
 'sample_weights': [1.6326703726425962, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.155179832390032,
 'weight_decay_Hydroxylation-K': 7.660669963698826,
 'weight_decay_Hydroxylation-P': 2.0651719855806063}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1712.661
[2,     1] loss: 1687.180
[3,     1] loss: 1690.267
[4,     1] loss: 1691.692
[5,     1] loss: 1687.750
[6,     1] loss: 1683.765
[7,     1] loss: 1684.335
[8,     1] loss: 1685.145
[9,     1] loss: 1678.616
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0041299031165648115,
 'learning_rate_Hydroxylation-K': 0.007981554309859569,
 'learning_rate_Hydroxylation-P': 0.005900117996809682,
 'log_base': 2.011546613911194,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3608031534,
 'sample_weights': [3.6987244599159355, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.6874235736378305,
 'weight_decay_Hydroxylation-K': 5.927704120618262,
 'weight_decay_Hydroxylation-P': 0.5487135108897105}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1411.029
[2,     1] loss: 1414.613
[3,     1] loss: 1414.101
[4,     1] loss: 1409.494
[5,     1] loss: 1408.782
[6,     1] loss: 1408.285
[7,     1] loss: 1404.637
[8,     1] loss: 1397.390
[9,     1] loss: 1390.646
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004418826317259885,
 'learning_rate_Hydroxylation-K': 0.009686555163751914,
 'learning_rate_Hydroxylation-P': 0.0034397906975810453,
 'log_base': 2.843144454019072,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2792076925,
 'sample_weights': [2.3886591292920176, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.29199369687672,
 'weight_decay_Hydroxylation-K': 6.745889820705931,
 'weight_decay_Hydroxylation-P': 1.888946453921984}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.795
[2,     1] loss: 1243.663
[3,     1] loss: 1245.933
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004194063659711282,
 'learning_rate_Hydroxylation-K': 0.001765173188689156,
 'learning_rate_Hydroxylation-P': 0.0065170959662889,
 'log_base': 1.1586394680945902,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1745508782,
 'sample_weights': [1.5976898699717883, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.160505952514402,
 'weight_decay_Hydroxylation-K': 4.210146489773587,
 'weight_decay_Hydroxylation-P': 0.9465354447466192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3673.292
[2,     1] loss: 3695.001
[3,     1] loss: 3676.637
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015328887480367671,
 'learning_rate_Hydroxylation-K': 0.00644142939755212,
 'learning_rate_Hydroxylation-P': 0.0026286155511377144,
 'log_base': 1.124347829405034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2948053125,
 'sample_weights': [11.337748451939628, 1.4172731534422736],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.70336051389941,
 'weight_decay_Hydroxylation-K': 3.538065925598963,
 'weight_decay_Hydroxylation-P': 0.6733943164048941}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4664.710
[2,     1] loss: 4622.136
[3,     1] loss: 4614.706
[4,     1] loss: 4630.285
[5,     1] loss: 4622.022
[6,     1] loss: 4611.177
[7,     1] loss: 4649.888
[8,     1] loss: 4614.672
[9,     1] loss: 4630.756
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002092150254842439,
 'learning_rate_Hydroxylation-K': 0.006153194492865712,
 'learning_rate_Hydroxylation-P': 0.0022159561513856196,
 'log_base': 1.1978733958900054,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1857650403,
 'sample_weights': [14.24401135784221, 1.7805700117947976],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.841652587630195,
 'weight_decay_Hydroxylation-K': 5.675375946157419,
 'weight_decay_Hydroxylation-P': 0.6174711765261716}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3009.777
[2,     1] loss: 3003.646
[3,     1] loss: 2993.717
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036914895771747733,
 'learning_rate_Hydroxylation-K': 0.007832498957729486,
 'learning_rate_Hydroxylation-P': 0.008443074685889366,
 'log_base': 2.1523977158506185,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 351558359,
 'sample_weights': [9.246543092053646, 1.15586241325303],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.845514656573804,
 'weight_decay_Hydroxylation-K': 0.09500823392628455,
 'weight_decay_Hydroxylation-P': 4.193359869120082}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1368.792
[2,     1] loss: 1367.855
[3,     1] loss: 1368.234
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005813453493536727,
 'learning_rate_Hydroxylation-K': 0.006623832151060793,
 'learning_rate_Hydroxylation-P': 0.003289866775564766,
 'log_base': 2.39295187903758,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 330925502,
 'sample_weights': [2.177773799563858, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.277705306670361,
 'weight_decay_Hydroxylation-K': 2.778065384556614,
 'weight_decay_Hydroxylation-P': 0.30334481547549075}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.467
[2,     1] loss: 1313.758
[3,     1] loss: 1309.698
[4,     1] loss: 1311.593
[5,     1] loss: 1303.932
[6,     1] loss: 1292.960
[7,     1] loss: 1266.381
[8,     1] loss: 1222.155
[9,     1] loss: 1191.035
[10,     1] loss: 1158.205
[11,     1] loss: 1114.726
[12,     1] loss: 1113.241
[13,     1] loss: 1096.359
[14,     1] loss: 1081.684
[15,     1] loss: 1089.240
[16,     1] loss: 1125.009
[17,     1] loss: 1085.287
[18,     1] loss: 1133.484
[19,     1] loss: 1090.220
[20,     1] loss: 1102.590
[21,     1] loss: 1043.491
[22,     1] loss: 1024.922
[23,     1] loss: 1017.907
[24,     1] loss: 951.147
[25,     1] loss: 997.817
[26,     1] loss: 1012.357
[27,     1] loss: 987.619
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008758959555599802,
 'learning_rate_Hydroxylation-K': 0.0050336230162499485,
 'learning_rate_Hydroxylation-P': 0.0069477665787982,
 'log_base': 2.053291291817453,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4007286254,
 'sample_weights': [1.913341144415181, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.545969157687745,
 'weight_decay_Hydroxylation-K': 7.2048535798025775,
 'weight_decay_Hydroxylation-P': 2.03900453075246}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.269
[2,     1] loss: 1394.992
[3,     1] loss: 1394.774
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005383523987424402,
 'learning_rate_Hydroxylation-K': 0.008105935442121553,
 'learning_rate_Hydroxylation-P': 0.001348781857592072,
 'log_base': 2.130144921027953,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3839780601,
 'sample_weights': [2.3204629063014797, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.880507492127894,
 'weight_decay_Hydroxylation-K': 7.914717771920228,
 'weight_decay_Hydroxylation-P': 8.159360122752238}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1374.822
[2,     1] loss: 1384.551
[3,     1] loss: 1376.271
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007051036398274897,
 'learning_rate_Hydroxylation-K': 0.0011843666648300843,
 'learning_rate_Hydroxylation-P': 0.0027638698523363113,
 'log_base': 1.1642972697809841,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2548250347,
 'sample_weights': [2.207703240348002, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.38740660914627,
 'weight_decay_Hydroxylation-K': 3.717797594836195,
 'weight_decay_Hydroxylation-P': 1.1315206829962976}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3583.812
[2,     1] loss: 3575.604
[3,     1] loss: 3555.052
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018977903202490353,
 'learning_rate_Hydroxylation-K': 0.0006419278626818854,
 'learning_rate_Hydroxylation-P': 0.006759813755121083,
 'log_base': 1.1147525711313175,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1458313269,
 'sample_weights': [10.974680219687473, 1.3718878760549724],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.839201058495764,
 'weight_decay_Hydroxylation-K': 5.304825914190569,
 'weight_decay_Hydroxylation-P': 2.8512547370011196}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4977.725
[2,     1] loss: 5001.657
[3,     1] loss: 4970.627
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007440685006457468,
 'learning_rate_Hydroxylation-K': 0.009111750759265201,
 'learning_rate_Hydroxylation-P': 0.00826956482855141,
 'log_base': 1.9818474837967406,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2835257686,
 'sample_weights': [15.367809755692553, 1.9210502231794706],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.4951635821434115,
 'weight_decay_Hydroxylation-K': 4.100900573644905,
 'weight_decay_Hydroxylation-P': 3.106844082168069}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1430.126
[2,     1] loss: 1424.540
[3,     1] loss: 1422.195
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002402766271603345,
 'learning_rate_Hydroxylation-K': 0.0005010734219541083,
 'learning_rate_Hydroxylation-P': 0.00557990308406832,
 'log_base': 1.7845001623870835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1040973720,
 'sample_weights': [2.440601159230992, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.392791966782656,
 'weight_decay_Hydroxylation-K': 8.351192843622535,
 'weight_decay_Hydroxylation-P': 1.9423927294768197}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1520.396
[2,     1] loss: 1516.574
[3,     1] loss: 1517.396
[4,     1] loss: 1516.482
[5,     1] loss: 1510.937
[6,     1] loss: 1513.959
[7,     1] loss: 1509.266
[8,     1] loss: 1503.540
[9,     1] loss: 1496.279
[10,     1] loss: 1479.671
[11,     1] loss: 1460.005
[12,     1] loss: 1427.206
[13,     1] loss: 1408.643
[14,     1] loss: 1379.641
[15,     1] loss: 1345.446
[16,     1] loss: 1334.313
[17,     1] loss: 1320.747
[18,     1] loss: 1286.714
[19,     1] loss: 1255.842
[20,     1] loss: 1281.631
[21,     1] loss: 1219.212
[22,     1] loss: 1237.484
[23,     1] loss: 1276.939
[24,     1] loss: 1329.178
[25,     1] loss: 1210.690
[26,     1] loss: 1319.951
[27,     1] loss: 1214.315
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006476460362628442,
 'learning_rate_Hydroxylation-K': 0.0015399886109613973,
 'learning_rate_Hydroxylation-P': 0.004455387386413006,
 'log_base': 1.0151604328504331,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1637962624,
 'sample_weights': [2.882632678640771, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.328505216760494,
 'weight_decay_Hydroxylation-K': 9.242442829901792,
 'weight_decay_Hydroxylation-P': 6.064374982947847}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36077.672
Exploding loss, terminate run (best metric=1.0961889028549194)
Finished Training
Total time taken: 0.21900033950805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36223.348
Exploding loss, terminate run (best metric=1.1028051376342773)
Finished Training
Total time taken: 0.20099949836730957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36074.273
Exploding loss, terminate run (best metric=1.0960276126861572)
Finished Training
Total time taken: 0.20900225639343262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35857.812
Exploding loss, terminate run (best metric=1.0713281631469727)
Finished Training
Total time taken: 0.2259984016418457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36045.504
Exploding loss, terminate run (best metric=1.0748515129089355)
Finished Training
Total time taken: 0.20399975776672363
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36440.375
Exploding loss, terminate run (best metric=1.098719596862793)
Finished Training
Total time taken: 0.20900273323059082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36072.273
Exploding loss, terminate run (best metric=1.098839521408081)
Finished Training
Total time taken: 0.20099902153015137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36274.715
Exploding loss, terminate run (best metric=1.0970287322998047)
Finished Training
Total time taken: 0.2089993953704834
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36023.926
Exploding loss, terminate run (best metric=1.0729494094848633)
Finished Training
Total time taken: 0.2199993133544922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36323.602
Exploding loss, terminate run (best metric=1.0743989944458008)
Finished Training
Total time taken: 0.21599960327148438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36174.977
Exploding loss, terminate run (best metric=1.0983308553695679)
Finished Training
Total time taken: 0.1979997158050537
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35707.098
Exploding loss, terminate run (best metric=1.1207318305969238)
Finished Training
Total time taken: 0.22600030899047852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36123.770
Exploding loss, terminate run (best metric=1.090606927871704)
Finished Training
Total time taken: 0.2259984016418457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36095.984
Exploding loss, terminate run (best metric=1.0766319036483765)
Finished Training
Total time taken: 0.20600032806396484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36006.012
Exploding loss, terminate run (best metric=1.0833067893981934)
Finished Training
Total time taken: 0.21200013160705566
{'Hydroxylation-K Validation Accuracy': 0.4493498817966903, 'Hydroxylation-K Validation Sensitivity': 0.6066666666666667, 'Hydroxylation-K Validation Specificity': 0.4105263157894737, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6021832358674464, 'Hydroxylation-K AUC PR': 0.31053421708036283, 'Hydroxylation-K MCC': 0.02355587734704561, 'Hydroxylation-K F1': 0.21482915005082495, 'Validation Loss (Hydroxylation-K)': 0.5583334883054097, 'Hydroxylation-P Validation Accuracy': 0.4382149975466558, 'Hydroxylation-P Validation Sensitivity': 0.608994708994709, 'Hydroxylation-P Validation Specificity': 0.401219512195122, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5556052512613937, 'Hydroxylation-P AUC PR': 0.24996151105083067, 'Hydroxylation-P MCC': 0.010350951755355487, 'Hydroxylation-P F1': 0.19918711071579803, 'Validation Loss (Hydroxylation-P)': 0.5318495710690816, 'Validation Loss (total)': 1.0901830593744914, 'TimeToTrain': 0.21213328043619792}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006717223307044599,
 'learning_rate_Hydroxylation-K': 0.00033148395691841136,
 'learning_rate_Hydroxylation-P': 0.0044359297222261425,
 'log_base': 1.3030277414778315,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4023716966,
 'sample_weights': [111.0333671358143, 13.850332427039374],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.211700025203742,
 'weight_decay_Hydroxylation-K': 7.956714632663274,
 'weight_decay_Hydroxylation-P': 7.347485981992522}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2234.557
[2,     1] loss: 2247.734
[3,     1] loss: 2236.586
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005195204928035204,
 'learning_rate_Hydroxylation-K': 0.0024925197104954493,
 'learning_rate_Hydroxylation-P': 0.004060248593019491,
 'log_base': 1.1526842378876658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 686867026,
 'sample_weights': [6.307149633181659, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.97304099795239,
 'weight_decay_Hydroxylation-K': 9.482614402626888,
 'weight_decay_Hydroxylation-P': 4.384217531500722}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3824.691
[2,     1] loss: 3809.896
[3,     1] loss: 3846.918
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005439209654452996,
 'learning_rate_Hydroxylation-K': 0.003605641895013395,
 'learning_rate_Hydroxylation-P': 0.004546327120497691,
 'log_base': 1.0224800840258221,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1155122179,
 'sample_weights': [11.748918834804892, 1.4686714312922136],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.3775808304819765,
 'weight_decay_Hydroxylation-K': 8.927676446448428,
 'weight_decay_Hydroxylation-P': 6.894075178793671}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24352.508
Exploding loss, terminate run (best metric=1.0963966846466064)
Finished Training
Total time taken: 0.2460029125213623
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24329.006
Exploding loss, terminate run (best metric=1.0928895473480225)
Finished Training
Total time taken: 0.19699978828430176
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24531.574
Exploding loss, terminate run (best metric=1.0931158065795898)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24368.617
Exploding loss, terminate run (best metric=1.0734889507293701)
Finished Training
Total time taken: 0.2240004539489746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24388.789
Exploding loss, terminate run (best metric=1.0843231678009033)
Finished Training
Total time taken: 0.23600149154663086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24442.051
Exploding loss, terminate run (best metric=1.0968650579452515)
Finished Training
Total time taken: 0.21900177001953125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24541.754
Exploding loss, terminate run (best metric=1.0934019088745117)
Finished Training
Total time taken: 0.23000097274780273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24354.340
Exploding loss, terminate run (best metric=1.0927987098693848)
Finished Training
Total time taken: 0.22699904441833496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24413.842
Exploding loss, terminate run (best metric=1.0722320079803467)
Finished Training
Total time taken: 0.22899961471557617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24392.908
Exploding loss, terminate run (best metric=1.0972323417663574)
Finished Training
Total time taken: 0.24000000953674316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24645.438
Exploding loss, terminate run (best metric=1.1237273216247559)
Finished Training
Total time taken: 0.2200002670288086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24525.137
Exploding loss, terminate run (best metric=1.0960544347763062)
Finished Training
Total time taken: 0.21300196647644043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24312.951
Exploding loss, terminate run (best metric=1.1034517288208008)
Finished Training
Total time taken: 0.2239983081817627
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24369.172
Exploding loss, terminate run (best metric=1.0733869075775146)
Finished Training
Total time taken: 0.21499991416931152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24510.188
Exploding loss, terminate run (best metric=1.072181224822998)
Finished Training
Total time taken: 0.19800043106079102
{'Hydroxylation-K Validation Accuracy': 0.4783096926713948, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6729434697855751, 'Hydroxylation-K AUC PR': 0.36216204617954423, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.17766830870279146, 'Validation Loss (Hydroxylation-K)': 0.558189841111501, 'Hydroxylation-P Validation Accuracy': 0.4780580342791398, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.571827114838487, 'Hydroxylation-P AUC PR': 0.27632418852857854, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16002571501225754, 'Validation Loss (Hydroxylation-P)': 0.5325798630714417, 'Validation Loss (total)': 1.0907697200775146, 'TimeToTrain': 0.221467129389445}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007137714760471883,
 'learning_rate_Hydroxylation-K': 0.0032433475989905847,
 'learning_rate_Hydroxylation-P': 0.0006981966858800967,
 'log_base': 1.0502525962317706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3542154111,
 'sample_weights': [75.15054165211667, 9.374299013023004],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.85104573619685,
 'weight_decay_Hydroxylation-K': 2.1188114083509584,
 'weight_decay_Hydroxylation-P': 5.352488773448339}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11084.459
[2,     1] loss: 11069.182
[3,     1] loss: 11065.758
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012477872563836877,
 'learning_rate_Hydroxylation-K': 0.0034772583831527096,
 'learning_rate_Hydroxylation-P': 0.0021280077312283044,
 'log_base': 1.081869657636843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 434518357,
 'sample_weights': [34.048933477211634, 4.256280647357133],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.230175016984536,
 'weight_decay_Hydroxylation-K': 8.542883084657676,
 'weight_decay_Hydroxylation-P': 0.8070426633711296}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6899.507
[2,     1] loss: 6864.003
[3,     1] loss: 6870.124
[4,     1] loss: 6872.155
[5,     1] loss: 6875.074
[6,     1] loss: 6886.846
[7,     1] loss: 6868.575
[8,     1] loss: 6865.357
[9,     1] loss: 6852.946
[10,     1] loss: 6867.398
[11,     1] loss: 6866.146
[12,     1] loss: 6832.481
[13,     1] loss: 6839.865
[14,     1] loss: 6802.032
[15,     1] loss: 6813.480
[16,     1] loss: 6752.713
[17,     1] loss: 6720.236
[18,     1] loss: 6562.150
[19,     1] loss: 6511.858
[20,     1] loss: 6474.967
[21,     1] loss: 6330.312
[22,     1] loss: 6334.283
[23,     1] loss: 6108.221
[24,     1] loss: 6127.735
[25,     1] loss: 5857.517
[26,     1] loss: 5823.602
[27,     1] loss: 5843.001
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005155409834962476,
 'learning_rate_Hydroxylation-K': 0.0021284893115488474,
 'learning_rate_Hydroxylation-P': 0.0039244765380393645,
 'log_base': 1.07958377488589,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3556936306,
 'sample_weights': [21.215251091985824, 2.652008548581308],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.851127708010884,
 'weight_decay_Hydroxylation-K': 8.241833654869811,
 'weight_decay_Hydroxylation-P': 4.715564089618351}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7081.808
[2,     1] loss: 7069.880
[3,     1] loss: 7072.713
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006744894168035753,
 'learning_rate_Hydroxylation-K': 0.0004992120268046383,
 'learning_rate_Hydroxylation-P': 0.00035843583943768537,
 'log_base': 1.0120946145855139,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3318281831,
 'sample_weights': [21.801249106379878, 2.725261122255148],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.454355019279463,
 'weight_decay_Hydroxylation-K': 9.9075401312135,
 'weight_decay_Hydroxylation-P': 4.234903916584592}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44996.035
Exploding loss, terminate run (best metric=1.0956649780273438)
Finished Training
Total time taken: 0.2370009422302246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45236.602
Exploding loss, terminate run (best metric=1.0927374362945557)
Finished Training
Total time taken: 0.20000100135803223
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45000.445
Exploding loss, terminate run (best metric=1.0940332412719727)
Finished Training
Total time taken: 0.2109992504119873
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45215.062
Exploding loss, terminate run (best metric=1.080886960029602)
Finished Training
Total time taken: 0.23400044441223145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 45114.863
Exploding loss, terminate run (best metric=1.0762951374053955)
Finished Training
Total time taken: 0.22800230979919434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45086.340
Exploding loss, terminate run (best metric=1.0955779552459717)
Finished Training
Total time taken: 0.2050008773803711
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44933.984
Exploding loss, terminate run (best metric=1.1016762256622314)
Finished Training
Total time taken: 0.2109992504119873
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45324.523
Exploding loss, terminate run (best metric=1.0923137664794922)
Finished Training
Total time taken: 0.2219996452331543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45259.195
Exploding loss, terminate run (best metric=1.0924112796783447)
Finished Training
Total time taken: 0.20299816131591797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 45336.660
Exploding loss, terminate run (best metric=1.0794336795806885)
Finished Training
Total time taken: 0.21300196647644043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45274.484
Exploding loss, terminate run (best metric=1.0960702896118164)
Finished Training
Total time taken: 0.20299839973449707
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45133.027
Exploding loss, terminate run (best metric=1.1041744947433472)
Finished Training
Total time taken: 0.2089996337890625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44996.602
Exploding loss, terminate run (best metric=1.1083581447601318)
Finished Training
Total time taken: 0.2330002784729004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45156.977
Exploding loss, terminate run (best metric=1.0704338550567627)
Finished Training
Total time taken: 0.21400189399719238
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44877.594
Exploding loss, terminate run (best metric=1.1122103929519653)
Finished Training
Total time taken: 0.20399928092956543
{'Hydroxylation-K Validation Accuracy': 0.6017139479905437, 'Hydroxylation-K Validation Sensitivity': 0.3466666666666667, 'Hydroxylation-K Validation Specificity': 0.6631578947368421, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6193567251461988, 'Hydroxylation-K AUC PR': 0.30301021416413176, 'Hydroxylation-K MCC': 0.01443605858076074, 'Hydroxylation-K F1': 0.13243021346469624, 'Validation Loss (Hydroxylation-K)': 0.5591765840848287, 'Hydroxylation-P Validation Accuracy': 0.6042545048474697, 'Hydroxylation-P Validation Sensitivity': 0.35555555555555557, 'Hydroxylation-P Validation Specificity': 0.6577235772357723, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6204094274582828, 'Hydroxylation-P AUC PR': 0.29480284678926144, 'Hydroxylation-P MCC': 0.017982119231173734, 'Hydroxylation-P F1': 0.1251884622037322, 'Validation Loss (Hydroxylation-P)': 0.5336419383684794, 'Validation Loss (total)': 1.092818522453308, 'TimeToTrain': 0.21513355573018392}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007465879015139696,
 'learning_rate_Hydroxylation-K': 0.0007690244793607719,
 'learning_rate_Hydroxylation-P': 0.0005856781071605359,
 'log_base': 1.2453478709235986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1519255940,
 'sample_weights': [138.96800159218233, 17.3349063297228],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.704161759958607,
 'weight_decay_Hydroxylation-K': 9.042314019470831,
 'weight_decay_Hydroxylation-P': 5.3858050575466}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2530.899
[2,     1] loss: 2513.748
[3,     1] loss: 2522.867
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025801613896956135,
 'learning_rate_Hydroxylation-K': 0.007576294083663247,
 'learning_rate_Hydroxylation-P': 0.006109178186119985,
 'log_base': 2.451653626576037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 399841019,
 'sample_weights': [7.608613212768205, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.206422552185369,
 'weight_decay_Hydroxylation-K': 6.175130871944254,
 'weight_decay_Hydroxylation-P': 3.7551614846527954}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.631
[2,     1] loss: 1302.346
[3,     1] loss: 1299.478
[4,     1] loss: 1299.909
[5,     1] loss: 1294.200
[6,     1] loss: 1295.900
[7,     1] loss: 1286.988
[8,     1] loss: 1277.509
[9,     1] loss: 1261.107
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030736067030548144,
 'learning_rate_Hydroxylation-K': 0.002051901588371078,
 'learning_rate_Hydroxylation-P': 0.0048733938459432965,
 'log_base': 1.315683637606202,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3689011944,
 'sample_weights': [1.8616330282819638, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4344799420798466,
 'weight_decay_Hydroxylation-K': 3.8985085335854466,
 'weight_decay_Hydroxylation-P': 7.741764959358479}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2197.638
[2,     1] loss: 2194.103
[3,     1] loss: 2197.813
[4,     1] loss: 2185.381
[5,     1] loss: 2186.170
[6,     1] loss: 2186.748
[7,     1] loss: 2191.446
[8,     1] loss: 2188.362
[9,     1] loss: 2185.069
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005558865059958859,
 'learning_rate_Hydroxylation-K': 0.004141906439948137,
 'learning_rate_Hydroxylation-P': 0.001895141989843677,
 'log_base': 1.795483870928602,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3689131050,
 'sample_weights': [6.084943178073536, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.050061354808614955,
 'weight_decay_Hydroxylation-K': 3.5672858839540527,
 'weight_decay_Hydroxylation-P': 2.8378606071535253}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1517.809
[2,     1] loss: 1514.326
[3,     1] loss: 1509.616
[4,     1] loss: 1510.870
[5,     1] loss: 1507.507
[6,     1] loss: 1504.134
[7,     1] loss: 1505.000
[8,     1] loss: 1496.138
[9,     1] loss: 1491.684
[10,     1] loss: 1466.769
[11,     1] loss: 1460.028
[12,     1] loss: 1435.556
[13,     1] loss: 1396.827
[14,     1] loss: 1356.980
[15,     1] loss: 1345.381
[16,     1] loss: 1290.426
[17,     1] loss: 1302.859
[18,     1] loss: 1321.111
[19,     1] loss: 1287.713
[20,     1] loss: 1319.665
[21,     1] loss: 1238.958
[22,     1] loss: 1261.312
[23,     1] loss: 1252.470
[24,     1] loss: 1312.809
[25,     1] loss: 1218.228
[26,     1] loss: 1248.391
[27,     1] loss: 1267.471
[28,     1] loss: 1214.230
[29,     1] loss: 1257.731
[30,     1] loss: 1259.577
[31,     1] loss: 1215.498
[32,     1] loss: 1185.271
[33,     1] loss: 1167.739
[34,     1] loss: 1200.437
[35,     1] loss: 1178.833
[36,     1] loss: 1158.634
[37,     1] loss: 1175.928
[38,     1] loss: 1131.501
[39,     1] loss: 1192.253
[40,     1] loss: 1152.179
[41,     1] loss: 1188.589
[42,     1] loss: 1077.302
[43,     1] loss: 1114.041
[44,     1] loss: 1115.508
[45,     1] loss: 1156.723
[46,     1] loss: 1082.701
[47,     1] loss: 1099.026
[48,     1] loss: 1130.562
[49,     1] loss: 1143.090
[50,     1] loss: 1117.076
[51,     1] loss: 1071.093
[52,     1] loss: 1070.259
[53,     1] loss: 1089.770
[54,     1] loss: 1096.000
[55,     1] loss: 1086.183
[56,     1] loss: 1093.810
[57,     1] loss: 1087.462
[58,     1] loss: 1092.746
[59,     1] loss: 1093.792
[60,     1] loss: 992.028
[61,     1] loss: 1035.930
[62,     1] loss: 1035.604
[63,     1] loss: 1076.439
[64,     1] loss: 1049.055
[65,     1] loss: 1017.297
[66,     1] loss: 1022.649
[67,     1] loss: 1103.146
[68,     1] loss: 1068.833
[69,     1] loss: 1053.040
[70,     1] loss: 1126.964
[71,     1] loss: 1001.800
[72,     1] loss: 1073.756
[73,     1] loss: 1002.382
[74,     1] loss: 987.750
[75,     1] loss: 984.312
[76,     1] loss: 1063.117
[77,     1] loss: 1043.016
[78,     1] loss: 987.231
[79,     1] loss: 998.133
[80,     1] loss: 991.278
[81,     1] loss: 954.816
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054494010110374995,
 'learning_rate_Hydroxylation-K': 0.007620651693649615,
 'learning_rate_Hydroxylation-P': 0.0054144464895806415,
 'log_base': 1.6559929292182982,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4222986251,
 'sample_weights': [2.8524102797329216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1846547127335434,
 'weight_decay_Hydroxylation-K': 1.3523533616520949,
 'weight_decay_Hydroxylation-P': 8.145198496056482}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1622.872
[2,     1] loss: 1600.752
[3,     1] loss: 1605.008
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005175863530116305,
 'learning_rate_Hydroxylation-K': 0.006186896481274168,
 'learning_rate_Hydroxylation-P': 0.004997104679507607,
 'log_base': 2.551820585845346,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3515277788,
 'sample_weights': [3.309755244711179, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.177841144877107,
 'weight_decay_Hydroxylation-K': 6.607953832640631,
 'weight_decay_Hydroxylation-P': 0.7537933181154628}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.776
[2,     1] loss: 1285.788
[3,     1] loss: 1285.935
[4,     1] loss: 1278.899
[5,     1] loss: 1284.406
[6,     1] loss: 1288.940
[7,     1] loss: 1279.978
[8,     1] loss: 1278.687
[9,     1] loss: 1279.113
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005329508057823426,
 'learning_rate_Hydroxylation-K': 0.004358819621940117,
 'learning_rate_Hydroxylation-P': 0.006624980062106338,
 'log_base': 1.0579991416853671,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 139879935,
 'sample_weights': [1.782056540042873, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.963237323847688,
 'weight_decay_Hydroxylation-K': 9.199046640669414,
 'weight_decay_Hydroxylation-P': 5.380843993380845}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9675.149
[2,     1] loss: 9641.324
[3,     1] loss: 9639.574
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005733095102066188,
 'learning_rate_Hydroxylation-K': 0.0014672478509261838,
 'learning_rate_Hydroxylation-P': 0.0008965010628194888,
 'log_base': 1.2387211999203533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3251295355,
 'sample_weights': [29.610806956862604, 3.701493460506575],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.929156166047425,
 'weight_decay_Hydroxylation-K': 9.937529929273799,
 'weight_decay_Hydroxylation-P': 3.2400804734441473}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2552.092
[2,     1] loss: 2546.660
[3,     1] loss: 2546.984
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00590350383686427,
 'learning_rate_Hydroxylation-K': 0.005893938005937583,
 'learning_rate_Hydroxylation-P': 0.004575083461495769,
 'log_base': 1.2602962871836756,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 859952545,
 'sample_weights': [7.798237114094126, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.17685068873446,
 'weight_decay_Hydroxylation-K': 8.05662713473045,
 'weight_decay_Hydroxylation-P': 7.878937295815937}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2434.761
[2,     1] loss: 2432.376
[3,     1] loss: 2432.084
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0060571061419116305,
 'learning_rate_Hydroxylation-K': 0.0012674058903208612,
 'learning_rate_Hydroxylation-P': 0.0003275142400575229,
 'log_base': 1.087877991073505,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2893824571,
 'sample_weights': [7.216191644831219, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.7842151446366294,
 'weight_decay_Hydroxylation-K': 3.0813846738137354,
 'weight_decay_Hydroxylation-P': 3.979719546428588}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6422.901
[2,     1] loss: 6437.549
[3,     1] loss: 6420.376
[4,     1] loss: 6436.835
[5,     1] loss: 6422.612
[6,     1] loss: 6449.304
[7,     1] loss: 6422.329
[8,     1] loss: 6431.970
[9,     1] loss: 6418.071
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005930518830217903,
 'learning_rate_Hydroxylation-K': 0.005395563309609582,
 'learning_rate_Hydroxylation-P': 0.008276523398986905,
 'log_base': 2.234235587138448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 517405752,
 'sample_weights': [19.820288925991107, 2.4776315603889327],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.719726208017459,
 'weight_decay_Hydroxylation-K': 9.13915501766499,
 'weight_decay_Hydroxylation-P': 4.605725069925807}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.359
[2,     1] loss: 1349.081
[3,     1] loss: 1347.723
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026944070010768203,
 'learning_rate_Hydroxylation-K': 0.002404897047760452,
 'learning_rate_Hydroxylation-P': 0.006563960697434746,
 'log_base': 1.9484548593647648,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 142704214,
 'sample_weights': [2.076682313473831, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.815202677962147,
 'weight_decay_Hydroxylation-K': 7.672780309630339,
 'weight_decay_Hydroxylation-P': 5.5902272814289535}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1438.870
[2,     1] loss: 1437.772
[3,     1] loss: 1436.493
[4,     1] loss: 1434.514
[5,     1] loss: 1435.004
[6,     1] loss: 1434.597
[7,     1] loss: 1431.122
[8,     1] loss: 1432.813
[9,     1] loss: 1429.805
[10,     1] loss: 1426.654
[11,     1] loss: 1424.028
[12,     1] loss: 1418.881
[13,     1] loss: 1413.670
[14,     1] loss: 1392.513
[15,     1] loss: 1379.927
[16,     1] loss: 1350.362
[17,     1] loss: 1340.638
[18,     1] loss: 1300.512
[19,     1] loss: 1282.894
[20,     1] loss: 1255.094
[21,     1] loss: 1251.859
[22,     1] loss: 1220.612
[23,     1] loss: 1193.637
[24,     1] loss: 1231.767
[25,     1] loss: 1176.566
[26,     1] loss: 1154.740
[27,     1] loss: 1200.452
[28,     1] loss: 1123.459
[29,     1] loss: 1175.512
[30,     1] loss: 1130.614
[31,     1] loss: 1111.958
[32,     1] loss: 1092.191
[33,     1] loss: 1095.618
[34,     1] loss: 1078.425
[35,     1] loss: 1150.937
[36,     1] loss: 1093.025
[37,     1] loss: 1085.950
[38,     1] loss: 1079.278
[39,     1] loss: 1029.394
[40,     1] loss: 991.682
[41,     1] loss: 1088.824
[42,     1] loss: 995.151
[43,     1] loss: 964.287
[44,     1] loss: 988.302
[45,     1] loss: 964.412
[46,     1] loss: 883.915
[47,     1] loss: 906.584
[48,     1] loss: 948.970
[49,     1] loss: 945.391
[50,     1] loss: 922.338
[51,     1] loss: 1005.005
[52,     1] loss: 930.216
[53,     1] loss: 825.414
[54,     1] loss: 900.062
[55,     1] loss: 858.111
[56,     1] loss: 868.360
[57,     1] loss: 890.844
[58,     1] loss: 912.493
[59,     1] loss: 749.734
[60,     1] loss: 797.501
[61,     1] loss: 945.232
[62,     1] loss: 890.519
[63,     1] loss: 793.730
[64,     1] loss: 843.099
[65,     1] loss: 754.234
[66,     1] loss: 803.417
[67,     1] loss: 749.169
[68,     1] loss: 724.320
Early stopping applied (best metric=0.7662242650985718)
Finished Training
Total time taken: 9.718008995056152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1436.762
[2,     1] loss: 1436.183
[3,     1] loss: 1434.519
[4,     1] loss: 1433.639
[5,     1] loss: 1434.688
[6,     1] loss: 1431.020
[7,     1] loss: 1430.753
[8,     1] loss: 1425.603
[9,     1] loss: 1416.093
[10,     1] loss: 1402.899
[11,     1] loss: 1392.023
[12,     1] loss: 1361.925
[13,     1] loss: 1335.251
[14,     1] loss: 1311.853
[15,     1] loss: 1284.785
[16,     1] loss: 1284.408
[17,     1] loss: 1226.742
[18,     1] loss: 1274.233
[19,     1] loss: 1223.085
[20,     1] loss: 1180.306
[21,     1] loss: 1201.825
[22,     1] loss: 1192.973
[23,     1] loss: 1205.370
[24,     1] loss: 1210.431
[25,     1] loss: 1160.062
[26,     1] loss: 1198.023
[27,     1] loss: 1144.921
[28,     1] loss: 1162.898
[29,     1] loss: 1146.730
[30,     1] loss: 1136.442
[31,     1] loss: 1095.051
[32,     1] loss: 1085.776
[33,     1] loss: 1082.890
[34,     1] loss: 1127.685
[35,     1] loss: 1066.211
[36,     1] loss: 1109.913
[37,     1] loss: 1015.383
[38,     1] loss: 1057.618
[39,     1] loss: 1056.808
[40,     1] loss: 1101.295
[41,     1] loss: 956.242
[42,     1] loss: 998.941
[43,     1] loss: 999.963
[44,     1] loss: 998.801
[45,     1] loss: 995.659
[46,     1] loss: 949.983
[47,     1] loss: 946.716
[48,     1] loss: 1018.741
[49,     1] loss: 922.704
[50,     1] loss: 925.230
[51,     1] loss: 869.556
[52,     1] loss: 898.996
[53,     1] loss: 843.215
[54,     1] loss: 891.048
[55,     1] loss: 962.172
[56,     1] loss: 1226.529
[57,     1] loss: 905.520
[58,     1] loss: 1049.600
[59,     1] loss: 908.286
[60,     1] loss: 981.530
[61,     1] loss: 892.754
[62,     1] loss: 881.777
[63,     1] loss: 858.561
Early stopping applied (best metric=0.7588902711868286)
Finished Training
Total time taken: 10.573010444641113
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1438.268
[2,     1] loss: 1436.708
[3,     1] loss: 1438.337
[4,     1] loss: 1434.459
[5,     1] loss: 1433.267
[6,     1] loss: 1435.830
[7,     1] loss: 1430.389
[8,     1] loss: 1426.844
[9,     1] loss: 1427.255
[10,     1] loss: 1416.720
[11,     1] loss: 1402.274
[12,     1] loss: 1370.871
[13,     1] loss: 1341.234
[14,     1] loss: 1320.119
[15,     1] loss: 1315.067
[16,     1] loss: 1271.229
[17,     1] loss: 1273.124
[18,     1] loss: 1257.282
[19,     1] loss: 1229.164
[20,     1] loss: 1192.008
[21,     1] loss: 1184.139
[22,     1] loss: 1172.755
[23,     1] loss: 1157.700
[24,     1] loss: 1197.207
[25,     1] loss: 1135.362
[26,     1] loss: 1115.996
[27,     1] loss: 1129.031
[28,     1] loss: 1137.890
[29,     1] loss: 1122.579
[30,     1] loss: 1035.961
[31,     1] loss: 1081.288
[32,     1] loss: 1067.609
[33,     1] loss: 1059.430
[34,     1] loss: 1088.010
[35,     1] loss: 1053.735
[36,     1] loss: 1005.363
[37,     1] loss: 971.211
[38,     1] loss: 1027.135
[39,     1] loss: 923.353
[40,     1] loss: 970.714
[41,     1] loss: 990.772
[42,     1] loss: 890.100
[43,     1] loss: 885.284
[44,     1] loss: 865.753
[45,     1] loss: 897.058
[46,     1] loss: 830.657
[47,     1] loss: 902.613
[48,     1] loss: 1017.409
[49,     1] loss: 991.385
[50,     1] loss: 853.433
[51,     1] loss: 827.488
[52,     1] loss: 891.011
[53,     1] loss: 851.069
Early stopping applied (best metric=0.7424461841583252)
Finished Training
Total time taken: 7.235006809234619
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1438.888
[2,     1] loss: 1437.532
[3,     1] loss: 1444.005
[4,     1] loss: 1439.710
[5,     1] loss: 1436.344
[6,     1] loss: 1437.462
[7,     1] loss: 1434.687
[8,     1] loss: 1431.732
[9,     1] loss: 1431.945
[10,     1] loss: 1429.842
[11,     1] loss: 1425.483
[12,     1] loss: 1420.421
[13,     1] loss: 1407.269
[14,     1] loss: 1394.958
[15,     1] loss: 1376.098
[16,     1] loss: 1335.863
[17,     1] loss: 1303.707
[18,     1] loss: 1291.485
[19,     1] loss: 1264.276
[20,     1] loss: 1265.055
[21,     1] loss: 1243.748
[22,     1] loss: 1246.830
[23,     1] loss: 1201.425
[24,     1] loss: 1204.109
[25,     1] loss: 1205.513
[26,     1] loss: 1242.175
[27,     1] loss: 1143.572
[28,     1] loss: 1160.569
[29,     1] loss: 1139.682
[30,     1] loss: 1145.270
[31,     1] loss: 1134.542
[32,     1] loss: 1122.195
[33,     1] loss: 1087.282
[34,     1] loss: 1141.117
[35,     1] loss: 1120.477
[36,     1] loss: 1149.503
[37,     1] loss: 1106.594
[38,     1] loss: 1120.217
[39,     1] loss: 1067.366
[40,     1] loss: 1041.510
[41,     1] loss: 1050.687
[42,     1] loss: 1088.307
[43,     1] loss: 1014.171
[44,     1] loss: 1049.561
[45,     1] loss: 975.406
[46,     1] loss: 951.774
[47,     1] loss: 965.064
[48,     1] loss: 952.317
[49,     1] loss: 938.830
[50,     1] loss: 1023.139
[51,     1] loss: 1053.673
[52,     1] loss: 914.523
[53,     1] loss: 951.152
[54,     1] loss: 886.893
[55,     1] loss: 913.474
[56,     1] loss: 973.269
[57,     1] loss: 937.679
[58,     1] loss: 844.684
[59,     1] loss: 882.589
[60,     1] loss: 906.852
[61,     1] loss: 803.473
[62,     1] loss: 801.396
[63,     1] loss: 878.724
[64,     1] loss: 914.314
[65,     1] loss: 795.888
[66,     1] loss: 876.292
[67,     1] loss: 941.271
[68,     1] loss: 838.246
[69,     1] loss: 1049.842
Early stopping applied (best metric=0.5951484441757202)
Finished Training
Total time taken: 9.625009298324585
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1439.072
[2,     1] loss: 1439.557
[3,     1] loss: 1442.210
[4,     1] loss: 1438.316
[5,     1] loss: 1437.561
[6,     1] loss: 1437.099
[7,     1] loss: 1432.861
[8,     1] loss: 1430.282
[9,     1] loss: 1425.490
[10,     1] loss: 1416.349
[11,     1] loss: 1400.664
[12,     1] loss: 1381.779
[13,     1] loss: 1357.771
[14,     1] loss: 1326.709
[15,     1] loss: 1292.518
[16,     1] loss: 1263.510
[17,     1] loss: 1248.616
[18,     1] loss: 1240.537
[19,     1] loss: 1196.841
[20,     1] loss: 1191.083
[21,     1] loss: 1169.319
[22,     1] loss: 1222.111
[23,     1] loss: 1186.032
[24,     1] loss: 1158.714
[25,     1] loss: 1168.251
[26,     1] loss: 1148.740
[27,     1] loss: 1137.490
[28,     1] loss: 1126.265
[29,     1] loss: 1132.830
[30,     1] loss: 1112.209
[31,     1] loss: 1097.050
[32,     1] loss: 1092.873
[33,     1] loss: 1115.226
[34,     1] loss: 1101.247
[35,     1] loss: 1062.093
[36,     1] loss: 1089.052
[37,     1] loss: 1106.029
[38,     1] loss: 1092.760
[39,     1] loss: 1096.188
[40,     1] loss: 1031.138
[41,     1] loss: 1022.936
[42,     1] loss: 1050.117
[43,     1] loss: 1005.977
[44,     1] loss: 1008.146
[45,     1] loss: 1029.220
[46,     1] loss: 1015.694
[47,     1] loss: 994.973
[48,     1] loss: 1009.034
[49,     1] loss: 1019.378
[50,     1] loss: 981.892
[51,     1] loss: 933.425
[52,     1] loss: 913.327
[53,     1] loss: 865.563
[54,     1] loss: 935.482
[55,     1] loss: 906.208
[56,     1] loss: 919.898
[57,     1] loss: 866.094
[58,     1] loss: 876.045
[59,     1] loss: 1056.118
[60,     1] loss: 961.005
[61,     1] loss: 834.799
[62,     1] loss: 935.443
[63,     1] loss: 907.398
[64,     1] loss: 881.329
[65,     1] loss: 885.007
[66,     1] loss: 851.784
[67,     1] loss: 826.297
[68,     1] loss: 800.599
Early stopping applied (best metric=0.7838214635848999)
Finished Training
Total time taken: 11.345011234283447
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1448.498
[2,     1] loss: 1439.896
[3,     1] loss: 1437.530
[4,     1] loss: 1433.547
[5,     1] loss: 1435.448
[6,     1] loss: 1437.094
[7,     1] loss: 1434.547
[8,     1] loss: 1434.535
[9,     1] loss: 1430.395
[10,     1] loss: 1429.663
[11,     1] loss: 1427.222
[12,     1] loss: 1425.037
[13,     1] loss: 1419.127
[14,     1] loss: 1410.072
[15,     1] loss: 1395.269
[16,     1] loss: 1366.251
[17,     1] loss: 1349.968
[18,     1] loss: 1322.581
[19,     1] loss: 1273.042
[20,     1] loss: 1258.741
[21,     1] loss: 1234.010
[22,     1] loss: 1212.101
[23,     1] loss: 1192.847
[24,     1] loss: 1166.760
[25,     1] loss: 1200.860
[26,     1] loss: 1166.481
[27,     1] loss: 1216.472
[28,     1] loss: 1104.573
[29,     1] loss: 1122.776
[30,     1] loss: 1172.240
[31,     1] loss: 1092.678
[32,     1] loss: 1083.398
[33,     1] loss: 1106.182
[34,     1] loss: 1081.334
[35,     1] loss: 1099.853
[36,     1] loss: 1045.652
[37,     1] loss: 1073.391
[38,     1] loss: 1007.232
[39,     1] loss: 1019.188
[40,     1] loss: 959.583
[41,     1] loss: 975.420
[42,     1] loss: 960.935
[43,     1] loss: 950.319
[44,     1] loss: 946.366
[45,     1] loss: 953.822
[46,     1] loss: 871.277
[47,     1] loss: 966.411
[48,     1] loss: 1169.409
[49,     1] loss: 898.577
[50,     1] loss: 956.728
[51,     1] loss: 926.696
[52,     1] loss: 920.212
[53,     1] loss: 843.048
[54,     1] loss: 1003.463
[55,     1] loss: 855.544
[56,     1] loss: 898.678
[57,     1] loss: 917.696
[58,     1] loss: 848.133
Early stopping applied (best metric=0.8935083746910095)
Finished Training
Total time taken: 9.728009462356567
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1439.584
[2,     1] loss: 1436.409
[3,     1] loss: 1433.399
[4,     1] loss: 1435.854
[5,     1] loss: 1435.282
[6,     1] loss: 1434.040
[7,     1] loss: 1438.189
[8,     1] loss: 1433.312
[9,     1] loss: 1431.070
[10,     1] loss: 1427.566
[11,     1] loss: 1423.765
[12,     1] loss: 1417.320
[13,     1] loss: 1399.778
[14,     1] loss: 1388.834
[15,     1] loss: 1353.354
[16,     1] loss: 1332.623
[17,     1] loss: 1295.401
[18,     1] loss: 1299.714
[19,     1] loss: 1218.908
[20,     1] loss: 1223.753
[21,     1] loss: 1195.943
[22,     1] loss: 1221.721
[23,     1] loss: 1187.438
[24,     1] loss: 1166.443
[25,     1] loss: 1139.594
[26,     1] loss: 1242.467
[27,     1] loss: 1115.481
[28,     1] loss: 1075.239
[29,     1] loss: 1159.753
[30,     1] loss: 1111.600
[31,     1] loss: 1153.266
[32,     1] loss: 1126.114
[33,     1] loss: 1161.475
[34,     1] loss: 1108.110
[35,     1] loss: 1045.792
[36,     1] loss: 1127.688
[37,     1] loss: 1001.534
[38,     1] loss: 1065.991
[39,     1] loss: 1056.011
[40,     1] loss: 1056.349
[41,     1] loss: 1057.492
[42,     1] loss: 1060.798
[43,     1] loss: 996.935
[44,     1] loss: 962.380
[45,     1] loss: 963.828
[46,     1] loss: 1004.873
[47,     1] loss: 952.622
Early stopping applied (best metric=0.8957688212394714)
Finished Training
Total time taken: 7.504007577896118
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1430.903
[2,     1] loss: 1428.329
[3,     1] loss: 1438.750
[4,     1] loss: 1436.979
[5,     1] loss: 1435.251
[6,     1] loss: 1436.494
[7,     1] loss: 1432.453
[8,     1] loss: 1428.781
[9,     1] loss: 1425.171
[10,     1] loss: 1413.492
[11,     1] loss: 1397.781
[12,     1] loss: 1376.825
[13,     1] loss: 1353.368
[14,     1] loss: 1310.766
[15,     1] loss: 1285.629
[16,     1] loss: 1263.872
[17,     1] loss: 1237.575
[18,     1] loss: 1212.186
[19,     1] loss: 1249.215
[20,     1] loss: 1210.137
[21,     1] loss: 1278.769
[22,     1] loss: 1121.569
[23,     1] loss: 1212.353
[24,     1] loss: 1117.473
[25,     1] loss: 1166.019
[26,     1] loss: 1203.093
[27,     1] loss: 1127.985
[28,     1] loss: 1142.797
[29,     1] loss: 1128.657
[30,     1] loss: 1105.105
[31,     1] loss: 1100.616
[32,     1] loss: 1100.587
[33,     1] loss: 1095.914
[34,     1] loss: 1004.525
[35,     1] loss: 1071.045
[36,     1] loss: 1060.132
[37,     1] loss: 1114.527
[38,     1] loss: 984.803
[39,     1] loss: 1000.708
[40,     1] loss: 1034.221
[41,     1] loss: 926.633
[42,     1] loss: 959.804
[43,     1] loss: 969.159
[44,     1] loss: 988.172
[45,     1] loss: 869.813
[46,     1] loss: 985.539
[47,     1] loss: 962.765
[48,     1] loss: 981.404
Early stopping applied (best metric=0.8607832789421082)
Finished Training
Total time taken: 6.930006980895996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1433.724
[2,     1] loss: 1436.630
[3,     1] loss: 1432.581
[4,     1] loss: 1439.351
[5,     1] loss: 1431.956
[6,     1] loss: 1429.973
[7,     1] loss: 1425.569
[8,     1] loss: 1419.304
[9,     1] loss: 1404.053
[10,     1] loss: 1384.242
[11,     1] loss: 1364.014
[12,     1] loss: 1337.504
[13,     1] loss: 1297.712
[14,     1] loss: 1285.979
[15,     1] loss: 1227.135
[16,     1] loss: 1220.228
[17,     1] loss: 1196.508
[18,     1] loss: 1199.237
[19,     1] loss: 1203.535
[20,     1] loss: 1182.016
[21,     1] loss: 1172.685
[22,     1] loss: 1159.162
[23,     1] loss: 1158.393
[24,     1] loss: 1105.767
[25,     1] loss: 1086.290
[26,     1] loss: 1078.962
[27,     1] loss: 1093.337
[28,     1] loss: 1093.474
[29,     1] loss: 1117.497
[30,     1] loss: 1056.626
[31,     1] loss: 1081.862
[32,     1] loss: 1095.757
[33,     1] loss: 1083.475
[34,     1] loss: 1053.985
[35,     1] loss: 1049.687
[36,     1] loss: 1032.776
[37,     1] loss: 1055.648
[38,     1] loss: 1013.087
[39,     1] loss: 989.819
[40,     1] loss: 984.494
[41,     1] loss: 965.231
[42,     1] loss: 956.603
[43,     1] loss: 941.217
[44,     1] loss: 969.074
[45,     1] loss: 945.137
[46,     1] loss: 961.175
[47,     1] loss: 1015.578
[48,     1] loss: 947.824
[49,     1] loss: 930.961
[50,     1] loss: 926.616
[51,     1] loss: 849.030
[52,     1] loss: 880.872
[53,     1] loss: 864.051
[54,     1] loss: 926.354
[55,     1] loss: 799.112
[56,     1] loss: 812.252
[57,     1] loss: 864.384
[58,     1] loss: 786.752
[59,     1] loss: 889.908
[60,     1] loss: 749.780
[61,     1] loss: 826.106
[62,     1] loss: 754.140
[63,     1] loss: 732.759
[64,     1] loss: 708.563
[65,     1] loss: 700.323
[66,     1] loss: 764.743
[67,     1] loss: 701.351
[68,     1] loss: 852.800
[69,     1] loss: 830.557
[70,     1] loss: 715.953
[71,     1] loss: 842.358
[72,     1] loss: 794.946
[73,     1] loss: 670.730
[74,     1] loss: 726.201
[75,     1] loss: 712.273
Early stopping applied (best metric=0.7151951789855957)
Finished Training
Total time taken: 10.290010213851929
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1443.326
[2,     1] loss: 1437.749
[3,     1] loss: 1438.758
[4,     1] loss: 1433.634
[5,     1] loss: 1437.225
[6,     1] loss: 1437.744
[7,     1] loss: 1433.423
[8,     1] loss: 1430.915
[9,     1] loss: 1425.873
[10,     1] loss: 1413.326
[11,     1] loss: 1402.994
[12,     1] loss: 1374.760
[13,     1] loss: 1344.278
[14,     1] loss: 1332.655
[15,     1] loss: 1272.079
[16,     1] loss: 1257.851
[17,     1] loss: 1212.818
[18,     1] loss: 1201.432
[19,     1] loss: 1186.786
[20,     1] loss: 1163.655
[21,     1] loss: 1186.608
[22,     1] loss: 1134.490
[23,     1] loss: 1220.229
[24,     1] loss: 1140.121
[25,     1] loss: 1152.927
[26,     1] loss: 1120.533
[27,     1] loss: 1089.285
[28,     1] loss: 1143.521
[29,     1] loss: 1067.027
[30,     1] loss: 1145.493
[31,     1] loss: 1079.055
[32,     1] loss: 1072.322
[33,     1] loss: 1030.021
[34,     1] loss: 1055.092
[35,     1] loss: 1013.087
[36,     1] loss: 1030.180
[37,     1] loss: 1018.060
[38,     1] loss: 1143.599
[39,     1] loss: 990.336
[40,     1] loss: 1031.536
[41,     1] loss: 1015.306
[42,     1] loss: 954.219
[43,     1] loss: 918.630
[44,     1] loss: 943.447
[45,     1] loss: 935.542
[46,     1] loss: 941.397
[47,     1] loss: 896.390
[48,     1] loss: 939.585
[49,     1] loss: 872.731
[50,     1] loss: 891.662
[51,     1] loss: 824.334
[52,     1] loss: 807.162
[53,     1] loss: 807.531
[54,     1] loss: 806.566
[55,     1] loss: 803.719
Early stopping applied (best metric=0.839564323425293)
Finished Training
Total time taken: 9.203011512756348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1436.448
[2,     1] loss: 1439.530
[3,     1] loss: 1435.899
[4,     1] loss: 1437.598
[5,     1] loss: 1434.503
[6,     1] loss: 1429.102
[7,     1] loss: 1426.049
[8,     1] loss: 1421.041
[9,     1] loss: 1417.315
[10,     1] loss: 1379.023
[11,     1] loss: 1365.284
[12,     1] loss: 1339.327
[13,     1] loss: 1309.028
[14,     1] loss: 1271.975
[15,     1] loss: 1255.260
[16,     1] loss: 1286.098
[17,     1] loss: 1213.900
[18,     1] loss: 1234.129
[19,     1] loss: 1220.145
[20,     1] loss: 1169.962
[21,     1] loss: 1196.162
[22,     1] loss: 1132.032
[23,     1] loss: 1188.545
[24,     1] loss: 1115.316
[25,     1] loss: 1175.943
[26,     1] loss: 1095.471
[27,     1] loss: 1119.122
[28,     1] loss: 1139.453
[29,     1] loss: 1137.797
[30,     1] loss: 1128.017
[31,     1] loss: 1098.050
[32,     1] loss: 1053.011
[33,     1] loss: 1066.634
[34,     1] loss: 1032.337
[35,     1] loss: 1043.120
[36,     1] loss: 1017.070
[37,     1] loss: 1042.400
[38,     1] loss: 1051.197
[39,     1] loss: 1008.331
[40,     1] loss: 952.284
[41,     1] loss: 925.506
[42,     1] loss: 957.322
[43,     1] loss: 868.604
[44,     1] loss: 911.454
[45,     1] loss: 951.760
[46,     1] loss: 920.493
[47,     1] loss: 906.064
[48,     1] loss: 878.557
[49,     1] loss: 846.272
[50,     1] loss: 828.235
[51,     1] loss: 850.319
[52,     1] loss: 913.131
[53,     1] loss: 922.343
[54,     1] loss: 852.134
[55,     1] loss: 814.342
[56,     1] loss: 852.470
[57,     1] loss: 786.830
[58,     1] loss: 783.522
[59,     1] loss: 821.747
[60,     1] loss: 812.267
[61,     1] loss: 766.681
Early stopping applied (best metric=0.8893425464630127)
Finished Training
Total time taken: 8.27200698852539
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1439.136
[2,     1] loss: 1438.843
[3,     1] loss: 1434.647
[4,     1] loss: 1437.750
[5,     1] loss: 1434.931
[6,     1] loss: 1430.534
[7,     1] loss: 1435.338
[8,     1] loss: 1432.439
[9,     1] loss: 1428.009
[10,     1] loss: 1424.109
[11,     1] loss: 1412.842
[12,     1] loss: 1389.928
[13,     1] loss: 1373.595
[14,     1] loss: 1339.838
[15,     1] loss: 1308.417
[16,     1] loss: 1277.446
[17,     1] loss: 1265.836
[18,     1] loss: 1232.009
[19,     1] loss: 1204.656
[20,     1] loss: 1168.216
[21,     1] loss: 1168.801
[22,     1] loss: 1177.363
[23,     1] loss: 1179.191
[24,     1] loss: 1139.896
[25,     1] loss: 1208.043
[26,     1] loss: 1154.549
[27,     1] loss: 1112.981
[28,     1] loss: 1124.962
[29,     1] loss: 1062.211
[30,     1] loss: 1097.485
[31,     1] loss: 1037.115
[32,     1] loss: 1091.782
[33,     1] loss: 1086.458
[34,     1] loss: 1031.787
[35,     1] loss: 1069.891
[36,     1] loss: 998.269
[37,     1] loss: 1036.884
[38,     1] loss: 980.796
[39,     1] loss: 1001.258
Early stopping applied (best metric=0.939212441444397)
Finished Training
Total time taken: 6.553006410598755
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1436.995
[2,     1] loss: 1441.283
[3,     1] loss: 1434.780
[4,     1] loss: 1432.586
[5,     1] loss: 1434.384
[6,     1] loss: 1434.054
[7,     1] loss: 1429.471
[8,     1] loss: 1429.594
[9,     1] loss: 1414.921
[10,     1] loss: 1399.873
[11,     1] loss: 1374.665
[12,     1] loss: 1352.321
[13,     1] loss: 1316.722
[14,     1] loss: 1293.783
[15,     1] loss: 1264.839
[16,     1] loss: 1297.370
[17,     1] loss: 1221.808
[18,     1] loss: 1201.200
[19,     1] loss: 1195.449
[20,     1] loss: 1151.421
[21,     1] loss: 1185.357
[22,     1] loss: 1212.973
[23,     1] loss: 1203.816
[24,     1] loss: 1181.756
[25,     1] loss: 1170.185
[26,     1] loss: 1130.163
[27,     1] loss: 1161.969
[28,     1] loss: 1144.968
[29,     1] loss: 1125.118
[30,     1] loss: 1091.669
[31,     1] loss: 1125.241
[32,     1] loss: 1095.716
[33,     1] loss: 1048.742
[34,     1] loss: 1066.117
[35,     1] loss: 1039.238
[36,     1] loss: 1029.250
[37,     1] loss: 1043.240
[38,     1] loss: 1024.860
[39,     1] loss: 966.432
[40,     1] loss: 1031.250
[41,     1] loss: 1040.822
[42,     1] loss: 1004.998
[43,     1] loss: 1071.358
[44,     1] loss: 992.294
[45,     1] loss: 955.515
[46,     1] loss: 931.666
[47,     1] loss: 910.937
[48,     1] loss: 978.930
[49,     1] loss: 926.570
[50,     1] loss: 916.626
[51,     1] loss: 849.847
[52,     1] loss: 886.295
[53,     1] loss: 984.733
[54,     1] loss: 884.091
[55,     1] loss: 861.991
[56,     1] loss: 920.411
[57,     1] loss: 879.809
[58,     1] loss: 892.909
[59,     1] loss: 820.440
[60,     1] loss: 891.483
[61,     1] loss: 791.128
[62,     1] loss: 785.953
[63,     1] loss: 868.896
[64,     1] loss: 828.666
[65,     1] loss: 869.820
[66,     1] loss: 858.672
[67,     1] loss: 778.140
[68,     1] loss: 831.031
[69,     1] loss: 767.095
[70,     1] loss: 817.115
[71,     1] loss: 762.797
[72,     1] loss: 736.643
[73,     1] loss: 777.730
[74,     1] loss: 734.632
[75,     1] loss: 756.916
[76,     1] loss: 730.518
[77,     1] loss: 738.444
[78,     1] loss: 649.849
Early stopping applied (best metric=0.8575966358184814)
Finished Training
Total time taken: 10.556009531021118
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1434.522
[2,     1] loss: 1438.330
[3,     1] loss: 1432.400
[4,     1] loss: 1433.449
[5,     1] loss: 1428.591
[6,     1] loss: 1426.266
[7,     1] loss: 1415.207
[8,     1] loss: 1398.844
[9,     1] loss: 1367.803
[10,     1] loss: 1343.163
[11,     1] loss: 1314.626
[12,     1] loss: 1277.570
[13,     1] loss: 1236.456
[14,     1] loss: 1196.473
[15,     1] loss: 1147.599
[16,     1] loss: 1223.719
[17,     1] loss: 1107.003
[18,     1] loss: 1156.061
[19,     1] loss: 1130.500
[20,     1] loss: 1116.308
[21,     1] loss: 1203.489
[22,     1] loss: 1086.753
[23,     1] loss: 1131.721
[24,     1] loss: 1124.463
[25,     1] loss: 1108.003
[26,     1] loss: 1125.877
[27,     1] loss: 1068.405
[28,     1] loss: 1053.969
[29,     1] loss: 1051.665
[30,     1] loss: 1027.017
[31,     1] loss: 1052.750
[32,     1] loss: 1019.493
Early stopping applied (best metric=1.0169198513031006)
Finished Training
Total time taken: 5.447005033493042
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1438.314
[2,     1] loss: 1434.078
[3,     1] loss: 1442.179
[4,     1] loss: 1434.320
[5,     1] loss: 1432.309
[6,     1] loss: 1426.312
[7,     1] loss: 1415.243
[8,     1] loss: 1399.988
[9,     1] loss: 1377.109
[10,     1] loss: 1344.785
[11,     1] loss: 1294.431
[12,     1] loss: 1273.618
[13,     1] loss: 1236.566
[14,     1] loss: 1211.033
[15,     1] loss: 1208.464
[16,     1] loss: 1184.317
[17,     1] loss: 1190.781
[18,     1] loss: 1157.268
[19,     1] loss: 1177.743
[20,     1] loss: 1145.210
[21,     1] loss: 1125.065
[22,     1] loss: 1163.661
[23,     1] loss: 1114.998
[24,     1] loss: 1095.336
[25,     1] loss: 1152.086
[26,     1] loss: 1069.994
[27,     1] loss: 1057.224
[28,     1] loss: 1070.151
[29,     1] loss: 1068.841
[30,     1] loss: 1047.700
[31,     1] loss: 1041.557
[32,     1] loss: 995.768
[33,     1] loss: 1027.417
[34,     1] loss: 983.816
[35,     1] loss: 975.873
[36,     1] loss: 962.642
[37,     1] loss: 983.883
[38,     1] loss: 977.368
[39,     1] loss: 935.738
[40,     1] loss: 910.190
[41,     1] loss: 846.890
[42,     1] loss: 968.511
[43,     1] loss: 1002.036
[44,     1] loss: 875.132
[45,     1] loss: 943.436
[46,     1] loss: 877.491
[47,     1] loss: 939.053
[48,     1] loss: 916.932
[49,     1] loss: 849.290
[50,     1] loss: 853.005
[51,     1] loss: 795.789
[52,     1] loss: 852.531
[53,     1] loss: 775.241
[54,     1] loss: 783.049
[55,     1] loss: 808.984
[56,     1] loss: 755.608
[57,     1] loss: 780.469
[58,     1] loss: 1028.562
[59,     1] loss: 987.937
[60,     1] loss: 752.844
[61,     1] loss: 937.144
[62,     1] loss: 786.360
[63,     1] loss: 814.253
[64,     1] loss: 846.152
[65,     1] loss: 723.946
[66,     1] loss: 787.565
Early stopping applied (best metric=0.9498703479766846)
Finished Training
Total time taken: 8.95500922203064
{'Hydroxylation-K Validation Accuracy': 0.7590425531914894, 'Hydroxylation-K Validation Sensitivity': 0.6814814814814815, 'Hydroxylation-K Validation Specificity': 0.7789473684210526, 'Hydroxylation-K Validation Precision': 0.4514611096964038, 'Hydroxylation-K AUC ROC': 0.7926120857699805, 'Hydroxylation-K AUC PR': 0.5894774847369896, 'Hydroxylation-K MCC': 0.4051323024237115, 'Hydroxylation-K F1': 0.5378571405219371, 'Validation Loss (Hydroxylation-K)': 0.43455999394257866, 'Hydroxylation-P Validation Accuracy': 0.7638097050911121, 'Hydroxylation-P Validation Sensitivity': 0.7577777777777778, 'Hydroxylation-P Validation Specificity': 0.765105491545713, 'Hydroxylation-P Validation Precision': 0.42001095512499875, 'Hydroxylation-P AUC ROC': 0.8219908585077671, 'Hydroxylation-P AUC PR': 0.5598864477279898, 'Hydroxylation-P MCC': 0.43115782078323744, 'Hydroxylation-P F1': 0.5358408206527803, 'Validation Loss (Hydroxylation-P)': 0.3990595022837321, 'Validation Loss (total)': 0.8336194952329, 'TimeToTrain': 8.795608647664388}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003901665889226446,
 'learning_rate_Hydroxylation-K': 0.00082906275131048,
 'learning_rate_Hydroxylation-P': 0.006961870205940432,
 'log_base': 2.0982819311299545,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 84615843,
 'sample_weights': [2.504632190573491, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.561091542308196,
 'weight_decay_Hydroxylation-K': 2.94135687945953,
 'weight_decay_Hydroxylation-P': 1.746187288222029}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.837
[2,     1] loss: 1384.711
[3,     1] loss: 1383.677
[4,     1] loss: 1382.137
[5,     1] loss: 1385.264
[6,     1] loss: 1378.136
[7,     1] loss: 1371.377
[8,     1] loss: 1359.879
[9,     1] loss: 1341.954
[10,     1] loss: 1306.609
[11,     1] loss: 1275.849
[12,     1] loss: 1229.359
[13,     1] loss: 1221.566
[14,     1] loss: 1177.866
[15,     1] loss: 1158.252
[16,     1] loss: 1166.273
[17,     1] loss: 1141.632
[18,     1] loss: 1149.215
[19,     1] loss: 1155.081
[20,     1] loss: 1109.403
[21,     1] loss: 1103.095
[22,     1] loss: 1085.305
[23,     1] loss: 1082.402
[24,     1] loss: 1045.410
[25,     1] loss: 1033.385
[26,     1] loss: 1092.941
[27,     1] loss: 1044.720
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00652156216156002,
 'learning_rate_Hydroxylation-K': 0.006054255310023205,
 'learning_rate_Hydroxylation-P': 0.004361150793563994,
 'log_base': 1.831519466028059,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1552795845,
 'sample_weights': [2.252598318995652, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7686656200114736,
 'weight_decay_Hydroxylation-K': 2.9144306278637657,
 'weight_decay_Hydroxylation-P': 6.779572356013703}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1494.804
[2,     1] loss: 1494.493
[3,     1] loss: 1489.275
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007103473506526976,
 'learning_rate_Hydroxylation-K': 0.005990333153865758,
 'learning_rate_Hydroxylation-P': 0.00688091092949694,
 'log_base': 1.7087138348176523,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2022250512,
 'sample_weights': [2.7587447260750224, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.587310281764639,
 'weight_decay_Hydroxylation-K': 2.6226058580237055,
 'weight_decay_Hydroxylation-P': 7.953027970748346}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1565.738
[2,     1] loss: 1562.867
[3,     1] loss: 1578.129
[4,     1] loss: 1566.405
[5,     1] loss: 1561.375
[6,     1] loss: 1563.796
[7,     1] loss: 1558.507
[8,     1] loss: 1563.845
[9,     1] loss: 1559.330
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007099381716661127,
 'learning_rate_Hydroxylation-K': 0.0036554080242133004,
 'learning_rate_Hydroxylation-P': 0.0015512184598906963,
 'log_base': 1.0126893243887642,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2516373896,
 'sample_weights': [3.1161388087120443, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.475781123043818,
 'weight_decay_Hydroxylation-K': 8.340214223296297,
 'weight_decay_Hydroxylation-P': 3.5286843167380857}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43180.117
Exploding loss, terminate run (best metric=1.1095019578933716)
Finished Training
Total time taken: 0.20299863815307617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42997.070
Exploding loss, terminate run (best metric=1.0937631130218506)
Finished Training
Total time taken: 0.2129991054534912
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43057.828
Exploding loss, terminate run (best metric=1.1009116172790527)
Finished Training
Total time taken: 0.2180025577545166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42974.992
Exploding loss, terminate run (best metric=1.0773365497589111)
Finished Training
Total time taken: 0.21299982070922852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 43278.406
Exploding loss, terminate run (best metric=1.0752496719360352)
Finished Training
Total time taken: 0.20099997520446777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43415.789
Exploding loss, terminate run (best metric=1.0965604782104492)
Finished Training
Total time taken: 0.20499873161315918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42942.055
Exploding loss, terminate run (best metric=1.0961840152740479)
Finished Training
Total time taken: 0.22699999809265137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42895.867
Exploding loss, terminate run (best metric=1.091141700744629)
Finished Training
Total time taken: 0.1979992389678955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43074.707
Exploding loss, terminate run (best metric=1.0891447067260742)
Finished Training
Total time taken: 0.21699976921081543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 42957.645
Exploding loss, terminate run (best metric=1.073133945465088)
Finished Training
Total time taken: 0.21300077438354492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42979.895
Exploding loss, terminate run (best metric=1.101467490196228)
Finished Training
Total time taken: 0.22099971771240234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42907.383
Exploding loss, terminate run (best metric=1.093247890472412)
Finished Training
Total time taken: 0.19699954986572266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42840.039
Exploding loss, terminate run (best metric=1.0929791927337646)
Finished Training
Total time taken: 0.22499895095825195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43026.582
Exploding loss, terminate run (best metric=1.0824143886566162)
Finished Training
Total time taken: 0.20299983024597168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 43343.359
Exploding loss, terminate run (best metric=1.073918104171753)
Finished Training
Total time taken: 0.20799970626831055
{'Hydroxylation-K Validation Accuracy': 0.5583333333333333, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6062962962962963, 'Hydroxylation-K AUC PR': 0.30492715430628353, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.13325123152709362, 'Validation Loss (Hydroxylation-K)': 0.5571834087371826, 'Hydroxylation-P Validation Accuracy': 0.5646086323875269, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.592019563973028, 'Hydroxylation-P AUC PR': 0.2679432758885794, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12022526748648812, 'Validation Loss (Hydroxylation-P)': 0.5326135476430257, 'Validation Loss (total)': 1.089796988169352, 'TimeToTrain': 0.21079975763956707}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00834101898627013,
 'learning_rate_Hydroxylation-K': 0.00671565375385759,
 'learning_rate_Hydroxylation-P': 0.0004566275207387384,
 'log_base': 1.2567950938260355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1246462929,
 'sample_weights': [132.49398625491162, 16.52733589506894],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.99167226287049,
 'weight_decay_Hydroxylation-K': 6.854284214968668,
 'weight_decay_Hydroxylation-P': 3.730604214551894}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2442.680
[2,     1] loss: 2466.737
[3,     1] loss: 2451.174
[4,     1] loss: 2449.900
[5,     1] loss: 2452.221
[6,     1] loss: 2437.126
[7,     1] loss: 2443.823
[8,     1] loss: 2442.016
[9,     1] loss: 2447.173
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024507177739492066,
 'learning_rate_Hydroxylation-K': 0.003514609660780034,
 'learning_rate_Hydroxylation-P': 0.00951034176643505,
 'log_base': 1.6373814127378534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2996396499,
 'sample_weights': [7.304022255348922, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.9196976332716176,
 'weight_decay_Hydroxylation-K': 5.004269668495894,
 'weight_decay_Hydroxylation-P': 8.755721677607035}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1632.101
[2,     1] loss: 1628.239
[3,     1] loss: 1621.829
[4,     1] loss: 1618.012
[5,     1] loss: 1624.530
[6,     1] loss: 1621.731
[7,     1] loss: 1621.132
[8,     1] loss: 1624.573
[9,     1] loss: 1622.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00718846450788131,
 'learning_rate_Hydroxylation-K': 0.004899003646170946,
 'learning_rate_Hydroxylation-P': 0.006024854399115217,
 'log_base': 1.783330287138092,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2478532468,
 'sample_weights': [3.385619585077178, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6446300955957379,
 'weight_decay_Hydroxylation-K': 5.843304775756083,
 'weight_decay_Hydroxylation-P': 8.489911675305253}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1516.879
[2,     1] loss: 1515.298
[3,     1] loss: 1523.837
[4,     1] loss: 1517.727
[5,     1] loss: 1516.634
[6,     1] loss: 1516.332
[7,     1] loss: 1512.811
[8,     1] loss: 1513.021
[9,     1] loss: 1510.423
[10,     1] loss: 1509.138
[11,     1] loss: 1505.771
[12,     1] loss: 1496.980
[13,     1] loss: 1478.074
[14,     1] loss: 1461.737
[15,     1] loss: 1430.675
[16,     1] loss: 1407.855
[17,     1] loss: 1344.512
[18,     1] loss: 1318.547
[19,     1] loss: 1313.416
[20,     1] loss: 1292.486
[21,     1] loss: 1295.593
[22,     1] loss: 1215.662
[23,     1] loss: 1252.242
[24,     1] loss: 1239.261
[25,     1] loss: 1270.996
[26,     1] loss: 1269.509
[27,     1] loss: 1228.084
[28,     1] loss: 1217.983
[29,     1] loss: 1219.421
[30,     1] loss: 1186.762
[31,     1] loss: 1218.148
[32,     1] loss: 1220.901
[33,     1] loss: 1149.857
[34,     1] loss: 1271.446
[35,     1] loss: 1198.223
[36,     1] loss: 1168.734
[37,     1] loss: 1225.984
[38,     1] loss: 1191.634
[39,     1] loss: 1181.762
[40,     1] loss: 1216.545
[41,     1] loss: 1188.785
[42,     1] loss: 1199.213
[43,     1] loss: 1146.603
[44,     1] loss: 1188.320
[45,     1] loss: 1134.557
[46,     1] loss: 1180.518
[47,     1] loss: 1136.683
[48,     1] loss: 1151.462
[49,     1] loss: 1161.115
[50,     1] loss: 1116.604
[51,     1] loss: 1092.377
[52,     1] loss: 1100.501
[53,     1] loss: 1052.578
[54,     1] loss: 1109.982
[55,     1] loss: 1122.112
[56,     1] loss: 1072.091
[57,     1] loss: 1125.548
[58,     1] loss: 1112.886
[59,     1] loss: 1050.413
[60,     1] loss: 1020.165
[61,     1] loss: 1120.864
[62,     1] loss: 1085.717
[63,     1] loss: 1100.213
[64,     1] loss: 1021.368
[65,     1] loss: 1008.638
[66,     1] loss: 986.824
[67,     1] loss: 955.572
[68,     1] loss: 985.775
[69,     1] loss: 990.203
[70,     1] loss: 1065.532
[71,     1] loss: 1027.351
[72,     1] loss: 1033.536
[73,     1] loss: 995.028
Early stopping applied (best metric=0.8209816217422485)
Finished Training
Total time taken: 12.189012289047241
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.160
[2,     1] loss: 1520.333
[3,     1] loss: 1508.119
[4,     1] loss: 1524.191
[5,     1] loss: 1525.697
[6,     1] loss: 1518.521
[7,     1] loss: 1513.210
[8,     1] loss: 1512.173
[9,     1] loss: 1506.827
[10,     1] loss: 1501.394
[11,     1] loss: 1491.709
[12,     1] loss: 1483.594
[13,     1] loss: 1455.474
[14,     1] loss: 1417.294
[15,     1] loss: 1407.694
[16,     1] loss: 1349.228
[17,     1] loss: 1316.794
[18,     1] loss: 1318.901
[19,     1] loss: 1319.589
[20,     1] loss: 1271.028
[21,     1] loss: 1302.061
[22,     1] loss: 1272.646
[23,     1] loss: 1250.126
[24,     1] loss: 1201.958
[25,     1] loss: 1231.141
[26,     1] loss: 1263.145
[27,     1] loss: 1224.126
[28,     1] loss: 1225.877
[29,     1] loss: 1233.310
[30,     1] loss: 1216.134
[31,     1] loss: 1264.163
[32,     1] loss: 1241.124
[33,     1] loss: 1219.692
[34,     1] loss: 1161.625
[35,     1] loss: 1175.080
[36,     1] loss: 1151.334
[37,     1] loss: 1190.881
[38,     1] loss: 1185.038
[39,     1] loss: 1180.124
[40,     1] loss: 1185.002
[41,     1] loss: 1177.635
[42,     1] loss: 1129.909
[43,     1] loss: 1176.315
[44,     1] loss: 1182.948
[45,     1] loss: 1125.943
[46,     1] loss: 1145.172
[47,     1] loss: 1095.027
[48,     1] loss: 1091.489
[49,     1] loss: 1104.846
[50,     1] loss: 1046.168
[51,     1] loss: 1092.924
[52,     1] loss: 1097.525
[53,     1] loss: 1032.848
[54,     1] loss: 1064.690
[55,     1] loss: 1094.474
[56,     1] loss: 997.821
[57,     1] loss: 1051.011
[58,     1] loss: 1066.252
[59,     1] loss: 1029.283
[60,     1] loss: 1014.214
[61,     1] loss: 1041.128
[62,     1] loss: 1030.559
[63,     1] loss: 1027.115
[64,     1] loss: 1070.050
[65,     1] loss: 1009.576
Early stopping applied (best metric=0.8494583964347839)
Finished Training
Total time taken: 8.829010009765625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1534.514
[2,     1] loss: 1524.733
[3,     1] loss: 1519.093
[4,     1] loss: 1517.879
[5,     1] loss: 1513.798
[6,     1] loss: 1515.000
[7,     1] loss: 1517.596
[8,     1] loss: 1518.004
[9,     1] loss: 1516.021
[10,     1] loss: 1512.277
[11,     1] loss: 1516.485
[12,     1] loss: 1522.044
[13,     1] loss: 1512.375
[14,     1] loss: 1517.749
[15,     1] loss: 1515.017
[16,     1] loss: 1514.477
[17,     1] loss: 1514.245
[18,     1] loss: 1511.343
[19,     1] loss: 1516.467
[20,     1] loss: 1508.194
[21,     1] loss: 1506.442
[22,     1] loss: 1498.918
[23,     1] loss: 1496.668
[24,     1] loss: 1494.455
[25,     1] loss: 1482.189
[26,     1] loss: 1463.882
[27,     1] loss: 1434.641
[28,     1] loss: 1423.860
[29,     1] loss: 1404.568
[30,     1] loss: 1353.618
[31,     1] loss: 1338.668
[32,     1] loss: 1310.540
[33,     1] loss: 1345.437
[34,     1] loss: 1308.718
[35,     1] loss: 1291.106
[36,     1] loss: 1235.046
[37,     1] loss: 1246.671
[38,     1] loss: 1266.259
[39,     1] loss: 1288.300
[40,     1] loss: 1265.328
[41,     1] loss: 1294.718
[42,     1] loss: 1267.607
[43,     1] loss: 1229.662
[44,     1] loss: 1240.925
[45,     1] loss: 1236.322
[46,     1] loss: 1244.857
[47,     1] loss: 1275.402
[48,     1] loss: 1237.115
[49,     1] loss: 1228.353
[50,     1] loss: 1224.389
[51,     1] loss: 1190.534
[52,     1] loss: 1207.009
[53,     1] loss: 1190.856
[54,     1] loss: 1131.037
[55,     1] loss: 1197.505
[56,     1] loss: 1202.655
[57,     1] loss: 1214.488
[58,     1] loss: 1130.036
[59,     1] loss: 1192.307
[60,     1] loss: 1176.141
[61,     1] loss: 1161.589
[62,     1] loss: 1158.936
[63,     1] loss: 1094.697
[64,     1] loss: 1121.107
[65,     1] loss: 1127.943
[66,     1] loss: 1121.507
[67,     1] loss: 1081.326
[68,     1] loss: 1074.330
[69,     1] loss: 1110.732
[70,     1] loss: 1129.129
[71,     1] loss: 1059.124
[72,     1] loss: 1045.248
[73,     1] loss: 1067.003
[74,     1] loss: 1029.760
[75,     1] loss: 1099.010
[76,     1] loss: 1074.942
[77,     1] loss: 1064.342
[78,     1] loss: 1151.774
[79,     1] loss: 1067.579
[80,     1] loss: 1054.216
[81,     1] loss: 978.789
[82,     1] loss: 963.182
[83,     1] loss: 1033.256
[84,     1] loss: 1127.582
[85,     1] loss: 1078.140
[86,     1] loss: 990.890
[87,     1] loss: 1028.783
[88,     1] loss: 1057.453
[89,     1] loss: 948.549
[90,     1] loss: 968.328
[91,     1] loss: 1011.964
[92,     1] loss: 1010.420
[93,     1] loss: 957.687
[94,     1] loss: 954.756
[95,     1] loss: 1018.252
[96,     1] loss: 1035.779
[97,     1] loss: 962.365
[98,     1] loss: 946.655
[99,     1] loss: 963.861
[100,     1] loss: 1024.727
Early stopping applied (best metric=0.6887760162353516)
Finished Training
Total time taken: 16.67801570892334
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1520.663
[2,     1] loss: 1539.523
[3,     1] loss: 1521.103
[4,     1] loss: 1517.876
[5,     1] loss: 1511.168
[6,     1] loss: 1523.647
[7,     1] loss: 1511.542
[8,     1] loss: 1512.232
[9,     1] loss: 1507.723
[10,     1] loss: 1509.400
[11,     1] loss: 1499.631
[12,     1] loss: 1494.020
[13,     1] loss: 1489.625
[14,     1] loss: 1470.281
[15,     1] loss: 1447.412
[16,     1] loss: 1426.627
[17,     1] loss: 1405.060
[18,     1] loss: 1337.927
[19,     1] loss: 1372.145
[20,     1] loss: 1321.794
[21,     1] loss: 1288.081
[22,     1] loss: 1345.007
[23,     1] loss: 1253.826
[24,     1] loss: 1243.625
[25,     1] loss: 1281.028
[26,     1] loss: 1241.605
[27,     1] loss: 1278.301
[28,     1] loss: 1249.539
[29,     1] loss: 1254.528
[30,     1] loss: 1238.012
[31,     1] loss: 1228.419
[32,     1] loss: 1231.514
[33,     1] loss: 1249.900
[34,     1] loss: 1224.559
[35,     1] loss: 1245.111
[36,     1] loss: 1228.708
[37,     1] loss: 1248.717
[38,     1] loss: 1207.961
[39,     1] loss: 1209.256
[40,     1] loss: 1199.523
[41,     1] loss: 1206.479
[42,     1] loss: 1225.457
[43,     1] loss: 1191.165
[44,     1] loss: 1190.176
[45,     1] loss: 1164.188
[46,     1] loss: 1144.085
[47,     1] loss: 1166.393
[48,     1] loss: 1162.832
[49,     1] loss: 1111.467
[50,     1] loss: 1148.916
[51,     1] loss: 1109.307
Early stopping applied (best metric=0.9192622303962708)
Finished Training
Total time taken: 8.611011505126953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1529.159
[2,     1] loss: 1520.341
[3,     1] loss: 1516.536
[4,     1] loss: 1517.950
[5,     1] loss: 1515.885
[6,     1] loss: 1513.798
[7,     1] loss: 1515.164
[8,     1] loss: 1520.284
[9,     1] loss: 1516.363
[10,     1] loss: 1520.186
[11,     1] loss: 1516.681
[12,     1] loss: 1515.218
[13,     1] loss: 1512.419
[14,     1] loss: 1509.872
[15,     1] loss: 1504.184
[16,     1] loss: 1496.491
[17,     1] loss: 1490.159
[18,     1] loss: 1471.034
[19,     1] loss: 1463.947
[20,     1] loss: 1421.353
[21,     1] loss: 1393.875
[22,     1] loss: 1363.370
[23,     1] loss: 1351.923
[24,     1] loss: 1361.240
[25,     1] loss: 1321.246
[26,     1] loss: 1299.153
[27,     1] loss: 1373.370
[28,     1] loss: 1258.417
[29,     1] loss: 1250.552
[30,     1] loss: 1271.645
[31,     1] loss: 1225.756
[32,     1] loss: 1225.423
[33,     1] loss: 1239.491
[34,     1] loss: 1248.656
[35,     1] loss: 1202.862
[36,     1] loss: 1254.194
[37,     1] loss: 1249.570
[38,     1] loss: 1245.795
[39,     1] loss: 1268.156
[40,     1] loss: 1176.355
[41,     1] loss: 1244.955
[42,     1] loss: 1221.728
[43,     1] loss: 1153.539
[44,     1] loss: 1169.875
[45,     1] loss: 1198.074
[46,     1] loss: 1132.633
[47,     1] loss: 1159.012
[48,     1] loss: 1156.938
[49,     1] loss: 1136.421
[50,     1] loss: 1187.826
[51,     1] loss: 1212.604
[52,     1] loss: 1154.718
[53,     1] loss: 1179.699
[54,     1] loss: 1122.238
[55,     1] loss: 1138.537
[56,     1] loss: 1161.333
[57,     1] loss: 1152.290
[58,     1] loss: 1110.796
[59,     1] loss: 1070.336
[60,     1] loss: 1110.235
[61,     1] loss: 1116.286
[62,     1] loss: 1111.241
[63,     1] loss: 1100.882
[64,     1] loss: 1074.504
[65,     1] loss: 1084.242
[66,     1] loss: 1041.850
[67,     1] loss: 1102.410
[68,     1] loss: 1065.534
[69,     1] loss: 1045.239
[70,     1] loss: 962.139
[71,     1] loss: 1003.588
[72,     1] loss: 1001.005
[73,     1] loss: 1004.063
[74,     1] loss: 1020.168
[75,     1] loss: 1011.705
[76,     1] loss: 1035.098
[77,     1] loss: 1107.967
Early stopping applied (best metric=0.8088362216949463)
Finished Training
Total time taken: 12.424010992050171
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1516.325
[2,     1] loss: 1525.854
[3,     1] loss: 1518.981
[4,     1] loss: 1520.788
[5,     1] loss: 1518.467
[6,     1] loss: 1518.158
[7,     1] loss: 1515.539
[8,     1] loss: 1516.117
[9,     1] loss: 1520.706
[10,     1] loss: 1521.787
[11,     1] loss: 1518.309
[12,     1] loss: 1519.788
[13,     1] loss: 1515.068
[14,     1] loss: 1517.444
[15,     1] loss: 1518.363
[16,     1] loss: 1514.564
[17,     1] loss: 1514.508
[18,     1] loss: 1514.833
[19,     1] loss: 1512.913
[20,     1] loss: 1513.169
[21,     1] loss: 1511.694
[22,     1] loss: 1509.977
[23,     1] loss: 1511.622
[24,     1] loss: 1513.864
[25,     1] loss: 1508.644
[26,     1] loss: 1502.615
[27,     1] loss: 1501.702
[28,     1] loss: 1494.663
[29,     1] loss: 1490.692
[30,     1] loss: 1487.921
[31,     1] loss: 1465.567
[32,     1] loss: 1443.859
[33,     1] loss: 1432.570
[34,     1] loss: 1399.220
[35,     1] loss: 1354.036
[36,     1] loss: 1323.811
[37,     1] loss: 1342.180
[38,     1] loss: 1334.091
[39,     1] loss: 1323.185
[40,     1] loss: 1298.320
[41,     1] loss: 1267.407
[42,     1] loss: 1239.094
[43,     1] loss: 1249.170
[44,     1] loss: 1206.870
[45,     1] loss: 1320.141
[46,     1] loss: 1240.192
[47,     1] loss: 1245.145
[48,     1] loss: 1271.001
[49,     1] loss: 1210.704
[50,     1] loss: 1200.527
[51,     1] loss: 1227.708
[52,     1] loss: 1188.824
[53,     1] loss: 1220.729
[54,     1] loss: 1142.855
[55,     1] loss: 1169.343
[56,     1] loss: 1158.034
[57,     1] loss: 1104.078
[58,     1] loss: 1185.784
[59,     1] loss: 1186.706
[60,     1] loss: 1180.011
[61,     1] loss: 1112.185
[62,     1] loss: 1153.506
[63,     1] loss: 1196.999
[64,     1] loss: 1160.407
[65,     1] loss: 1159.236
[66,     1] loss: 1140.458
[67,     1] loss: 1062.302
[68,     1] loss: 1072.056
[69,     1] loss: 1124.059
[70,     1] loss: 1121.523
[71,     1] loss: 1188.886
[72,     1] loss: 1078.141
[73,     1] loss: 1099.694
[74,     1] loss: 1062.434
[75,     1] loss: 1057.976
[76,     1] loss: 1113.483
[77,     1] loss: 1000.015
[78,     1] loss: 1135.411
[79,     1] loss: 1020.783
[80,     1] loss: 1095.350
[81,     1] loss: 1048.698
[82,     1] loss: 1082.229
[83,     1] loss: 1030.204
Early stopping applied (best metric=0.8421579599380493)
Finished Training
Total time taken: 14.062066316604614
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.925
[2,     1] loss: 1518.195
[3,     1] loss: 1513.134
[4,     1] loss: 1514.236
[5,     1] loss: 1510.191
[6,     1] loss: 1514.942
[7,     1] loss: 1513.743
[8,     1] loss: 1508.597
[9,     1] loss: 1497.879
[10,     1] loss: 1498.008
[11,     1] loss: 1473.518
[12,     1] loss: 1461.034
[13,     1] loss: 1410.951
[14,     1] loss: 1390.514
[15,     1] loss: 1339.886
[16,     1] loss: 1343.387
[17,     1] loss: 1321.186
[18,     1] loss: 1315.031
[19,     1] loss: 1336.743
[20,     1] loss: 1315.148
[21,     1] loss: 1291.253
[22,     1] loss: 1313.869
[23,     1] loss: 1307.276
[24,     1] loss: 1227.550
[25,     1] loss: 1308.998
[26,     1] loss: 1270.172
[27,     1] loss: 1219.584
[28,     1] loss: 1241.928
[29,     1] loss: 1223.911
[30,     1] loss: 1225.578
[31,     1] loss: 1183.595
[32,     1] loss: 1167.417
[33,     1] loss: 1158.587
[34,     1] loss: 1267.395
[35,     1] loss: 1191.506
[36,     1] loss: 1162.139
[37,     1] loss: 1177.737
[38,     1] loss: 1118.095
[39,     1] loss: 1224.820
[40,     1] loss: 1229.122
[41,     1] loss: 1114.005
[42,     1] loss: 1155.013
[43,     1] loss: 1124.045
[44,     1] loss: 1076.881
[45,     1] loss: 1198.655
[46,     1] loss: 1107.337
[47,     1] loss: 1097.545
[48,     1] loss: 1122.130
[49,     1] loss: 1136.553
[50,     1] loss: 1121.886
[51,     1] loss: 1081.073
[52,     1] loss: 1079.014
[53,     1] loss: 1090.778
[54,     1] loss: 1071.205
[55,     1] loss: 1114.827
[56,     1] loss: 1101.129
[57,     1] loss: 1040.729
[58,     1] loss: 994.797
[59,     1] loss: 1048.576
[60,     1] loss: 1062.911
[61,     1] loss: 1017.206
[62,     1] loss: 1098.688
[63,     1] loss: 1021.894
[64,     1] loss: 1075.918
[65,     1] loss: 1018.820
[66,     1] loss: 1021.079
[67,     1] loss: 994.459
[68,     1] loss: 960.528
[69,     1] loss: 934.759
[70,     1] loss: 963.281
[71,     1] loss: 978.401
Early stopping applied (best metric=0.781203031539917)
Finished Training
Total time taken: 9.699008226394653
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1522.323
[2,     1] loss: 1529.639
[3,     1] loss: 1513.731
[4,     1] loss: 1520.424
[5,     1] loss: 1510.338
[6,     1] loss: 1510.302
[7,     1] loss: 1512.856
[8,     1] loss: 1504.402
[9,     1] loss: 1514.992
[10,     1] loss: 1489.348
[11,     1] loss: 1480.490
[12,     1] loss: 1455.311
[13,     1] loss: 1421.910
[14,     1] loss: 1380.687
[15,     1] loss: 1361.019
[16,     1] loss: 1312.488
[17,     1] loss: 1281.758
[18,     1] loss: 1272.860
[19,     1] loss: 1242.515
[20,     1] loss: 1261.191
[21,     1] loss: 1228.152
[22,     1] loss: 1282.619
[23,     1] loss: 1225.318
[24,     1] loss: 1244.112
[25,     1] loss: 1229.047
[26,     1] loss: 1231.572
[27,     1] loss: 1211.550
[28,     1] loss: 1212.963
[29,     1] loss: 1187.435
[30,     1] loss: 1214.013
[31,     1] loss: 1149.095
[32,     1] loss: 1176.637
[33,     1] loss: 1155.021
[34,     1] loss: 1183.578
[35,     1] loss: 1134.204
[36,     1] loss: 1169.467
[37,     1] loss: 1120.157
[38,     1] loss: 1135.154
[39,     1] loss: 1140.838
[40,     1] loss: 1110.529
[41,     1] loss: 1061.284
[42,     1] loss: 1068.028
[43,     1] loss: 1126.084
[44,     1] loss: 1122.380
[45,     1] loss: 1050.391
[46,     1] loss: 1096.259
[47,     1] loss: 1104.217
[48,     1] loss: 1070.797
[49,     1] loss: 1059.126
[50,     1] loss: 1049.732
[51,     1] loss: 1036.174
[52,     1] loss: 1065.453
[53,     1] loss: 1077.927
[54,     1] loss: 1030.012
[55,     1] loss: 948.415
[56,     1] loss: 1035.206
[57,     1] loss: 1069.450
[58,     1] loss: 983.788
[59,     1] loss: 1096.910
[60,     1] loss: 1007.329
[61,     1] loss: 1046.592
[62,     1] loss: 982.236
[63,     1] loss: 984.149
[64,     1] loss: 1004.751
[65,     1] loss: 969.343
[66,     1] loss: 983.845
[67,     1] loss: 950.857
[68,     1] loss: 926.418
[69,     1] loss: 882.986
[70,     1] loss: 935.869
Early stopping applied (best metric=0.9201750755310059)
Finished Training
Total time taken: 11.670011758804321
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1517.456
[2,     1] loss: 1520.571
[3,     1] loss: 1522.550
[4,     1] loss: 1527.426
[5,     1] loss: 1520.187
[6,     1] loss: 1513.138
[7,     1] loss: 1511.326
[8,     1] loss: 1511.965
[9,     1] loss: 1524.269
[10,     1] loss: 1520.360
[11,     1] loss: 1513.889
[12,     1] loss: 1515.027
[13,     1] loss: 1511.367
[14,     1] loss: 1515.568
[15,     1] loss: 1507.477
[16,     1] loss: 1508.000
[17,     1] loss: 1502.543
[18,     1] loss: 1499.349
[19,     1] loss: 1496.477
[20,     1] loss: 1476.428
[21,     1] loss: 1469.752
[22,     1] loss: 1437.473
[23,     1] loss: 1393.077
[24,     1] loss: 1348.437
[25,     1] loss: 1341.111
[26,     1] loss: 1285.603
[27,     1] loss: 1289.115
[28,     1] loss: 1355.145
[29,     1] loss: 1237.817
[30,     1] loss: 1291.411
[31,     1] loss: 1234.843
[32,     1] loss: 1253.271
[33,     1] loss: 1259.524
[34,     1] loss: 1201.373
[35,     1] loss: 1245.677
[36,     1] loss: 1246.084
[37,     1] loss: 1220.198
[38,     1] loss: 1209.896
[39,     1] loss: 1215.479
[40,     1] loss: 1235.701
[41,     1] loss: 1187.780
[42,     1] loss: 1177.391
[43,     1] loss: 1170.037
[44,     1] loss: 1176.980
[45,     1] loss: 1159.979
[46,     1] loss: 1180.098
[47,     1] loss: 1117.293
[48,     1] loss: 1077.270
[49,     1] loss: 1096.703
[50,     1] loss: 1164.925
[51,     1] loss: 1149.027
[52,     1] loss: 1091.432
[53,     1] loss: 1127.744
[54,     1] loss: 1102.023
[55,     1] loss: 1063.589
[56,     1] loss: 1066.111
[57,     1] loss: 1125.085
[58,     1] loss: 1030.367
Early stopping applied (best metric=0.8904249668121338)
Finished Training
Total time taken: 9.80700945854187
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1525.587
[2,     1] loss: 1535.825
[3,     1] loss: 1524.051
[4,     1] loss: 1526.313
[5,     1] loss: 1520.243
[6,     1] loss: 1521.022
[7,     1] loss: 1518.467
[8,     1] loss: 1519.149
[9,     1] loss: 1525.388
[10,     1] loss: 1518.953
[11,     1] loss: 1525.556
[12,     1] loss: 1518.370
[13,     1] loss: 1516.847
[14,     1] loss: 1516.939
[15,     1] loss: 1518.272
[16,     1] loss: 1522.740
[17,     1] loss: 1516.480
[18,     1] loss: 1514.615
[19,     1] loss: 1517.739
[20,     1] loss: 1519.197
[21,     1] loss: 1515.677
[22,     1] loss: 1517.701
[23,     1] loss: 1514.604
[24,     1] loss: 1515.327
[25,     1] loss: 1514.260
[26,     1] loss: 1513.377
[27,     1] loss: 1516.005
[28,     1] loss: 1516.057
[29,     1] loss: 1516.300
[30,     1] loss: 1515.358
[31,     1] loss: 1513.751
Early stopping applied (best metric=1.0727689266204834)
Finished Training
Total time taken: 5.259005069732666
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1515.780
[2,     1] loss: 1513.881
[3,     1] loss: 1541.761
[4,     1] loss: 1519.956
[5,     1] loss: 1518.331
[6,     1] loss: 1517.290
[7,     1] loss: 1508.494
[8,     1] loss: 1515.104
[9,     1] loss: 1505.340
[10,     1] loss: 1505.381
[11,     1] loss: 1495.773
[12,     1] loss: 1495.561
[13,     1] loss: 1478.829
[14,     1] loss: 1472.273
[15,     1] loss: 1446.490
[16,     1] loss: 1433.376
[17,     1] loss: 1416.810
[18,     1] loss: 1388.246
[19,     1] loss: 1355.066
[20,     1] loss: 1283.863
[21,     1] loss: 1348.005
[22,     1] loss: 1311.212
[23,     1] loss: 1282.948
[24,     1] loss: 1308.656
[25,     1] loss: 1261.748
[26,     1] loss: 1259.374
[27,     1] loss: 1256.611
[28,     1] loss: 1322.841
[29,     1] loss: 1287.144
[30,     1] loss: 1276.037
[31,     1] loss: 1262.550
[32,     1] loss: 1274.378
[33,     1] loss: 1182.942
[34,     1] loss: 1240.049
[35,     1] loss: 1257.816
[36,     1] loss: 1173.816
[37,     1] loss: 1200.326
[38,     1] loss: 1207.613
[39,     1] loss: 1198.613
[40,     1] loss: 1142.166
[41,     1] loss: 1169.851
[42,     1] loss: 1119.652
[43,     1] loss: 1148.132
[44,     1] loss: 1147.039
[45,     1] loss: 1173.707
[46,     1] loss: 1182.164
[47,     1] loss: 1148.651
[48,     1] loss: 1211.012
[49,     1] loss: 1177.240
[50,     1] loss: 1177.562
[51,     1] loss: 1139.743
[52,     1] loss: 1130.416
[53,     1] loss: 1179.485
[54,     1] loss: 1147.956
[55,     1] loss: 1116.997
[56,     1] loss: 1089.721
[57,     1] loss: 1142.966
[58,     1] loss: 1120.459
[59,     1] loss: 1175.868
[60,     1] loss: 1185.649
[61,     1] loss: 1080.776
[62,     1] loss: 1080.206
[63,     1] loss: 1090.099
[64,     1] loss: 1071.317
[65,     1] loss: 1042.445
[66,     1] loss: 1081.593
[67,     1] loss: 1062.968
[68,     1] loss: 1109.522
[69,     1] loss: 1103.905
[70,     1] loss: 1093.434
[71,     1] loss: 1130.268
[72,     1] loss: 1060.847
[73,     1] loss: 1049.958
[74,     1] loss: 1062.012
[75,     1] loss: 1086.421
[76,     1] loss: 1076.686
[77,     1] loss: 996.048
[78,     1] loss: 1005.479
[79,     1] loss: 1042.249
[80,     1] loss: 1033.053
[81,     1] loss: 1023.166
[82,     1] loss: 1062.359
[83,     1] loss: 968.773
[84,     1] loss: 1028.098
[85,     1] loss: 996.262
[86,     1] loss: 1029.702
[87,     1] loss: 1012.696
[88,     1] loss: 979.884
[89,     1] loss: 944.553
[90,     1] loss: 965.130
[91,     1] loss: 1006.402
[92,     1] loss: 1070.375
Early stopping applied (best metric=0.7070019245147705)
Finished Training
Total time taken: 12.433011293411255
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1517.052
[2,     1] loss: 1514.049
[3,     1] loss: 1520.434
[4,     1] loss: 1514.275
[5,     1] loss: 1519.631
[6,     1] loss: 1513.893
[7,     1] loss: 1507.724
[8,     1] loss: 1525.531
[9,     1] loss: 1523.191
[10,     1] loss: 1519.232
[11,     1] loss: 1510.999
[12,     1] loss: 1514.802
[13,     1] loss: 1513.310
[14,     1] loss: 1513.232
[15,     1] loss: 1505.561
[16,     1] loss: 1502.420
[17,     1] loss: 1493.157
[18,     1] loss: 1489.028
[19,     1] loss: 1474.830
[20,     1] loss: 1439.485
[21,     1] loss: 1430.069
[22,     1] loss: 1362.717
[23,     1] loss: 1340.366
[24,     1] loss: 1325.166
[25,     1] loss: 1296.688
[26,     1] loss: 1271.642
[27,     1] loss: 1286.082
[28,     1] loss: 1272.614
[29,     1] loss: 1274.300
[30,     1] loss: 1213.756
[31,     1] loss: 1274.854
[32,     1] loss: 1204.891
[33,     1] loss: 1222.583
[34,     1] loss: 1213.639
[35,     1] loss: 1211.891
[36,     1] loss: 1208.899
[37,     1] loss: 1187.003
[38,     1] loss: 1220.933
[39,     1] loss: 1224.245
[40,     1] loss: 1163.702
[41,     1] loss: 1159.342
[42,     1] loss: 1197.920
[43,     1] loss: 1147.013
Early stopping applied (best metric=0.9637584090232849)
Finished Training
Total time taken: 7.311007261276245
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1519.605
[2,     1] loss: 1512.098
[3,     1] loss: 1521.437
[4,     1] loss: 1525.934
[5,     1] loss: 1518.144
[6,     1] loss: 1517.950
[7,     1] loss: 1511.837
[8,     1] loss: 1516.375
[9,     1] loss: 1513.204
[10,     1] loss: 1515.556
[11,     1] loss: 1515.911
[12,     1] loss: 1513.297
[13,     1] loss: 1510.403
[14,     1] loss: 1508.373
[15,     1] loss: 1509.340
[16,     1] loss: 1503.788
[17,     1] loss: 1506.693
[18,     1] loss: 1492.954
[19,     1] loss: 1477.703
[20,     1] loss: 1470.565
[21,     1] loss: 1440.666
[22,     1] loss: 1423.115
[23,     1] loss: 1386.109
[24,     1] loss: 1350.816
[25,     1] loss: 1335.334
[26,     1] loss: 1311.470
[27,     1] loss: 1284.929
[28,     1] loss: 1362.619
[29,     1] loss: 1294.500
[30,     1] loss: 1236.407
[31,     1] loss: 1276.894
[32,     1] loss: 1254.386
[33,     1] loss: 1269.667
[34,     1] loss: 1247.481
[35,     1] loss: 1266.364
[36,     1] loss: 1251.855
[37,     1] loss: 1224.324
[38,     1] loss: 1318.132
[39,     1] loss: 1243.923
[40,     1] loss: 1250.046
[41,     1] loss: 1178.000
[42,     1] loss: 1241.943
[43,     1] loss: 1237.263
[44,     1] loss: 1216.479
[45,     1] loss: 1218.032
[46,     1] loss: 1195.024
[47,     1] loss: 1144.995
[48,     1] loss: 1155.164
[49,     1] loss: 1184.578
[50,     1] loss: 1196.250
[51,     1] loss: 1252.652
[52,     1] loss: 1176.319
[53,     1] loss: 1179.789
[54,     1] loss: 1238.595
[55,     1] loss: 1174.902
[56,     1] loss: 1115.420
[57,     1] loss: 1130.624
[58,     1] loss: 1124.507
[59,     1] loss: 1134.019
[60,     1] loss: 1163.695
[61,     1] loss: 1082.802
[62,     1] loss: 1188.285
[63,     1] loss: 1112.501
[64,     1] loss: 1125.718
[65,     1] loss: 1124.854
[66,     1] loss: 1126.999
[67,     1] loss: 1053.281
[68,     1] loss: 1077.544
[69,     1] loss: 1126.107
[70,     1] loss: 1070.747
[71,     1] loss: 1064.927
[72,     1] loss: 1089.010
[73,     1] loss: 1141.695
[74,     1] loss: 1094.229
[75,     1] loss: 1103.005
[76,     1] loss: 1107.403
[77,     1] loss: 1087.117
[78,     1] loss: 1068.038
Early stopping applied (best metric=0.8269294500350952)
Finished Training
Total time taken: 13.066014766693115
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1512.606
[2,     1] loss: 1518.454
[3,     1] loss: 1512.183
[4,     1] loss: 1500.849
[5,     1] loss: 1514.260
[6,     1] loss: 1504.161
[7,     1] loss: 1499.997
[8,     1] loss: 1476.220
[9,     1] loss: 1463.028
[10,     1] loss: 1438.170
[11,     1] loss: 1352.273
[12,     1] loss: 1336.746
[13,     1] loss: 1363.832
[14,     1] loss: 1287.609
[15,     1] loss: 1264.406
[16,     1] loss: 1318.609
[17,     1] loss: 1270.596
[18,     1] loss: 1234.660
[19,     1] loss: 1264.172
[20,     1] loss: 1226.144
[21,     1] loss: 1280.873
[22,     1] loss: 1230.034
[23,     1] loss: 1195.701
[24,     1] loss: 1279.079
[25,     1] loss: 1170.577
[26,     1] loss: 1215.587
[27,     1] loss: 1172.418
[28,     1] loss: 1214.948
[29,     1] loss: 1206.809
[30,     1] loss: 1205.756
[31,     1] loss: 1142.890
[32,     1] loss: 1201.365
[33,     1] loss: 1228.972
[34,     1] loss: 1139.628
[35,     1] loss: 1201.757
[36,     1] loss: 1136.743
[37,     1] loss: 1123.662
[38,     1] loss: 1124.601
[39,     1] loss: 1139.125
[40,     1] loss: 1093.038
[41,     1] loss: 1122.255
[42,     1] loss: 1146.378
[43,     1] loss: 1128.346
[44,     1] loss: 1134.095
[45,     1] loss: 1056.311
[46,     1] loss: 1073.589
[47,     1] loss: 1098.154
[48,     1] loss: 1087.349
[49,     1] loss: 1138.625
[50,     1] loss: 1072.403
[51,     1] loss: 1052.622
[52,     1] loss: 1070.104
[53,     1] loss: 1066.680
[54,     1] loss: 1116.942
[55,     1] loss: 1077.776
[56,     1] loss: 1022.313
[57,     1] loss: 1091.171
[58,     1] loss: 1058.220
[59,     1] loss: 1006.154
[60,     1] loss: 977.365
[61,     1] loss: 1025.434
[62,     1] loss: 1041.172
[63,     1] loss: 941.303
[64,     1] loss: 935.704
[65,     1] loss: 966.090
[66,     1] loss: 967.589
Early stopping applied (best metric=0.7101438045501709)
Finished Training
Total time taken: 8.952008962631226
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1518.468
[2,     1] loss: 1526.874
[3,     1] loss: 1517.145
[4,     1] loss: 1521.655
[5,     1] loss: 1511.274
[6,     1] loss: 1515.491
[7,     1] loss: 1517.606
[8,     1] loss: 1514.976
[9,     1] loss: 1513.790
[10,     1] loss: 1517.265
[11,     1] loss: 1511.595
[12,     1] loss: 1511.730
[13,     1] loss: 1529.962
[14,     1] loss: 1514.452
[15,     1] loss: 1509.073
[16,     1] loss: 1511.311
[17,     1] loss: 1509.015
[18,     1] loss: 1509.710
[19,     1] loss: 1503.831
[20,     1] loss: 1491.516
[21,     1] loss: 1486.865
[22,     1] loss: 1463.757
[23,     1] loss: 1433.351
[24,     1] loss: 1432.997
[25,     1] loss: 1381.233
[26,     1] loss: 1348.558
[27,     1] loss: 1376.736
[28,     1] loss: 1318.445
[29,     1] loss: 1331.747
[30,     1] loss: 1320.773
[31,     1] loss: 1275.562
[32,     1] loss: 1296.806
[33,     1] loss: 1266.540
[34,     1] loss: 1281.619
[35,     1] loss: 1324.545
[36,     1] loss: 1259.023
[37,     1] loss: 1288.916
[38,     1] loss: 1205.664
[39,     1] loss: 1284.255
[40,     1] loss: 1245.317
[41,     1] loss: 1232.599
[42,     1] loss: 1230.134
[43,     1] loss: 1218.146
[44,     1] loss: 1191.828
[45,     1] loss: 1255.168
[46,     1] loss: 1173.329
[47,     1] loss: 1195.842
[48,     1] loss: 1237.631
[49,     1] loss: 1184.613
[50,     1] loss: 1214.474
[51,     1] loss: 1183.276
[52,     1] loss: 1180.337
[53,     1] loss: 1138.358
[54,     1] loss: 1146.206
[55,     1] loss: 1128.362
[56,     1] loss: 1149.463
[57,     1] loss: 1134.890
[58,     1] loss: 1131.137
[59,     1] loss: 1082.262
[60,     1] loss: 1171.280
Early stopping applied (best metric=0.8903293013572693)
Finished Training
Total time taken: 10.04601001739502
{'Hydroxylation-K Validation Accuracy': 0.7305260047281323, 'Hydroxylation-K Validation Sensitivity': 0.7088888888888889, 'Hydroxylation-K Validation Specificity': 0.7368421052631579, 'Hydroxylation-K Validation Precision': 0.437978196188459, 'Hydroxylation-K AUC ROC': 0.7790253411306043, 'Hydroxylation-K AUC PR': 0.5350494669390782, 'Hydroxylation-K MCC': 0.3895221960349581, 'Hydroxylation-K F1': 0.5311702897210144, 'Validation Loss (Hydroxylation-K)': 0.44722920457522075, 'Hydroxylation-P Validation Accuracy': 0.7411196216774106, 'Hydroxylation-P Validation Sensitivity': 0.7768253968253969, 'Hydroxylation-P Validation Specificity': 0.733540326200808, 'Hydroxylation-P Validation Precision': 0.42453742960731705, 'Hydroxylation-P AUC ROC': 0.8191387944773401, 'Hydroxylation-P AUC PR': 0.5670685512804072, 'Hydroxylation-P MCC': 0.4257142006643718, 'Hydroxylation-P F1': 0.5385144678539213, 'Validation Loss (Hydroxylation-P)': 0.39891794323921204, 'Validation Loss (total)': 0.8461471557617187, 'TimeToTrain': 10.735747575759888}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006076623758483072,
 'learning_rate_Hydroxylation-K': 0.0037338186356430736,
 'learning_rate_Hydroxylation-P': 0.002260208103648147,
 'log_base': 1.0156350359304847,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 309997183,
 'sample_weights': [2.888041302158615, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.611114541564369,
 'weight_decay_Hydroxylation-K': 9.630532768846898,
 'weight_decay_Hydroxylation-P': 4.138622203679743}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35049.910
Exploding loss, terminate run (best metric=1.1001900434494019)
Finished Training
Total time taken: 0.2090015411376953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34964.930
Exploding loss, terminate run (best metric=1.0980697870254517)
Finished Training
Total time taken: 0.22499966621398926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35006.688
Exploding loss, terminate run (best metric=1.1014049053192139)
Finished Training
Total time taken: 0.19900035858154297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34912.609
Exploding loss, terminate run (best metric=1.0727641582489014)
Finished Training
Total time taken: 0.21599864959716797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34887.758
Exploding loss, terminate run (best metric=1.1273632049560547)
Finished Training
Total time taken: 0.2369997501373291
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34960.836
Exploding loss, terminate run (best metric=1.0964235067367554)
Finished Training
Total time taken: 0.23400020599365234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35062.094
Exploding loss, terminate run (best metric=1.107818365097046)
Finished Training
Total time taken: 0.21699953079223633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35042.113
Exploding loss, terminate run (best metric=1.0949950218200684)
Finished Training
Total time taken: 0.20000100135803223
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34818.641
Exploding loss, terminate run (best metric=1.109464406967163)
Finished Training
Total time taken: 0.2349998950958252
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34973.312
Exploding loss, terminate run (best metric=1.0752694606781006)
Finished Training
Total time taken: 0.22200274467468262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34822.281
Exploding loss, terminate run (best metric=1.1028006076812744)
Finished Training
Total time taken: 0.19900035858154297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35334.074
Exploding loss, terminate run (best metric=1.1037800312042236)
Finished Training
Total time taken: 0.2279975414276123
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34899.031
Exploding loss, terminate run (best metric=1.0960209369659424)
Finished Training
Total time taken: 0.20599865913391113
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35137.941
Exploding loss, terminate run (best metric=1.0720382928848267)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34706.078
Exploding loss, terminate run (best metric=1.1054942607879639)
Finished Training
Total time taken: 0.22600102424621582
{'Hydroxylation-K Validation Accuracy': 0.615661938534279, 'Hydroxylation-K Validation Sensitivity': 0.3037037037037037, 'Hydroxylation-K Validation Specificity': 0.6964912280701754, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6068615984405458, 'Hydroxylation-K AUC PR': 0.3116991374893135, 'Hydroxylation-K MCC': 0.0001542763539620919, 'Hydroxylation-K F1': 0.1078817733990148, 'Validation Loss (Hydroxylation-K)': 0.5601184686024984, 'Hydroxylation-P Validation Accuracy': 0.6393561071350016, 'Hydroxylation-P Validation Sensitivity': 0.30666666666666664, 'Hydroxylation-P Validation Specificity': 0.710838445807771, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6068882230598526, 'Hydroxylation-P AUC PR': 0.28140395463738016, 'Hydroxylation-P MCC': 0.013731121766493111, 'Hydroxylation-P F1': 0.10539326810513253, 'Validation Loss (Hydroxylation-P)': 0.5374746719996134, 'Validation Loss (total)': 1.0975931326548258, 'TimeToTrain': 0.2171333948771159}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008014637138332868,
 'learning_rate_Hydroxylation-K': 0.003523121085218254,
 'learning_rate_Hydroxylation-P': 0.003930957243839901,
 'log_base': 1.718516653687506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 834505055,
 'sample_weights': [107.68816635722715, 13.433051171739335],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.72413768594391,
 'weight_decay_Hydroxylation-K': 3.8602593104465015,
 'weight_decay_Hydroxylation-P': 7.531035521582096}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1562.359
[2,     1] loss: 1565.387
[3,     1] loss: 1565.453
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007054368369450661,
 'learning_rate_Hydroxylation-K': 0.003118392915907996,
 'learning_rate_Hydroxylation-P': 0.002770812679993035,
 'log_base': 1.3609185083346202,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 807901858,
 'sample_weights': [3.0832166691166436, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.470917185470082,
 'weight_decay_Hydroxylation-K': 4.845989317135292,
 'weight_decay_Hydroxylation-P': 4.051066289632663}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2051.819
[2,     1] loss: 2056.769
[3,     1] loss: 2043.516
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002909082326058814,
 'learning_rate_Hydroxylation-K': 0.009088557996707419,
 'learning_rate_Hydroxylation-P': 0.00870721537366294,
 'log_base': 2.9547746522531058,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 439515594,
 'sample_weights': [5.417458411505528, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.725881815755162,
 'weight_decay_Hydroxylation-K': 2.4907963514051135,
 'weight_decay_Hydroxylation-P': 3.64163314888713}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.001
[2,     1] loss: 1234.926
[3,     1] loss: 1230.535
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007302208254017289,
 'learning_rate_Hydroxylation-K': 0.0011894179094417965,
 'learning_rate_Hydroxylation-P': 0.00591983354345718,
 'log_base': 1.0400314772084363,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 124182507,
 'sample_weights': [1.5408977751455613, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.197028971975746,
 'weight_decay_Hydroxylation-K': 1.628925855040559,
 'weight_decay_Hydroxylation-P': 4.653077570879009}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13855.002
[2,     1] loss: 13788.713
[3,     1] loss: 13802.391
[4,     1] loss: 13836.760
[5,     1] loss: 13786.496
[6,     1] loss: 13808.348
[7,     1] loss: 13798.827
[8,     1] loss: 13759.601
[9,     1] loss: 13744.807
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007275748781964802,
 'learning_rate_Hydroxylation-K': 0.001033342792849909,
 'learning_rate_Hydroxylation-P': 0.001163717211697662,
 'log_base': 1.1153101291938243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1671112999,
 'sample_weights': [42.53252223690905, 5.3167700950575565],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.871209750517288,
 'weight_decay_Hydroxylation-K': 7.409654330298981,
 'weight_decay_Hydroxylation-P': 1.0838428784655907}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4944.967
[2,     1] loss: 4971.104
[3,     1] loss: 4967.575
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008936897334249646,
 'learning_rate_Hydroxylation-K': 0.00419615193403714,
 'learning_rate_Hydroxylation-P': 0.0030591298050520817,
 'log_base': 1.8806478612425077,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 996383123,
 'sample_weights': [15.297395455608823, 1.912248096588805],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7358061032573335,
 'weight_decay_Hydroxylation-K': 5.791824706151329,
 'weight_decay_Hydroxylation-P': 0.14249521046960956}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1468.332
[2,     1] loss: 1464.240
[3,     1] loss: 1467.123
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005593842501063134,
 'learning_rate_Hydroxylation-K': 0.003230278015811153,
 'learning_rate_Hydroxylation-P': 0.0064841276480564515,
 'log_base': 1.069687362779926,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2676933363,
 'sample_weights': [2.6431285617674565, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.314256982803024,
 'weight_decay_Hydroxylation-K': 7.558504246939792,
 'weight_decay_Hydroxylation-P': 8.69878271097512}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8087.575
[2,     1] loss: 8048.282
[3,     1] loss: 8096.840
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064278511811853315,
 'learning_rate_Hydroxylation-K': 0.0007803050929497436,
 'learning_rate_Hydroxylation-P': 0.001360901011289212,
 'log_base': 2.0555228561919865,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3397373180,
 'sample_weights': [24.781532270783885, 3.0978108693650492],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.823854319783585,
 'weight_decay_Hydroxylation-K': 1.8970972615686574,
 'weight_decay_Hydroxylation-P': 0.6805776056771199}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.672
[2,     1] loss: 1403.372
[3,     1] loss: 1398.475
[4,     1] loss: 1394.690
[5,     1] loss: 1399.116
[6,     1] loss: 1394.621
[7,     1] loss: 1395.457
[8,     1] loss: 1394.455
[9,     1] loss: 1391.267
[10,     1] loss: 1386.201
[11,     1] loss: 1378.313
[12,     1] loss: 1358.354
[13,     1] loss: 1317.132
[14,     1] loss: 1293.711
[15,     1] loss: 1241.555
[16,     1] loss: 1188.873
[17,     1] loss: 1229.473
[18,     1] loss: 1194.648
[19,     1] loss: 1198.581
[20,     1] loss: 1198.289
[21,     1] loss: 1185.646
[22,     1] loss: 1200.386
[23,     1] loss: 1147.579
[24,     1] loss: 1151.017
[25,     1] loss: 1125.107
[26,     1] loss: 1138.442
[27,     1] loss: 1072.924
[28,     1] loss: 1102.557
[29,     1] loss: 1103.371
[30,     1] loss: 1094.564
[31,     1] loss: 1013.890
[32,     1] loss: 1072.392
[33,     1] loss: 1100.348
[34,     1] loss: 1062.372
[35,     1] loss: 1029.127
[36,     1] loss: 1056.479
[37,     1] loss: 978.994
[38,     1] loss: 1029.025
[39,     1] loss: 990.470
[40,     1] loss: 1062.163
[41,     1] loss: 1059.575
[42,     1] loss: 940.362
[43,     1] loss: 982.330
[44,     1] loss: 965.511
[45,     1] loss: 916.214
[46,     1] loss: 1001.530
[47,     1] loss: 910.780
[48,     1] loss: 949.329
[49,     1] loss: 961.646
[50,     1] loss: 1136.382
[51,     1] loss: 1012.164
[52,     1] loss: 1010.257
[53,     1] loss: 947.203
[54,     1] loss: 971.574
[55,     1] loss: 927.411
[56,     1] loss: 903.901
[57,     1] loss: 912.972
[58,     1] loss: 869.028
[59,     1] loss: 865.938
[60,     1] loss: 1024.010
[61,     1] loss: 906.431
[62,     1] loss: 825.437
[63,     1] loss: 898.375
[64,     1] loss: 814.842
[65,     1] loss: 751.189
[66,     1] loss: 763.515
Early stopping applied (best metric=0.7261186242103577)
Finished Training
Total time taken: 9.086011171340942
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1409.567
[2,     1] loss: 1398.768
[3,     1] loss: 1395.971
[4,     1] loss: 1394.563
[5,     1] loss: 1395.328
[6,     1] loss: 1396.927
[7,     1] loss: 1396.350
[8,     1] loss: 1394.946
[9,     1] loss: 1397.062
[10,     1] loss: 1392.719
[11,     1] loss: 1388.739
[12,     1] loss: 1379.968
[13,     1] loss: 1368.285
[14,     1] loss: 1341.731
[15,     1] loss: 1305.444
[16,     1] loss: 1284.980
[17,     1] loss: 1249.111
[18,     1] loss: 1195.794
[19,     1] loss: 1176.867
[20,     1] loss: 1166.612
[21,     1] loss: 1198.924
[22,     1] loss: 1128.862
[23,     1] loss: 1123.836
[24,     1] loss: 1098.119
[25,     1] loss: 1105.142
[26,     1] loss: 1088.650
[27,     1] loss: 1082.022
[28,     1] loss: 1099.008
[29,     1] loss: 1028.687
[30,     1] loss: 1020.410
[31,     1] loss: 1043.513
[32,     1] loss: 1028.209
[33,     1] loss: 1202.346
[34,     1] loss: 1229.565
[35,     1] loss: 1099.861
[36,     1] loss: 1094.776
[37,     1] loss: 1114.734
[38,     1] loss: 1119.292
[39,     1] loss: 1079.898
[40,     1] loss: 1106.134
[41,     1] loss: 1052.771
[42,     1] loss: 1030.043
[43,     1] loss: 1088.377
[44,     1] loss: 1013.956
[45,     1] loss: 1002.296
[46,     1] loss: 1064.685
[47,     1] loss: 968.511
[48,     1] loss: 1106.949
[49,     1] loss: 1006.671
[50,     1] loss: 996.631
[51,     1] loss: 1007.064
[52,     1] loss: 960.259
[53,     1] loss: 1013.030
[54,     1] loss: 918.309
[55,     1] loss: 1076.967
[56,     1] loss: 905.972
[57,     1] loss: 1029.441
[58,     1] loss: 983.141
[59,     1] loss: 975.951
[60,     1] loss: 977.331
[61,     1] loss: 916.641
[62,     1] loss: 1045.157
[63,     1] loss: 913.000
[64,     1] loss: 958.529
[65,     1] loss: 896.381
[66,     1] loss: 915.923
[67,     1] loss: 894.942
[68,     1] loss: 852.890
[69,     1] loss: 856.763
[70,     1] loss: 1000.462
[71,     1] loss: 1105.523
[72,     1] loss: 827.426
[73,     1] loss: 1008.946
[74,     1] loss: 880.503
[75,     1] loss: 959.143
[76,     1] loss: 891.490
[77,     1] loss: 877.968
[78,     1] loss: 775.634
[79,     1] loss: 922.816
[80,     1] loss: 1138.781
[81,     1] loss: 1368.099
[82,     1] loss: 1046.677
[83,     1] loss: 1034.447
[84,     1] loss: 1207.505
[85,     1] loss: 1058.340
[86,     1] loss: 1069.559
[87,     1] loss: 1055.545
[88,     1] loss: 1012.054
[89,     1] loss: 1012.413
[90,     1] loss: 1053.664
[91,     1] loss: 973.805
[92,     1] loss: 1006.087
[93,     1] loss: 952.725
[94,     1] loss: 976.215
[95,     1] loss: 957.699
Early stopping applied (best metric=0.8341363072395325)
Finished Training
Total time taken: 14.21601128578186
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.666
[2,     1] loss: 1397.684
[3,     1] loss: 1399.482
[4,     1] loss: 1393.248
[5,     1] loss: 1400.366
[6,     1] loss: 1392.712
[7,     1] loss: 1388.843
[8,     1] loss: 1382.382
[9,     1] loss: 1370.264
[10,     1] loss: 1348.015
[11,     1] loss: 1297.721
[12,     1] loss: 1280.476
[13,     1] loss: 1259.018
[14,     1] loss: 1209.361
[15,     1] loss: 1158.656
[16,     1] loss: 1173.769
[17,     1] loss: 1127.684
[18,     1] loss: 1162.914
[19,     1] loss: 1104.341
[20,     1] loss: 1104.561
[21,     1] loss: 1109.049
[22,     1] loss: 1121.015
[23,     1] loss: 1083.069
[24,     1] loss: 1041.064
[25,     1] loss: 1068.479
[26,     1] loss: 1050.773
[27,     1] loss: 1170.415
[28,     1] loss: 1073.952
[29,     1] loss: 1041.934
[30,     1] loss: 1053.893
[31,     1] loss: 1046.447
[32,     1] loss: 1001.229
[33,     1] loss: 1022.234
[34,     1] loss: 938.544
[35,     1] loss: 972.385
[36,     1] loss: 965.401
[37,     1] loss: 906.225
[38,     1] loss: 936.989
[39,     1] loss: 1108.540
[40,     1] loss: 1179.911
[41,     1] loss: 947.505
[42,     1] loss: 1050.068
[43,     1] loss: 1057.627
[44,     1] loss: 972.249
[45,     1] loss: 974.379
[46,     1] loss: 967.302
[47,     1] loss: 919.703
[48,     1] loss: 909.016
[49,     1] loss: 966.414
[50,     1] loss: 912.452
[51,     1] loss: 903.599
[52,     1] loss: 872.766
[53,     1] loss: 929.725
[54,     1] loss: 951.663
[55,     1] loss: 972.695
[56,     1] loss: 789.329
[57,     1] loss: 847.760
[58,     1] loss: 959.714
[59,     1] loss: 795.123
[60,     1] loss: 910.325
[61,     1] loss: 913.887
[62,     1] loss: 796.619
[63,     1] loss: 824.074
[64,     1] loss: 778.588
[65,     1] loss: 788.032
[66,     1] loss: 827.365
[67,     1] loss: 780.418
[68,     1] loss: 713.945
[69,     1] loss: 685.335
[70,     1] loss: 673.163
[71,     1] loss: 1080.655
Early stopping applied (best metric=0.8603665828704834)
Finished Training
Total time taken: 11.886011600494385
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.120
[2,     1] loss: 1409.809
[3,     1] loss: 1402.297
[4,     1] loss: 1398.121
[5,     1] loss: 1398.130
[6,     1] loss: 1398.841
[7,     1] loss: 1397.490
[8,     1] loss: 1396.473
[9,     1] loss: 1395.155
[10,     1] loss: 1395.408
[11,     1] loss: 1392.933
[12,     1] loss: 1398.592
[13,     1] loss: 1395.214
[14,     1] loss: 1391.393
[15,     1] loss: 1389.774
[16,     1] loss: 1384.692
[17,     1] loss: 1375.185
[18,     1] loss: 1368.688
[19,     1] loss: 1338.169
[20,     1] loss: 1296.988
[21,     1] loss: 1286.517
[22,     1] loss: 1251.841
[23,     1] loss: 1207.983
[24,     1] loss: 1267.039
[25,     1] loss: 1202.410
[26,     1] loss: 1181.712
[27,     1] loss: 1165.826
[28,     1] loss: 1158.635
[29,     1] loss: 1166.615
[30,     1] loss: 1113.836
[31,     1] loss: 1155.868
[32,     1] loss: 1072.562
[33,     1] loss: 1101.015
[34,     1] loss: 1109.483
[35,     1] loss: 1148.664
[36,     1] loss: 1043.228
[37,     1] loss: 1047.472
[38,     1] loss: 1080.747
[39,     1] loss: 1011.433
[40,     1] loss: 1111.581
[41,     1] loss: 1039.230
[42,     1] loss: 1113.579
[43,     1] loss: 1042.796
[44,     1] loss: 1095.352
[45,     1] loss: 993.464
[46,     1] loss: 1062.974
[47,     1] loss: 1037.785
[48,     1] loss: 1039.750
[49,     1] loss: 967.536
[50,     1] loss: 995.798
[51,     1] loss: 976.237
[52,     1] loss: 951.213
[53,     1] loss: 958.471
[54,     1] loss: 984.417
[55,     1] loss: 994.923
[56,     1] loss: 989.646
[57,     1] loss: 968.979
[58,     1] loss: 961.268
[59,     1] loss: 908.325
[60,     1] loss: 867.650
[61,     1] loss: 1000.544
[62,     1] loss: 1235.227
[63,     1] loss: 1119.094
[64,     1] loss: 964.331
[65,     1] loss: 1018.733
[66,     1] loss: 1032.596
[67,     1] loss: 1000.613
[68,     1] loss: 1040.841
[69,     1] loss: 966.803
[70,     1] loss: 986.138
[71,     1] loss: 946.539
[72,     1] loss: 923.831
[73,     1] loss: 917.778
[74,     1] loss: 945.743
[75,     1] loss: 853.101
[76,     1] loss: 840.510
[77,     1] loss: 953.502
[78,     1] loss: 1968.872
[79,     1] loss: 1027.073
[80,     1] loss: 1343.102
[81,     1] loss: 1214.450
[82,     1] loss: 1263.431
[83,     1] loss: 1278.619
[84,     1] loss: 1259.963
[85,     1] loss: 1213.371
[86,     1] loss: 1179.301
[87,     1] loss: 1141.397
[88,     1] loss: 1169.804
[89,     1] loss: 1176.821
[90,     1] loss: 1139.622
[91,     1] loss: 1147.037
[92,     1] loss: 1123.635
[93,     1] loss: 1095.185
[94,     1] loss: 1131.122
[95,     1] loss: 1054.725
[96,     1] loss: 1071.411
Early stopping applied (best metric=0.756596565246582)
Finished Training
Total time taken: 15.718017816543579
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1399.538
[2,     1] loss: 1394.994
[3,     1] loss: 1397.864
[4,     1] loss: 1398.398
[5,     1] loss: 1390.355
[6,     1] loss: 1391.470
[7,     1] loss: 1372.358
[8,     1] loss: 1353.037
[9,     1] loss: 1312.364
[10,     1] loss: 1240.750
[11,     1] loss: 1206.097
[12,     1] loss: 1177.007
[13,     1] loss: 1183.539
[14,     1] loss: 1219.514
[15,     1] loss: 1238.696
[16,     1] loss: 1150.340
[17,     1] loss: 1188.422
[18,     1] loss: 1167.058
[19,     1] loss: 1165.409
[20,     1] loss: 1183.871
[21,     1] loss: 1105.158
[22,     1] loss: 1098.353
[23,     1] loss: 1089.081
[24,     1] loss: 1057.604
[25,     1] loss: 1040.766
[26,     1] loss: 1001.203
[27,     1] loss: 994.786
[28,     1] loss: 969.128
[29,     1] loss: 1061.670
[30,     1] loss: 1071.665
[31,     1] loss: 1001.918
[32,     1] loss: 1012.892
[33,     1] loss: 1033.993
[34,     1] loss: 953.853
[35,     1] loss: 973.976
[36,     1] loss: 944.330
[37,     1] loss: 910.827
[38,     1] loss: 901.762
[39,     1] loss: 1387.033
[40,     1] loss: 1094.580
[41,     1] loss: 956.941
[42,     1] loss: 1023.338
[43,     1] loss: 1096.501
[44,     1] loss: 1006.719
[45,     1] loss: 986.485
[46,     1] loss: 1059.366
[47,     1] loss: 962.055
[48,     1] loss: 978.837
Early stopping applied (best metric=0.8201236724853516)
Finished Training
Total time taken: 8.028006076812744
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.897
[2,     1] loss: 1402.384
[3,     1] loss: 1398.834
[4,     1] loss: 1399.656
[5,     1] loss: 1393.278
[6,     1] loss: 1392.711
[7,     1] loss: 1396.549
[8,     1] loss: 1393.817
[9,     1] loss: 1388.810
[10,     1] loss: 1378.621
[11,     1] loss: 1363.444
[12,     1] loss: 1330.894
[13,     1] loss: 1299.480
[14,     1] loss: 1230.990
[15,     1] loss: 1199.273
[16,     1] loss: 1182.803
[17,     1] loss: 1146.065
[18,     1] loss: 1130.912
[19,     1] loss: 1159.641
[20,     1] loss: 1081.653
[21,     1] loss: 1133.587
[22,     1] loss: 1108.626
[23,     1] loss: 1160.905
[24,     1] loss: 1093.154
[25,     1] loss: 1066.746
[26,     1] loss: 1079.168
[27,     1] loss: 1049.325
[28,     1] loss: 1049.576
[29,     1] loss: 1083.832
[30,     1] loss: 1078.156
[31,     1] loss: 1064.655
[32,     1] loss: 1028.963
[33,     1] loss: 1115.241
[34,     1] loss: 1012.739
[35,     1] loss: 999.866
[36,     1] loss: 957.758
[37,     1] loss: 998.217
[38,     1] loss: 900.721
[39,     1] loss: 1085.465
[40,     1] loss: 903.540
[41,     1] loss: 986.984
[42,     1] loss: 873.067
[43,     1] loss: 965.982
[44,     1] loss: 868.413
[45,     1] loss: 982.493
[46,     1] loss: 871.064
[47,     1] loss: 853.679
[48,     1] loss: 964.575
[49,     1] loss: 829.914
Early stopping applied (best metric=0.8827030658721924)
Finished Training
Total time taken: 8.288012027740479
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.978
[2,     1] loss: 1399.538
[3,     1] loss: 1398.027
[4,     1] loss: 1395.125
[5,     1] loss: 1389.630
[6,     1] loss: 1401.225
[7,     1] loss: 1401.673
[8,     1] loss: 1393.310
[9,     1] loss: 1394.711
[10,     1] loss: 1394.335
[11,     1] loss: 1387.200
[12,     1] loss: 1381.242
[13,     1] loss: 1362.867
[14,     1] loss: 1332.135
[15,     1] loss: 1295.433
[16,     1] loss: 1263.909
[17,     1] loss: 1188.019
[18,     1] loss: 1194.270
[19,     1] loss: 1126.239
[20,     1] loss: 1092.380
[21,     1] loss: 1192.351
[22,     1] loss: 1122.291
[23,     1] loss: 1141.324
[24,     1] loss: 1139.049
[25,     1] loss: 1101.640
[26,     1] loss: 1098.455
[27,     1] loss: 1126.549
[28,     1] loss: 1107.177
[29,     1] loss: 1004.719
[30,     1] loss: 977.856
[31,     1] loss: 1013.023
[32,     1] loss: 1007.558
[33,     1] loss: 1021.234
[34,     1] loss: 1001.963
[35,     1] loss: 976.770
[36,     1] loss: 1005.963
[37,     1] loss: 966.797
[38,     1] loss: 945.562
[39,     1] loss: 864.534
[40,     1] loss: 846.740
[41,     1] loss: 1168.120
[42,     1] loss: 1681.927
[43,     1] loss: 908.918
[44,     1] loss: 1078.725
[45,     1] loss: 1177.806
[46,     1] loss: 1143.478
[47,     1] loss: 1116.480
[48,     1] loss: 1124.966
[49,     1] loss: 1134.176
[50,     1] loss: 1115.476
[51,     1] loss: 1038.759
[52,     1] loss: 1056.757
[53,     1] loss: 1074.156
[54,     1] loss: 1046.549
Early stopping applied (best metric=0.8909420967102051)
Finished Training
Total time taken: 7.34900689125061
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.189
[2,     1] loss: 1406.623
[3,     1] loss: 1398.366
[4,     1] loss: 1394.481
[5,     1] loss: 1400.863
[6,     1] loss: 1395.478
[7,     1] loss: 1396.003
[8,     1] loss: 1392.590
[9,     1] loss: 1391.352
[10,     1] loss: 1390.667
[11,     1] loss: 1382.832
[12,     1] loss: 1366.117
[13,     1] loss: 1343.292
[14,     1] loss: 1310.540
[15,     1] loss: 1294.586
[16,     1] loss: 1245.254
[17,     1] loss: 1238.265
[18,     1] loss: 1179.206
[19,     1] loss: 1143.088
[20,     1] loss: 1138.400
[21,     1] loss: 1060.565
[22,     1] loss: 1166.774
[23,     1] loss: 1213.763
[24,     1] loss: 1061.037
[25,     1] loss: 1122.385
[26,     1] loss: 1123.736
[27,     1] loss: 1068.234
[28,     1] loss: 1076.021
[29,     1] loss: 1001.477
[30,     1] loss: 1034.569
[31,     1] loss: 985.207
[32,     1] loss: 1003.789
[33,     1] loss: 1120.050
[34,     1] loss: 1072.593
[35,     1] loss: 956.209
[36,     1] loss: 1042.429
[37,     1] loss: 1017.389
[38,     1] loss: 959.522
[39,     1] loss: 1061.510
[40,     1] loss: 923.340
[41,     1] loss: 962.094
[42,     1] loss: 906.369
[43,     1] loss: 926.978
[44,     1] loss: 883.605
[45,     1] loss: 950.292
[46,     1] loss: 893.680
[47,     1] loss: 976.938
[48,     1] loss: 1016.703
[49,     1] loss: 936.423
[50,     1] loss: 1029.698
[51,     1] loss: 959.866
[52,     1] loss: 922.324
[53,     1] loss: 940.922
[54,     1] loss: 913.997
[55,     1] loss: 883.268
[56,     1] loss: 880.030
[57,     1] loss: 848.326
[58,     1] loss: 914.320
[59,     1] loss: 1066.772
[60,     1] loss: 865.217
[61,     1] loss: 899.827
[62,     1] loss: 851.746
[63,     1] loss: 899.472
[64,     1] loss: 865.135
[65,     1] loss: 810.745
[66,     1] loss: 765.908
[67,     1] loss: 804.543
[68,     1] loss: 804.474
Early stopping applied (best metric=0.9794875383377075)
Finished Training
Total time taken: 11.393011569976807
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.134
[2,     1] loss: 1407.072
[3,     1] loss: 1393.871
[4,     1] loss: 1398.862
[5,     1] loss: 1396.528
[6,     1] loss: 1396.075
[7,     1] loss: 1395.965
[8,     1] loss: 1395.531
[9,     1] loss: 1392.413
[10,     1] loss: 1390.909
[11,     1] loss: 1386.159
[12,     1] loss: 1378.889
[13,     1] loss: 1357.456
[14,     1] loss: 1326.229
[15,     1] loss: 1288.622
[16,     1] loss: 1234.919
[17,     1] loss: 1164.863
[18,     1] loss: 1192.207
[19,     1] loss: 1252.667
[20,     1] loss: 1229.932
[21,     1] loss: 1157.150
[22,     1] loss: 1166.424
[23,     1] loss: 1149.199
[24,     1] loss: 1117.678
[25,     1] loss: 1171.572
[26,     1] loss: 1127.729
[27,     1] loss: 1110.291
[28,     1] loss: 1079.422
[29,     1] loss: 1064.658
[30,     1] loss: 1054.251
[31,     1] loss: 1034.644
[32,     1] loss: 1031.600
[33,     1] loss: 1013.446
[34,     1] loss: 1086.414
[35,     1] loss: 1181.917
[36,     1] loss: 1027.315
[37,     1] loss: 1103.440
[38,     1] loss: 1065.278
[39,     1] loss: 963.939
[40,     1] loss: 1024.494
[41,     1] loss: 974.679
[42,     1] loss: 964.601
[43,     1] loss: 940.173
[44,     1] loss: 1021.603
[45,     1] loss: 906.159
[46,     1] loss: 1011.199
[47,     1] loss: 952.223
[48,     1] loss: 971.567
[49,     1] loss: 921.252
[50,     1] loss: 912.701
[51,     1] loss: 876.219
[52,     1] loss: 862.973
[53,     1] loss: 924.257
[54,     1] loss: 1124.836
[55,     1] loss: 867.552
[56,     1] loss: 1049.017
[57,     1] loss: 925.306
[58,     1] loss: 953.142
[59,     1] loss: 843.802
[60,     1] loss: 1035.148
[61,     1] loss: 902.910
[62,     1] loss: 834.305
[63,     1] loss: 839.847
[64,     1] loss: 818.155
Early stopping applied (best metric=0.8531132340431213)
Finished Training
Total time taken: 10.754013061523438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1402.341
[2,     1] loss: 1403.916
[3,     1] loss: 1400.667
[4,     1] loss: 1404.372
[5,     1] loss: 1398.395
[6,     1] loss: 1397.249
[7,     1] loss: 1393.120
[8,     1] loss: 1395.585
[9,     1] loss: 1391.480
[10,     1] loss: 1397.161
[11,     1] loss: 1386.868
[12,     1] loss: 1379.760
[13,     1] loss: 1357.490
[14,     1] loss: 1345.970
[15,     1] loss: 1311.244
[16,     1] loss: 1286.596
[17,     1] loss: 1247.278
[18,     1] loss: 1212.305
[19,     1] loss: 1192.209
[20,     1] loss: 1131.176
[21,     1] loss: 1149.755
[22,     1] loss: 1128.501
[23,     1] loss: 1107.151
[24,     1] loss: 1106.647
[25,     1] loss: 1134.383
[26,     1] loss: 1165.715
[27,     1] loss: 1074.765
[28,     1] loss: 1132.598
[29,     1] loss: 1091.082
[30,     1] loss: 1118.277
[31,     1] loss: 1069.312
[32,     1] loss: 1121.789
[33,     1] loss: 1078.654
[34,     1] loss: 1046.368
[35,     1] loss: 1025.053
[36,     1] loss: 1057.633
[37,     1] loss: 1038.917
[38,     1] loss: 1046.035
[39,     1] loss: 1007.552
[40,     1] loss: 1002.327
[41,     1] loss: 1055.257
[42,     1] loss: 964.232
[43,     1] loss: 1000.323
[44,     1] loss: 1105.472
[45,     1] loss: 1022.160
[46,     1] loss: 1036.638
[47,     1] loss: 1020.180
[48,     1] loss: 1034.155
[49,     1] loss: 946.593
[50,     1] loss: 988.132
[51,     1] loss: 890.806
[52,     1] loss: 990.247
[53,     1] loss: 1055.082
[54,     1] loss: 1053.335
[55,     1] loss: 939.506
[56,     1] loss: 1020.895
[57,     1] loss: 929.035
[58,     1] loss: 903.642
[59,     1] loss: 1014.380
[60,     1] loss: 867.391
[61,     1] loss: 916.620
[62,     1] loss: 976.172
[63,     1] loss: 858.040
[64,     1] loss: 937.400
[65,     1] loss: 950.406
[66,     1] loss: 880.486
[67,     1] loss: 1098.251
[68,     1] loss: 848.886
[69,     1] loss: 1007.379
[70,     1] loss: 824.273
[71,     1] loss: 1023.706
Early stopping applied (best metric=0.7144805192947388)
Finished Training
Total time taken: 9.603006601333618
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.101
[2,     1] loss: 1398.290
[3,     1] loss: 1398.264
[4,     1] loss: 1396.129
[5,     1] loss: 1392.137
[6,     1] loss: 1401.782
[7,     1] loss: 1395.703
[8,     1] loss: 1393.738
[9,     1] loss: 1395.017
[10,     1] loss: 1391.762
[11,     1] loss: 1388.601
[12,     1] loss: 1379.696
[13,     1] loss: 1368.932
[14,     1] loss: 1347.727
[15,     1] loss: 1304.965
[16,     1] loss: 1270.999
[17,     1] loss: 1223.610
[18,     1] loss: 1157.546
[19,     1] loss: 1266.534
[20,     1] loss: 1270.584
[21,     1] loss: 1128.989
[22,     1] loss: 1162.741
[23,     1] loss: 1119.772
[24,     1] loss: 1132.402
[25,     1] loss: 1170.152
[26,     1] loss: 1093.711
[27,     1] loss: 1100.443
[28,     1] loss: 1100.594
[29,     1] loss: 1122.054
[30,     1] loss: 994.467
[31,     1] loss: 1037.754
[32,     1] loss: 1037.370
[33,     1] loss: 980.033
[34,     1] loss: 1000.673
[35,     1] loss: 996.370
[36,     1] loss: 952.556
[37,     1] loss: 955.474
[38,     1] loss: 927.636
[39,     1] loss: 945.808
[40,     1] loss: 922.512
[41,     1] loss: 1172.355
[42,     1] loss: 1423.596
[43,     1] loss: 1140.561
[44,     1] loss: 1138.568
[45,     1] loss: 1190.354
[46,     1] loss: 1178.716
[47,     1] loss: 1192.294
[48,     1] loss: 1159.622
[49,     1] loss: 1144.222
[50,     1] loss: 1127.983
[51,     1] loss: 1125.470
[52,     1] loss: 1069.694
[53,     1] loss: 1022.143
[54,     1] loss: 999.195
[55,     1] loss: 1009.488
[56,     1] loss: 1042.024
[57,     1] loss: 960.845
[58,     1] loss: 975.069
[59,     1] loss: 934.417
[60,     1] loss: 988.438
[61,     1] loss: 925.190
[62,     1] loss: 913.575
Early stopping applied (best metric=0.9290357828140259)
Finished Training
Total time taken: 10.508009195327759
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.033
[2,     1] loss: 1396.345
[3,     1] loss: 1401.609
[4,     1] loss: 1400.101
[5,     1] loss: 1395.501
[6,     1] loss: 1401.400
[7,     1] loss: 1390.495
[8,     1] loss: 1389.411
[9,     1] loss: 1386.246
[10,     1] loss: 1375.168
[11,     1] loss: 1341.725
[12,     1] loss: 1304.914
[13,     1] loss: 1273.144
[14,     1] loss: 1253.021
[15,     1] loss: 1220.962
[16,     1] loss: 1167.840
[17,     1] loss: 1176.438
[18,     1] loss: 1135.311
[19,     1] loss: 1173.744
[20,     1] loss: 1210.517
[21,     1] loss: 1112.029
[22,     1] loss: 1165.141
[23,     1] loss: 1150.624
[24,     1] loss: 1153.250
[25,     1] loss: 1190.235
[26,     1] loss: 1122.286
[27,     1] loss: 1137.455
[28,     1] loss: 1107.319
[29,     1] loss: 1059.979
[30,     1] loss: 1126.566
[31,     1] loss: 1016.383
[32,     1] loss: 1062.561
[33,     1] loss: 1040.689
[34,     1] loss: 1069.739
[35,     1] loss: 1025.660
[36,     1] loss: 1036.491
[37,     1] loss: 995.029
[38,     1] loss: 976.722
[39,     1] loss: 1085.621
[40,     1] loss: 1010.614
[41,     1] loss: 970.705
[42,     1] loss: 963.143
[43,     1] loss: 926.431
[44,     1] loss: 924.101
[45,     1] loss: 947.545
[46,     1] loss: 1007.173
[47,     1] loss: 1015.429
[48,     1] loss: 923.459
[49,     1] loss: 1005.332
[50,     1] loss: 877.659
[51,     1] loss: 908.704
[52,     1] loss: 861.844
[53,     1] loss: 870.517
[54,     1] loss: 866.504
[55,     1] loss: 1007.104
[56,     1] loss: 948.802
[57,     1] loss: 789.440
[58,     1] loss: 875.596
[59,     1] loss: 919.795
[60,     1] loss: 826.846
[61,     1] loss: 787.505
[62,     1] loss: 807.817
[63,     1] loss: 910.451
[64,     1] loss: 1042.315
[65,     1] loss: 1217.127
[66,     1] loss: 945.063
[67,     1] loss: 999.062
[68,     1] loss: 1099.910
[69,     1] loss: 989.816
[70,     1] loss: 971.842
[71,     1] loss: 970.128
[72,     1] loss: 933.885
Early stopping applied (best metric=0.7640725374221802)
Finished Training
Total time taken: 9.80400824546814
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.125
[2,     1] loss: 1399.133
[3,     1] loss: 1400.402
[4,     1] loss: 1398.150
[5,     1] loss: 1398.987
[6,     1] loss: 1393.533
[7,     1] loss: 1391.569
[8,     1] loss: 1397.557
[9,     1] loss: 1394.461
[10,     1] loss: 1387.705
[11,     1] loss: 1384.172
[12,     1] loss: 1368.467
[13,     1] loss: 1344.280
[14,     1] loss: 1319.104
[15,     1] loss: 1303.103
[16,     1] loss: 1238.972
[17,     1] loss: 1203.631
[18,     1] loss: 1182.057
[19,     1] loss: 1179.509
[20,     1] loss: 1138.499
[21,     1] loss: 1201.514
[22,     1] loss: 1125.253
[23,     1] loss: 1136.468
[24,     1] loss: 1089.241
[25,     1] loss: 1072.854
[26,     1] loss: 1079.320
[27,     1] loss: 1072.201
[28,     1] loss: 1058.050
[29,     1] loss: 1089.482
[30,     1] loss: 993.349
[31,     1] loss: 1112.975
[32,     1] loss: 1022.899
[33,     1] loss: 1065.633
[34,     1] loss: 1052.735
[35,     1] loss: 990.023
[36,     1] loss: 1003.611
[37,     1] loss: 1051.107
[38,     1] loss: 950.306
[39,     1] loss: 914.906
[40,     1] loss: 1054.862
[41,     1] loss: 945.623
[42,     1] loss: 1013.765
[43,     1] loss: 958.631
[44,     1] loss: 975.931
[45,     1] loss: 895.902
[46,     1] loss: 905.215
[47,     1] loss: 933.099
[48,     1] loss: 848.622
[49,     1] loss: 829.975
[50,     1] loss: 838.309
[51,     1] loss: 993.281
[52,     1] loss: 1279.150
[53,     1] loss: 938.441
[54,     1] loss: 1049.714
[55,     1] loss: 1122.768
[56,     1] loss: 1001.858
[57,     1] loss: 1002.484
[58,     1] loss: 999.655
[59,     1] loss: 923.409
[60,     1] loss: 946.320
[61,     1] loss: 933.219
[62,     1] loss: 979.477
Early stopping applied (best metric=0.88067227602005)
Finished Training
Total time taken: 10.460009813308716
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.366
[2,     1] loss: 1398.914
[3,     1] loss: 1394.359
[4,     1] loss: 1395.302
[5,     1] loss: 1394.210
[6,     1] loss: 1387.656
[7,     1] loss: 1379.442
[8,     1] loss: 1362.484
[9,     1] loss: 1351.848
[10,     1] loss: 1291.886
[11,     1] loss: 1305.968
[12,     1] loss: 1276.377
[13,     1] loss: 1229.557
[14,     1] loss: 1177.449
[15,     1] loss: 1189.422
[16,     1] loss: 1173.569
[17,     1] loss: 1181.467
[18,     1] loss: 1156.088
[19,     1] loss: 1157.627
[20,     1] loss: 1141.642
[21,     1] loss: 1099.294
[22,     1] loss: 1082.603
[23,     1] loss: 1161.510
[24,     1] loss: 1137.886
[25,     1] loss: 1054.762
[26,     1] loss: 1082.008
[27,     1] loss: 1047.343
[28,     1] loss: 1041.961
[29,     1] loss: 1045.380
[30,     1] loss: 996.287
[31,     1] loss: 1069.752
[32,     1] loss: 1064.548
[33,     1] loss: 980.370
[34,     1] loss: 967.252
[35,     1] loss: 941.012
[36,     1] loss: 946.845
[37,     1] loss: 929.575
[38,     1] loss: 976.299
[39,     1] loss: 1276.273
[40,     1] loss: 1001.920
[41,     1] loss: 1126.784
[42,     1] loss: 1072.648
[43,     1] loss: 1089.430
[44,     1] loss: 1084.070
[45,     1] loss: 1075.138
[46,     1] loss: 1024.011
[47,     1] loss: 1003.948
[48,     1] loss: 1038.950
[49,     1] loss: 964.625
[50,     1] loss: 992.042
[51,     1] loss: 908.231
[52,     1] loss: 1017.688
[53,     1] loss: 895.170
[54,     1] loss: 1006.836
[55,     1] loss: 902.822
[56,     1] loss: 913.573
[57,     1] loss: 924.112
[58,     1] loss: 782.635
[59,     1] loss: 945.499
[60,     1] loss: 1311.395
[61,     1] loss: 862.826
[62,     1] loss: 1161.762
[63,     1] loss: 976.589
[64,     1] loss: 988.944
[65,     1] loss: 1033.021
[66,     1] loss: 1003.912
[67,     1] loss: 992.309
[68,     1] loss: 952.323
[69,     1] loss: 920.638
[70,     1] loss: 1020.862
[71,     1] loss: 898.463
[72,     1] loss: 904.674
[73,     1] loss: 879.943
[74,     1] loss: 914.737
[75,     1] loss: 901.902
[76,     1] loss: 745.347
[77,     1] loss: 857.833
[78,     1] loss: 859.330
[79,     1] loss: 833.826
[80,     1] loss: 833.377
[81,     1] loss: 769.528
[82,     1] loss: 755.610
[83,     1] loss: 890.623
[84,     1] loss: 1647.964
[85,     1] loss: 997.003
[86,     1] loss: 1096.218
[87,     1] loss: 1136.947
[88,     1] loss: 1119.107
[89,     1] loss: 1063.539
[90,     1] loss: 1010.878
[91,     1] loss: 1017.545
[92,     1] loss: 982.281
[93,     1] loss: 1036.510
[94,     1] loss: 991.636
Early stopping applied (best metric=0.7968481779098511)
Finished Training
Total time taken: 15.857017040252686
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1405.929
[2,     1] loss: 1403.715
[3,     1] loss: 1393.123
[4,     1] loss: 1401.719
[5,     1] loss: 1399.953
[6,     1] loss: 1397.302
[7,     1] loss: 1398.538
[8,     1] loss: 1395.499
[9,     1] loss: 1397.662
[10,     1] loss: 1391.276
[11,     1] loss: 1385.457
[12,     1] loss: 1373.935
[13,     1] loss: 1352.345
[14,     1] loss: 1312.334
[15,     1] loss: 1301.600
[16,     1] loss: 1249.928
[17,     1] loss: 1220.839
[18,     1] loss: 1216.399
[19,     1] loss: 1185.439
[20,     1] loss: 1164.762
[21,     1] loss: 1140.289
[22,     1] loss: 1129.407
[23,     1] loss: 1127.219
[24,     1] loss: 1089.844
[25,     1] loss: 1131.779
[26,     1] loss: 1108.764
[27,     1] loss: 1108.989
[28,     1] loss: 1079.522
[29,     1] loss: 1111.807
[30,     1] loss: 1180.786
[31,     1] loss: 1108.918
[32,     1] loss: 1112.470
[33,     1] loss: 1076.232
[34,     1] loss: 1066.981
[35,     1] loss: 1030.142
[36,     1] loss: 1071.072
[37,     1] loss: 1005.422
[38,     1] loss: 1044.253
[39,     1] loss: 943.004
[40,     1] loss: 990.698
[41,     1] loss: 999.959
[42,     1] loss: 957.051
[43,     1] loss: 1074.262
[44,     1] loss: 943.377
[45,     1] loss: 1012.819
[46,     1] loss: 957.065
[47,     1] loss: 993.123
[48,     1] loss: 911.764
[49,     1] loss: 971.449
[50,     1] loss: 1022.752
[51,     1] loss: 873.236
[52,     1] loss: 946.597
[53,     1] loss: 899.462
[54,     1] loss: 946.526
[55,     1] loss: 920.311
[56,     1] loss: 813.702
Early stopping applied (best metric=0.8069794178009033)
Finished Training
Total time taken: 9.519012212753296
{'Hydroxylation-K Validation Accuracy': 0.7523640661938534, 'Hydroxylation-K Validation Sensitivity': 0.7044444444444444, 'Hydroxylation-K Validation Specificity': 0.7649122807017543, 'Hydroxylation-K Validation Precision': 0.44254442692657797, 'Hydroxylation-K AUC ROC': 0.7933723196881092, 'Hydroxylation-K AUC PR': 0.5379913176437402, 'Hydroxylation-K MCC': 0.40831306749888263, 'Hydroxylation-K F1': 0.5333563444701552, 'Validation Loss (Hydroxylation-K)': 0.4545199930667877, 'Hydroxylation-P Validation Accuracy': 0.7740283234353587, 'Hydroxylation-P Validation Sensitivity': 0.800952380952381, 'Hydroxylation-P Validation Specificity': 0.7680906778392937, 'Hydroxylation-P Validation Precision': 0.4498213721649009, 'Hydroxylation-P AUC ROC': 0.8408976578810485, 'Hydroxylation-P AUC PR': 0.5774892535686175, 'Hydroxylation-P MCC': 0.4747781520534587, 'Hydroxylation-P F1': 0.5684682668318009, 'Validation Loss (Hydroxylation-P)': 0.3785251021385193, 'Validation Loss (total)': 0.8330450932184855, 'TimeToTrain': 10.831277640660604}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007563601856101743,
 'learning_rate_Hydroxylation-K': 0.00423388536205211,
 'learning_rate_Hydroxylation-P': 0.00114880207411464,
 'log_base': 1.1798306165195793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3433737860,
 'sample_weights': [2.318683421051638, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.793368490539576,
 'weight_decay_Hydroxylation-K': 8.13409091663601,
 'weight_decay_Hydroxylation-P': 4.678794405132425}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3320.705
[2,     1] loss: 3279.086
[3,     1] loss: 3261.325
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008270766430804587,
 'learning_rate_Hydroxylation-K': 0.0031451855321843265,
 'learning_rate_Hydroxylation-P': 0.007370394382216849,
 'log_base': 2.1958928194607368,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3531398706,
 'sample_weights': [10.095145642550166, 1.2619418185123348],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4562739361700796,
 'weight_decay_Hydroxylation-K': 2.8345629568714377,
 'weight_decay_Hydroxylation-P': 5.244336822831889}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1355.749
[2,     1] loss: 1355.183
[3,     1] loss: 1354.323
[4,     1] loss: 1350.834
[5,     1] loss: 1355.566
[6,     1] loss: 1354.692
[7,     1] loss: 1345.188
[8,     1] loss: 1343.110
[9,     1] loss: 1319.476
[10,     1] loss: 1295.750
[11,     1] loss: 1268.933
[12,     1] loss: 1215.465
[13,     1] loss: 1172.819
[14,     1] loss: 1154.340
[15,     1] loss: 1233.867
[16,     1] loss: 1148.467
[17,     1] loss: 1113.488
[18,     1] loss: 1129.195
[19,     1] loss: 1124.079
[20,     1] loss: 1127.798
[21,     1] loss: 1127.408
[22,     1] loss: 1092.947
[23,     1] loss: 1060.885
[24,     1] loss: 1099.788
[25,     1] loss: 1045.467
[26,     1] loss: 1025.611
[27,     1] loss: 991.874
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007945312481692243,
 'learning_rate_Hydroxylation-K': 0.003748833382059193,
 'learning_rate_Hydroxylation-P': 0.003722652393555624,
 'log_base': 2.8058485494779926,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2259696276,
 'sample_weights': [2.122383801057258, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6685508327620964,
 'weight_decay_Hydroxylation-K': 9.674596457857103,
 'weight_decay_Hydroxylation-P': 1.837762040527368}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.587
[2,     1] loss: 1249.117
[3,     1] loss: 1247.844
[4,     1] loss: 1252.082
[5,     1] loss: 1248.093
[6,     1] loss: 1250.440
[7,     1] loss: 1248.632
[8,     1] loss: 1248.807
[9,     1] loss: 1248.337
[10,     1] loss: 1245.356
[11,     1] loss: 1243.885
[12,     1] loss: 1238.866
[13,     1] loss: 1233.734
[14,     1] loss: 1223.977
[15,     1] loss: 1207.387
[16,     1] loss: 1192.738
[17,     1] loss: 1160.715
[18,     1] loss: 1145.771
[19,     1] loss: 1109.477
[20,     1] loss: 1086.954
[21,     1] loss: 1068.894
[22,     1] loss: 1013.104
[23,     1] loss: 996.691
[24,     1] loss: 985.053
[25,     1] loss: 1014.028
[26,     1] loss: 986.488
[27,     1] loss: 915.656
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00455829421183281,
 'learning_rate_Hydroxylation-K': 0.003892187862046089,
 'learning_rate_Hydroxylation-P': 0.004504085852040043,
 'log_base': 1.0346946426603314,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3415618668,
 'sample_weights': [1.6181384396833582, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.64417669244825,
 'weight_decay_Hydroxylation-K': 9.975821611898684,
 'weight_decay_Hydroxylation-P': 7.453834193486607}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15876.197
Exploding loss, terminate run (best metric=1.0965752601623535)
Finished Training
Total time taken: 0.19899868965148926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15955.260
Exploding loss, terminate run (best metric=1.0918704271316528)
Finished Training
Total time taken: 0.21800017356872559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15909.061
Exploding loss, terminate run (best metric=1.0900624990463257)
Finished Training
Total time taken: 0.22500276565551758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15867.832
Exploding loss, terminate run (best metric=1.070689082145691)
Finished Training
Total time taken: 0.21699905395507812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15902.357
Exploding loss, terminate run (best metric=1.0741784572601318)
Finished Training
Total time taken: 0.19900226593017578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15909.521
Exploding loss, terminate run (best metric=1.0975029468536377)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15817.510
Exploding loss, terminate run (best metric=1.0935280323028564)
Finished Training
Total time taken: 0.22700023651123047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15895.303
Exploding loss, terminate run (best metric=1.0913687944412231)
Finished Training
Total time taken: 0.20200204849243164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15927.188
Exploding loss, terminate run (best metric=1.0857419967651367)
Finished Training
Total time taken: 0.21799921989440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15860.570
Exploding loss, terminate run (best metric=1.0763154029846191)
Finished Training
Total time taken: 0.21899962425231934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15934.729
Exploding loss, terminate run (best metric=1.096703052520752)
Finished Training
Total time taken: 0.20599985122680664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15794.506
Exploding loss, terminate run (best metric=1.093908429145813)
Finished Training
Total time taken: 0.21000266075134277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15877.494
Exploding loss, terminate run (best metric=1.101067066192627)
Finished Training
Total time taken: 0.22499895095825195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15890.257
Exploding loss, terminate run (best metric=1.0726488828659058)
Finished Training
Total time taken: 0.20100021362304688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15888.365
Exploding loss, terminate run (best metric=1.082895040512085)
Finished Training
Total time taken: 0.23399972915649414
{'Hydroxylation-K Validation Accuracy': 0.4027777777777778, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6588693957115009, 'Hydroxylation-K AUC PR': 0.36744299721841817, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22520525451559936, 'Validation Loss (Hydroxylation-K)': 0.5573786854743957, 'Hydroxylation-P Validation Accuracy': 0.3921752872104631, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.630936224108078, 'Hydroxylation-P AUC PR': 0.3133291728505435, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20030771800612496, 'Validation Loss (Hydroxylation-P)': 0.5302916884422302, 'Validation Loss (total)': 1.0876703580220541, 'TimeToTrain': 0.21460037231445311}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009157017715478808,
 'learning_rate_Hydroxylation-K': 0.00692673632592977,
 'learning_rate_Hydroxylation-P': 0.002344127721597562,
 'log_base': 2.754743337792345,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3297757347,
 'sample_weights': [48.984468950684516, 6.110335984849475],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.391206270197698,
 'weight_decay_Hydroxylation-K': 7.162274462742875,
 'weight_decay_Hydroxylation-P': 1.168594434607099}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.635
[2,     1] loss: 1256.383
[3,     1] loss: 1255.176
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008719260390144456,
 'learning_rate_Hydroxylation-K': 0.0014401962627001324,
 'learning_rate_Hydroxylation-P': 0.006191340216031277,
 'log_base': 1.3244915928034027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 818799995,
 'sample_weights': [1.6474915158589247, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.1683885689180173,
 'weight_decay_Hydroxylation-K': 5.003541253513642,
 'weight_decay_Hydroxylation-P': 3.1504073510731967}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2178.269
[2,     1] loss: 2145.763
[3,     1] loss: 2186.299
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005265363194681769,
 'learning_rate_Hydroxylation-K': 0.004456252828557078,
 'learning_rate_Hydroxylation-P': 0.004965912972505628,
 'log_base': 1.281115072881715,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 132063839,
 'sample_weights': [5.940472459946014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.185971418946863,
 'weight_decay_Hydroxylation-K': 9.95243257601484,
 'weight_decay_Hydroxylation-P': 7.785366219180779}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2349.844
[2,     1] loss: 2324.545
[3,     1] loss: 2326.877
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029960720608632535,
 'learning_rate_Hydroxylation-K': 0.00208851581512647,
 'learning_rate_Hydroxylation-P': 0.003930033615907315,
 'log_base': 1.0151489968954046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 966213015,
 'sample_weights': [6.738939262277132, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.581714237326604,
 'weight_decay_Hydroxylation-K': 4.640288748247842,
 'weight_decay_Hydroxylation-P': 0.3884277653590833}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36232.531
Exploding loss, terminate run (best metric=1.0959980487823486)
Finished Training
Total time taken: 0.23600006103515625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36053.828
Exploding loss, terminate run (best metric=1.0967458486557007)
Finished Training
Total time taken: 0.23600029945373535
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36062.305
Exploding loss, terminate run (best metric=1.092257022857666)
Finished Training
Total time taken: 0.22500276565551758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36112.391
Exploding loss, terminate run (best metric=1.0803215503692627)
Finished Training
Total time taken: 0.19899797439575195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36055.383
Exploding loss, terminate run (best metric=1.0727641582489014)
Finished Training
Total time taken: 0.21400046348571777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36075.617
Exploding loss, terminate run (best metric=1.0951696634292603)
Finished Training
Total time taken: 0.2220015525817871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36058.074
Exploding loss, terminate run (best metric=1.0920400619506836)
Finished Training
Total time taken: 0.20099878311157227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36039.043
Exploding loss, terminate run (best metric=1.0922497510910034)
Finished Training
Total time taken: 0.21300005912780762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36027.070
Exploding loss, terminate run (best metric=1.072850227355957)
Finished Training
Total time taken: 0.21700263023376465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36015.363
Exploding loss, terminate run (best metric=1.0732167959213257)
Finished Training
Total time taken: 0.1999983787536621
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36188.398
Exploding loss, terminate run (best metric=1.098271131515503)
Finished Training
Total time taken: 0.2259998321533203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36099.516
Exploding loss, terminate run (best metric=1.0922514200210571)
Finished Training
Total time taken: 0.20200157165527344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35810.848
Exploding loss, terminate run (best metric=1.0928771495819092)
Finished Training
Total time taken: 0.20399904251098633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36157.812
Exploding loss, terminate run (best metric=1.0816383361816406)
Finished Training
Total time taken: 0.21899986267089844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36055.258
Exploding loss, terminate run (best metric=1.0819261074066162)
Finished Training
Total time taken: 0.2200000286102295
{'Hydroxylation-K Validation Accuracy': 0.47712765957446807, 'Hydroxylation-K Validation Sensitivity': 0.5777777777777777, 'Hydroxylation-K Validation Specificity': 0.44912280701754387, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6412670565302144, 'Hydroxylation-K AUC PR': 0.3118393047179175, 'Hydroxylation-K MCC': 0.02233746762534249, 'Hydroxylation-K F1': 0.2127881773399015, 'Validation Loss (Hydroxylation-K)': 0.5567262490590413, 'Hydroxylation-P Validation Accuracy': 0.47726064666768186, 'Hydroxylation-P Validation Sensitivity': 0.5752380952380952, 'Hydroxylation-P Validation Specificity': 0.4560327198364008, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6129282711721362, 'Hydroxylation-P AUC PR': 0.2973864513094003, 'Hydroxylation-P MCC': 0.02783561220533593, 'Hydroxylation-P F1': 0.19576304358193244, 'Validation Loss (Hydroxylation-P)': 0.5306455810864766, 'Validation Loss (total)': 1.087371818224589, 'TimeToTrain': 0.21560022036234539}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011151842617060535,
 'learning_rate_Hydroxylation-K': 0.0010133995220380971,
 'learning_rate_Hydroxylation-P': 0.009017174346247341,
 'log_base': 2.749405424152838,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3068648423,
 'sample_weights': [111.11655861034912, 13.860709754208003],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.527204201130138,
 'weight_decay_Hydroxylation-K': 8.545510455003443,
 'weight_decay_Hydroxylation-P': 6.060111007652356}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.525
[2,     1] loss: 1258.428
[3,     1] loss: 1256.493
[4,     1] loss: 1254.003
[5,     1] loss: 1251.961
[6,     1] loss: 1253.939
[7,     1] loss: 1245.907
[8,     1] loss: 1242.480
[9,     1] loss: 1238.761
[10,     1] loss: 1231.814
[11,     1] loss: 1217.490
[12,     1] loss: 1200.602
[13,     1] loss: 1186.001
[14,     1] loss: 1156.583
[15,     1] loss: 1160.353
[16,     1] loss: 1127.370
[17,     1] loss: 1109.997
[18,     1] loss: 1123.434
[19,     1] loss: 1104.061
[20,     1] loss: 1071.444
[21,     1] loss: 1067.752
[22,     1] loss: 1052.502
[23,     1] loss: 1034.974
[24,     1] loss: 1041.382
[25,     1] loss: 1043.027
[26,     1] loss: 1019.359
[27,     1] loss: 1073.950
[28,     1] loss: 1060.111
[29,     1] loss: 1032.839
[30,     1] loss: 1057.067
[31,     1] loss: 1002.071
[32,     1] loss: 1037.135
[33,     1] loss: 1006.735
[34,     1] loss: 1030.422
[35,     1] loss: 1012.452
[36,     1] loss: 992.528
[37,     1] loss: 985.285
[38,     1] loss: 1012.617
[39,     1] loss: 982.041
[40,     1] loss: 960.849
[41,     1] loss: 981.638
[42,     1] loss: 987.209
[43,     1] loss: 945.402
[44,     1] loss: 982.298
[45,     1] loss: 896.628
[46,     1] loss: 914.052
[47,     1] loss: 971.333
[48,     1] loss: 921.372
[49,     1] loss: 915.416
[50,     1] loss: 958.524
[51,     1] loss: 915.959
[52,     1] loss: 931.001
[53,     1] loss: 915.605
[54,     1] loss: 961.151
[55,     1] loss: 874.976
[56,     1] loss: 938.750
[57,     1] loss: 891.988
[58,     1] loss: 896.947
[59,     1] loss: 891.120
[60,     1] loss: 861.316
[61,     1] loss: 852.779
[62,     1] loss: 862.171
[63,     1] loss: 835.451
[64,     1] loss: 873.580
[65,     1] loss: 867.537
[66,     1] loss: 816.726
[67,     1] loss: 808.640
[68,     1] loss: 870.550
[69,     1] loss: 798.362
[70,     1] loss: 848.288
[71,     1] loss: 776.260
[72,     1] loss: 810.699
[73,     1] loss: 771.646
[74,     1] loss: 787.379
[75,     1] loss: 773.223
[76,     1] loss: 791.440
[77,     1] loss: 742.314
[78,     1] loss: 747.747
[79,     1] loss: 738.642
[80,     1] loss: 706.960
[81,     1] loss: 751.824
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034377709909170964,
 'learning_rate_Hydroxylation-K': 0.007100388593378068,
 'learning_rate_Hydroxylation-P': 0.0009108132360533434,
 'log_base': 1.217710076433342,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 273677250,
 'sample_weights': [1.6506510156299954, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.168105741746643,
 'weight_decay_Hydroxylation-K': 2.6265365214447245,
 'weight_decay_Hydroxylation-P': 0.33539427600675453}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2754.891
[2,     1] loss: 2755.924
[3,     1] loss: 2745.830
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00688983924051448,
 'learning_rate_Hydroxylation-K': 0.005084707673255554,
 'learning_rate_Hydroxylation-P': 0.006969591581732559,
 'log_base': 2.395553719272426,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1891257831,
 'sample_weights': [8.475530674894241, 1.059482148296303],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4563683215194834,
 'weight_decay_Hydroxylation-K': 4.207003683474927,
 'weight_decay_Hydroxylation-P': 7.777603767202661}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.794
[2,     1] loss: 1310.655
[3,     1] loss: 1310.040
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007046765984603145,
 'learning_rate_Hydroxylation-K': 6.098317313930608e-05,
 'learning_rate_Hydroxylation-P': 0.0007708399492390055,
 'log_base': 1.0388188156565334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3084702888,
 'sample_weights': [1.9109611097104218, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.734006160650595,
 'weight_decay_Hydroxylation-K': 9.953938580176887,
 'weight_decay_Hydroxylation-P': 4.8786229335772155}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14316.900
[2,     1] loss: 14226.988
[3,     1] loss: 14301.887
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002596782693516275,
 'learning_rate_Hydroxylation-K': 0.002449179881401728,
 'learning_rate_Hydroxylation-P': 0.004920918914228496,
 'log_base': 1.1088096832469347,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1250985707,
 'sample_weights': [43.83545331797446, 5.479642754457898],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.216049692180222,
 'weight_decay_Hydroxylation-K': 4.974348282020116,
 'weight_decay_Hydroxylation-P': 0.44378710675815936}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5252.355
[2,     1] loss: 5245.101
[3,     1] loss: 5262.952
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003751627305498908,
 'learning_rate_Hydroxylation-K': 0.004005094807272481,
 'learning_rate_Hydroxylation-P': 0.004273222221595134,
 'log_base': 1.704380021405635,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1786788004,
 'sample_weights': [16.163135863425918, 2.0204698165405666],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.8498254047251885,
 'weight_decay_Hydroxylation-K': 1.7196447021843237,
 'weight_decay_Hydroxylation-P': 0.5900609115001162}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1572.264
[2,     1] loss: 1570.176
[3,     1] loss: 1572.928
[4,     1] loss: 1565.537
[5,     1] loss: 1561.973
[6,     1] loss: 1564.723
[7,     1] loss: 1565.917
[8,     1] loss: 1563.276
[9,     1] loss: 1561.077
[10,     1] loss: 1565.615
[11,     1] loss: 1556.504
[12,     1] loss: 1545.955
[13,     1] loss: 1538.776
[14,     1] loss: 1521.745
[15,     1] loss: 1492.164
[16,     1] loss: 1445.451
[17,     1] loss: 1392.527
[18,     1] loss: 1346.190
[19,     1] loss: 1361.312
[20,     1] loss: 1293.241
[21,     1] loss: 1311.767
[22,     1] loss: 1315.909
[23,     1] loss: 1274.203
[24,     1] loss: 1323.009
[25,     1] loss: 1314.555
[26,     1] loss: 1253.686
[27,     1] loss: 1259.018
[28,     1] loss: 1244.495
[29,     1] loss: 1248.854
[30,     1] loss: 1222.650
[31,     1] loss: 1203.717
[32,     1] loss: 1175.201
[33,     1] loss: 1172.953
[34,     1] loss: 1168.301
[35,     1] loss: 1161.976
[36,     1] loss: 1294.232
[37,     1] loss: 1097.608
[38,     1] loss: 1092.190
[39,     1] loss: 1155.280
[40,     1] loss: 1123.785
[41,     1] loss: 1076.662
[42,     1] loss: 1088.518
[43,     1] loss: 1079.118
[44,     1] loss: 1161.959
[45,     1] loss: 1051.170
[46,     1] loss: 1054.170
[47,     1] loss: 1022.262
[48,     1] loss: 1031.406
[49,     1] loss: 1005.457
[50,     1] loss: 938.161
[51,     1] loss: 979.777
[52,     1] loss: 895.046
[53,     1] loss: 984.404
[54,     1] loss: 947.799
[55,     1] loss: 969.951
[56,     1] loss: 1144.132
[57,     1] loss: 1054.128
[58,     1] loss: 962.444
[59,     1] loss: 1028.710
[60,     1] loss: 1009.817
[61,     1] loss: 969.994
[62,     1] loss: 978.865
[63,     1] loss: 862.602
[64,     1] loss: 986.501
[65,     1] loss: 832.689
[66,     1] loss: 829.997
[67,     1] loss: 862.505
[68,     1] loss: 787.772
[69,     1] loss: 863.863
[70,     1] loss: 1009.291
[71,     1] loss: 1104.129
Early stopping applied (best metric=0.6515955924987793)
Finished Training
Total time taken: 11.853014469146729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1571.957
[2,     1] loss: 1567.371
[3,     1] loss: 1565.967
[4,     1] loss: 1569.276
[5,     1] loss: 1561.748
[6,     1] loss: 1563.738
[7,     1] loss: 1547.805
[8,     1] loss: 1545.119
[9,     1] loss: 1518.791
[10,     1] loss: 1487.506
[11,     1] loss: 1451.680
[12,     1] loss: 1406.148
[13,     1] loss: 1374.425
[14,     1] loss: 1341.273
[15,     1] loss: 1300.763
[16,     1] loss: 1335.260
[17,     1] loss: 1286.422
[18,     1] loss: 1311.141
[19,     1] loss: 1419.691
[20,     1] loss: 1275.255
[21,     1] loss: 1368.663
[22,     1] loss: 1280.947
[23,     1] loss: 1287.483
[24,     1] loss: 1292.923
[25,     1] loss: 1262.786
[26,     1] loss: 1266.272
[27,     1] loss: 1240.514
[28,     1] loss: 1237.833
[29,     1] loss: 1233.752
[30,     1] loss: 1225.177
[31,     1] loss: 1192.504
[32,     1] loss: 1249.007
[33,     1] loss: 1153.954
[34,     1] loss: 1188.004
[35,     1] loss: 1209.367
[36,     1] loss: 1166.457
[37,     1] loss: 1143.034
[38,     1] loss: 1065.866
[39,     1] loss: 1127.763
[40,     1] loss: 1133.454
[41,     1] loss: 1029.039
[42,     1] loss: 1143.736
[43,     1] loss: 1064.934
[44,     1] loss: 1071.221
[45,     1] loss: 1165.409
[46,     1] loss: 1053.022
[47,     1] loss: 1023.299
[48,     1] loss: 1041.522
[49,     1] loss: 1039.910
[50,     1] loss: 925.877
[51,     1] loss: 1051.034
[52,     1] loss: 980.784
[53,     1] loss: 1365.569
[54,     1] loss: 1143.246
[55,     1] loss: 1125.614
[56,     1] loss: 1087.671
[57,     1] loss: 1214.208
[58,     1] loss: 1132.787
[59,     1] loss: 1074.327
[60,     1] loss: 1058.056
[61,     1] loss: 1081.593
[62,     1] loss: 1056.767
[63,     1] loss: 1042.369
[64,     1] loss: 1020.212
[65,     1] loss: 1097.949
[66,     1] loss: 982.679
[67,     1] loss: 1005.893
[68,     1] loss: 959.295
[69,     1] loss: 933.642
[70,     1] loss: 1020.972
[71,     1] loss: 964.592
Early stopping applied (best metric=0.7560433149337769)
Finished Training
Total time taken: 11.855011940002441
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.493
[2,     1] loss: 1574.174
[3,     1] loss: 1562.443
[4,     1] loss: 1567.703
[5,     1] loss: 1576.288
[6,     1] loss: 1564.480
[7,     1] loss: 1565.322
[8,     1] loss: 1567.675
[9,     1] loss: 1562.348
[10,     1] loss: 1561.718
[11,     1] loss: 1556.256
[12,     1] loss: 1556.740
[13,     1] loss: 1551.371
[14,     1] loss: 1528.281
[15,     1] loss: 1502.533
[16,     1] loss: 1476.319
[17,     1] loss: 1435.450
[18,     1] loss: 1402.418
[19,     1] loss: 1347.827
[20,     1] loss: 1337.043
[21,     1] loss: 1284.024
[22,     1] loss: 1334.209
[23,     1] loss: 1294.223
[24,     1] loss: 1245.966
[25,     1] loss: 1246.485
[26,     1] loss: 1267.924
[27,     1] loss: 1249.866
[28,     1] loss: 1177.629
[29,     1] loss: 1175.281
[30,     1] loss: 1196.127
[31,     1] loss: 1142.209
[32,     1] loss: 1101.565
[33,     1] loss: 1204.814
[34,     1] loss: 1170.508
[35,     1] loss: 1101.967
[36,     1] loss: 1084.860
[37,     1] loss: 1089.342
[38,     1] loss: 1159.496
[39,     1] loss: 1101.881
[40,     1] loss: 1001.787
[41,     1] loss: 1045.742
[42,     1] loss: 992.860
[43,     1] loss: 1076.565
[44,     1] loss: 1110.109
[45,     1] loss: 990.594
[46,     1] loss: 976.059
[47,     1] loss: 932.806
[48,     1] loss: 897.544
[49,     1] loss: 853.545
[50,     1] loss: 878.271
[51,     1] loss: 936.778
Early stopping applied (best metric=0.916032075881958)
Finished Training
Total time taken: 7.2220072746276855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1567.422
[2,     1] loss: 1565.137
[3,     1] loss: 1567.843
[4,     1] loss: 1575.095
[5,     1] loss: 1568.592
[6,     1] loss: 1566.506
[7,     1] loss: 1569.711
[8,     1] loss: 1561.570
[9,     1] loss: 1564.224
[10,     1] loss: 1563.832
[11,     1] loss: 1556.449
[12,     1] loss: 1549.881
[13,     1] loss: 1534.915
[14,     1] loss: 1517.200
[15,     1] loss: 1476.657
[16,     1] loss: 1455.296
[17,     1] loss: 1401.172
[18,     1] loss: 1344.568
[19,     1] loss: 1381.748
[20,     1] loss: 1321.245
[21,     1] loss: 1336.100
[22,     1] loss: 1405.843
[23,     1] loss: 1320.538
[24,     1] loss: 1329.718
[25,     1] loss: 1286.629
[26,     1] loss: 1306.203
[27,     1] loss: 1297.739
[28,     1] loss: 1273.566
[29,     1] loss: 1291.969
[30,     1] loss: 1269.450
[31,     1] loss: 1211.236
[32,     1] loss: 1240.169
[33,     1] loss: 1203.657
[34,     1] loss: 1148.343
[35,     1] loss: 1144.661
[36,     1] loss: 1152.467
[37,     1] loss: 1170.132
[38,     1] loss: 1165.237
[39,     1] loss: 1093.315
[40,     1] loss: 1090.160
[41,     1] loss: 1080.222
[42,     1] loss: 1045.119
[43,     1] loss: 1065.939
[44,     1] loss: 1020.148
[45,     1] loss: 991.375
[46,     1] loss: 1014.091
[47,     1] loss: 1371.494
[48,     1] loss: 1378.168
[49,     1] loss: 1053.128
[50,     1] loss: 1139.549
[51,     1] loss: 1166.100
[52,     1] loss: 1070.266
[53,     1] loss: 1065.955
[54,     1] loss: 1137.426
[55,     1] loss: 1088.364
[56,     1] loss: 1020.861
[57,     1] loss: 1052.993
Early stopping applied (best metric=0.8133425712585449)
Finished Training
Total time taken: 7.920007944107056
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1577.812
[2,     1] loss: 1571.513
[3,     1] loss: 1569.237
[4,     1] loss: 1566.400
[5,     1] loss: 1569.033
[6,     1] loss: 1570.584
[7,     1] loss: 1570.658
[8,     1] loss: 1568.891
[9,     1] loss: 1558.502
[10,     1] loss: 1550.824
[11,     1] loss: 1543.201
[12,     1] loss: 1496.476
[13,     1] loss: 1465.620
[14,     1] loss: 1422.111
[15,     1] loss: 1381.744
[16,     1] loss: 1403.010
[17,     1] loss: 1361.406
[18,     1] loss: 1330.198
[19,     1] loss: 1305.310
[20,     1] loss: 1314.030
[21,     1] loss: 1261.068
[22,     1] loss: 1335.527
[23,     1] loss: 1268.353
[24,     1] loss: 1277.111
[25,     1] loss: 1273.571
[26,     1] loss: 1244.808
[27,     1] loss: 1233.322
[28,     1] loss: 1230.008
[29,     1] loss: 1309.377
[30,     1] loss: 1187.127
[31,     1] loss: 1162.350
[32,     1] loss: 1174.889
[33,     1] loss: 1095.939
[34,     1] loss: 1141.188
[35,     1] loss: 1052.891
[36,     1] loss: 1181.574
[37,     1] loss: 1132.075
[38,     1] loss: 1139.423
[39,     1] loss: 1190.875
[40,     1] loss: 1116.594
[41,     1] loss: 1121.677
[42,     1] loss: 1118.911
[43,     1] loss: 1091.027
[44,     1] loss: 1035.692
[45,     1] loss: 1052.024
[46,     1] loss: 966.003
[47,     1] loss: 1036.150
[48,     1] loss: 967.156
[49,     1] loss: 952.635
[50,     1] loss: 1317.904
[51,     1] loss: 1371.401
[52,     1] loss: 975.628
[53,     1] loss: 1124.618
[54,     1] loss: 1118.839
[55,     1] loss: 1110.496
[56,     1] loss: 1116.101
[57,     1] loss: 1134.193
[58,     1] loss: 1077.543
[59,     1] loss: 1040.348
Early stopping applied (best metric=0.7622509002685547)
Finished Training
Total time taken: 9.879008769989014
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.884
[2,     1] loss: 1570.902
[3,     1] loss: 1566.070
[4,     1] loss: 1566.804
[5,     1] loss: 1571.337
[6,     1] loss: 1565.671
[7,     1] loss: 1566.550
[8,     1] loss: 1563.391
[9,     1] loss: 1560.138
[10,     1] loss: 1553.230
[11,     1] loss: 1534.532
[12,     1] loss: 1507.830
[13,     1] loss: 1466.193
[14,     1] loss: 1449.466
[15,     1] loss: 1403.071
[16,     1] loss: 1395.437
[17,     1] loss: 1355.605
[18,     1] loss: 1363.177
[19,     1] loss: 1356.041
[20,     1] loss: 1356.439
[21,     1] loss: 1349.631
[22,     1] loss: 1269.271
[23,     1] loss: 1280.241
[24,     1] loss: 1273.325
[25,     1] loss: 1234.376
[26,     1] loss: 1195.532
[27,     1] loss: 1199.556
[28,     1] loss: 1135.438
[29,     1] loss: 1194.456
[30,     1] loss: 1127.091
[31,     1] loss: 1155.131
[32,     1] loss: 1077.619
[33,     1] loss: 1153.199
[34,     1] loss: 1238.656
[35,     1] loss: 1090.634
[36,     1] loss: 1198.917
[37,     1] loss: 1127.203
[38,     1] loss: 1187.557
[39,     1] loss: 1086.714
[40,     1] loss: 1157.947
[41,     1] loss: 1110.452
[42,     1] loss: 1043.106
[43,     1] loss: 1046.559
[44,     1] loss: 1000.877
[45,     1] loss: 1083.375
[46,     1] loss: 944.163
[47,     1] loss: 1051.610
[48,     1] loss: 923.599
[49,     1] loss: 960.028
[50,     1] loss: 1015.967
Early stopping applied (best metric=0.798480749130249)
Finished Training
Total time taken: 7.096008539199829
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1574.053
[2,     1] loss: 1567.855
[3,     1] loss: 1568.907
[4,     1] loss: 1573.618
[5,     1] loss: 1571.162
[6,     1] loss: 1570.741
[7,     1] loss: 1564.058
[8,     1] loss: 1562.279
[9,     1] loss: 1560.522
[10,     1] loss: 1555.276
[11,     1] loss: 1549.735
[12,     1] loss: 1529.029
[13,     1] loss: 1516.495
[14,     1] loss: 1487.357
[15,     1] loss: 1432.544
[16,     1] loss: 1414.297
[17,     1] loss: 1365.451
[18,     1] loss: 1362.469
[19,     1] loss: 1315.243
[20,     1] loss: 1389.662
[21,     1] loss: 1299.199
[22,     1] loss: 1433.309
[23,     1] loss: 1310.216
[24,     1] loss: 1402.083
[25,     1] loss: 1329.512
[26,     1] loss: 1247.762
[27,     1] loss: 1350.939
[28,     1] loss: 1299.755
[29,     1] loss: 1272.874
[30,     1] loss: 1251.664
[31,     1] loss: 1280.678
[32,     1] loss: 1218.261
[33,     1] loss: 1213.046
[34,     1] loss: 1247.373
[35,     1] loss: 1202.378
[36,     1] loss: 1176.070
[37,     1] loss: 1167.722
[38,     1] loss: 1140.990
[39,     1] loss: 1157.044
[40,     1] loss: 1140.827
[41,     1] loss: 1150.322
[42,     1] loss: 1087.094
[43,     1] loss: 1137.583
[44,     1] loss: 1152.779
[45,     1] loss: 1101.338
[46,     1] loss: 1120.151
[47,     1] loss: 1052.866
[48,     1] loss: 1170.984
[49,     1] loss: 1105.393
[50,     1] loss: 1175.376
[51,     1] loss: 1078.692
[52,     1] loss: 1042.002
[53,     1] loss: 1077.763
[54,     1] loss: 1037.987
[55,     1] loss: 1000.577
[56,     1] loss: 1101.189
[57,     1] loss: 1034.530
[58,     1] loss: 958.409
[59,     1] loss: 941.326
[60,     1] loss: 987.906
[61,     1] loss: 1198.512
[62,     1] loss: 1563.715
[63,     1] loss: 978.651
[64,     1] loss: 1280.601
[65,     1] loss: 1162.311
[66,     1] loss: 1111.710
[67,     1] loss: 1125.708
[68,     1] loss: 1222.424
[69,     1] loss: 1195.640
[70,     1] loss: 1156.177
[71,     1] loss: 1085.772
[72,     1] loss: 1096.227
[73,     1] loss: 1085.639
[74,     1] loss: 1056.377
[75,     1] loss: 1040.726
[76,     1] loss: 1112.941
[77,     1] loss: 1025.225
[78,     1] loss: 965.298
[79,     1] loss: 1009.495
Early stopping applied (best metric=0.6876351833343506)
Finished Training
Total time taken: 13.23001217842102
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1568.752
[2,     1] loss: 1570.751
[3,     1] loss: 1568.608
[4,     1] loss: 1569.584
[5,     1] loss: 1570.264
[6,     1] loss: 1566.547
[7,     1] loss: 1564.676
[8,     1] loss: 1561.776
[9,     1] loss: 1564.239
[10,     1] loss: 1561.898
[11,     1] loss: 1556.666
[12,     1] loss: 1551.495
[13,     1] loss: 1545.849
[14,     1] loss: 1529.866
[15,     1] loss: 1506.615
[16,     1] loss: 1458.120
[17,     1] loss: 1456.203
[18,     1] loss: 1396.522
[19,     1] loss: 1373.131
[20,     1] loss: 1343.294
[21,     1] loss: 1323.781
[22,     1] loss: 1346.454
[23,     1] loss: 1257.374
[24,     1] loss: 1311.715
[25,     1] loss: 1271.835
[26,     1] loss: 1285.662
[27,     1] loss: 1203.043
[28,     1] loss: 1202.699
[29,     1] loss: 1254.835
[30,     1] loss: 1188.665
[31,     1] loss: 1227.797
[32,     1] loss: 1188.983
[33,     1] loss: 1157.215
[34,     1] loss: 1159.024
[35,     1] loss: 1107.810
[36,     1] loss: 1039.172
[37,     1] loss: 1082.302
[38,     1] loss: 1075.599
[39,     1] loss: 1056.524
[40,     1] loss: 1003.552
[41,     1] loss: 1023.214
[42,     1] loss: 1136.709
[43,     1] loss: 1022.914
[44,     1] loss: 972.461
[45,     1] loss: 943.455
Early stopping applied (best metric=0.7797741889953613)
Finished Training
Total time taken: 7.5330071449279785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1567.340
[2,     1] loss: 1566.095
[3,     1] loss: 1567.220
[4,     1] loss: 1568.406
[5,     1] loss: 1568.160
[6,     1] loss: 1566.946
[7,     1] loss: 1566.795
[8,     1] loss: 1566.166
[9,     1] loss: 1564.940
[10,     1] loss: 1564.227
[11,     1] loss: 1551.847
[12,     1] loss: 1548.928
[13,     1] loss: 1531.172
[14,     1] loss: 1516.022
[15,     1] loss: 1470.111
[16,     1] loss: 1443.826
[17,     1] loss: 1400.287
[18,     1] loss: 1384.230
[19,     1] loss: 1334.401
[20,     1] loss: 1325.760
[21,     1] loss: 1280.465
[22,     1] loss: 1282.860
[23,     1] loss: 1287.161
[24,     1] loss: 1328.737
[25,     1] loss: 1239.584
[26,     1] loss: 1317.583
[27,     1] loss: 1318.123
[28,     1] loss: 1306.289
[29,     1] loss: 1284.647
[30,     1] loss: 1200.901
[31,     1] loss: 1276.482
[32,     1] loss: 1223.330
[33,     1] loss: 1260.396
[34,     1] loss: 1179.387
[35,     1] loss: 1140.742
[36,     1] loss: 1155.626
[37,     1] loss: 1195.326
[38,     1] loss: 1120.457
[39,     1] loss: 1094.523
[40,     1] loss: 1103.208
[41,     1] loss: 1220.004
[42,     1] loss: 1192.632
[43,     1] loss: 1085.864
[44,     1] loss: 1053.443
[45,     1] loss: 1069.815
[46,     1] loss: 1100.306
[47,     1] loss: 1097.331
[48,     1] loss: 1135.980
[49,     1] loss: 1120.536
[50,     1] loss: 991.465
[51,     1] loss: 992.084
[52,     1] loss: 1069.868
[53,     1] loss: 980.745
[54,     1] loss: 1026.854
[55,     1] loss: 986.224
[56,     1] loss: 1002.399
[57,     1] loss: 1036.368
[58,     1] loss: 937.625
[59,     1] loss: 921.088
[60,     1] loss: 937.173
[61,     1] loss: 948.507
[62,     1] loss: 891.239
[63,     1] loss: 833.985
[64,     1] loss: 852.606
[65,     1] loss: 891.385
[66,     1] loss: 804.053
[67,     1] loss: 859.707
[68,     1] loss: 759.572
[69,     1] loss: 834.074
Early stopping applied (best metric=0.7323795557022095)
Finished Training
Total time taken: 9.365009307861328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1575.491
[2,     1] loss: 1568.941
[3,     1] loss: 1577.096
[4,     1] loss: 1565.059
[5,     1] loss: 1568.624
[6,     1] loss: 1568.426
[7,     1] loss: 1570.201
[8,     1] loss: 1572.696
[9,     1] loss: 1557.504
[10,     1] loss: 1560.697
[11,     1] loss: 1555.960
[12,     1] loss: 1552.210
[13,     1] loss: 1540.768
[14,     1] loss: 1525.339
[15,     1] loss: 1496.607
[16,     1] loss: 1481.450
[17,     1] loss: 1464.060
[18,     1] loss: 1421.393
[19,     1] loss: 1420.073
[20,     1] loss: 1372.702
[21,     1] loss: 1409.511
[22,     1] loss: 1291.137
[23,     1] loss: 1348.044
[24,     1] loss: 1364.383
[25,     1] loss: 1323.655
[26,     1] loss: 1298.774
[27,     1] loss: 1272.873
[28,     1] loss: 1267.624
[29,     1] loss: 1242.067
[30,     1] loss: 1258.614
[31,     1] loss: 1256.677
[32,     1] loss: 1251.449
[33,     1] loss: 1191.135
[34,     1] loss: 1180.767
[35,     1] loss: 1180.853
[36,     1] loss: 1123.739
[37,     1] loss: 1165.409
[38,     1] loss: 1147.450
[39,     1] loss: 1112.469
[40,     1] loss: 1172.028
[41,     1] loss: 1153.888
[42,     1] loss: 1053.436
[43,     1] loss: 1078.418
[44,     1] loss: 1081.166
[45,     1] loss: 1119.439
[46,     1] loss: 1121.393
[47,     1] loss: 1004.550
[48,     1] loss: 1077.428
[49,     1] loss: 938.849
[50,     1] loss: 987.702
[51,     1] loss: 1093.560
[52,     1] loss: 985.595
[53,     1] loss: 879.908
[54,     1] loss: 1016.343
[55,     1] loss: 940.319
[56,     1] loss: 961.176
[57,     1] loss: 944.584
[58,     1] loss: 893.510
[59,     1] loss: 947.348
[60,     1] loss: 916.290
[61,     1] loss: 961.981
[62,     1] loss: 842.340
[63,     1] loss: 895.552
[64,     1] loss: 837.358
[65,     1] loss: 811.966
[66,     1] loss: 837.792
[67,     1] loss: 861.288
[68,     1] loss: 1220.200
[69,     1] loss: 1182.242
[70,     1] loss: 919.042
[71,     1] loss: 965.398
[72,     1] loss: 994.722
[73,     1] loss: 934.676
[74,     1] loss: 1037.245
[75,     1] loss: 922.525
[76,     1] loss: 910.799
[77,     1] loss: 844.910
[78,     1] loss: 1007.678
[79,     1] loss: 1001.699
[80,     1] loss: 838.093
[81,     1] loss: 1029.137
[82,     1] loss: 806.630
[83,     1] loss: 817.362
[84,     1] loss: 860.979
[85,     1] loss: 791.127
Early stopping applied (best metric=0.7091151475906372)
Finished Training
Total time taken: 14.222013711929321
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1569.371
[2,     1] loss: 1567.641
[3,     1] loss: 1573.822
[4,     1] loss: 1564.437
[5,     1] loss: 1566.102
[6,     1] loss: 1561.701
[7,     1] loss: 1562.830
[8,     1] loss: 1553.479
[9,     1] loss: 1542.818
[10,     1] loss: 1525.832
[11,     1] loss: 1498.272
[12,     1] loss: 1452.976
[13,     1] loss: 1422.630
[14,     1] loss: 1390.340
[15,     1] loss: 1337.732
[16,     1] loss: 1321.628
[17,     1] loss: 1289.556
[18,     1] loss: 1296.821
[19,     1] loss: 1313.092
[20,     1] loss: 1317.012
[21,     1] loss: 1311.478
[22,     1] loss: 1279.771
[23,     1] loss: 1211.611
[24,     1] loss: 1246.957
[25,     1] loss: 1243.584
[26,     1] loss: 1191.618
[27,     1] loss: 1202.197
[28,     1] loss: 1205.771
[29,     1] loss: 1225.450
[30,     1] loss: 1146.962
[31,     1] loss: 1192.631
[32,     1] loss: 1153.018
[33,     1] loss: 1162.468
[34,     1] loss: 1104.777
[35,     1] loss: 1189.700
[36,     1] loss: 1096.661
[37,     1] loss: 1139.677
[38,     1] loss: 1097.695
[39,     1] loss: 1109.302
[40,     1] loss: 1057.375
[41,     1] loss: 1023.069
[42,     1] loss: 1043.913
[43,     1] loss: 993.182
[44,     1] loss: 1069.992
[45,     1] loss: 1041.479
[46,     1] loss: 1009.411
[47,     1] loss: 957.704
[48,     1] loss: 975.000
[49,     1] loss: 1069.402
[50,     1] loss: 1285.860
[51,     1] loss: 1046.519
[52,     1] loss: 1012.023
[53,     1] loss: 1027.690
[54,     1] loss: 1017.072
[55,     1] loss: 1003.303
[56,     1] loss: 951.080
[57,     1] loss: 940.495
[58,     1] loss: 1018.132
[59,     1] loss: 898.888
[60,     1] loss: 946.328
[61,     1] loss: 958.917
[62,     1] loss: 915.537
[63,     1] loss: 915.486
[64,     1] loss: 790.371
[65,     1] loss: 878.577
[66,     1] loss: 848.108
[67,     1] loss: 873.730
[68,     1] loss: 837.503
[69,     1] loss: 753.547
[70,     1] loss: 856.223
[71,     1] loss: 762.699
Early stopping applied (best metric=0.7481045126914978)
Finished Training
Total time taken: 9.677008628845215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1568.371
[2,     1] loss: 1567.853
[3,     1] loss: 1567.469
[4,     1] loss: 1573.096
[5,     1] loss: 1568.193
[6,     1] loss: 1570.061
[7,     1] loss: 1565.133
[8,     1] loss: 1568.227
[9,     1] loss: 1565.675
[10,     1] loss: 1565.739
[11,     1] loss: 1563.403
[12,     1] loss: 1561.625
[13,     1] loss: 1553.696
[14,     1] loss: 1552.182
[15,     1] loss: 1542.126
[16,     1] loss: 1525.104
[17,     1] loss: 1498.211
[18,     1] loss: 1480.842
[19,     1] loss: 1414.103
[20,     1] loss: 1409.594
[21,     1] loss: 1386.257
[22,     1] loss: 1368.646
[23,     1] loss: 1358.040
[24,     1] loss: 1335.518
[25,     1] loss: 1302.428
[26,     1] loss: 1288.574
[27,     1] loss: 1256.835
[28,     1] loss: 1277.545
[29,     1] loss: 1256.702
[30,     1] loss: 1298.175
[31,     1] loss: 1212.004
[32,     1] loss: 1242.615
[33,     1] loss: 1285.299
[34,     1] loss: 1159.415
[35,     1] loss: 1192.979
[36,     1] loss: 1155.076
[37,     1] loss: 1196.984
[38,     1] loss: 1169.379
[39,     1] loss: 1204.227
[40,     1] loss: 1134.969
[41,     1] loss: 1224.051
[42,     1] loss: 1060.448
[43,     1] loss: 1194.365
[44,     1] loss: 1071.764
[45,     1] loss: 1040.657
[46,     1] loss: 1053.108
[47,     1] loss: 1090.321
[48,     1] loss: 1089.341
[49,     1] loss: 1000.575
[50,     1] loss: 964.615
[51,     1] loss: 912.162
[52,     1] loss: 956.906
[53,     1] loss: 879.458
[54,     1] loss: 890.882
[55,     1] loss: 943.541
[56,     1] loss: 917.095
[57,     1] loss: 1017.476
[58,     1] loss: 952.086
[59,     1] loss: 830.037
[60,     1] loss: 919.099
[61,     1] loss: 839.224
[62,     1] loss: 816.659
[63,     1] loss: 873.005
[64,     1] loss: 865.240
[65,     1] loss: 735.004
[66,     1] loss: 820.406
[67,     1] loss: 855.670
[68,     1] loss: 718.128
[69,     1] loss: 792.067
Early stopping applied (best metric=0.7819116115570068)
Finished Training
Total time taken: 9.432008266448975
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1570.912
[2,     1] loss: 1569.977
[3,     1] loss: 1570.694
[4,     1] loss: 1569.698
[5,     1] loss: 1568.560
[6,     1] loss: 1562.874
[7,     1] loss: 1560.488
[8,     1] loss: 1554.113
[9,     1] loss: 1546.535
[10,     1] loss: 1524.271
[11,     1] loss: 1498.536
[12,     1] loss: 1459.359
[13,     1] loss: 1430.315
[14,     1] loss: 1435.999
[15,     1] loss: 1375.039
[16,     1] loss: 1355.818
[17,     1] loss: 1313.218
[18,     1] loss: 1336.678
[19,     1] loss: 1294.137
[20,     1] loss: 1268.363
[21,     1] loss: 1283.386
[22,     1] loss: 1247.439
[23,     1] loss: 1288.596
[24,     1] loss: 1239.912
[25,     1] loss: 1164.211
[26,     1] loss: 1202.766
[27,     1] loss: 1223.008
[28,     1] loss: 1129.633
[29,     1] loss: 1094.424
[30,     1] loss: 1139.184
[31,     1] loss: 1083.516
[32,     1] loss: 1071.932
[33,     1] loss: 1075.943
[34,     1] loss: 1147.651
[35,     1] loss: 1133.590
[36,     1] loss: 1030.113
[37,     1] loss: 1108.457
[38,     1] loss: 956.644
[39,     1] loss: 1099.768
[40,     1] loss: 1038.857
[41,     1] loss: 1020.143
[42,     1] loss: 1044.927
[43,     1] loss: 982.300
[44,     1] loss: 993.925
Early stopping applied (best metric=0.9643017053604126)
Finished Training
Total time taken: 7.431006669998169
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1566.422
[2,     1] loss: 1579.549
[3,     1] loss: 1567.035
[4,     1] loss: 1569.340
[5,     1] loss: 1570.144
[6,     1] loss: 1567.365
[7,     1] loss: 1570.852
[8,     1] loss: 1560.962
[9,     1] loss: 1563.726
[10,     1] loss: 1566.126
[11,     1] loss: 1557.078
[12,     1] loss: 1555.413
[13,     1] loss: 1528.907
[14,     1] loss: 1508.058
[15,     1] loss: 1471.258
[16,     1] loss: 1458.191
[17,     1] loss: 1405.782
[18,     1] loss: 1394.393
[19,     1] loss: 1368.298
[20,     1] loss: 1334.927
[21,     1] loss: 1356.997
[22,     1] loss: 1349.035
[23,     1] loss: 1407.444
[24,     1] loss: 1299.099
[25,     1] loss: 1373.504
[26,     1] loss: 1302.946
[27,     1] loss: 1290.724
[28,     1] loss: 1287.583
[29,     1] loss: 1228.046
[30,     1] loss: 1295.527
[31,     1] loss: 1247.965
[32,     1] loss: 1281.334
[33,     1] loss: 1251.513
[34,     1] loss: 1189.549
[35,     1] loss: 1202.776
[36,     1] loss: 1168.950
[37,     1] loss: 1150.990
[38,     1] loss: 1164.501
[39,     1] loss: 1265.001
[40,     1] loss: 1166.024
[41,     1] loss: 1192.309
[42,     1] loss: 1139.423
[43,     1] loss: 1144.410
[44,     1] loss: 1133.192
[45,     1] loss: 1119.499
[46,     1] loss: 1050.635
[47,     1] loss: 1117.357
[48,     1] loss: 1012.848
[49,     1] loss: 1048.092
[50,     1] loss: 1053.268
[51,     1] loss: 993.798
[52,     1] loss: 1149.969
[53,     1] loss: 1126.985
[54,     1] loss: 1013.654
[55,     1] loss: 1007.871
[56,     1] loss: 933.156
[57,     1] loss: 952.123
[58,     1] loss: 916.861
[59,     1] loss: 999.187
Early stopping applied (best metric=0.7943755388259888)
Finished Training
Total time taken: 8.055006742477417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1577.631
[2,     1] loss: 1570.667
[3,     1] loss: 1569.423
[4,     1] loss: 1569.546
[5,     1] loss: 1572.626
[6,     1] loss: 1569.844
[7,     1] loss: 1565.377
[8,     1] loss: 1566.954
[9,     1] loss: 1560.406
[10,     1] loss: 1561.557
[11,     1] loss: 1545.068
[12,     1] loss: 1528.235
[13,     1] loss: 1497.532
[14,     1] loss: 1484.936
[15,     1] loss: 1460.089
[16,     1] loss: 1423.160
[17,     1] loss: 1396.736
[18,     1] loss: 1410.068
[19,     1] loss: 1358.409
[20,     1] loss: 1366.091
[21,     1] loss: 1320.095
[22,     1] loss: 1275.282
[23,     1] loss: 1359.632
[24,     1] loss: 1286.999
[25,     1] loss: 1316.858
[26,     1] loss: 1304.040
[27,     1] loss: 1295.039
[28,     1] loss: 1328.149
[29,     1] loss: 1252.443
[30,     1] loss: 1220.003
[31,     1] loss: 1293.734
[32,     1] loss: 1246.155
[33,     1] loss: 1217.123
[34,     1] loss: 1240.548
[35,     1] loss: 1196.664
[36,     1] loss: 1199.835
[37,     1] loss: 1195.450
[38,     1] loss: 1163.494
[39,     1] loss: 1122.395
[40,     1] loss: 1145.169
[41,     1] loss: 1181.507
[42,     1] loss: 1124.868
[43,     1] loss: 1155.985
[44,     1] loss: 1166.542
[45,     1] loss: 1082.846
[46,     1] loss: 1131.939
[47,     1] loss: 1136.610
[48,     1] loss: 1064.531
[49,     1] loss: 1057.247
[50,     1] loss: 1063.707
[51,     1] loss: 1031.511
[52,     1] loss: 1045.643
[53,     1] loss: 1166.139
[54,     1] loss: 1073.630
[55,     1] loss: 1000.378
[56,     1] loss: 1009.240
[57,     1] loss: 989.874
[58,     1] loss: 1031.476
[59,     1] loss: 933.422
[60,     1] loss: 925.169
[61,     1] loss: 966.288
[62,     1] loss: 958.511
[63,     1] loss: 984.052
[64,     1] loss: 953.545
[65,     1] loss: 1112.280
[66,     1] loss: 1232.332
[67,     1] loss: 950.984
[68,     1] loss: 1137.784
[69,     1] loss: 975.387
[70,     1] loss: 1039.683
[71,     1] loss: 1144.916
[72,     1] loss: 1055.449
[73,     1] loss: 1036.349
[74,     1] loss: 997.250
[75,     1] loss: 988.155
[76,     1] loss: 964.317
[77,     1] loss: 989.288
[78,     1] loss: 841.641
[79,     1] loss: 903.141
[80,     1] loss: 842.054
Early stopping applied (best metric=0.6853863000869751)
Finished Training
Total time taken: 13.4290132522583
{'Hydroxylation-K Validation Accuracy': 0.7761524822695035, 'Hydroxylation-K Validation Sensitivity': 0.7644444444444445, 'Hydroxylation-K Validation Specificity': 0.7789473684210526, 'Hydroxylation-K Validation Precision': 0.4770515206041522, 'Hydroxylation-K AUC ROC': 0.8284795321637427, 'Hydroxylation-K AUC PR': 0.6283223086524958, 'Hydroxylation-K MCC': 0.46918792975833273, 'Hydroxylation-K F1': 0.5837335844232396, 'Validation Loss (Hydroxylation-K)': 0.4049023310343424, 'Hydroxylation-P Validation Accuracy': 0.7992978021420233, 'Hydroxylation-P Validation Sensitivity': 0.7802645502645503, 'Hydroxylation-P Validation Specificity': 0.8034216170382563, 'Hydroxylation-P Validation Precision': 0.46826481627428784, 'Hydroxylation-P AUC ROC': 0.8521299522519035, 'Hydroxylation-P AUC PR': 0.5914497980469084, 'Hydroxylation-P MCC': 0.4906193612831377, 'Hydroxylation-P F1': 0.5816651193295548, 'Validation Loss (Hydroxylation-P)': 0.3671462575594584, 'Validation Loss (total)': 0.7720485965410868, 'TimeToTrain': 9.879942989349365}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002333112619135303,
 'learning_rate_Hydroxylation-K': 0.0018449358473732035,
 'learning_rate_Hydroxylation-P': 0.003188497407812254,
 'log_base': 1.0990552130793307,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 447338288,
 'sample_weights': [3.133302861741007, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.429653244703223,
 'weight_decay_Hydroxylation-K': 8.647662821787907,
 'weight_decay_Hydroxylation-P': 8.332300588864177}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5740.481
[2,     1] loss: 5749.909
[3,     1] loss: 5735.447
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004114976632431599,
 'learning_rate_Hydroxylation-K': 0.0003556662660685386,
 'learning_rate_Hydroxylation-P': 0.008447345566535864,
 'log_base': 1.1233029331918476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1421710560,
 'sample_weights': [17.675246165973384, 2.209490885929116],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.531290872705145,
 'weight_decay_Hydroxylation-K': 8.318119778776818,
 'weight_decay_Hydroxylation-P': 9.911337600004126}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4666.918
[2,     1] loss: 4669.010
[3,     1] loss: 4656.965
[4,     1] loss: 4641.154
[5,     1] loss: 4646.617
[6,     1] loss: 4637.634
[7,     1] loss: 4654.881
[8,     1] loss: 4636.591
[9,     1] loss: 4652.035
[10,     1] loss: 4650.680
[11,     1] loss: 4657.427
[12,     1] loss: 4625.421
[13,     1] loss: 4617.950
[14,     1] loss: 4602.137
[15,     1] loss: 4566.073
[16,     1] loss: 4526.624
[17,     1] loss: 4459.003
[18,     1] loss: 4346.225
[19,     1] loss: 4356.218
[20,     1] loss: 4189.448
[21,     1] loss: 4078.439
[22,     1] loss: 3937.717
[23,     1] loss: 3925.797
[24,     1] loss: 3780.790
[25,     1] loss: 3829.110
[26,     1] loss: 3627.102
[27,     1] loss: 3691.411
[28,     1] loss: 4119.816
[29,     1] loss: 3590.160
[30,     1] loss: 4243.076
[31,     1] loss: 3356.337
[32,     1] loss: 3767.656
[33,     1] loss: 3760.156
[34,     1] loss: 3638.751
[35,     1] loss: 3668.519
[36,     1] loss: 3568.455
[37,     1] loss: 3380.721
[38,     1] loss: 3672.739
[39,     1] loss: 3386.210
[40,     1] loss: 3271.559
[41,     1] loss: 3353.989
[42,     1] loss: 3084.448
[43,     1] loss: 3424.292
[44,     1] loss: 2958.128
[45,     1] loss: 2661.984
[46,     1] loss: 2784.338
[47,     1] loss: 3152.330
[48,     1] loss: 3023.145
[49,     1] loss: 2489.319
[50,     1] loss: 2671.677
[51,     1] loss: 2455.307
[52,     1] loss: 2745.155
[53,     1] loss: 2596.494
[54,     1] loss: 2715.824
[55,     1] loss: 2526.690
[56,     1] loss: 2330.807
[57,     1] loss: 2426.346
[58,     1] loss: 2284.141
[59,     1] loss: 2582.263
[60,     1] loss: 2375.228
[61,     1] loss: 2221.506
[62,     1] loss: 2199.552
[63,     1] loss: 2448.550
[64,     1] loss: 2280.522
Early stopping applied (best metric=0.8382204174995422)
Finished Training
Total time taken: 8.725008487701416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4657.072
[2,     1] loss: 4649.294
[3,     1] loss: 4638.023
[4,     1] loss: 4650.972
[5,     1] loss: 4600.849
[6,     1] loss: 4675.692
[7,     1] loss: 4572.721
[8,     1] loss: 4550.915
[9,     1] loss: 4506.769
[10,     1] loss: 4392.726
[11,     1] loss: 4250.150
[12,     1] loss: 4067.258
[13,     1] loss: 3957.101
[14,     1] loss: 3651.208
[15,     1] loss: 3865.286
[16,     1] loss: 4193.897
[17,     1] loss: 3998.328
[18,     1] loss: 3865.010
[19,     1] loss: 3825.619
[20,     1] loss: 3733.338
[21,     1] loss: 3638.766
[22,     1] loss: 3628.688
[23,     1] loss: 3639.259
[24,     1] loss: 3683.646
[25,     1] loss: 3563.678
[26,     1] loss: 3303.295
[27,     1] loss: 3516.434
[28,     1] loss: 3450.431
[29,     1] loss: 3282.492
[30,     1] loss: 3499.076
[31,     1] loss: 3230.007
[32,     1] loss: 3256.832
[33,     1] loss: 3411.747
[34,     1] loss: 3254.301
[35,     1] loss: 3143.526
[36,     1] loss: 3078.651
[37,     1] loss: 3023.326
[38,     1] loss: 3011.548
[39,     1] loss: 3502.138
[40,     1] loss: 4079.464
[41,     1] loss: 3743.945
[42,     1] loss: 3238.844
[43,     1] loss: 3212.979
[44,     1] loss: 3743.945
[45,     1] loss: 3552.016
[46,     1] loss: 3265.990
[47,     1] loss: 3384.682
[48,     1] loss: 3391.616
[49,     1] loss: 3409.681
[50,     1] loss: 3422.296
[51,     1] loss: 3151.649
[52,     1] loss: 3202.825
[53,     1] loss: 2917.713
[54,     1] loss: 2868.802
[55,     1] loss: 2911.136
[56,     1] loss: 2855.358
[57,     1] loss: 2750.499
[58,     1] loss: 2584.971
Early stopping applied (best metric=0.7851570844650269)
Finished Training
Total time taken: 9.761010646820068
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4687.257
[2,     1] loss: 4679.236
[3,     1] loss: 4664.421
[4,     1] loss: 4680.758
[5,     1] loss: 4674.711
[6,     1] loss: 4665.921
[7,     1] loss: 4659.875
[8,     1] loss: 4656.065
[9,     1] loss: 4651.415
[10,     1] loss: 4653.835
[11,     1] loss: 4653.911
[12,     1] loss: 4627.966
[13,     1] loss: 4628.606
[14,     1] loss: 4623.538
[15,     1] loss: 4573.309
[16,     1] loss: 4540.161
[17,     1] loss: 4502.703
[18,     1] loss: 4467.082
[19,     1] loss: 4373.877
[20,     1] loss: 4351.962
[21,     1] loss: 4267.863
[22,     1] loss: 4303.471
[23,     1] loss: 4272.489
[24,     1] loss: 4167.310
[25,     1] loss: 4081.427
[26,     1] loss: 4069.673
[27,     1] loss: 4006.845
[28,     1] loss: 4054.383
[29,     1] loss: 3857.970
[30,     1] loss: 3799.687
[31,     1] loss: 4078.824
[32,     1] loss: 4015.680
[33,     1] loss: 3712.403
[34,     1] loss: 3999.554
[35,     1] loss: 3645.192
[36,     1] loss: 3568.820
[37,     1] loss: 3576.604
[38,     1] loss: 3403.298
[39,     1] loss: 3451.368
[40,     1] loss: 3420.379
[41,     1] loss: 3438.884
[42,     1] loss: 3448.368
[43,     1] loss: 3466.428
[44,     1] loss: 3203.959
[45,     1] loss: 3210.119
[46,     1] loss: 3263.469
[47,     1] loss: 3221.636
[48,     1] loss: 3039.999
[49,     1] loss: 3014.763
[50,     1] loss: 2993.250
[51,     1] loss: 2894.193
[52,     1] loss: 3015.867
[53,     1] loss: 2866.723
[54,     1] loss: 2689.440
[55,     1] loss: 2829.506
[56,     1] loss: 2872.244
[57,     1] loss: 3390.230
[58,     1] loss: 2904.490
[59,     1] loss: 2579.309
[60,     1] loss: 2615.052
[61,     1] loss: 2634.120
[62,     1] loss: 2587.280
[63,     1] loss: 2441.589
[64,     1] loss: 2291.301
[65,     1] loss: 2749.391
[66,     1] loss: 2868.717
[67,     1] loss: 2597.545
[68,     1] loss: 2240.780
[69,     1] loss: 2540.234
[70,     1] loss: 2415.119
[71,     1] loss: 2225.201
[72,     1] loss: 2353.653
[73,     1] loss: 2319.222
[74,     1] loss: 2788.964
[75,     1] loss: 2340.225
[76,     1] loss: 2463.042
[77,     1] loss: 2295.550
[78,     1] loss: 2067.492
[79,     1] loss: 2506.383
[80,     1] loss: 2318.753
[81,     1] loss: 2298.825
Early stopping applied (best metric=0.7625022530555725)
Finished Training
Total time taken: 13.475012063980103
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4636.643
[2,     1] loss: 4678.438
[3,     1] loss: 4664.442
[4,     1] loss: 4674.566
[5,     1] loss: 4646.347
[6,     1] loss: 4653.166
[7,     1] loss: 4647.241
[8,     1] loss: 4656.561
[9,     1] loss: 4627.255
[10,     1] loss: 4650.482
[11,     1] loss: 4640.180
[12,     1] loss: 4622.916
[13,     1] loss: 4623.490
[14,     1] loss: 4607.348
[15,     1] loss: 4575.400
[16,     1] loss: 4525.349
[17,     1] loss: 4418.014
[18,     1] loss: 4325.259
[19,     1] loss: 4182.025
[20,     1] loss: 4048.865
[21,     1] loss: 3978.702
[22,     1] loss: 3903.514
[23,     1] loss: 3833.812
[24,     1] loss: 4001.143
[25,     1] loss: 3772.058
[26,     1] loss: 3819.391
[27,     1] loss: 3684.443
[28,     1] loss: 3866.580
[29,     1] loss: 3644.426
[30,     1] loss: 3628.283
[31,     1] loss: 3328.475
[32,     1] loss: 3566.955
[33,     1] loss: 3323.562
[34,     1] loss: 3410.983
[35,     1] loss: 3189.073
[36,     1] loss: 3467.075
[37,     1] loss: 3098.207
[38,     1] loss: 3479.639
[39,     1] loss: 3279.531
[40,     1] loss: 3230.581
[41,     1] loss: 2809.546
[42,     1] loss: 3135.031
[43,     1] loss: 2899.438
[44,     1] loss: 3055.117
[45,     1] loss: 2798.652
[46,     1] loss: 3202.804
[47,     1] loss: 2901.632
[48,     1] loss: 2994.190
[49,     1] loss: 2701.465
[50,     1] loss: 2526.545
[51,     1] loss: 2519.832
[52,     1] loss: 2914.388
[53,     1] loss: 2799.648
[54,     1] loss: 2575.663
[55,     1] loss: 2364.031
[56,     1] loss: 2434.976
[57,     1] loss: 2654.925
[58,     1] loss: 2291.375
[59,     1] loss: 3085.492
[60,     1] loss: 3217.146
[61,     1] loss: 2851.485
[62,     1] loss: 2654.273
[63,     1] loss: 2610.585
[64,     1] loss: 3233.200
[65,     1] loss: 2601.960
[66,     1] loss: 2689.763
[67,     1] loss: 2798.820
[68,     1] loss: 2297.194
[69,     1] loss: 2687.090
[70,     1] loss: 2241.012
[71,     1] loss: 2434.771
[72,     1] loss: 2209.992
[73,     1] loss: 2208.871
[74,     1] loss: 2107.829
[75,     1] loss: 2206.683
[76,     1] loss: 2098.733
[77,     1] loss: 2066.367
[78,     1] loss: 1838.889
[79,     1] loss: 2127.075
[80,     1] loss: 1971.150
[81,     1] loss: 2067.998
[82,     1] loss: 2096.790
[83,     1] loss: 1926.729
[84,     1] loss: 2168.795
[85,     1] loss: 3774.955
Early stopping applied (best metric=0.8342179656028748)
Finished Training
Total time taken: 11.89501404762268
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4678.443
[2,     1] loss: 4642.240
[3,     1] loss: 4634.875
[4,     1] loss: 4651.426
[5,     1] loss: 4665.644
[6,     1] loss: 4656.630
[7,     1] loss: 4652.826
[8,     1] loss: 4693.636
[9,     1] loss: 4637.465
[10,     1] loss: 4607.363
[11,     1] loss: 4575.139
[12,     1] loss: 4535.232
[13,     1] loss: 4539.879
[14,     1] loss: 4419.597
[15,     1] loss: 4351.352
[16,     1] loss: 4137.241
[17,     1] loss: 4297.406
[18,     1] loss: 3999.004
[19,     1] loss: 3949.339
[20,     1] loss: 4021.827
[21,     1] loss: 3967.179
[22,     1] loss: 3920.189
[23,     1] loss: 4018.504
[24,     1] loss: 3917.644
[25,     1] loss: 3879.069
[26,     1] loss: 3708.758
[27,     1] loss: 3537.808
[28,     1] loss: 3740.138
[29,     1] loss: 3625.027
[30,     1] loss: 3625.335
[31,     1] loss: 3526.810
[32,     1] loss: 3671.207
[33,     1] loss: 3372.136
[34,     1] loss: 3403.329
[35,     1] loss: 3290.999
[36,     1] loss: 3029.634
[37,     1] loss: 3243.815
[38,     1] loss: 2849.706
[39,     1] loss: 3383.742
[40,     1] loss: 3230.627
[41,     1] loss: 3161.305
[42,     1] loss: 3183.464
[43,     1] loss: 2704.373
[44,     1] loss: 2832.944
[45,     1] loss: 3069.785
[46,     1] loss: 2787.282
[47,     1] loss: 3220.664
[48,     1] loss: 2946.274
[49,     1] loss: 3026.916
[50,     1] loss: 2939.894
[51,     1] loss: 2886.137
[52,     1] loss: 2714.082
[53,     1] loss: 3053.367
[54,     1] loss: 2855.920
[55,     1] loss: 2645.466
[56,     1] loss: 2791.882
[57,     1] loss: 2624.087
[58,     1] loss: 2513.601
[59,     1] loss: 2495.940
Early stopping applied (best metric=0.8073140382766724)
Finished Training
Total time taken: 8.465007543563843
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4655.263
[2,     1] loss: 4653.171
[3,     1] loss: 4673.407
[4,     1] loss: 4681.081
[5,     1] loss: 4654.719
[6,     1] loss: 4645.781
[7,     1] loss: 4631.690
[8,     1] loss: 4619.936
[9,     1] loss: 4607.408
[10,     1] loss: 4605.855
[11,     1] loss: 4545.021
[12,     1] loss: 4496.623
[13,     1] loss: 4420.702
[14,     1] loss: 4252.805
[15,     1] loss: 4231.225
[16,     1] loss: 4006.697
[17,     1] loss: 3993.042
[18,     1] loss: 3777.781
[19,     1] loss: 3945.504
[20,     1] loss: 3546.296
[21,     1] loss: 3598.638
[22,     1] loss: 3791.527
[23,     1] loss: 3513.324
[24,     1] loss: 3462.146
[25,     1] loss: 3790.108
[26,     1] loss: 3510.036
[27,     1] loss: 3300.885
[28,     1] loss: 3738.830
[29,     1] loss: 3428.435
[30,     1] loss: 3377.513
[31,     1] loss: 3612.669
[32,     1] loss: 3091.593
[33,     1] loss: 3113.841
[34,     1] loss: 3061.071
[35,     1] loss: 3179.899
[36,     1] loss: 2952.136
[37,     1] loss: 2995.444
[38,     1] loss: 3179.626
[39,     1] loss: 3087.882
[40,     1] loss: 3315.013
[41,     1] loss: 3379.417
[42,     1] loss: 2902.259
[43,     1] loss: 2902.251
[44,     1] loss: 2969.138
[45,     1] loss: 3010.993
[46,     1] loss: 2878.062
[47,     1] loss: 2546.404
[48,     1] loss: 2841.362
[49,     1] loss: 2809.693
[50,     1] loss: 3141.776
[51,     1] loss: 2629.069
[52,     1] loss: 2659.880
[53,     1] loss: 2495.121
[54,     1] loss: 2751.100
[55,     1] loss: 2348.886
[56,     1] loss: 2361.260
[57,     1] loss: 2229.678
[58,     1] loss: 2464.290
[59,     1] loss: 2703.820
[60,     1] loss: 2204.035
[61,     1] loss: 2455.109
[62,     1] loss: 2047.975
[63,     1] loss: 2583.270
[64,     1] loss: 2273.517
[65,     1] loss: 2158.804
[66,     1] loss: 3059.779
[67,     1] loss: 2282.493
[68,     1] loss: 2186.110
[69,     1] loss: 2303.050
[70,     1] loss: 2242.966
[71,     1] loss: 2173.647
[72,     1] loss: 1926.791
Early stopping applied (best metric=0.9532680511474609)
Finished Training
Total time taken: 12.07801342010498
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4655.982
[2,     1] loss: 4688.010
[3,     1] loss: 4666.390
[4,     1] loss: 4659.989
[5,     1] loss: 4650.825
[6,     1] loss: 4653.146
[7,     1] loss: 4648.798
[8,     1] loss: 4652.776
[9,     1] loss: 4650.362
[10,     1] loss: 4647.217
[11,     1] loss: 4639.261
[12,     1] loss: 4642.038
[13,     1] loss: 4641.096
[14,     1] loss: 4612.107
[15,     1] loss: 4590.655
[16,     1] loss: 4587.064
[17,     1] loss: 4506.457
[18,     1] loss: 4444.737
[19,     1] loss: 4418.165
[20,     1] loss: 4229.930
[21,     1] loss: 4089.110
[22,     1] loss: 4028.452
[23,     1] loss: 3946.465
[24,     1] loss: 3821.227
[25,     1] loss: 3702.857
[26,     1] loss: 4279.237
[27,     1] loss: 4040.301
[28,     1] loss: 3794.619
[29,     1] loss: 3845.035
[30,     1] loss: 3967.232
[31,     1] loss: 3537.705
[32,     1] loss: 3651.388
[33,     1] loss: 3395.658
[34,     1] loss: 3409.018
[35,     1] loss: 3596.072
[36,     1] loss: 3422.558
[37,     1] loss: 3262.775
[38,     1] loss: 3214.625
[39,     1] loss: 3064.905
[40,     1] loss: 3093.479
[41,     1] loss: 2982.692
[42,     1] loss: 3003.920
[43,     1] loss: 2993.180
[44,     1] loss: 2629.246
[45,     1] loss: 3291.731
[46,     1] loss: 3006.159
[47,     1] loss: 2968.672
[48,     1] loss: 3072.165
[49,     1] loss: 3023.087
[50,     1] loss: 2437.485
Early stopping applied (best metric=0.8984301090240479)
Finished Training
Total time taken: 6.826006650924683
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4662.627
[2,     1] loss: 4654.362
[3,     1] loss: 4656.110
[4,     1] loss: 4651.085
[5,     1] loss: 4652.319
[6,     1] loss: 4662.894
[7,     1] loss: 4643.757
[8,     1] loss: 4636.940
[9,     1] loss: 4617.102
[10,     1] loss: 4631.666
[11,     1] loss: 4604.795
[12,     1] loss: 4536.837
[13,     1] loss: 4483.220
[14,     1] loss: 4426.315
[15,     1] loss: 4316.655
[16,     1] loss: 4200.748
[17,     1] loss: 4064.358
[18,     1] loss: 4180.562
[19,     1] loss: 3880.620
[20,     1] loss: 3941.216
[21,     1] loss: 3830.031
[22,     1] loss: 3741.748
[23,     1] loss: 3751.765
[24,     1] loss: 3528.366
[25,     1] loss: 3474.865
[26,     1] loss: 3748.213
[27,     1] loss: 3558.052
[28,     1] loss: 3540.620
[29,     1] loss: 3312.214
[30,     1] loss: 3100.418
[31,     1] loss: 3098.937
[32,     1] loss: 3838.923
[33,     1] loss: 3508.225
[34,     1] loss: 3415.896
[35,     1] loss: 3109.678
Early stopping applied (best metric=1.0541892051696777)
Finished Training
Total time taken: 5.8640055656433105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4659.646
[2,     1] loss: 4646.365
[3,     1] loss: 4639.637
[4,     1] loss: 4652.785
[5,     1] loss: 4667.426
[6,     1] loss: 4652.988
[7,     1] loss: 4630.703
[8,     1] loss: 4612.566
[9,     1] loss: 4639.157
[10,     1] loss: 4588.316
[11,     1] loss: 4573.301
[12,     1] loss: 4473.560
[13,     1] loss: 4310.830
[14,     1] loss: 4215.242
[15,     1] loss: 4073.902
[16,     1] loss: 4094.636
[17,     1] loss: 3974.628
[18,     1] loss: 3954.823
[19,     1] loss: 3908.216
[20,     1] loss: 4124.018
[21,     1] loss: 3850.819
[22,     1] loss: 3440.085
[23,     1] loss: 3910.026
[24,     1] loss: 3527.571
[25,     1] loss: 3758.104
[26,     1] loss: 3960.083
[27,     1] loss: 3690.438
[28,     1] loss: 3336.825
[29,     1] loss: 3510.248
[30,     1] loss: 3405.155
[31,     1] loss: 3512.809
[32,     1] loss: 3283.534
[33,     1] loss: 3303.286
[34,     1] loss: 3003.288
[35,     1] loss: 3505.762
[36,     1] loss: 3455.741
[37,     1] loss: 3278.896
[38,     1] loss: 3016.441
[39,     1] loss: 3261.228
[40,     1] loss: 2817.312
[41,     1] loss: 2949.271
[42,     1] loss: 3156.911
[43,     1] loss: 3031.900
[44,     1] loss: 3068.855
[45,     1] loss: 2747.171
[46,     1] loss: 2990.735
[47,     1] loss: 2692.698
[48,     1] loss: 2939.102
[49,     1] loss: 2910.919
[50,     1] loss: 2614.912
[51,     1] loss: 2660.872
[52,     1] loss: 3049.478
[53,     1] loss: 2835.155
[54,     1] loss: 2975.426
[55,     1] loss: 2988.510
[56,     1] loss: 2937.871
[57,     1] loss: 2877.808
Early stopping applied (best metric=0.8973971605300903)
Finished Training
Total time taken: 7.779006481170654
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4650.843
[2,     1] loss: 4677.898
[3,     1] loss: 4667.238
[4,     1] loss: 4663.582
[5,     1] loss: 4637.154
[6,     1] loss: 4652.161
[7,     1] loss: 4633.035
[8,     1] loss: 4638.798
[9,     1] loss: 4650.385
[10,     1] loss: 4624.289
[11,     1] loss: 4593.292
[12,     1] loss: 4553.975
[13,     1] loss: 4537.261
[14,     1] loss: 4500.588
[15,     1] loss: 4408.587
[16,     1] loss: 4355.047
[17,     1] loss: 4163.450
[18,     1] loss: 4122.149
[19,     1] loss: 3951.172
[20,     1] loss: 4026.600
[21,     1] loss: 3676.928
[22,     1] loss: 3870.870
[23,     1] loss: 3824.833
[24,     1] loss: 4179.768
[25,     1] loss: 3754.799
[26,     1] loss: 3486.542
[27,     1] loss: 3404.455
[28,     1] loss: 3584.525
[29,     1] loss: 3711.439
[30,     1] loss: 3396.152
[31,     1] loss: 3398.244
[32,     1] loss: 3345.771
[33,     1] loss: 3239.961
[34,     1] loss: 3208.435
[35,     1] loss: 3905.181
[36,     1] loss: 3275.381
[37,     1] loss: 3074.796
[38,     1] loss: 3018.148
[39,     1] loss: 3055.445
[40,     1] loss: 3177.910
Early stopping applied (best metric=0.9013746976852417)
Finished Training
Total time taken: 6.7720091342926025
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4668.115
[2,     1] loss: 4671.408
[3,     1] loss: 4681.189
[4,     1] loss: 4632.134
[5,     1] loss: 4667.352
[6,     1] loss: 4665.149
[7,     1] loss: 4652.506
[8,     1] loss: 4658.538
[9,     1] loss: 4671.137
[10,     1] loss: 4655.765
[11,     1] loss: 4651.104
[12,     1] loss: 4639.435
[13,     1] loss: 4636.823
[14,     1] loss: 4665.877
[15,     1] loss: 4619.038
[16,     1] loss: 4608.950
[17,     1] loss: 4589.296
[18,     1] loss: 4608.388
[19,     1] loss: 4537.399
[20,     1] loss: 4478.427
[21,     1] loss: 4432.580
[22,     1] loss: 4357.213
[23,     1] loss: 4337.177
[24,     1] loss: 4175.377
[25,     1] loss: 4085.967
[26,     1] loss: 3905.741
[27,     1] loss: 3846.993
[28,     1] loss: 3832.920
[29,     1] loss: 3916.208
[30,     1] loss: 3747.478
[31,     1] loss: 3675.980
[32,     1] loss: 3577.540
[33,     1] loss: 3602.608
[34,     1] loss: 4033.260
[35,     1] loss: 4159.010
[36,     1] loss: 3422.699
[37,     1] loss: 3727.304
[38,     1] loss: 3233.827
[39,     1] loss: 3403.097
[40,     1] loss: 3193.410
[41,     1] loss: 3204.680
[42,     1] loss: 3287.548
[43,     1] loss: 3109.289
[44,     1] loss: 3019.641
[45,     1] loss: 3222.735
[46,     1] loss: 3260.719
[47,     1] loss: 3098.941
[48,     1] loss: 3195.272
[49,     1] loss: 2784.115
[50,     1] loss: 3065.056
[51,     1] loss: 3041.810
[52,     1] loss: 2853.430
[53,     1] loss: 2767.460
[54,     1] loss: 2782.643
[55,     1] loss: 2651.001
[56,     1] loss: 2753.640
[57,     1] loss: 2514.715
[58,     1] loss: 2954.006
[59,     1] loss: 2833.938
[60,     1] loss: 2432.566
Early stopping applied (best metric=0.8192821741104126)
Finished Training
Total time taken: 8.179006338119507
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4674.856
[2,     1] loss: 4651.855
[3,     1] loss: 4657.594
[4,     1] loss: 4668.774
[5,     1] loss: 4665.729
[6,     1] loss: 4655.908
[7,     1] loss: 4652.230
[8,     1] loss: 4625.642
[9,     1] loss: 4614.598
[10,     1] loss: 4607.086
[11,     1] loss: 4596.331
[12,     1] loss: 4558.392
[13,     1] loss: 4472.004
[14,     1] loss: 4388.440
[15,     1] loss: 4290.180
[16,     1] loss: 4132.961
[17,     1] loss: 3958.560
[18,     1] loss: 3994.898
[19,     1] loss: 3754.290
[20,     1] loss: 3735.566
[21,     1] loss: 3694.598
[22,     1] loss: 3656.138
[23,     1] loss: 3632.372
[24,     1] loss: 3628.516
[25,     1] loss: 3501.693
[26,     1] loss: 3748.170
[27,     1] loss: 3675.418
[28,     1] loss: 3758.035
[29,     1] loss: 3508.356
[30,     1] loss: 3473.751
[31,     1] loss: 3478.310
[32,     1] loss: 3473.936
[33,     1] loss: 3306.550
[34,     1] loss: 3227.505
[35,     1] loss: 3391.546
[36,     1] loss: 3081.744
[37,     1] loss: 3098.903
[38,     1] loss: 3257.447
[39,     1] loss: 3488.805
[40,     1] loss: 2769.029
[41,     1] loss: 3552.959
[42,     1] loss: 2867.319
[43,     1] loss: 3310.343
[44,     1] loss: 2927.609
[45,     1] loss: 2968.165
[46,     1] loss: 3111.710
[47,     1] loss: 2905.076
[48,     1] loss: 3037.157
[49,     1] loss: 2877.300
[50,     1] loss: 2883.581
Early stopping applied (best metric=0.9917277097702026)
Finished Training
Total time taken: 7.347009181976318
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4651.632
[2,     1] loss: 4671.411
[3,     1] loss: 4662.035
[4,     1] loss: 4647.150
[5,     1] loss: 4637.632
[6,     1] loss: 4678.581
[7,     1] loss: 4666.065
[8,     1] loss: 4665.824
[9,     1] loss: 4634.403
[10,     1] loss: 4640.620
[11,     1] loss: 4638.664
[12,     1] loss: 4640.230
[13,     1] loss: 4622.127
[14,     1] loss: 4629.167
[15,     1] loss: 4630.011
[16,     1] loss: 4601.779
[17,     1] loss: 4583.565
[18,     1] loss: 4551.366
[19,     1] loss: 4463.511
[20,     1] loss: 4357.648
[21,     1] loss: 4285.682
[22,     1] loss: 4187.457
[23,     1] loss: 4083.471
[24,     1] loss: 3958.205
[25,     1] loss: 4092.123
[26,     1] loss: 3867.616
[27,     1] loss: 3999.965
[28,     1] loss: 4002.285
[29,     1] loss: 4165.834
[30,     1] loss: 3931.890
[31,     1] loss: 3812.859
[32,     1] loss: 3692.592
[33,     1] loss: 3706.207
[34,     1] loss: 3579.387
[35,     1] loss: 3663.156
[36,     1] loss: 3434.644
[37,     1] loss: 3248.534
[38,     1] loss: 3167.363
[39,     1] loss: 3398.887
[40,     1] loss: 3351.821
[41,     1] loss: 3098.198
[42,     1] loss: 3269.028
[43,     1] loss: 3048.066
[44,     1] loss: 3128.317
[45,     1] loss: 2914.764
[46,     1] loss: 3505.955
[47,     1] loss: 3375.679
[48,     1] loss: 3260.090
[49,     1] loss: 3290.616
[50,     1] loss: 3541.218
[51,     1] loss: 3062.981
[52,     1] loss: 3052.064
[53,     1] loss: 2965.220
[54,     1] loss: 2840.829
[55,     1] loss: 2912.090
[56,     1] loss: 3057.814
[57,     1] loss: 2982.067
[58,     1] loss: 2559.919
[59,     1] loss: 3106.726
[60,     1] loss: 2590.144
[61,     1] loss: 2649.987
[62,     1] loss: 2529.593
[63,     1] loss: 2543.944
[64,     1] loss: 2771.793
[65,     1] loss: 2402.343
[66,     1] loss: 2194.301
[67,     1] loss: 2576.620
[68,     1] loss: 2665.003
[69,     1] loss: 2662.275
[70,     1] loss: 2327.160
[71,     1] loss: 2376.469
[72,     1] loss: 2349.584
[73,     1] loss: 2642.782
[74,     1] loss: 2361.207
[75,     1] loss: 2622.822
[76,     1] loss: 2507.027
[77,     1] loss: 2415.811
[78,     1] loss: 2290.123
[79,     1] loss: 2632.869
[80,     1] loss: 2544.793
[81,     1] loss: 2515.120
[82,     1] loss: 2422.292
[83,     1] loss: 2443.309
[84,     1] loss: 2024.913
[85,     1] loss: 2300.939
[86,     1] loss: 1970.104
[87,     1] loss: 2212.551
[88,     1] loss: 1983.348
[89,     1] loss: 1929.836
[90,     1] loss: 1887.527
[91,     1] loss: 1938.365
[92,     1] loss: 1985.900
[93,     1] loss: 2470.480
[94,     1] loss: 2131.291
[95,     1] loss: 1922.590
Early stopping applied (best metric=0.7821508646011353)
Finished Training
Total time taken: 15.87201452255249
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4649.591
[2,     1] loss: 4652.165
[3,     1] loss: 4697.818
[4,     1] loss: 4679.606
[5,     1] loss: 4650.961
[6,     1] loss: 4669.131
[7,     1] loss: 4641.493
[8,     1] loss: 4642.324
[9,     1] loss: 4633.650
[10,     1] loss: 4634.078
[11,     1] loss: 4620.130
[12,     1] loss: 4580.354
[13,     1] loss: 4532.935
[14,     1] loss: 4475.839
[15,     1] loss: 4349.913
[16,     1] loss: 4212.363
[17,     1] loss: 4201.143
[18,     1] loss: 4076.025
[19,     1] loss: 4023.383
[20,     1] loss: 4019.700
[21,     1] loss: 3791.506
[22,     1] loss: 3724.487
[23,     1] loss: 3721.025
[24,     1] loss: 3775.864
[25,     1] loss: 3606.846
[26,     1] loss: 3797.840
[27,     1] loss: 3670.240
[28,     1] loss: 3642.145
[29,     1] loss: 3586.907
[30,     1] loss: 3538.506
[31,     1] loss: 3259.522
[32,     1] loss: 3756.762
[33,     1] loss: 3261.675
[34,     1] loss: 3731.926
[35,     1] loss: 3142.056
[36,     1] loss: 3123.594
[37,     1] loss: 2997.829
[38,     1] loss: 2903.280
Early stopping applied (best metric=1.0065805912017822)
Finished Training
Total time taken: 6.390008449554443
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 4662.807
[2,     1] loss: 4664.058
[3,     1] loss: 4659.759
[4,     1] loss: 4671.824
[5,     1] loss: 4673.825
[6,     1] loss: 4657.769
[7,     1] loss: 4646.503
[8,     1] loss: 4651.250
[9,     1] loss: 4653.470
[10,     1] loss: 4650.288
[11,     1] loss: 4615.940
[12,     1] loss: 4591.637
[13,     1] loss: 4556.620
[14,     1] loss: 4516.394
[15,     1] loss: 4366.534
[16,     1] loss: 4321.956
[17,     1] loss: 4210.686
[18,     1] loss: 4110.822
[19,     1] loss: 4117.743
[20,     1] loss: 3954.122
[21,     1] loss: 3922.999
[22,     1] loss: 3789.219
[23,     1] loss: 3846.851
[24,     1] loss: 3562.729
[25,     1] loss: 3709.297
[26,     1] loss: 3714.347
[27,     1] loss: 3453.721
[28,     1] loss: 3356.990
[29,     1] loss: 3807.485
[30,     1] loss: 3409.700
[31,     1] loss: 3835.847
[32,     1] loss: 3380.383
[33,     1] loss: 3455.539
[34,     1] loss: 3512.245
[35,     1] loss: 3237.812
[36,     1] loss: 3575.830
[37,     1] loss: 3268.356
[38,     1] loss: 3312.487
[39,     1] loss: 3080.112
Early stopping applied (best metric=0.9500577449798584)
Finished Training
Total time taken: 5.3510050773620605
{'Hydroxylation-K Validation Accuracy': 0.7281619385342789, 'Hydroxylation-K Validation Sensitivity': 0.7414814814814815, 'Hydroxylation-K Validation Specificity': 0.724561403508772, 'Hydroxylation-K Validation Precision': 0.4165852781358457, 'Hydroxylation-K AUC ROC': 0.7767056530214425, 'Hydroxylation-K AUC PR': 0.5424461819844077, 'Hydroxylation-K MCC': 0.3944270480792194, 'Hydroxylation-K F1': 0.5289806683580119, 'Validation Loss (Hydroxylation-K)': 0.45552817185719807, 'Hydroxylation-P Validation Accuracy': 0.7382648596517943, 'Hydroxylation-P Validation Sensitivity': 0.7366137566137566, 'Hydroxylation-P Validation Specificity': 0.7386153922888922, 'Hydroxylation-P Validation Precision': 0.38694342993986, 'Hydroxylation-P AUC ROC': 0.79619161823374, 'Hydroxylation-P AUC PR': 0.5227376745797392, 'Hydroxylation-P MCC': 0.3882778218046467, 'Hydroxylation-P F1': 0.4992427587433963, 'Validation Loss (Hydroxylation-P)': 0.4299298286437988, 'Validation Loss (total)': 0.8854580044746398, 'TimeToTrain': 8.985275840759277}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004001229793364325,
 'learning_rate_Hydroxylation-K': 0.0035082580537287036,
 'learning_rate_Hydroxylation-P': 0.004245881529934844,
 'log_base': 1.1793690258719518,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2515206897,
 'sample_weights': [14.368562713766444, 1.792338422386156],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.523591066848311,
 'weight_decay_Hydroxylation-K': 8.738395185985233,
 'weight_decay_Hydroxylation-P': 6.336377743355031}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3298.390
[2,     1] loss: 3288.245
[3,     1] loss: 3284.896
[4,     1] loss: 3281.113
[5,     1] loss: 3284.903
[6,     1] loss: 3268.762
[7,     1] loss: 3267.267
[8,     1] loss: 3275.258
[9,     1] loss: 3251.773
[10,     1] loss: 3229.166
[11,     1] loss: 3198.506
[12,     1] loss: 3138.816
[13,     1] loss: 3066.893
[14,     1] loss: 3022.581
[15,     1] loss: 2888.432
[16,     1] loss: 2839.098
[17,     1] loss: 2777.140
[18,     1] loss: 2957.802
[19,     1] loss: 2680.352
[20,     1] loss: 2660.464
[21,     1] loss: 2622.640
[22,     1] loss: 2634.882
[23,     1] loss: 2632.316
[24,     1] loss: 2506.671
[25,     1] loss: 2957.754
[26,     1] loss: 2471.917
[27,     1] loss: 2520.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047180925635177985,
 'learning_rate_Hydroxylation-K': 0.006965875163379197,
 'learning_rate_Hydroxylation-P': 0.003600614272369436,
 'log_base': 2.0597358612561942,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1166274853,
 'sample_weights': [10.11909008482402, 1.2649349891010748],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.560349491675776,
 'weight_decay_Hydroxylation-K': 1.878930327189638,
 'weight_decay_Hydroxylation-P': 3.964711774022634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.855
[2,     1] loss: 1397.781
[3,     1] loss: 1396.531
[4,     1] loss: 1391.708
[5,     1] loss: 1391.929
[6,     1] loss: 1395.028
[7,     1] loss: 1387.890
[8,     1] loss: 1383.698
[9,     1] loss: 1371.765
[10,     1] loss: 1349.401
[11,     1] loss: 1320.905
[12,     1] loss: 1290.037
[13,     1] loss: 1275.379
[14,     1] loss: 1219.559
[15,     1] loss: 1200.297
[16,     1] loss: 1172.087
[17,     1] loss: 1242.965
[18,     1] loss: 1146.307
[19,     1] loss: 1155.797
[20,     1] loss: 1100.067
[21,     1] loss: 1099.166
[22,     1] loss: 1134.825
[23,     1] loss: 1060.835
[24,     1] loss: 1088.289
[25,     1] loss: 1042.265
[26,     1] loss: 1013.443
[27,     1] loss: 1042.084
[28,     1] loss: 1012.635
[29,     1] loss: 976.293
[30,     1] loss: 994.269
[31,     1] loss: 1079.800
[32,     1] loss: 974.996
[33,     1] loss: 1105.630
[34,     1] loss: 1098.355
[35,     1] loss: 1046.742
[36,     1] loss: 1056.269
[37,     1] loss: 1063.935
[38,     1] loss: 972.016
[39,     1] loss: 1004.233
[40,     1] loss: 1024.325
[41,     1] loss: 975.889
[42,     1] loss: 969.456
[43,     1] loss: 893.588
[44,     1] loss: 986.960
[45,     1] loss: 864.851
[46,     1] loss: 962.667
[47,     1] loss: 840.417
[48,     1] loss: 932.682
Early stopping applied (best metric=0.8194036483764648)
Finished Training
Total time taken: 8.034008502960205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.983
[2,     1] loss: 1399.644
[3,     1] loss: 1394.303
[4,     1] loss: 1395.482
[5,     1] loss: 1392.914
[6,     1] loss: 1388.051
[7,     1] loss: 1377.069
[8,     1] loss: 1366.818
[9,     1] loss: 1348.715
[10,     1] loss: 1304.774
[11,     1] loss: 1272.278
[12,     1] loss: 1236.935
[13,     1] loss: 1214.366
[14,     1] loss: 1176.805
[15,     1] loss: 1162.947
[16,     1] loss: 1165.443
[17,     1] loss: 1148.975
[18,     1] loss: 1081.650
[19,     1] loss: 1119.264
[20,     1] loss: 1076.593
[21,     1] loss: 1139.906
[22,     1] loss: 1088.373
[23,     1] loss: 1083.669
[24,     1] loss: 1023.398
[25,     1] loss: 1079.135
[26,     1] loss: 1043.484
[27,     1] loss: 1033.489
[28,     1] loss: 1048.482
[29,     1] loss: 1030.973
[30,     1] loss: 972.427
[31,     1] loss: 997.254
[32,     1] loss: 1008.650
[33,     1] loss: 938.133
[34,     1] loss: 895.759
[35,     1] loss: 1016.010
[36,     1] loss: 1000.249
[37,     1] loss: 974.764
[38,     1] loss: 948.849
[39,     1] loss: 939.615
[40,     1] loss: 936.656
[41,     1] loss: 906.680
[42,     1] loss: 927.920
[43,     1] loss: 933.323
[44,     1] loss: 906.517
[45,     1] loss: 846.824
[46,     1] loss: 822.391
[47,     1] loss: 900.443
[48,     1] loss: 1068.867
[49,     1] loss: 858.723
[50,     1] loss: 903.071
[51,     1] loss: 833.276
[52,     1] loss: 906.582
Early stopping applied (best metric=0.860524594783783)
Finished Training
Total time taken: 8.741008520126343
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.578
[2,     1] loss: 1394.442
[3,     1] loss: 1397.392
[4,     1] loss: 1395.453
[5,     1] loss: 1396.013
[6,     1] loss: 1394.158
[7,     1] loss: 1394.347
[8,     1] loss: 1394.274
[9,     1] loss: 1390.702
[10,     1] loss: 1390.048
[11,     1] loss: 1383.301
[12,     1] loss: 1375.552
[13,     1] loss: 1360.759
[14,     1] loss: 1337.555
[15,     1] loss: 1285.888
[16,     1] loss: 1261.140
[17,     1] loss: 1207.620
[18,     1] loss: 1199.529
[19,     1] loss: 1175.796
[20,     1] loss: 1151.611
[21,     1] loss: 1122.401
[22,     1] loss: 1179.056
[23,     1] loss: 1093.826
[24,     1] loss: 1193.694
[25,     1] loss: 1097.692
[26,     1] loss: 1117.631
[27,     1] loss: 1139.593
[28,     1] loss: 1067.916
[29,     1] loss: 1161.075
[30,     1] loss: 1034.604
[31,     1] loss: 1095.772
[32,     1] loss: 1072.555
[33,     1] loss: 1057.843
[34,     1] loss: 1033.711
[35,     1] loss: 1032.674
[36,     1] loss: 1006.815
[37,     1] loss: 1004.379
[38,     1] loss: 1004.770
[39,     1] loss: 917.200
[40,     1] loss: 947.425
[41,     1] loss: 898.992
[42,     1] loss: 855.814
[43,     1] loss: 922.278
[44,     1] loss: 1133.843
[45,     1] loss: 1042.191
[46,     1] loss: 863.578
[47,     1] loss: 975.569
[48,     1] loss: 918.040
[49,     1] loss: 889.797
[50,     1] loss: 929.028
[51,     1] loss: 852.695
[52,     1] loss: 802.344
[53,     1] loss: 799.743
[54,     1] loss: 835.805
[55,     1] loss: 862.365
[56,     1] loss: 756.352
[57,     1] loss: 823.049
[58,     1] loss: 844.933
Early stopping applied (best metric=0.7934495210647583)
Finished Training
Total time taken: 9.710010051727295
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.399
[2,     1] loss: 1392.973
[3,     1] loss: 1396.481
[4,     1] loss: 1396.396
[5,     1] loss: 1390.994
[6,     1] loss: 1395.587
[7,     1] loss: 1390.654
[8,     1] loss: 1386.112
[9,     1] loss: 1379.592
[10,     1] loss: 1368.325
[11,     1] loss: 1348.962
[12,     1] loss: 1329.865
[13,     1] loss: 1282.812
[14,     1] loss: 1233.716
[15,     1] loss: 1238.021
[16,     1] loss: 1208.333
[17,     1] loss: 1179.070
[18,     1] loss: 1192.917
[19,     1] loss: 1136.266
[20,     1] loss: 1135.256
[21,     1] loss: 1174.813
[22,     1] loss: 1123.656
[23,     1] loss: 1095.700
[24,     1] loss: 1164.732
[25,     1] loss: 1117.758
[26,     1] loss: 1079.831
[27,     1] loss: 1069.024
[28,     1] loss: 1053.776
[29,     1] loss: 1045.180
[30,     1] loss: 1019.933
[31,     1] loss: 1075.274
[32,     1] loss: 1071.123
[33,     1] loss: 984.424
[34,     1] loss: 981.445
[35,     1] loss: 1012.874
[36,     1] loss: 996.208
[37,     1] loss: 1028.731
[38,     1] loss: 965.016
[39,     1] loss: 940.550
[40,     1] loss: 917.344
[41,     1] loss: 912.784
[42,     1] loss: 918.881
[43,     1] loss: 1063.459
[44,     1] loss: 1472.520
[45,     1] loss: 964.258
[46,     1] loss: 1102.046
[47,     1] loss: 1129.936
[48,     1] loss: 1070.143
[49,     1] loss: 1093.218
[50,     1] loss: 1088.651
[51,     1] loss: 1094.133
[52,     1] loss: 1051.650
[53,     1] loss: 1000.995
[54,     1] loss: 1014.788
[55,     1] loss: 997.410
[56,     1] loss: 986.520
[57,     1] loss: 980.187
[58,     1] loss: 986.824
[59,     1] loss: 917.230
[60,     1] loss: 975.052
Early stopping applied (best metric=0.6857280731201172)
Finished Training
Total time taken: 9.9090096950531
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1396.750
[2,     1] loss: 1393.763
[3,     1] loss: 1396.108
[4,     1] loss: 1391.038
[5,     1] loss: 1398.065
[6,     1] loss: 1381.871
[7,     1] loss: 1380.959
[8,     1] loss: 1334.769
[9,     1] loss: 1312.991
[10,     1] loss: 1263.432
[11,     1] loss: 1214.175
[12,     1] loss: 1209.650
[13,     1] loss: 1201.095
[14,     1] loss: 1189.547
[15,     1] loss: 1174.212
[16,     1] loss: 1132.971
[17,     1] loss: 1168.765
[18,     1] loss: 1097.544
[19,     1] loss: 1124.464
[20,     1] loss: 1091.611
[21,     1] loss: 1146.209
[22,     1] loss: 1095.964
[23,     1] loss: 1117.695
[24,     1] loss: 1109.771
[25,     1] loss: 1042.503
[26,     1] loss: 1067.177
[27,     1] loss: 1045.985
[28,     1] loss: 1060.659
[29,     1] loss: 1021.525
[30,     1] loss: 1012.811
[31,     1] loss: 982.154
[32,     1] loss: 1006.694
[33,     1] loss: 991.279
[34,     1] loss: 987.557
[35,     1] loss: 961.958
[36,     1] loss: 900.964
[37,     1] loss: 955.764
[38,     1] loss: 964.124
[39,     1] loss: 1028.967
[40,     1] loss: 916.659
[41,     1] loss: 926.311
[42,     1] loss: 999.885
[43,     1] loss: 886.418
[44,     1] loss: 915.757
[45,     1] loss: 938.984
[46,     1] loss: 854.673
[47,     1] loss: 821.940
[48,     1] loss: 914.516
[49,     1] loss: 997.041
[50,     1] loss: 963.410
[51,     1] loss: 861.660
[52,     1] loss: 970.174
[53,     1] loss: 827.862
[54,     1] loss: 921.605
[55,     1] loss: 821.125
[56,     1] loss: 845.083
Early stopping applied (best metric=0.7337208390235901)
Finished Training
Total time taken: 9.411009311676025
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.805
[2,     1] loss: 1392.698
[3,     1] loss: 1395.551
[4,     1] loss: 1396.814
[5,     1] loss: 1386.998
[6,     1] loss: 1382.071
[7,     1] loss: 1370.300
[8,     1] loss: 1330.553
[9,     1] loss: 1304.319
[10,     1] loss: 1260.605
[11,     1] loss: 1201.252
[12,     1] loss: 1238.860
[13,     1] loss: 1255.943
[14,     1] loss: 1191.176
[15,     1] loss: 1183.114
[16,     1] loss: 1163.645
[17,     1] loss: 1186.472
[18,     1] loss: 1224.923
[19,     1] loss: 1165.420
[20,     1] loss: 1149.860
[21,     1] loss: 1169.331
[22,     1] loss: 1204.454
[23,     1] loss: 1099.822
[24,     1] loss: 1134.230
[25,     1] loss: 1146.912
[26,     1] loss: 1078.098
[27,     1] loss: 1042.932
[28,     1] loss: 1015.817
[29,     1] loss: 1084.555
[30,     1] loss: 1015.451
[31,     1] loss: 1001.910
[32,     1] loss: 1032.941
[33,     1] loss: 993.769
[34,     1] loss: 986.113
[35,     1] loss: 968.767
[36,     1] loss: 1004.540
[37,     1] loss: 985.090
[38,     1] loss: 953.427
[39,     1] loss: 1122.836
[40,     1] loss: 976.329
[41,     1] loss: 978.210
[42,     1] loss: 1012.349
[43,     1] loss: 1039.710
[44,     1] loss: 934.122
[45,     1] loss: 1020.220
[46,     1] loss: 913.595
[47,     1] loss: 978.029
[48,     1] loss: 911.669
[49,     1] loss: 887.892
[50,     1] loss: 871.130
[51,     1] loss: 851.599
[52,     1] loss: 859.339
[53,     1] loss: 960.877
[54,     1] loss: 830.889
Early stopping applied (best metric=0.8077273964881897)
Finished Training
Total time taken: 7.366006851196289
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.709
[2,     1] loss: 1392.490
[3,     1] loss: 1405.093
[4,     1] loss: 1399.433
[5,     1] loss: 1398.683
[6,     1] loss: 1392.453
[7,     1] loss: 1390.277
[8,     1] loss: 1388.277
[9,     1] loss: 1384.425
[10,     1] loss: 1369.710
[11,     1] loss: 1363.567
[12,     1] loss: 1332.162
[13,     1] loss: 1287.419
[14,     1] loss: 1263.948
[15,     1] loss: 1234.539
[16,     1] loss: 1202.283
[17,     1] loss: 1205.176
[18,     1] loss: 1156.428
[19,     1] loss: 1150.502
[20,     1] loss: 1276.999
[21,     1] loss: 1143.649
[22,     1] loss: 1201.191
[23,     1] loss: 1128.480
[24,     1] loss: 1174.407
[25,     1] loss: 1136.632
[26,     1] loss: 1085.423
[27,     1] loss: 1105.731
[28,     1] loss: 1098.639
[29,     1] loss: 1099.265
[30,     1] loss: 1061.352
[31,     1] loss: 1038.877
[32,     1] loss: 1068.088
[33,     1] loss: 1030.719
[34,     1] loss: 996.202
[35,     1] loss: 1025.729
[36,     1] loss: 949.371
[37,     1] loss: 1017.295
[38,     1] loss: 943.057
[39,     1] loss: 949.057
[40,     1] loss: 1002.689
[41,     1] loss: 961.340
[42,     1] loss: 885.977
[43,     1] loss: 899.853
[44,     1] loss: 872.046
[45,     1] loss: 966.828
[46,     1] loss: 1317.160
[47,     1] loss: 1049.546
[48,     1] loss: 990.270
[49,     1] loss: 964.062
[50,     1] loss: 1067.788
[51,     1] loss: 1004.703
[52,     1] loss: 927.753
[53,     1] loss: 948.129
[54,     1] loss: 974.383
[55,     1] loss: 924.708
[56,     1] loss: 908.249
[57,     1] loss: 898.933
[58,     1] loss: 882.771
[59,     1] loss: 869.429
[60,     1] loss: 851.807
[61,     1] loss: 854.356
[62,     1] loss: 851.299
[63,     1] loss: 858.040
[64,     1] loss: 811.614
[65,     1] loss: 794.911
[66,     1] loss: 803.482
[67,     1] loss: 751.846
[68,     1] loss: 840.802
[69,     1] loss: 716.312
[70,     1] loss: 697.518
[71,     1] loss: 771.241
[72,     1] loss: 1286.584
[73,     1] loss: 1143.927
[74,     1] loss: 847.154
[75,     1] loss: 941.374
[76,     1] loss: 1046.078
[77,     1] loss: 960.149
[78,     1] loss: 904.546
Early stopping applied (best metric=0.8063353300094604)
Finished Training
Total time taken: 13.018013000488281
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.854
[2,     1] loss: 1405.707
[3,     1] loss: 1389.401
[4,     1] loss: 1390.663
[5,     1] loss: 1391.971
[6,     1] loss: 1385.358
[7,     1] loss: 1377.312
[8,     1] loss: 1365.387
[9,     1] loss: 1337.410
[10,     1] loss: 1291.348
[11,     1] loss: 1240.077
[12,     1] loss: 1201.104
[13,     1] loss: 1179.852
[14,     1] loss: 1149.098
[15,     1] loss: 1153.864
[16,     1] loss: 1155.751
[17,     1] loss: 1143.906
[18,     1] loss: 1074.674
[19,     1] loss: 1120.700
[20,     1] loss: 1070.522
[21,     1] loss: 1096.425
[22,     1] loss: 1043.252
[23,     1] loss: 1056.957
[24,     1] loss: 1053.313
[25,     1] loss: 1055.178
[26,     1] loss: 1049.412
[27,     1] loss: 1016.258
[28,     1] loss: 1020.307
[29,     1] loss: 1005.387
[30,     1] loss: 995.875
[31,     1] loss: 984.529
[32,     1] loss: 962.058
[33,     1] loss: 1033.467
[34,     1] loss: 1065.969
[35,     1] loss: 969.381
[36,     1] loss: 990.535
[37,     1] loss: 912.438
[38,     1] loss: 940.516
[39,     1] loss: 945.312
[40,     1] loss: 917.004
[41,     1] loss: 898.901
[42,     1] loss: 852.200
[43,     1] loss: 912.136
[44,     1] loss: 928.486
[45,     1] loss: 796.436
[46,     1] loss: 870.622
[47,     1] loss: 790.298
[48,     1] loss: 843.957
[49,     1] loss: 785.506
[50,     1] loss: 862.446
[51,     1] loss: 964.134
[52,     1] loss: 1114.701
[53,     1] loss: 845.307
[54,     1] loss: 919.468
[55,     1] loss: 884.989
[56,     1] loss: 852.806
[57,     1] loss: 883.898
[58,     1] loss: 807.216
[59,     1] loss: 772.931
[60,     1] loss: 789.116
[61,     1] loss: 793.241
Early stopping applied (best metric=0.8800494074821472)
Finished Training
Total time taken: 8.371009349822998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.183
[2,     1] loss: 1397.514
[3,     1] loss: 1394.987
[4,     1] loss: 1394.617
[5,     1] loss: 1397.494
[6,     1] loss: 1395.047
[7,     1] loss: 1390.571
[8,     1] loss: 1386.126
[9,     1] loss: 1384.759
[10,     1] loss: 1371.885
[11,     1] loss: 1360.688
[12,     1] loss: 1330.948
[13,     1] loss: 1301.090
[14,     1] loss: 1255.189
[15,     1] loss: 1220.662
[16,     1] loss: 1189.381
[17,     1] loss: 1270.405
[18,     1] loss: 1237.800
[19,     1] loss: 1188.402
[20,     1] loss: 1234.470
[21,     1] loss: 1175.393
[22,     1] loss: 1149.905
[23,     1] loss: 1167.869
[24,     1] loss: 1148.771
[25,     1] loss: 1159.753
[26,     1] loss: 1121.412
[27,     1] loss: 1143.354
[28,     1] loss: 1089.783
[29,     1] loss: 1079.383
[30,     1] loss: 1061.944
[31,     1] loss: 1061.854
[32,     1] loss: 1008.736
[33,     1] loss: 1033.963
[34,     1] loss: 1089.687
[35,     1] loss: 1041.165
[36,     1] loss: 981.858
[37,     1] loss: 1028.669
[38,     1] loss: 961.125
[39,     1] loss: 980.744
[40,     1] loss: 1009.039
[41,     1] loss: 938.609
[42,     1] loss: 1075.523
[43,     1] loss: 1256.524
[44,     1] loss: 1049.394
[45,     1] loss: 1020.208
[46,     1] loss: 994.093
[47,     1] loss: 1014.275
[48,     1] loss: 1037.515
[49,     1] loss: 972.155
[50,     1] loss: 1006.450
[51,     1] loss: 1042.047
[52,     1] loss: 925.898
[53,     1] loss: 969.043
[54,     1] loss: 987.582
[55,     1] loss: 912.410
[56,     1] loss: 879.323
[57,     1] loss: 856.984
[58,     1] loss: 896.066
[59,     1] loss: 910.855
[60,     1] loss: 896.446
[61,     1] loss: 854.259
[62,     1] loss: 868.997
[63,     1] loss: 835.264
[64,     1] loss: 819.124
[65,     1] loss: 814.267
[66,     1] loss: 740.994
[67,     1] loss: 1090.152
[68,     1] loss: 1186.901
[69,     1] loss: 818.692
[70,     1] loss: 966.694
[71,     1] loss: 870.643
[72,     1] loss: 901.871
[73,     1] loss: 878.167
[74,     1] loss: 811.670
[75,     1] loss: 838.036
[76,     1] loss: 793.878
[77,     1] loss: 750.037
[78,     1] loss: 739.819
[79,     1] loss: 740.956
[80,     1] loss: 789.421
[81,     1] loss: 741.892
[82,     1] loss: 753.977
[83,     1] loss: 695.856
[84,     1] loss: 723.595
[85,     1] loss: 796.686
[86,     1] loss: 871.117
[87,     1] loss: 686.107
[88,     1] loss: 700.172
[89,     1] loss: 683.617
[90,     1] loss: 673.161
[91,     1] loss: 572.029
[92,     1] loss: 549.295
[93,     1] loss: 700.213
[94,     1] loss: 1061.725
[95,     1] loss: 1127.279
[96,     1] loss: 669.403
[97,     1] loss: 867.139
[98,     1] loss: 778.047
Early stopping applied (best metric=0.7240313291549683)
Finished Training
Total time taken: 13.57601523399353
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1395.481
[2,     1] loss: 1397.359
[3,     1] loss: 1399.886
[4,     1] loss: 1392.653
[5,     1] loss: 1398.028
[6,     1] loss: 1389.327
[7,     1] loss: 1379.913
[8,     1] loss: 1365.153
[9,     1] loss: 1348.519
[10,     1] loss: 1292.997
[11,     1] loss: 1267.019
[12,     1] loss: 1218.827
[13,     1] loss: 1140.734
[14,     1] loss: 1180.538
[15,     1] loss: 1157.262
[16,     1] loss: 1171.253
[17,     1] loss: 1161.406
[18,     1] loss: 1130.713
[19,     1] loss: 1132.018
[20,     1] loss: 1101.241
[21,     1] loss: 1135.509
[22,     1] loss: 1119.866
[23,     1] loss: 1078.436
[24,     1] loss: 1119.223
[25,     1] loss: 1092.031
[26,     1] loss: 1057.139
[27,     1] loss: 1048.652
[28,     1] loss: 1017.450
[29,     1] loss: 994.477
[30,     1] loss: 1015.749
[31,     1] loss: 982.095
[32,     1] loss: 917.497
[33,     1] loss: 957.826
[34,     1] loss: 1056.282
[35,     1] loss: 979.068
[36,     1] loss: 982.253
[37,     1] loss: 964.831
[38,     1] loss: 964.450
[39,     1] loss: 977.574
[40,     1] loss: 952.674
[41,     1] loss: 926.711
[42,     1] loss: 853.733
[43,     1] loss: 933.348
[44,     1] loss: 875.589
[45,     1] loss: 921.271
[46,     1] loss: 891.092
[47,     1] loss: 794.627
[48,     1] loss: 856.646
[49,     1] loss: 930.080
Early stopping applied (best metric=0.842642605304718)
Finished Training
Total time taken: 7.041007041931152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.926
[2,     1] loss: 1394.275
[3,     1] loss: 1396.132
[4,     1] loss: 1395.671
[5,     1] loss: 1392.732
[6,     1] loss: 1396.780
[7,     1] loss: 1391.711
[8,     1] loss: 1386.826
[9,     1] loss: 1381.738
[10,     1] loss: 1377.985
[11,     1] loss: 1360.986
[12,     1] loss: 1349.180
[13,     1] loss: 1299.231
[14,     1] loss: 1272.310
[15,     1] loss: 1256.273
[16,     1] loss: 1215.537
[17,     1] loss: 1199.797
[18,     1] loss: 1231.380
[19,     1] loss: 1189.762
[20,     1] loss: 1148.854
[21,     1] loss: 1168.833
[22,     1] loss: 1157.520
[23,     1] loss: 1139.123
[24,     1] loss: 1121.814
[25,     1] loss: 1165.480
[26,     1] loss: 1200.232
[27,     1] loss: 1090.808
[28,     1] loss: 1117.609
[29,     1] loss: 1080.930
[30,     1] loss: 1106.442
[31,     1] loss: 1055.147
[32,     1] loss: 1091.159
[33,     1] loss: 1039.743
[34,     1] loss: 1052.396
[35,     1] loss: 1014.419
[36,     1] loss: 1028.743
[37,     1] loss: 1007.612
[38,     1] loss: 1044.867
[39,     1] loss: 1036.305
[40,     1] loss: 958.470
[41,     1] loss: 966.722
[42,     1] loss: 967.415
[43,     1] loss: 918.336
[44,     1] loss: 933.808
[45,     1] loss: 994.444
[46,     1] loss: 960.640
[47,     1] loss: 906.741
[48,     1] loss: 979.115
[49,     1] loss: 933.583
[50,     1] loss: 898.112
[51,     1] loss: 924.762
[52,     1] loss: 930.892
[53,     1] loss: 869.719
[54,     1] loss: 945.660
[55,     1] loss: 934.241
[56,     1] loss: 838.065
[57,     1] loss: 882.089
[58,     1] loss: 870.384
[59,     1] loss: 881.601
[60,     1] loss: 919.314
[61,     1] loss: 901.064
[62,     1] loss: 794.439
[63,     1] loss: 989.114
[64,     1] loss: 820.216
[65,     1] loss: 920.261
[66,     1] loss: 872.675
[67,     1] loss: 853.536
[68,     1] loss: 859.280
[69,     1] loss: 820.179
[70,     1] loss: 903.299
[71,     1] loss: 700.251
[72,     1] loss: 851.178
[73,     1] loss: 701.721
[74,     1] loss: 791.483
[75,     1] loss: 833.687
[76,     1] loss: 705.594
[77,     1] loss: 829.037
[78,     1] loss: 694.419
Early stopping applied (best metric=0.6942163705825806)
Finished Training
Total time taken: 10.634010791778564
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.157
[2,     1] loss: 1397.499
[3,     1] loss: 1396.673
[4,     1] loss: 1391.240
[5,     1] loss: 1401.767
[6,     1] loss: 1393.250
[7,     1] loss: 1391.634
[8,     1] loss: 1390.017
[9,     1] loss: 1386.196
[10,     1] loss: 1374.233
[11,     1] loss: 1361.828
[12,     1] loss: 1332.158
[13,     1] loss: 1299.593
[14,     1] loss: 1279.693
[15,     1] loss: 1220.243
[16,     1] loss: 1197.000
[17,     1] loss: 1226.546
[18,     1] loss: 1172.317
[19,     1] loss: 1179.218
[20,     1] loss: 1166.518
[21,     1] loss: 1156.594
[22,     1] loss: 1144.638
[23,     1] loss: 1150.558
[24,     1] loss: 1150.768
[25,     1] loss: 1129.696
[26,     1] loss: 1125.005
[27,     1] loss: 1123.539
[28,     1] loss: 1075.921
[29,     1] loss: 1085.208
[30,     1] loss: 1105.122
[31,     1] loss: 1030.975
[32,     1] loss: 1029.114
[33,     1] loss: 1049.921
[34,     1] loss: 1006.176
[35,     1] loss: 1023.577
[36,     1] loss: 1026.177
[37,     1] loss: 994.924
[38,     1] loss: 952.984
[39,     1] loss: 986.972
[40,     1] loss: 1076.477
[41,     1] loss: 1068.255
[42,     1] loss: 957.058
[43,     1] loss: 1033.104
[44,     1] loss: 984.254
[45,     1] loss: 1044.288
[46,     1] loss: 931.201
[47,     1] loss: 1017.320
[48,     1] loss: 960.413
[49,     1] loss: 974.765
[50,     1] loss: 985.038
[51,     1] loss: 926.899
[52,     1] loss: 906.196
[53,     1] loss: 1003.883
[54,     1] loss: 836.349
[55,     1] loss: 1046.601
[56,     1] loss: 946.850
[57,     1] loss: 843.866
[58,     1] loss: 861.015
[59,     1] loss: 850.734
[60,     1] loss: 836.877
[61,     1] loss: 835.323
[62,     1] loss: 863.924
[63,     1] loss: 825.779
[64,     1] loss: 878.730
[65,     1] loss: 883.035
[66,     1] loss: 749.894
[67,     1] loss: 737.012
[68,     1] loss: 729.780
[69,     1] loss: 681.799
[70,     1] loss: 714.148
[71,     1] loss: 671.880
[72,     1] loss: 853.375
[73,     1] loss: 1549.688
[74,     1] loss: 1209.354
[75,     1] loss: 1197.411
[76,     1] loss: 1030.893
[77,     1] loss: 1082.390
[78,     1] loss: 1106.140
[79,     1] loss: 1208.897
[80,     1] loss: 1178.512
[81,     1] loss: 1141.745
[82,     1] loss: 1117.072
[83,     1] loss: 1091.969
[84,     1] loss: 1075.050
[85,     1] loss: 1070.503
[86,     1] loss: 1008.813
[87,     1] loss: 992.037
[88,     1] loss: 987.973
[89,     1] loss: 1061.597
Early stopping applied (best metric=0.6740797162055969)
Finished Training
Total time taken: 12.35401177406311
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.894
[2,     1] loss: 1400.559
[3,     1] loss: 1396.417
[4,     1] loss: 1393.956
[5,     1] loss: 1396.642
[6,     1] loss: 1391.723
[7,     1] loss: 1389.587
[8,     1] loss: 1392.975
[9,     1] loss: 1389.490
[10,     1] loss: 1382.017
[11,     1] loss: 1379.439
[12,     1] loss: 1361.753
[13,     1] loss: 1343.118
[14,     1] loss: 1305.884
[15,     1] loss: 1287.246
[16,     1] loss: 1245.938
[17,     1] loss: 1217.733
[18,     1] loss: 1211.643
[19,     1] loss: 1197.346
[20,     1] loss: 1151.473
[21,     1] loss: 1187.297
[22,     1] loss: 1130.664
[23,     1] loss: 1118.667
[24,     1] loss: 1124.036
[25,     1] loss: 1127.184
[26,     1] loss: 1115.047
[27,     1] loss: 1072.830
[28,     1] loss: 1061.443
[29,     1] loss: 1077.978
[30,     1] loss: 1075.305
[31,     1] loss: 976.359
[32,     1] loss: 1111.027
[33,     1] loss: 1054.412
[34,     1] loss: 1011.674
[35,     1] loss: 1050.238
[36,     1] loss: 986.786
[37,     1] loss: 968.716
[38,     1] loss: 939.045
[39,     1] loss: 993.749
[40,     1] loss: 924.133
[41,     1] loss: 933.037
[42,     1] loss: 927.845
[43,     1] loss: 904.254
[44,     1] loss: 1042.156
[45,     1] loss: 1113.679
[46,     1] loss: 862.524
[47,     1] loss: 982.018
[48,     1] loss: 905.436
[49,     1] loss: 935.865
[50,     1] loss: 993.873
[51,     1] loss: 880.869
[52,     1] loss: 974.489
[53,     1] loss: 844.189
[54,     1] loss: 871.740
[55,     1] loss: 846.498
[56,     1] loss: 1012.594
[57,     1] loss: 775.180
[58,     1] loss: 922.239
[59,     1] loss: 777.821
[60,     1] loss: 822.987
Early stopping applied (best metric=0.8642572164535522)
Finished Training
Total time taken: 8.55200982093811
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.286
[2,     1] loss: 1393.553
[3,     1] loss: 1400.285
[4,     1] loss: 1393.874
[5,     1] loss: 1392.274
[6,     1] loss: 1389.990
[7,     1] loss: 1385.206
[8,     1] loss: 1379.354
[9,     1] loss: 1362.494
[10,     1] loss: 1332.986
[11,     1] loss: 1281.861
[12,     1] loss: 1282.792
[13,     1] loss: 1241.984
[14,     1] loss: 1210.708
[15,     1] loss: 1216.224
[16,     1] loss: 1195.147
[17,     1] loss: 1161.338
[18,     1] loss: 1166.511
[19,     1] loss: 1151.611
[20,     1] loss: 1106.905
[21,     1] loss: 1131.785
[22,     1] loss: 1092.626
[23,     1] loss: 1123.243
[24,     1] loss: 1098.003
[25,     1] loss: 1011.771
[26,     1] loss: 1117.019
[27,     1] loss: 1093.788
[28,     1] loss: 1014.464
[29,     1] loss: 1083.337
[30,     1] loss: 1058.427
[31,     1] loss: 1028.521
[32,     1] loss: 1005.938
[33,     1] loss: 1008.112
[34,     1] loss: 1057.416
[35,     1] loss: 965.538
[36,     1] loss: 1031.164
[37,     1] loss: 978.493
[38,     1] loss: 975.653
[39,     1] loss: 984.741
[40,     1] loss: 907.218
[41,     1] loss: 954.053
[42,     1] loss: 891.789
[43,     1] loss: 930.598
[44,     1] loss: 902.700
[45,     1] loss: 1039.780
[46,     1] loss: 1119.808
[47,     1] loss: 887.342
[48,     1] loss: 973.119
[49,     1] loss: 916.439
[50,     1] loss: 965.207
[51,     1] loss: 986.229
[52,     1] loss: 923.536
[53,     1] loss: 882.276
[54,     1] loss: 852.270
[55,     1] loss: 859.017
[56,     1] loss: 911.972
[57,     1] loss: 819.035
[58,     1] loss: 789.818
[59,     1] loss: 809.157
[60,     1] loss: 768.883
[61,     1] loss: 735.390
[62,     1] loss: 850.785
[63,     1] loss: 849.939
[64,     1] loss: 814.279
[65,     1] loss: 736.117
[66,     1] loss: 854.359
[67,     1] loss: 819.115
[68,     1] loss: 749.082
[69,     1] loss: 870.033
[70,     1] loss: 1075.068
[71,     1] loss: 764.914
[72,     1] loss: 823.725
[73,     1] loss: 740.496
[74,     1] loss: 813.766
[75,     1] loss: 741.041
[76,     1] loss: 809.956
[77,     1] loss: 736.497
[78,     1] loss: 683.097
[79,     1] loss: 699.664
[80,     1] loss: 655.451
Early stopping applied (best metric=0.6504963636398315)
Finished Training
Total time taken: 10.648010730743408
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1398.649
[2,     1] loss: 1399.025
[3,     1] loss: 1407.550
[4,     1] loss: 1394.895
[5,     1] loss: 1397.866
[6,     1] loss: 1392.367
[7,     1] loss: 1394.354
[8,     1] loss: 1397.463
[9,     1] loss: 1393.647
[10,     1] loss: 1393.296
[11,     1] loss: 1387.977
[12,     1] loss: 1383.391
[13,     1] loss: 1376.324
[14,     1] loss: 1364.098
[15,     1] loss: 1343.784
[16,     1] loss: 1309.592
[17,     1] loss: 1299.576
[18,     1] loss: 1242.582
[19,     1] loss: 1205.405
[20,     1] loss: 1180.362
[21,     1] loss: 1189.249
[22,     1] loss: 1130.707
[23,     1] loss: 1152.580
[24,     1] loss: 1127.979
[25,     1] loss: 1154.253
[26,     1] loss: 1090.887
[27,     1] loss: 1131.830
[28,     1] loss: 1118.806
[29,     1] loss: 1122.483
[30,     1] loss: 1073.369
[31,     1] loss: 1109.578
[32,     1] loss: 1088.656
[33,     1] loss: 1076.199
[34,     1] loss: 1094.212
[35,     1] loss: 1065.809
[36,     1] loss: 1032.556
[37,     1] loss: 1086.656
[38,     1] loss: 1017.160
[39,     1] loss: 1039.747
[40,     1] loss: 1007.508
[41,     1] loss: 970.552
[42,     1] loss: 1008.654
[43,     1] loss: 1000.866
[44,     1] loss: 952.211
[45,     1] loss: 901.458
[46,     1] loss: 902.569
[47,     1] loss: 920.942
[48,     1] loss: 846.520
[49,     1] loss: 858.719
[50,     1] loss: 899.988
[51,     1] loss: 846.211
[52,     1] loss: 900.856
[53,     1] loss: 1202.083
[54,     1] loss: 1280.306
[55,     1] loss: 1063.916
[56,     1] loss: 1028.804
[57,     1] loss: 1089.683
[58,     1] loss: 1111.346
[59,     1] loss: 1053.239
[60,     1] loss: 1045.838
[61,     1] loss: 1012.575
[62,     1] loss: 1002.016
[63,     1] loss: 1025.443
[64,     1] loss: 1012.450
[65,     1] loss: 924.086
[66,     1] loss: 971.810
[67,     1] loss: 944.840
[68,     1] loss: 919.477
[69,     1] loss: 934.133
[70,     1] loss: 882.616
[71,     1] loss: 877.970
[72,     1] loss: 862.194
[73,     1] loss: 861.553
[74,     1] loss: 843.636
[75,     1] loss: 835.636
[76,     1] loss: 788.606
[77,     1] loss: 865.841
[78,     1] loss: 771.212
[79,     1] loss: 803.141
[80,     1] loss: 760.646
[81,     1] loss: 717.652
[82,     1] loss: 843.392
[83,     1] loss: 1287.870
[84,     1] loss: 998.476
[85,     1] loss: 1052.882
[86,     1] loss: 953.465
[87,     1] loss: 983.182
[88,     1] loss: 976.873
[89,     1] loss: 959.363
[90,     1] loss: 933.292
[91,     1] loss: 977.562
[92,     1] loss: 928.645
Early stopping applied (best metric=0.8944685459136963)
Finished Training
Total time taken: 13.19401240348816
{'Hydroxylation-K Validation Accuracy': 0.7858156028368795, 'Hydroxylation-K Validation Sensitivity': 0.7259259259259259, 'Hydroxylation-K Validation Specificity': 0.8017543859649123, 'Hydroxylation-K Validation Precision': 0.4848751411986706, 'Hydroxylation-K AUC ROC': 0.8350877192982455, 'Hydroxylation-K AUC PR': 0.6232656343042657, 'Hydroxylation-K MCC': 0.46208652417443663, 'Hydroxylation-K F1': 0.5765997652374464, 'Validation Loss (Hydroxylation-K)': 0.4074628015359243, 'Hydroxylation-P Validation Accuracy': 0.8049490719591222, 'Hydroxylation-P Validation Sensitivity': 0.7685714285714286, 'Hydroxylation-P Validation Specificity': 0.8127213327347997, 'Hydroxylation-P Validation Precision': 0.47514285932304057, 'Hydroxylation-P AUC ROC': 0.8436196378396738, 'Hydroxylation-P AUC PR': 0.5813245464166775, 'Hydroxylation-P MCC': 0.49241943015883705, 'Hydroxylation-P F1': 0.5846813466875247, 'Validation Loss (Hydroxylation-P)': 0.37461259762446086, 'Validation Loss (total)': 0.7820753971735637, 'TimeToTrain': 10.037276871999104}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029898761486900826,
 'learning_rate_Hydroxylation-K': 0.0033286730460963436,
 'learning_rate_Hydroxylation-P': 0.004763939791936699,
 'log_base': 1.6150312527328292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3213358521,
 'sample_weights': [2.31211317163728, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.567475631733343,
 'weight_decay_Hydroxylation-K': 1.9950755613461424,
 'weight_decay_Hydroxylation-P': 0.8322124328450354}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1648.507
[2,     1] loss: 1638.704
[3,     1] loss: 1643.199
[4,     1] loss: 1645.971
[5,     1] loss: 1639.568
[6,     1] loss: 1641.104
[7,     1] loss: 1639.012
[8,     1] loss: 1639.225
[9,     1] loss: 1636.025
[10,     1] loss: 1639.351
[11,     1] loss: 1626.881
[12,     1] loss: 1635.626
[13,     1] loss: 1621.679
[14,     1] loss: 1605.373
[15,     1] loss: 1591.074
[16,     1] loss: 1559.125
[17,     1] loss: 1536.366
[18,     1] loss: 1496.536
[19,     1] loss: 1479.997
[20,     1] loss: 1440.940
[21,     1] loss: 1396.094
[22,     1] loss: 1368.411
[23,     1] loss: 1413.551
[24,     1] loss: 1390.621
[25,     1] loss: 1358.830
[26,     1] loss: 1323.078
[27,     1] loss: 1347.118
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005567249835004212,
 'learning_rate_Hydroxylation-K': 0.0014894109992695007,
 'learning_rate_Hydroxylation-P': 0.005633209963191508,
 'log_base': 1.1470629267178396,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 611036016,
 'sample_weights': [3.4826914444618313, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.713542026365844,
 'weight_decay_Hydroxylation-K': 9.469447077894005,
 'weight_decay_Hydroxylation-P': 5.6997707163292075}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3945.674
[2,     1] loss: 3944.198
[3,     1] loss: 3969.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003449838869593185,
 'learning_rate_Hydroxylation-K': 0.0007450277903040104,
 'learning_rate_Hydroxylation-P': 0.003853553428098523,
 'log_base': 1.8480506000137307,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1724292039,
 'sample_weights': [12.167536271730969, 1.5210006267610585],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.48283527210995,
 'weight_decay_Hydroxylation-K': 1.2250297406494335,
 'weight_decay_Hydroxylation-P': 0.629340626004279}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1483.140
[2,     1] loss: 1481.446
[3,     1] loss: 1481.345
[4,     1] loss: 1479.669
[5,     1] loss: 1482.149
[6,     1] loss: 1477.635
[7,     1] loss: 1475.859
[8,     1] loss: 1464.563
[9,     1] loss: 1461.410
[10,     1] loss: 1438.493
[11,     1] loss: 1418.066
[12,     1] loss: 1380.826
[13,     1] loss: 1356.768
[14,     1] loss: 1304.255
[15,     1] loss: 1311.231
[16,     1] loss: 1238.631
[17,     1] loss: 1280.354
[18,     1] loss: 1214.343
[19,     1] loss: 1215.209
[20,     1] loss: 1150.747
[21,     1] loss: 1198.817
[22,     1] loss: 1192.570
[23,     1] loss: 1176.322
[24,     1] loss: 1177.794
[25,     1] loss: 1194.424
[26,     1] loss: 1105.345
[27,     1] loss: 1137.576
[28,     1] loss: 1172.292
[29,     1] loss: 1153.842
[30,     1] loss: 1154.438
[31,     1] loss: 1169.243
[32,     1] loss: 1073.133
[33,     1] loss: 1131.174
[34,     1] loss: 1100.658
[35,     1] loss: 1116.712
[36,     1] loss: 1061.074
[37,     1] loss: 1033.941
[38,     1] loss: 1097.344
[39,     1] loss: 1048.652
[40,     1] loss: 1077.184
[41,     1] loss: 1060.259
[42,     1] loss: 1106.161
[43,     1] loss: 1017.623
[44,     1] loss: 1019.732
[45,     1] loss: 987.483
[46,     1] loss: 960.268
[47,     1] loss: 971.745
[48,     1] loss: 967.598
[49,     1] loss: 954.909
[50,     1] loss: 915.610
[51,     1] loss: 917.891
[52,     1] loss: 935.442
[53,     1] loss: 894.326
[54,     1] loss: 1126.618
[55,     1] loss: 1169.222
[56,     1] loss: 923.548
[57,     1] loss: 1042.331
[58,     1] loss: 1040.889
[59,     1] loss: 931.987
[60,     1] loss: 999.539
[61,     1] loss: 987.873
[62,     1] loss: 908.631
[63,     1] loss: 915.669
[64,     1] loss: 928.132
[65,     1] loss: 834.709
[66,     1] loss: 818.334
[67,     1] loss: 801.776
Early stopping applied (best metric=0.7054827213287354)
Finished Training
Total time taken: 9.379012107849121
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1483.547
[2,     1] loss: 1481.102
[3,     1] loss: 1483.197
[4,     1] loss: 1482.567
[5,     1] loss: 1480.645
[6,     1] loss: 1478.611
[7,     1] loss: 1476.764
[8,     1] loss: 1478.869
[9,     1] loss: 1477.042
[10,     1] loss: 1474.148
[11,     1] loss: 1467.561
[12,     1] loss: 1452.899
[13,     1] loss: 1442.109
[14,     1] loss: 1407.253
[15,     1] loss: 1374.384
[16,     1] loss: 1360.516
[17,     1] loss: 1323.860
[18,     1] loss: 1293.385
[19,     1] loss: 1287.873
[20,     1] loss: 1261.237
[21,     1] loss: 1232.460
[22,     1] loss: 1202.619
[23,     1] loss: 1259.619
[24,     1] loss: 1171.042
[25,     1] loss: 1212.527
[26,     1] loss: 1192.475
[27,     1] loss: 1178.460
[28,     1] loss: 1184.514
[29,     1] loss: 1123.829
[30,     1] loss: 1158.861
[31,     1] loss: 1139.239
[32,     1] loss: 1057.034
[33,     1] loss: 1094.475
[34,     1] loss: 1063.041
[35,     1] loss: 1162.879
[36,     1] loss: 1068.745
[37,     1] loss: 990.581
[38,     1] loss: 1025.423
[39,     1] loss: 989.934
[40,     1] loss: 1025.260
[41,     1] loss: 934.352
[42,     1] loss: 904.740
[43,     1] loss: 900.043
[44,     1] loss: 949.703
[45,     1] loss: 902.819
[46,     1] loss: 847.622
[47,     1] loss: 834.078
[48,     1] loss: 881.879
[49,     1] loss: 851.195
[50,     1] loss: 787.430
[51,     1] loss: 901.205
[52,     1] loss: 856.137
[53,     1] loss: 803.325
[54,     1] loss: 820.023
[55,     1] loss: 924.594
[56,     1] loss: 793.256
[57,     1] loss: 870.903
Early stopping applied (best metric=0.8938497304916382)
Finished Training
Total time taken: 7.977007865905762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1489.461
[2,     1] loss: 1484.853
[3,     1] loss: 1479.559
[4,     1] loss: 1480.880
[5,     1] loss: 1480.617
[6,     1] loss: 1476.703
[7,     1] loss: 1478.320
[8,     1] loss: 1480.058
[9,     1] loss: 1477.865
[10,     1] loss: 1484.229
[11,     1] loss: 1474.224
[12,     1] loss: 1471.136
[13,     1] loss: 1467.214
[14,     1] loss: 1457.752
[15,     1] loss: 1447.429
[16,     1] loss: 1425.969
[17,     1] loss: 1392.479
[18,     1] loss: 1367.458
[19,     1] loss: 1332.618
[20,     1] loss: 1310.964
[21,     1] loss: 1273.099
[22,     1] loss: 1275.807
[23,     1] loss: 1255.283
[24,     1] loss: 1239.145
[25,     1] loss: 1257.717
[26,     1] loss: 1244.030
[27,     1] loss: 1251.886
[28,     1] loss: 1253.855
[29,     1] loss: 1228.056
[30,     1] loss: 1229.002
[31,     1] loss: 1222.522
[32,     1] loss: 1194.473
[33,     1] loss: 1202.489
[34,     1] loss: 1145.416
[35,     1] loss: 1220.518
[36,     1] loss: 1136.853
[37,     1] loss: 1129.787
[38,     1] loss: 1144.036
[39,     1] loss: 1146.691
[40,     1] loss: 1165.551
[41,     1] loss: 1115.659
[42,     1] loss: 1129.580
[43,     1] loss: 1065.038
[44,     1] loss: 1054.101
[45,     1] loss: 1131.123
[46,     1] loss: 1134.980
[47,     1] loss: 1042.586
[48,     1] loss: 1037.211
[49,     1] loss: 1028.856
[50,     1] loss: 1051.298
[51,     1] loss: 1092.457
[52,     1] loss: 1016.210
[53,     1] loss: 928.968
[54,     1] loss: 985.030
[55,     1] loss: 911.410
[56,     1] loss: 849.014
[57,     1] loss: 985.556
[58,     1] loss: 962.303
[59,     1] loss: 962.581
[60,     1] loss: 915.627
[61,     1] loss: 912.077
[62,     1] loss: 877.169
[63,     1] loss: 858.763
[64,     1] loss: 866.855
[65,     1] loss: 912.426
[66,     1] loss: 1059.508
[67,     1] loss: 1009.060
[68,     1] loss: 867.350
[69,     1] loss: 1079.566
[70,     1] loss: 928.434
[71,     1] loss: 921.530
[72,     1] loss: 957.231
[73,     1] loss: 872.858
[74,     1] loss: 943.860
[75,     1] loss: 838.274
[76,     1] loss: 839.670
[77,     1] loss: 912.567
[78,     1] loss: 773.575
[79,     1] loss: 976.121
[80,     1] loss: 797.289
[81,     1] loss: 928.374
[82,     1] loss: 859.908
[83,     1] loss: 952.715
[84,     1] loss: 849.167
Early stopping applied (best metric=0.734682559967041)
Finished Training
Total time taken: 11.519010543823242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.209
[2,     1] loss: 1486.311
[3,     1] loss: 1480.709
[4,     1] loss: 1478.451
[5,     1] loss: 1476.779
[6,     1] loss: 1468.426
[7,     1] loss: 1459.607
[8,     1] loss: 1438.078
[9,     1] loss: 1401.249
[10,     1] loss: 1360.830
[11,     1] loss: 1329.537
[12,     1] loss: 1291.976
[13,     1] loss: 1347.496
[14,     1] loss: 1314.574
[15,     1] loss: 1297.608
[16,     1] loss: 1224.332
[17,     1] loss: 1255.217
[18,     1] loss: 1238.969
[19,     1] loss: 1213.887
[20,     1] loss: 1220.080
[21,     1] loss: 1219.483
[22,     1] loss: 1196.980
[23,     1] loss: 1159.827
[24,     1] loss: 1218.356
[25,     1] loss: 1211.788
[26,     1] loss: 1176.000
[27,     1] loss: 1133.423
[28,     1] loss: 1133.592
[29,     1] loss: 1146.078
[30,     1] loss: 1159.057
[31,     1] loss: 1062.487
[32,     1] loss: 1097.164
[33,     1] loss: 1048.440
[34,     1] loss: 1099.434
[35,     1] loss: 1033.527
[36,     1] loss: 1070.180
[37,     1] loss: 1128.312
[38,     1] loss: 1018.110
[39,     1] loss: 1221.756
[40,     1] loss: 998.376
[41,     1] loss: 1113.179
[42,     1] loss: 946.513
[43,     1] loss: 1088.971
[44,     1] loss: 979.736
[45,     1] loss: 995.338
[46,     1] loss: 1030.120
[47,     1] loss: 1038.666
[48,     1] loss: 988.571
[49,     1] loss: 924.660
[50,     1] loss: 962.145
[51,     1] loss: 933.556
[52,     1] loss: 925.701
[53,     1] loss: 925.005
[54,     1] loss: 904.252
[55,     1] loss: 885.664
[56,     1] loss: 893.172
[57,     1] loss: 774.556
Early stopping applied (best metric=0.7878592610359192)
Finished Training
Total time taken: 8.51600456237793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1478.076
[2,     1] loss: 1486.766
[3,     1] loss: 1487.582
[4,     1] loss: 1488.226
[5,     1] loss: 1482.290
[6,     1] loss: 1481.154
[7,     1] loss: 1481.025
[8,     1] loss: 1481.635
[9,     1] loss: 1478.584
[10,     1] loss: 1479.073
[11,     1] loss: 1476.570
[12,     1] loss: 1472.979
[13,     1] loss: 1463.401
[14,     1] loss: 1448.034
[15,     1] loss: 1426.463
[16,     1] loss: 1414.677
[17,     1] loss: 1384.420
[18,     1] loss: 1361.526
[19,     1] loss: 1330.477
[20,     1] loss: 1282.467
[21,     1] loss: 1311.605
[22,     1] loss: 1292.400
[23,     1] loss: 1294.327
[24,     1] loss: 1256.959
[25,     1] loss: 1276.589
[26,     1] loss: 1268.165
[27,     1] loss: 1253.327
[28,     1] loss: 1240.260
[29,     1] loss: 1233.065
[30,     1] loss: 1209.820
[31,     1] loss: 1178.138
[32,     1] loss: 1186.787
[33,     1] loss: 1151.111
[34,     1] loss: 1189.132
[35,     1] loss: 1183.305
[36,     1] loss: 1089.019
[37,     1] loss: 1140.216
[38,     1] loss: 1120.935
[39,     1] loss: 1110.050
[40,     1] loss: 1159.475
[41,     1] loss: 1134.222
[42,     1] loss: 1115.677
[43,     1] loss: 1095.984
[44,     1] loss: 1109.996
[45,     1] loss: 1051.495
[46,     1] loss: 1082.270
[47,     1] loss: 1070.679
[48,     1] loss: 1035.366
[49,     1] loss: 1115.157
[50,     1] loss: 1011.512
[51,     1] loss: 1034.137
[52,     1] loss: 933.887
[53,     1] loss: 1066.024
[54,     1] loss: 976.562
[55,     1] loss: 1024.523
[56,     1] loss: 954.132
[57,     1] loss: 957.049
[58,     1] loss: 950.266
[59,     1] loss: 891.980
[60,     1] loss: 958.733
[61,     1] loss: 934.402
[62,     1] loss: 887.847
[63,     1] loss: 877.780
[64,     1] loss: 960.327
[65,     1] loss: 1123.497
[66,     1] loss: 1011.525
[67,     1] loss: 873.152
[68,     1] loss: 922.592
[69,     1] loss: 888.191
[70,     1] loss: 884.678
[71,     1] loss: 917.149
[72,     1] loss: 898.630
[73,     1] loss: 899.734
[74,     1] loss: 774.244
[75,     1] loss: 969.899
[76,     1] loss: 905.284
[77,     1] loss: 817.936
[78,     1] loss: 781.090
[79,     1] loss: 771.230
[80,     1] loss: 799.095
[81,     1] loss: 751.639
Early stopping applied (best metric=0.6334240436553955)
Finished Training
Total time taken: 10.845010757446289
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.286
[2,     1] loss: 1485.616
[3,     1] loss: 1480.666
[4,     1] loss: 1481.051
[5,     1] loss: 1483.511
[6,     1] loss: 1479.053
[7,     1] loss: 1478.709
[8,     1] loss: 1474.547
[9,     1] loss: 1470.775
[10,     1] loss: 1462.376
[11,     1] loss: 1454.458
[12,     1] loss: 1428.654
[13,     1] loss: 1413.371
[14,     1] loss: 1378.674
[15,     1] loss: 1340.777
[16,     1] loss: 1347.292
[17,     1] loss: 1308.809
[18,     1] loss: 1304.754
[19,     1] loss: 1293.369
[20,     1] loss: 1251.941
[21,     1] loss: 1233.032
[22,     1] loss: 1239.381
[23,     1] loss: 1249.047
[24,     1] loss: 1199.017
[25,     1] loss: 1202.660
[26,     1] loss: 1193.608
[27,     1] loss: 1160.479
[28,     1] loss: 1192.037
[29,     1] loss: 1131.004
[30,     1] loss: 1143.741
[31,     1] loss: 1155.798
[32,     1] loss: 1164.886
[33,     1] loss: 1098.301
[34,     1] loss: 1111.297
[35,     1] loss: 1081.078
[36,     1] loss: 1042.230
[37,     1] loss: 1003.842
[38,     1] loss: 1073.548
[39,     1] loss: 1060.425
[40,     1] loss: 975.145
[41,     1] loss: 926.397
[42,     1] loss: 909.048
[43,     1] loss: 1038.993
[44,     1] loss: 964.067
[45,     1] loss: 922.694
Early stopping applied (best metric=0.8160094618797302)
Finished Training
Total time taken: 6.876009464263916
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.856
[2,     1] loss: 1481.868
[3,     1] loss: 1479.591
[4,     1] loss: 1479.581
[5,     1] loss: 1476.484
[6,     1] loss: 1489.284
[7,     1] loss: 1473.319
[8,     1] loss: 1479.719
[9,     1] loss: 1483.055
[10,     1] loss: 1476.017
[11,     1] loss: 1476.445
[12,     1] loss: 1467.271
[13,     1] loss: 1459.920
[14,     1] loss: 1445.240
[15,     1] loss: 1418.570
[16,     1] loss: 1394.363
[17,     1] loss: 1344.122
[18,     1] loss: 1339.040
[19,     1] loss: 1254.181
[20,     1] loss: 1251.089
[21,     1] loss: 1288.366
[22,     1] loss: 1260.724
[23,     1] loss: 1224.877
[24,     1] loss: 1210.484
[25,     1] loss: 1170.445
[26,     1] loss: 1169.455
[27,     1] loss: 1113.976
[28,     1] loss: 1121.906
[29,     1] loss: 1125.393
[30,     1] loss: 1185.374
[31,     1] loss: 1178.996
[32,     1] loss: 1121.737
[33,     1] loss: 1109.205
[34,     1] loss: 1080.644
[35,     1] loss: 1056.407
[36,     1] loss: 1206.270
[37,     1] loss: 1110.668
[38,     1] loss: 1057.964
[39,     1] loss: 1030.787
[40,     1] loss: 1092.098
[41,     1] loss: 1054.105
[42,     1] loss: 1002.451
[43,     1] loss: 1010.402
[44,     1] loss: 1053.049
[45,     1] loss: 1022.571
[46,     1] loss: 1025.694
[47,     1] loss: 973.884
[48,     1] loss: 1016.962
[49,     1] loss: 922.097
[50,     1] loss: 1010.470
[51,     1] loss: 881.568
[52,     1] loss: 967.489
[53,     1] loss: 930.013
[54,     1] loss: 986.567
[55,     1] loss: 852.630
[56,     1] loss: 846.059
[57,     1] loss: 987.505
[58,     1] loss: 887.611
[59,     1] loss: 848.263
[60,     1] loss: 866.345
[61,     1] loss: 862.479
[62,     1] loss: 834.520
[63,     1] loss: 827.744
[64,     1] loss: 815.229
[65,     1] loss: 970.667
[66,     1] loss: 814.606
[67,     1] loss: 837.120
[68,     1] loss: 895.017
[69,     1] loss: 789.511
[70,     1] loss: 781.765
[71,     1] loss: 865.040
[72,     1] loss: 828.403
[73,     1] loss: 826.509
[74,     1] loss: 777.792
[75,     1] loss: 755.152
[76,     1] loss: 834.504
[77,     1] loss: 747.498
[78,     1] loss: 798.005
[79,     1] loss: 731.461
[80,     1] loss: 736.299
[81,     1] loss: 728.432
Early stopping applied (best metric=0.9376811981201172)
Finished Training
Total time taken: 10.916009187698364
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1479.621
[2,     1] loss: 1486.312
[3,     1] loss: 1476.887
[4,     1] loss: 1483.970
[5,     1] loss: 1482.451
[6,     1] loss: 1475.657
[7,     1] loss: 1475.749
[8,     1] loss: 1472.094
[9,     1] loss: 1472.221
[10,     1] loss: 1467.885
[11,     1] loss: 1453.509
[12,     1] loss: 1440.586
[13,     1] loss: 1414.766
[14,     1] loss: 1371.800
[15,     1] loss: 1331.764
[16,     1] loss: 1293.628
[17,     1] loss: 1297.338
[18,     1] loss: 1258.094
[19,     1] loss: 1256.243
[20,     1] loss: 1242.702
[21,     1] loss: 1213.374
[22,     1] loss: 1219.234
[23,     1] loss: 1148.537
[24,     1] loss: 1181.684
[25,     1] loss: 1139.663
[26,     1] loss: 1163.694
[27,     1] loss: 1127.687
[28,     1] loss: 1183.043
[29,     1] loss: 1113.120
[30,     1] loss: 1106.119
[31,     1] loss: 1077.066
[32,     1] loss: 1034.227
[33,     1] loss: 1050.058
[34,     1] loss: 1034.369
[35,     1] loss: 1103.477
[36,     1] loss: 1004.150
[37,     1] loss: 990.902
[38,     1] loss: 1023.414
[39,     1] loss: 993.990
[40,     1] loss: 999.481
[41,     1] loss: 973.359
[42,     1] loss: 1015.676
[43,     1] loss: 920.240
[44,     1] loss: 975.187
[45,     1] loss: 910.764
[46,     1] loss: 908.981
[47,     1] loss: 867.863
[48,     1] loss: 872.893
[49,     1] loss: 837.875
[50,     1] loss: 916.971
[51,     1] loss: 923.081
Early stopping applied (best metric=0.7711950540542603)
Finished Training
Total time taken: 6.654006481170654
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1478.890
[2,     1] loss: 1488.280
[3,     1] loss: 1479.414
[4,     1] loss: 1481.984
[5,     1] loss: 1475.021
[6,     1] loss: 1483.446
[7,     1] loss: 1487.845
[8,     1] loss: 1480.406
[9,     1] loss: 1476.504
[10,     1] loss: 1480.411
[11,     1] loss: 1476.647
[12,     1] loss: 1469.016
[13,     1] loss: 1468.565
[14,     1] loss: 1456.935
[15,     1] loss: 1442.614
[16,     1] loss: 1430.500
[17,     1] loss: 1407.720
[18,     1] loss: 1385.333
[19,     1] loss: 1355.900
[20,     1] loss: 1338.346
[21,     1] loss: 1316.519
[22,     1] loss: 1295.779
[23,     1] loss: 1252.321
[24,     1] loss: 1282.800
[25,     1] loss: 1246.535
[26,     1] loss: 1264.717
[27,     1] loss: 1250.152
[28,     1] loss: 1250.465
[29,     1] loss: 1226.491
[30,     1] loss: 1213.703
[31,     1] loss: 1187.315
[32,     1] loss: 1211.820
[33,     1] loss: 1146.923
[34,     1] loss: 1136.692
[35,     1] loss: 1140.900
[36,     1] loss: 1150.432
[37,     1] loss: 1104.429
[38,     1] loss: 1171.395
[39,     1] loss: 1146.880
[40,     1] loss: 1077.414
[41,     1] loss: 1143.380
[42,     1] loss: 1137.774
[43,     1] loss: 1098.727
[44,     1] loss: 1069.990
[45,     1] loss: 1065.858
[46,     1] loss: 1066.885
[47,     1] loss: 1017.307
[48,     1] loss: 1107.357
[49,     1] loss: 1075.414
[50,     1] loss: 1013.864
[51,     1] loss: 1017.360
[52,     1] loss: 1015.514
[53,     1] loss: 991.856
[54,     1] loss: 946.974
[55,     1] loss: 1111.995
[56,     1] loss: 1021.347
[57,     1] loss: 949.054
[58,     1] loss: 1002.489
[59,     1] loss: 914.099
[60,     1] loss: 998.962
[61,     1] loss: 952.183
[62,     1] loss: 957.427
[63,     1] loss: 936.922
[64,     1] loss: 963.019
[65,     1] loss: 981.056
[66,     1] loss: 914.877
Early stopping applied (best metric=0.767254114151001)
Finished Training
Total time taken: 9.634009599685669
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1485.983
[2,     1] loss: 1486.425
[3,     1] loss: 1478.854
[4,     1] loss: 1485.473
[5,     1] loss: 1481.709
[6,     1] loss: 1479.433
[7,     1] loss: 1479.375
[8,     1] loss: 1475.851
[9,     1] loss: 1471.319
[10,     1] loss: 1459.873
[11,     1] loss: 1454.496
[12,     1] loss: 1427.438
[13,     1] loss: 1409.218
[14,     1] loss: 1375.128
[15,     1] loss: 1354.114
[16,     1] loss: 1314.115
[17,     1] loss: 1292.677
[18,     1] loss: 1260.959
[19,     1] loss: 1298.214
[20,     1] loss: 1243.563
[21,     1] loss: 1238.323
[22,     1] loss: 1249.539
[23,     1] loss: 1207.422
[24,     1] loss: 1256.678
[25,     1] loss: 1211.845
[26,     1] loss: 1203.860
[27,     1] loss: 1196.877
[28,     1] loss: 1124.888
[29,     1] loss: 1131.792
[30,     1] loss: 1125.672
[31,     1] loss: 1122.957
[32,     1] loss: 1119.550
[33,     1] loss: 1101.265
[34,     1] loss: 1066.986
[35,     1] loss: 1090.773
[36,     1] loss: 1074.223
[37,     1] loss: 1069.421
[38,     1] loss: 1031.479
[39,     1] loss: 1060.934
[40,     1] loss: 1035.473
[41,     1] loss: 969.276
[42,     1] loss: 992.174
[43,     1] loss: 1003.423
[44,     1] loss: 998.891
[45,     1] loss: 952.819
[46,     1] loss: 905.533
[47,     1] loss: 891.831
[48,     1] loss: 901.031
[49,     1] loss: 1000.729
[50,     1] loss: 993.669
[51,     1] loss: 818.071
[52,     1] loss: 920.061
[53,     1] loss: 819.693
[54,     1] loss: 843.371
[55,     1] loss: 910.411
[56,     1] loss: 799.091
[57,     1] loss: 815.736
[58,     1] loss: 846.707
[59,     1] loss: 875.195
[60,     1] loss: 833.823
[61,     1] loss: 790.499
[62,     1] loss: 798.620
[63,     1] loss: 857.095
[64,     1] loss: 940.672
Early stopping applied (best metric=0.7660243511199951)
Finished Training
Total time taken: 8.551007986068726
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1477.243
[2,     1] loss: 1490.861
[3,     1] loss: 1480.297
[4,     1] loss: 1478.043
[5,     1] loss: 1483.688
[6,     1] loss: 1481.934
[7,     1] loss: 1476.443
[8,     1] loss: 1477.572
[9,     1] loss: 1480.295
[10,     1] loss: 1473.249
[11,     1] loss: 1472.932
[12,     1] loss: 1469.899
[13,     1] loss: 1466.856
[14,     1] loss: 1446.515
[15,     1] loss: 1427.712
[16,     1] loss: 1397.072
[17,     1] loss: 1355.148
[18,     1] loss: 1341.760
[19,     1] loss: 1314.217
[20,     1] loss: 1269.011
[21,     1] loss: 1224.211
[22,     1] loss: 1276.941
[23,     1] loss: 1183.953
[24,     1] loss: 1225.281
[25,     1] loss: 1246.702
[26,     1] loss: 1222.904
[27,     1] loss: 1152.590
[28,     1] loss: 1198.123
[29,     1] loss: 1165.487
[30,     1] loss: 1168.136
[31,     1] loss: 1131.374
[32,     1] loss: 1143.859
[33,     1] loss: 1141.800
[34,     1] loss: 1123.822
[35,     1] loss: 1096.054
[36,     1] loss: 1088.369
[37,     1] loss: 1082.060
[38,     1] loss: 1010.154
[39,     1] loss: 1028.009
[40,     1] loss: 1013.430
[41,     1] loss: 1026.146
[42,     1] loss: 1005.564
[43,     1] loss: 1073.499
[44,     1] loss: 1012.770
[45,     1] loss: 1018.473
[46,     1] loss: 1060.865
[47,     1] loss: 947.569
[48,     1] loss: 996.758
[49,     1] loss: 960.725
[50,     1] loss: 963.074
[51,     1] loss: 914.354
[52,     1] loss: 926.810
[53,     1] loss: 871.355
[54,     1] loss: 923.969
[55,     1] loss: 866.682
[56,     1] loss: 761.353
[57,     1] loss: 901.592
[58,     1] loss: 930.782
[59,     1] loss: 823.628
[60,     1] loss: 748.519
[61,     1] loss: 878.682
[62,     1] loss: 861.577
[63,     1] loss: 830.531
[64,     1] loss: 848.107
[65,     1] loss: 768.133
Early stopping applied (best metric=0.9209566116333008)
Finished Training
Total time taken: 9.560009717941284
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.323
[2,     1] loss: 1484.566
[3,     1] loss: 1478.343
[4,     1] loss: 1480.219
[5,     1] loss: 1478.148
[6,     1] loss: 1473.323
[7,     1] loss: 1465.021
[8,     1] loss: 1451.211
[9,     1] loss: 1442.970
[10,     1] loss: 1422.180
[11,     1] loss: 1380.814
[12,     1] loss: 1357.187
[13,     1] loss: 1310.040
[14,     1] loss: 1276.342
[15,     1] loss: 1254.045
[16,     1] loss: 1269.476
[17,     1] loss: 1231.544
[18,     1] loss: 1257.605
[19,     1] loss: 1203.771
[20,     1] loss: 1208.537
[21,     1] loss: 1198.725
[22,     1] loss: 1238.108
[23,     1] loss: 1175.288
[24,     1] loss: 1156.655
[25,     1] loss: 1155.341
[26,     1] loss: 1166.973
[27,     1] loss: 1157.937
[28,     1] loss: 1115.730
[29,     1] loss: 1103.552
[30,     1] loss: 1132.147
[31,     1] loss: 1057.404
[32,     1] loss: 1054.721
[33,     1] loss: 1023.343
[34,     1] loss: 1075.600
[35,     1] loss: 990.920
[36,     1] loss: 1065.596
[37,     1] loss: 1038.456
[38,     1] loss: 1065.190
[39,     1] loss: 1001.455
[40,     1] loss: 972.940
[41,     1] loss: 1027.909
[42,     1] loss: 984.876
[43,     1] loss: 1038.754
[44,     1] loss: 968.275
[45,     1] loss: 930.265
[46,     1] loss: 897.937
[47,     1] loss: 923.093
[48,     1] loss: 907.465
[49,     1] loss: 1039.048
[50,     1] loss: 1008.670
[51,     1] loss: 899.045
[52,     1] loss: 992.664
[53,     1] loss: 866.791
[54,     1] loss: 1010.953
[55,     1] loss: 900.301
[56,     1] loss: 919.162
[57,     1] loss: 827.207
[58,     1] loss: 834.647
[59,     1] loss: 857.324
[60,     1] loss: 795.526
[61,     1] loss: 781.743
[62,     1] loss: 743.126
[63,     1] loss: 723.299
[64,     1] loss: 763.664
Early stopping applied (best metric=0.7961240410804749)
Finished Training
Total time taken: 8.309006929397583
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.395
[2,     1] loss: 1480.459
[3,     1] loss: 1481.075
[4,     1] loss: 1479.020
[5,     1] loss: 1481.384
[6,     1] loss: 1478.330
[7,     1] loss: 1474.956
[8,     1] loss: 1467.009
[9,     1] loss: 1451.833
[10,     1] loss: 1426.101
[11,     1] loss: 1388.403
[12,     1] loss: 1383.621
[13,     1] loss: 1317.274
[14,     1] loss: 1310.490
[15,     1] loss: 1287.501
[16,     1] loss: 1263.197
[17,     1] loss: 1214.925
[18,     1] loss: 1217.048
[19,     1] loss: 1256.534
[20,     1] loss: 1209.835
[21,     1] loss: 1232.594
[22,     1] loss: 1194.076
[23,     1] loss: 1199.250
[24,     1] loss: 1170.637
[25,     1] loss: 1184.493
[26,     1] loss: 1179.379
[27,     1] loss: 1153.239
[28,     1] loss: 1125.977
[29,     1] loss: 1097.540
[30,     1] loss: 1101.862
[31,     1] loss: 1093.595
[32,     1] loss: 1102.680
[33,     1] loss: 1116.149
[34,     1] loss: 1091.245
[35,     1] loss: 1029.334
[36,     1] loss: 1041.104
[37,     1] loss: 1007.499
[38,     1] loss: 1037.996
[39,     1] loss: 1053.340
[40,     1] loss: 997.685
[41,     1] loss: 991.565
[42,     1] loss: 977.355
[43,     1] loss: 1018.397
[44,     1] loss: 961.379
[45,     1] loss: 963.324
[46,     1] loss: 943.652
[47,     1] loss: 872.564
[48,     1] loss: 876.786
[49,     1] loss: 898.449
[50,     1] loss: 873.043
[51,     1] loss: 953.283
[52,     1] loss: 1139.881
[53,     1] loss: 905.073
[54,     1] loss: 906.057
[55,     1] loss: 919.350
[56,     1] loss: 910.420
[57,     1] loss: 862.957
[58,     1] loss: 872.215
[59,     1] loss: 796.606
[60,     1] loss: 859.524
[61,     1] loss: 905.630
[62,     1] loss: 826.673
[63,     1] loss: 853.038
Early stopping applied (best metric=0.830186128616333)
Finished Training
Total time taken: 9.054011583328247
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1482.904
[2,     1] loss: 1478.469
[3,     1] loss: 1483.412
[4,     1] loss: 1479.350
[5,     1] loss: 1484.457
[6,     1] loss: 1477.462
[7,     1] loss: 1475.806
[8,     1] loss: 1479.261
[9,     1] loss: 1470.296
[10,     1] loss: 1467.821
[11,     1] loss: 1458.406
[12,     1] loss: 1450.318
[13,     1] loss: 1418.886
[14,     1] loss: 1402.465
[15,     1] loss: 1357.286
[16,     1] loss: 1346.548
[17,     1] loss: 1309.692
[18,     1] loss: 1272.252
[19,     1] loss: 1274.900
[20,     1] loss: 1289.769
[21,     1] loss: 1267.770
[22,     1] loss: 1253.362
[23,     1] loss: 1252.886
[24,     1] loss: 1259.287
[25,     1] loss: 1216.645
[26,     1] loss: 1236.939
[27,     1] loss: 1201.546
[28,     1] loss: 1178.326
[29,     1] loss: 1212.101
[30,     1] loss: 1174.178
[31,     1] loss: 1135.182
[32,     1] loss: 1127.945
[33,     1] loss: 1066.486
[34,     1] loss: 1081.328
[35,     1] loss: 1102.459
[36,     1] loss: 1065.080
[37,     1] loss: 1130.019
[38,     1] loss: 1034.089
[39,     1] loss: 974.738
[40,     1] loss: 1031.298
[41,     1] loss: 1043.786
[42,     1] loss: 1141.816
[43,     1] loss: 1047.904
[44,     1] loss: 1020.685
[45,     1] loss: 1053.951
[46,     1] loss: 980.625
[47,     1] loss: 1042.317
[48,     1] loss: 995.628
[49,     1] loss: 939.442
[50,     1] loss: 1006.057
[51,     1] loss: 897.587
[52,     1] loss: 1058.436
[53,     1] loss: 879.302
[54,     1] loss: 1026.847
[55,     1] loss: 1002.511
[56,     1] loss: 950.412
[57,     1] loss: 949.531
[58,     1] loss: 913.331
[59,     1] loss: 890.089
[60,     1] loss: 815.522
[61,     1] loss: 878.572
[62,     1] loss: 790.182
[63,     1] loss: 824.812
[64,     1] loss: 935.851
[65,     1] loss: 818.553
[66,     1] loss: 840.810
[67,     1] loss: 776.915
[68,     1] loss: 750.036
[69,     1] loss: 746.660
[70,     1] loss: 746.724
Early stopping applied (best metric=0.7509697675704956)
Finished Training
Total time taken: 9.318009614944458
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1482.032
[2,     1] loss: 1478.325
[3,     1] loss: 1481.676
[4,     1] loss: 1479.389
[5,     1] loss: 1486.158
[6,     1] loss: 1481.399
[7,     1] loss: 1476.899
[8,     1] loss: 1469.423
[9,     1] loss: 1464.767
[10,     1] loss: 1456.577
[11,     1] loss: 1437.959
[12,     1] loss: 1398.243
[13,     1] loss: 1366.110
[14,     1] loss: 1362.456
[15,     1] loss: 1322.422
[16,     1] loss: 1300.835
[17,     1] loss: 1263.525
[18,     1] loss: 1278.780
[19,     1] loss: 1267.070
[20,     1] loss: 1244.653
[21,     1] loss: 1256.305
[22,     1] loss: 1216.706
[23,     1] loss: 1237.396
[24,     1] loss: 1200.553
[25,     1] loss: 1139.325
[26,     1] loss: 1179.839
[27,     1] loss: 1160.598
[28,     1] loss: 1178.117
[29,     1] loss: 1112.721
[30,     1] loss: 1111.519
[31,     1] loss: 1115.669
[32,     1] loss: 1045.873
[33,     1] loss: 1033.033
[34,     1] loss: 1079.323
[35,     1] loss: 1037.343
[36,     1] loss: 941.947
[37,     1] loss: 1013.026
[38,     1] loss: 1040.240
[39,     1] loss: 897.309
[40,     1] loss: 1049.289
[41,     1] loss: 1180.047
[42,     1] loss: 1001.909
[43,     1] loss: 1067.858
[44,     1] loss: 971.260
[45,     1] loss: 1035.432
[46,     1] loss: 953.849
[47,     1] loss: 995.746
[48,     1] loss: 937.721
[49,     1] loss: 889.844
[50,     1] loss: 900.188
[51,     1] loss: 894.493
[52,     1] loss: 926.314
[53,     1] loss: 903.351
[54,     1] loss: 844.800
[55,     1] loss: 926.826
[56,     1] loss: 827.461
[57,     1] loss: 851.393
[58,     1] loss: 900.799
[59,     1] loss: 817.894
[60,     1] loss: 767.550
[61,     1] loss: 875.783
[62,     1] loss: 814.166
[63,     1] loss: 801.537
[64,     1] loss: 782.163
[65,     1] loss: 868.943
[66,     1] loss: 875.474
[67,     1] loss: 687.952
[68,     1] loss: 860.513
[69,     1] loss: 858.006
[70,     1] loss: 745.291
[71,     1] loss: 812.511
[72,     1] loss: 713.550
Early stopping applied (best metric=0.6229526996612549)
Finished Training
Total time taken: 9.362009048461914
{'Hydroxylation-K Validation Accuracy': 0.780466903073286, 'Hydroxylation-K Validation Sensitivity': 0.7511111111111111, 'Hydroxylation-K Validation Specificity': 0.787719298245614, 'Hydroxylation-K Validation Precision': 0.4880658165364048, 'Hydroxylation-K AUC ROC': 0.8120077972709552, 'Hydroxylation-K AUC PR': 0.5965615072529684, 'Hydroxylation-K MCC': 0.4717054379298319, 'Hydroxylation-K F1': 0.5865007638630827, 'Validation Loss (Hydroxylation-K)': 0.4202962100505829, 'Hydroxylation-P Validation Accuracy': 0.8161054261205015, 'Hydroxylation-P Validation Sensitivity': 0.781957671957672, 'Hydroxylation-P Validation Specificity': 0.8233777245747917, 'Hydroxylation-P Validation Precision': 0.5023760015344769, 'Hydroxylation-P AUC ROC': 0.8598498757409426, 'Hydroxylation-P AUC PR': 0.6316137771045218, 'Hydroxylation-P MCC': 0.5203565471811586, 'Hydroxylation-P F1': 0.6071669306721296, 'Validation Loss (Hydroxylation-P)': 0.36201390624046326, 'Validation Loss (total)': 0.7823101162910462, 'TimeToTrain': 9.09800903002421}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002008781120005473,
 'learning_rate_Hydroxylation-K': 0.0027328098083449666,
 'learning_rate_Hydroxylation-P': 0.004171348754011149,
 'log_base': 2.29392137493061,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 719075307,
 'sample_weights': [2.7203977248925613, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.06325748470453,
 'weight_decay_Hydroxylation-K': 7.197092696067131,
 'weight_decay_Hydroxylation-P': 5.453965798484285}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.471
[2,     1] loss: 1330.052
[3,     1] loss: 1333.517
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038909317928955056,
 'learning_rate_Hydroxylation-K': 0.0014039659648792803,
 'learning_rate_Hydroxylation-P': 0.0047370232877665995,
 'log_base': 2.0400834715348557,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 685032309,
 'sample_weights': [2.010740768145047, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.002470816499314,
 'weight_decay_Hydroxylation-K': 5.9698873547959534,
 'weight_decay_Hydroxylation-P': 0.7481203417288684}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.906
[2,     1] loss: 1404.302
[3,     1] loss: 1396.859
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004240002000463061,
 'learning_rate_Hydroxylation-K': 0.008371330753804446,
 'learning_rate_Hydroxylation-P': 0.0047503742110060095,
 'log_base': 2.1384692154403453,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1588984247,
 'sample_weights': [2.3414654499739744, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.832509540557716,
 'weight_decay_Hydroxylation-K': 3.864338600494715,
 'weight_decay_Hydroxylation-P': 5.049135783555065}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1370.484
[2,     1] loss: 1373.853
[3,     1] loss: 1369.020
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000500942294445255,
 'learning_rate_Hydroxylation-K': 0.004730297221900929,
 'learning_rate_Hydroxylation-P': 0.005815358211661344,
 'log_base': 1.4703841184016235,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2240150986,
 'sample_weights': [2.1963748915779493, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.685037238959959,
 'weight_decay_Hydroxylation-K': 6.356491416871581,
 'weight_decay_Hydroxylation-P': 5.029084498386608}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1828.261
[2,     1] loss: 1821.864
[3,     1] loss: 1823.865
[4,     1] loss: 1824.314
[5,     1] loss: 1819.819
[6,     1] loss: 1821.585
[7,     1] loss: 1818.949
[8,     1] loss: 1818.370
[9,     1] loss: 1820.457
[10,     1] loss: 1820.525
[11,     1] loss: 1817.800
[12,     1] loss: 1817.004
[13,     1] loss: 1817.722
[14,     1] loss: 1815.819
[15,     1] loss: 1816.317
[16,     1] loss: 1814.457
[17,     1] loss: 1813.237
[18,     1] loss: 1812.917
[19,     1] loss: 1810.896
[20,     1] loss: 1809.963
[21,     1] loss: 1804.754
[22,     1] loss: 1809.050
[23,     1] loss: 1804.531
[24,     1] loss: 1801.937
[25,     1] loss: 1794.712
[26,     1] loss: 1786.786
[27,     1] loss: 1784.377
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004409577497985346,
 'learning_rate_Hydroxylation-K': 0.004208519966479259,
 'learning_rate_Hydroxylation-P': 0.007956682386533109,
 'log_base': 2.6053483495860896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2754479102,
 'sample_weights': [4.330325917728259, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.540504634130404,
 'weight_decay_Hydroxylation-K': 9.370532469024127,
 'weight_decay_Hydroxylation-P': 0.13682227665500782}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.310
[2,     1] loss: 1275.732
[3,     1] loss: 1277.737
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005097036606640667,
 'learning_rate_Hydroxylation-K': 0.007421375607513693,
 'learning_rate_Hydroxylation-P': 0.001858041643462771,
 'log_base': 1.5679885629095007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1084433606,
 'sample_weights': [1.7434228742312778, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.784065633558184,
 'weight_decay_Hydroxylation-K': 0.9485486125850094,
 'weight_decay_Hydroxylation-P': 0.6151223816005649}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1685.599
[2,     1] loss: 1696.182
[3,     1] loss: 1689.016
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014277415870474412,
 'learning_rate_Hydroxylation-K': 0.009684577970266726,
 'learning_rate_Hydroxylation-P': 0.0010153146608489214,
 'log_base': 2.367190362130856,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2810024673,
 'sample_weights': [3.7115758074322778, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.793383891669329,
 'weight_decay_Hydroxylation-K': 8.957102949860023,
 'weight_decay_Hydroxylation-P': 8.520030281026875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.970
[2,     1] loss: 1314.802
[3,     1] loss: 1319.461
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00010967866619724475,
 'learning_rate_Hydroxylation-K': 0.009866420557949425,
 'learning_rate_Hydroxylation-P': 0.008690623037273375,
 'log_base': 2.4001555877770966,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4230315655,
 'sample_weights': [1.937374818338525, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.086912785320433,
 'weight_decay_Hydroxylation-K': 7.200420054830806,
 'weight_decay_Hydroxylation-P': 5.590348829694903}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1312.018
[2,     1] loss: 1312.304
[3,     1] loss: 1315.984
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022621388060906013,
 'learning_rate_Hydroxylation-K': 0.007795997589783906,
 'learning_rate_Hydroxylation-P': 0.003316897586938295,
 'log_base': 1.0981958254923017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2313850767,
 'sample_weights': [1.9067723009534108, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.504761259644138,
 'weight_decay_Hydroxylation-K': 8.844079044246582,
 'weight_decay_Hydroxylation-P': 6.239895566056841}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5829.802
[2,     1] loss: 5777.108
[3,     1] loss: 5772.866
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006414761251711119,
 'learning_rate_Hydroxylation-K': 0.005229529488400827,
 'learning_rate_Hydroxylation-P': 0.007848562260857374,
 'log_base': 1.6177899520267651,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 255651076,
 'sample_weights': [17.822854375273444, 2.227942622899244],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.333504282114442,
 'weight_decay_Hydroxylation-K': 2.8557638409912984,
 'weight_decay_Hydroxylation-P': 0.4275289321265301}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1643.073
[2,     1] loss: 1636.116
[3,     1] loss: 1646.564
[4,     1] loss: 1647.551
[5,     1] loss: 1636.391
[6,     1] loss: 1634.848
[7,     1] loss: 1627.711
[8,     1] loss: 1619.767
[9,     1] loss: 1621.279
[10,     1] loss: 1589.448
[11,     1] loss: 1572.468
[12,     1] loss: 1521.963
[13,     1] loss: 1474.456
[14,     1] loss: 1465.082
[15,     1] loss: 1432.189
[16,     1] loss: 1412.053
[17,     1] loss: 1338.016
[18,     1] loss: 1355.997
[19,     1] loss: 1351.873
[20,     1] loss: 1429.464
[21,     1] loss: 1375.069
[22,     1] loss: 1359.309
[23,     1] loss: 1361.615
[24,     1] loss: 1319.723
[25,     1] loss: 1310.717
[26,     1] loss: 1326.814
[27,     1] loss: 1310.841
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032314572545484596,
 'learning_rate_Hydroxylation-K': 0.0064506994047990925,
 'learning_rate_Hydroxylation-P': 0.0005379705038099508,
 'log_base': 2.6431025731491546,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2987990786,
 'sample_weights': [3.4703357364537784, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.152646432515859,
 'weight_decay_Hydroxylation-K': 0.5345200837757635,
 'weight_decay_Hydroxylation-P': 4.4861957678151585}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.623
[2,     1] loss: 1269.380
[3,     1] loss: 1269.674
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002596117804703268,
 'learning_rate_Hydroxylation-K': 0.007595672081911218,
 'learning_rate_Hydroxylation-P': 0.005355201624913713,
 'log_base': 2.142016506513882,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2878090593,
 'sample_weights': [1.717616370752064, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.680950422942784,
 'weight_decay_Hydroxylation-K': 1.0365278346280449,
 'weight_decay_Hydroxylation-P': 3.571682112584465}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1386.944
[2,     1] loss: 1371.804
[3,     1] loss: 1370.281
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003059516944207936,
 'learning_rate_Hydroxylation-K': 0.0026845578247840906,
 'learning_rate_Hydroxylation-P': 0.004602340020237394,
 'log_base': 1.0715476981455179,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 851868112,
 'sample_weights': [2.191595977749458, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.665791003215306,
 'weight_decay_Hydroxylation-K': 9.52088681271738,
 'weight_decay_Hydroxylation-P': 8.262344094227139}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7846.728
[2,     1] loss: 7830.196
[3,     1] loss: 7865.967
[4,     1] loss: 7787.871
[5,     1] loss: 7842.270
[6,     1] loss: 7833.313
[7,     1] loss: 7853.309
[8,     1] loss: 7793.130
[9,     1] loss: 7827.585
[10,     1] loss: 7763.809
[11,     1] loss: 7726.874
[12,     1] loss: 7647.580
[13,     1] loss: 7599.250
[14,     1] loss: 7485.683
[15,     1] loss: 7215.677
[16,     1] loss: 7132.858
[17,     1] loss: 6815.888
[18,     1] loss: 7107.664
[19,     1] loss: 6929.492
[20,     1] loss: 6866.460
[21,     1] loss: 6722.479
[22,     1] loss: 6766.157
[23,     1] loss: 6731.653
[24,     1] loss: 6294.914
[25,     1] loss: 6213.114
[26,     1] loss: 6253.047
[27,     1] loss: 5658.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004384490296033983,
 'learning_rate_Hydroxylation-K': 0.003165716837920786,
 'learning_rate_Hydroxylation-P': 0.003915749414715407,
 'log_base': 1.0322823902539837,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4000598990,
 'sample_weights': [24.15839797483593, 3.0199160816671355],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.704355842703503,
 'weight_decay_Hydroxylation-K': 4.353845246103811,
 'weight_decay_Hydroxylation-P': 1.257754427069813}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17082.119
Exploding loss, terminate run (best metric=1.0966706275939941)
Finished Training
Total time taken: 0.20099997520446777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16967.637
Exploding loss, terminate run (best metric=1.1018471717834473)
Finished Training
Total time taken: 0.24099993705749512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17167.703
Exploding loss, terminate run (best metric=1.0912306308746338)
Finished Training
Total time taken: 0.21700072288513184
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17159.891
Exploding loss, terminate run (best metric=1.075595736503601)
Finished Training
Total time taken: 0.23199939727783203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16990.898
Exploding loss, terminate run (best metric=1.0868639945983887)
Finished Training
Total time taken: 0.2279980182647705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17055.816
Exploding loss, terminate run (best metric=1.0965855121612549)
Finished Training
Total time taken: 0.2220008373260498
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17120.359
Exploding loss, terminate run (best metric=1.0924999713897705)
Finished Training
Total time taken: 0.2240009307861328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17063.074
Exploding loss, terminate run (best metric=1.093005657196045)
Finished Training
Total time taken: 0.20600295066833496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17065.602
Exploding loss, terminate run (best metric=1.0717674493789673)
Finished Training
Total time taken: 0.20499920845031738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17101.086
Exploding loss, terminate run (best metric=1.073652982711792)
Finished Training
Total time taken: 0.20099854469299316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17047.020
Exploding loss, terminate run (best metric=1.0982890129089355)
Finished Training
Total time taken: 0.24000120162963867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17121.705
Exploding loss, terminate run (best metric=1.103078007698059)
Finished Training
Total time taken: 0.1809990406036377
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17034.180
Exploding loss, terminate run (best metric=1.097233772277832)
Finished Training
Total time taken: 0.18699979782104492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17006.293
Exploding loss, terminate run (best metric=1.0719618797302246)
Finished Training
Total time taken: 0.18400025367736816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17232.695
Exploding loss, terminate run (best metric=1.072898030281067)
Finished Training
Total time taken: 0.17600297927856445
{'Hydroxylation-K Validation Accuracy': 0.35939716312056735, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.26666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.7276218323586745, 'Hydroxylation-K AUC PR': 0.44621917155735913, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.24507389162561577, 'Validation Loss (Hydroxylation-K)': 0.5584389964739481, 'Hydroxylation-P Validation Accuracy': 0.34919608141718694, 'Hydroxylation-P Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-P Validation Specificity': 0.26666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6108492724570833, 'Hydroxylation-P AUC PR': 0.3225117269307488, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.22042192266968585, 'Validation Loss (Hydroxylation-P)': 0.5297730247179667, 'Validation Loss (total)': 1.0882120291392008, 'TimeToTrain': 0.20966691970825196}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006113837807950151,
 'learning_rate_Hydroxylation-K': 0.0033957204423141515,
 'learning_rate_Hydroxylation-P': 0.004421575339937058,
 'log_base': 1.9242180597050853,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3431974854,
 'sample_weights': [52.58301891695736, 6.559220086753953],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.574145719337853,
 'weight_decay_Hydroxylation-K': 5.095458086377793,
 'weight_decay_Hydroxylation-P': 0.7752634619920615}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1447.142
[2,     1] loss: 1445.013
[3,     1] loss: 1446.078
[4,     1] loss: 1444.964
[5,     1] loss: 1447.966
[6,     1] loss: 1444.190
[7,     1] loss: 1446.903
[8,     1] loss: 1444.411
[9,     1] loss: 1442.221
[10,     1] loss: 1440.767
[11,     1] loss: 1437.414
[12,     1] loss: 1438.392
[13,     1] loss: 1426.464
[14,     1] loss: 1406.545
[15,     1] loss: 1383.890
[16,     1] loss: 1347.197
[17,     1] loss: 1347.301
[18,     1] loss: 1302.132
[19,     1] loss: 1241.288
[20,     1] loss: 1243.566
[21,     1] loss: 1232.027
[22,     1] loss: 1243.696
[23,     1] loss: 1174.764
[24,     1] loss: 1231.505
[25,     1] loss: 1224.889
[26,     1] loss: 1184.132
[27,     1] loss: 1191.669
[28,     1] loss: 1157.761
[29,     1] loss: 1207.884
[30,     1] loss: 1210.055
[31,     1] loss: 1165.749
[32,     1] loss: 1178.121
[33,     1] loss: 1120.614
[34,     1] loss: 1116.262
[35,     1] loss: 1071.900
[36,     1] loss: 1104.027
[37,     1] loss: 1074.430
[38,     1] loss: 1032.680
[39,     1] loss: 984.818
[40,     1] loss: 1023.826
[41,     1] loss: 1069.157
[42,     1] loss: 1228.968
[43,     1] loss: 1216.571
[44,     1] loss: 1061.817
[45,     1] loss: 1106.553
[46,     1] loss: 1141.184
[47,     1] loss: 1089.021
[48,     1] loss: 1123.471
[49,     1] loss: 1045.710
[50,     1] loss: 1012.446
[51,     1] loss: 1069.333
[52,     1] loss: 995.403
[53,     1] loss: 1022.897
[54,     1] loss: 982.485
[55,     1] loss: 946.716
[56,     1] loss: 981.905
[57,     1] loss: 1002.216
[58,     1] loss: 1007.207
[59,     1] loss: 930.345
Early stopping applied (best metric=0.7836242914199829)
Finished Training
Total time taken: 7.686007499694824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1446.371
[2,     1] loss: 1450.298
[3,     1] loss: 1446.003
[4,     1] loss: 1443.423
[5,     1] loss: 1443.832
[6,     1] loss: 1437.877
[7,     1] loss: 1435.689
[8,     1] loss: 1425.639
[9,     1] loss: 1408.320
[10,     1] loss: 1371.271
[11,     1] loss: 1370.498
[12,     1] loss: 1345.619
[13,     1] loss: 1250.945
[14,     1] loss: 1282.132
[15,     1] loss: 1225.722
[16,     1] loss: 1277.619
[17,     1] loss: 1193.050
[18,     1] loss: 1248.801
[19,     1] loss: 1195.948
[20,     1] loss: 1208.783
[21,     1] loss: 1168.909
[22,     1] loss: 1225.616
[23,     1] loss: 1164.045
[24,     1] loss: 1143.161
[25,     1] loss: 1143.556
[26,     1] loss: 1109.852
[27,     1] loss: 1128.298
[28,     1] loss: 1132.742
[29,     1] loss: 1134.591
[30,     1] loss: 1088.776
[31,     1] loss: 1078.510
[32,     1] loss: 1081.556
[33,     1] loss: 1046.175
[34,     1] loss: 1047.118
[35,     1] loss: 1314.883
[36,     1] loss: 1083.989
[37,     1] loss: 1263.588
[38,     1] loss: 1151.878
[39,     1] loss: 1165.612
[40,     1] loss: 1193.049
[41,     1] loss: 1188.285
[42,     1] loss: 1096.393
[43,     1] loss: 1092.520
[44,     1] loss: 1090.498
[45,     1] loss: 1107.215
[46,     1] loss: 1087.436
[47,     1] loss: 1017.749
[48,     1] loss: 1092.506
[49,     1] loss: 1027.080
[50,     1] loss: 1067.346
[51,     1] loss: 1004.290
[52,     1] loss: 1050.700
[53,     1] loss: 1018.872
[54,     1] loss: 1043.003
[55,     1] loss: 981.366
[56,     1] loss: 1017.143
[57,     1] loss: 881.856
[58,     1] loss: 902.839
[59,     1] loss: 919.523
[60,     1] loss: 1056.060
[61,     1] loss: 1073.732
[62,     1] loss: 898.537
[63,     1] loss: 996.114
[64,     1] loss: 935.524
[65,     1] loss: 1094.757
[66,     1] loss: 981.807
[67,     1] loss: 920.360
[68,     1] loss: 979.630
[69,     1] loss: 861.048
[70,     1] loss: 957.839
[71,     1] loss: 950.739
[72,     1] loss: 863.413
[73,     1] loss: 823.241
[74,     1] loss: 873.509
[75,     1] loss: 926.983
[76,     1] loss: 1056.406
[77,     1] loss: 1411.724
[78,     1] loss: 945.169
[79,     1] loss: 1162.255
[80,     1] loss: 1100.495
[81,     1] loss: 1105.774
[82,     1] loss: 1123.369
[83,     1] loss: 991.808
[84,     1] loss: 1127.955
[85,     1] loss: 1016.441
[86,     1] loss: 1018.407
[87,     1] loss: 998.249
[88,     1] loss: 948.013
Early stopping applied (best metric=0.6611740589141846)
Finished Training
Total time taken: 11.923011541366577
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1450.749
[2,     1] loss: 1455.706
[3,     1] loss: 1443.497
[4,     1] loss: 1445.625
[5,     1] loss: 1444.820
[6,     1] loss: 1443.902
[7,     1] loss: 1444.285
[8,     1] loss: 1442.739
[9,     1] loss: 1441.060
[10,     1] loss: 1436.530
[11,     1] loss: 1430.195
[12,     1] loss: 1405.594
[13,     1] loss: 1378.412
[14,     1] loss: 1347.158
[15,     1] loss: 1309.100
[16,     1] loss: 1269.908
[17,     1] loss: 1228.067
[18,     1] loss: 1217.899
[19,     1] loss: 1172.568
[20,     1] loss: 1210.547
[21,     1] loss: 1102.618
[22,     1] loss: 1213.528
[23,     1] loss: 1291.996
[24,     1] loss: 1146.495
[25,     1] loss: 1216.167
[26,     1] loss: 1204.235
[27,     1] loss: 1160.900
[28,     1] loss: 1154.580
[29,     1] loss: 1187.758
[30,     1] loss: 1158.271
[31,     1] loss: 1152.548
[32,     1] loss: 1120.707
[33,     1] loss: 1069.373
[34,     1] loss: 1105.092
[35,     1] loss: 1054.300
[36,     1] loss: 1085.976
[37,     1] loss: 1045.295
[38,     1] loss: 1047.583
[39,     1] loss: 995.458
[40,     1] loss: 1015.737
[41,     1] loss: 1007.731
[42,     1] loss: 967.537
[43,     1] loss: 941.156
[44,     1] loss: 971.047
[45,     1] loss: 1130.037
[46,     1] loss: 1234.644
[47,     1] loss: 972.318
[48,     1] loss: 1149.704
[49,     1] loss: 996.701
[50,     1] loss: 1059.239
[51,     1] loss: 1102.080
[52,     1] loss: 987.924
[53,     1] loss: 1067.094
[54,     1] loss: 950.221
[55,     1] loss: 1027.004
[56,     1] loss: 904.057
[57,     1] loss: 1104.395
[58,     1] loss: 973.990
[59,     1] loss: 897.649
[60,     1] loss: 975.840
[61,     1] loss: 864.096
[62,     1] loss: 906.133
[63,     1] loss: 978.318
[64,     1] loss: 855.971
[65,     1] loss: 783.057
[66,     1] loss: 783.409
[67,     1] loss: 796.311
[68,     1] loss: 965.589
[69,     1] loss: 1371.999
[70,     1] loss: 1226.440
[71,     1] loss: 1052.227
[72,     1] loss: 1054.961
[73,     1] loss: 1171.439
[74,     1] loss: 1182.250
[75,     1] loss: 1103.770
[76,     1] loss: 1117.596
[77,     1] loss: 1120.556
[78,     1] loss: 1059.637
[79,     1] loss: 1054.732
[80,     1] loss: 1067.757
[81,     1] loss: 1048.076
[82,     1] loss: 1056.649
[83,     1] loss: 1028.339
[84,     1] loss: 1054.585
[85,     1] loss: 1027.708
Early stopping applied (best metric=0.8358688354492188)
Finished Training
Total time taken: 11.618014335632324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1453.325
[2,     1] loss: 1448.731
[3,     1] loss: 1443.565
[4,     1] loss: 1454.312
[5,     1] loss: 1446.817
[6,     1] loss: 1442.012
[7,     1] loss: 1444.382
[8,     1] loss: 1446.206
[9,     1] loss: 1445.431
[10,     1] loss: 1444.613
[11,     1] loss: 1445.673
[12,     1] loss: 1439.581
[13,     1] loss: 1439.035
[14,     1] loss: 1432.570
[15,     1] loss: 1420.065
[16,     1] loss: 1409.122
[17,     1] loss: 1382.724
[18,     1] loss: 1364.672
[19,     1] loss: 1331.492
[20,     1] loss: 1313.486
[21,     1] loss: 1275.200
[22,     1] loss: 1283.162
[23,     1] loss: 1275.927
[24,     1] loss: 1259.531
[25,     1] loss: 1238.688
[26,     1] loss: 1188.067
[27,     1] loss: 1214.494
[28,     1] loss: 1254.250
[29,     1] loss: 1165.962
[30,     1] loss: 1169.824
[31,     1] loss: 1136.159
[32,     1] loss: 1109.698
[33,     1] loss: 1135.086
[34,     1] loss: 1241.610
[35,     1] loss: 1208.870
[36,     1] loss: 1178.500
[37,     1] loss: 1165.771
[38,     1] loss: 1172.897
[39,     1] loss: 1175.332
[40,     1] loss: 1137.664
[41,     1] loss: 1096.116
[42,     1] loss: 1149.219
[43,     1] loss: 1039.873
[44,     1] loss: 1148.732
[45,     1] loss: 993.472
[46,     1] loss: 1075.136
[47,     1] loss: 1013.230
[48,     1] loss: 1127.586
[49,     1] loss: 989.614
[50,     1] loss: 1032.383
[51,     1] loss: 1008.330
[52,     1] loss: 1073.805
[53,     1] loss: 1060.679
[54,     1] loss: 976.357
[55,     1] loss: 1024.889
[56,     1] loss: 911.520
[57,     1] loss: 982.637
[58,     1] loss: 1017.132
[59,     1] loss: 906.059
[60,     1] loss: 1073.297
[61,     1] loss: 1009.339
[62,     1] loss: 920.884
[63,     1] loss: 991.828
[64,     1] loss: 1088.207
[65,     1] loss: 927.067
[66,     1] loss: 934.699
[67,     1] loss: 1176.365
[68,     1] loss: 990.420
[69,     1] loss: 944.924
[70,     1] loss: 1060.878
[71,     1] loss: 967.292
[72,     1] loss: 984.703
[73,     1] loss: 949.016
[74,     1] loss: 960.401
[75,     1] loss: 939.179
[76,     1] loss: 896.469
[77,     1] loss: 861.996
[78,     1] loss: 874.316
[79,     1] loss: 948.135
[80,     1] loss: 1078.151
[81,     1] loss: 1168.774
Early stopping applied (best metric=0.7222132086753845)
Finished Training
Total time taken: 11.068008661270142
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1450.402
[2,     1] loss: 1453.090
[3,     1] loss: 1444.693
[4,     1] loss: 1452.597
[5,     1] loss: 1450.025
[6,     1] loss: 1448.199
[7,     1] loss: 1449.177
[8,     1] loss: 1446.911
[9,     1] loss: 1445.228
[10,     1] loss: 1448.550
[11,     1] loss: 1444.565
[12,     1] loss: 1442.905
[13,     1] loss: 1439.173
[14,     1] loss: 1433.346
[15,     1] loss: 1423.682
[16,     1] loss: 1398.613
[17,     1] loss: 1376.116
[18,     1] loss: 1369.689
[19,     1] loss: 1312.947
[20,     1] loss: 1295.799
[21,     1] loss: 1252.197
[22,     1] loss: 1236.118
[23,     1] loss: 1214.652
[24,     1] loss: 1286.090
[25,     1] loss: 1314.001
[26,     1] loss: 1176.148
[27,     1] loss: 1277.355
[28,     1] loss: 1257.834
[29,     1] loss: 1227.150
[30,     1] loss: 1256.599
[31,     1] loss: 1230.649
[32,     1] loss: 1194.448
[33,     1] loss: 1192.806
[34,     1] loss: 1179.543
[35,     1] loss: 1151.143
[36,     1] loss: 1141.580
[37,     1] loss: 1114.640
[38,     1] loss: 1158.813
[39,     1] loss: 1014.906
[40,     1] loss: 1258.864
[41,     1] loss: 1060.722
[42,     1] loss: 1156.575
[43,     1] loss: 1072.106
[44,     1] loss: 1105.556
[45,     1] loss: 1145.498
[46,     1] loss: 1033.305
[47,     1] loss: 1083.835
[48,     1] loss: 1019.666
[49,     1] loss: 1043.060
[50,     1] loss: 1028.963
[51,     1] loss: 1065.842
[52,     1] loss: 976.671
[53,     1] loss: 962.915
[54,     1] loss: 960.271
[55,     1] loss: 976.390
[56,     1] loss: 913.112
[57,     1] loss: 871.781
[58,     1] loss: 939.992
[59,     1] loss: 1626.233
[60,     1] loss: 1113.649
[61,     1] loss: 1066.081
[62,     1] loss: 1103.173
[63,     1] loss: 1129.892
[64,     1] loss: 1090.615
[65,     1] loss: 1084.983
[66,     1] loss: 1050.649
[67,     1] loss: 1048.726
Early stopping applied (best metric=0.6508548259735107)
Finished Training
Total time taken: 9.063007593154907
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1447.562
[2,     1] loss: 1460.325
[3,     1] loss: 1446.260
[4,     1] loss: 1444.737
[5,     1] loss: 1448.270
[6,     1] loss: 1447.146
[7,     1] loss: 1444.795
[8,     1] loss: 1445.605
[9,     1] loss: 1442.621
[10,     1] loss: 1446.615
[11,     1] loss: 1446.196
[12,     1] loss: 1444.090
[13,     1] loss: 1442.083
[14,     1] loss: 1445.791
[15,     1] loss: 1441.554
[16,     1] loss: 1443.771
[17,     1] loss: 1443.506
[18,     1] loss: 1440.429
[19,     1] loss: 1432.116
[20,     1] loss: 1430.315
[21,     1] loss: 1422.606
[22,     1] loss: 1413.914
[23,     1] loss: 1396.961
[24,     1] loss: 1376.286
[25,     1] loss: 1358.540
[26,     1] loss: 1331.812
[27,     1] loss: 1293.607
[28,     1] loss: 1310.080
[29,     1] loss: 1255.599
[30,     1] loss: 1269.026
[31,     1] loss: 1235.383
[32,     1] loss: 1250.446
[33,     1] loss: 1260.429
[34,     1] loss: 1234.080
[35,     1] loss: 1250.927
[36,     1] loss: 1197.549
[37,     1] loss: 1236.134
[38,     1] loss: 1185.413
[39,     1] loss: 1241.657
[40,     1] loss: 1207.523
[41,     1] loss: 1191.703
[42,     1] loss: 1189.456
[43,     1] loss: 1163.718
[44,     1] loss: 1137.628
[45,     1] loss: 1165.169
[46,     1] loss: 1187.830
[47,     1] loss: 1153.229
[48,     1] loss: 1126.242
[49,     1] loss: 1144.232
[50,     1] loss: 1064.188
[51,     1] loss: 1070.384
[52,     1] loss: 1056.957
[53,     1] loss: 1251.571
[54,     1] loss: 1147.159
[55,     1] loss: 1040.794
[56,     1] loss: 1058.362
[57,     1] loss: 1043.815
[58,     1] loss: 1092.869
[59,     1] loss: 1001.794
[60,     1] loss: 1054.619
[61,     1] loss: 1035.184
[62,     1] loss: 996.847
[63,     1] loss: 988.160
[64,     1] loss: 1014.166
[65,     1] loss: 1317.557
[66,     1] loss: 982.724
[67,     1] loss: 1292.371
[68,     1] loss: 1112.301
[69,     1] loss: 1233.251
[70,     1] loss: 1112.008
[71,     1] loss: 1136.572
[72,     1] loss: 1184.004
[73,     1] loss: 1099.520
[74,     1] loss: 1130.385
[75,     1] loss: 1123.831
[76,     1] loss: 1111.105
[77,     1] loss: 1075.895
[78,     1] loss: 1042.381
Early stopping applied (best metric=0.9079512357711792)
Finished Training
Total time taken: 10.217010021209717
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1450.035
[2,     1] loss: 1444.741
[3,     1] loss: 1441.182
[4,     1] loss: 1452.251
[5,     1] loss: 1444.431
[6,     1] loss: 1443.717
[7,     1] loss: 1442.891
[8,     1] loss: 1436.541
[9,     1] loss: 1425.089
[10,     1] loss: 1408.615
[11,     1] loss: 1384.779
[12,     1] loss: 1342.243
[13,     1] loss: 1314.694
[14,     1] loss: 1255.501
[15,     1] loss: 1203.166
[16,     1] loss: 1218.165
[17,     1] loss: 1217.527
[18,     1] loss: 1131.601
[19,     1] loss: 1231.429
[20,     1] loss: 1237.713
[21,     1] loss: 1139.563
[22,     1] loss: 1158.292
[23,     1] loss: 1133.948
[24,     1] loss: 1150.820
[25,     1] loss: 1113.928
[26,     1] loss: 1148.678
[27,     1] loss: 1064.061
[28,     1] loss: 1086.894
[29,     1] loss: 1042.936
[30,     1] loss: 1121.811
[31,     1] loss: 1080.365
[32,     1] loss: 1053.550
[33,     1] loss: 996.054
[34,     1] loss: 993.654
[35,     1] loss: 1005.626
[36,     1] loss: 949.098
[37,     1] loss: 916.081
[38,     1] loss: 1172.524
[39,     1] loss: 1647.672
[40,     1] loss: 1030.750
[41,     1] loss: 1155.647
[42,     1] loss: 1213.799
[43,     1] loss: 1214.015
[44,     1] loss: 1174.650
[45,     1] loss: 1159.141
[46,     1] loss: 1175.004
[47,     1] loss: 1095.450
[48,     1] loss: 1104.749
[49,     1] loss: 1076.877
[50,     1] loss: 1037.597
[51,     1] loss: 1049.088
[52,     1] loss: 1051.335
[53,     1] loss: 965.951
[54,     1] loss: 1001.785
[55,     1] loss: 1063.287
[56,     1] loss: 1102.294
Early stopping applied (best metric=0.8874180912971497)
Finished Training
Total time taken: 8.17800784111023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1442.170
[2,     1] loss: 1452.975
[3,     1] loss: 1445.275
[4,     1] loss: 1442.574
[5,     1] loss: 1428.691
[6,     1] loss: 1400.720
[7,     1] loss: 1366.852
[8,     1] loss: 1352.688
[9,     1] loss: 1306.953
[10,     1] loss: 1237.727
[11,     1] loss: 1270.953
[12,     1] loss: 1227.242
[13,     1] loss: 1251.376
[14,     1] loss: 1201.583
[15,     1] loss: 1232.944
[16,     1] loss: 1204.143
[17,     1] loss: 1216.374
[18,     1] loss: 1203.438
[19,     1] loss: 1125.087
[20,     1] loss: 1109.702
[21,     1] loss: 1118.687
[22,     1] loss: 1085.632
[23,     1] loss: 1138.029
[24,     1] loss: 1060.860
[25,     1] loss: 1076.327
[26,     1] loss: 1063.610
[27,     1] loss: 1005.723
[28,     1] loss: 1057.141
[29,     1] loss: 1139.293
[30,     1] loss: 1155.743
[31,     1] loss: 1051.132
[32,     1] loss: 1104.510
[33,     1] loss: 1124.722
[34,     1] loss: 1074.768
[35,     1] loss: 1112.036
[36,     1] loss: 1013.391
[37,     1] loss: 955.854
[38,     1] loss: 946.785
[39,     1] loss: 993.735
[40,     1] loss: 1013.334
[41,     1] loss: 1055.553
[42,     1] loss: 934.020
[43,     1] loss: 990.515
[44,     1] loss: 980.954
[45,     1] loss: 920.556
[46,     1] loss: 923.108
[47,     1] loss: 982.822
[48,     1] loss: 995.113
[49,     1] loss: 900.888
[50,     1] loss: 947.676
[51,     1] loss: 1055.448
[52,     1] loss: 910.024
[53,     1] loss: 939.373
[54,     1] loss: 954.576
[55,     1] loss: 817.471
[56,     1] loss: 948.506
[57,     1] loss: 844.069
Early stopping applied (best metric=0.7953479290008545)
Finished Training
Total time taken: 7.122007369995117
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1456.093
[2,     1] loss: 1448.235
[3,     1] loss: 1443.477
[4,     1] loss: 1453.192
[5,     1] loss: 1447.682
[6,     1] loss: 1444.920
[7,     1] loss: 1444.866
[8,     1] loss: 1440.126
[9,     1] loss: 1441.533
[10,     1] loss: 1440.862
[11,     1] loss: 1441.579
[12,     1] loss: 1432.637
[13,     1] loss: 1424.380
[14,     1] loss: 1400.281
[15,     1] loss: 1381.292
[16,     1] loss: 1364.438
[17,     1] loss: 1352.151
[18,     1] loss: 1307.814
[19,     1] loss: 1266.168
[20,     1] loss: 1236.420
[21,     1] loss: 1207.894
[22,     1] loss: 1154.749
[23,     1] loss: 1193.734
[24,     1] loss: 1115.615
[25,     1] loss: 1097.577
[26,     1] loss: 1135.990
[27,     1] loss: 1114.020
[28,     1] loss: 1364.666
[29,     1] loss: 1162.974
[30,     1] loss: 1146.608
[31,     1] loss: 1135.684
[32,     1] loss: 1154.463
[33,     1] loss: 1105.444
[34,     1] loss: 1102.713
[35,     1] loss: 1138.645
[36,     1] loss: 1073.200
[37,     1] loss: 1012.717
[38,     1] loss: 992.770
[39,     1] loss: 968.029
[40,     1] loss: 1082.035
[41,     1] loss: 1116.237
[42,     1] loss: 968.216
[43,     1] loss: 995.481
[44,     1] loss: 1003.107
[45,     1] loss: 966.147
[46,     1] loss: 972.417
[47,     1] loss: 934.016
[48,     1] loss: 935.144
[49,     1] loss: 949.199
[50,     1] loss: 895.909
[51,     1] loss: 981.515
[52,     1] loss: 1805.483
[53,     1] loss: 916.783
[54,     1] loss: 1263.718
[55,     1] loss: 1074.667
[56,     1] loss: 1128.094
[57,     1] loss: 1163.045
[58,     1] loss: 1134.082
Early stopping applied (best metric=0.8702356815338135)
Finished Training
Total time taken: 8.265007734298706
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1450.795
[2,     1] loss: 1446.612
[3,     1] loss: 1445.376
[4,     1] loss: 1454.505
[5,     1] loss: 1448.371
[6,     1] loss: 1448.193
[7,     1] loss: 1443.284
[8,     1] loss: 1443.428
[9,     1] loss: 1440.449
[10,     1] loss: 1433.215
[11,     1] loss: 1427.688
[12,     1] loss: 1411.983
[13,     1] loss: 1380.061
[14,     1] loss: 1361.180
[15,     1] loss: 1311.583
[16,     1] loss: 1247.260
[17,     1] loss: 1250.040
[18,     1] loss: 1298.497
[19,     1] loss: 1217.179
[20,     1] loss: 1155.568
[21,     1] loss: 1155.505
[22,     1] loss: 1172.709
[23,     1] loss: 1166.876
[24,     1] loss: 1130.815
[25,     1] loss: 1100.610
[26,     1] loss: 1096.403
[27,     1] loss: 1026.271
[28,     1] loss: 1095.392
[29,     1] loss: 1131.012
[30,     1] loss: 1239.451
[31,     1] loss: 1087.515
[32,     1] loss: 1127.304
[33,     1] loss: 1117.688
[34,     1] loss: 1101.143
[35,     1] loss: 1017.513
[36,     1] loss: 1064.806
[37,     1] loss: 1000.297
[38,     1] loss: 1061.360
[39,     1] loss: 1175.422
[40,     1] loss: 986.335
[41,     1] loss: 1019.800
[42,     1] loss: 1026.478
[43,     1] loss: 1078.681
[44,     1] loss: 978.446
[45,     1] loss: 1070.428
[46,     1] loss: 998.526
[47,     1] loss: 1042.723
[48,     1] loss: 923.541
[49,     1] loss: 1065.954
[50,     1] loss: 1132.908
[51,     1] loss: 938.070
[52,     1] loss: 998.668
[53,     1] loss: 980.056
[54,     1] loss: 984.731
[55,     1] loss: 956.214
[56,     1] loss: 907.137
[57,     1] loss: 896.614
[58,     1] loss: 905.241
[59,     1] loss: 944.054
[60,     1] loss: 962.771
[61,     1] loss: 946.175
[62,     1] loss: 886.939
[63,     1] loss: 919.609
[64,     1] loss: 910.422
[65,     1] loss: 781.384
[66,     1] loss: 828.230
[67,     1] loss: 992.800
[68,     1] loss: 1864.324
[69,     1] loss: 898.258
[70,     1] loss: 1117.305
[71,     1] loss: 1044.391
[72,     1] loss: 1104.406
Early stopping applied (best metric=0.8000415563583374)
Finished Training
Total time taken: 9.48000955581665
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1447.756
[2,     1] loss: 1440.591
[3,     1] loss: 1454.343
[4,     1] loss: 1447.576
[5,     1] loss: 1445.577
[6,     1] loss: 1445.570
[7,     1] loss: 1447.478
[8,     1] loss: 1445.690
[9,     1] loss: 1444.645
[10,     1] loss: 1444.679
[11,     1] loss: 1443.259
[12,     1] loss: 1440.602
[13,     1] loss: 1439.206
[14,     1] loss: 1433.870
[15,     1] loss: 1427.000
[16,     1] loss: 1408.692
[17,     1] loss: 1390.065
[18,     1] loss: 1370.219
[19,     1] loss: 1323.289
[20,     1] loss: 1303.277
[21,     1] loss: 1266.158
[22,     1] loss: 1257.965
[23,     1] loss: 1261.595
[24,     1] loss: 1295.893
[25,     1] loss: 1188.095
[26,     1] loss: 1228.948
[27,     1] loss: 1221.480
[28,     1] loss: 1237.774
[29,     1] loss: 1176.037
[30,     1] loss: 1205.194
[31,     1] loss: 1190.489
[32,     1] loss: 1155.002
[33,     1] loss: 1151.309
[34,     1] loss: 1137.704
[35,     1] loss: 1128.302
[36,     1] loss: 1113.652
[37,     1] loss: 1111.801
[38,     1] loss: 1179.308
[39,     1] loss: 1058.495
[40,     1] loss: 1088.681
[41,     1] loss: 1102.793
[42,     1] loss: 1046.916
[43,     1] loss: 1012.118
[44,     1] loss: 1054.196
[45,     1] loss: 1040.711
[46,     1] loss: 1153.681
[47,     1] loss: 1287.720
[48,     1] loss: 1043.376
[49,     1] loss: 1146.703
[50,     1] loss: 1169.705
[51,     1] loss: 1113.011
[52,     1] loss: 1165.976
[53,     1] loss: 1142.950
[54,     1] loss: 1085.026
[55,     1] loss: 1043.596
[56,     1] loss: 1079.817
[57,     1] loss: 1030.977
[58,     1] loss: 1014.265
[59,     1] loss: 989.379
[60,     1] loss: 1027.526
[61,     1] loss: 985.034
[62,     1] loss: 964.126
[63,     1] loss: 1016.033
[64,     1] loss: 1129.347
[65,     1] loss: 1014.423
[66,     1] loss: 968.830
[67,     1] loss: 962.710
[68,     1] loss: 899.987
[69,     1] loss: 932.630
[70,     1] loss: 1215.790
[71,     1] loss: 1353.574
[72,     1] loss: 1000.948
[73,     1] loss: 1136.544
[74,     1] loss: 1109.908
[75,     1] loss: 1073.646
[76,     1] loss: 1088.463
[77,     1] loss: 1040.391
[78,     1] loss: 1056.275
[79,     1] loss: 1048.372
[80,     1] loss: 1049.019
[81,     1] loss: 1031.736
[82,     1] loss: 1064.802
Early stopping applied (best metric=0.7693562507629395)
Finished Training
Total time taken: 10.754010438919067
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1449.125
[2,     1] loss: 1446.170
[3,     1] loss: 1449.483
[4,     1] loss: 1446.682
[5,     1] loss: 1447.602
[6,     1] loss: 1443.269
[7,     1] loss: 1444.998
[8,     1] loss: 1443.512
[9,     1] loss: 1444.405
[10,     1] loss: 1445.962
[11,     1] loss: 1444.456
[12,     1] loss: 1445.021
[13,     1] loss: 1441.322
[14,     1] loss: 1443.814
[15,     1] loss: 1432.299
[16,     1] loss: 1426.946
[17,     1] loss: 1406.532
[18,     1] loss: 1391.820
[19,     1] loss: 1355.036
[20,     1] loss: 1317.896
[21,     1] loss: 1308.516
[22,     1] loss: 1261.434
[23,     1] loss: 1273.790
[24,     1] loss: 1293.805
[25,     1] loss: 1199.374
[26,     1] loss: 1195.370
[27,     1] loss: 1219.693
[28,     1] loss: 1216.257
[29,     1] loss: 1151.932
[30,     1] loss: 1176.441
[31,     1] loss: 1181.878
[32,     1] loss: 1137.013
[33,     1] loss: 1124.143
[34,     1] loss: 1212.213
[35,     1] loss: 1110.360
[36,     1] loss: 1110.193
[37,     1] loss: 1083.386
[38,     1] loss: 1124.345
[39,     1] loss: 1045.017
[40,     1] loss: 1010.269
[41,     1] loss: 1203.783
[42,     1] loss: 1094.468
[43,     1] loss: 1019.547
[44,     1] loss: 1028.541
[45,     1] loss: 1010.163
[46,     1] loss: 1020.160
[47,     1] loss: 989.505
[48,     1] loss: 1014.158
[49,     1] loss: 955.019
[50,     1] loss: 915.139
[51,     1] loss: 1173.716
[52,     1] loss: 1039.933
[53,     1] loss: 998.933
[54,     1] loss: 996.790
[55,     1] loss: 976.767
[56,     1] loss: 1027.060
[57,     1] loss: 887.309
[58,     1] loss: 898.695
[59,     1] loss: 910.721
[60,     1] loss: 878.873
[61,     1] loss: 881.976
[62,     1] loss: 1055.879
[63,     1] loss: 1629.609
[64,     1] loss: 963.444
[65,     1] loss: 1212.920
[66,     1] loss: 1148.610
[67,     1] loss: 1147.313
[68,     1] loss: 1149.290
[69,     1] loss: 1105.761
Early stopping applied (best metric=0.7238746881484985)
Finished Training
Total time taken: 10.043010234832764
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1452.390
[2,     1] loss: 1451.200
[3,     1] loss: 1447.961
[4,     1] loss: 1445.367
[5,     1] loss: 1445.464
[6,     1] loss: 1444.877
[7,     1] loss: 1448.707
[8,     1] loss: 1444.537
[9,     1] loss: 1442.348
[10,     1] loss: 1438.390
[11,     1] loss: 1431.349
[12,     1] loss: 1419.121
[13,     1] loss: 1392.509
[14,     1] loss: 1366.260
[15,     1] loss: 1324.454
[16,     1] loss: 1361.198
[17,     1] loss: 1308.784
[18,     1] loss: 1233.168
[19,     1] loss: 1260.022
[20,     1] loss: 1234.213
[21,     1] loss: 1262.582
[22,     1] loss: 1210.211
[23,     1] loss: 1224.369
[24,     1] loss: 1234.015
[25,     1] loss: 1183.166
[26,     1] loss: 1204.671
[27,     1] loss: 1186.466
[28,     1] loss: 1160.554
[29,     1] loss: 1127.108
[30,     1] loss: 1111.151
[31,     1] loss: 1079.213
[32,     1] loss: 1080.980
[33,     1] loss: 1027.872
[34,     1] loss: 1128.456
[35,     1] loss: 1208.255
[36,     1] loss: 1107.124
[37,     1] loss: 1146.982
[38,     1] loss: 1105.876
[39,     1] loss: 1147.156
[40,     1] loss: 1108.986
[41,     1] loss: 1050.350
[42,     1] loss: 1053.271
[43,     1] loss: 1045.063
[44,     1] loss: 1064.324
[45,     1] loss: 1196.659
[46,     1] loss: 1097.356
[47,     1] loss: 1029.163
[48,     1] loss: 1094.925
[49,     1] loss: 1104.678
[50,     1] loss: 998.958
[51,     1] loss: 1126.038
[52,     1] loss: 963.127
[53,     1] loss: 1010.540
[54,     1] loss: 974.912
[55,     1] loss: 1047.496
[56,     1] loss: 1140.955
[57,     1] loss: 926.122
[58,     1] loss: 1092.355
[59,     1] loss: 961.423
[60,     1] loss: 989.119
[61,     1] loss: 969.701
[62,     1] loss: 917.509
[63,     1] loss: 1002.394
[64,     1] loss: 961.302
[65,     1] loss: 880.827
[66,     1] loss: 967.341
[67,     1] loss: 1109.334
[68,     1] loss: 901.538
[69,     1] loss: 924.444
[70,     1] loss: 1043.739
[71,     1] loss: 909.564
[72,     1] loss: 867.313
[73,     1] loss: 891.690
[74,     1] loss: 860.354
[75,     1] loss: 944.668
[76,     1] loss: 999.525
[77,     1] loss: 1225.342
[78,     1] loss: 944.184
[79,     1] loss: 1142.586
[80,     1] loss: 1058.836
[81,     1] loss: 985.012
[82,     1] loss: 1077.854
[83,     1] loss: 992.545
[84,     1] loss: 1066.733
[85,     1] loss: 914.314
[86,     1] loss: 967.782
[87,     1] loss: 888.430
[88,     1] loss: 920.069
[89,     1] loss: 1086.969
[90,     1] loss: 846.896
Early stopping applied (best metric=0.6767231822013855)
Finished Training
Total time taken: 11.972011804580688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1445.104
[2,     1] loss: 1447.353
[3,     1] loss: 1443.104
[4,     1] loss: 1442.159
[5,     1] loss: 1446.179
[6,     1] loss: 1443.274
[7,     1] loss: 1438.129
[8,     1] loss: 1440.239
[9,     1] loss: 1435.101
[10,     1] loss: 1422.728
[11,     1] loss: 1404.949
[12,     1] loss: 1379.368
[13,     1] loss: 1320.322
[14,     1] loss: 1317.652
[15,     1] loss: 1268.165
[16,     1] loss: 1265.338
[17,     1] loss: 1181.738
[18,     1] loss: 1190.058
[19,     1] loss: 1245.375
[20,     1] loss: 1169.042
[21,     1] loss: 1259.005
[22,     1] loss: 1163.645
[23,     1] loss: 1188.240
[24,     1] loss: 1153.834
[25,     1] loss: 1176.183
[26,     1] loss: 1136.044
[27,     1] loss: 1170.810
[28,     1] loss: 1091.267
[29,     1] loss: 1114.889
[30,     1] loss: 1107.955
[31,     1] loss: 1146.688
[32,     1] loss: 1084.268
[33,     1] loss: 1095.919
[34,     1] loss: 1049.181
[35,     1] loss: 1063.329
[36,     1] loss: 1094.207
[37,     1] loss: 1082.238
[38,     1] loss: 1048.474
[39,     1] loss: 1015.431
[40,     1] loss: 1078.877
[41,     1] loss: 1011.458
[42,     1] loss: 1093.924
[43,     1] loss: 1059.897
[44,     1] loss: 931.391
[45,     1] loss: 979.889
[46,     1] loss: 959.181
[47,     1] loss: 896.978
[48,     1] loss: 948.441
[49,     1] loss: 1107.996
[50,     1] loss: 1667.941
[51,     1] loss: 1007.771
[52,     1] loss: 1131.036
[53,     1] loss: 1162.665
[54,     1] loss: 1192.247
[55,     1] loss: 1183.034
[56,     1] loss: 1169.087
[57,     1] loss: 1109.555
[58,     1] loss: 1120.236
[59,     1] loss: 1140.997
[60,     1] loss: 1097.326
[61,     1] loss: 1109.848
[62,     1] loss: 1075.638
[63,     1] loss: 1062.084
[64,     1] loss: 1095.487
[65,     1] loss: 1060.962
[66,     1] loss: 1022.712
[67,     1] loss: 1048.331
Early stopping applied (best metric=0.7608107328414917)
Finished Training
Total time taken: 8.962008953094482
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1447.983
[2,     1] loss: 1453.537
[3,     1] loss: 1449.781
[4,     1] loss: 1443.972
[5,     1] loss: 1452.482
[6,     1] loss: 1451.541
[7,     1] loss: 1443.914
[8,     1] loss: 1443.305
[9,     1] loss: 1442.249
[10,     1] loss: 1432.478
[11,     1] loss: 1421.772
[12,     1] loss: 1407.174
[13,     1] loss: 1369.278
[14,     1] loss: 1347.242
[15,     1] loss: 1297.093
[16,     1] loss: 1253.287
[17,     1] loss: 1211.308
[18,     1] loss: 1231.222
[19,     1] loss: 1181.673
[20,     1] loss: 1167.033
[21,     1] loss: 1146.446
[22,     1] loss: 1210.439
[23,     1] loss: 1186.874
[24,     1] loss: 1142.643
[25,     1] loss: 1134.884
[26,     1] loss: 1178.949
[27,     1] loss: 1133.423
[28,     1] loss: 1092.995
[29,     1] loss: 1130.041
[30,     1] loss: 1094.734
[31,     1] loss: 1073.264
[32,     1] loss: 1076.857
[33,     1] loss: 1028.004
[34,     1] loss: 1024.958
[35,     1] loss: 1292.349
[36,     1] loss: 1076.707
[37,     1] loss: 1154.333
[38,     1] loss: 1105.978
[39,     1] loss: 1119.853
[40,     1] loss: 1109.227
[41,     1] loss: 1086.358
[42,     1] loss: 1043.474
[43,     1] loss: 1095.914
[44,     1] loss: 984.110
[45,     1] loss: 1036.123
[46,     1] loss: 926.797
[47,     1] loss: 974.241
[48,     1] loss: 978.484
[49,     1] loss: 1023.557
[50,     1] loss: 931.263
[51,     1] loss: 972.707
[52,     1] loss: 1029.130
[53,     1] loss: 984.859
[54,     1] loss: 923.457
[55,     1] loss: 943.176
[56,     1] loss: 1010.185
[57,     1] loss: 922.793
[58,     1] loss: 952.980
Early stopping applied (best metric=0.9180095195770264)
Finished Training
Total time taken: 7.993008136749268
{'Hydroxylation-K Validation Accuracy': 0.7727541371158392, 'Hydroxylation-K Validation Sensitivity': 0.705925925925926, 'Hydroxylation-K Validation Specificity': 0.7894736842105263, 'Hydroxylation-K Validation Precision': 0.47869189691171116, 'Hydroxylation-K AUC ROC': 0.842514619883041, 'Hydroxylation-K AUC PR': 0.6528253557015231, 'Hydroxylation-K MCC': 0.43974993526192757, 'Hydroxylation-K F1': 0.5618613281881648, 'Validation Loss (Hydroxylation-K)': 0.4044643302758535, 'Hydroxylation-P Validation Accuracy': 0.7829022046258227, 'Hydroxylation-P Validation Sensitivity': 0.7844973544973545, 'Hydroxylation-P Validation Specificity': 0.7826225746920046, 'Hydroxylation-P Validation Precision': 0.44081290922878347, 'Hydroxylation-P AUC ROC': 0.84449944659394, 'Hydroxylation-P AUC PR': 0.5581052833357277, 'Hydroxylation-P MCC': 0.46721929865600437, 'Hydroxylation-P F1': 0.5621435399278664, 'Validation Loss (Hydroxylation-P)': 0.3797692735989889, 'Validation Loss (total)': 0.7842336058616638, 'TimeToTrain': 9.622942781448364}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007065551548422873,
 'learning_rate_Hydroxylation-K': 0.005040027468259829,
 'learning_rate_Hydroxylation-P': 0.002193314265538859,
 'log_base': 2.4489036359163245,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1263080891,
 'sample_weights': [2.552530630368687, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.617368170378253,
 'weight_decay_Hydroxylation-K': 0.8215514061881617,
 'weight_decay_Hydroxylation-P': 6.288699581987581}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.327
[2,     1] loss: 1309.023
[3,     1] loss: 1301.889
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006996224867204917,
 'learning_rate_Hydroxylation-K': 0.006540858864283873,
 'learning_rate_Hydroxylation-P': 0.005614522340192586,
 'log_base': 2.1494804425753298,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 570225411,
 'sample_weights': [1.8639658211449348, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.046780675778299,
 'weight_decay_Hydroxylation-K': 7.8506890648801075,
 'weight_decay_Hydroxylation-P': 1.0847195093757867}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1367.825
[2,     1] loss: 1378.628
[3,     1] loss: 1367.965
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002604471436327788,
 'learning_rate_Hydroxylation-K': 0.002653406067982908,
 'learning_rate_Hydroxylation-P': 0.0023872463434690986,
 'log_base': 1.5939747350172035,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 358339804,
 'sample_weights': [2.1816336638154668, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.813864543685476,
 'weight_decay_Hydroxylation-K': 6.394292605568992,
 'weight_decay_Hydroxylation-P': 0.7015549644847414}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1658.142
[2,     1] loss: 1673.510
[3,     1] loss: 1671.845
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021973602414904767,
 'learning_rate_Hydroxylation-K': 1.3166362001647519e-05,
 'learning_rate_Hydroxylation-P': 0.0038892815617486923,
 'log_base': 1.5726630881700576,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 359952105,
 'sample_weights': [3.5807231040769483, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.5765668948575025,
 'weight_decay_Hydroxylation-K': 1.8809832422298332,
 'weight_decay_Hydroxylation-P': 0.8807604325197389}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1692.654
[2,     1] loss: 1696.370
[3,     1] loss: 1685.915
[4,     1] loss: 1683.766
[5,     1] loss: 1682.073
[6,     1] loss: 1683.314
[7,     1] loss: 1679.569
[8,     1] loss: 1673.915
[9,     1] loss: 1663.446
[10,     1] loss: 1652.486
[11,     1] loss: 1636.042
[12,     1] loss: 1616.901
[13,     1] loss: 1594.058
[14,     1] loss: 1574.196
[15,     1] loss: 1537.142
[16,     1] loss: 1530.642
[17,     1] loss: 1467.721
[18,     1] loss: 1441.128
[19,     1] loss: 1421.032
[20,     1] loss: 1471.189
[21,     1] loss: 1384.316
[22,     1] loss: 1374.024
[23,     1] loss: 1426.244
[24,     1] loss: 1323.482
[25,     1] loss: 1365.436
[26,     1] loss: 1372.771
[27,     1] loss: 1409.585
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021935092005875676,
 'learning_rate_Hydroxylation-K': 0.0015042415962149489,
 'learning_rate_Hydroxylation-P': 0.006380619797206062,
 'log_base': 1.0827906819074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1065125722,
 'sample_weights': [3.687173642921895, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.549794849571752,
 'weight_decay_Hydroxylation-K': 6.490118881245682,
 'weight_decay_Hydroxylation-P': 2.121961891377374}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6820.291
[2,     1] loss: 6798.361
[3,     1] loss: 6796.427
[4,     1] loss: 6795.739
[5,     1] loss: 6823.316
[6,     1] loss: 6798.121
[7,     1] loss: 6764.027
[8,     1] loss: 6790.323
[9,     1] loss: 6828.006
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029181535346287646,
 'learning_rate_Hydroxylation-K': 0.0035263344298738404,
 'learning_rate_Hydroxylation-P': 0.004313021485068982,
 'log_base': 1.0444904071109269,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 220390641,
 'sample_weights': [20.98828300851034, 2.6236364451816234],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.667916579941435,
 'weight_decay_Hydroxylation-K': 5.011187559795487,
 'weight_decay_Hydroxylation-P': 0.20942255409498944}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12598.371
[2,     1] loss: 12460.110
[3,     1] loss: 12431.221
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019028842750271051,
 'learning_rate_Hydroxylation-K': 0.008540216646406732,
 'learning_rate_Hydroxylation-P': 0.0056086146413753,
 'log_base': 2.6881322815277273,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 663521426,
 'sample_weights': [38.35233132609723, 4.79422610148858],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.822458956771209,
 'weight_decay_Hydroxylation-K': 0.6269112623468662,
 'weight_decay_Hydroxylation-P': 6.1307676534515405}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.921
[2,     1] loss: 1265.027
[3,     1] loss: 1260.441
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068607438690484685,
 'learning_rate_Hydroxylation-K': 0.007489970314849071,
 'learning_rate_Hydroxylation-P': 0.0030109497042028763,
 'log_base': 1.8303713248968991,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1460224761,
 'sample_weights': [1.6882730760838476, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.456631494250635,
 'weight_decay_Hydroxylation-K': 2.382094551410127,
 'weight_decay_Hydroxylation-P': 4.371585021465903}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1496.704
[2,     1] loss: 1490.535
[3,     1] loss: 1490.252
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008671635592376435,
 'learning_rate_Hydroxylation-K': 0.007300819941355016,
 'learning_rate_Hydroxylation-P': 0.00010781166522501806,
 'log_base': 1.872043671158121,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1111396540,
 'sample_weights': [2.761606409323577, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1786962456719456,
 'weight_decay_Hydroxylation-K': 4.016020648417115,
 'weight_decay_Hydroxylation-P': 4.134836699824815}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1468.222
[2,     1] loss: 1474.395
[3,     1] loss: 1465.008
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007901712471147286,
 'learning_rate_Hydroxylation-K': 0.00787191180248716,
 'learning_rate_Hydroxylation-P': 0.00993561439939309,
 'log_base': 1.3013508305201311,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 699599963,
 'sample_weights': [2.662458361929506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6124609078006347,
 'weight_decay_Hydroxylation-K': 7.985501866784413,
 'weight_decay_Hydroxylation-P': 0.2751934056444725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2238.355
[2,     1] loss: 2281.706
[3,     1] loss: 2247.499
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004416201937550213,
 'learning_rate_Hydroxylation-K': 0.005487953439971498,
 'learning_rate_Hydroxylation-P': 0.005146735622871382,
 'log_base': 2.276585386180848,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1313240349,
 'sample_weights': [6.337984966705117, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.382852711575196,
 'weight_decay_Hydroxylation-K': 1.4652305222113555,
 'weight_decay_Hydroxylation-P': 7.9107984549729}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1337.106
[2,     1] loss: 1335.949
[3,     1] loss: 1334.874
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004239809958533349,
 'learning_rate_Hydroxylation-K': 0.0006479555291691616,
 'learning_rate_Hydroxylation-P': 0.0040502511265068115,
 'log_base': 1.843134494491658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3925354291,
 'sample_weights': [2.029282198507101, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.739295010532112,
 'weight_decay_Hydroxylation-K': 4.667497392711923,
 'weight_decay_Hydroxylation-P': 0.05841991817368081}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1484.759
[2,     1] loss: 1487.506
[3,     1] loss: 1487.908
[4,     1] loss: 1481.782
[5,     1] loss: 1484.334
[6,     1] loss: 1476.724
[7,     1] loss: 1474.544
[8,     1] loss: 1468.930
[9,     1] loss: 1458.823
[10,     1] loss: 1423.864
[11,     1] loss: 1400.097
[12,     1] loss: 1363.188
[13,     1] loss: 1300.128
[14,     1] loss: 1292.036
[15,     1] loss: 1268.137
[16,     1] loss: 1246.182
[17,     1] loss: 1197.198
[18,     1] loss: 1275.150
[19,     1] loss: 1214.256
[20,     1] loss: 1227.131
[21,     1] loss: 1185.769
[22,     1] loss: 1197.903
[23,     1] loss: 1165.340
[24,     1] loss: 1117.999
[25,     1] loss: 1204.659
[26,     1] loss: 1162.398
[27,     1] loss: 1132.076
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00010139028548657083,
 'learning_rate_Hydroxylation-K': 0.004656959277286274,
 'learning_rate_Hydroxylation-P': 0.007909834078461945,
 'log_base': 1.076120536589269,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4025577264,
 'sample_weights': [2.730223164388386, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.106866364998236,
 'weight_decay_Hydroxylation-K': 1.7651388121009157,
 'weight_decay_Hydroxylation-P': 8.844796044279757}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7407.508
[2,     1] loss: 7378.799
[3,     1] loss: 7405.696
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005840915146694979,
 'learning_rate_Hydroxylation-K': 0.0037907378912561,
 'learning_rate_Hydroxylation-P': 0.002688527382986878,
 'log_base': 1.0459691853131985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3363938123,
 'sample_weights': [22.75608983910885, 2.8446208118836833],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.5210369254322975,
 'weight_decay_Hydroxylation-K': 9.563930579719248,
 'weight_decay_Hydroxylation-P': 6.915712012608033}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12157.703
[2,     1] loss: 12143.841
[3,     1] loss: 12003.489
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004592865303698403,
 'learning_rate_Hydroxylation-K': 0.0013577683703238069,
 'learning_rate_Hydroxylation-P': 0.0036577211462087552,
 'log_base': 1.053874424794289,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1128068747,
 'sample_weights': [37.14503941889697, 4.643308799372011],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.144465404114936,
 'weight_decay_Hydroxylation-K': 3.6321627264532923,
 'weight_decay_Hydroxylation-P': 1.6595783196110157}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10317.802
[2,     1] loss: 10400.777
[3,     1] loss: 10452.527
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003408047839850893,
 'learning_rate_Hydroxylation-K': 0.0005472086779688141,
 'learning_rate_Hydroxylation-P': 0.0037542177875261626,
 'log_base': 2.2126509123328844,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2566362224,
 'sample_weights': [31.81509645816505, 3.9770402629297803],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.026021488326958,
 'weight_decay_Hydroxylation-K': 1.8880225926042355,
 'weight_decay_Hydroxylation-P': 0.42426371866330737}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.415
[2,     1] loss: 1352.692
[3,     1] loss: 1346.339
[4,     1] loss: 1354.571
[5,     1] loss: 1346.961
[6,     1] loss: 1343.148
[7,     1] loss: 1332.645
[8,     1] loss: 1321.708
[9,     1] loss: 1295.092
[10,     1] loss: 1267.602
[11,     1] loss: 1221.133
[12,     1] loss: 1176.844
[13,     1] loss: 1178.734
[14,     1] loss: 1131.759
[15,     1] loss: 1145.532
[16,     1] loss: 1123.207
[17,     1] loss: 1102.540
[18,     1] loss: 1109.762
[19,     1] loss: 1113.021
[20,     1] loss: 1079.651
[21,     1] loss: 1051.471
[22,     1] loss: 1075.148
[23,     1] loss: 1086.756
[24,     1] loss: 1024.611
[25,     1] loss: 1045.708
[26,     1] loss: 1003.797
[27,     1] loss: 1009.495
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006895886870162429,
 'learning_rate_Hydroxylation-K': 0.0040236467915750265,
 'learning_rate_Hydroxylation-P': 0.0017957598930663451,
 'log_base': 1.1738758753967484,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3252497519,
 'sample_weights': [2.1020667663884445, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.544353789456815,
 'weight_decay_Hydroxylation-K': 5.413502248351001,
 'weight_decay_Hydroxylation-P': 0.8695587110334788}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3399.982
[2,     1] loss: 3391.142
[3,     1] loss: 3374.026
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005623922428232293,
 'learning_rate_Hydroxylation-K': 0.002400747462105726,
 'learning_rate_Hydroxylation-P': 0.0031681032994289367,
 'log_base': 1.1812276710845269,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3370121945,
 'sample_weights': [10.413778678269624, 1.3017724823552728],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.160064052151128,
 'weight_decay_Hydroxylation-K': 7.8316900470583075,
 'weight_decay_Hydroxylation-P': 8.15164163100736}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3271.138
[2,     1] loss: 3253.517
[3,     1] loss: 3255.299
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00807505120453774,
 'learning_rate_Hydroxylation-K': 0.0018696813234828098,
 'learning_rate_Hydroxylation-P': 0.009608506614565242,
 'log_base': 2.6063458875179384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1380630437,
 'sample_weights': [10.023416860936441, 1.252975375400531],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9626810791139295,
 'weight_decay_Hydroxylation-K': 2.7727481240394334,
 'weight_decay_Hydroxylation-P': 4.702365781815831}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1276.067
[2,     1] loss: 1276.508
[3,     1] loss: 1277.217
[4,     1] loss: 1275.859
[5,     1] loss: 1268.352
[6,     1] loss: 1262.201
[7,     1] loss: 1247.989
[8,     1] loss: 1212.371
[9,     1] loss: 1168.807
[10,     1] loss: 1166.971
[11,     1] loss: 1093.572
[12,     1] loss: 1122.123
[13,     1] loss: 1106.823
[14,     1] loss: 1104.941
[15,     1] loss: 1054.208
[16,     1] loss: 1035.423
[17,     1] loss: 1019.154
[18,     1] loss: 1065.311
[19,     1] loss: 1018.690
[20,     1] loss: 1053.009
[21,     1] loss: 994.060
[22,     1] loss: 1008.409
[23,     1] loss: 999.188
[24,     1] loss: 1005.832
[25,     1] loss: 961.623
[26,     1] loss: 986.930
[27,     1] loss: 929.756
[28,     1] loss: 964.421
[29,     1] loss: 927.695
[30,     1] loss: 889.798
[31,     1] loss: 884.323
[32,     1] loss: 889.314
[33,     1] loss: 896.270
[34,     1] loss: 1120.249
[35,     1] loss: 975.888
[36,     1] loss: 1007.529
[37,     1] loss: 906.254
[38,     1] loss: 974.275
[39,     1] loss: 955.244
[40,     1] loss: 958.697
[41,     1] loss: 998.181
[42,     1] loss: 861.677
[43,     1] loss: 918.786
[44,     1] loss: 863.284
[45,     1] loss: 950.408
[46,     1] loss: 869.604
[47,     1] loss: 827.038
[48,     1] loss: 835.501
[49,     1] loss: 779.773
[50,     1] loss: 850.616
[51,     1] loss: 850.207
[52,     1] loss: 794.201
[53,     1] loss: 1009.263
[54,     1] loss: 1073.605
[55,     1] loss: 845.643
[56,     1] loss: 940.407
[57,     1] loss: 892.965
[58,     1] loss: 860.522
[59,     1] loss: 942.230
[60,     1] loss: 844.830
[61,     1] loss: 887.610
[62,     1] loss: 817.613
[63,     1] loss: 910.397
[64,     1] loss: 833.109
[65,     1] loss: 881.547
[66,     1] loss: 821.279
[67,     1] loss: 848.140
[68,     1] loss: 778.340
[69,     1] loss: 725.931
[70,     1] loss: 881.272
[71,     1] loss: 832.397
Early stopping applied (best metric=0.7879407405853271)
Finished Training
Total time taken: 8.685010194778442
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.114
[2,     1] loss: 1276.402
[3,     1] loss: 1282.382
[4,     1] loss: 1277.508
[5,     1] loss: 1274.035
[6,     1] loss: 1266.402
[7,     1] loss: 1257.788
[8,     1] loss: 1235.419
[9,     1] loss: 1212.107
[10,     1] loss: 1171.603
[11,     1] loss: 1117.944
[12,     1] loss: 1078.366
[13,     1] loss: 1102.893
[14,     1] loss: 1060.162
[15,     1] loss: 1030.114
[16,     1] loss: 1050.024
[17,     1] loss: 1039.873
[18,     1] loss: 997.578
[19,     1] loss: 997.683
[20,     1] loss: 1028.079
[21,     1] loss: 959.692
[22,     1] loss: 964.838
[23,     1] loss: 937.408
[24,     1] loss: 948.744
[25,     1] loss: 898.961
[26,     1] loss: 879.507
[27,     1] loss: 973.047
[28,     1] loss: 1171.583
[29,     1] loss: 895.550
[30,     1] loss: 1011.010
[31,     1] loss: 968.369
[32,     1] loss: 953.215
[33,     1] loss: 976.446
[34,     1] loss: 944.920
[35,     1] loss: 915.996
[36,     1] loss: 896.413
[37,     1] loss: 842.344
[38,     1] loss: 828.419
[39,     1] loss: 808.975
Early stopping applied (best metric=0.8783873915672302)
Finished Training
Total time taken: 4.7250025272369385
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.407
[2,     1] loss: 1277.997
[3,     1] loss: 1273.251
[4,     1] loss: 1275.661
[5,     1] loss: 1271.847
[6,     1] loss: 1277.302
[7,     1] loss: 1277.545
[8,     1] loss: 1277.860
[9,     1] loss: 1274.890
[10,     1] loss: 1272.672
[11,     1] loss: 1276.638
[12,     1] loss: 1269.633
[13,     1] loss: 1268.716
[14,     1] loss: 1265.966
[15,     1] loss: 1254.603
[16,     1] loss: 1240.104
[17,     1] loss: 1208.792
[18,     1] loss: 1177.794
[19,     1] loss: 1153.853
[20,     1] loss: 1137.412
[21,     1] loss: 1161.559
[22,     1] loss: 1113.000
[23,     1] loss: 1036.414
[24,     1] loss: 1118.679
[25,     1] loss: 1052.917
[26,     1] loss: 1067.541
[27,     1] loss: 1069.925
[28,     1] loss: 1046.329
[29,     1] loss: 1028.487
[30,     1] loss: 1039.470
[31,     1] loss: 963.045
[32,     1] loss: 984.910
[33,     1] loss: 938.287
[34,     1] loss: 1013.719
[35,     1] loss: 892.333
[36,     1] loss: 975.765
[37,     1] loss: 985.757
[38,     1] loss: 1036.318
[39,     1] loss: 1031.083
[40,     1] loss: 969.588
[41,     1] loss: 987.334
[42,     1] loss: 963.374
[43,     1] loss: 941.640
[44,     1] loss: 934.500
[45,     1] loss: 876.786
[46,     1] loss: 903.091
[47,     1] loss: 874.235
[48,     1] loss: 871.532
[49,     1] loss: 880.385
[50,     1] loss: 858.063
[51,     1] loss: 818.877
[52,     1] loss: 811.732
[53,     1] loss: 1088.016
[54,     1] loss: 1692.167
[55,     1] loss: 1035.906
[56,     1] loss: 1056.453
[57,     1] loss: 1132.048
[58,     1] loss: 1216.371
[59,     1] loss: 1132.959
[60,     1] loss: 1169.126
[61,     1] loss: 1177.823
[62,     1] loss: 1150.987
[63,     1] loss: 1118.529
[64,     1] loss: 1121.770
[65,     1] loss: 1128.306
[66,     1] loss: 1099.559
[67,     1] loss: 1125.692
[68,     1] loss: 1103.650
[69,     1] loss: 1097.846
[70,     1] loss: 1118.427
Early stopping applied (best metric=0.7813419103622437)
Finished Training
Total time taken: 8.514008283615112
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.705
[2,     1] loss: 1272.844
[3,     1] loss: 1279.961
[4,     1] loss: 1275.204
[5,     1] loss: 1274.621
[6,     1] loss: 1273.132
[7,     1] loss: 1269.117
[8,     1] loss: 1260.141
[9,     1] loss: 1242.070
[10,     1] loss: 1209.157
[11,     1] loss: 1173.499
[12,     1] loss: 1106.446
[13,     1] loss: 1078.564
[14,     1] loss: 1072.301
[15,     1] loss: 1192.766
[16,     1] loss: 1027.927
[17,     1] loss: 1070.569
[18,     1] loss: 1051.834
[19,     1] loss: 1078.637
[20,     1] loss: 1003.649
[21,     1] loss: 1027.878
[22,     1] loss: 1008.724
[23,     1] loss: 1026.039
[24,     1] loss: 1019.784
[25,     1] loss: 988.077
[26,     1] loss: 979.999
[27,     1] loss: 960.755
[28,     1] loss: 959.366
[29,     1] loss: 902.469
[30,     1] loss: 936.537
[31,     1] loss: 1024.343
[32,     1] loss: 931.234
[33,     1] loss: 901.107
[34,     1] loss: 928.035
[35,     1] loss: 909.036
[36,     1] loss: 889.614
[37,     1] loss: 913.174
[38,     1] loss: 823.168
[39,     1] loss: 971.695
[40,     1] loss: 982.320
[41,     1] loss: 882.569
[42,     1] loss: 949.536
[43,     1] loss: 873.480
[44,     1] loss: 956.826
[45,     1] loss: 830.536
[46,     1] loss: 848.113
[47,     1] loss: 852.184
[48,     1] loss: 764.612
[49,     1] loss: 743.968
[50,     1] loss: 780.930
[51,     1] loss: 904.357
[52,     1] loss: 1108.016
[53,     1] loss: 822.631
[54,     1] loss: 957.279
[55,     1] loss: 918.147
[56,     1] loss: 900.153
[57,     1] loss: 935.727
[58,     1] loss: 813.476
[59,     1] loss: 819.784
[60,     1] loss: 775.989
[61,     1] loss: 838.071
[62,     1] loss: 790.257
[63,     1] loss: 748.707
[64,     1] loss: 834.179
[65,     1] loss: 783.079
[66,     1] loss: 744.000
[67,     1] loss: 730.275
[68,     1] loss: 710.195
[69,     1] loss: 782.295
[70,     1] loss: 1213.103
Early stopping applied (best metric=0.8213990926742554)
Finished Training
Total time taken: 8.533008098602295
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1292.723
[2,     1] loss: 1275.634
[3,     1] loss: 1280.869
[4,     1] loss: 1276.965
[5,     1] loss: 1276.222
[6,     1] loss: 1276.370
[7,     1] loss: 1278.266
[8,     1] loss: 1274.583
[9,     1] loss: 1277.421
[10,     1] loss: 1277.979
[11,     1] loss: 1274.407
[12,     1] loss: 1272.451
[13,     1] loss: 1267.529
[14,     1] loss: 1262.644
[15,     1] loss: 1249.940
[16,     1] loss: 1224.289
[17,     1] loss: 1193.450
[18,     1] loss: 1163.794
[19,     1] loss: 1120.595
[20,     1] loss: 1093.703
[21,     1] loss: 1124.434
[22,     1] loss: 1066.597
[23,     1] loss: 1019.369
[24,     1] loss: 974.026
[25,     1] loss: 981.888
[26,     1] loss: 972.846
[27,     1] loss: 944.518
[28,     1] loss: 961.503
[29,     1] loss: 947.011
[30,     1] loss: 896.381
[31,     1] loss: 943.516
[32,     1] loss: 999.927
[33,     1] loss: 936.965
[34,     1] loss: 919.973
[35,     1] loss: 918.981
[36,     1] loss: 875.444
[37,     1] loss: 902.794
[38,     1] loss: 867.928
[39,     1] loss: 885.571
[40,     1] loss: 887.540
[41,     1] loss: 908.797
[42,     1] loss: 864.668
[43,     1] loss: 826.633
[44,     1] loss: 907.205
[45,     1] loss: 817.077
[46,     1] loss: 771.774
[47,     1] loss: 861.409
[48,     1] loss: 943.297
[49,     1] loss: 928.180
[50,     1] loss: 784.341
[51,     1] loss: 884.270
[52,     1] loss: 791.996
[53,     1] loss: 793.819
[54,     1] loss: 795.024
[55,     1] loss: 745.120
[56,     1] loss: 739.592
[57,     1] loss: 889.448
[58,     1] loss: 1284.524
[59,     1] loss: 835.047
[60,     1] loss: 968.357
[61,     1] loss: 977.599
[62,     1] loss: 928.600
[63,     1] loss: 936.470
[64,     1] loss: 921.124
[65,     1] loss: 882.761
[66,     1] loss: 931.239
[67,     1] loss: 857.563
[68,     1] loss: 892.260
[69,     1] loss: 814.376
[70,     1] loss: 862.759
[71,     1] loss: 834.342
[72,     1] loss: 785.611
[73,     1] loss: 802.925
[74,     1] loss: 717.467
[75,     1] loss: 783.471
Early stopping applied (best metric=0.821983814239502)
Finished Training
Total time taken: 8.939008712768555
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.275
[2,     1] loss: 1280.933
[3,     1] loss: 1271.785
[4,     1] loss: 1274.181
[5,     1] loss: 1270.860
[6,     1] loss: 1267.528
[7,     1] loss: 1265.582
[8,     1] loss: 1239.893
[9,     1] loss: 1222.214
[10,     1] loss: 1202.321
[11,     1] loss: 1126.269
[12,     1] loss: 1084.848
[13,     1] loss: 1120.418
[14,     1] loss: 1062.919
[15,     1] loss: 1109.250
[16,     1] loss: 1020.915
[17,     1] loss: 1044.144
[18,     1] loss: 1075.562
[19,     1] loss: 1049.876
[20,     1] loss: 1008.874
[21,     1] loss: 1010.763
[22,     1] loss: 975.942
[23,     1] loss: 1004.379
[24,     1] loss: 957.539
[25,     1] loss: 974.524
[26,     1] loss: 958.283
[27,     1] loss: 964.341
[28,     1] loss: 940.894
[29,     1] loss: 966.624
[30,     1] loss: 945.007
[31,     1] loss: 921.419
[32,     1] loss: 928.129
[33,     1] loss: 908.037
[34,     1] loss: 917.677
[35,     1] loss: 818.595
[36,     1] loss: 848.611
[37,     1] loss: 1066.085
[38,     1] loss: 1183.150
[39,     1] loss: 887.450
[40,     1] loss: 993.161
[41,     1] loss: 981.001
[42,     1] loss: 966.278
[43,     1] loss: 959.095
[44,     1] loss: 903.422
[45,     1] loss: 907.551
[46,     1] loss: 928.201
[47,     1] loss: 873.354
[48,     1] loss: 900.675
[49,     1] loss: 869.741
[50,     1] loss: 844.600
[51,     1] loss: 894.298
[52,     1] loss: 877.692
[53,     1] loss: 907.684
[54,     1] loss: 853.077
[55,     1] loss: 857.512
[56,     1] loss: 820.673
[57,     1] loss: 856.248
[58,     1] loss: 904.999
[59,     1] loss: 860.573
[60,     1] loss: 786.922
[61,     1] loss: 751.295
[62,     1] loss: 812.599
[63,     1] loss: 864.550
[64,     1] loss: 1252.081
[65,     1] loss: 997.516
[66,     1] loss: 938.063
[67,     1] loss: 921.747
[68,     1] loss: 996.891
[69,     1] loss: 919.381
[70,     1] loss: 890.123
[71,     1] loss: 911.257
[72,     1] loss: 856.406
[73,     1] loss: 846.400
[74,     1] loss: 839.006
[75,     1] loss: 879.132
[76,     1] loss: 838.419
[77,     1] loss: 836.202
[78,     1] loss: 794.983
[79,     1] loss: 762.133
Early stopping applied (best metric=0.8263046741485596)
Finished Training
Total time taken: 9.379008769989014
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.960
[2,     1] loss: 1283.814
[3,     1] loss: 1274.338
[4,     1] loss: 1276.750
[5,     1] loss: 1273.433
[6,     1] loss: 1271.395
[7,     1] loss: 1269.370
[8,     1] loss: 1266.787
[9,     1] loss: 1247.382
[10,     1] loss: 1235.459
[11,     1] loss: 1189.849
[12,     1] loss: 1169.938
[13,     1] loss: 1147.073
[14,     1] loss: 1081.313
[15,     1] loss: 1123.056
[16,     1] loss: 1074.658
[17,     1] loss: 1004.834
[18,     1] loss: 1044.789
[19,     1] loss: 984.693
[20,     1] loss: 1123.102
[21,     1] loss: 1009.479
[22,     1] loss: 1052.339
[23,     1] loss: 1020.120
[24,     1] loss: 986.716
[25,     1] loss: 1000.134
[26,     1] loss: 981.369
[27,     1] loss: 953.941
[28,     1] loss: 961.656
[29,     1] loss: 952.186
[30,     1] loss: 899.000
[31,     1] loss: 973.190
[32,     1] loss: 903.908
[33,     1] loss: 898.926
[34,     1] loss: 900.065
[35,     1] loss: 901.606
[36,     1] loss: 897.762
[37,     1] loss: 877.666
[38,     1] loss: 853.125
[39,     1] loss: 933.252
[40,     1] loss: 1339.754
[41,     1] loss: 850.879
[42,     1] loss: 1011.494
[43,     1] loss: 946.421
[44,     1] loss: 953.315
[45,     1] loss: 966.722
[46,     1] loss: 976.194
[47,     1] loss: 916.766
[48,     1] loss: 968.298
[49,     1] loss: 972.308
[50,     1] loss: 911.151
[51,     1] loss: 912.704
[52,     1] loss: 886.620
[53,     1] loss: 872.320
[54,     1] loss: 970.078
[55,     1] loss: 833.937
[56,     1] loss: 906.807
[57,     1] loss: 813.308
Early stopping applied (best metric=0.8733516931533813)
Finished Training
Total time taken: 6.752006769180298
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.671
[2,     1] loss: 1284.668
[3,     1] loss: 1276.526
[4,     1] loss: 1274.503
[5,     1] loss: 1274.437
[6,     1] loss: 1276.383
[7,     1] loss: 1272.380
[8,     1] loss: 1270.353
[9,     1] loss: 1267.797
[10,     1] loss: 1262.476
[11,     1] loss: 1254.412
[12,     1] loss: 1227.556
[13,     1] loss: 1189.154
[14,     1] loss: 1164.157
[15,     1] loss: 1168.434
[16,     1] loss: 1099.468
[17,     1] loss: 1104.939
[18,     1] loss: 1124.324
[19,     1] loss: 1072.955
[20,     1] loss: 1082.807
[21,     1] loss: 1044.590
[22,     1] loss: 1037.953
[23,     1] loss: 1023.941
[24,     1] loss: 1022.768
[25,     1] loss: 1005.893
[26,     1] loss: 1043.795
[27,     1] loss: 974.972
[28,     1] loss: 956.132
[29,     1] loss: 969.124
[30,     1] loss: 935.783
[31,     1] loss: 968.673
[32,     1] loss: 935.782
[33,     1] loss: 903.158
[34,     1] loss: 880.302
[35,     1] loss: 913.530
[36,     1] loss: 868.418
[37,     1] loss: 861.768
[38,     1] loss: 919.886
[39,     1] loss: 1120.714
[40,     1] loss: 1066.811
[41,     1] loss: 1021.780
[42,     1] loss: 1021.685
[43,     1] loss: 1022.652
[44,     1] loss: 1022.074
[45,     1] loss: 957.706
[46,     1] loss: 913.533
[47,     1] loss: 947.387
[48,     1] loss: 932.733
[49,     1] loss: 934.650
[50,     1] loss: 877.258
[51,     1] loss: 909.226
[52,     1] loss: 879.993
[53,     1] loss: 873.533
[54,     1] loss: 863.404
[55,     1] loss: 861.531
[56,     1] loss: 880.212
[57,     1] loss: 928.558
[58,     1] loss: 887.015
[59,     1] loss: 793.582
[60,     1] loss: 899.476
[61,     1] loss: 819.587
[62,     1] loss: 919.898
[63,     1] loss: 924.447
[64,     1] loss: 810.328
[65,     1] loss: 845.646
[66,     1] loss: 920.810
[67,     1] loss: 791.277
[68,     1] loss: 775.451
[69,     1] loss: 874.938
[70,     1] loss: 813.986
[71,     1] loss: 768.109
[72,     1] loss: 755.338
[73,     1] loss: 964.839
[74,     1] loss: 1085.571
[75,     1] loss: 780.460
[76,     1] loss: 952.517
[77,     1] loss: 846.083
[78,     1] loss: 859.864
Early stopping applied (best metric=0.7745775580406189)
Finished Training
Total time taken: 9.246009588241577
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.531
[2,     1] loss: 1294.603
[3,     1] loss: 1276.193
[4,     1] loss: 1277.885
[5,     1] loss: 1276.231
[6,     1] loss: 1275.472
[7,     1] loss: 1273.810
[8,     1] loss: 1277.122
[9,     1] loss: 1276.863
[10,     1] loss: 1276.444
[11,     1] loss: 1275.276
[12,     1] loss: 1276.874
[13,     1] loss: 1273.671
[14,     1] loss: 1276.662
[15,     1] loss: 1275.023
[16,     1] loss: 1275.946
[17,     1] loss: 1275.831
[18,     1] loss: 1275.782
[19,     1] loss: 1274.792
[20,     1] loss: 1274.802
[21,     1] loss: 1275.036
[22,     1] loss: 1274.302
[23,     1] loss: 1274.987
[24,     1] loss: 1273.121
[25,     1] loss: 1272.348
[26,     1] loss: 1271.094
[27,     1] loss: 1267.929
[28,     1] loss: 1264.539
[29,     1] loss: 1242.711
[30,     1] loss: 1226.966
[31,     1] loss: 1203.662
[32,     1] loss: 1168.160
[33,     1] loss: 1177.079
[34,     1] loss: 1111.115
[35,     1] loss: 1183.487
[36,     1] loss: 1153.582
[37,     1] loss: 1151.944
[38,     1] loss: 1040.180
[39,     1] loss: 1086.425
[40,     1] loss: 1045.641
[41,     1] loss: 1033.563
[42,     1] loss: 999.995
[43,     1] loss: 1007.112
[44,     1] loss: 946.020
[45,     1] loss: 946.088
[46,     1] loss: 902.601
[47,     1] loss: 971.796
[48,     1] loss: 905.889
[49,     1] loss: 925.362
[50,     1] loss: 870.457
[51,     1] loss: 890.828
[52,     1] loss: 851.171
[53,     1] loss: 841.973
[54,     1] loss: 813.489
[55,     1] loss: 954.473
[56,     1] loss: 1008.556
[57,     1] loss: 854.117
[58,     1] loss: 895.344
[59,     1] loss: 837.832
[60,     1] loss: 869.565
[61,     1] loss: 782.171
[62,     1] loss: 838.970
[63,     1] loss: 830.298
[64,     1] loss: 753.478
[65,     1] loss: 755.818
[66,     1] loss: 770.577
[67,     1] loss: 824.218
[68,     1] loss: 1072.049
[69,     1] loss: 817.074
[70,     1] loss: 937.995
[71,     1] loss: 879.679
[72,     1] loss: 929.129
[73,     1] loss: 809.859
[74,     1] loss: 888.664
[75,     1] loss: 799.010
[76,     1] loss: 797.041
Early stopping applied (best metric=0.9585567712783813)
Finished Training
Total time taken: 9.105011701583862
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1286.040
[2,     1] loss: 1276.343
[3,     1] loss: 1281.418
[4,     1] loss: 1277.583
[5,     1] loss: 1277.508
[6,     1] loss: 1279.815
[7,     1] loss: 1278.045
[8,     1] loss: 1277.902
[9,     1] loss: 1275.784
[10,     1] loss: 1275.525
[11,     1] loss: 1276.491
[12,     1] loss: 1274.906
[13,     1] loss: 1275.564
[14,     1] loss: 1276.844
[15,     1] loss: 1273.219
[16,     1] loss: 1271.698
[17,     1] loss: 1264.324
[18,     1] loss: 1256.514
[19,     1] loss: 1236.881
[20,     1] loss: 1204.626
[21,     1] loss: 1180.934
[22,     1] loss: 1144.215
[23,     1] loss: 1116.362
[24,     1] loss: 1076.698
[25,     1] loss: 1048.015
[26,     1] loss: 1055.786
[27,     1] loss: 1068.879
[28,     1] loss: 1084.281
[29,     1] loss: 1025.994
[30,     1] loss: 1038.597
[31,     1] loss: 966.604
[32,     1] loss: 992.145
[33,     1] loss: 969.580
[34,     1] loss: 976.470
[35,     1] loss: 989.069
[36,     1] loss: 992.706
[37,     1] loss: 965.558
[38,     1] loss: 943.320
[39,     1] loss: 912.971
[40,     1] loss: 958.635
[41,     1] loss: 1264.332
[42,     1] loss: 922.650
[43,     1] loss: 1016.456
[44,     1] loss: 976.062
[45,     1] loss: 1009.355
[46,     1] loss: 1026.598
[47,     1] loss: 985.348
[48,     1] loss: 980.521
[49,     1] loss: 934.810
[50,     1] loss: 941.262
[51,     1] loss: 986.158
[52,     1] loss: 905.211
[53,     1] loss: 951.767
[54,     1] loss: 934.018
[55,     1] loss: 894.907
[56,     1] loss: 871.181
[57,     1] loss: 840.461
[58,     1] loss: 859.571
[59,     1] loss: 1137.586
Early stopping applied (best metric=0.8175925016403198)
Finished Training
Total time taken: 7.124007940292358
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.951
[2,     1] loss: 1278.750
[3,     1] loss: 1279.060
[4,     1] loss: 1273.528
[5,     1] loss: 1277.472
[6,     1] loss: 1273.456
[7,     1] loss: 1273.947
[8,     1] loss: 1275.462
[9,     1] loss: 1271.864
[10,     1] loss: 1271.494
[11,     1] loss: 1266.740
[12,     1] loss: 1260.317
[13,     1] loss: 1246.437
[14,     1] loss: 1217.366
[15,     1] loss: 1183.163
[16,     1] loss: 1168.997
[17,     1] loss: 1166.760
[18,     1] loss: 1108.173
[19,     1] loss: 1139.367
[20,     1] loss: 1072.520
[21,     1] loss: 1133.623
[22,     1] loss: 1055.244
[23,     1] loss: 1053.521
[24,     1] loss: 1036.706
[25,     1] loss: 1041.979
[26,     1] loss: 1069.358
[27,     1] loss: 1047.043
[28,     1] loss: 1020.896
[29,     1] loss: 957.462
[30,     1] loss: 1007.760
[31,     1] loss: 960.154
[32,     1] loss: 981.061
[33,     1] loss: 1026.633
[34,     1] loss: 952.811
[35,     1] loss: 968.652
[36,     1] loss: 942.286
[37,     1] loss: 884.868
[38,     1] loss: 1056.077
[39,     1] loss: 1044.341
[40,     1] loss: 944.552
[41,     1] loss: 972.644
[42,     1] loss: 963.597
[43,     1] loss: 920.813
[44,     1] loss: 943.911
[45,     1] loss: 921.721
[46,     1] loss: 969.667
[47,     1] loss: 907.986
[48,     1] loss: 899.321
[49,     1] loss: 902.391
[50,     1] loss: 857.155
[51,     1] loss: 873.952
[52,     1] loss: 830.028
[53,     1] loss: 865.783
[54,     1] loss: 911.403
[55,     1] loss: 844.889
[56,     1] loss: 800.850
[57,     1] loss: 801.340
[58,     1] loss: 935.632
[59,     1] loss: 1334.393
[60,     1] loss: 797.932
[61,     1] loss: 1049.728
[62,     1] loss: 921.261
[63,     1] loss: 964.586
[64,     1] loss: 1027.063
[65,     1] loss: 919.278
[66,     1] loss: 967.938
[67,     1] loss: 910.206
[68,     1] loss: 874.620
[69,     1] loss: 911.516
[70,     1] loss: 852.882
[71,     1] loss: 886.331
Early stopping applied (best metric=0.7499238848686218)
Finished Training
Total time taken: 8.466007947921753
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.382
[2,     1] loss: 1275.829
[3,     1] loss: 1284.176
[4,     1] loss: 1273.450
[5,     1] loss: 1276.703
[6,     1] loss: 1276.351
[7,     1] loss: 1275.819
[8,     1] loss: 1273.986
[9,     1] loss: 1272.372
[10,     1] loss: 1273.601
[11,     1] loss: 1272.177
[12,     1] loss: 1267.614
[13,     1] loss: 1261.260
[14,     1] loss: 1247.285
[15,     1] loss: 1222.104
[16,     1] loss: 1197.929
[17,     1] loss: 1150.393
[18,     1] loss: 1141.777
[19,     1] loss: 1082.049
[20,     1] loss: 1030.381
[21,     1] loss: 1064.205
[22,     1] loss: 1021.371
[23,     1] loss: 1009.272
[24,     1] loss: 963.013
[25,     1] loss: 948.767
[26,     1] loss: 915.912
[27,     1] loss: 947.519
[28,     1] loss: 948.951
[29,     1] loss: 933.671
[30,     1] loss: 873.727
[31,     1] loss: 1092.248
[32,     1] loss: 986.799
[33,     1] loss: 929.318
[34,     1] loss: 959.121
[35,     1] loss: 953.801
[36,     1] loss: 906.537
[37,     1] loss: 921.757
[38,     1] loss: 864.277
[39,     1] loss: 884.135
[40,     1] loss: 891.443
[41,     1] loss: 923.927
[42,     1] loss: 902.673
[43,     1] loss: 832.832
[44,     1] loss: 847.473
[45,     1] loss: 850.909
[46,     1] loss: 880.226
[47,     1] loss: 905.190
[48,     1] loss: 779.481
[49,     1] loss: 829.487
[50,     1] loss: 846.090
[51,     1] loss: 795.381
[52,     1] loss: 704.891
[53,     1] loss: 770.253
[54,     1] loss: 747.428
[55,     1] loss: 695.545
[56,     1] loss: 826.602
[57,     1] loss: 1212.478
[58,     1] loss: 1018.483
[59,     1] loss: 909.503
[60,     1] loss: 916.533
[61,     1] loss: 932.204
Early stopping applied (best metric=0.9131081104278564)
Finished Training
Total time taken: 7.347007513046265
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.229
[2,     1] loss: 1274.733
[3,     1] loss: 1280.134
[4,     1] loss: 1273.901
[5,     1] loss: 1278.314
[6,     1] loss: 1273.560
[7,     1] loss: 1275.193
[8,     1] loss: 1275.246
[9,     1] loss: 1269.969
[10,     1] loss: 1268.670
[11,     1] loss: 1268.675
[12,     1] loss: 1257.686
[13,     1] loss: 1248.059
[14,     1] loss: 1223.833
[15,     1] loss: 1187.525
[16,     1] loss: 1168.463
[17,     1] loss: 1112.693
[18,     1] loss: 1090.815
[19,     1] loss: 1112.322
[20,     1] loss: 1055.820
[21,     1] loss: 1011.525
[22,     1] loss: 1051.442
[23,     1] loss: 1088.647
[24,     1] loss: 1053.223
[25,     1] loss: 1034.710
[26,     1] loss: 994.966
[27,     1] loss: 994.979
[28,     1] loss: 976.640
[29,     1] loss: 979.699
[30,     1] loss: 932.072
[31,     1] loss: 944.399
[32,     1] loss: 911.458
[33,     1] loss: 932.550
[34,     1] loss: 931.029
[35,     1] loss: 1189.134
[36,     1] loss: 927.656
[37,     1] loss: 1159.167
[38,     1] loss: 985.658
[39,     1] loss: 1016.101
[40,     1] loss: 1081.828
[41,     1] loss: 1047.941
[42,     1] loss: 973.295
[43,     1] loss: 968.516
[44,     1] loss: 947.782
[45,     1] loss: 911.251
[46,     1] loss: 949.630
[47,     1] loss: 878.438
[48,     1] loss: 923.473
[49,     1] loss: 859.654
[50,     1] loss: 847.175
[51,     1] loss: 819.883
[52,     1] loss: 848.364
[53,     1] loss: 840.245
[54,     1] loss: 910.951
[55,     1] loss: 901.158
[56,     1] loss: 786.360
[57,     1] loss: 819.231
[58,     1] loss: 820.688
[59,     1] loss: 738.330
[60,     1] loss: 833.775
[61,     1] loss: 1000.182
[62,     1] loss: 748.943
[63,     1] loss: 849.441
[64,     1] loss: 862.006
[65,     1] loss: 828.105
[66,     1] loss: 858.483
[67,     1] loss: 759.940
[68,     1] loss: 750.190
[69,     1] loss: 726.576
[70,     1] loss: 830.623
[71,     1] loss: 810.167
[72,     1] loss: 1132.755
[73,     1] loss: 880.448
[74,     1] loss: 876.292
[75,     1] loss: 870.631
[76,     1] loss: 906.117
[77,     1] loss: 836.247
[78,     1] loss: 919.209
[79,     1] loss: 826.051
[80,     1] loss: 841.530
[81,     1] loss: 776.528
[82,     1] loss: 756.717
[83,     1] loss: 805.641
[84,     1] loss: 695.401
[85,     1] loss: 695.623
[86,     1] loss: 633.058
[87,     1] loss: 724.706
[88,     1] loss: 1234.503
[89,     1] loss: 1083.862
[90,     1] loss: 1016.794
[91,     1] loss: 1061.293
[92,     1] loss: 1047.273
[93,     1] loss: 966.795
[94,     1] loss: 977.240
[95,     1] loss: 910.788
[96,     1] loss: 922.240
[97,     1] loss: 893.482
[98,     1] loss: 850.211
[99,     1] loss: 885.377
[100,     1] loss: 814.381
[101,     1] loss: 877.170
[102,     1] loss: 804.214
Early stopping applied (best metric=0.8406515121459961)
Finished Training
Total time taken: 12.102012157440186
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1276.207
[2,     1] loss: 1272.770
[3,     1] loss: 1275.205
[4,     1] loss: 1270.220
[5,     1] loss: 1271.636
[6,     1] loss: 1272.207
[7,     1] loss: 1261.811
[8,     1] loss: 1243.723
[9,     1] loss: 1205.303
[10,     1] loss: 1185.215
[11,     1] loss: 1147.656
[12,     1] loss: 1126.331
[13,     1] loss: 1065.839
[14,     1] loss: 1112.576
[15,     1] loss: 1060.045
[16,     1] loss: 1052.546
[17,     1] loss: 1047.502
[18,     1] loss: 1033.802
[19,     1] loss: 1024.824
[20,     1] loss: 1008.647
[21,     1] loss: 1001.301
[22,     1] loss: 948.326
[23,     1] loss: 932.367
[24,     1] loss: 1148.266
[25,     1] loss: 1007.566
[26,     1] loss: 913.787
[27,     1] loss: 976.368
[28,     1] loss: 973.558
[29,     1] loss: 877.566
[30,     1] loss: 928.364
[31,     1] loss: 936.509
[32,     1] loss: 926.072
[33,     1] loss: 900.121
[34,     1] loss: 905.226
[35,     1] loss: 906.798
[36,     1] loss: 838.987
[37,     1] loss: 884.261
[38,     1] loss: 833.913
[39,     1] loss: 833.797
[40,     1] loss: 902.029
[41,     1] loss: 906.867
[42,     1] loss: 802.212
[43,     1] loss: 797.011
[44,     1] loss: 848.281
[45,     1] loss: 796.486
[46,     1] loss: 758.141
[47,     1] loss: 871.559
[48,     1] loss: 1405.873
[49,     1] loss: 759.975
[50,     1] loss: 1022.813
[51,     1] loss: 926.331
[52,     1] loss: 921.050
[53,     1] loss: 965.526
[54,     1] loss: 992.696
[55,     1] loss: 919.015
[56,     1] loss: 912.267
Early stopping applied (best metric=0.8870030045509338)
Finished Training
Total time taken: 6.655006170272827
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1279.047
[2,     1] loss: 1280.264
[3,     1] loss: 1278.942
[4,     1] loss: 1275.917
[5,     1] loss: 1275.683
[6,     1] loss: 1276.872
[7,     1] loss: 1269.661
[8,     1] loss: 1258.454
[9,     1] loss: 1242.957
[10,     1] loss: 1215.571
[11,     1] loss: 1172.452
[12,     1] loss: 1166.261
[13,     1] loss: 1106.286
[14,     1] loss: 1050.143
[15,     1] loss: 1070.042
[16,     1] loss: 1052.027
[17,     1] loss: 984.219
[18,     1] loss: 1057.449
[19,     1] loss: 1057.016
[20,     1] loss: 1091.531
[21,     1] loss: 1007.571
[22,     1] loss: 1016.357
[23,     1] loss: 1035.514
[24,     1] loss: 1003.754
[25,     1] loss: 1004.012
[26,     1] loss: 990.395
[27,     1] loss: 976.617
[28,     1] loss: 962.065
[29,     1] loss: 961.227
[30,     1] loss: 962.252
[31,     1] loss: 956.790
[32,     1] loss: 968.008
[33,     1] loss: 930.774
[34,     1] loss: 906.274
[35,     1] loss: 990.718
[36,     1] loss: 906.610
[37,     1] loss: 897.211
[38,     1] loss: 924.231
[39,     1] loss: 973.096
[40,     1] loss: 936.037
[41,     1] loss: 869.988
[42,     1] loss: 870.226
[43,     1] loss: 884.991
[44,     1] loss: 837.451
[45,     1] loss: 809.330
[46,     1] loss: 828.130
[47,     1] loss: 946.154
[48,     1] loss: 1111.619
[49,     1] loss: 958.808
[50,     1] loss: 969.680
[51,     1] loss: 956.173
[52,     1] loss: 960.761
[53,     1] loss: 864.002
[54,     1] loss: 929.532
[55,     1] loss: 895.214
[56,     1] loss: 869.642
[57,     1] loss: 823.456
[58,     1] loss: 843.414
[59,     1] loss: 844.890
[60,     1] loss: 826.987
[61,     1] loss: 850.122
[62,     1] loss: 800.286
[63,     1] loss: 849.939
[64,     1] loss: 956.485
[65,     1] loss: 976.081
[66,     1] loss: 783.829
[67,     1] loss: 874.685
[68,     1] loss: 802.004
[69,     1] loss: 823.930
[70,     1] loss: 888.257
[71,     1] loss: 808.996
[72,     1] loss: 734.400
[73,     1] loss: 746.137
[74,     1] loss: 858.833
[75,     1] loss: 934.056
[76,     1] loss: 896.360
[77,     1] loss: 773.960
[78,     1] loss: 894.080
[79,     1] loss: 783.164
[80,     1] loss: 848.505
[81,     1] loss: 770.188
[82,     1] loss: 806.272
[83,     1] loss: 809.960
[84,     1] loss: 739.725
[85,     1] loss: 831.494
[86,     1] loss: 801.874
[87,     1] loss: 727.316
[88,     1] loss: 773.642
[89,     1] loss: 852.108
[90,     1] loss: 745.159
[91,     1] loss: 679.463
[92,     1] loss: 750.677
[93,     1] loss: 812.470
[94,     1] loss: 1394.100
[95,     1] loss: 825.998
[96,     1] loss: 1119.080
[97,     1] loss: 1007.283
[98,     1] loss: 1012.271
[99,     1] loss: 952.764
[100,     1] loss: 894.599
[101,     1] loss: 874.150
[102,     1] loss: 997.885
[103,     1] loss: 841.821
[104,     1] loss: 878.770
[105,     1] loss: 826.725
[106,     1] loss: 840.417
[107,     1] loss: 785.368
[108,     1] loss: 790.144
[109,     1] loss: 926.317
[110,     1] loss: 1111.700
[111,     1] loss: 961.045
Early stopping applied (best metric=0.7291785478591919)
Finished Training
Total time taken: 13.150012969970703
{'Hydroxylation-K Validation Accuracy': 0.750531914893617, 'Hydroxylation-K Validation Sensitivity': 0.674074074074074, 'Hydroxylation-K Validation Specificity': 0.7701754385964912, 'Hydroxylation-K Validation Precision': 0.42980344274461924, 'Hydroxylation-K AUC ROC': 0.8060818713450293, 'Hydroxylation-K AUC PR': 0.5966984333471406, 'Hydroxylation-K MCC': 0.38501722548198214, 'Hydroxylation-K F1': 0.5208155852883489, 'Validation Loss (Hydroxylation-K)': 0.4350287616252899, 'Hydroxylation-P Validation Accuracy': 0.7700690997072906, 'Hydroxylation-P Validation Sensitivity': 0.7523280423280423, 'Hydroxylation-P Validation Specificity': 0.7740061848471246, 'Hydroxylation-P Validation Precision': 0.4319338684035824, 'Hydroxylation-P AUC ROC': 0.8380245323560451, 'Hydroxylation-P AUC PR': 0.5909011341671782, 'Hydroxylation-P MCC': 0.4397981009897689, 'Hydroxylation-P F1': 0.5405969159506053, 'Validation Loss (Hydroxylation-P)': 0.3957246442635854, 'Validation Loss (total)': 0.8307534138361613, 'TimeToTrain': 8.581475289662679}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005306987841766979,
 'learning_rate_Hydroxylation-K': 0.0004826687877037619,
 'learning_rate_Hydroxylation-P': 0.008701278169224416,
 'log_base': 2.6362020084586466,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2479456104,
 'sample_weights': [1.7440189338327978, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.621222065342382,
 'weight_decay_Hydroxylation-K': 6.499187112484323,
 'weight_decay_Hydroxylation-P': 1.3316529942026292}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.316
[2,     1] loss: 1271.824
[3,     1] loss: 1271.098
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008924761459344784,
 'learning_rate_Hydroxylation-K': 0.00042041816279182655,
 'learning_rate_Hydroxylation-P': 0.00427513903594522,
 'log_base': 2.2812711730105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3741086083,
 'sample_weights': [1.7222485838382258, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.1640881677598106,
 'weight_decay_Hydroxylation-K': 5.811701535254298,
 'weight_decay_Hydroxylation-P': 4.024483503145612}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1337.066
[2,     1] loss: 1339.215
[3,     1] loss: 1336.330
[4,     1] loss: 1334.153
[5,     1] loss: 1335.495
[6,     1] loss: 1335.041
[7,     1] loss: 1343.733
[8,     1] loss: 1338.808
[9,     1] loss: 1331.231
[10,     1] loss: 1331.275
[11,     1] loss: 1330.096
[12,     1] loss: 1330.570
[13,     1] loss: 1326.487
[14,     1] loss: 1324.084
[15,     1] loss: 1328.373
[16,     1] loss: 1314.883
[17,     1] loss: 1308.109
[18,     1] loss: 1293.465
[19,     1] loss: 1271.361
[20,     1] loss: 1251.494
[21,     1] loss: 1218.141
[22,     1] loss: 1180.969
[23,     1] loss: 1138.966
[24,     1] loss: 1119.846
[25,     1] loss: 1154.389
[26,     1] loss: 1101.324
[27,     1] loss: 1100.260
[28,     1] loss: 1093.622
[29,     1] loss: 1069.516
[30,     1] loss: 1110.502
[31,     1] loss: 1088.196
[32,     1] loss: 1067.041
[33,     1] loss: 1073.198
[34,     1] loss: 1082.152
[35,     1] loss: 1090.401
[36,     1] loss: 1083.350
[37,     1] loss: 1096.414
[38,     1] loss: 1103.349
[39,     1] loss: 1091.890
[40,     1] loss: 1077.275
[41,     1] loss: 1089.264
[42,     1] loss: 1078.953
[43,     1] loss: 1056.249
[44,     1] loss: 1002.022
[45,     1] loss: 1048.014
[46,     1] loss: 1039.340
[47,     1] loss: 1006.830
[48,     1] loss: 1042.052
[49,     1] loss: 1007.308
[50,     1] loss: 1029.661
[51,     1] loss: 1033.383
[52,     1] loss: 1009.156
[53,     1] loss: 1014.004
[54,     1] loss: 1029.428
[55,     1] loss: 980.559
[56,     1] loss: 992.702
[57,     1] loss: 977.268
[58,     1] loss: 966.107
[59,     1] loss: 963.744
[60,     1] loss: 1067.979
[61,     1] loss: 963.969
[62,     1] loss: 973.335
[63,     1] loss: 977.099
[64,     1] loss: 1035.594
[65,     1] loss: 940.266
[66,     1] loss: 997.776
[67,     1] loss: 968.376
[68,     1] loss: 992.044
[69,     1] loss: 979.842
[70,     1] loss: 960.537
[71,     1] loss: 977.864
[72,     1] loss: 982.254
[73,     1] loss: 959.512
[74,     1] loss: 979.562
[75,     1] loss: 1015.773
[76,     1] loss: 955.249
[77,     1] loss: 965.914
[78,     1] loss: 951.696
[79,     1] loss: 940.021
[80,     1] loss: 970.354
[81,     1] loss: 999.420
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005216306345481807,
 'learning_rate_Hydroxylation-K': 0.002907784980368151,
 'learning_rate_Hydroxylation-P': 0.00125027586559074,
 'log_base': 1.9411125196109815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3465046337,
 'sample_weights': [2.0242230061636453, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.896059357418158,
 'weight_decay_Hydroxylation-K': 3.915047792283035,
 'weight_decay_Hydroxylation-P': 1.7169074338582457}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1437.338
[2,     1] loss: 1436.818
[3,     1] loss: 1443.201
[4,     1] loss: 1439.743
[5,     1] loss: 1436.256
[6,     1] loss: 1447.889
[7,     1] loss: 1438.612
[8,     1] loss: 1435.164
[9,     1] loss: 1428.910
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008923810573936484,
 'learning_rate_Hydroxylation-K': 0.0018803600845319797,
 'learning_rate_Hydroxylation-P': 0.006832952775900975,
 'log_base': 2.6755694924981874,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1551421950,
 'sample_weights': [2.5170218989577173, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.287879757379779,
 'weight_decay_Hydroxylation-K': 2.1071361449704855,
 'weight_decay_Hydroxylation-P': 4.909191697797255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.679
[2,     1] loss: 1272.406
[3,     1] loss: 1266.715
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005838516284523343,
 'learning_rate_Hydroxylation-K': 0.0026287370090546437,
 'learning_rate_Hydroxylation-P': 0.00956252517033848,
 'log_base': 1.9378293698537448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1389958800,
 'sample_weights': [1.6963088595336988, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.3229671084702765,
 'weight_decay_Hydroxylation-K': 4.3821538622483045,
 'weight_decay_Hydroxylation-P': 6.022155185915113}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1435.857
[2,     1] loss: 1447.319
[3,     1] loss: 1441.313
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0062515172196544715,
 'learning_rate_Hydroxylation-K': 0.0003412316581821466,
 'learning_rate_Hydroxylation-P': 0.008022123499153759,
 'log_base': 2.634605470785603,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1524675733,
 'sample_weights': [2.523462400927848, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.08653715069421,
 'weight_decay_Hydroxylation-K': 3.3402620960289173,
 'weight_decay_Hydroxylation-P': 5.749701268781811}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.880
[2,     1] loss: 1273.624
[3,     1] loss: 1276.523
[4,     1] loss: 1268.133
[5,     1] loss: 1272.331
[6,     1] loss: 1268.252
[7,     1] loss: 1260.381
[8,     1] loss: 1248.718
[9,     1] loss: 1226.777
[10,     1] loss: 1190.198
[11,     1] loss: 1144.955
[12,     1] loss: 1112.799
[13,     1] loss: 1121.870
[14,     1] loss: 1121.560
[15,     1] loss: 1029.155
[16,     1] loss: 1051.208
[17,     1] loss: 1036.249
[18,     1] loss: 969.188
[19,     1] loss: 1004.299
[20,     1] loss: 994.191
[21,     1] loss: 1016.270
[22,     1] loss: 997.461
[23,     1] loss: 963.767
[24,     1] loss: 944.507
[25,     1] loss: 937.412
[26,     1] loss: 977.235
[27,     1] loss: 966.092
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0051669841825202996,
 'learning_rate_Hydroxylation-K': 0.004027427965601067,
 'learning_rate_Hydroxylation-P': 0.003427589737482906,
 'log_base': 1.045667618166739,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3952789433,
 'sample_weights': [1.7233256033448705, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.331191170120496,
 'weight_decay_Hydroxylation-K': 4.259466548031201,
 'weight_decay_Hydroxylation-P': 1.7990953891993922}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12082.086
[2,     1] loss: 12256.325
[3,     1] loss: 12157.860
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 1.117362974380201e-05,
 'learning_rate_Hydroxylation-K': 0.00945375896426002,
 'learning_rate_Hydroxylation-P': 0.006703031210057789,
 'log_base': 2.587934259338706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 135049501,
 'sample_weights': [37.3848968215272, 4.67329212973458],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.006554138549754,
 'weight_decay_Hydroxylation-K': 2.9847051897734964,
 'weight_decay_Hydroxylation-P': 5.249512284771709}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.903
[2,     1] loss: 1278.306
[3,     1] loss: 1278.631
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004679050754413006,
 'learning_rate_Hydroxylation-K': 0.009978115945061613,
 'learning_rate_Hydroxylation-P': 0.005850348091979235,
 'log_base': 2.3504641286906036,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1975017511,
 'sample_weights': [1.7557192359664773, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.743865226495787,
 'weight_decay_Hydroxylation-K': 0.2833033766018729,
 'weight_decay_Hydroxylation-P': 3.672883550544021}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.066
[2,     1] loss: 1318.290
[3,     1] loss: 1318.118
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007684896482550755,
 'learning_rate_Hydroxylation-K': 0.0009264033433483819,
 'learning_rate_Hydroxylation-P': 0.007074389034107201,
 'log_base': 2.490647474499446,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1515428616,
 'sample_weights': [1.953449711317618, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.967641293235392,
 'weight_decay_Hydroxylation-K': 5.609904942395477,
 'weight_decay_Hydroxylation-P': 6.146022511208532}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.331
[2,     1] loss: 1293.142
[3,     1] loss: 1292.498
[4,     1] loss: 1291.610
[5,     1] loss: 1296.383
[6,     1] loss: 1291.148
[7,     1] loss: 1285.986
[8,     1] loss: 1285.749
[9,     1] loss: 1273.085
[10,     1] loss: 1256.309
[11,     1] loss: 1228.951
[12,     1] loss: 1195.022
[13,     1] loss: 1169.734
[14,     1] loss: 1111.981
[15,     1] loss: 1130.809
[16,     1] loss: 1108.857
[17,     1] loss: 1060.316
[18,     1] loss: 1092.849
[19,     1] loss: 1040.248
[20,     1] loss: 1092.188
[21,     1] loss: 987.602
[22,     1] loss: 1004.759
[23,     1] loss: 995.025
[24,     1] loss: 1040.738
[25,     1] loss: 956.546
[26,     1] loss: 945.884
[27,     1] loss: 965.476
[28,     1] loss: 955.837
[29,     1] loss: 957.890
[30,     1] loss: 982.555
[31,     1] loss: 935.308
[32,     1] loss: 911.289
[33,     1] loss: 928.660
[34,     1] loss: 914.210
[35,     1] loss: 854.488
[36,     1] loss: 994.301
[37,     1] loss: 925.918
[38,     1] loss: 903.729
[39,     1] loss: 921.511
[40,     1] loss: 831.541
[41,     1] loss: 969.520
[42,     1] loss: 882.872
[43,     1] loss: 903.460
[44,     1] loss: 917.431
[45,     1] loss: 852.957
[46,     1] loss: 904.536
[47,     1] loss: 861.236
[48,     1] loss: 805.133
[49,     1] loss: 811.940
[50,     1] loss: 978.530
[51,     1] loss: 913.739
[52,     1] loss: 836.818
[53,     1] loss: 864.546
[54,     1] loss: 823.599
[55,     1] loss: 768.142
[56,     1] loss: 1139.444
[57,     1] loss: 871.766
[58,     1] loss: 872.328
[59,     1] loss: 910.240
[60,     1] loss: 761.283
[61,     1] loss: 861.254
Early stopping applied (best metric=0.8389663100242615)
Finished Training
Total time taken: 7.2010064125061035
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1289.939
[2,     1] loss: 1297.826
[3,     1] loss: 1295.552
[4,     1] loss: 1293.346
[5,     1] loss: 1293.536
[6,     1] loss: 1291.617
[7,     1] loss: 1288.524
[8,     1] loss: 1284.813
[9,     1] loss: 1274.015
[10,     1] loss: 1249.262
[11,     1] loss: 1204.404
[12,     1] loss: 1185.971
[13,     1] loss: 1190.886
[14,     1] loss: 1104.613
[15,     1] loss: 1188.974
[16,     1] loss: 1047.007
[17,     1] loss: 1098.973
[18,     1] loss: 1060.993
[19,     1] loss: 1047.801
[20,     1] loss: 1066.704
[21,     1] loss: 1045.802
[22,     1] loss: 1033.699
[23,     1] loss: 1018.880
[24,     1] loss: 999.929
[25,     1] loss: 1009.608
[26,     1] loss: 988.389
[27,     1] loss: 962.998
[28,     1] loss: 945.938
[29,     1] loss: 930.450
[30,     1] loss: 1025.932
[31,     1] loss: 1132.884
[32,     1] loss: 987.734
[33,     1] loss: 992.092
[34,     1] loss: 1082.228
[35,     1] loss: 928.414
[36,     1] loss: 1027.803
[37,     1] loss: 981.778
[38,     1] loss: 974.845
[39,     1] loss: 887.258
[40,     1] loss: 895.690
[41,     1] loss: 890.479
[42,     1] loss: 904.807
[43,     1] loss: 891.610
[44,     1] loss: 851.112
[45,     1] loss: 813.204
[46,     1] loss: 1036.086
[47,     1] loss: 1091.491
Early stopping applied (best metric=0.8919559717178345)
Finished Training
Total time taken: 5.652006149291992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.024
[2,     1] loss: 1297.084
[3,     1] loss: 1304.098
[4,     1] loss: 1299.228
[5,     1] loss: 1292.125
[6,     1] loss: 1294.995
[7,     1] loss: 1295.423
[8,     1] loss: 1295.052
[9,     1] loss: 1293.538
[10,     1] loss: 1292.853
[11,     1] loss: 1293.635
[12,     1] loss: 1295.464
[13,     1] loss: 1292.564
[14,     1] loss: 1291.944
[15,     1] loss: 1294.493
[16,     1] loss: 1293.587
[17,     1] loss: 1291.571
[18,     1] loss: 1289.837
[19,     1] loss: 1286.042
[20,     1] loss: 1279.910
[21,     1] loss: 1269.812
[22,     1] loss: 1250.385
[23,     1] loss: 1226.508
[24,     1] loss: 1220.417
[25,     1] loss: 1183.168
[26,     1] loss: 1115.988
[27,     1] loss: 1115.126
[28,     1] loss: 1071.793
[29,     1] loss: 1084.131
[30,     1] loss: 1032.789
[31,     1] loss: 1063.371
[32,     1] loss: 1082.755
[33,     1] loss: 1031.066
[34,     1] loss: 1005.530
[35,     1] loss: 1052.251
[36,     1] loss: 1002.103
[37,     1] loss: 989.683
[38,     1] loss: 1015.592
[39,     1] loss: 1002.326
[40,     1] loss: 925.193
[41,     1] loss: 939.133
[42,     1] loss: 954.641
[43,     1] loss: 976.934
[44,     1] loss: 944.603
[45,     1] loss: 876.376
[46,     1] loss: 822.360
[47,     1] loss: 870.230
[48,     1] loss: 1576.225
[49,     1] loss: 948.511
[50,     1] loss: 1230.495
[51,     1] loss: 1081.586
[52,     1] loss: 1121.611
[53,     1] loss: 1161.881
[54,     1] loss: 1138.275
[55,     1] loss: 1068.344
[56,     1] loss: 1073.569
[57,     1] loss: 1078.264
[58,     1] loss: 1033.572
[59,     1] loss: 1052.683
[60,     1] loss: 1047.573
[61,     1] loss: 1021.356
[62,     1] loss: 999.355
[63,     1] loss: 1017.150
[64,     1] loss: 982.997
[65,     1] loss: 975.061
Early stopping applied (best metric=0.8917014598846436)
Finished Training
Total time taken: 7.745007276535034
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.590
[2,     1] loss: 1300.639
[3,     1] loss: 1294.253
[4,     1] loss: 1294.338
[5,     1] loss: 1294.729
[6,     1] loss: 1293.364
[7,     1] loss: 1293.744
[8,     1] loss: 1294.088
[9,     1] loss: 1292.745
[10,     1] loss: 1292.545
[11,     1] loss: 1292.026
[12,     1] loss: 1287.030
[13,     1] loss: 1281.908
[14,     1] loss: 1270.205
[15,     1] loss: 1255.191
[16,     1] loss: 1229.192
[17,     1] loss: 1209.181
[18,     1] loss: 1159.221
[19,     1] loss: 1140.250
[20,     1] loss: 1103.790
[21,     1] loss: 1070.335
[22,     1] loss: 1070.184
[23,     1] loss: 1195.367
[24,     1] loss: 1015.197
[25,     1] loss: 1155.213
[26,     1] loss: 1042.219
[27,     1] loss: 1098.954
[28,     1] loss: 1079.274
[29,     1] loss: 1042.205
[30,     1] loss: 1063.281
[31,     1] loss: 997.977
[32,     1] loss: 989.114
[33,     1] loss: 1000.240
[34,     1] loss: 1000.391
[35,     1] loss: 913.883
[36,     1] loss: 971.221
[37,     1] loss: 996.198
[38,     1] loss: 938.502
[39,     1] loss: 1000.786
[40,     1] loss: 920.086
[41,     1] loss: 939.050
[42,     1] loss: 936.611
[43,     1] loss: 925.890
[44,     1] loss: 982.441
[45,     1] loss: 911.144
[46,     1] loss: 910.242
[47,     1] loss: 946.852
[48,     1] loss: 895.372
[49,     1] loss: 882.465
[50,     1] loss: 901.568
[51,     1] loss: 896.377
[52,     1] loss: 941.047
[53,     1] loss: 945.374
[54,     1] loss: 899.874
[55,     1] loss: 879.345
[56,     1] loss: 902.133
[57,     1] loss: 823.928
[58,     1] loss: 915.498
[59,     1] loss: 970.668
[60,     1] loss: 830.486
[61,     1] loss: 915.624
[62,     1] loss: 877.662
[63,     1] loss: 818.674
[64,     1] loss: 896.160
[65,     1] loss: 989.885
[66,     1] loss: 903.335
[67,     1] loss: 848.691
[68,     1] loss: 820.972
[69,     1] loss: 810.660
[70,     1] loss: 862.159
[71,     1] loss: 1055.594
[72,     1] loss: 1012.323
[73,     1] loss: 863.484
Early stopping applied (best metric=0.8813621997833252)
Finished Training
Total time taken: 8.632009267807007
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1292.044
[2,     1] loss: 1311.154
[3,     1] loss: 1296.741
[4,     1] loss: 1300.147
[5,     1] loss: 1294.353
[6,     1] loss: 1295.182
[7,     1] loss: 1295.086
[8,     1] loss: 1294.613
[9,     1] loss: 1293.036
[10,     1] loss: 1293.145
[11,     1] loss: 1292.890
[12,     1] loss: 1289.992
[13,     1] loss: 1283.700
[14,     1] loss: 1272.395
[15,     1] loss: 1261.708
[16,     1] loss: 1238.147
[17,     1] loss: 1234.262
[18,     1] loss: 1200.358
[19,     1] loss: 1148.957
[20,     1] loss: 1141.000
[21,     1] loss: 1187.926
[22,     1] loss: 1067.053
[23,     1] loss: 1181.953
[24,     1] loss: 1129.329
[25,     1] loss: 1128.986
[26,     1] loss: 1072.623
[27,     1] loss: 1119.074
[28,     1] loss: 1102.173
[29,     1] loss: 1086.762
[30,     1] loss: 1048.798
[31,     1] loss: 1029.491
[32,     1] loss: 1014.278
[33,     1] loss: 986.132
[34,     1] loss: 981.583
[35,     1] loss: 1008.415
[36,     1] loss: 963.711
[37,     1] loss: 988.585
[38,     1] loss: 981.516
[39,     1] loss: 975.036
[40,     1] loss: 964.252
[41,     1] loss: 954.452
[42,     1] loss: 1122.778
[43,     1] loss: 1320.234
[44,     1] loss: 1041.877
[45,     1] loss: 1110.822
[46,     1] loss: 1137.803
[47,     1] loss: 1114.202
[48,     1] loss: 1120.358
[49,     1] loss: 1080.798
[50,     1] loss: 1107.336
[51,     1] loss: 1046.906
[52,     1] loss: 1031.203
[53,     1] loss: 1036.000
[54,     1] loss: 1013.168
[55,     1] loss: 994.853
[56,     1] loss: 1015.320
[57,     1] loss: 965.520
[58,     1] loss: 1004.152
[59,     1] loss: 985.155
[60,     1] loss: 985.161
Early stopping applied (best metric=0.8546872138977051)
Finished Training
Total time taken: 7.134006500244141
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.532
[2,     1] loss: 1302.971
[3,     1] loss: 1296.433
[4,     1] loss: 1294.835
[5,     1] loss: 1296.263
[6,     1] loss: 1292.742
[7,     1] loss: 1294.494
[8,     1] loss: 1290.839
[9,     1] loss: 1294.037
[10,     1] loss: 1291.972
[11,     1] loss: 1291.163
[12,     1] loss: 1290.347
[13,     1] loss: 1286.735
[14,     1] loss: 1282.381
[15,     1] loss: 1260.692
[16,     1] loss: 1252.205
[17,     1] loss: 1230.545
[18,     1] loss: 1184.185
[19,     1] loss: 1172.120
[20,     1] loss: 1148.941
[21,     1] loss: 1095.237
[22,     1] loss: 1228.155
[23,     1] loss: 1090.232
[24,     1] loss: 1121.629
[25,     1] loss: 1076.442
[26,     1] loss: 1124.339
[27,     1] loss: 1123.734
[28,     1] loss: 1087.653
[29,     1] loss: 1082.332
[30,     1] loss: 1064.330
[31,     1] loss: 1051.743
[32,     1] loss: 1031.198
[33,     1] loss: 1010.806
[34,     1] loss: 1062.234
[35,     1] loss: 1032.009
[36,     1] loss: 989.964
[37,     1] loss: 1012.758
[38,     1] loss: 971.757
[39,     1] loss: 1023.399
[40,     1] loss: 921.073
[41,     1] loss: 980.555
[42,     1] loss: 940.814
[43,     1] loss: 997.738
[44,     1] loss: 1031.776
[45,     1] loss: 936.505
[46,     1] loss: 1026.328
[47,     1] loss: 959.417
[48,     1] loss: 979.117
[49,     1] loss: 965.263
[50,     1] loss: 916.695
[51,     1] loss: 939.020
[52,     1] loss: 982.255
[53,     1] loss: 917.511
[54,     1] loss: 913.281
[55,     1] loss: 954.138
[56,     1] loss: 903.108
[57,     1] loss: 972.455
[58,     1] loss: 1030.497
[59,     1] loss: 912.699
[60,     1] loss: 982.319
[61,     1] loss: 914.639
[62,     1] loss: 899.343
[63,     1] loss: 1042.027
[64,     1] loss: 882.419
[65,     1] loss: 904.038
[66,     1] loss: 893.490
[67,     1] loss: 873.827
[68,     1] loss: 1024.949
[69,     1] loss: 1164.337
Early stopping applied (best metric=0.7362852096557617)
Finished Training
Total time taken: 8.190011262893677
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.548
[2,     1] loss: 1295.409
[3,     1] loss: 1292.810
[4,     1] loss: 1291.654
[5,     1] loss: 1283.319
[6,     1] loss: 1267.678
[7,     1] loss: 1245.419
[8,     1] loss: 1237.124
[9,     1] loss: 1138.973
[10,     1] loss: 1210.031
[11,     1] loss: 1129.956
[12,     1] loss: 1126.703
[13,     1] loss: 1088.565
[14,     1] loss: 1075.829
[15,     1] loss: 1023.830
[16,     1] loss: 1032.841
[17,     1] loss: 1031.047
[18,     1] loss: 1022.765
[19,     1] loss: 1022.654
[20,     1] loss: 995.575
[21,     1] loss: 998.686
[22,     1] loss: 1040.837
[23,     1] loss: 960.546
[24,     1] loss: 971.971
[25,     1] loss: 980.156
[26,     1] loss: 923.509
[27,     1] loss: 996.669
[28,     1] loss: 1098.950
[29,     1] loss: 943.842
[30,     1] loss: 997.129
[31,     1] loss: 951.501
[32,     1] loss: 939.307
[33,     1] loss: 916.524
[34,     1] loss: 929.669
[35,     1] loss: 848.549
[36,     1] loss: 923.814
[37,     1] loss: 902.526
[38,     1] loss: 865.629
[39,     1] loss: 869.027
[40,     1] loss: 870.885
[41,     1] loss: 887.742
[42,     1] loss: 946.032
[43,     1] loss: 967.078
[44,     1] loss: 873.205
[45,     1] loss: 893.088
[46,     1] loss: 796.549
[47,     1] loss: 853.335
[48,     1] loss: 920.376
[49,     1] loss: 855.127
[50,     1] loss: 813.479
[51,     1] loss: 811.434
[52,     1] loss: 843.240
[53,     1] loss: 1397.277
[54,     1] loss: 811.537
[55,     1] loss: 1049.480
[56,     1] loss: 985.371
Early stopping applied (best metric=0.8551889657974243)
Finished Training
Total time taken: 6.644005537033081
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.906
[2,     1] loss: 1302.135
[3,     1] loss: 1294.067
[4,     1] loss: 1294.825
[5,     1] loss: 1295.303
[6,     1] loss: 1293.171
[7,     1] loss: 1293.668
[8,     1] loss: 1291.185
[9,     1] loss: 1293.418
[10,     1] loss: 1290.469
[11,     1] loss: 1287.980
[12,     1] loss: 1282.118
[13,     1] loss: 1269.265
[14,     1] loss: 1251.912
[15,     1] loss: 1221.742
[16,     1] loss: 1204.313
[17,     1] loss: 1174.919
[18,     1] loss: 1155.122
[19,     1] loss: 1132.982
[20,     1] loss: 1105.843
[21,     1] loss: 1114.310
[22,     1] loss: 1132.137
[23,     1] loss: 1119.150
[24,     1] loss: 1050.751
[25,     1] loss: 1054.988
[26,     1] loss: 1032.736
[27,     1] loss: 1039.017
[28,     1] loss: 1032.998
[29,     1] loss: 978.075
[30,     1] loss: 983.698
[31,     1] loss: 1012.275
[32,     1] loss: 1127.404
[33,     1] loss: 1100.192
[34,     1] loss: 1006.874
[35,     1] loss: 1063.139
[36,     1] loss: 1053.486
[37,     1] loss: 1034.116
[38,     1] loss: 1033.430
[39,     1] loss: 991.344
[40,     1] loss: 1007.382
[41,     1] loss: 1026.480
[42,     1] loss: 954.137
[43,     1] loss: 937.270
[44,     1] loss: 937.339
[45,     1] loss: 918.499
[46,     1] loss: 889.229
[47,     1] loss: 982.781
[48,     1] loss: 1136.518
[49,     1] loss: 1015.592
[50,     1] loss: 1026.010
[51,     1] loss: 967.073
[52,     1] loss: 1018.237
[53,     1] loss: 958.140
[54,     1] loss: 954.816
[55,     1] loss: 918.857
[56,     1] loss: 1043.993
[57,     1] loss: 1082.511
[58,     1] loss: 948.477
[59,     1] loss: 985.454
[60,     1] loss: 963.949
[61,     1] loss: 994.430
[62,     1] loss: 947.622
[63,     1] loss: 986.811
[64,     1] loss: 911.814
Early stopping applied (best metric=0.8019790649414062)
Finished Training
Total time taken: 7.594007968902588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.275
[2,     1] loss: 1293.797
[3,     1] loss: 1290.774
[4,     1] loss: 1299.811
[5,     1] loss: 1294.479
[6,     1] loss: 1292.478
[7,     1] loss: 1294.166
[8,     1] loss: 1292.613
[9,     1] loss: 1292.773
[10,     1] loss: 1292.792
[11,     1] loss: 1292.134
[12,     1] loss: 1290.248
[13,     1] loss: 1287.178
[14,     1] loss: 1281.858
[15,     1] loss: 1268.791
[16,     1] loss: 1252.258
[17,     1] loss: 1233.794
[18,     1] loss: 1203.001
[19,     1] loss: 1149.765
[20,     1] loss: 1099.292
[21,     1] loss: 1088.870
[22,     1] loss: 1251.187
[23,     1] loss: 1092.220
[24,     1] loss: 1075.891
[25,     1] loss: 1054.062
[26,     1] loss: 1084.604
[27,     1] loss: 1070.632
[28,     1] loss: 1032.864
[29,     1] loss: 1053.110
[30,     1] loss: 991.490
[31,     1] loss: 996.728
[32,     1] loss: 1017.751
[33,     1] loss: 984.005
[34,     1] loss: 964.035
[35,     1] loss: 946.665
[36,     1] loss: 970.962
[37,     1] loss: 973.522
[38,     1] loss: 942.358
[39,     1] loss: 920.197
[40,     1] loss: 997.958
[41,     1] loss: 942.323
[42,     1] loss: 863.078
[43,     1] loss: 933.508
[44,     1] loss: 819.527
[45,     1] loss: 891.849
[46,     1] loss: 982.128
[47,     1] loss: 888.073
[48,     1] loss: 876.440
[49,     1] loss: 956.656
[50,     1] loss: 859.472
[51,     1] loss: 923.102
[52,     1] loss: 803.589
[53,     1] loss: 831.055
[54,     1] loss: 1045.054
[55,     1] loss: 896.748
[56,     1] loss: 881.639
[57,     1] loss: 933.110
[58,     1] loss: 890.305
[59,     1] loss: 967.841
[60,     1] loss: 886.832
[61,     1] loss: 911.092
[62,     1] loss: 873.675
[63,     1] loss: 852.159
[64,     1] loss: 863.174
[65,     1] loss: 776.026
[66,     1] loss: 793.745
[67,     1] loss: 817.755
[68,     1] loss: 912.283
[69,     1] loss: 1232.944
[70,     1] loss: 902.979
[71,     1] loss: 1054.488
[72,     1] loss: 996.785
[73,     1] loss: 1030.701
[74,     1] loss: 955.103
[75,     1] loss: 919.822
[76,     1] loss: 904.600
[77,     1] loss: 920.894
[78,     1] loss: 887.981
[79,     1] loss: 971.997
Early stopping applied (best metric=0.7926347851753235)
Finished Training
Total time taken: 9.340611457824707
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1298.303
[2,     1] loss: 1295.853
[3,     1] loss: 1293.594
[4,     1] loss: 1293.949
[5,     1] loss: 1298.805
[6,     1] loss: 1292.637
[7,     1] loss: 1287.780
[8,     1] loss: 1278.304
[9,     1] loss: 1255.720
[10,     1] loss: 1224.603
[11,     1] loss: 1210.106
[12,     1] loss: 1155.982
[13,     1] loss: 1123.400
[14,     1] loss: 1064.094
[15,     1] loss: 1070.718
[16,     1] loss: 1069.877
[17,     1] loss: 1089.181
[18,     1] loss: 1125.341
[19,     1] loss: 1036.486
[20,     1] loss: 1077.184
[21,     1] loss: 1029.664
[22,     1] loss: 1042.432
[23,     1] loss: 999.009
[24,     1] loss: 953.403
[25,     1] loss: 986.529
[26,     1] loss: 968.546
[27,     1] loss: 962.900
[28,     1] loss: 998.559
[29,     1] loss: 971.052
[30,     1] loss: 1028.849
[31,     1] loss: 931.498
[32,     1] loss: 973.167
[33,     1] loss: 926.177
[34,     1] loss: 909.855
[35,     1] loss: 874.625
[36,     1] loss: 823.244
[37,     1] loss: 852.674
[38,     1] loss: 1055.830
[39,     1] loss: 1377.533
[40,     1] loss: 951.988
[41,     1] loss: 1075.240
[42,     1] loss: 1062.042
[43,     1] loss: 1052.150
[44,     1] loss: 1070.876
[45,     1] loss: 1049.961
[46,     1] loss: 1008.019
[47,     1] loss: 1007.240
[48,     1] loss: 1016.866
[49,     1] loss: 938.525
[50,     1] loss: 930.764
Early stopping applied (best metric=0.8709918260574341)
Finished Training
Total time taken: 5.9350059032440186
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.396
[2,     1] loss: 1293.310
[3,     1] loss: 1294.009
[4,     1] loss: 1294.208
[5,     1] loss: 1292.095
[6,     1] loss: 1293.259
[7,     1] loss: 1292.302
[8,     1] loss: 1291.053
[9,     1] loss: 1288.121
[10,     1] loss: 1282.837
[11,     1] loss: 1271.871
[12,     1] loss: 1260.017
[13,     1] loss: 1230.611
[14,     1] loss: 1201.422
[15,     1] loss: 1170.711
[16,     1] loss: 1140.009
[17,     1] loss: 1134.194
[18,     1] loss: 1107.128
[19,     1] loss: 1091.590
[20,     1] loss: 1114.771
[21,     1] loss: 1107.964
[22,     1] loss: 1055.087
[23,     1] loss: 1078.016
[24,     1] loss: 1027.424
[25,     1] loss: 1043.871
[26,     1] loss: 1011.558
[27,     1] loss: 1021.981
[28,     1] loss: 962.352
[29,     1] loss: 958.551
[30,     1] loss: 931.856
[31,     1] loss: 1028.436
[32,     1] loss: 1166.534
[33,     1] loss: 978.625
[34,     1] loss: 1040.252
[35,     1] loss: 1009.710
[36,     1] loss: 1034.250
[37,     1] loss: 992.433
[38,     1] loss: 954.580
[39,     1] loss: 1016.598
[40,     1] loss: 931.935
[41,     1] loss: 925.277
[42,     1] loss: 910.399
[43,     1] loss: 972.578
[44,     1] loss: 934.325
[45,     1] loss: 987.031
[46,     1] loss: 897.839
[47,     1] loss: 924.861
[48,     1] loss: 927.308
[49,     1] loss: 865.434
[50,     1] loss: 832.115
[51,     1] loss: 804.349
[52,     1] loss: 996.250
[53,     1] loss: 1669.237
[54,     1] loss: 868.689
[55,     1] loss: 1118.430
[56,     1] loss: 1046.642
[57,     1] loss: 1078.550
[58,     1] loss: 1083.731
[59,     1] loss: 1043.280
[60,     1] loss: 1034.831
[61,     1] loss: 993.664
[62,     1] loss: 1074.096
[63,     1] loss: 1032.559
[64,     1] loss: 1066.348
[65,     1] loss: 1017.856
[66,     1] loss: 1020.937
[67,     1] loss: 992.123
[68,     1] loss: 998.718
[69,     1] loss: 949.214
Early stopping applied (best metric=0.8265326023101807)
Finished Training
Total time taken: 8.186009883880615
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.302
[2,     1] loss: 1295.931
[3,     1] loss: 1290.991
[4,     1] loss: 1294.461
[5,     1] loss: 1291.670
[6,     1] loss: 1291.176
[7,     1] loss: 1286.035
[8,     1] loss: 1281.696
[9,     1] loss: 1261.829
[10,     1] loss: 1230.536
[11,     1] loss: 1198.406
[12,     1] loss: 1178.026
[13,     1] loss: 1167.550
[14,     1] loss: 1089.452
[15,     1] loss: 1132.249
[16,     1] loss: 1100.917
[17,     1] loss: 1072.171
[18,     1] loss: 1032.778
[19,     1] loss: 1060.494
[20,     1] loss: 1034.284
[21,     1] loss: 996.604
[22,     1] loss: 977.594
[23,     1] loss: 985.130
[24,     1] loss: 995.985
[25,     1] loss: 947.440
[26,     1] loss: 964.602
[27,     1] loss: 964.688
[28,     1] loss: 981.739
[29,     1] loss: 902.914
[30,     1] loss: 939.047
[31,     1] loss: 955.583
[32,     1] loss: 867.378
[33,     1] loss: 1010.734
[34,     1] loss: 914.170
[35,     1] loss: 877.116
[36,     1] loss: 929.864
[37,     1] loss: 810.953
[38,     1] loss: 872.793
[39,     1] loss: 968.874
[40,     1] loss: 910.241
[41,     1] loss: 895.849
[42,     1] loss: 881.694
[43,     1] loss: 819.261
[44,     1] loss: 855.938
[45,     1] loss: 1053.866
[46,     1] loss: 952.655
[47,     1] loss: 905.218
[48,     1] loss: 956.702
[49,     1] loss: 1010.694
[50,     1] loss: 938.565
[51,     1] loss: 899.326
[52,     1] loss: 914.554
[53,     1] loss: 916.157
[54,     1] loss: 865.705
[55,     1] loss: 891.950
[56,     1] loss: 917.530
[57,     1] loss: 818.935
[58,     1] loss: 806.978
[59,     1] loss: 866.092
[60,     1] loss: 815.600
[61,     1] loss: 720.438
[62,     1] loss: 766.218
Early stopping applied (best metric=0.8661463260650635)
Finished Training
Total time taken: 7.34885573387146
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.511
[2,     1] loss: 1305.916
[3,     1] loss: 1300.276
[4,     1] loss: 1296.268
[5,     1] loss: 1292.274
[6,     1] loss: 1295.526
[7,     1] loss: 1294.236
[8,     1] loss: 1293.792
[9,     1] loss: 1292.365
[10,     1] loss: 1293.914
[11,     1] loss: 1290.910
[12,     1] loss: 1288.801
[13,     1] loss: 1286.167
[14,     1] loss: 1282.686
[15,     1] loss: 1278.770
[16,     1] loss: 1268.369
[17,     1] loss: 1254.987
[18,     1] loss: 1246.693
[19,     1] loss: 1222.037
[20,     1] loss: 1193.942
[21,     1] loss: 1172.412
[22,     1] loss: 1142.514
[23,     1] loss: 1127.418
[24,     1] loss: 1063.469
[25,     1] loss: 1056.889
[26,     1] loss: 1168.228
[27,     1] loss: 1094.260
[28,     1] loss: 1024.026
[29,     1] loss: 1048.298
[30,     1] loss: 1015.662
[31,     1] loss: 1043.683
[32,     1] loss: 1005.439
[33,     1] loss: 959.566
[34,     1] loss: 970.734
[35,     1] loss: 1010.475
[36,     1] loss: 1376.699
[37,     1] loss: 1033.524
[38,     1] loss: 1131.057
[39,     1] loss: 1075.880
[40,     1] loss: 1066.760
[41,     1] loss: 1101.500
[42,     1] loss: 1083.836
[43,     1] loss: 1049.862
[44,     1] loss: 1076.247
[45,     1] loss: 1052.682
[46,     1] loss: 1022.575
[47,     1] loss: 1049.178
[48,     1] loss: 1014.911
[49,     1] loss: 980.714
[50,     1] loss: 1031.570
[51,     1] loss: 989.533
[52,     1] loss: 997.838
[53,     1] loss: 951.173
[54,     1] loss: 930.738
Early stopping applied (best metric=0.8400665521621704)
Finished Training
Total time taken: 6.421005964279175
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1299.644
[2,     1] loss: 1293.114
[3,     1] loss: 1293.559
[4,     1] loss: 1291.926
[5,     1] loss: 1290.609
[6,     1] loss: 1294.801
[7,     1] loss: 1293.960
[8,     1] loss: 1292.399
[9,     1] loss: 1294.368
[10,     1] loss: 1294.840
[11,     1] loss: 1291.223
[12,     1] loss: 1291.164
[13,     1] loss: 1288.610
[14,     1] loss: 1284.977
[15,     1] loss: 1277.873
[16,     1] loss: 1265.311
[17,     1] loss: 1248.227
[18,     1] loss: 1230.550
[19,     1] loss: 1185.776
[20,     1] loss: 1146.439
[21,     1] loss: 1118.301
[22,     1] loss: 1078.677
[23,     1] loss: 1056.643
[24,     1] loss: 1076.216
[25,     1] loss: 1053.646
[26,     1] loss: 1027.505
[27,     1] loss: 1038.740
[28,     1] loss: 1024.855
[29,     1] loss: 1047.617
[30,     1] loss: 1051.025
[31,     1] loss: 953.077
[32,     1] loss: 978.419
[33,     1] loss: 928.890
[34,     1] loss: 908.704
[35,     1] loss: 922.300
[36,     1] loss: 873.070
[37,     1] loss: 919.186
[38,     1] loss: 1299.470
[39,     1] loss: 1090.233
[40,     1] loss: 1054.651
[41,     1] loss: 1087.780
[42,     1] loss: 1083.065
[43,     1] loss: 1078.772
[44,     1] loss: 1008.552
[45,     1] loss: 966.443
[46,     1] loss: 1002.382
[47,     1] loss: 996.345
[48,     1] loss: 1013.227
[49,     1] loss: 980.559
[50,     1] loss: 948.291
[51,     1] loss: 976.536
[52,     1] loss: 931.319
[53,     1] loss: 948.354
[54,     1] loss: 934.648
[55,     1] loss: 918.698
[56,     1] loss: 972.131
Early stopping applied (best metric=0.7776317596435547)
Finished Training
Total time taken: 6.865007638931274
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1303.268
[2,     1] loss: 1298.186
[3,     1] loss: 1305.669
[4,     1] loss: 1302.466
[5,     1] loss: 1297.150
[6,     1] loss: 1298.284
[7,     1] loss: 1293.995
[8,     1] loss: 1298.417
[9,     1] loss: 1295.578
[10,     1] loss: 1295.195
[11,     1] loss: 1295.674
[12,     1] loss: 1294.833
[13,     1] loss: 1295.107
[14,     1] loss: 1295.162
[15,     1] loss: 1294.959
[16,     1] loss: 1294.727
[17,     1] loss: 1294.253
[18,     1] loss: 1294.838
[19,     1] loss: 1294.273
[20,     1] loss: 1292.976
[21,     1] loss: 1292.240
[22,     1] loss: 1290.481
[23,     1] loss: 1285.365
[24,     1] loss: 1281.382
[25,     1] loss: 1273.221
[26,     1] loss: 1258.711
[27,     1] loss: 1227.208
[28,     1] loss: 1196.053
[29,     1] loss: 1154.336
[30,     1] loss: 1123.753
[31,     1] loss: 1212.345
[32,     1] loss: 1119.543
[33,     1] loss: 1044.807
[34,     1] loss: 1117.614
[35,     1] loss: 1080.741
[36,     1] loss: 1053.040
[37,     1] loss: 1083.962
[38,     1] loss: 1038.949
[39,     1] loss: 1079.595
[40,     1] loss: 1021.786
[41,     1] loss: 1044.296
[42,     1] loss: 1027.002
[43,     1] loss: 1019.381
[44,     1] loss: 990.395
[45,     1] loss: 1023.953
[46,     1] loss: 950.397
[47,     1] loss: 932.419
[48,     1] loss: 1002.309
[49,     1] loss: 1109.396
[50,     1] loss: 1171.545
[51,     1] loss: 1045.979
[52,     1] loss: 1106.666
[53,     1] loss: 1135.515
[54,     1] loss: 1063.350
[55,     1] loss: 1052.611
[56,     1] loss: 1054.174
[57,     1] loss: 965.884
[58,     1] loss: 984.298
[59,     1] loss: 968.893
[60,     1] loss: 995.075
[61,     1] loss: 953.091
[62,     1] loss: 952.009
[63,     1] loss: 952.620
[64,     1] loss: 969.192
[65,     1] loss: 909.265
[66,     1] loss: 951.246
[67,     1] loss: 930.301
[68,     1] loss: 963.785
[69,     1] loss: 962.601
[70,     1] loss: 884.809
[71,     1] loss: 930.320
[72,     1] loss: 880.234
[73,     1] loss: 951.265
[74,     1] loss: 890.584
[75,     1] loss: 946.978
[76,     1] loss: 1425.517
[77,     1] loss: 981.609
[78,     1] loss: 1112.924
[79,     1] loss: 1077.564
[80,     1] loss: 1107.404
[81,     1] loss: 1029.438
[82,     1] loss: 1031.575
[83,     1] loss: 1017.549
[84,     1] loss: 1020.591
[85,     1] loss: 985.560
[86,     1] loss: 960.038
[87,     1] loss: 958.760
[88,     1] loss: 931.178
[89,     1] loss: 944.107
[90,     1] loss: 884.986
[91,     1] loss: 1022.704
[92,     1] loss: 931.226
Early stopping applied (best metric=0.7685424089431763)
Finished Training
Total time taken: 11.332011222839355
{'Hydroxylation-K Validation Accuracy': 0.7728132387706855, 'Hydroxylation-K Validation Sensitivity': 0.6955555555555555, 'Hydroxylation-K Validation Specificity': 0.7929824561403509, 'Hydroxylation-K Validation Precision': 0.4801252582986329, 'Hydroxylation-K AUC ROC': 0.795906432748538, 'Hydroxylation-K AUC PR': 0.6141089896132541, 'Hydroxylation-K MCC': 0.43629572428230207, 'Hydroxylation-K F1': 0.559701181562251, 'Validation Loss (Hydroxylation-K)': 0.44085349241892496, 'Hydroxylation-P Validation Accuracy': 0.7996514390132481, 'Hydroxylation-P Validation Sensitivity': 0.7558201058201058, 'Hydroxylation-P Validation Specificity': 0.8091500822983689, 'Hydroxylation-P Validation Precision': 0.4702882545953114, 'Hydroxylation-P AUC ROC': 0.8390732249639178, 'Hydroxylation-P AUC PR': 0.559674843057027, 'Hydroxylation-P MCC': 0.48051193404797915, 'Hydroxylation-P F1': 0.5744138561665918, 'Validation Loss (Hydroxylation-P)': 0.3921246846516927, 'Validation Loss (total)': 0.8329781770706177, 'TimeToTrain': 7.614704545338949}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007495356681335581,
 'learning_rate_Hydroxylation-K': 0.0023296443040976564,
 'learning_rate_Hydroxylation-P': 0.008754085369829918,
 'log_base': 2.7601984932458454,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3263059508,
 'sample_weights': [1.8307981923500747, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.3853678365405,
 'weight_decay_Hydroxylation-K': 4.186617817679779,
 'weight_decay_Hydroxylation-P': 4.209210798036814}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.904
[2,     1] loss: 1279.053
[3,     1] loss: 1256.566
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002448189364492596,
 'learning_rate_Hydroxylation-K': 0.009289286367361822,
 'learning_rate_Hydroxylation-P': 0.005285408066326576,
 'log_base': 1.9136392004462903,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 150230268,
 'sample_weights': [1.6442813754918395, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.711544780527047,
 'weight_decay_Hydroxylation-K': 8.797672507772695,
 'weight_decay_Hydroxylation-P': 6.4822895601131325}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1446.562
[2,     1] loss: 1454.785
[3,     1] loss: 1452.499
[4,     1] loss: 1448.958
[5,     1] loss: 1450.837
[6,     1] loss: 1447.045
[7,     1] loss: 1452.438
[8,     1] loss: 1449.741
[9,     1] loss: 1445.454
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008793749099012284,
 'learning_rate_Hydroxylation-K': 0.0044204912416716475,
 'learning_rate_Hydroxylation-P': 0.007954605744816372,
 'log_base': 1.8171818195714144,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1824509661,
 'sample_weights': [2.57230467422306, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.985123795226297,
 'weight_decay_Hydroxylation-K': 5.020149603640652,
 'weight_decay_Hydroxylation-P': 0.7453904454857909}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1494.700
[2,     1] loss: 1517.284
[3,     1] loss: 1497.343
[4,     1] loss: 1498.840
[5,     1] loss: 1497.690
[6,     1] loss: 1496.782
[7,     1] loss: 1498.286
[8,     1] loss: 1497.096
[9,     1] loss: 1497.304
[10,     1] loss: 1495.793
[11,     1] loss: 1495.751
[12,     1] loss: 1498.090
[13,     1] loss: 1494.476
[14,     1] loss: 1495.909
[15,     1] loss: 1495.325
[16,     1] loss: 1493.835
[17,     1] loss: 1490.835
[18,     1] loss: 1483.986
[19,     1] loss: 1466.986
[20,     1] loss: 1445.608
[21,     1] loss: 1436.416
[22,     1] loss: 1382.171
[23,     1] loss: 1295.895
[24,     1] loss: 1314.745
[25,     1] loss: 1316.481
[26,     1] loss: 1209.155
[27,     1] loss: 1255.752
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003446069820032578,
 'learning_rate_Hydroxylation-K': 0.0005046725559492557,
 'learning_rate_Hydroxylation-P': 0.0038078260157375413,
 'log_base': 2.784013136024911,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3290399081,
 'sample_weights': [2.7950442016648975, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.222451949201079,
 'weight_decay_Hydroxylation-K': 7.65398106489034,
 'weight_decay_Hydroxylation-P': 2.497059966067708}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.433
[2,     1] loss: 1255.156
[3,     1] loss: 1251.523
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031077475855458073,
 'learning_rate_Hydroxylation-K': 0.002041479460449022,
 'learning_rate_Hydroxylation-P': 0.0031098991964021074,
 'log_base': 2.71778483079868,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 369632084,
 'sample_weights': [1.6304852128073561, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.789069613919759,
 'weight_decay_Hydroxylation-K': 8.301254482465643,
 'weight_decay_Hydroxylation-P': 0.6889520480297047}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.209
[2,     1] loss: 1259.375
[3,     1] loss: 1259.250
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004211967474464056,
 'learning_rate_Hydroxylation-K': 0.0023410173135310665,
 'learning_rate_Hydroxylation-P': 0.0005807907355070549,
 'log_base': 2.738656027815024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1763350041,
 'sample_weights': [1.6697484641662432, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.963881060543544,
 'weight_decay_Hydroxylation-K': 8.5957712063211,
 'weight_decay_Hydroxylation-P': 3.41441636796932}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.910
[2,     1] loss: 1260.503
[3,     1] loss: 1254.670
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002508815244959989,
 'learning_rate_Hydroxylation-K': 0.005208187955986773,
 'learning_rate_Hydroxylation-P': 0.0019890189253291895,
 'log_base': 2.2949671013241124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3998035306,
 'sample_weights': [1.6570693149417215, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.764597603742761,
 'weight_decay_Hydroxylation-K': 6.948414148068578,
 'weight_decay_Hydroxylation-P': 4.213227309424982}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1340.587
[2,     1] loss: 1332.952
[3,     1] loss: 1331.676
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007248854001063318,
 'learning_rate_Hydroxylation-K': 0.001903297952271308,
 'learning_rate_Hydroxylation-P': 0.007468493322110765,
 'log_base': 2.732339129899421,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 726002305,
 'sample_weights': [2.0096375973239144, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.530709995582313,
 'weight_decay_Hydroxylation-K': 0.9890500993135241,
 'weight_decay_Hydroxylation-P': 2.540600313335785}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.667
[2,     1] loss: 1261.605
[3,     1] loss: 1264.105
[4,     1] loss: 1258.204
[5,     1] loss: 1256.209
[6,     1] loss: 1260.216
[7,     1] loss: 1258.098
[8,     1] loss: 1260.048
[9,     1] loss: 1255.757
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036054445881177886,
 'learning_rate_Hydroxylation-K': 0.0035453590804421812,
 'learning_rate_Hydroxylation-P': 0.005717241986871175,
 'log_base': 2.0745543108177174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3286769449,
 'sample_weights': [1.660876237390127, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.295840482022753,
 'weight_decay_Hydroxylation-K': 0.48673850671574304,
 'weight_decay_Hydroxylation-P': 0.4225747573678792}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.713
[2,     1] loss: 1393.006
[3,     1] loss: 1390.366
[4,     1] loss: 1383.994
[5,     1] loss: 1385.918
[6,     1] loss: 1378.842
[7,     1] loss: 1377.519
[8,     1] loss: 1356.107
[9,     1] loss: 1337.000
[10,     1] loss: 1301.858
[11,     1] loss: 1252.962
[12,     1] loss: 1229.264
[13,     1] loss: 1212.973
[14,     1] loss: 1174.320
[15,     1] loss: 1160.542
[16,     1] loss: 1142.619
[17,     1] loss: 1139.845
[18,     1] loss: 1115.066
[19,     1] loss: 1137.983
[20,     1] loss: 1130.196
[21,     1] loss: 1113.279
[22,     1] loss: 1118.551
[23,     1] loss: 1101.757
[24,     1] loss: 1104.406
[25,     1] loss: 1074.894
[26,     1] loss: 1093.618
[27,     1] loss: 1128.640
[28,     1] loss: 1093.903
[29,     1] loss: 1020.062
[30,     1] loss: 1032.903
[31,     1] loss: 1041.416
[32,     1] loss: 1017.953
[33,     1] loss: 992.936
[34,     1] loss: 982.820
[35,     1] loss: 1004.498
[36,     1] loss: 981.732
[37,     1] loss: 985.370
[38,     1] loss: 963.853
[39,     1] loss: 1134.034
[40,     1] loss: 987.417
[41,     1] loss: 934.233
[42,     1] loss: 956.768
[43,     1] loss: 965.279
[44,     1] loss: 946.718
[45,     1] loss: 1004.704
[46,     1] loss: 907.172
[47,     1] loss: 917.916
[48,     1] loss: 863.130
[49,     1] loss: 886.771
[50,     1] loss: 885.538
[51,     1] loss: 928.933
[52,     1] loss: 787.729
[53,     1] loss: 880.068
[54,     1] loss: 886.364
Early stopping applied (best metric=0.7778035402297974)
Finished Training
Total time taken: 6.427006006240845
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.664
[2,     1] loss: 1388.475
[3,     1] loss: 1389.504
[4,     1] loss: 1389.415
[5,     1] loss: 1390.054
[6,     1] loss: 1391.875
[7,     1] loss: 1387.287
[8,     1] loss: 1384.415
[9,     1] loss: 1384.160
[10,     1] loss: 1376.559
[11,     1] loss: 1367.727
[12,     1] loss: 1353.729
[13,     1] loss: 1326.044
[14,     1] loss: 1293.681
[15,     1] loss: 1257.878
[16,     1] loss: 1250.570
[17,     1] loss: 1202.293
[18,     1] loss: 1209.748
[19,     1] loss: 1173.669
[20,     1] loss: 1151.135
[21,     1] loss: 1177.997
[22,     1] loss: 1130.001
[23,     1] loss: 1109.227
[24,     1] loss: 1110.809
[25,     1] loss: 1107.526
[26,     1] loss: 1140.599
[27,     1] loss: 1108.641
[28,     1] loss: 1079.676
[29,     1] loss: 1064.012
[30,     1] loss: 1132.356
[31,     1] loss: 1084.787
[32,     1] loss: 1051.010
[33,     1] loss: 1057.462
[34,     1] loss: 1024.554
[35,     1] loss: 1066.644
[36,     1] loss: 985.599
[37,     1] loss: 986.289
[38,     1] loss: 1087.766
[39,     1] loss: 1016.800
[40,     1] loss: 990.265
[41,     1] loss: 997.131
[42,     1] loss: 989.076
[43,     1] loss: 981.055
[44,     1] loss: 960.390
[45,     1] loss: 913.665
[46,     1] loss: 968.856
[47,     1] loss: 1045.848
[48,     1] loss: 1265.162
[49,     1] loss: 913.572
[50,     1] loss: 1097.433
[51,     1] loss: 938.891
[52,     1] loss: 1014.011
[53,     1] loss: 999.217
[54,     1] loss: 1005.269
[55,     1] loss: 983.309
[56,     1] loss: 967.854
[57,     1] loss: 904.599
[58,     1] loss: 917.939
[59,     1] loss: 894.760
[60,     1] loss: 874.262
[61,     1] loss: 906.010
[62,     1] loss: 885.735
[63,     1] loss: 833.082
[64,     1] loss: 823.879
[65,     1] loss: 836.538
[66,     1] loss: 849.213
[67,     1] loss: 856.438
[68,     1] loss: 830.366
[69,     1] loss: 928.054
[70,     1] loss: 805.345
[71,     1] loss: 891.333
[72,     1] loss: 813.984
[73,     1] loss: 921.739
[74,     1] loss: 793.308
[75,     1] loss: 874.804
[76,     1] loss: 755.488
[77,     1] loss: 826.930
[78,     1] loss: 797.862
[79,     1] loss: 759.923
[80,     1] loss: 754.998
[81,     1] loss: 716.144
[82,     1] loss: 732.364
[83,     1] loss: 697.000
[84,     1] loss: 603.588
[85,     1] loss: 745.409
[86,     1] loss: 715.228
[87,     1] loss: 685.869
[88,     1] loss: 951.354
[89,     1] loss: 854.283
[90,     1] loss: 668.319
[91,     1] loss: 796.699
[92,     1] loss: 682.152
[93,     1] loss: 722.938
[94,     1] loss: 713.470
[95,     1] loss: 754.653
[96,     1] loss: 634.282
[97,     1] loss: 739.901
[98,     1] loss: 624.453
[99,     1] loss: 674.408
[100,     1] loss: 662.147
[101,     1] loss: 592.209
[102,     1] loss: 613.194
[103,     1] loss: 584.397
[104,     1] loss: 652.350
[105,     1] loss: 606.405
[106,     1] loss: 566.874
[107,     1] loss: 562.295
[108,     1] loss: 682.924
[109,     1] loss: 504.384
[110,     1] loss: 543.917
Early stopping applied (best metric=0.639968752861023)
Finished Training
Total time taken: 12.946013450622559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.775
[2,     1] loss: 1396.713
[3,     1] loss: 1393.082
[4,     1] loss: 1388.600
[5,     1] loss: 1389.236
[6,     1] loss: 1391.571
[7,     1] loss: 1385.688
[8,     1] loss: 1379.656
[9,     1] loss: 1374.809
[10,     1] loss: 1363.491
[11,     1] loss: 1349.944
[12,     1] loss: 1327.828
[13,     1] loss: 1292.582
[14,     1] loss: 1272.192
[15,     1] loss: 1229.759
[16,     1] loss: 1202.750
[17,     1] loss: 1189.347
[18,     1] loss: 1127.141
[19,     1] loss: 1117.810
[20,     1] loss: 1168.346
[21,     1] loss: 1134.975
[22,     1] loss: 1129.188
[23,     1] loss: 1187.860
[24,     1] loss: 1075.660
[25,     1] loss: 1179.115
[26,     1] loss: 1065.671
[27,     1] loss: 1126.151
[28,     1] loss: 1038.797
[29,     1] loss: 1085.163
[30,     1] loss: 1071.665
[31,     1] loss: 1046.297
[32,     1] loss: 1098.345
[33,     1] loss: 1016.630
[34,     1] loss: 1037.428
[35,     1] loss: 991.984
[36,     1] loss: 1038.197
[37,     1] loss: 945.595
[38,     1] loss: 985.656
[39,     1] loss: 999.869
[40,     1] loss: 986.214
[41,     1] loss: 976.563
[42,     1] loss: 975.672
[43,     1] loss: 1011.213
[44,     1] loss: 950.670
[45,     1] loss: 946.854
[46,     1] loss: 936.733
[47,     1] loss: 888.029
[48,     1] loss: 974.206
[49,     1] loss: 874.957
[50,     1] loss: 873.797
[51,     1] loss: 874.721
[52,     1] loss: 913.585
[53,     1] loss: 826.106
[54,     1] loss: 881.655
[55,     1] loss: 861.710
[56,     1] loss: 836.846
[57,     1] loss: 859.597
[58,     1] loss: 767.779
[59,     1] loss: 828.684
[60,     1] loss: 727.500
Early stopping applied (best metric=0.7863434553146362)
Finished Training
Total time taken: 7.160005807876587
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.160
[2,     1] loss: 1389.140
[3,     1] loss: 1393.031
[4,     1] loss: 1387.734
[5,     1] loss: 1388.372
[6,     1] loss: 1389.779
[7,     1] loss: 1383.731
[8,     1] loss: 1376.159
[9,     1] loss: 1375.297
[10,     1] loss: 1359.885
[11,     1] loss: 1344.233
[12,     1] loss: 1311.854
[13,     1] loss: 1274.118
[14,     1] loss: 1240.302
[15,     1] loss: 1212.824
[16,     1] loss: 1201.219
[17,     1] loss: 1201.134
[18,     1] loss: 1232.921
[19,     1] loss: 1163.570
[20,     1] loss: 1227.056
[21,     1] loss: 1142.840
[22,     1] loss: 1197.316
[23,     1] loss: 1149.619
[24,     1] loss: 1176.054
[25,     1] loss: 1160.580
[26,     1] loss: 1157.717
[27,     1] loss: 1146.984
[28,     1] loss: 1138.320
[29,     1] loss: 1147.005
[30,     1] loss: 1060.802
[31,     1] loss: 1068.290
[32,     1] loss: 1059.292
[33,     1] loss: 1105.051
[34,     1] loss: 1049.310
[35,     1] loss: 1052.148
[36,     1] loss: 1022.029
[37,     1] loss: 1058.373
[38,     1] loss: 1038.984
[39,     1] loss: 1013.921
[40,     1] loss: 1020.810
[41,     1] loss: 1024.633
[42,     1] loss: 1013.461
[43,     1] loss: 1035.755
[44,     1] loss: 969.524
[45,     1] loss: 978.458
[46,     1] loss: 963.793
[47,     1] loss: 930.756
[48,     1] loss: 955.271
[49,     1] loss: 967.404
[50,     1] loss: 915.222
[51,     1] loss: 864.584
[52,     1] loss: 861.751
[53,     1] loss: 900.520
[54,     1] loss: 937.469
[55,     1] loss: 946.335
[56,     1] loss: 844.374
[57,     1] loss: 855.851
[58,     1] loss: 889.498
[59,     1] loss: 838.082
[60,     1] loss: 865.244
[61,     1] loss: 812.095
[62,     1] loss: 908.766
[63,     1] loss: 915.529
[64,     1] loss: 797.310
[65,     1] loss: 854.083
[66,     1] loss: 777.686
[67,     1] loss: 921.042
[68,     1] loss: 775.664
[69,     1] loss: 850.177
[70,     1] loss: 733.055
[71,     1] loss: 710.993
Early stopping applied (best metric=0.6851035356521606)
Finished Training
Total time taken: 8.392009973526001
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1390.523
[2,     1] loss: 1391.926
[3,     1] loss: 1389.364
[4,     1] loss: 1385.274
[5,     1] loss: 1386.703
[6,     1] loss: 1380.179
[7,     1] loss: 1376.181
[8,     1] loss: 1352.356
[9,     1] loss: 1321.310
[10,     1] loss: 1282.618
[11,     1] loss: 1265.301
[12,     1] loss: 1223.585
[13,     1] loss: 1207.233
[14,     1] loss: 1173.534
[15,     1] loss: 1177.428
[16,     1] loss: 1153.740
[17,     1] loss: 1136.947
[18,     1] loss: 1223.446
[19,     1] loss: 1139.911
[20,     1] loss: 1163.516
[21,     1] loss: 1111.865
[22,     1] loss: 1156.219
[23,     1] loss: 1098.512
[24,     1] loss: 1133.023
[25,     1] loss: 1133.558
[26,     1] loss: 1005.312
[27,     1] loss: 1066.511
[28,     1] loss: 1075.453
[29,     1] loss: 1055.097
[30,     1] loss: 1011.785
[31,     1] loss: 1014.796
[32,     1] loss: 1027.027
[33,     1] loss: 994.663
[34,     1] loss: 990.207
[35,     1] loss: 952.090
[36,     1] loss: 925.951
[37,     1] loss: 1025.718
[38,     1] loss: 953.681
[39,     1] loss: 908.625
[40,     1] loss: 962.743
[41,     1] loss: 868.151
[42,     1] loss: 940.733
[43,     1] loss: 943.170
[44,     1] loss: 922.802
[45,     1] loss: 914.152
[46,     1] loss: 839.533
[47,     1] loss: 921.650
[48,     1] loss: 922.883
[49,     1] loss: 1012.539
[50,     1] loss: 970.387
[51,     1] loss: 888.821
[52,     1] loss: 878.368
[53,     1] loss: 858.397
[54,     1] loss: 862.148
[55,     1] loss: 828.074
[56,     1] loss: 842.895
[57,     1] loss: 782.971
[58,     1] loss: 835.864
[59,     1] loss: 781.561
[60,     1] loss: 804.642
[61,     1] loss: 920.097
[62,     1] loss: 893.224
[63,     1] loss: 727.452
[64,     1] loss: 867.544
[65,     1] loss: 776.114
[66,     1] loss: 803.043
[67,     1] loss: 769.327
[68,     1] loss: 766.492
[69,     1] loss: 794.960
[70,     1] loss: 618.922
[71,     1] loss: 772.564
[72,     1] loss: 935.599
[73,     1] loss: 695.850
[74,     1] loss: 692.924
[75,     1] loss: 717.045
[76,     1] loss: 644.641
Early stopping applied (best metric=0.760184645652771)
Finished Training
Total time taken: 9.031007528305054
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.306
[2,     1] loss: 1388.946
[3,     1] loss: 1395.760
[4,     1] loss: 1394.724
[5,     1] loss: 1389.081
[6,     1] loss: 1393.546
[7,     1] loss: 1391.116
[8,     1] loss: 1390.360
[9,     1] loss: 1386.782
[10,     1] loss: 1386.374
[11,     1] loss: 1381.340
[12,     1] loss: 1375.634
[13,     1] loss: 1363.565
[14,     1] loss: 1352.035
[15,     1] loss: 1335.318
[16,     1] loss: 1307.777
[17,     1] loss: 1271.528
[18,     1] loss: 1231.802
[19,     1] loss: 1234.205
[20,     1] loss: 1216.041
[21,     1] loss: 1159.724
[22,     1] loss: 1163.736
[23,     1] loss: 1175.576
[24,     1] loss: 1145.044
[25,     1] loss: 1168.956
[26,     1] loss: 1110.214
[27,     1] loss: 1143.563
[28,     1] loss: 1102.004
[29,     1] loss: 1112.141
[30,     1] loss: 1109.199
[31,     1] loss: 1124.034
[32,     1] loss: 1067.643
[33,     1] loss: 1114.627
[34,     1] loss: 1070.195
[35,     1] loss: 1046.659
[36,     1] loss: 1115.274
[37,     1] loss: 1008.813
[38,     1] loss: 1017.404
[39,     1] loss: 1023.739
[40,     1] loss: 1002.367
[41,     1] loss: 997.212
[42,     1] loss: 1010.685
[43,     1] loss: 1001.813
[44,     1] loss: 927.605
[45,     1] loss: 965.409
[46,     1] loss: 924.061
[47,     1] loss: 958.797
[48,     1] loss: 904.759
[49,     1] loss: 890.648
[50,     1] loss: 914.820
[51,     1] loss: 967.087
[52,     1] loss: 1003.753
[53,     1] loss: 825.517
[54,     1] loss: 1017.790
[55,     1] loss: 912.584
[56,     1] loss: 925.127
[57,     1] loss: 868.333
[58,     1] loss: 924.678
[59,     1] loss: 832.303
[60,     1] loss: 861.930
[61,     1] loss: 836.414
[62,     1] loss: 806.744
[63,     1] loss: 797.495
[64,     1] loss: 793.490
[65,     1] loss: 752.157
[66,     1] loss: 842.542
[67,     1] loss: 1036.843
[68,     1] loss: 938.418
[69,     1] loss: 847.994
[70,     1] loss: 885.594
[71,     1] loss: 855.921
[72,     1] loss: 805.713
Early stopping applied (best metric=0.7316194176673889)
Finished Training
Total time taken: 8.529008150100708
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.630
[2,     1] loss: 1392.510
[3,     1] loss: 1391.976
[4,     1] loss: 1392.102
[5,     1] loss: 1388.490
[6,     1] loss: 1390.485
[7,     1] loss: 1389.156
[8,     1] loss: 1388.627
[9,     1] loss: 1382.918
[10,     1] loss: 1375.028
[11,     1] loss: 1358.805
[12,     1] loss: 1341.029
[13,     1] loss: 1316.068
[14,     1] loss: 1266.210
[15,     1] loss: 1225.852
[16,     1] loss: 1246.194
[17,     1] loss: 1162.720
[18,     1] loss: 1147.116
[19,     1] loss: 1178.419
[20,     1] loss: 1161.193
[21,     1] loss: 1136.878
[22,     1] loss: 1120.656
[23,     1] loss: 1101.420
[24,     1] loss: 1107.583
[25,     1] loss: 1058.048
[26,     1] loss: 1077.966
[27,     1] loss: 1095.975
[28,     1] loss: 1050.488
[29,     1] loss: 1035.605
[30,     1] loss: 1041.746
[31,     1] loss: 1030.514
[32,     1] loss: 1061.318
[33,     1] loss: 1012.321
[34,     1] loss: 989.684
[35,     1] loss: 1061.617
[36,     1] loss: 961.757
[37,     1] loss: 1040.735
[38,     1] loss: 952.563
[39,     1] loss: 1050.147
[40,     1] loss: 918.189
[41,     1] loss: 991.801
[42,     1] loss: 956.413
[43,     1] loss: 929.920
[44,     1] loss: 918.426
[45,     1] loss: 978.179
[46,     1] loss: 915.436
[47,     1] loss: 1020.244
[48,     1] loss: 923.383
[49,     1] loss: 944.030
[50,     1] loss: 863.483
[51,     1] loss: 940.384
[52,     1] loss: 892.909
Early stopping applied (best metric=0.8781514167785645)
Finished Training
Total time taken: 6.159006118774414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.448
[2,     1] loss: 1390.695
[3,     1] loss: 1394.044
[4,     1] loss: 1385.644
[5,     1] loss: 1389.106
[6,     1] loss: 1388.176
[7,     1] loss: 1381.028
[8,     1] loss: 1373.656
[9,     1] loss: 1358.137
[10,     1] loss: 1338.451
[11,     1] loss: 1302.811
[12,     1] loss: 1278.008
[13,     1] loss: 1244.402
[14,     1] loss: 1201.808
[15,     1] loss: 1195.954
[16,     1] loss: 1155.576
[17,     1] loss: 1185.649
[18,     1] loss: 1161.855
[19,     1] loss: 1124.707
[20,     1] loss: 1157.960
[21,     1] loss: 1145.959
[22,     1] loss: 1146.381
[23,     1] loss: 1114.686
[24,     1] loss: 1079.546
[25,     1] loss: 1105.998
[26,     1] loss: 1114.674
[27,     1] loss: 1052.240
[28,     1] loss: 1034.927
[29,     1] loss: 1079.233
[30,     1] loss: 1059.194
[31,     1] loss: 996.534
[32,     1] loss: 1093.659
[33,     1] loss: 1116.158
[34,     1] loss: 1074.996
[35,     1] loss: 1012.276
[36,     1] loss: 1028.378
[37,     1] loss: 974.830
[38,     1] loss: 988.261
[39,     1] loss: 920.467
[40,     1] loss: 982.498
[41,     1] loss: 927.941
[42,     1] loss: 1032.234
[43,     1] loss: 859.271
[44,     1] loss: 877.987
[45,     1] loss: 893.271
[46,     1] loss: 870.516
[47,     1] loss: 902.545
[48,     1] loss: 837.017
[49,     1] loss: 893.674
[50,     1] loss: 851.417
[51,     1] loss: 826.853
Early stopping applied (best metric=0.8450534343719482)
Finished Training
Total time taken: 6.087006092071533
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.657
[2,     1] loss: 1391.190
[3,     1] loss: 1389.600
[4,     1] loss: 1389.352
[5,     1] loss: 1386.359
[6,     1] loss: 1392.437
[7,     1] loss: 1386.464
[8,     1] loss: 1380.866
[9,     1] loss: 1378.507
[10,     1] loss: 1369.337
[11,     1] loss: 1347.554
[12,     1] loss: 1340.357
[13,     1] loss: 1311.925
[14,     1] loss: 1294.900
[15,     1] loss: 1249.682
[16,     1] loss: 1219.693
[17,     1] loss: 1189.941
[18,     1] loss: 1204.773
[19,     1] loss: 1202.097
[20,     1] loss: 1142.448
[21,     1] loss: 1174.573
[22,     1] loss: 1140.186
[23,     1] loss: 1133.215
[24,     1] loss: 1129.776
[25,     1] loss: 1107.101
[26,     1] loss: 1138.737
[27,     1] loss: 1102.637
[28,     1] loss: 1080.761
[29,     1] loss: 1073.125
[30,     1] loss: 1091.395
[31,     1] loss: 1101.926
[32,     1] loss: 986.132
[33,     1] loss: 1077.408
[34,     1] loss: 988.219
[35,     1] loss: 999.558
[36,     1] loss: 993.853
[37,     1] loss: 1062.254
[38,     1] loss: 1104.761
[39,     1] loss: 967.423
[40,     1] loss: 1032.210
[41,     1] loss: 963.866
[42,     1] loss: 1031.040
[43,     1] loss: 1007.910
[44,     1] loss: 956.639
[45,     1] loss: 952.554
[46,     1] loss: 953.117
[47,     1] loss: 884.437
[48,     1] loss: 928.165
[49,     1] loss: 869.754
[50,     1] loss: 862.216
[51,     1] loss: 884.272
[52,     1] loss: 851.204
[53,     1] loss: 791.899
[54,     1] loss: 843.618
[55,     1] loss: 805.328
[56,     1] loss: 742.899
[57,     1] loss: 759.476
[58,     1] loss: 829.967
[59,     1] loss: 792.490
[60,     1] loss: 837.069
[61,     1] loss: 859.580
[62,     1] loss: 782.809
Early stopping applied (best metric=0.676902174949646)
Finished Training
Total time taken: 7.421008348464966
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1391.544
[2,     1] loss: 1392.053
[3,     1] loss: 1392.090
[4,     1] loss: 1389.796
[5,     1] loss: 1393.169
[6,     1] loss: 1387.513
[7,     1] loss: 1386.274
[8,     1] loss: 1388.956
[9,     1] loss: 1385.571
[10,     1] loss: 1375.313
[11,     1] loss: 1359.791
[12,     1] loss: 1341.796
[13,     1] loss: 1301.654
[14,     1] loss: 1282.240
[15,     1] loss: 1234.121
[16,     1] loss: 1229.547
[17,     1] loss: 1189.101
[18,     1] loss: 1198.610
[19,     1] loss: 1185.090
[20,     1] loss: 1177.976
[21,     1] loss: 1205.153
[22,     1] loss: 1105.076
[23,     1] loss: 1094.484
[24,     1] loss: 1097.226
[25,     1] loss: 1058.932
[26,     1] loss: 1092.227
[27,     1] loss: 1030.824
[28,     1] loss: 1067.811
[29,     1] loss: 1086.798
[30,     1] loss: 994.399
[31,     1] loss: 1053.085
[32,     1] loss: 1123.822
[33,     1] loss: 1034.523
[34,     1] loss: 1098.261
[35,     1] loss: 1062.062
[36,     1] loss: 1019.946
[37,     1] loss: 1005.165
[38,     1] loss: 1023.659
[39,     1] loss: 924.181
[40,     1] loss: 1016.044
[41,     1] loss: 954.060
[42,     1] loss: 963.868
[43,     1] loss: 929.865
[44,     1] loss: 978.698
[45,     1] loss: 897.980
[46,     1] loss: 952.951
[47,     1] loss: 918.240
[48,     1] loss: 911.983
[49,     1] loss: 934.463
[50,     1] loss: 853.310
Early stopping applied (best metric=0.7507504224777222)
Finished Training
Total time taken: 5.942006349563599
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.655
[2,     1] loss: 1390.312
[3,     1] loss: 1390.828
[4,     1] loss: 1386.883
[5,     1] loss: 1384.645
[6,     1] loss: 1380.754
[7,     1] loss: 1373.314
[8,     1] loss: 1361.500
[9,     1] loss: 1342.971
[10,     1] loss: 1309.455
[11,     1] loss: 1284.381
[12,     1] loss: 1249.993
[13,     1] loss: 1199.296
[14,     1] loss: 1206.494
[15,     1] loss: 1161.535
[16,     1] loss: 1155.495
[17,     1] loss: 1156.995
[18,     1] loss: 1199.317
[19,     1] loss: 1143.904
[20,     1] loss: 1125.670
[21,     1] loss: 1167.191
[22,     1] loss: 1096.614
[23,     1] loss: 1094.248
[24,     1] loss: 1064.197
[25,     1] loss: 1116.576
[26,     1] loss: 1092.503
[27,     1] loss: 1043.744
[28,     1] loss: 1018.548
[29,     1] loss: 1036.906
[30,     1] loss: 1057.737
[31,     1] loss: 1054.650
[32,     1] loss: 1001.563
[33,     1] loss: 1002.482
[34,     1] loss: 985.682
[35,     1] loss: 1017.622
[36,     1] loss: 1025.677
[37,     1] loss: 963.199
[38,     1] loss: 973.926
[39,     1] loss: 989.821
[40,     1] loss: 926.440
[41,     1] loss: 934.373
[42,     1] loss: 927.476
[43,     1] loss: 907.564
[44,     1] loss: 939.308
[45,     1] loss: 945.403
[46,     1] loss: 892.627
[47,     1] loss: 887.191
[48,     1] loss: 850.796
[49,     1] loss: 849.322
[50,     1] loss: 913.567
[51,     1] loss: 1068.292
[52,     1] loss: 976.081
[53,     1] loss: 866.894
[54,     1] loss: 908.457
[55,     1] loss: 870.266
[56,     1] loss: 853.527
[57,     1] loss: 788.958
[58,     1] loss: 819.139
[59,     1] loss: 784.326
[60,     1] loss: 796.551
[61,     1] loss: 777.723
[62,     1] loss: 738.244
[63,     1] loss: 789.440
[64,     1] loss: 756.019
[65,     1] loss: 749.102
[66,     1] loss: 686.767
[67,     1] loss: 697.367
[68,     1] loss: 693.209
[69,     1] loss: 897.578
[70,     1] loss: 1118.387
[71,     1] loss: 888.557
[72,     1] loss: 757.401
[73,     1] loss: 801.814
[74,     1] loss: 839.259
[75,     1] loss: 794.534
[76,     1] loss: 825.702
[77,     1] loss: 767.094
[78,     1] loss: 750.690
[79,     1] loss: 691.095
[80,     1] loss: 704.148
[81,     1] loss: 722.976
[82,     1] loss: 696.711
[83,     1] loss: 655.724
Early stopping applied (best metric=0.8573497533798218)
Finished Training
Total time taken: 9.852009534835815
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.137
[2,     1] loss: 1390.035
[3,     1] loss: 1394.705
[4,     1] loss: 1384.673
[5,     1] loss: 1382.825
[6,     1] loss: 1391.296
[7,     1] loss: 1388.409
[8,     1] loss: 1385.830
[9,     1] loss: 1369.346
[10,     1] loss: 1353.228
[11,     1] loss: 1324.942
[12,     1] loss: 1304.983
[13,     1] loss: 1262.570
[14,     1] loss: 1233.054
[15,     1] loss: 1208.391
[16,     1] loss: 1162.293
[17,     1] loss: 1165.863
[18,     1] loss: 1181.603
[19,     1] loss: 1217.288
[20,     1] loss: 1155.921
[21,     1] loss: 1151.417
[22,     1] loss: 1119.200
[23,     1] loss: 1156.008
[24,     1] loss: 1120.829
[25,     1] loss: 1114.344
[26,     1] loss: 1136.407
[27,     1] loss: 1106.120
[28,     1] loss: 1086.373
[29,     1] loss: 1068.678
[30,     1] loss: 1034.159
[31,     1] loss: 1054.178
[32,     1] loss: 1024.158
[33,     1] loss: 1011.772
[34,     1] loss: 1015.805
[35,     1] loss: 1006.559
[36,     1] loss: 984.231
[37,     1] loss: 1051.799
[38,     1] loss: 989.593
[39,     1] loss: 989.281
[40,     1] loss: 940.635
[41,     1] loss: 980.125
[42,     1] loss: 959.411
[43,     1] loss: 889.607
[44,     1] loss: 943.919
[45,     1] loss: 979.108
[46,     1] loss: 872.663
[47,     1] loss: 868.265
[48,     1] loss: 946.704
[49,     1] loss: 850.648
[50,     1] loss: 809.492
[51,     1] loss: 862.101
[52,     1] loss: 852.890
[53,     1] loss: 802.657
Early stopping applied (best metric=0.8301761746406555)
Finished Training
Total time taken: 6.301006317138672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.726
[2,     1] loss: 1391.310
[3,     1] loss: 1391.792
[4,     1] loss: 1396.229
[5,     1] loss: 1391.717
[6,     1] loss: 1386.878
[7,     1] loss: 1386.103
[8,     1] loss: 1381.055
[9,     1] loss: 1381.499
[10,     1] loss: 1374.306
[11,     1] loss: 1360.970
[12,     1] loss: 1338.220
[13,     1] loss: 1319.705
[14,     1] loss: 1276.717
[15,     1] loss: 1262.024
[16,     1] loss: 1246.318
[17,     1] loss: 1223.464
[18,     1] loss: 1207.287
[19,     1] loss: 1209.768
[20,     1] loss: 1169.795
[21,     1] loss: 1115.475
[22,     1] loss: 1165.679
[23,     1] loss: 1129.477
[24,     1] loss: 1098.531
[25,     1] loss: 1150.711
[26,     1] loss: 1142.571
[27,     1] loss: 1079.125
[28,     1] loss: 1066.608
[29,     1] loss: 1181.497
[30,     1] loss: 1109.169
[31,     1] loss: 1051.343
[32,     1] loss: 1023.506
[33,     1] loss: 1043.060
[34,     1] loss: 1001.773
[35,     1] loss: 1000.847
[36,     1] loss: 1061.806
[37,     1] loss: 1030.573
[38,     1] loss: 1003.731
[39,     1] loss: 1015.560
[40,     1] loss: 1038.027
[41,     1] loss: 963.973
[42,     1] loss: 1013.148
[43,     1] loss: 912.638
[44,     1] loss: 974.256
[45,     1] loss: 862.915
[46,     1] loss: 950.132
[47,     1] loss: 910.121
[48,     1] loss: 980.472
[49,     1] loss: 941.057
[50,     1] loss: 877.280
[51,     1] loss: 891.000
[52,     1] loss: 839.053
[53,     1] loss: 930.376
[54,     1] loss: 900.828
[55,     1] loss: 853.206
[56,     1] loss: 929.560
[57,     1] loss: 861.892
[58,     1] loss: 830.419
[59,     1] loss: 852.052
[60,     1] loss: 847.783
[61,     1] loss: 784.125
[62,     1] loss: 826.604
[63,     1] loss: 832.813
[64,     1] loss: 796.970
[65,     1] loss: 925.247
[66,     1] loss: 679.314
[67,     1] loss: 890.766
[68,     1] loss: 788.380
[69,     1] loss: 790.413
[70,     1] loss: 734.935
[71,     1] loss: 811.086
[72,     1] loss: 707.359
[73,     1] loss: 766.743
[74,     1] loss: 736.580
[75,     1] loss: 810.779
[76,     1] loss: 872.574
[77,     1] loss: 665.482
[78,     1] loss: 985.790
[79,     1] loss: 672.491
[80,     1] loss: 764.091
[81,     1] loss: 674.582
[82,     1] loss: 705.227
[83,     1] loss: 714.000
[84,     1] loss: 684.233
[85,     1] loss: 648.241
[86,     1] loss: 705.157
[87,     1] loss: 610.264
[88,     1] loss: 732.439
[89,     1] loss: 601.188
[90,     1] loss: 526.796
[91,     1] loss: 604.785
[92,     1] loss: 580.465
[93,     1] loss: 541.335
[94,     1] loss: 536.768
[95,     1] loss: 498.345
[96,     1] loss: 526.965
[97,     1] loss: 502.641
[98,     1] loss: 586.894
[99,     1] loss: 1322.490
[100,     1] loss: 827.913
[101,     1] loss: 639.427
[102,     1] loss: 745.947
[103,     1] loss: 757.881
[104,     1] loss: 702.861
Early stopping applied (best metric=0.6839375495910645)
Finished Training
Total time taken: 12.291012048721313
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.669
[2,     1] loss: 1391.360
[3,     1] loss: 1392.381
[4,     1] loss: 1391.201
[5,     1] loss: 1390.576
[6,     1] loss: 1387.244
[7,     1] loss: 1389.722
[8,     1] loss: 1387.973
[9,     1] loss: 1386.846
[10,     1] loss: 1382.573
[11,     1] loss: 1382.751
[12,     1] loss: 1372.429
[13,     1] loss: 1351.514
[14,     1] loss: 1331.760
[15,     1] loss: 1308.585
[16,     1] loss: 1253.524
[17,     1] loss: 1241.925
[18,     1] loss: 1164.396
[19,     1] loss: 1161.351
[20,     1] loss: 1156.171
[21,     1] loss: 1153.435
[22,     1] loss: 1185.824
[23,     1] loss: 1159.333
[24,     1] loss: 1171.980
[25,     1] loss: 1109.853
[26,     1] loss: 1113.072
[27,     1] loss: 1060.743
[28,     1] loss: 1121.669
[29,     1] loss: 1056.150
[30,     1] loss: 1120.459
[31,     1] loss: 1088.969
[32,     1] loss: 1081.249
[33,     1] loss: 1072.698
[34,     1] loss: 996.737
[35,     1] loss: 1077.829
[36,     1] loss: 1041.960
[37,     1] loss: 982.340
[38,     1] loss: 992.515
[39,     1] loss: 978.398
[40,     1] loss: 1028.649
[41,     1] loss: 1018.203
[42,     1] loss: 983.504
[43,     1] loss: 956.815
[44,     1] loss: 955.614
[45,     1] loss: 943.696
[46,     1] loss: 903.374
[47,     1] loss: 931.932
[48,     1] loss: 997.138
[49,     1] loss: 1048.706
[50,     1] loss: 1029.095
[51,     1] loss: 852.417
[52,     1] loss: 924.884
[53,     1] loss: 872.396
[54,     1] loss: 885.347
[55,     1] loss: 901.630
[56,     1] loss: 844.063
[57,     1] loss: 837.151
[58,     1] loss: 871.972
[59,     1] loss: 929.594
[60,     1] loss: 808.779
[61,     1] loss: 795.145
[62,     1] loss: 951.480
[63,     1] loss: 833.296
[64,     1] loss: 849.557
[65,     1] loss: 833.379
[66,     1] loss: 887.578
[67,     1] loss: 837.150
[68,     1] loss: 861.156
[69,     1] loss: 872.517
[70,     1] loss: 771.551
[71,     1] loss: 832.257
[72,     1] loss: 752.052
[73,     1] loss: 747.003
[74,     1] loss: 842.818
[75,     1] loss: 736.995
[76,     1] loss: 689.286
[77,     1] loss: 734.503
[78,     1] loss: 661.070
[79,     1] loss: 682.301
[80,     1] loss: 675.203
[81,     1] loss: 723.730
[82,     1] loss: 584.099
[83,     1] loss: 742.111
[84,     1] loss: 759.083
[85,     1] loss: 620.626
[86,     1] loss: 710.131
[87,     1] loss: 594.278
[88,     1] loss: 592.491
Early stopping applied (best metric=0.8169024586677551)
Finished Training
Total time taken: 10.417013168334961
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1389.933
[2,     1] loss: 1393.608
[3,     1] loss: 1388.729
[4,     1] loss: 1386.282
[5,     1] loss: 1384.820
[6,     1] loss: 1375.693
[7,     1] loss: 1357.142
[8,     1] loss: 1326.993
[9,     1] loss: 1298.668
[10,     1] loss: 1251.085
[11,     1] loss: 1219.713
[12,     1] loss: 1193.190
[13,     1] loss: 1233.565
[14,     1] loss: 1216.313
[15,     1] loss: 1192.501
[16,     1] loss: 1171.894
[17,     1] loss: 1156.897
[18,     1] loss: 1160.443
[19,     1] loss: 1132.319
[20,     1] loss: 1158.507
[21,     1] loss: 1176.127
[22,     1] loss: 1120.940
[23,     1] loss: 1159.813
[24,     1] loss: 1147.940
[25,     1] loss: 1065.184
[26,     1] loss: 1151.907
[27,     1] loss: 1097.908
[28,     1] loss: 1079.582
[29,     1] loss: 1078.713
[30,     1] loss: 1035.357
[31,     1] loss: 1042.100
[32,     1] loss: 1072.000
[33,     1] loss: 995.635
[34,     1] loss: 1039.477
[35,     1] loss: 976.503
[36,     1] loss: 1026.471
[37,     1] loss: 981.451
[38,     1] loss: 987.080
[39,     1] loss: 963.839
[40,     1] loss: 984.515
[41,     1] loss: 939.919
[42,     1] loss: 919.889
[43,     1] loss: 907.518
[44,     1] loss: 948.359
[45,     1] loss: 902.848
[46,     1] loss: 986.359
[47,     1] loss: 1224.122
[48,     1] loss: 842.579
[49,     1] loss: 1056.196
[50,     1] loss: 903.106
[51,     1] loss: 926.653
[52,     1] loss: 1034.100
[53,     1] loss: 984.836
[54,     1] loss: 930.490
[55,     1] loss: 953.047
[56,     1] loss: 924.880
[57,     1] loss: 891.456
[58,     1] loss: 904.457
[59,     1] loss: 832.073
[60,     1] loss: 901.700
[61,     1] loss: 861.634
[62,     1] loss: 801.588
[63,     1] loss: 791.816
[64,     1] loss: 832.772
[65,     1] loss: 790.568
[66,     1] loss: 734.678
[67,     1] loss: 799.147
[68,     1] loss: 702.228
[69,     1] loss: 779.136
[70,     1] loss: 835.368
[71,     1] loss: 741.166
[72,     1] loss: 650.212
[73,     1] loss: 683.703
[74,     1] loss: 710.971
[75,     1] loss: 670.492
[76,     1] loss: 654.687
[77,     1] loss: 708.791
[78,     1] loss: 941.091
[79,     1] loss: 1242.061
[80,     1] loss: 643.900
[81,     1] loss: 1015.499
[82,     1] loss: 835.099
[83,     1] loss: 811.470
[84,     1] loss: 907.005
[85,     1] loss: 890.079
[86,     1] loss: 772.600
[87,     1] loss: 827.729
[88,     1] loss: 865.227
[89,     1] loss: 689.838
[90,     1] loss: 763.482
[91,     1] loss: 737.609
[92,     1] loss: 728.827
[93,     1] loss: 655.327
[94,     1] loss: 686.662
[95,     1] loss: 698.281
[96,     1] loss: 606.285
[97,     1] loss: 638.911
[98,     1] loss: 652.461
[99,     1] loss: 580.253
[100,     1] loss: 630.599
[101,     1] loss: 595.313
[102,     1] loss: 765.377
[103,     1] loss: 737.823
[104,     1] loss: 576.886
[105,     1] loss: 657.513
[106,     1] loss: 638.424
[107,     1] loss: 511.086
[108,     1] loss: 642.080
[109,     1] loss: 482.358
[110,     1] loss: 589.234
[111,     1] loss: 760.550
Early stopping applied (best metric=0.6285677552223206)
Finished Training
Total time taken: 13.11201286315918
{'Hydroxylation-K Validation Accuracy': 0.7997635933806146, 'Hydroxylation-K Validation Sensitivity': 0.78, 'Hydroxylation-K Validation Specificity': 0.8052631578947368, 'Hydroxylation-K Validation Precision': 0.5114161818573584, 'Hydroxylation-K AUC ROC': 0.8399220272904484, 'Hydroxylation-K AUC PR': 0.6680553601094398, 'Hydroxylation-K MCC': 0.5107939899706344, 'Hydroxylation-K F1': 0.6146774867644433, 'Validation Loss (Hydroxylation-K)': 0.39054984947045646, 'Hydroxylation-P Validation Accuracy': 0.7949948056782228, 'Hydroxylation-P Validation Sensitivity': 0.792010582010582, 'Hydroxylation-P Validation Specificity': 0.7957279664821189, 'Hydroxylation-P Validation Precision': 0.45836336620815943, 'Hydroxylation-P AUC ROC': 0.8610695086720075, 'Hydroxylation-P AUC PR': 0.6225371978629552, 'Hydroxylation-P MCC': 0.4879813740112036, 'Hydroxylation-P F1': 0.5784889960458848, 'Validation Loss (Hydroxylation-P)': 0.366037784020106, 'Validation Loss (total)': 0.7565876324971517, 'TimeToTrain': 8.671142117182415}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005064946003205616,
 'learning_rate_Hydroxylation-K': 0.00485263164533081,
 'learning_rate_Hydroxylation-P': 0.001173816746623535,
 'log_base': 2.2471868174488123,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1129145787,
 'sample_weights': [2.2894003636921396, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.151708642801949,
 'weight_decay_Hydroxylation-K': 6.313113914256033,
 'weight_decay_Hydroxylation-P': 7.30799790728487}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.790
[2,     1] loss: 1346.831
[3,     1] loss: 1339.317
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007239884987162012,
 'learning_rate_Hydroxylation-K': 0.00577587981623497,
 'learning_rate_Hydroxylation-P': 0.004731435641544604,
 'log_base': 2.0405258829859307,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3349670731,
 'sample_weights': [2.0618576969250904, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.454855111628996,
 'weight_decay_Hydroxylation-K': 5.543436382253445,
 'weight_decay_Hydroxylation-P': 7.190460643829799}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.640
[2,     1] loss: 1402.258
[3,     1] loss: 1403.089
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00312716180418424,
 'learning_rate_Hydroxylation-K': 0.00539387452058937,
 'learning_rate_Hydroxylation-P': 0.0013661790587353969,
 'log_base': 2.537027145094898,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1274260230,
 'sample_weights': [2.3407535759409464, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.80449198186356,
 'weight_decay_Hydroxylation-K': 8.241166758030369,
 'weight_decay_Hydroxylation-P': 2.2115600759524843}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.201
[2,     1] loss: 1288.723
[3,     1] loss: 1286.272
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003655007721533941,
 'learning_rate_Hydroxylation-K': 0.0006977509086240821,
 'learning_rate_Hydroxylation-P': 0.0051109302164659785,
 'log_base': 1.2473565250735121,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 804549603,
 'sample_weights': [1.7931855372193228, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.607576582626677,
 'weight_decay_Hydroxylation-K': 5.12610358706983,
 'weight_decay_Hydroxylation-P': 1.4372554010945253}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2504.892
[2,     1] loss: 2501.984
[3,     1] loss: 2505.938
[4,     1] loss: 2507.416
[5,     1] loss: 2496.665
[6,     1] loss: 2495.836
[7,     1] loss: 2494.809
[8,     1] loss: 2492.022
[9,     1] loss: 2480.259
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004354615259250331,
 'learning_rate_Hydroxylation-K': 0.00475102811292692,
 'learning_rate_Hydroxylation-P': 0.0011268271397713219,
 'log_base': 2.9305957715216815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1316732612,
 'sample_weights': [7.553134602446773, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.776300254742667,
 'weight_decay_Hydroxylation-K': 5.062226278474013,
 'weight_decay_Hydroxylation-P': 4.353759058607884}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.974
[2,     1] loss: 1239.155
[3,     1] loss: 1234.304
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038463086604857252,
 'learning_rate_Hydroxylation-K': 0.0017701547589994264,
 'learning_rate_Hydroxylation-P': 0.0027261808302598816,
 'log_base': 1.080090743031458,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1110045340,
 'sample_weights': [1.552673213536741, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.228681807712205,
 'weight_decay_Hydroxylation-K': 0.6188738345215954,
 'weight_decay_Hydroxylation-P': 2.589451377135738}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7040.970
[2,     1] loss: 7060.828
[3,     1] loss: 7013.855
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026192490527655217,
 'learning_rate_Hydroxylation-K': 0.0025939623149858404,
 'learning_rate_Hydroxylation-P': 0.0022453505797949847,
 'log_base': 2.9483414471093092,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1468261476,
 'sample_weights': [21.66839990216665, 2.7086543319928924],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.906001266323312,
 'weight_decay_Hydroxylation-K': 5.413545202276644,
 'weight_decay_Hydroxylation-P': 1.3343212884522575}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.023
[2,     1] loss: 1240.437
[3,     1] loss: 1232.155
[4,     1] loss: 1231.014
[5,     1] loss: 1231.959
[6,     1] loss: 1229.801
[7,     1] loss: 1221.180
[8,     1] loss: 1214.914
[9,     1] loss: 1203.724
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018151241515521265,
 'learning_rate_Hydroxylation-K': 0.008525531885429321,
 'learning_rate_Hydroxylation-P': 0.002444038768305624,
 'log_base': 2.4797181492443787,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1447092068,
 'sample_weights': [1.5440039567728359, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.399963016422621,
 'weight_decay_Hydroxylation-K': 7.693009420696261,
 'weight_decay_Hydroxylation-P': 8.411521987840484}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1298.222
[2,     1] loss: 1300.319
[3,     1] loss: 1299.683
[4,     1] loss: 1294.823
[5,     1] loss: 1295.858
[6,     1] loss: 1293.848
[7,     1] loss: 1290.113
[8,     1] loss: 1287.174
[9,     1] loss: 1295.900
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 20,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036054445881177886,
 'learning_rate_Hydroxylation-K': 0.0035453590804421812,
 'learning_rate_Hydroxylation-P': 0.005717241986871175,
 'log_base': 2.0745543108177174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 423524270,
 'sample_weights': [1.8383004074194869, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.295840482022753,
 'weight_decay_Hydroxylation-K': 0.48673850671574304,
 'weight_decay_Hydroxylation-P': 0.4225747573678792}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.014
[2,     1] loss: 1389.719
[3,     1] loss: 1389.384
[4,     1] loss: 1398.767
[5,     1] loss: 1391.016
[6,     1] loss: 1382.446
[7,     1] loss: 1376.541
[8,     1] loss: 1372.034
[9,     1] loss: 1355.620
[10,     1] loss: 1336.745
[11,     1] loss: 1311.153
[12,     1] loss: 1285.184
[13,     1] loss: 1267.932
[14,     1] loss: 1207.479
[15,     1] loss: 1197.222
[16,     1] loss: 1179.244
[17,     1] loss: 1183.305
[18,     1] loss: 1155.357
[19,     1] loss: 1191.243
[20,     1] loss: 1147.339
[21,     1] loss: 1132.208
[22,     1] loss: 1110.377
[23,     1] loss: 1083.444
[24,     1] loss: 1050.005
[25,     1] loss: 1099.346
[26,     1] loss: 1048.563
[27,     1] loss: 981.003
[28,     1] loss: 1013.242
[29,     1] loss: 1055.430
[30,     1] loss: 988.241
[31,     1] loss: 1071.678
[32,     1] loss: 1062.549
[33,     1] loss: 986.664
[34,     1] loss: 981.790
[35,     1] loss: 981.957
[36,     1] loss: 940.378
[37,     1] loss: 1011.119
[38,     1] loss: 906.391
[39,     1] loss: 938.392
[40,     1] loss: 935.550
[41,     1] loss: 911.264
[42,     1] loss: 857.096
[43,     1] loss: 894.701
[44,     1] loss: 886.599
[45,     1] loss: 833.457
[46,     1] loss: 888.001
[47,     1] loss: 824.656
[48,     1] loss: 918.338
[49,     1] loss: 991.434
[50,     1] loss: 817.038
[51,     1] loss: 889.739
[52,     1] loss: 821.746
[53,     1] loss: 873.752
[54,     1] loss: 778.155
[55,     1] loss: 815.158
[56,     1] loss: 771.270
[57,     1] loss: 782.327
[58,     1] loss: 779.427
[59,     1] loss: 742.909
[60,     1] loss: 781.238
[61,     1] loss: 936.774
[62,     1] loss: 749.345
Early stopping applied (best metric=0.8061231970787048)
Finished Training
Total time taken: 7.913006067276001
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.594
[2,     1] loss: 1392.963
[3,     1] loss: 1391.664
[4,     1] loss: 1389.612
[5,     1] loss: 1388.279
[6,     1] loss: 1385.328
[7,     1] loss: 1378.566
[8,     1] loss: 1372.105
[9,     1] loss: 1348.171
[10,     1] loss: 1319.421
[11,     1] loss: 1284.366
[12,     1] loss: 1285.369
[13,     1] loss: 1258.772
[14,     1] loss: 1196.631
[15,     1] loss: 1210.161
[16,     1] loss: 1190.654
[17,     1] loss: 1195.276
[18,     1] loss: 1169.391
[19,     1] loss: 1170.546
[20,     1] loss: 1132.495
[21,     1] loss: 1191.780
[22,     1] loss: 1101.022
[23,     1] loss: 1125.783
[24,     1] loss: 1103.953
[25,     1] loss: 1045.214
[26,     1] loss: 1080.199
[27,     1] loss: 1072.828
[28,     1] loss: 1025.400
[29,     1] loss: 1047.453
[30,     1] loss: 1009.624
[31,     1] loss: 979.093
[32,     1] loss: 1002.290
[33,     1] loss: 1030.339
[34,     1] loss: 964.099
[35,     1] loss: 1037.416
[36,     1] loss: 966.594
[37,     1] loss: 963.199
[38,     1] loss: 955.787
[39,     1] loss: 917.861
[40,     1] loss: 931.861
[41,     1] loss: 928.106
[42,     1] loss: 881.567
[43,     1] loss: 828.813
[44,     1] loss: 802.729
[45,     1] loss: 801.555
[46,     1] loss: 820.512
[47,     1] loss: 840.647
[48,     1] loss: 989.331
[49,     1] loss: 1285.481
[50,     1] loss: 817.892
[51,     1] loss: 994.226
[52,     1] loss: 948.192
[53,     1] loss: 945.692
[54,     1] loss: 995.426
[55,     1] loss: 951.692
Early stopping applied (best metric=0.877810001373291)
Finished Training
Total time taken: 7.047007083892822
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.486
[2,     1] loss: 1394.390
[3,     1] loss: 1390.304
[4,     1] loss: 1394.888
[5,     1] loss: 1393.826
[6,     1] loss: 1387.846
[7,     1] loss: 1382.756
[8,     1] loss: 1383.673
[9,     1] loss: 1376.498
[10,     1] loss: 1365.173
[11,     1] loss: 1345.129
[12,     1] loss: 1314.519
[13,     1] loss: 1280.863
[14,     1] loss: 1231.019
[15,     1] loss: 1197.081
[16,     1] loss: 1175.528
[17,     1] loss: 1176.151
[18,     1] loss: 1152.221
[19,     1] loss: 1118.067
[20,     1] loss: 1158.373
[21,     1] loss: 1092.028
[22,     1] loss: 1125.597
[23,     1] loss: 1159.419
[24,     1] loss: 1091.383
[25,     1] loss: 1064.239
[26,     1] loss: 1072.188
[27,     1] loss: 1056.286
[28,     1] loss: 1037.377
[29,     1] loss: 1053.630
[30,     1] loss: 994.311
[31,     1] loss: 1073.779
[32,     1] loss: 1029.493
[33,     1] loss: 999.167
[34,     1] loss: 1002.622
[35,     1] loss: 940.231
[36,     1] loss: 1040.791
[37,     1] loss: 928.202
[38,     1] loss: 954.016
[39,     1] loss: 954.098
[40,     1] loss: 953.995
[41,     1] loss: 932.326
[42,     1] loss: 854.389
[43,     1] loss: 961.266
[44,     1] loss: 815.808
[45,     1] loss: 968.928
[46,     1] loss: 869.006
[47,     1] loss: 908.738
[48,     1] loss: 846.749
[49,     1] loss: 933.928
[50,     1] loss: 842.923
[51,     1] loss: 908.693
[52,     1] loss: 820.846
[53,     1] loss: 816.374
[54,     1] loss: 802.298
[55,     1] loss: 832.165
[56,     1] loss: 783.822
Early stopping applied (best metric=0.8601663708686829)
Finished Training
Total time taken: 7.214004278182983
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1391.219
[2,     1] loss: 1390.458
[3,     1] loss: 1388.913
[4,     1] loss: 1392.547
[5,     1] loss: 1388.080
[6,     1] loss: 1389.285
[7,     1] loss: 1384.077
[8,     1] loss: 1383.824
[9,     1] loss: 1383.368
[10,     1] loss: 1367.942
[11,     1] loss: 1350.752
[12,     1] loss: 1318.159
[13,     1] loss: 1291.183
[14,     1] loss: 1261.587
[15,     1] loss: 1229.744
[16,     1] loss: 1216.460
[17,     1] loss: 1171.892
[18,     1] loss: 1143.160
[19,     1] loss: 1146.246
[20,     1] loss: 1212.794
[21,     1] loss: 1171.732
[22,     1] loss: 1118.871
[23,     1] loss: 1100.551
[24,     1] loss: 1103.861
[25,     1] loss: 1107.559
[26,     1] loss: 1074.557
[27,     1] loss: 1011.300
[28,     1] loss: 1055.672
[29,     1] loss: 1007.058
[30,     1] loss: 1045.148
[31,     1] loss: 1035.144
[32,     1] loss: 1070.195
[33,     1] loss: 1031.857
[34,     1] loss: 1069.256
[35,     1] loss: 969.890
[36,     1] loss: 990.565
[37,     1] loss: 978.728
[38,     1] loss: 974.211
[39,     1] loss: 913.528
[40,     1] loss: 935.373
[41,     1] loss: 932.788
[42,     1] loss: 863.816
[43,     1] loss: 858.099
[44,     1] loss: 865.669
[45,     1] loss: 936.312
[46,     1] loss: 825.936
[47,     1] loss: 863.903
[48,     1] loss: 932.512
[49,     1] loss: 869.229
[50,     1] loss: 831.660
[51,     1] loss: 818.739
[52,     1] loss: 796.128
[53,     1] loss: 813.854
[54,     1] loss: 751.399
Early stopping applied (best metric=0.8446498513221741)
Finished Training
Total time taken: 7.020007848739624
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1392.722
[2,     1] loss: 1391.245
[3,     1] loss: 1389.151
[4,     1] loss: 1391.979
[5,     1] loss: 1390.123
[6,     1] loss: 1390.707
[7,     1] loss: 1389.870
[8,     1] loss: 1387.739
[9,     1] loss: 1387.016
[10,     1] loss: 1380.440
[11,     1] loss: 1381.610
[12,     1] loss: 1363.980
[13,     1] loss: 1345.449
[14,     1] loss: 1327.492
[15,     1] loss: 1294.891
[16,     1] loss: 1269.256
[17,     1] loss: 1225.286
[18,     1] loss: 1213.866
[19,     1] loss: 1187.193
[20,     1] loss: 1206.977
[21,     1] loss: 1219.643
[22,     1] loss: 1200.437
[23,     1] loss: 1206.429
[24,     1] loss: 1182.668
[25,     1] loss: 1168.521
[26,     1] loss: 1150.814
[27,     1] loss: 1192.002
[28,     1] loss: 1156.099
[29,     1] loss: 1153.878
[30,     1] loss: 1109.099
[31,     1] loss: 1101.304
[32,     1] loss: 1090.493
[33,     1] loss: 1080.901
[34,     1] loss: 1048.121
[35,     1] loss: 1050.282
[36,     1] loss: 1081.312
[37,     1] loss: 997.288
[38,     1] loss: 1033.967
[39,     1] loss: 1065.228
[40,     1] loss: 1033.105
[41,     1] loss: 1083.645
[42,     1] loss: 1015.534
[43,     1] loss: 1021.926
[44,     1] loss: 1033.030
[45,     1] loss: 1027.368
[46,     1] loss: 1014.568
[47,     1] loss: 1013.360
[48,     1] loss: 906.162
[49,     1] loss: 983.115
[50,     1] loss: 884.806
[51,     1] loss: 918.203
[52,     1] loss: 918.110
[53,     1] loss: 984.080
[54,     1] loss: 990.562
[55,     1] loss: 820.993
[56,     1] loss: 971.523
[57,     1] loss: 876.724
[58,     1] loss: 910.577
[59,     1] loss: 885.349
[60,     1] loss: 862.919
[61,     1] loss: 836.573
[62,     1] loss: 898.675
[63,     1] loss: 955.638
[64,     1] loss: 817.256
[65,     1] loss: 935.701
[66,     1] loss: 793.024
[67,     1] loss: 844.303
[68,     1] loss: 812.262
[69,     1] loss: 819.901
[70,     1] loss: 860.928
[71,     1] loss: 823.952
[72,     1] loss: 766.371
[73,     1] loss: 820.852
[74,     1] loss: 842.140
[75,     1] loss: 690.903
[76,     1] loss: 797.736
[77,     1] loss: 699.943
[78,     1] loss: 681.958
[79,     1] loss: 705.568
[80,     1] loss: 754.009
[81,     1] loss: 645.455
[82,     1] loss: 751.799
[83,     1] loss: 900.574
[84,     1] loss: 645.429
[85,     1] loss: 776.077
[86,     1] loss: 806.692
[87,     1] loss: 685.938
[88,     1] loss: 770.256
[89,     1] loss: 721.968
Early stopping applied (best metric=0.7060345411300659)
Finished Training
Total time taken: 11.528008222579956
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.381
[2,     1] loss: 1394.644
[3,     1] loss: 1394.134
[4,     1] loss: 1390.586
[5,     1] loss: 1391.232
[6,     1] loss: 1387.100
[7,     1] loss: 1383.331
[8,     1] loss: 1381.392
[9,     1] loss: 1387.128
[10,     1] loss: 1369.381
[11,     1] loss: 1349.835
[12,     1] loss: 1327.972
[13,     1] loss: 1299.797
[14,     1] loss: 1262.769
[15,     1] loss: 1237.807
[16,     1] loss: 1225.103
[17,     1] loss: 1188.786
[18,     1] loss: 1122.537
[19,     1] loss: 1130.299
[20,     1] loss: 1087.363
[21,     1] loss: 1092.520
[22,     1] loss: 1107.534
[23,     1] loss: 1111.370
[24,     1] loss: 1068.065
[25,     1] loss: 1059.953
[26,     1] loss: 1046.890
[27,     1] loss: 981.801
[28,     1] loss: 992.602
[29,     1] loss: 1059.002
[30,     1] loss: 1031.460
[31,     1] loss: 1052.271
[32,     1] loss: 1041.647
[33,     1] loss: 1013.351
[34,     1] loss: 1006.538
[35,     1] loss: 956.341
[36,     1] loss: 978.950
[37,     1] loss: 954.115
[38,     1] loss: 945.774
[39,     1] loss: 948.121
[40,     1] loss: 941.204
[41,     1] loss: 891.133
[42,     1] loss: 973.399
[43,     1] loss: 1053.955
[44,     1] loss: 908.275
[45,     1] loss: 901.720
[46,     1] loss: 890.514
[47,     1] loss: 894.001
[48,     1] loss: 854.371
[49,     1] loss: 889.898
[50,     1] loss: 852.165
[51,     1] loss: 847.040
[52,     1] loss: 872.635
[53,     1] loss: 786.921
[54,     1] loss: 762.071
[55,     1] loss: 815.015
[56,     1] loss: 975.594
[57,     1] loss: 1168.301
[58,     1] loss: 832.557
[59,     1] loss: 968.157
[60,     1] loss: 949.506
[61,     1] loss: 877.625
[62,     1] loss: 935.086
[63,     1] loss: 960.461
[64,     1] loss: 921.601
[65,     1] loss: 867.005
[66,     1] loss: 877.680
[67,     1] loss: 838.029
[68,     1] loss: 842.938
[69,     1] loss: 784.047
[70,     1] loss: 886.128
[71,     1] loss: 757.309
[72,     1] loss: 762.185
[73,     1] loss: 764.969
[74,     1] loss: 718.772
Early stopping applied (best metric=0.8236361742019653)
Finished Training
Total time taken: 9.667005777359009
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.364
[2,     1] loss: 1390.684
[3,     1] loss: 1393.067
[4,     1] loss: 1383.833
[5,     1] loss: 1385.057
[6,     1] loss: 1380.051
[7,     1] loss: 1363.653
[8,     1] loss: 1346.203
[9,     1] loss: 1307.046
[10,     1] loss: 1248.789
[11,     1] loss: 1208.820
[12,     1] loss: 1193.784
[13,     1] loss: 1158.365
[14,     1] loss: 1156.484
[15,     1] loss: 1179.191
[16,     1] loss: 1144.514
[17,     1] loss: 1124.595
[18,     1] loss: 1140.293
[19,     1] loss: 1076.832
[20,     1] loss: 1122.619
[21,     1] loss: 1104.709
[22,     1] loss: 1105.667
[23,     1] loss: 1058.705
[24,     1] loss: 1035.375
[25,     1] loss: 1060.526
[26,     1] loss: 997.368
[27,     1] loss: 1024.383
[28,     1] loss: 1082.909
[29,     1] loss: 1005.280
[30,     1] loss: 980.121
[31,     1] loss: 1013.561
[32,     1] loss: 972.865
[33,     1] loss: 979.414
[34,     1] loss: 925.960
[35,     1] loss: 900.487
[36,     1] loss: 1001.279
[37,     1] loss: 969.839
[38,     1] loss: 972.059
[39,     1] loss: 950.503
[40,     1] loss: 948.282
[41,     1] loss: 953.307
[42,     1] loss: 852.081
[43,     1] loss: 905.797
Early stopping applied (best metric=0.9112920761108398)
Finished Training
Total time taken: 5.662003517150879
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.147
[2,     1] loss: 1386.620
[3,     1] loss: 1386.038
[4,     1] loss: 1390.302
[5,     1] loss: 1389.107
[6,     1] loss: 1383.498
[7,     1] loss: 1377.203
[8,     1] loss: 1369.540
[9,     1] loss: 1349.418
[10,     1] loss: 1329.501
[11,     1] loss: 1301.903
[12,     1] loss: 1263.854
[13,     1] loss: 1244.709
[14,     1] loss: 1214.215
[15,     1] loss: 1181.450
[16,     1] loss: 1146.308
[17,     1] loss: 1200.262
[18,     1] loss: 1142.469
[19,     1] loss: 1128.458
[20,     1] loss: 1076.889
[21,     1] loss: 1067.962
[22,     1] loss: 1146.053
[23,     1] loss: 1074.115
[24,     1] loss: 1085.388
[25,     1] loss: 1086.956
[26,     1] loss: 1069.792
[27,     1] loss: 1080.401
[28,     1] loss: 1024.303
[29,     1] loss: 1061.435
[30,     1] loss: 1032.203
[31,     1] loss: 1035.812
[32,     1] loss: 969.432
[33,     1] loss: 1035.652
[34,     1] loss: 1015.838
[35,     1] loss: 1010.847
[36,     1] loss: 980.065
[37,     1] loss: 1003.013
[38,     1] loss: 937.413
[39,     1] loss: 985.897
[40,     1] loss: 918.895
[41,     1] loss: 969.281
[42,     1] loss: 905.443
[43,     1] loss: 967.959
[44,     1] loss: 946.205
[45,     1] loss: 882.094
[46,     1] loss: 928.405
[47,     1] loss: 913.963
[48,     1] loss: 895.760
[49,     1] loss: 955.772
[50,     1] loss: 832.651
[51,     1] loss: 929.941
[52,     1] loss: 852.482
[53,     1] loss: 844.359
[54,     1] loss: 847.955
[55,     1] loss: 761.458
[56,     1] loss: 795.133
[57,     1] loss: 772.538
[58,     1] loss: 801.206
[59,     1] loss: 820.572
[60,     1] loss: 765.021
[61,     1] loss: 759.647
[62,     1] loss: 821.853
[63,     1] loss: 819.017
[64,     1] loss: 1033.506
[65,     1] loss: 761.332
Early stopping applied (best metric=0.8693372011184692)
Finished Training
Total time taken: 8.515008449554443
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.416
[2,     1] loss: 1392.087
[3,     1] loss: 1386.512
[4,     1] loss: 1389.931
[5,     1] loss: 1391.898
[6,     1] loss: 1384.735
[7,     1] loss: 1382.488
[8,     1] loss: 1379.370
[9,     1] loss: 1379.314
[10,     1] loss: 1367.826
[11,     1] loss: 1338.788
[12,     1] loss: 1302.057
[13,     1] loss: 1267.510
[14,     1] loss: 1237.323
[15,     1] loss: 1208.708
[16,     1] loss: 1187.265
[17,     1] loss: 1154.008
[18,     1] loss: 1144.910
[19,     1] loss: 1165.582
[20,     1] loss: 1187.766
[21,     1] loss: 1107.744
[22,     1] loss: 1122.872
[23,     1] loss: 1079.486
[24,     1] loss: 1147.097
[25,     1] loss: 1102.686
[26,     1] loss: 1112.669
[27,     1] loss: 1063.064
[28,     1] loss: 1086.439
[29,     1] loss: 1107.949
[30,     1] loss: 1087.147
[31,     1] loss: 1037.118
[32,     1] loss: 1029.182
[33,     1] loss: 1002.402
[34,     1] loss: 1031.178
[35,     1] loss: 964.137
[36,     1] loss: 1001.934
[37,     1] loss: 956.023
[38,     1] loss: 973.517
[39,     1] loss: 940.510
[40,     1] loss: 953.033
[41,     1] loss: 951.500
[42,     1] loss: 884.592
[43,     1] loss: 935.890
[44,     1] loss: 908.159
[45,     1] loss: 881.364
[46,     1] loss: 854.261
[47,     1] loss: 850.633
[48,     1] loss: 826.095
[49,     1] loss: 811.357
[50,     1] loss: 883.833
[51,     1] loss: 845.027
[52,     1] loss: 809.748
[53,     1] loss: 732.670
[54,     1] loss: 805.800
[55,     1] loss: 743.417
[56,     1] loss: 721.418
[57,     1] loss: 761.438
[58,     1] loss: 942.601
Early stopping applied (best metric=0.8571795225143433)
Finished Training
Total time taken: 7.658004522323608
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1402.198
[2,     1] loss: 1393.511
[3,     1] loss: 1389.044
[4,     1] loss: 1387.839
[5,     1] loss: 1379.935
[6,     1] loss: 1365.416
[7,     1] loss: 1341.412
[8,     1] loss: 1313.751
[9,     1] loss: 1247.973
[10,     1] loss: 1218.767
[11,     1] loss: 1192.504
[12,     1] loss: 1163.604
[13,     1] loss: 1161.485
[14,     1] loss: 1149.363
[15,     1] loss: 1196.197
[16,     1] loss: 1143.234
[17,     1] loss: 1110.828
[18,     1] loss: 1129.869
[19,     1] loss: 1101.293
[20,     1] loss: 1112.779
[21,     1] loss: 1093.164
[22,     1] loss: 1079.803
[23,     1] loss: 1050.360
[24,     1] loss: 1010.871
[25,     1] loss: 1067.995
[26,     1] loss: 1023.037
[27,     1] loss: 1058.388
[28,     1] loss: 1002.322
[29,     1] loss: 1019.482
[30,     1] loss: 1010.091
[31,     1] loss: 1010.800
[32,     1] loss: 983.240
[33,     1] loss: 972.819
[34,     1] loss: 1017.135
[35,     1] loss: 937.151
[36,     1] loss: 920.179
[37,     1] loss: 970.859
[38,     1] loss: 941.654
[39,     1] loss: 944.122
[40,     1] loss: 862.623
[41,     1] loss: 867.314
[42,     1] loss: 864.292
[43,     1] loss: 909.198
[44,     1] loss: 861.819
[45,     1] loss: 903.666
[46,     1] loss: 963.957
[47,     1] loss: 887.237
[48,     1] loss: 881.082
[49,     1] loss: 897.796
[50,     1] loss: 840.174
[51,     1] loss: 871.089
[52,     1] loss: 779.705
[53,     1] loss: 841.257
[54,     1] loss: 764.179
[55,     1] loss: 964.026
[56,     1] loss: 810.963
[57,     1] loss: 792.732
[58,     1] loss: 823.209
[59,     1] loss: 755.023
[60,     1] loss: 811.922
[61,     1] loss: 802.041
[62,     1] loss: 819.869
[63,     1] loss: 731.636
[64,     1] loss: 787.942
[65,     1] loss: 666.260
[66,     1] loss: 755.335
[67,     1] loss: 744.343
[68,     1] loss: 729.166
[69,     1] loss: 699.852
[70,     1] loss: 841.962
[71,     1] loss: 663.554
[72,     1] loss: 739.156
[73,     1] loss: 751.399
[74,     1] loss: 651.362
[75,     1] loss: 767.925
[76,     1] loss: 631.343
[77,     1] loss: 680.155
[78,     1] loss: 689.424
[79,     1] loss: 587.948
[80,     1] loss: 678.001
Early stopping applied (best metric=0.8138634562492371)
Finished Training
Total time taken: 10.49800968170166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.591
[2,     1] loss: 1394.584
[3,     1] loss: 1384.108
[4,     1] loss: 1388.415
[5,     1] loss: 1384.382
[6,     1] loss: 1383.836
[7,     1] loss: 1375.777
[8,     1] loss: 1363.768
[9,     1] loss: 1329.825
[10,     1] loss: 1286.751
[11,     1] loss: 1254.002
[12,     1] loss: 1227.112
[13,     1] loss: 1217.990
[14,     1] loss: 1167.889
[15,     1] loss: 1197.244
[16,     1] loss: 1142.065
[17,     1] loss: 1123.022
[18,     1] loss: 1147.344
[19,     1] loss: 1150.204
[20,     1] loss: 1145.753
[21,     1] loss: 1077.673
[22,     1] loss: 1102.924
[23,     1] loss: 1077.184
[24,     1] loss: 1130.161
[25,     1] loss: 1077.764
[26,     1] loss: 1095.726
[27,     1] loss: 1012.475
[28,     1] loss: 1025.909
[29,     1] loss: 1004.976
[30,     1] loss: 983.369
[31,     1] loss: 949.318
[32,     1] loss: 969.601
[33,     1] loss: 996.023
[34,     1] loss: 966.319
[35,     1] loss: 1052.447
[36,     1] loss: 933.649
[37,     1] loss: 1006.130
[38,     1] loss: 914.022
[39,     1] loss: 982.702
[40,     1] loss: 905.420
[41,     1] loss: 938.916
[42,     1] loss: 918.351
[43,     1] loss: 854.795
[44,     1] loss: 856.480
[45,     1] loss: 877.144
[46,     1] loss: 889.460
[47,     1] loss: 848.252
[48,     1] loss: 821.605
[49,     1] loss: 833.306
[50,     1] loss: 803.729
[51,     1] loss: 803.964
[52,     1] loss: 787.554
[53,     1] loss: 749.269
[54,     1] loss: 864.511
[55,     1] loss: 1198.302
[56,     1] loss: 929.279
[57,     1] loss: 862.047
[58,     1] loss: 843.508
[59,     1] loss: 905.650
[60,     1] loss: 881.731
[61,     1] loss: 850.492
[62,     1] loss: 788.474
[63,     1] loss: 724.258
[64,     1] loss: 813.674
[65,     1] loss: 743.002
[66,     1] loss: 698.323
[67,     1] loss: 698.793
[68,     1] loss: 785.568
[69,     1] loss: 728.729
[70,     1] loss: 650.439
[71,     1] loss: 702.996
Early stopping applied (best metric=0.8598363399505615)
Finished Training
Total time taken: 9.474010705947876
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.606
[2,     1] loss: 1390.119
[3,     1] loss: 1389.852
[4,     1] loss: 1387.100
[5,     1] loss: 1387.801
[6,     1] loss: 1386.481
[7,     1] loss: 1387.351
[8,     1] loss: 1377.719
[9,     1] loss: 1373.573
[10,     1] loss: 1357.630
[11,     1] loss: 1335.222
[12,     1] loss: 1303.656
[13,     1] loss: 1274.227
[14,     1] loss: 1233.389
[15,     1] loss: 1202.174
[16,     1] loss: 1214.447
[17,     1] loss: 1205.349
[18,     1] loss: 1147.409
[19,     1] loss: 1153.363
[20,     1] loss: 1179.651
[21,     1] loss: 1137.231
[22,     1] loss: 1141.164
[23,     1] loss: 1138.000
[24,     1] loss: 1149.531
[25,     1] loss: 1099.543
[26,     1] loss: 1088.953
[27,     1] loss: 1079.427
[28,     1] loss: 1085.164
[29,     1] loss: 1034.277
[30,     1] loss: 1075.546
[31,     1] loss: 1017.963
[32,     1] loss: 1021.298
[33,     1] loss: 965.963
[34,     1] loss: 989.489
[35,     1] loss: 957.320
[36,     1] loss: 983.470
[37,     1] loss: 970.283
[38,     1] loss: 971.810
[39,     1] loss: 1050.821
[40,     1] loss: 1032.497
[41,     1] loss: 931.507
[42,     1] loss: 978.750
[43,     1] loss: 919.343
Early stopping applied (best metric=0.8086816072463989)
Finished Training
Total time taken: 5.886005640029907
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.121
[2,     1] loss: 1385.748
[3,     1] loss: 1389.852
[4,     1] loss: 1388.695
[5,     1] loss: 1374.979
[6,     1] loss: 1369.442
[7,     1] loss: 1356.987
[8,     1] loss: 1325.358
[9,     1] loss: 1292.834
[10,     1] loss: 1278.088
[11,     1] loss: 1225.274
[12,     1] loss: 1197.926
[13,     1] loss: 1198.930
[14,     1] loss: 1155.053
[15,     1] loss: 1175.669
[16,     1] loss: 1190.543
[17,     1] loss: 1190.857
[18,     1] loss: 1145.557
[19,     1] loss: 1120.438
[20,     1] loss: 1102.133
[21,     1] loss: 1142.179
[22,     1] loss: 1059.229
[23,     1] loss: 1119.666
[24,     1] loss: 1077.579
[25,     1] loss: 1089.270
[26,     1] loss: 1032.093
[27,     1] loss: 1077.418
[28,     1] loss: 1063.473
[29,     1] loss: 1048.500
[30,     1] loss: 1048.306
[31,     1] loss: 1075.950
[32,     1] loss: 1041.470
[33,     1] loss: 973.814
[34,     1] loss: 1026.419
[35,     1] loss: 1023.570
[36,     1] loss: 981.484
[37,     1] loss: 972.226
[38,     1] loss: 980.591
[39,     1] loss: 925.069
[40,     1] loss: 957.886
[41,     1] loss: 910.226
[42,     1] loss: 902.232
[43,     1] loss: 868.212
[44,     1] loss: 922.374
[45,     1] loss: 948.812
[46,     1] loss: 903.648
[47,     1] loss: 929.143
[48,     1] loss: 1001.384
[49,     1] loss: 857.893
[50,     1] loss: 911.194
[51,     1] loss: 848.271
[52,     1] loss: 818.888
[53,     1] loss: 827.964
[54,     1] loss: 857.613
[55,     1] loss: 785.395
[56,     1] loss: 721.039
[57,     1] loss: 784.697
[58,     1] loss: 773.313
[59,     1] loss: 721.282
[60,     1] loss: 748.696
[61,     1] loss: 800.147
[62,     1] loss: 651.439
[63,     1] loss: 737.186
[64,     1] loss: 736.077
[65,     1] loss: 698.404
[66,     1] loss: 752.880
[67,     1] loss: 676.049
[68,     1] loss: 738.512
[69,     1] loss: 1010.204
[70,     1] loss: 894.165
Early stopping applied (best metric=0.8622777462005615)
Finished Training
Total time taken: 9.356006860733032
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.831
[2,     1] loss: 1393.478
[3,     1] loss: 1389.080
[4,     1] loss: 1391.518
[5,     1] loss: 1390.542
[6,     1] loss: 1389.199
[7,     1] loss: 1384.531
[8,     1] loss: 1385.396
[9,     1] loss: 1392.765
[10,     1] loss: 1373.282
[11,     1] loss: 1367.861
[12,     1] loss: 1339.526
[13,     1] loss: 1316.959
[14,     1] loss: 1279.363
[15,     1] loss: 1251.371
[16,     1] loss: 1234.151
[17,     1] loss: 1203.914
[18,     1] loss: 1167.456
[19,     1] loss: 1162.809
[20,     1] loss: 1108.091
[21,     1] loss: 1121.665
[22,     1] loss: 1164.342
[23,     1] loss: 1108.054
[24,     1] loss: 1100.382
[25,     1] loss: 1102.839
[26,     1] loss: 1062.560
[27,     1] loss: 1107.019
[28,     1] loss: 1076.633
[29,     1] loss: 1060.032
[30,     1] loss: 1042.304
[31,     1] loss: 1007.943
[32,     1] loss: 1044.451
[33,     1] loss: 988.127
[34,     1] loss: 1017.813
[35,     1] loss: 1024.727
[36,     1] loss: 981.304
[37,     1] loss: 1076.841
[38,     1] loss: 919.273
[39,     1] loss: 987.708
[40,     1] loss: 980.169
[41,     1] loss: 953.246
[42,     1] loss: 996.960
[43,     1] loss: 932.277
[44,     1] loss: 929.433
[45,     1] loss: 933.753
[46,     1] loss: 1002.506
[47,     1] loss: 888.268
[48,     1] loss: 995.055
[49,     1] loss: 878.944
[50,     1] loss: 931.246
[51,     1] loss: 901.000
[52,     1] loss: 824.383
[53,     1] loss: 816.133
[54,     1] loss: 882.718
[55,     1] loss: 890.481
[56,     1] loss: 834.465
[57,     1] loss: 908.023
[58,     1] loss: 790.920
[59,     1] loss: 932.413
[60,     1] loss: 801.662
[61,     1] loss: 887.554
[62,     1] loss: 783.865
[63,     1] loss: 746.549
[64,     1] loss: 726.531
[65,     1] loss: 685.308
[66,     1] loss: 710.100
[67,     1] loss: 744.154
[68,     1] loss: 779.126
[69,     1] loss: 920.766
[70,     1] loss: 693.503
[71,     1] loss: 782.639
[72,     1] loss: 796.253
[73,     1] loss: 804.227
[74,     1] loss: 738.869
[75,     1] loss: 779.772
[76,     1] loss: 690.022
[77,     1] loss: 726.549
Early stopping applied (best metric=0.781875729560852)
Finished Training
Total time taken: 10.469009160995483
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1394.726
[2,     1] loss: 1394.131
[3,     1] loss: 1400.539
[4,     1] loss: 1391.151
[5,     1] loss: 1390.733
[6,     1] loss: 1387.555
[7,     1] loss: 1390.888
[8,     1] loss: 1386.493
[9,     1] loss: 1385.181
[10,     1] loss: 1378.278
[11,     1] loss: 1365.425
[12,     1] loss: 1342.186
[13,     1] loss: 1328.062
[14,     1] loss: 1301.337
[15,     1] loss: 1272.201
[16,     1] loss: 1252.886
[17,     1] loss: 1231.754
[18,     1] loss: 1203.819
[19,     1] loss: 1187.060
[20,     1] loss: 1146.159
[21,     1] loss: 1180.914
[22,     1] loss: 1124.212
[23,     1] loss: 1128.145
[24,     1] loss: 1117.638
[25,     1] loss: 1154.270
[26,     1] loss: 1138.985
[27,     1] loss: 1085.456
[28,     1] loss: 1091.924
[29,     1] loss: 1143.505
[30,     1] loss: 1059.070
[31,     1] loss: 1093.917
[32,     1] loss: 1088.273
[33,     1] loss: 1051.069
[34,     1] loss: 1104.452
[35,     1] loss: 1054.485
[36,     1] loss: 1037.339
[37,     1] loss: 1033.410
[38,     1] loss: 967.768
[39,     1] loss: 954.364
[40,     1] loss: 1023.576
[41,     1] loss: 1032.403
[42,     1] loss: 947.830
[43,     1] loss: 978.288
[44,     1] loss: 899.643
[45,     1] loss: 985.207
[46,     1] loss: 928.091
[47,     1] loss: 919.799
[48,     1] loss: 887.629
[49,     1] loss: 920.560
[50,     1] loss: 902.263
[51,     1] loss: 863.430
[52,     1] loss: 882.526
[53,     1] loss: 907.488
[54,     1] loss: 839.444
[55,     1] loss: 980.802
[56,     1] loss: 837.485
[57,     1] loss: 823.710
[58,     1] loss: 780.614
[59,     1] loss: 804.359
[60,     1] loss: 699.279
[61,     1] loss: 801.679
[62,     1] loss: 829.410
Early stopping applied (best metric=0.7654321193695068)
Finished Training
Total time taken: 8.500006437301636
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.054
[2,     1] loss: 1396.894
[3,     1] loss: 1391.917
[4,     1] loss: 1390.125
[5,     1] loss: 1386.334
[6,     1] loss: 1385.749
[7,     1] loss: 1374.850
[8,     1] loss: 1359.292
[9,     1] loss: 1342.743
[10,     1] loss: 1308.424
[11,     1] loss: 1282.549
[12,     1] loss: 1217.729
[13,     1] loss: 1178.116
[14,     1] loss: 1208.204
[15,     1] loss: 1152.827
[16,     1] loss: 1128.433
[17,     1] loss: 1149.107
[18,     1] loss: 1121.966
[19,     1] loss: 1086.135
[20,     1] loss: 1076.215
[21,     1] loss: 1119.127
[22,     1] loss: 1120.710
[23,     1] loss: 1091.219
[24,     1] loss: 1078.792
[25,     1] loss: 1070.239
[26,     1] loss: 1036.573
[27,     1] loss: 1074.297
[28,     1] loss: 1024.950
[29,     1] loss: 1068.055
[30,     1] loss: 1039.408
[31,     1] loss: 1063.428
[32,     1] loss: 1015.913
[33,     1] loss: 1011.107
[34,     1] loss: 938.607
[35,     1] loss: 943.284
[36,     1] loss: 921.271
[37,     1] loss: 935.124
[38,     1] loss: 1098.991
[39,     1] loss: 947.946
[40,     1] loss: 880.712
[41,     1] loss: 957.412
[42,     1] loss: 907.408
[43,     1] loss: 965.239
[44,     1] loss: 896.835
[45,     1] loss: 846.887
[46,     1] loss: 864.973
[47,     1] loss: 876.947
[48,     1] loss: 883.817
[49,     1] loss: 880.915
[50,     1] loss: 825.634
[51,     1] loss: 818.543
[52,     1] loss: 915.397
[53,     1] loss: 993.903
[54,     1] loss: 824.583
[55,     1] loss: 865.842
[56,     1] loss: 819.412
Early stopping applied (best metric=0.7706563472747803)
Finished Training
Total time taken: 7.660006999969482
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1388.370
[2,     1] loss: 1396.710
[3,     1] loss: 1390.801
[4,     1] loss: 1391.056
[5,     1] loss: 1392.542
[6,     1] loss: 1390.621
[7,     1] loss: 1386.700
[8,     1] loss: 1386.226
[9,     1] loss: 1383.474
[10,     1] loss: 1386.996
[11,     1] loss: 1377.329
[12,     1] loss: 1369.505
[13,     1] loss: 1351.404
[14,     1] loss: 1336.428
[15,     1] loss: 1312.711
[16,     1] loss: 1289.077
[17,     1] loss: 1248.169
[18,     1] loss: 1204.690
[19,     1] loss: 1181.755
[20,     1] loss: 1183.335
[21,     1] loss: 1148.369
[22,     1] loss: 1115.754
[23,     1] loss: 1141.945
[24,     1] loss: 1112.980
[25,     1] loss: 1142.054
[26,     1] loss: 1088.338
[27,     1] loss: 1126.615
[28,     1] loss: 1155.392
[29,     1] loss: 1075.944
[30,     1] loss: 1104.195
[31,     1] loss: 1049.918
[32,     1] loss: 1089.596
[33,     1] loss: 1000.767
[34,     1] loss: 1069.928
[35,     1] loss: 1030.802
[36,     1] loss: 1006.031
[37,     1] loss: 990.410
[38,     1] loss: 961.322
[39,     1] loss: 987.448
[40,     1] loss: 1018.056
[41,     1] loss: 1003.989
[42,     1] loss: 1058.317
[43,     1] loss: 905.621
[44,     1] loss: 975.942
[45,     1] loss: 887.425
[46,     1] loss: 941.025
[47,     1] loss: 921.181
[48,     1] loss: 908.741
[49,     1] loss: 874.930
[50,     1] loss: 839.526
[51,     1] loss: 823.171
[52,     1] loss: 817.616
[53,     1] loss: 863.607
[54,     1] loss: 992.538
[55,     1] loss: 1166.685
[56,     1] loss: 811.268
[57,     1] loss: 956.151
[58,     1] loss: 808.646
[59,     1] loss: 917.883
[60,     1] loss: 886.464
[61,     1] loss: 831.847
[62,     1] loss: 882.784
[63,     1] loss: 800.864
[64,     1] loss: 777.915
[65,     1] loss: 831.071
[66,     1] loss: 819.511
Early stopping applied (best metric=0.8216660022735596)
Finished Training
Total time taken: 9.087007284164429
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.236
[2,     1] loss: 1395.434
[3,     1] loss: 1390.414
[4,     1] loss: 1389.169
[5,     1] loss: 1390.072
[6,     1] loss: 1391.576
[7,     1] loss: 1385.174
[8,     1] loss: 1383.978
[9,     1] loss: 1377.465
[10,     1] loss: 1371.179
[11,     1] loss: 1356.023
[12,     1] loss: 1326.917
[13,     1] loss: 1299.074
[14,     1] loss: 1275.041
[15,     1] loss: 1219.782
[16,     1] loss: 1207.655
[17,     1] loss: 1183.202
[18,     1] loss: 1159.455
[19,     1] loss: 1187.722
[20,     1] loss: 1136.940
[21,     1] loss: 1170.735
[22,     1] loss: 1093.966
[23,     1] loss: 1140.784
[24,     1] loss: 1107.618
[25,     1] loss: 1142.424
[26,     1] loss: 1146.931
[27,     1] loss: 1091.891
[28,     1] loss: 1070.748
[29,     1] loss: 1063.504
[30,     1] loss: 1054.338
[31,     1] loss: 1059.339
[32,     1] loss: 1030.890
[33,     1] loss: 1017.178
[34,     1] loss: 961.963
[35,     1] loss: 1020.358
[36,     1] loss: 1023.566
[37,     1] loss: 1036.705
[38,     1] loss: 984.876
[39,     1] loss: 984.149
[40,     1] loss: 989.807
[41,     1] loss: 970.837
[42,     1] loss: 983.108
[43,     1] loss: 975.117
[44,     1] loss: 937.701
[45,     1] loss: 899.641
[46,     1] loss: 935.982
[47,     1] loss: 919.445
[48,     1] loss: 1061.089
[49,     1] loss: 1038.902
[50,     1] loss: 904.588
[51,     1] loss: 938.695
[52,     1] loss: 986.348
[53,     1] loss: 891.474
[54,     1] loss: 918.850
[55,     1] loss: 835.955
[56,     1] loss: 892.169
[57,     1] loss: 913.412
[58,     1] loss: 808.343
[59,     1] loss: 846.073
[60,     1] loss: 799.988
[61,     1] loss: 874.579
[62,     1] loss: 836.301
[63,     1] loss: 759.378
[64,     1] loss: 907.049
[65,     1] loss: 735.678
Early stopping applied (best metric=0.8290436267852783)
Finished Training
Total time taken: 8.969005823135376
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.869
[2,     1] loss: 1388.462
[3,     1] loss: 1387.490
[4,     1] loss: 1385.138
[5,     1] loss: 1387.234
[6,     1] loss: 1384.964
[7,     1] loss: 1378.840
[8,     1] loss: 1371.656
[9,     1] loss: 1339.531
[10,     1] loss: 1315.155
[11,     1] loss: 1287.881
[12,     1] loss: 1237.893
[13,     1] loss: 1226.067
[14,     1] loss: 1194.442
[15,     1] loss: 1178.068
[16,     1] loss: 1156.137
[17,     1] loss: 1148.404
[18,     1] loss: 1160.484
[19,     1] loss: 1156.488
[20,     1] loss: 1127.325
[21,     1] loss: 1103.550
[22,     1] loss: 1163.244
[23,     1] loss: 1104.704
[24,     1] loss: 1064.400
[25,     1] loss: 1068.739
[26,     1] loss: 1028.763
[27,     1] loss: 1109.585
[28,     1] loss: 1048.133
[29,     1] loss: 1140.844
[30,     1] loss: 1023.622
[31,     1] loss: 1039.637
[32,     1] loss: 971.681
[33,     1] loss: 1018.992
[34,     1] loss: 989.047
[35,     1] loss: 1071.359
[36,     1] loss: 979.742
[37,     1] loss: 911.784
[38,     1] loss: 950.713
[39,     1] loss: 875.792
[40,     1] loss: 914.145
[41,     1] loss: 918.574
[42,     1] loss: 980.566
[43,     1] loss: 977.178
[44,     1] loss: 910.697
[45,     1] loss: 860.876
[46,     1] loss: 881.032
[47,     1] loss: 899.561
[48,     1] loss: 850.048
[49,     1] loss: 834.427
[50,     1] loss: 777.120
[51,     1] loss: 774.012
[52,     1] loss: 811.181
[53,     1] loss: 848.033
[54,     1] loss: 905.711
Early stopping applied (best metric=0.8504520654678345)
Finished Training
Total time taken: 7.487004041671753
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1396.721
[2,     1] loss: 1390.929
[3,     1] loss: 1392.270
[4,     1] loss: 1396.852
[5,     1] loss: 1391.318
[6,     1] loss: 1390.264
[7,     1] loss: 1391.037
[8,     1] loss: 1389.602
[9,     1] loss: 1386.829
[10,     1] loss: 1378.249
[11,     1] loss: 1364.443
[12,     1] loss: 1336.880
[13,     1] loss: 1318.966
[14,     1] loss: 1283.314
[15,     1] loss: 1263.882
[16,     1] loss: 1215.325
[17,     1] loss: 1202.456
[18,     1] loss: 1215.701
[19,     1] loss: 1144.504
[20,     1] loss: 1175.615
[21,     1] loss: 1231.566
[22,     1] loss: 1143.771
[23,     1] loss: 1162.210
[24,     1] loss: 1126.615
[25,     1] loss: 1106.624
[26,     1] loss: 1092.966
[27,     1] loss: 1124.852
[28,     1] loss: 1092.682
[29,     1] loss: 1146.605
[30,     1] loss: 1085.216
[31,     1] loss: 1074.122
[32,     1] loss: 1087.222
[33,     1] loss: 1025.578
[34,     1] loss: 996.604
[35,     1] loss: 992.580
[36,     1] loss: 1008.989
[37,     1] loss: 1011.660
[38,     1] loss: 966.334
[39,     1] loss: 973.910
[40,     1] loss: 1015.631
[41,     1] loss: 1028.294
[42,     1] loss: 944.348
[43,     1] loss: 943.139
[44,     1] loss: 864.894
[45,     1] loss: 938.460
[46,     1] loss: 862.498
[47,     1] loss: 882.755
[48,     1] loss: 862.803
[49,     1] loss: 844.790
[50,     1] loss: 837.654
[51,     1] loss: 893.106
[52,     1] loss: 1046.420
[53,     1] loss: 913.207
[54,     1] loss: 909.851
[55,     1] loss: 925.250
[56,     1] loss: 888.118
[57,     1] loss: 877.282
[58,     1] loss: 890.476
[59,     1] loss: 826.781
[60,     1] loss: 837.127
[61,     1] loss: 834.446
[62,     1] loss: 769.105
[63,     1] loss: 779.665
[64,     1] loss: 758.276
[65,     1] loss: 781.535
[66,     1] loss: 754.680
[67,     1] loss: 728.764
[68,     1] loss: 734.516
[69,     1] loss: 871.635
[70,     1] loss: 882.335
[71,     1] loss: 760.060
[72,     1] loss: 855.433
[73,     1] loss: 755.114
[74,     1] loss: 759.891
[75,     1] loss: 733.270
[76,     1] loss: 717.506
[77,     1] loss: 704.840
[78,     1] loss: 667.910
[79,     1] loss: 944.972
[80,     1] loss: 780.333
[81,     1] loss: 629.397
Early stopping applied (best metric=0.8273465633392334)
Finished Training
Total time taken: 11.334879636764526
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.985
[2,     1] loss: 1401.213
[3,     1] loss: 1386.547
[4,     1] loss: 1388.373
[5,     1] loss: 1383.183
[6,     1] loss: 1373.326
[7,     1] loss: 1348.521
[8,     1] loss: 1321.659
[9,     1] loss: 1291.656
[10,     1] loss: 1255.845
[11,     1] loss: 1212.566
[12,     1] loss: 1204.183
[13,     1] loss: 1209.227
[14,     1] loss: 1160.157
[15,     1] loss: 1143.723
[16,     1] loss: 1168.282
[17,     1] loss: 1132.660
[18,     1] loss: 1161.305
[19,     1] loss: 1125.266
[20,     1] loss: 1115.657
[21,     1] loss: 1140.025
[22,     1] loss: 1088.575
[23,     1] loss: 1126.658
[24,     1] loss: 1059.267
[25,     1] loss: 1038.664
[26,     1] loss: 1013.824
[27,     1] loss: 1034.760
[28,     1] loss: 999.150
[29,     1] loss: 1042.294
[30,     1] loss: 1063.868
[31,     1] loss: 1055.478
[32,     1] loss: 1004.559
[33,     1] loss: 1047.208
[34,     1] loss: 990.891
[35,     1] loss: 1012.083
[36,     1] loss: 978.687
[37,     1] loss: 992.359
[38,     1] loss: 895.745
[39,     1] loss: 956.678
[40,     1] loss: 1012.808
[41,     1] loss: 926.885
[42,     1] loss: 929.948
[43,     1] loss: 996.071
[44,     1] loss: 875.908
[45,     1] loss: 921.069
[46,     1] loss: 867.954
[47,     1] loss: 898.252
[48,     1] loss: 862.408
[49,     1] loss: 881.856
[50,     1] loss: 870.894
[51,     1] loss: 893.241
[52,     1] loss: 852.258
[53,     1] loss: 824.722
[54,     1] loss: 844.246
[55,     1] loss: 772.159
[56,     1] loss: 785.046
[57,     1] loss: 767.981
[58,     1] loss: 705.676
[59,     1] loss: 783.525
[60,     1] loss: 865.791
[61,     1] loss: 921.450
[62,     1] loss: 803.426
[63,     1] loss: 772.809
[64,     1] loss: 849.673
[65,     1] loss: 788.310
[66,     1] loss: 862.861
[67,     1] loss: 782.554
[68,     1] loss: 809.054
[69,     1] loss: 726.051
[70,     1] loss: 827.904
[71,     1] loss: 711.999
[72,     1] loss: 819.539
[73,     1] loss: 718.380
[74,     1] loss: 760.434
Early stopping applied (best metric=0.7767109870910645)
Finished Training
Total time taken: 10.318010091781616
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.444
[2,     1] loss: 1395.399
[3,     1] loss: 1392.861
[4,     1] loss: 1392.104
[5,     1] loss: 1389.112
[6,     1] loss: 1389.018
[7,     1] loss: 1398.077
[8,     1] loss: 1386.008
[9,     1] loss: 1388.370
[10,     1] loss: 1386.917
[11,     1] loss: 1385.498
[12,     1] loss: 1383.961
[13,     1] loss: 1381.368
[14,     1] loss: 1369.832
[15,     1] loss: 1362.830
[16,     1] loss: 1333.384
[17,     1] loss: 1312.410
[18,     1] loss: 1293.201
[19,     1] loss: 1248.542
[20,     1] loss: 1224.729
[21,     1] loss: 1224.407
[22,     1] loss: 1205.669
[23,     1] loss: 1182.870
[24,     1] loss: 1180.390
[25,     1] loss: 1195.405
[26,     1] loss: 1143.312
[27,     1] loss: 1151.307
[28,     1] loss: 1159.967
[29,     1] loss: 1143.858
[30,     1] loss: 1114.161
[31,     1] loss: 1133.890
[32,     1] loss: 1096.236
[33,     1] loss: 1065.189
[34,     1] loss: 1097.084
[35,     1] loss: 1057.726
[36,     1] loss: 1057.212
[37,     1] loss: 1039.109
[38,     1] loss: 1011.760
[39,     1] loss: 954.033
[40,     1] loss: 980.394
[41,     1] loss: 1034.141
[42,     1] loss: 1302.836
[43,     1] loss: 1082.696
[44,     1] loss: 1039.482
[45,     1] loss: 978.541
[46,     1] loss: 1062.207
[47,     1] loss: 1030.700
[48,     1] loss: 1031.575
[49,     1] loss: 1083.074
[50,     1] loss: 965.755
[51,     1] loss: 927.180
[52,     1] loss: 984.156
[53,     1] loss: 905.915
[54,     1] loss: 865.992
[55,     1] loss: 947.765
[56,     1] loss: 872.421
[57,     1] loss: 996.729
[58,     1] loss: 868.559
[59,     1] loss: 945.494
[60,     1] loss: 809.161
[61,     1] loss: 882.078
[62,     1] loss: 823.903
[63,     1] loss: 926.944
[64,     1] loss: 770.263
[65,     1] loss: 872.442
[66,     1] loss: 806.065
[67,     1] loss: 852.605
[68,     1] loss: 793.950
[69,     1] loss: 882.606
[70,     1] loss: 890.106
[71,     1] loss: 869.477
[72,     1] loss: 777.759
[73,     1] loss: 794.380
[74,     1] loss: 760.852
[75,     1] loss: 811.600
[76,     1] loss: 711.223
[77,     1] loss: 804.579
[78,     1] loss: 708.831
[79,     1] loss: 739.220
[80,     1] loss: 898.152
[81,     1] loss: 876.706
[82,     1] loss: 671.820
[83,     1] loss: 822.558
[84,     1] loss: 694.187
[85,     1] loss: 799.351
[86,     1] loss: 687.700
[87,     1] loss: 716.156
[88,     1] loss: 678.885
[89,     1] loss: 668.584
[90,     1] loss: 617.846
[91,     1] loss: 617.910
[92,     1] loss: 610.747
[93,     1] loss: 638.218
[94,     1] loss: 732.587
[95,     1] loss: 644.041
[96,     1] loss: 670.498
[97,     1] loss: 664.966
[98,     1] loss: 588.897
[99,     1] loss: 647.923
[100,     1] loss: 677.078
[101,     1] loss: 699.329
[102,     1] loss: 581.209
[103,     1] loss: 690.166
[104,     1] loss: 705.116
[105,     1] loss: 613.915
[106,     1] loss: 814.400
[107,     1] loss: 656.488
[108,     1] loss: 649.338
[109,     1] loss: 676.345
[110,     1] loss: 587.900
[111,     1] loss: 659.454
[112,     1] loss: 586.543
[113,     1] loss: 556.382
[114,     1] loss: 600.866
[115,     1] loss: 509.800
[116,     1] loss: 573.124
[117,     1] loss: 549.187
[118,     1] loss: 513.709
Early stopping applied (best metric=0.7129297852516174)
Finished Training
Total time taken: 16.470014572143555
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.858
[2,     1] loss: 1394.804
[3,     1] loss: 1390.179
[4,     1] loss: 1394.201
[5,     1] loss: 1391.499
[6,     1] loss: 1388.347
[7,     1] loss: 1383.160
[8,     1] loss: 1378.198
[9,     1] loss: 1370.917
[10,     1] loss: 1352.686
[11,     1] loss: 1330.911
[12,     1] loss: 1301.637
[13,     1] loss: 1307.688
[14,     1] loss: 1248.393
[15,     1] loss: 1245.871
[16,     1] loss: 1226.627
[17,     1] loss: 1174.991
[18,     1] loss: 1190.003
[19,     1] loss: 1162.777
[20,     1] loss: 1139.484
[21,     1] loss: 1139.567
[22,     1] loss: 1135.789
[23,     1] loss: 1161.037
[24,     1] loss: 1153.655
[25,     1] loss: 1079.017
[26,     1] loss: 1145.619
[27,     1] loss: 1056.716
[28,     1] loss: 1106.899
[29,     1] loss: 1062.963
[30,     1] loss: 1090.315
[31,     1] loss: 1125.296
[32,     1] loss: 1038.361
[33,     1] loss: 1060.045
[34,     1] loss: 1020.897
[35,     1] loss: 1069.316
[36,     1] loss: 998.608
[37,     1] loss: 971.811
[38,     1] loss: 991.809
[39,     1] loss: 962.231
[40,     1] loss: 911.235
[41,     1] loss: 964.752
[42,     1] loss: 911.427
[43,     1] loss: 893.157
[44,     1] loss: 880.280
[45,     1] loss: 873.849
[46,     1] loss: 876.507
[47,     1] loss: 879.728
[48,     1] loss: 844.369
[49,     1] loss: 928.191
[50,     1] loss: 1146.909
[51,     1] loss: 867.606
[52,     1] loss: 977.882
[53,     1] loss: 883.456
[54,     1] loss: 921.865
[55,     1] loss: 906.312
[56,     1] loss: 865.861
[57,     1] loss: 838.137
[58,     1] loss: 835.735
[59,     1] loss: 829.247
[60,     1] loss: 778.031
[61,     1] loss: 788.449
[62,     1] loss: 819.477
[63,     1] loss: 774.903
[64,     1] loss: 783.244
Early stopping applied (best metric=0.751821756362915)
Finished Training
Total time taken: 9.044994115829468
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.431
[2,     1] loss: 1390.317
[3,     1] loss: 1391.597
[4,     1] loss: 1393.435
[5,     1] loss: 1389.330
[6,     1] loss: 1382.131
[7,     1] loss: 1375.126
[8,     1] loss: 1370.209
[9,     1] loss: 1347.906
[10,     1] loss: 1313.639
[11,     1] loss: 1278.979
[12,     1] loss: 1244.278
[13,     1] loss: 1228.404
[14,     1] loss: 1182.201
[15,     1] loss: 1205.057
[16,     1] loss: 1153.654
[17,     1] loss: 1130.111
[18,     1] loss: 1164.821
[19,     1] loss: 1167.645
[20,     1] loss: 1109.456
[21,     1] loss: 1110.128
[22,     1] loss: 1164.384
[23,     1] loss: 1092.214
[24,     1] loss: 1080.318
[25,     1] loss: 1086.660
[26,     1] loss: 1073.046
[27,     1] loss: 1022.024
[28,     1] loss: 1012.966
[29,     1] loss: 1105.145
[30,     1] loss: 1048.983
[31,     1] loss: 981.203
[32,     1] loss: 1082.460
[33,     1] loss: 962.782
[34,     1] loss: 1080.212
[35,     1] loss: 977.371
[36,     1] loss: 1033.107
[37,     1] loss: 1006.361
[38,     1] loss: 1021.380
[39,     1] loss: 984.155
[40,     1] loss: 1017.779
[41,     1] loss: 923.784
[42,     1] loss: 965.708
[43,     1] loss: 907.263
[44,     1] loss: 901.549
[45,     1] loss: 931.538
[46,     1] loss: 872.636
[47,     1] loss: 875.453
[48,     1] loss: 998.702
[49,     1] loss: 1090.871
[50,     1] loss: 955.072
[51,     1] loss: 877.154
[52,     1] loss: 918.342
[53,     1] loss: 881.743
[54,     1] loss: 880.821
[55,     1] loss: 959.095
[56,     1] loss: 835.043
[57,     1] loss: 886.393
[58,     1] loss: 815.226
[59,     1] loss: 802.224
[60,     1] loss: 755.508
[61,     1] loss: 745.812
[62,     1] loss: 815.568
[63,     1] loss: 716.492
[64,     1] loss: 773.813
[65,     1] loss: 818.489
[66,     1] loss: 994.018
[67,     1] loss: 802.858
[68,     1] loss: 804.602
[69,     1] loss: 793.465
[70,     1] loss: 796.482
Early stopping applied (best metric=0.8077817559242249)
Finished Training
Total time taken: 9.969008207321167
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1393.357
[2,     1] loss: 1395.603
[3,     1] loss: 1391.280
[4,     1] loss: 1395.780
[5,     1] loss: 1389.181
[6,     1] loss: 1387.775
[7,     1] loss: 1385.333
[8,     1] loss: 1380.275
[9,     1] loss: 1371.446
[10,     1] loss: 1359.006
[11,     1] loss: 1329.387
[12,     1] loss: 1294.739
[13,     1] loss: 1254.592
[14,     1] loss: 1246.114
[15,     1] loss: 1231.814
[16,     1] loss: 1221.009
[17,     1] loss: 1180.116
[18,     1] loss: 1194.707
[19,     1] loss: 1156.525
[20,     1] loss: 1192.865
[21,     1] loss: 1149.045
[22,     1] loss: 1152.170
[23,     1] loss: 1119.036
[24,     1] loss: 1142.670
[25,     1] loss: 1133.004
[26,     1] loss: 1104.941
[27,     1] loss: 1061.790
[28,     1] loss: 1046.780
[29,     1] loss: 1035.750
[30,     1] loss: 1066.085
[31,     1] loss: 1036.422
[32,     1] loss: 1022.247
[33,     1] loss: 1038.931
[34,     1] loss: 999.466
[35,     1] loss: 1031.188
[36,     1] loss: 1045.328
[37,     1] loss: 1009.659
[38,     1] loss: 963.272
[39,     1] loss: 969.650
[40,     1] loss: 951.947
[41,     1] loss: 949.752
[42,     1] loss: 909.017
[43,     1] loss: 967.183
[44,     1] loss: 908.692
[45,     1] loss: 960.200
[46,     1] loss: 895.109
[47,     1] loss: 909.890
[48,     1] loss: 869.143
[49,     1] loss: 898.031
[50,     1] loss: 874.522
[51,     1] loss: 839.393
[52,     1] loss: 798.798
[53,     1] loss: 904.649
[54,     1] loss: 1084.158
[55,     1] loss: 869.275
[56,     1] loss: 861.393
[57,     1] loss: 894.829
[58,     1] loss: 881.000
[59,     1] loss: 813.114
[60,     1] loss: 842.602
[61,     1] loss: 840.040
[62,     1] loss: 791.335
[63,     1] loss: 800.055
[64,     1] loss: 804.121
[65,     1] loss: 743.631
[66,     1] loss: 712.329
[67,     1] loss: 715.214
[68,     1] loss: 700.969
[69,     1] loss: 858.544
[70,     1] loss: 960.494
[71,     1] loss: 725.164
[72,     1] loss: 816.830
[73,     1] loss: 738.775
[74,     1] loss: 801.401
[75,     1] loss: 718.871
[76,     1] loss: 769.815
[77,     1] loss: 787.465
[78,     1] loss: 694.068
[79,     1] loss: 732.338
[80,     1] loss: 694.995
[81,     1] loss: 637.701
[82,     1] loss: 644.442
[83,     1] loss: 661.481
[84,     1] loss: 608.426
[85,     1] loss: 612.159
[86,     1] loss: 694.662
[87,     1] loss: 634.302
[88,     1] loss: 635.086
[89,     1] loss: 627.201
[90,     1] loss: 643.081
[91,     1] loss: 569.437
[92,     1] loss: 606.189
[93,     1] loss: 638.752
Early stopping applied (best metric=0.7895776033401489)
Finished Training
Total time taken: 13.288011312484741
results!
{'gpu_mode': True, 'epochs': 200, 'batch_size': 2048, 'learning_rate': 0.0036054445881177886, 'test_data_ratio': 0.2, 'data_sample_mode': ['oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample'], 'crossValidation': True, 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'folds': 5, 'earlyStopping': True, 'ValidationMetric': 'Validation Loss (total)', 'earlyStoppingPatience': 20, 'CV_Repeats': 5, 'Experiment Name': 'Model architecture - added max, ranges, bceloss', 'CreateFigures': False, 'weight_decay': 7.295840482022753, 'embeddingType': 'adaptiveEmbedding', 'LSTM_layers': 1, 'LSTM_hidden_size': 64, 'LSTM_dropout': 0, 'MultiTask': True, 'MultiTask_sample_method': 'balanced', 'UseUncertaintyBasedLoss': False, 'useLrWeight': True, 'CNNType': 'Musite', 'FCType': 'Adapt', 'layerToSplitOn': 'FC', 'dontAverageLoss': False, 'useWeightDecayWeight': False, 'SeperateTuningLRandWD': True, 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'], 'n_trials': 500, 'FloatsToTune': {'learning_rate': [1e-05, 0.01], 'weight_decay': [0, 10], 'log_base': [1.01, 3], 'learning_rate_Hydroxylation-K': [1e-05, 0.01], 'learning_rate_Hydroxylation-P': [1e-05, 0.01], 'weight_decay_Hydroxylation-P': [0, 10], 'weight_decay_Hydroxylation-K': [0, 10]}, 'IntsToTune': {}, 'log_base': 2.0745543108177174, 'learning_rate_Hydroxylation-K': 0.0035453590804421812, 'learning_rate_Hydroxylation-P': 0.005717241986871175, 'weight_decay_Hydroxylation-P': 0.4225747573678792, 'weight_decay_Hydroxylation-K': 0.48673850671574304, 'random_state': 423524295, 'current_CV_Repeat': 5, 'sample_weights': [2.2894003636921396, 1], 'WeightDecayWeights': [], 'currentFold': 4}
{'Hydroxylation-K Validation Accuracy': 0.7654964539007092, 'Hydroxylation-K Validation Sensitivity': 0.6777777777777778, 'Hydroxylation-K Validation Specificity': 0.7873684210526316, 'Hydroxylation-K Validation Precision': 0.463855802264471, 'Hydroxylation-K AUC ROC': 0.7971461988304094, 'Hydroxylation-K AUC PR': 0.5931195258455619, 'Hydroxylation-K MCC': 0.41410409287228084, 'Hydroxylation-K F1': 0.5425608409692367, 'Validation Loss (Hydroxylation-K)': 0.43721963167190553, 'Hydroxylation-P Validation Accuracy': 0.7919243185625096, 'Hydroxylation-P Validation Sensitivity': 0.7763492063492063, 'Hydroxylation-P Validation Specificity': 0.7953179709711208, 'Hydroxylation-P Validation Precision': 0.4584900186743545, 'Hydroxylation-P AUC ROC': 0.8442026475610943, 'Hydroxylation-P AUC PR': 0.5773343823877345, 'Hydroxylation-P MCC': 0.47882599098043643, 'Hydroxylation-P F1': 0.5718184156268762, 'Validation Loss (Hydroxylation-P)': 0.3782276690006256, 'Validation Loss (total)': 0.8154472970962524, 'TimeToTrain': 9.201401853561402}
{'Hydroxylation-K Validation Accuracy': 0.05808083574354203, 'Hydroxylation-K Validation Sensitivity': 0.09611806868415541, 'Hydroxylation-K Validation Specificity': 0.07631578947368421, 'Hydroxylation-K Validation Precision': 0.11362455391206946, 'Hydroxylation-K AUC ROC': 0.05602667025368987, 'Hydroxylation-K AUC PR': 0.1411893979010116, 'Hydroxylation-K MCC': 0.10942528399860087, 'Hydroxylation-K F1': 0.08147512160201914, 'Validation Loss (Hydroxylation-K)': 0.047287549570292055, 'Hydroxylation-P Validation Accuracy': 0.041325156358210956, 'Hydroxylation-P Validation Sensitivity': 0.07817493889756827, 'Hydroxylation-P Validation Specificity': 0.05774822315165673, 'Hydroxylation-P Validation Precision': 0.06527340660768025, 'Hydroxylation-P AUC ROC': 0.039869525418619677, 'Hydroxylation-P AUC PR': 0.0735709420962505, 'Hydroxylation-P MCC': 0.06205504566750745, 'Hydroxylation-P F1': 0.049085225995008475, 'Validation Loss (Hydroxylation-P)': 0.03509870384195084, 'Validation Loss (total)': 0.050382839219279156, 'TimeToTrain': 2.347139886363093}
