{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'learning_rate': 0.009816133570445872,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3029327766,
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.62502644751757}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.675
[3,    32] loss: 0.668
[4,    32] loss: 0.653
[5,    32] loss: 0.659
[6,    32] loss: 0.655
[7,    32] loss: 0.658
[8,    32] loss: 0.649
[9,    32] loss: 0.695
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
Early stopping applied (best metric=0.5186650156974792)
Finished Training
Total time taken: 86.77903318405151
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.658
[3,    32] loss: 0.654
[4,    32] loss: 0.657
[5,    32] loss: 0.674
[6,    32] loss: 0.671
[7,    32] loss: 0.662
[8,    32] loss: 0.659
[9,    32] loss: 0.674
[10,    32] loss: 0.669
[11,    32] loss: 0.671
[12,    32] loss: 0.668
[13,    32] loss: 0.672
[14,    32] loss: 0.672
[15,    32] loss: 0.669
[16,    32] loss: 0.669
[17,    32] loss: 0.674
[18,    32] loss: 0.676
[19,    32] loss: 0.668
[20,    32] loss: 0.683
[21,    32] loss: 0.680
[22,    32] loss: 0.671
[23,    32] loss: 0.673
[24,    32] loss: 0.687
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
[31,    32] loss: 0.693
Early stopping applied (best metric=0.5158849358558655)
Finished Training
Total time taken: 78.70970320701599
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.664
[3,    32] loss: 0.665
[4,    32] loss: 0.671
[5,    32] loss: 0.680
[6,    32] loss: 0.680
[7,    32] loss: 0.673
[8,    32] loss: 0.661
[9,    32] loss: 0.665
[10,    32] loss: 0.662
[11,    32] loss: 0.654
[12,    32] loss: 0.651
[13,    32] loss: 0.648
[14,    32] loss: 0.671
[15,    32] loss: 0.658
[16,    32] loss: 0.672
[17,    32] loss: 0.659
[18,    32] loss: 0.649
[19,    32] loss: 0.653
[20,    32] loss: 0.653
[21,    32] loss: 0.653
[22,    32] loss: 0.681
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
[31,    32] loss: 0.693
[32,    32] loss: 0.693
[33,    32] loss: 0.693
[34,    32] loss: 0.693
[35,    32] loss: 0.693
[36,    32] loss: 0.693
[37,    32] loss: 0.693
[38,    32] loss: 0.693
[39,    32] loss: 0.693
[40,    32] loss: 0.693
[41,    32] loss: 0.693
[42,    32] loss: 0.693
[43,    32] loss: 0.693
[44,    32] loss: 0.693
Early stopping applied (best metric=0.5135793685913086)
Finished Training
Total time taken: 106.10683012008667
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.662
[3,    32] loss: 0.675
[4,    32] loss: 0.669
[5,    32] loss: 0.687
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
Early stopping applied (best metric=0.5389756560325623)
Finished Training
Total time taken: 69.58407521247864
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.693
[5,    32] loss: 0.693
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
Early stopping applied (best metric=0.5550122261047363)
Finished Training
Total time taken: 72.01808047294617
{'S-palmitoylation-C Validation Accuracy': 0.6303359546809328, 'S-palmitoylation-C Validation Sensitivity': 0.5651485148514852, 'S-palmitoylation-C Validation Specificity': 0.6466823057456366, 'S-palmitoylation-C Validation Precision': nan, 'S-palmitoylation-C AUC ROC': 0.6493833362272753, 'S-palmitoylation-C AUC PR': 0.41267132511686394, 'S-palmitoylation-C MCC': 0.17787695873268686, 'S-palmitoylation-C F1': 0.32821727151295554, 'Validation Loss (S-palmitoylation-C)': 0.5284234404563903, 'Validation Loss (total)': 0.5284234404563903, 'TimeToTrain': 82.6395444393158}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00022214677274348503,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3789099694,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.75103803525827}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.666
[3,    32] loss: 0.630
[4,    32] loss: 0.617
[5,    32] loss: 0.603
[6,    32] loss: 0.599
[7,    32] loss: 0.589
[8,    32] loss: 0.582
[9,    32] loss: 0.575
[10,    32] loss: 0.566
[11,    32] loss: 0.551
[12,    32] loss: 0.539
[13,    32] loss: 0.532
[14,    32] loss: 0.527
[15,    32] loss: 0.520
[16,    32] loss: 0.504
[17,    32] loss: 0.492
[18,    32] loss: 0.482
[19,    32] loss: 0.482
[20,    32] loss: 0.471
[21,    32] loss: 0.470
[22,    32] loss: 0.455
[23,    32] loss: 0.452
[24,    32] loss: 0.452
[25,    32] loss: 0.435
[26,    32] loss: 0.443
[27,    32] loss: 0.432
[28,    32] loss: 0.427
[29,    32] loss: 0.426
[30,    32] loss: 0.427
[31,    32] loss: 0.421
[32,    32] loss: 0.411
[33,    32] loss: 0.413
[34,    32] loss: 0.410
[35,    32] loss: 0.415
[36,    32] loss: 0.408
[37,    32] loss: 0.407
[38,    32] loss: 0.410
[39,    32] loss: 0.404
[40,    32] loss: 0.397
Early stopping applied (best metric=0.4735942482948303)
Finished Training
Total time taken: 96.25010466575623
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.674
[3,    32] loss: 0.637
[4,    32] loss: 0.613
[5,    32] loss: 0.603
[6,    32] loss: 0.604
[7,    32] loss: 0.595
[8,    32] loss: 0.575
[9,    32] loss: 0.571
[10,    32] loss: 0.559
[11,    32] loss: 0.548
[12,    32] loss: 0.534
[13,    32] loss: 0.524
[14,    32] loss: 0.509
[15,    32] loss: 0.499
[16,    32] loss: 0.488
[17,    32] loss: 0.484
[18,    32] loss: 0.471
[19,    32] loss: 0.462
[20,    32] loss: 0.452
[21,    32] loss: 0.452
[22,    32] loss: 0.447
[23,    32] loss: 0.441
[24,    32] loss: 0.446
[25,    32] loss: 0.433
[26,    32] loss: 0.423
[27,    32] loss: 0.423
[28,    32] loss: 0.421
[29,    32] loss: 0.404
[30,    32] loss: 0.416
[31,    32] loss: 0.407
[32,    32] loss: 0.408
[33,    32] loss: 0.410
[34,    32] loss: 0.402
[35,    32] loss: 0.396
[36,    32] loss: 0.402
[37,    32] loss: 0.404
[38,    32] loss: 0.403
[39,    32] loss: 0.407
[40,    32] loss: 0.395
[41,    32] loss: 0.382
[42,    32] loss: 0.402
[43,    32] loss: 0.393
[44,    32] loss: 0.389
[45,    32] loss: 0.424
[46,    32] loss: 0.395
[47,    32] loss: 0.396
[48,    32] loss: 0.390
[49,    32] loss: 0.386
Early stopping applied (best metric=0.483694463968277)
Finished Training
Total time taken: 117.52712917327881
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.677
[3,    32] loss: 0.638
[4,    32] loss: 0.618
[5,    32] loss: 0.608
[6,    32] loss: 0.598
[7,    32] loss: 0.596
[8,    32] loss: 0.583
[9,    32] loss: 0.576
[10,    32] loss: 0.565
[11,    32] loss: 0.547
[12,    32] loss: 0.539
[13,    32] loss: 0.537
[14,    32] loss: 0.525
[15,    32] loss: 0.506
[16,    32] loss: 0.502
[17,    32] loss: 0.488
[18,    32] loss: 0.482
[19,    32] loss: 0.485
[20,    32] loss: 0.469
[21,    32] loss: 0.459
[22,    32] loss: 0.447
[23,    32] loss: 0.444
[24,    32] loss: 0.441
[25,    32] loss: 0.435
[26,    32] loss: 0.430
[27,    32] loss: 0.427
[28,    32] loss: 0.429
[29,    32] loss: 0.416
[30,    32] loss: 0.427
[31,    32] loss: 0.417
[32,    32] loss: 0.415
[33,    32] loss: 0.413
[34,    32] loss: 0.399
[35,    32] loss: 0.406
[36,    32] loss: 0.421
[37,    32] loss: 0.397
Early stopping applied (best metric=0.46600401401519775)
Finished Training
Total time taken: 88.79709434509277
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.656
[3,    32] loss: 0.631
[4,    32] loss: 0.616
[5,    32] loss: 0.600
[6,    32] loss: 0.596
[7,    32] loss: 0.591
[8,    32] loss: 0.577
[9,    32] loss: 0.577
[10,    32] loss: 0.567
[11,    32] loss: 0.557
[12,    32] loss: 0.544
[13,    32] loss: 0.535
[14,    32] loss: 0.526
[15,    32] loss: 0.511
[16,    32] loss: 0.500
[17,    32] loss: 0.497
[18,    32] loss: 0.477
[19,    32] loss: 0.472
[20,    32] loss: 0.463
[21,    32] loss: 0.455
[22,    32] loss: 0.445
[23,    32] loss: 0.442
[24,    32] loss: 0.443
[25,    32] loss: 0.448
[26,    32] loss: 0.433
[27,    32] loss: 0.433
[28,    32] loss: 0.432
[29,    32] loss: 0.424
[30,    32] loss: 0.413
[31,    32] loss: 0.418
[32,    32] loss: 0.415
[33,    32] loss: 0.419
[34,    32] loss: 0.407
[35,    32] loss: 0.403
Early stopping applied (best metric=0.47382259368896484)
Finished Training
Total time taken: 84.11809206008911
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.665
[3,    32] loss: 0.632
[4,    32] loss: 0.621
[5,    32] loss: 0.608
[6,    32] loss: 0.595
[7,    32] loss: 0.593
[8,    32] loss: 0.585
[9,    32] loss: 0.574
[10,    32] loss: 0.560
[11,    32] loss: 0.551
[12,    32] loss: 0.538
[13,    32] loss: 0.524
[14,    32] loss: 0.516
[15,    32] loss: 0.513
[16,    32] loss: 0.502
[17,    32] loss: 0.490
[18,    32] loss: 0.485
[19,    32] loss: 0.478
[20,    32] loss: 0.464
[21,    32] loss: 0.456
[22,    32] loss: 0.461
[23,    32] loss: 0.444
[24,    32] loss: 0.442
[25,    32] loss: 0.435
[26,    32] loss: 0.439
[27,    32] loss: 0.440
[28,    32] loss: 0.429
[29,    32] loss: 0.424
[30,    32] loss: 0.431
[31,    32] loss: 0.415
[32,    32] loss: 0.413
[33,    32] loss: 0.405
[34,    32] loss: 0.418
Early stopping applied (best metric=0.4791434407234192)
Finished Training
Total time taken: 81.48708987236023
{'S-palmitoylation-C Validation Accuracy': 0.7043978462100733, 'S-palmitoylation-C Validation Sensitivity': 0.6427722772277228, 'S-palmitoylation-C Validation Specificity': 0.7198456462331915, 'S-palmitoylation-C Validation Precision': 0.36561983337157944, 'S-palmitoylation-C AUC ROC': 0.7492431714211609, 'S-palmitoylation-C AUC PR': 0.44376782783405766, 'S-palmitoylation-C MCC': 0.30403933991440935, 'S-palmitoylation-C F1': 0.46596621852505044, 'Validation Loss (S-palmitoylation-C)': 0.4752517521381378, 'Validation Loss (total)': 0.4752517521381378, 'TimeToTrain': 93.63590202331542}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00924411564741312,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1010744165,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.616202240494008}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.693
[5,    32] loss: 0.693
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
[31,    32] loss: 0.693
[32,    32] loss: 0.693
[33,    32] loss: 0.693
[34,    32] loss: 0.693
[35,    32] loss: 0.693
Early stopping applied (best metric=0.5549297332763672)
Finished Training
Total time taken: 83.71809101104736
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.672
[3,    32] loss: 0.675
[4,    32] loss: 0.693
[5,    32] loss: 0.693
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
Early stopping applied (best metric=0.5306113958358765)
Finished Training
Total time taken: 64.8240704536438
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.667
[3,    32] loss: 0.652
[4,    32] loss: 0.654
[5,    32] loss: 0.656
[6,    32] loss: 0.660
[7,    32] loss: 0.677
[8,    32] loss: 0.672
[9,    32] loss: 0.657
[10,    32] loss: 0.665
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
Early stopping applied (best metric=0.5109158158302307)
Finished Training
Total time taken: 66.81707262992859
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.693
[5,    32] loss: 0.693
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
Early stopping applied (best metric=0.5550122261047363)
Finished Training
Total time taken: 69.55507636070251
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.666
[3,    32] loss: 0.661
[4,    32] loss: 0.660
[5,    32] loss: 0.664
[6,    32] loss: 0.660
[7,    32] loss: 0.660
[8,    32] loss: 0.661
[9,    32] loss: 0.664
[10,    32] loss: 0.677
[11,    32] loss: 0.678
[12,    32] loss: 0.690
[13,    32] loss: 0.689
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
[28,    32] loss: 0.693
[29,    32] loss: 0.693
[30,    32] loss: 0.693
[31,    32] loss: 0.693
[32,    32] loss: 0.693
Early stopping applied (best metric=0.5218244791030884)
Finished Training
Total time taken: 76.6230821609497
{'S-palmitoylation-C Validation Accuracy': 0.7282924062836726, 'S-palmitoylation-C Validation Sensitivity': 0.35128712871287127, 'S-palmitoylation-C Validation Specificity': 0.8227995594116618, 'S-palmitoylation-C Validation Precision': nan, 'S-palmitoylation-C AUC ROC': 0.624693524806979, 'S-palmitoylation-C AUC PR': 0.4734810865102525, 'S-palmitoylation-C MCC': 0.1467958150017267, 'S-palmitoylation-C F1': 0.25333905827853564, 'Validation Loss (S-palmitoylation-C)': 0.5346587300300598, 'Validation Loss (total)': 0.5346587300300598, 'TimeToTrain': 72.30747852325439}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004175668434568369,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2277442400,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.48676539587773}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.643
[3,    32] loss: 0.624
[4,    32] loss: 0.606
[5,    32] loss: 0.594
[6,    32] loss: 0.585
[7,    32] loss: 0.568
[8,    32] loss: 0.550
[9,    32] loss: 0.537
[10,    32] loss: 0.530
[11,    32] loss: 0.539
[12,    32] loss: 0.517
[13,    32] loss: 0.522
[14,    32] loss: 0.514
[15,    32] loss: 0.500
[16,    32] loss: 0.498
[17,    32] loss: 0.491
[18,    32] loss: 0.495
[19,    32] loss: 0.493
[20,    32] loss: 0.486
[21,    32] loss: 0.491
[22,    32] loss: 0.486
[23,    32] loss: 0.479
[24,    32] loss: 0.483
[25,    32] loss: 0.484
[26,    32] loss: 0.487
[27,    32] loss: 0.483
[28,    32] loss: 0.487
[29,    32] loss: 0.485
[30,    32] loss: 0.494
[31,    32] loss: 0.480
[32,    32] loss: 0.487
[33,    32] loss: 0.479
Early stopping applied (best metric=0.4716671109199524)
Finished Training
Total time taken: 79.04508566856384
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.646
[3,    32] loss: 0.621
[4,    32] loss: 0.606
[5,    32] loss: 0.597
[6,    32] loss: 0.585
[7,    32] loss: 0.575
[8,    32] loss: 0.560
[9,    32] loss: 0.550
[10,    32] loss: 0.536
[11,    32] loss: 0.532
[12,    32] loss: 0.527
[13,    32] loss: 0.514
[14,    32] loss: 0.506
[15,    32] loss: 0.515
[16,    32] loss: 0.500
[17,    32] loss: 0.491
[18,    32] loss: 0.487
[19,    32] loss: 0.480
[20,    32] loss: 0.500
[21,    32] loss: 0.489
[22,    32] loss: 0.489
[23,    32] loss: 0.486
[24,    32] loss: 0.478
[25,    32] loss: 0.496
[26,    32] loss: 0.484
[27,    32] loss: 0.482
[28,    32] loss: 0.480
[29,    32] loss: 0.475
[30,    32] loss: 0.486
[31,    32] loss: 0.493
[32,    32] loss: 0.490
[33,    32] loss: 0.476
[34,    32] loss: 0.482
[35,    32] loss: 0.488
[36,    32] loss: 0.493
[37,    32] loss: 0.489
[38,    32] loss: 0.476
[39,    32] loss: 0.484
[40,    32] loss: 0.477
[41,    32] loss: 0.478
[42,    32] loss: 0.484
[43,    32] loss: 0.480
[44,    32] loss: 0.488
[45,    32] loss: 0.483
[46,    32] loss: 0.479
[47,    32] loss: 0.481
[48,    32] loss: 0.483
[49,    32] loss: 0.487
[50,    32] loss: 0.484
[51,    32] loss: 0.493
[52,    32] loss: 0.481
Early stopping applied (best metric=0.46197810769081116)
Finished Training
Total time taken: 124.41613721847534
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.647
[3,    32] loss: 0.627
[4,    32] loss: 0.606
[5,    32] loss: 0.595
[6,    32] loss: 0.590
[7,    32] loss: 0.575
[8,    32] loss: 0.561
[9,    32] loss: 0.549
[10,    32] loss: 0.537
[11,    32] loss: 0.537
[12,    32] loss: 0.516
[13,    32] loss: 0.517
[14,    32] loss: 0.515
[15,    32] loss: 0.498
[16,    32] loss: 0.505
[17,    32] loss: 0.496
[18,    32] loss: 0.505
[19,    32] loss: 0.509
[20,    32] loss: 0.486
[21,    32] loss: 0.489
[22,    32] loss: 0.486
[23,    32] loss: 0.495
[24,    32] loss: 0.486
[25,    32] loss: 0.485
[26,    32] loss: 0.486
[27,    32] loss: 0.490
[28,    32] loss: 0.479
[29,    32] loss: 0.494
[30,    32] loss: 0.483
[31,    32] loss: 0.483
[32,    32] loss: 0.480
[33,    32] loss: 0.488
[34,    32] loss: 0.484
[35,    32] loss: 0.498
[36,    32] loss: 0.489
[37,    32] loss: 0.488
[38,    32] loss: 0.489
[39,    32] loss: 0.487
[40,    32] loss: 0.497
[41,    32] loss: 0.479
[42,    32] loss: 0.494
[43,    32] loss: 0.481
[44,    32] loss: 0.484
[45,    32] loss: 0.488
[46,    32] loss: 0.490
[47,    32] loss: 0.497
[48,    32] loss: 0.490
[49,    32] loss: 0.484
[50,    32] loss: 0.487
[51,    32] loss: 0.499
[52,    32] loss: 0.483
[53,    32] loss: 0.487
[54,    32] loss: 0.489
[55,    32] loss: 0.489
[56,    32] loss: 0.487
[57,    32] loss: 0.490
Early stopping applied (best metric=0.4745967984199524)
Finished Training
Total time taken: 136.3501501083374
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.648
[3,    32] loss: 0.623
[4,    32] loss: 0.601
[5,    32] loss: 0.592
[6,    32] loss: 0.580
[7,    32] loss: 0.574
[8,    32] loss: 0.562
[9,    32] loss: 0.544
[10,    32] loss: 0.545
[11,    32] loss: 0.533
[12,    32] loss: 0.523
[13,    32] loss: 0.510
[14,    32] loss: 0.500
[15,    32] loss: 0.508
[16,    32] loss: 0.493
[17,    32] loss: 0.496
[18,    32] loss: 0.488
[19,    32] loss: 0.495
[20,    32] loss: 0.493
[21,    32] loss: 0.491
[22,    32] loss: 0.479
[23,    32] loss: 0.477
[24,    32] loss: 0.468
[25,    32] loss: 0.471
[26,    32] loss: 0.479
[27,    32] loss: 0.478
[28,    32] loss: 0.476
[29,    32] loss: 0.482
[30,    32] loss: 0.471
[31,    32] loss: 0.480
[32,    32] loss: 0.478
[33,    32] loss: 0.477
[34,    32] loss: 0.470
[35,    32] loss: 0.478
[36,    32] loss: 0.475
[37,    32] loss: 0.475
[38,    32] loss: 0.481
Early stopping applied (best metric=0.4687764644622803)
Finished Training
Total time taken: 91.2360999584198
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.642
[3,    32] loss: 0.615
[4,    32] loss: 0.599
[5,    32] loss: 0.598
[6,    32] loss: 0.579
[7,    32] loss: 0.569
[8,    32] loss: 0.561
[9,    32] loss: 0.538
[10,    32] loss: 0.542
[11,    32] loss: 0.527
[12,    32] loss: 0.515
[13,    32] loss: 0.507
[14,    32] loss: 0.499
[15,    32] loss: 0.509
[16,    32] loss: 0.492
[17,    32] loss: 0.489
[18,    32] loss: 0.490
[19,    32] loss: 0.493
[20,    32] loss: 0.479
[21,    32] loss: 0.476
[22,    32] loss: 0.473
[23,    32] loss: 0.489
[24,    32] loss: 0.480
[25,    32] loss: 0.487
[26,    32] loss: 0.484
[27,    32] loss: 0.483
[28,    32] loss: 0.482
[29,    32] loss: 0.478
[30,    32] loss: 0.474
[31,    32] loss: 0.491
[32,    32] loss: 0.480
[33,    32] loss: 0.480
[34,    32] loss: 0.478
[35,    32] loss: 0.484
[36,    32] loss: 0.481
[37,    32] loss: 0.481
[38,    32] loss: 0.482
Early stopping applied (best metric=0.47012919187545776)
Finished Training
Total time taken: 90.91209697723389
{'S-palmitoylation-C Validation Accuracy': 0.6955842895580887, 'S-palmitoylation-C Validation Sensitivity': 0.6796039603960397, 'S-palmitoylation-C Validation Specificity': 0.6995877985614347, 'S-palmitoylation-C Validation Precision': 0.3623902106103899, 'S-palmitoylation-C AUC ROC': 0.7580816993377146, 'S-palmitoylation-C AUC PR': 0.4535411260939377, 'S-palmitoylation-C MCC': 0.3137196864745068, 'S-palmitoylation-C F1': 0.47230355231456533, 'Validation Loss (S-palmitoylation-C)': 0.4694295346736908, 'Validation Loss (total)': 0.4694295346736908, 'TimeToTrain': 104.39191398620605}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0071572141670296924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 374266411,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.342250029894464}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.635
[3,    32] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048012783721161955,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1490606763,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.276556980320688}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.647
[3,    32] loss: 0.635
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006617479861609807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2414855479,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.154106252240636}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.638
[3,    32] loss: 0.623
[4,    32] loss: 0.618
[5,    32] loss: 0.632
[6,    32] loss: 0.619
[7,    32] loss: 0.617
[8,    32] loss: 0.614
[9,    32] loss: 0.614
[10,    32] loss: 0.618
[11,    32] loss: 0.622
[12,    32] loss: 0.616
[13,    32] loss: 0.616
[14,    32] loss: 0.629
[15,    32] loss: 0.620
[16,    32] loss: 0.617
[17,    32] loss: 0.622
[18,    32] loss: 0.611
[19,    32] loss: 0.616
[20,    32] loss: 0.618
[21,    32] loss: 0.619
[22,    32] loss: 0.616
[23,    32] loss: 0.626
[24,    32] loss: 0.619
[25,    32] loss: 0.614
[26,    32] loss: 0.620
[27,    32] loss: 0.620
[28,    32] loss: 0.625
[29,    32] loss: 0.619
[30,    32] loss: 0.617
[31,    32] loss: 0.618
[32,    32] loss: 0.618
[33,    32] loss: 0.618
[34,    32] loss: 0.620
[35,    32] loss: 0.616
[36,    32] loss: 0.627
[37,    32] loss: 0.618
[38,    32] loss: 0.619
[39,    32] loss: 0.619
[40,    32] loss: 0.615
[41,    32] loss: 0.619
[42,    32] loss: 0.619
[43,    32] loss: 0.623
[44,    32] loss: 0.616
[45,    32] loss: 0.618
[46,    32] loss: 0.615
[47,    32] loss: 0.618
[48,    32] loss: 0.613
[49,    32] loss: 0.619
[50,    32] loss: 0.619
[51,    32] loss: 0.623
Early stopping applied (best metric=0.49170276522636414)
Finished Training
Total time taken: 122.1151351928711
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.632
[3,    32] loss: 0.626
[4,    32] loss: 0.625
[5,    32] loss: 0.626
[6,    32] loss: 0.620
[7,    32] loss: 0.614
[8,    32] loss: 0.618
[9,    32] loss: 0.619
[10,    32] loss: 0.612
[11,    32] loss: 0.616
[12,    32] loss: 0.617
[13,    32] loss: 0.618
[14,    32] loss: 0.612
[15,    32] loss: 0.616
[16,    32] loss: 0.613
[17,    32] loss: 0.610
[18,    32] loss: 0.618
[19,    32] loss: 0.618
[20,    32] loss: 0.612
[21,    32] loss: 0.621
[22,    32] loss: 0.624
[23,    32] loss: 0.620
[24,    32] loss: 0.609
[25,    32] loss: 0.618
[26,    32] loss: 0.618
[27,    32] loss: 0.615
[28,    32] loss: 0.619
[29,    32] loss: 0.607
[30,    32] loss: 0.622
[31,    32] loss: 0.620
[32,    32] loss: 0.621
[33,    32] loss: 0.614
[34,    32] loss: 0.614
[35,    32] loss: 0.621
Early stopping applied (best metric=0.48199066519737244)
Finished Training
Total time taken: 83.80309081077576
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.635
[3,    32] loss: 0.636
[4,    32] loss: 0.623
[5,    32] loss: 0.624
[6,    32] loss: 0.619
[7,    32] loss: 0.622
[8,    32] loss: 0.620
[9,    32] loss: 0.620
[10,    32] loss: 0.618
[11,    32] loss: 0.616
[12,    32] loss: 0.619
[13,    32] loss: 0.629
[14,    32] loss: 0.617
[15,    32] loss: 0.611
[16,    32] loss: 0.612
[17,    32] loss: 0.610
[18,    32] loss: 0.617
[19,    32] loss: 0.622
[20,    32] loss: 0.616
[21,    32] loss: 0.620
[22,    32] loss: 0.616
[23,    32] loss: 0.619
[24,    32] loss: 0.619
[25,    32] loss: 0.613
[26,    32] loss: 0.625
[27,    32] loss: 0.626
[28,    32] loss: 0.617
[29,    32] loss: 0.611
[30,    32] loss: 0.620
[31,    32] loss: 0.612
[32,    32] loss: 0.617
[33,    32] loss: 0.622
[34,    32] loss: 0.618
[35,    32] loss: 0.613
[36,    32] loss: 0.613
[37,    32] loss: 0.622
[38,    32] loss: 0.621
[39,    32] loss: 0.615
[40,    32] loss: 0.617
[41,    32] loss: 0.615
[42,    32] loss: 0.618
[43,    32] loss: 0.621
[44,    32] loss: 0.625
[45,    32] loss: 0.611
[46,    32] loss: 0.614
[47,    32] loss: 0.621
[48,    32] loss: 0.618
[49,    32] loss: 0.622
[50,    32] loss: 0.618
[51,    32] loss: 0.616
[52,    32] loss: 0.620
[53,    32] loss: 0.613
[54,    32] loss: 0.615
[55,    32] loss: 0.622
[56,    32] loss: 0.616
[57,    32] loss: 0.613
[58,    32] loss: 0.613
[59,    32] loss: 0.616
[60,    32] loss: 0.615
[61,    32] loss: 0.618
[62,    32] loss: 0.617
[63,    32] loss: 0.627
[64,    32] loss: 0.623
[65,    32] loss: 0.612
[66,    32] loss: 0.618
[67,    32] loss: 0.632
[68,    32] loss: 0.619
[69,    32] loss: 0.619
[70,    32] loss: 0.620
[71,    32] loss: 0.621
[72,    32] loss: 0.617
[73,    32] loss: 0.614
[74,    32] loss: 0.619
[75,    32] loss: 0.623
[76,    32] loss: 0.622
[77,    32] loss: 0.618
[78,    32] loss: 0.617
Early stopping applied (best metric=0.4893016219139099)
Finished Training
Total time taken: 186.3462038040161
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.628
[3,    32] loss: 0.624
[4,    32] loss: 0.624
[5,    32] loss: 0.609
[6,    32] loss: 0.611
[7,    32] loss: 0.616
[8,    32] loss: 0.621
[9,    32] loss: 0.619
[10,    32] loss: 0.617
[11,    32] loss: 0.613
[12,    32] loss: 0.616
[13,    32] loss: 0.614
[14,    32] loss: 0.619
[15,    32] loss: 0.614
[16,    32] loss: 0.622
[17,    32] loss: 0.618
[18,    32] loss: 0.614
[19,    32] loss: 0.620
[20,    32] loss: 0.618
[21,    32] loss: 0.610
[22,    32] loss: 0.612
[23,    32] loss: 0.619
[24,    32] loss: 0.620
[25,    32] loss: 0.621
[26,    32] loss: 0.616
[27,    32] loss: 0.616
[28,    32] loss: 0.617
[29,    32] loss: 0.618
[30,    32] loss: 0.610
Early stopping applied (best metric=0.48539525270462036)
Finished Training
Total time taken: 71.77007746696472
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.641
[3,    32] loss: 0.629
[4,    32] loss: 0.616
[5,    32] loss: 0.629
[6,    32] loss: 0.617
[7,    32] loss: 0.613
[8,    32] loss: 0.620
[9,    32] loss: 0.618
[10,    32] loss: 0.622
[11,    32] loss: 0.619
[12,    32] loss: 0.628
[13,    32] loss: 0.620
[14,    32] loss: 0.617
[15,    32] loss: 0.623
[16,    32] loss: 0.612
[17,    32] loss: 0.619
[18,    32] loss: 0.625
[19,    32] loss: 0.614
[20,    32] loss: 0.620
[21,    32] loss: 0.618
[22,    32] loss: 0.617
[23,    32] loss: 0.621
[24,    32] loss: 0.618
[25,    32] loss: 0.623
[26,    32] loss: 0.620
[27,    32] loss: 0.613
[28,    32] loss: 0.614
[29,    32] loss: 0.619
[30,    32] loss: 0.608
[31,    32] loss: 0.618
[32,    32] loss: 0.617
[33,    32] loss: 0.617
Early stopping applied (best metric=0.48041069507598877)
Finished Training
Total time taken: 78.99259877204895
{'S-palmitoylation-C Validation Accuracy': 0.6589138105950333, 'S-palmitoylation-C Validation Sensitivity': 0.6887128712871288, 'S-palmitoylation-C Validation Specificity': 0.6514432225045033, 'S-palmitoylation-C Validation Precision': 0.33259806432581135, 'S-palmitoylation-C AUC ROC': 0.7350816772092441, 'S-palmitoylation-C AUC PR': 0.43274777545701654, 'S-palmitoylation-C MCC': 0.2771897375216892, 'S-palmitoylation-C F1': 0.4477697336440857, 'Validation Loss (S-palmitoylation-C)': 0.4857602000236511, 'Validation Loss (total)': 0.4857602000236511, 'TimeToTrain': 108.60542120933533}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006304592262528562,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3340717910,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.435211177503852}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.639
[3,    32] loss: 0.616
[4,    32] loss: 0.615
[5,    32] loss: 0.602
[6,    32] loss: 0.596
[7,    32] loss: 0.605
[8,    32] loss: 0.594
[9,    32] loss: 0.592
[10,    32] loss: 0.597
[11,    32] loss: 0.604
[12,    32] loss: 0.603
[13,    32] loss: 0.600
[14,    32] loss: 0.594
[15,    32] loss: 0.597
[16,    32] loss: 0.597
[17,    32] loss: 0.599
[18,    32] loss: 0.599
[19,    32] loss: 0.596
[20,    32] loss: 0.595
[21,    32] loss: 0.596
[22,    32] loss: 0.600
[23,    32] loss: 0.590
[24,    32] loss: 0.599
[25,    32] loss: 0.599
[26,    32] loss: 0.603
[27,    32] loss: 0.599
[28,    32] loss: 0.592
[29,    32] loss: 0.600
[30,    32] loss: 0.606
[31,    32] loss: 0.598
[32,    32] loss: 0.589
[33,    32] loss: 0.595
[34,    32] loss: 0.601
[35,    32] loss: 0.595
Early stopping applied (best metric=0.4778461456298828)
Finished Training
Total time taken: 83.56509017944336
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.620
[3,    32] loss: 0.613
[4,    32] loss: 0.606
[5,    32] loss: 0.600
[6,    32] loss: 0.596
[7,    32] loss: 0.593
[8,    32] loss: 0.596
[9,    32] loss: 0.597
[10,    32] loss: 0.589
[11,    32] loss: 0.601
[12,    32] loss: 0.596
[13,    32] loss: 0.594
[14,    32] loss: 0.601
[15,    32] loss: 0.599
[16,    32] loss: 0.592
[17,    32] loss: 0.598
[18,    32] loss: 0.597
[19,    32] loss: 0.595
[20,    32] loss: 0.601
[21,    32] loss: 0.598
[22,    32] loss: 0.595
[23,    32] loss: 0.590
[24,    32] loss: 0.600
[25,    32] loss: 0.600
[26,    32] loss: 0.599
[27,    32] loss: 0.603
[28,    32] loss: 0.603
[29,    32] loss: 0.607
[30,    32] loss: 0.603
[31,    32] loss: 0.589
[32,    32] loss: 0.603
[33,    32] loss: 0.597
[34,    32] loss: 0.588
[35,    32] loss: 0.598
[36,    32] loss: 0.595
[37,    32] loss: 0.601
[38,    32] loss: 0.597
[39,    32] loss: 0.595
[40,    32] loss: 0.598
[41,    32] loss: 0.594
[42,    32] loss: 0.591
[43,    32] loss: 0.593
[44,    32] loss: 0.600
[45,    32] loss: 0.598
[46,    32] loss: 0.595
[47,    32] loss: 0.595
[48,    32] loss: 0.600
[49,    32] loss: 0.600
[50,    32] loss: 0.597
[51,    32] loss: 0.601
[52,    32] loss: 0.596
[53,    32] loss: 0.595
[54,    32] loss: 0.597
[55,    32] loss: 0.596
[56,    32] loss: 0.594
[57,    32] loss: 0.601
[58,    32] loss: 0.597
[59,    32] loss: 0.595
[60,    32] loss: 0.596
[61,    32] loss: 0.596
[62,    32] loss: 0.595
[63,    32] loss: 0.591
[64,    32] loss: 0.601
[65,    32] loss: 0.607
[66,    32] loss: 0.594
[67,    32] loss: 0.594
[68,    32] loss: 0.605
[69,    32] loss: 0.596
[70,    32] loss: 0.593
[71,    32] loss: 0.595
[72,    32] loss: 0.598
[73,    32] loss: 0.605
[74,    32] loss: 0.610
[75,    32] loss: 0.598
[76,    32] loss: 0.596
[77,    32] loss: 0.597
[78,    32] loss: 0.602
[79,    32] loss: 0.594
[80,    32] loss: 0.599
[81,    32] loss: 0.594
[82,    32] loss: 0.598
[83,    32] loss: 0.610
[84,    32] loss: 0.597
[85,    32] loss: 0.598
[86,    32] loss: 0.596
[87,    32] loss: 0.592
[88,    32] loss: 0.596
[89,    32] loss: 0.599
Early stopping applied (best metric=0.4828271269798279)
Finished Training
Total time taken: 212.28623056411743
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.615
[3,    32] loss: 0.606
[4,    32] loss: 0.599
[5,    32] loss: 0.594
[6,    32] loss: 0.593
[7,    32] loss: 0.598
[8,    32] loss: 0.593
[9,    32] loss: 0.604
[10,    32] loss: 0.596
[11,    32] loss: 0.598
[12,    32] loss: 0.595
[13,    32] loss: 0.590
[14,    32] loss: 0.599
[15,    32] loss: 0.603
[16,    32] loss: 0.594
[17,    32] loss: 0.599
[18,    32] loss: 0.593
[19,    32] loss: 0.596
[20,    32] loss: 0.600
[21,    32] loss: 0.594
[22,    32] loss: 0.593
[23,    32] loss: 0.597
[24,    32] loss: 0.600
[25,    32] loss: 0.595
[26,    32] loss: 0.596
[27,    32] loss: 0.604
[28,    32] loss: 0.595
[29,    32] loss: 0.595
[30,    32] loss: 0.599
[31,    32] loss: 0.594
[32,    32] loss: 0.600
[33,    32] loss: 0.599
[34,    32] loss: 0.595
[35,    32] loss: 0.595
[36,    32] loss: 0.603
[37,    32] loss: 0.596
[38,    32] loss: 0.602
[39,    32] loss: 0.593
[40,    32] loss: 0.596
[41,    32] loss: 0.595
[42,    32] loss: 0.604
[43,    32] loss: 0.607
[44,    32] loss: 0.595
[45,    32] loss: 0.599
[46,    32] loss: 0.594
[47,    32] loss: 0.593
[48,    32] loss: 0.596
[49,    32] loss: 0.595
[50,    32] loss: 0.594
[51,    32] loss: 0.594
[52,    32] loss: 0.599
[53,    32] loss: 0.602
[54,    32] loss: 0.606
[55,    32] loss: 0.595
[56,    32] loss: 0.592
[57,    32] loss: 0.602
[58,    32] loss: 0.609
[59,    32] loss: 0.600
[60,    32] loss: 0.605
Early stopping applied (best metric=0.4844801127910614)
Finished Training
Total time taken: 143.22515535354614
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.610
[3,    32] loss: 0.605
[4,    32] loss: 0.597
[5,    32] loss: 0.603
[6,    32] loss: 0.595
[7,    32] loss: 0.593
[8,    32] loss: 0.595
[9,    32] loss: 0.593
[10,    32] loss: 0.590
[11,    32] loss: 0.591
[12,    32] loss: 0.597
[13,    32] loss: 0.597
[14,    32] loss: 0.594
[15,    32] loss: 0.595
[16,    32] loss: 0.588
[17,    32] loss: 0.594
[18,    32] loss: 0.601
[19,    32] loss: 0.588
[20,    32] loss: 0.596
[21,    32] loss: 0.595
[22,    32] loss: 0.591
[23,    32] loss: 0.593
[24,    32] loss: 0.584
[25,    32] loss: 0.596
[26,    32] loss: 0.595
[27,    32] loss: 0.593
[28,    32] loss: 0.589
[29,    32] loss: 0.591
[30,    32] loss: 0.598
[31,    32] loss: 0.595
[32,    32] loss: 0.589
[33,    32] loss: 0.594
[34,    32] loss: 0.588
[35,    32] loss: 0.596
[36,    32] loss: 0.590
[37,    32] loss: 0.598
[38,    32] loss: 0.598
[39,    32] loss: 0.598
[40,    32] loss: 0.591
[41,    32] loss: 0.592
[42,    32] loss: 0.599
[43,    32] loss: 0.593
[44,    32] loss: 0.594
[45,    32] loss: 0.588
[46,    32] loss: 0.595
[47,    32] loss: 0.594
[48,    32] loss: 0.590
[49,    32] loss: 0.599
[50,    32] loss: 0.595
[51,    32] loss: 0.602
Early stopping applied (best metric=0.4940911531448364)
Finished Training
Total time taken: 121.64913320541382
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.632
[3,    32] loss: 0.616
[4,    32] loss: 0.606
[5,    32] loss: 0.600
[6,    32] loss: 0.608
[7,    32] loss: 0.594
[8,    32] loss: 0.599
[9,    32] loss: 0.599
[10,    32] loss: 0.597
[11,    32] loss: 0.596
[12,    32] loss: 0.597
[13,    32] loss: 0.596
[14,    32] loss: 0.601
[15,    32] loss: 0.592
[16,    32] loss: 0.596
[17,    32] loss: 0.597
[18,    32] loss: 0.602
[19,    32] loss: 0.589
[20,    32] loss: 0.596
[21,    32] loss: 0.596
[22,    32] loss: 0.599
[23,    32] loss: 0.592
[24,    32] loss: 0.605
[25,    32] loss: 0.604
[26,    32] loss: 0.595
[27,    32] loss: 0.596
[28,    32] loss: 0.601
[29,    32] loss: 0.593
[30,    32] loss: 0.603
[31,    32] loss: 0.598
[32,    32] loss: 0.595
[33,    32] loss: 0.598
[34,    32] loss: 0.596
[35,    32] loss: 0.597
[36,    32] loss: 0.602
[37,    32] loss: 0.598
[38,    32] loss: 0.596
[39,    32] loss: 0.604
[40,    32] loss: 0.596
[41,    32] loss: 0.595
[42,    32] loss: 0.596
[43,    32] loss: 0.600
[44,    32] loss: 0.597
[45,    32] loss: 0.604
Early stopping applied (best metric=0.48199281096458435)
Finished Training
Total time taken: 107.09911775588989
{'S-palmitoylation-C Validation Accuracy': 0.6570846644864112, 'S-palmitoylation-C Validation Sensitivity': 0.7001980198019803, 'S-palmitoylation-C Validation Specificity': 0.6462755254163781, 'S-palmitoylation-C Validation Precision': 0.332939616550641, 'S-palmitoylation-C AUC ROC': 0.7363303964599718, 'S-palmitoylation-C AUC PR': 0.41809837199810423, 'S-palmitoylation-C MCC': 0.28171867916341375, 'S-palmitoylation-C F1': 0.4504669236297475, 'Validation Loss (S-palmitoylation-C)': 0.4842474699020386, 'Validation Loss (total)': 0.4842474699020386, 'TimeToTrain': 133.56494541168212}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003250726028027023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3376742169,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.594123471554882}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.612
[3,    32] loss: 0.600
[4,    32] loss: 0.566
[5,    32] loss: 0.555
[6,    32] loss: 0.554
[7,    32] loss: 0.535
[8,    32] loss: 0.533
[9,    32] loss: 0.523
[10,    32] loss: 0.523
[11,    32] loss: 0.515
[12,    32] loss: 0.517
[13,    32] loss: 0.505
[14,    32] loss: 0.500
[15,    32] loss: 0.501
[16,    32] loss: 0.510
[17,    32] loss: 0.493
[18,    32] loss: 0.500
[19,    32] loss: 0.512
[20,    32] loss: 0.505
[21,    32] loss: 0.494
[22,    32] loss: 0.501
[23,    32] loss: 0.492
[24,    32] loss: 0.495
[25,    32] loss: 0.494
[26,    32] loss: 0.497
[27,    32] loss: 0.491
[28,    32] loss: 0.493
[29,    32] loss: 0.504
[30,    32] loss: 0.498
[31,    32] loss: 0.489
[32,    32] loss: 0.490
[33,    32] loss: 0.492
[34,    32] loss: 0.496
[35,    32] loss: 0.493
[36,    32] loss: 0.491
[37,    32] loss: 0.488
[38,    32] loss: 0.494
[39,    32] loss: 0.491
Early stopping applied (best metric=0.4749791622161865)
Finished Training
Total time taken: 92.69810056686401
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.615
[3,    32] loss: 0.588
[4,    32] loss: 0.569
[5,    32] loss: 0.568
[6,    32] loss: 0.552
[7,    32] loss: 0.539
[8,    32] loss: 0.522
[9,    32] loss: 0.524
[10,    32] loss: 0.519
[11,    32] loss: 0.516
[12,    32] loss: 0.519
[13,    32] loss: 0.503
[14,    32] loss: 0.509
[15,    32] loss: 0.501
[16,    32] loss: 0.502
[17,    32] loss: 0.498
[18,    32] loss: 0.496
[19,    32] loss: 0.502
[20,    32] loss: 0.497
[21,    32] loss: 0.496
[22,    32] loss: 0.491
[23,    32] loss: 0.500
[24,    32] loss: 0.492
[25,    32] loss: 0.500
[26,    32] loss: 0.484
[27,    32] loss: 0.497
[28,    32] loss: 0.493
[29,    32] loss: 0.493
[30,    32] loss: 0.490
[31,    32] loss: 0.489
Early stopping applied (best metric=0.4684966206550598)
Finished Training
Total time taken: 74.05808091163635
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.689
[2,    32] loss: 0.615
[3,    32] loss: 0.598
[4,    32] loss: 0.571
[5,    32] loss: 0.557
[6,    32] loss: 0.546
[7,    32] loss: 0.534
[8,    32] loss: 0.530
[9,    32] loss: 0.522
[10,    32] loss: 0.518
[11,    32] loss: 0.513
[12,    32] loss: 0.508
[13,    32] loss: 0.500
[14,    32] loss: 0.511
[15,    32] loss: 0.511
[16,    32] loss: 0.501
[17,    32] loss: 0.502
[18,    32] loss: 0.492
[19,    32] loss: 0.502
[20,    32] loss: 0.497
[21,    32] loss: 0.491
[22,    32] loss: 0.490
[23,    32] loss: 0.497
[24,    32] loss: 0.498
[25,    32] loss: 0.497
[26,    32] loss: 0.493
[27,    32] loss: 0.489
[28,    32] loss: 0.491
[29,    32] loss: 0.506
[30,    32] loss: 0.500
[31,    32] loss: 0.476
[32,    32] loss: 0.491
[33,    32] loss: 0.492
[34,    32] loss: 0.489
[35,    32] loss: 0.487
[36,    32] loss: 0.495
[37,    32] loss: 0.487
Early stopping applied (best metric=0.482410728931427)
Finished Training
Total time taken: 88.3430962562561
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.613
[3,    32] loss: 0.604
[4,    32] loss: 0.571
[5,    32] loss: 0.574
[6,    32] loss: 0.552
[7,    32] loss: 0.533
[8,    32] loss: 0.539
[9,    32] loss: 0.518
[10,    32] loss: 0.522
[11,    32] loss: 0.516
[12,    32] loss: 0.517
[13,    32] loss: 0.503
[14,    32] loss: 0.504
[15,    32] loss: 0.514
[16,    32] loss: 0.500
[17,    32] loss: 0.498
[18,    32] loss: 0.499
[19,    32] loss: 0.505
[20,    32] loss: 0.498
[21,    32] loss: 0.502
[22,    32] loss: 0.497
[23,    32] loss: 0.497
[24,    32] loss: 0.496
[25,    32] loss: 0.499
[26,    32] loss: 0.498
[27,    32] loss: 0.496
[28,    32] loss: 0.493
[29,    32] loss: 0.486
[30,    32] loss: 0.492
[31,    32] loss: 0.500
[32,    32] loss: 0.491
[33,    32] loss: 0.487
[34,    32] loss: 0.497
Early stopping applied (best metric=0.4796062707901001)
Finished Training
Total time taken: 81.44808912277222
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.612
[3,    32] loss: 0.585
[4,    32] loss: 0.578
[5,    32] loss: 0.571
[6,    32] loss: 0.549
[7,    32] loss: 0.539
[8,    32] loss: 0.537
[9,    32] loss: 0.529
[10,    32] loss: 0.521
[11,    32] loss: 0.513
[12,    32] loss: 0.518
[13,    32] loss: 0.506
[14,    32] loss: 0.510
[15,    32] loss: 0.498
[16,    32] loss: 0.495
[17,    32] loss: 0.511
[18,    32] loss: 0.487
[19,    32] loss: 0.494
[20,    32] loss: 0.489
[21,    32] loss: 0.498
[22,    32] loss: 0.492
[23,    32] loss: 0.500
[24,    32] loss: 0.498
[25,    32] loss: 0.492
[26,    32] loss: 0.493
[27,    32] loss: 0.495
[28,    32] loss: 0.490
[29,    32] loss: 0.491
[30,    32] loss: 0.492
[31,    32] loss: 0.500
[32,    32] loss: 0.496
[33,    32] loss: 0.491
[34,    32] loss: 0.496
[35,    32] loss: 0.492
[36,    32] loss: 0.494
[37,    32] loss: 0.491
[38,    32] loss: 0.488
[39,    32] loss: 0.490
[40,    32] loss: 0.491
[41,    32] loss: 0.495
[42,    32] loss: 0.492
[43,    32] loss: 0.487
[44,    32] loss: 0.496
[45,    32] loss: 0.495
[46,    32] loss: 0.484
[47,    32] loss: 0.488
[48,    32] loss: 0.495
[49,    32] loss: 0.495
[50,    32] loss: 0.492
Early stopping applied (best metric=0.4648094177246094)
Finished Training
Total time taken: 119.73112940788269
{'S-palmitoylation-C Validation Accuracy': 0.7156711532039043, 'S-palmitoylation-C Validation Sensitivity': 0.6506930693069307, 'S-palmitoylation-C Validation Specificity': 0.7319610370089276, 'S-palmitoylation-C Validation Precision': 0.37887890493027915, 'S-palmitoylation-C AUC ROC': 0.7555769162822384, 'S-palmitoylation-C AUC PR': 0.4543558593254686, 'S-palmitoylation-C MCC': 0.32275505086019046, 'S-palmitoylation-C F1': 0.478287225203485, 'Validation Loss (S-palmitoylation-C)': 0.4740604400634766, 'Validation Loss (total)': 0.4740604400634766, 'TimeToTrain': 91.25569925308227}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035645264668695683,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3038482358,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.27395305259914}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.615
[3,    32] loss: 0.608
[4,    32] loss: 0.598
[5,    32] loss: 0.598
[6,    32] loss: 0.593
[7,    32] loss: 0.606
[8,    32] loss: 0.591
[9,    32] loss: 0.592
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006992770814313742,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2562887985,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.360121465843353}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.649
[3,    32] loss: 0.641
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035233281828182302,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1577576366,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.278085136959392}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.627
[3,    32] loss: 0.613
[4,    32] loss: 0.611
[5,    32] loss: 0.614
[6,    32] loss: 0.605
[7,    32] loss: 0.613
[8,    32] loss: 0.614
[9,    32] loss: 0.611
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008016996930701395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3382129524,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.759732975229785}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006854365665656875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4142211996,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.16979883351695}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.622
[3,    32] loss: 0.609
[4,    32] loss: 0.592
[5,    32] loss: 0.569
[6,    32] loss: 0.562
[7,    32] loss: 0.547
[8,    32] loss: 0.535
[9,    32] loss: 0.522
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007312462515561137,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3626290750,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.401828861432033}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.701
[2,    32] loss: 0.656
[3,    32] loss: 0.634
[4,    32] loss: 0.650
[5,    32] loss: 0.643
[6,    32] loss: 0.635
[7,    32] loss: 0.636
[8,    32] loss: 0.631
[9,    32] loss: 0.635
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004783368403596891,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2547691983,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.395260020102922}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.631
[3,    32] loss: 0.608
[4,    32] loss: 0.595
[5,    32] loss: 0.590
[6,    32] loss: 0.587
[7,    32] loss: 0.584
[8,    32] loss: 0.584
[9,    32] loss: 0.586
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006445031459243692,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2161449496,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.05135805706765}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.653
[3,    32] loss: 0.650
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006849030453450333,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 48716439,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.141293389036182}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.671
[3,    32] loss: 0.645
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000612206528429907,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1221684756,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.175015848011787}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.629
[3,    32] loss: 0.615
[4,    32] loss: 0.597
[5,    32] loss: 0.591
[6,    32] loss: 0.583
[7,    32] loss: 0.567
[8,    32] loss: 0.554
[9,    32] loss: 0.543
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012432663600262057,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1872289844,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.292951447805635}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.632
[3,    32] loss: 0.621
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00031358383997174573,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 588913292,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.677515520132747}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.651
[3,    32] loss: 0.623
[4,    32] loss: 0.610
[5,    32] loss: 0.597
[6,    32] loss: 0.588
[7,    32] loss: 0.578
[8,    32] loss: 0.567
[9,    32] loss: 0.554
[10,    32] loss: 0.544
[11,    32] loss: 0.535
[12,    32] loss: 0.533
[13,    32] loss: 0.510
[14,    32] loss: 0.507
[15,    32] loss: 0.505
[16,    32] loss: 0.490
[17,    32] loss: 0.487
[18,    32] loss: 0.490
[19,    32] loss: 0.488
[20,    32] loss: 0.473
[21,    32] loss: 0.471
[22,    32] loss: 0.472
[23,    32] loss: 0.474
[24,    32] loss: 0.468
[25,    32] loss: 0.467
[26,    32] loss: 0.471
[27,    32] loss: 0.462
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00014920299971331362,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1381108221,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.5253905077849694}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.683
[3,    32] loss: 0.650
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027843578891316965,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4115261824,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.20620543963826}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.701
[2,    32] loss: 0.633
[3,    32] loss: 0.614
[4,    32] loss: 0.611
[5,    32] loss: 0.611
[6,    32] loss: 0.611
[7,    32] loss: 0.613
[8,    32] loss: 0.603
[9,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00028797762991933807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 110147183,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.805181764432723}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.665
[3,    32] loss: 0.631
[4,    32] loss: 0.613
[5,    32] loss: 0.605
[6,    32] loss: 0.594
[7,    32] loss: 0.585
[8,    32] loss: 0.569
[9,    32] loss: 0.564
[10,    32] loss: 0.545
[11,    32] loss: 0.539
[12,    32] loss: 0.520
[13,    32] loss: 0.504
[14,    32] loss: 0.495
[15,    32] loss: 0.484
[16,    32] loss: 0.468
[17,    32] loss: 0.467
[18,    32] loss: 0.460
[19,    32] loss: 0.462
[20,    32] loss: 0.448
[21,    32] loss: 0.440
[22,    32] loss: 0.437
[23,    32] loss: 0.428
[24,    32] loss: 0.431
[25,    32] loss: 0.416
[26,    32] loss: 0.414
[27,    32] loss: 0.415
[28,    32] loss: 0.413
[29,    32] loss: 0.412
[30,    32] loss: 0.416
[31,    32] loss: 0.399
[32,    32] loss: 0.405
[33,    32] loss: 0.407
[34,    32] loss: 0.405
[35,    32] loss: 0.401
[36,    32] loss: 0.401
[37,    32] loss: 0.390
[38,    32] loss: 0.391
[39,    32] loss: 0.399
[40,    32] loss: 0.394
Early stopping applied (best metric=0.46884942054748535)
Finished Training
Total time taken: 95.44610595703125
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.656
[3,    32] loss: 0.623
[4,    32] loss: 0.604
[5,    32] loss: 0.598
[6,    32] loss: 0.590
[7,    32] loss: 0.577
[8,    32] loss: 0.567
[9,    32] loss: 0.554
[10,    32] loss: 0.541
[11,    32] loss: 0.524
[12,    32] loss: 0.514
[13,    32] loss: 0.506
[14,    32] loss: 0.498
[15,    32] loss: 0.479
[16,    32] loss: 0.464
[17,    32] loss: 0.453
[18,    32] loss: 0.453
[19,    32] loss: 0.456
[20,    32] loss: 0.438
[21,    32] loss: 0.433
[22,    32] loss: 0.420
[23,    32] loss: 0.425
[24,    32] loss: 0.426
[25,    32] loss: 0.421
[26,    32] loss: 0.405
[27,    32] loss: 0.411
[28,    32] loss: 0.412
[29,    32] loss: 0.409
[30,    32] loss: 0.392
[31,    32] loss: 0.396
[32,    32] loss: 0.385
[33,    32] loss: 0.387
[34,    32] loss: 0.390
[35,    32] loss: 0.390
[36,    32] loss: 0.388
[37,    32] loss: 0.403
[38,    32] loss: 0.389
[39,    32] loss: 0.387
Early stopping applied (best metric=0.4772554636001587)
Finished Training
Total time taken: 92.9971022605896
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.668
[3,    32] loss: 0.628
[4,    32] loss: 0.611
[5,    32] loss: 0.606
[6,    32] loss: 0.587
[7,    32] loss: 0.574
[8,    32] loss: 0.564
[9,    32] loss: 0.553
[10,    32] loss: 0.538
[11,    32] loss: 0.527
[12,    32] loss: 0.505
[13,    32] loss: 0.494
[14,    32] loss: 0.482
[15,    32] loss: 0.472
[16,    32] loss: 0.472
[17,    32] loss: 0.473
[18,    32] loss: 0.447
[19,    32] loss: 0.443
[20,    32] loss: 0.436
[21,    32] loss: 0.439
[22,    32] loss: 0.430
[23,    32] loss: 0.423
[24,    32] loss: 0.419
[25,    32] loss: 0.411
[26,    32] loss: 0.407
[27,    32] loss: 0.408
[28,    32] loss: 0.408
[29,    32] loss: 0.405
[30,    32] loss: 0.404
[31,    32] loss: 0.396
[32,    32] loss: 0.400
[33,    32] loss: 0.385
[34,    32] loss: 0.396
[35,    32] loss: 0.400
Early stopping applied (best metric=0.48324671387672424)
Finished Training
Total time taken: 83.58009171485901
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.659
[3,    32] loss: 0.625
[4,    32] loss: 0.614
[5,    32] loss: 0.599
[6,    32] loss: 0.593
[7,    32] loss: 0.576
[8,    32] loss: 0.567
[9,    32] loss: 0.557
[10,    32] loss: 0.544
[11,    32] loss: 0.533
[12,    32] loss: 0.519
[13,    32] loss: 0.501
[14,    32] loss: 0.495
[15,    32] loss: 0.485
[16,    32] loss: 0.474
[17,    32] loss: 0.461
[18,    32] loss: 0.451
[19,    32] loss: 0.452
[20,    32] loss: 0.443
[21,    32] loss: 0.438
[22,    32] loss: 0.435
[23,    32] loss: 0.427
[24,    32] loss: 0.420
[25,    32] loss: 0.417
[26,    32] loss: 0.410
[27,    32] loss: 0.412
[28,    32] loss: 0.409
[29,    32] loss: 0.405
[30,    32] loss: 0.399
[31,    32] loss: 0.395
[32,    32] loss: 0.394
[33,    32] loss: 0.400
[34,    32] loss: 0.389
[35,    32] loss: 0.394
[36,    32] loss: 0.408
[37,    32] loss: 0.381
[38,    32] loss: 0.399
[39,    32] loss: 0.383
[40,    32] loss: 0.384
[41,    32] loss: 0.388
[42,    32] loss: 0.386
[43,    32] loss: 0.386
Early stopping applied (best metric=0.46293240785598755)
Finished Training
Total time taken: 102.51211190223694
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.656
[3,    32] loss: 0.623
[4,    32] loss: 0.613
[5,    32] loss: 0.601
[6,    32] loss: 0.588
[7,    32] loss: 0.584
[8,    32] loss: 0.571
[9,    32] loss: 0.558
[10,    32] loss: 0.547
[11,    32] loss: 0.538
[12,    32] loss: 0.511
[13,    32] loss: 0.505
[14,    32] loss: 0.496
[15,    32] loss: 0.483
[16,    32] loss: 0.470
[17,    32] loss: 0.467
[18,    32] loss: 0.460
[19,    32] loss: 0.453
[20,    32] loss: 0.435
[21,    32] loss: 0.440
[22,    32] loss: 0.430
[23,    32] loss: 0.434
[24,    32] loss: 0.419
[25,    32] loss: 0.418
[26,    32] loss: 0.413
[27,    32] loss: 0.403
[28,    32] loss: 0.400
[29,    32] loss: 0.404
[30,    32] loss: 0.402
[31,    32] loss: 0.401
[32,    32] loss: 0.397
[33,    32] loss: 0.398
[34,    32] loss: 0.382
[35,    32] loss: 0.389
[36,    32] loss: 0.387
[37,    32] loss: 0.389
[38,    32] loss: 0.394
[39,    32] loss: 0.373
[40,    32] loss: 0.379
Early stopping applied (best metric=0.47203192114830017)
Finished Training
Total time taken: 95.5018413066864
{'S-palmitoylation-C Validation Accuracy': 0.7082874282437601, 'S-palmitoylation-C Validation Sensitivity': 0.6502970297029703, 'S-palmitoylation-C Validation Specificity': 0.7228239544035424, 'S-palmitoylation-C Validation Precision': 0.3726705669705764, 'S-palmitoylation-C AUC ROC': 0.7545981228957672, 'S-palmitoylation-C AUC PR': 0.4506593085284851, 'S-palmitoylation-C MCC': 0.3140673626165481, 'S-palmitoylation-C F1': 0.4728104155621631, 'Validation Loss (S-palmitoylation-C)': 0.4728631854057312, 'Validation Loss (total)': 0.4728631854057312, 'TimeToTrain': 94.00745062828064}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015940598935620513,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 553820986,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.918328195187208}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.619
[3,    32] loss: 0.605
[4,    32] loss: 0.589
[5,    32] loss: 0.575
[6,    32] loss: 0.560
[7,    32] loss: 0.561
[8,    32] loss: 0.558
[9,    32] loss: 0.552
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00036475266725046124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1987951071,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.29299464666847}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.705
[2,    32] loss: 0.658
[3,    32] loss: 0.622
[4,    32] loss: 0.605
[5,    32] loss: 0.601
[6,    32] loss: 0.586
[7,    32] loss: 0.581
[8,    32] loss: 0.573
[9,    32] loss: 0.564
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031381908557588864,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3173795239,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.044600331070821}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.622
[3,    32] loss: 0.611
[4,    32] loss: 0.594
[5,    32] loss: 0.593
[6,    32] loss: 0.594
[7,    32] loss: 0.585
[8,    32] loss: 0.588
[9,    32] loss: 0.579
[10,    32] loss: 0.600
[11,    32] loss: 0.591
[12,    32] loss: 0.590
[13,    32] loss: 0.582
[14,    32] loss: 0.584
[15,    32] loss: 0.580
[16,    32] loss: 0.589
[17,    32] loss: 0.589
[18,    32] loss: 0.585
[19,    32] loss: 0.583
[20,    32] loss: 0.597
[21,    32] loss: 0.596
[22,    32] loss: 0.585
[23,    32] loss: 0.590
[24,    32] loss: 0.600
[25,    32] loss: 0.594
[26,    32] loss: 0.585
[27,    32] loss: 0.575
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003840346628689032,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2015899113,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.308872602233146}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.633
[3,    32] loss: 0.622
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003539014016501232,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3408541528,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8036325357816203}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.618
[3,    32] loss: 0.598
[4,    32] loss: 0.585
[5,    32] loss: 0.562
[6,    32] loss: 0.550
[7,    32] loss: 0.543
[8,    32] loss: 0.524
[9,    32] loss: 0.521
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00928122308269282,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1222606405,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.555441712851792}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.679
[3,    32] loss: 0.649
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00046526068099773244,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2464992816,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.088167435806803}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.636
[3,    32] loss: 0.609
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009174192328727342,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 216061132,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.218293272885894}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.670
[3,    32] loss: 0.650
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008448144014562037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 47875091,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5969864046814859}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.632
[3,    32] loss: 0.619
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009374939188861044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1695820025,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.24815857693018}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.669
[3,    32] loss: 0.656
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006102324578198821,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2051491535,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.699362588137454}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.635
[3,    32] loss: 0.615
[4,    32] loss: 0.602
[5,    32] loss: 0.584
[6,    32] loss: 0.573
[7,    32] loss: 0.563
[8,    32] loss: 0.552
[9,    32] loss: 0.545
[10,    32] loss: 0.537
[11,    32] loss: 0.528
[12,    32] loss: 0.530
[13,    32] loss: 0.519
[14,    32] loss: 0.521
[15,    32] loss: 0.517
[16,    32] loss: 0.514
[17,    32] loss: 0.520
[18,    32] loss: 0.513
[19,    32] loss: 0.513
[20,    32] loss: 0.506
[21,    32] loss: 0.504
[22,    32] loss: 0.504
[23,    32] loss: 0.504
[24,    32] loss: 0.517
[25,    32] loss: 0.511
[26,    32] loss: 0.505
[27,    32] loss: 0.502
[28,    32] loss: 0.502
[29,    32] loss: 0.507
[30,    32] loss: 0.505
[31,    32] loss: 0.513
[32,    32] loss: 0.501
[33,    32] loss: 0.506
[34,    32] loss: 0.507
[35,    32] loss: 0.505
[36,    32] loss: 0.514
[37,    32] loss: 0.505
[38,    32] loss: 0.522
[39,    32] loss: 0.510
[40,    32] loss: 0.513
[41,    32] loss: 0.518
[42,    32] loss: 0.504
[43,    32] loss: 0.510
[44,    32] loss: 0.514
[45,    32] loss: 0.514
[46,    32] loss: 0.510
[47,    32] loss: 0.512
[48,    32] loss: 0.521
[49,    32] loss: 0.505
[50,    32] loss: 0.509
[51,    32] loss: 0.519
[52,    32] loss: 0.510
[53,    32] loss: 0.512
[54,    32] loss: 0.522
[55,    32] loss: 0.511
[56,    32] loss: 0.511
Early stopping applied (best metric=0.46831074357032776)
Finished Training
Total time taken: 133.139146566391
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.627
[3,    32] loss: 0.608
[4,    32] loss: 0.595
[5,    32] loss: 0.575
[6,    32] loss: 0.568
[7,    32] loss: 0.566
[8,    32] loss: 0.551
[9,    32] loss: 0.543
[10,    32] loss: 0.531
[11,    32] loss: 0.529
[12,    32] loss: 0.520
[13,    32] loss: 0.523
[14,    32] loss: 0.513
[15,    32] loss: 0.514
[16,    32] loss: 0.506
[17,    32] loss: 0.513
[18,    32] loss: 0.505
[19,    32] loss: 0.511
[20,    32] loss: 0.510
[21,    32] loss: 0.510
[22,    32] loss: 0.503
[23,    32] loss: 0.499
[24,    32] loss: 0.501
[25,    32] loss: 0.507
[26,    32] loss: 0.505
[27,    32] loss: 0.501
[28,    32] loss: 0.520
[29,    32] loss: 0.498
[30,    32] loss: 0.502
[31,    32] loss: 0.500
[32,    32] loss: 0.500
[33,    32] loss: 0.501
[34,    32] loss: 0.498
[35,    32] loss: 0.501
[36,    32] loss: 0.495
[37,    32] loss: 0.519
[38,    32] loss: 0.498
[39,    32] loss: 0.517
[40,    32] loss: 0.499
[41,    32] loss: 0.496
[42,    32] loss: 0.498
[43,    32] loss: 0.503
[44,    32] loss: 0.504
[45,    32] loss: 0.504
[46,    32] loss: 0.498
[47,    32] loss: 0.500
[48,    32] loss: 0.498
[49,    32] loss: 0.514
[50,    32] loss: 0.494
[51,    32] loss: 0.492
Early stopping applied (best metric=0.4738682508468628)
Finished Training
Total time taken: 121.9201340675354
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.639
[3,    32] loss: 0.619
[4,    32] loss: 0.599
[5,    32] loss: 0.589
[6,    32] loss: 0.575
[7,    32] loss: 0.562
[8,    32] loss: 0.548
[9,    32] loss: 0.543
[10,    32] loss: 0.539
[11,    32] loss: 0.530
[12,    32] loss: 0.520
[13,    32] loss: 0.513
[14,    32] loss: 0.527
[15,    32] loss: 0.513
[16,    32] loss: 0.505
[17,    32] loss: 0.510
[18,    32] loss: 0.499
[19,    32] loss: 0.503
[20,    32] loss: 0.510
[21,    32] loss: 0.499
[22,    32] loss: 0.505
[23,    32] loss: 0.503
[24,    32] loss: 0.497
[25,    32] loss: 0.496
[26,    32] loss: 0.500
[27,    32] loss: 0.496
[28,    32] loss: 0.507
[29,    32] loss: 0.494
[30,    32] loss: 0.502
[31,    32] loss: 0.505
[32,    32] loss: 0.503
[33,    32] loss: 0.503
[34,    32] loss: 0.510
[35,    32] loss: 0.490
[36,    32] loss: 0.499
[37,    32] loss: 0.509
[38,    32] loss: 0.503
Early stopping applied (best metric=0.46803954243659973)
Finished Training
Total time taken: 90.90409994125366
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.635
[3,    32] loss: 0.619
[4,    32] loss: 0.603
[5,    32] loss: 0.593
[6,    32] loss: 0.569
[7,    32] loss: 0.559
[8,    32] loss: 0.550
[9,    32] loss: 0.550
[10,    32] loss: 0.533
[11,    32] loss: 0.531
[12,    32] loss: 0.536
[13,    32] loss: 0.521
[14,    32] loss: 0.520
[15,    32] loss: 0.515
[16,    32] loss: 0.509
[17,    32] loss: 0.508
[18,    32] loss: 0.511
[19,    32] loss: 0.510
[20,    32] loss: 0.507
[21,    32] loss: 0.499
[22,    32] loss: 0.509
[23,    32] loss: 0.516
[24,    32] loss: 0.502
[25,    32] loss: 0.511
[26,    32] loss: 0.499
[27,    32] loss: 0.505
[28,    32] loss: 0.504
[29,    32] loss: 0.509
[30,    32] loss: 0.510
[31,    32] loss: 0.499
[32,    32] loss: 0.506
[33,    32] loss: 0.506
[34,    32] loss: 0.501
[35,    32] loss: 0.507
[36,    32] loss: 0.514
[37,    32] loss: 0.499
[38,    32] loss: 0.507
[39,    32] loss: 0.508
[40,    32] loss: 0.509
[41,    32] loss: 0.522
[42,    32] loss: 0.505
[43,    32] loss: 0.511
[44,    32] loss: 0.505
[45,    32] loss: 0.514
[46,    32] loss: 0.510
[47,    32] loss: 0.502
Early stopping applied (best metric=0.4707016944885254)
Finished Training
Total time taken: 112.68812322616577
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.630
[3,    32] loss: 0.611
[4,    32] loss: 0.588
[5,    32] loss: 0.580
[6,    32] loss: 0.560
[7,    32] loss: 0.557
[8,    32] loss: 0.538
[9,    32] loss: 0.528
[10,    32] loss: 0.538
[11,    32] loss: 0.526
[12,    32] loss: 0.512
[13,    32] loss: 0.515
[14,    32] loss: 0.523
[15,    32] loss: 0.508
[16,    32] loss: 0.501
[17,    32] loss: 0.506
[18,    32] loss: 0.495
[19,    32] loss: 0.504
[20,    32] loss: 0.503
[21,    32] loss: 0.495
[22,    32] loss: 0.495
[23,    32] loss: 0.490
[24,    32] loss: 0.501
[25,    32] loss: 0.495
[26,    32] loss: 0.497
[27,    32] loss: 0.501
[28,    32] loss: 0.498
[29,    32] loss: 0.493
[30,    32] loss: 0.511
[31,    32] loss: 0.497
[32,    32] loss: 0.498
[33,    32] loss: 0.507
Early stopping applied (best metric=0.48732680082321167)
Finished Training
Total time taken: 79.11008644104004
{'S-palmitoylation-C Validation Accuracy': 0.6866160670964164, 'S-palmitoylation-C Validation Sensitivity': 0.6788118811881189, 'S-palmitoylation-C Validation Specificity': 0.6885731886718529, 'S-palmitoylation-C Validation Precision': 0.35409751149265506, 'S-palmitoylation-C AUC ROC': 0.7518433504836706, 'S-palmitoylation-C AUC PR': 0.4515272174829272, 'S-palmitoylation-C MCC': 0.3027053208558388, 'S-palmitoylation-C F1': 0.4650636831111517, 'Validation Loss (S-palmitoylation-C)': 0.4736494064331055, 'Validation Loss (total)': 0.4736494064331055, 'TimeToTrain': 107.55231804847718}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00803539667739777,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1126901667,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.476218901236851}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.621
[3,    32] loss: 0.615
[4,    32] loss: 0.591
[5,    32] loss: 0.593
[6,    32] loss: 0.584
[7,    32] loss: 0.580
[8,    32] loss: 0.582
[9,    32] loss: 0.588
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004136993991846942,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 309456752,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.353219471360001}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.651
[3,    32] loss: 0.622
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003967419163663843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 416691408,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.575568345471538}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.621
[3,    32] loss: 0.618
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00179690156448961,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2455435871,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.444502291988153}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.615
[3,    32] loss: 0.603
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007141599515265389,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2672194745,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.447222057639932}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.626
[3,    32] loss: 0.608
[4,    32] loss: 0.601
[5,    32] loss: 0.577
[6,    32] loss: 0.571
[7,    32] loss: 0.553
[8,    32] loss: 0.544
[9,    32] loss: 0.540
[10,    32] loss: 0.530
[11,    32] loss: 0.527
[12,    32] loss: 0.525
[13,    32] loss: 0.523
[14,    32] loss: 0.526
[15,    32] loss: 0.506
[16,    32] loss: 0.515
[17,    32] loss: 0.526
[18,    32] loss: 0.511
[19,    32] loss: 0.514
[20,    32] loss: 0.513
[21,    32] loss: 0.512
[22,    32] loss: 0.507
[23,    32] loss: 0.510
[24,    32] loss: 0.509
[25,    32] loss: 0.509
[26,    32] loss: 0.504
[27,    32] loss: 0.519
[28,    32] loss: 0.508
[29,    32] loss: 0.506
[30,    32] loss: 0.507
[31,    32] loss: 0.489
[32,    32] loss: 0.508
[33,    32] loss: 0.504
[34,    32] loss: 0.516
[35,    32] loss: 0.505
Early stopping applied (best metric=0.4684191942214966)
Finished Training
Total time taken: 83.69509196281433
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.628
[3,    32] loss: 0.613
[4,    32] loss: 0.591
[5,    32] loss: 0.577
[6,    32] loss: 0.568
[7,    32] loss: 0.554
[8,    32] loss: 0.541
[9,    32] loss: 0.538
[10,    32] loss: 0.533
[11,    32] loss: 0.518
[12,    32] loss: 0.526
[13,    32] loss: 0.520
[14,    32] loss: 0.509
[15,    32] loss: 0.519
[16,    32] loss: 0.512
[17,    32] loss: 0.508
[18,    32] loss: 0.513
[19,    32] loss: 0.513
[20,    32] loss: 0.516
[21,    32] loss: 0.506
[22,    32] loss: 0.508
[23,    32] loss: 0.506
[24,    32] loss: 0.506
[25,    32] loss: 0.511
[26,    32] loss: 0.517
[27,    32] loss: 0.507
[28,    32] loss: 0.501
[29,    32] loss: 0.504
[30,    32] loss: 0.511
[31,    32] loss: 0.505
[32,    32] loss: 0.519
Early stopping applied (best metric=0.47291356325149536)
Finished Training
Total time taken: 77.0560851097107
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.638
[3,    32] loss: 0.615
[4,    32] loss: 0.597
[5,    32] loss: 0.578
[6,    32] loss: 0.566
[7,    32] loss: 0.556
[8,    32] loss: 0.542
[9,    32] loss: 0.533
[10,    32] loss: 0.530
[11,    32] loss: 0.526
[12,    32] loss: 0.518
[13,    32] loss: 0.515
[14,    32] loss: 0.514
[15,    32] loss: 0.512
[16,    32] loss: 0.510
[17,    32] loss: 0.502
[18,    32] loss: 0.509
[19,    32] loss: 0.518
[20,    32] loss: 0.508
[21,    32] loss: 0.512
[22,    32] loss: 0.502
[23,    32] loss: 0.510
[24,    32] loss: 0.506
[25,    32] loss: 0.501
[26,    32] loss: 0.504
[27,    32] loss: 0.501
[28,    32] loss: 0.510
[29,    32] loss: 0.500
[30,    32] loss: 0.498
[31,    32] loss: 0.500
[32,    32] loss: 0.504
[33,    32] loss: 0.513
[34,    32] loss: 0.501
[35,    32] loss: 0.516
[36,    32] loss: 0.520
[37,    32] loss: 0.506
[38,    32] loss: 0.507
[39,    32] loss: 0.507
[40,    32] loss: 0.506
[41,    32] loss: 0.526
[42,    32] loss: 0.516
[43,    32] loss: 0.506
[44,    32] loss: 0.510
[45,    32] loss: 0.511
[46,    32] loss: 0.525
Early stopping applied (best metric=0.4545646607875824)
Finished Training
Total time taken: 109.82123947143555
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.645
[3,    32] loss: 0.611
[4,    32] loss: 0.595
[5,    32] loss: 0.585
[6,    32] loss: 0.563
[7,    32] loss: 0.562
[8,    32] loss: 0.542
[9,    32] loss: 0.545
[10,    32] loss: 0.537
[11,    32] loss: 0.519
[12,    32] loss: 0.518
[13,    32] loss: 0.510
[14,    32] loss: 0.517
[15,    32] loss: 0.506
[16,    32] loss: 0.511
[17,    32] loss: 0.519
[18,    32] loss: 0.516
[19,    32] loss: 0.517
[20,    32] loss: 0.509
[21,    32] loss: 0.519
[22,    32] loss: 0.508
[23,    32] loss: 0.494
[24,    32] loss: 0.501
[25,    32] loss: 0.514
[26,    32] loss: 0.507
[27,    32] loss: 0.510
[28,    32] loss: 0.510
[29,    32] loss: 0.515
[30,    32] loss: 0.509
[31,    32] loss: 0.514
[32,    32] loss: 0.511
[33,    32] loss: 0.510
[34,    32] loss: 0.508
[35,    32] loss: 0.504
[36,    32] loss: 0.509
[37,    32] loss: 0.505
[38,    32] loss: 0.506
[39,    32] loss: 0.507
[40,    32] loss: 0.513
[41,    32] loss: 0.506
[42,    32] loss: 0.503
[43,    32] loss: 0.513
[44,    32] loss: 0.510
[45,    32] loss: 0.511
[46,    32] loss: 0.503
[47,    32] loss: 0.518
[48,    32] loss: 0.507
[49,    32] loss: 0.517
[50,    32] loss: 0.511
[51,    32] loss: 0.499
[52,    32] loss: 0.512
[53,    32] loss: 0.517
[54,    32] loss: 0.505
Early stopping applied (best metric=0.4667724072933197)
Finished Training
Total time taken: 129.25469732284546
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.639
[3,    32] loss: 0.612
[4,    32] loss: 0.598
[5,    32] loss: 0.585
[6,    32] loss: 0.566
[7,    32] loss: 0.556
[8,    32] loss: 0.552
[9,    32] loss: 0.535
[10,    32] loss: 0.549
[11,    32] loss: 0.525
[12,    32] loss: 0.524
[13,    32] loss: 0.524
[14,    32] loss: 0.516
[15,    32] loss: 0.519
[16,    32] loss: 0.517
[17,    32] loss: 0.518
[18,    32] loss: 0.508
[19,    32] loss: 0.503
[20,    32] loss: 0.512
[21,    32] loss: 0.512
[22,    32] loss: 0.512
[23,    32] loss: 0.519
[24,    32] loss: 0.508
[25,    32] loss: 0.511
[26,    32] loss: 0.517
[27,    32] loss: 0.509
[28,    32] loss: 0.505
[29,    32] loss: 0.515
[30,    32] loss: 0.508
[31,    32] loss: 0.504
[32,    32] loss: 0.513
[33,    32] loss: 0.505
Early stopping applied (best metric=0.47144564986228943)
Finished Training
Total time taken: 78.81413269042969
{'S-palmitoylation-C Validation Accuracy': 0.7321786801262784, 'S-palmitoylation-C Validation Sensitivity': 0.622970297029703, 'S-palmitoylation-C Validation Specificity': 0.7595538919868612, 'S-palmitoylation-C Validation Precision': 0.3958660811153439, 'S-palmitoylation-C AUC ROC': 0.7656706111051594, 'S-palmitoylation-C AUC PR': 0.46814997254666374, 'S-palmitoylation-C MCC': 0.3302474246822302, 'S-palmitoylation-C F1': 0.48309926381513485, 'Validation Loss (S-palmitoylation-C)': 0.4668230950832367, 'Validation Loss (total)': 0.4668230950832367, 'TimeToTrain': 95.72824931144714}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002942013748437126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1802786356,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.926023429072346}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.612
[3,    32] loss: 0.593
[4,    32] loss: 0.588
[5,    32] loss: 0.575
[6,    32] loss: 0.557
[7,    32] loss: 0.560
[8,    32] loss: 0.558
[9,    32] loss: 0.552
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0057174897068803295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 649467626,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.250535947535658}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.615
[3,    32] loss: 0.599
[4,    32] loss: 0.583
[5,    32] loss: 0.587
[6,    32] loss: 0.578
[7,    32] loss: 0.569
[8,    32] loss: 0.579
[9,    32] loss: 0.569
[10,    32] loss: 0.569
[11,    32] loss: 0.575
[12,    32] loss: 0.566
[13,    32] loss: 0.575
[14,    32] loss: 0.570
[15,    32] loss: 0.575
[16,    32] loss: 0.565
[17,    32] loss: 0.577
[18,    32] loss: 0.573
[19,    32] loss: 0.567
[20,    32] loss: 0.574
[21,    32] loss: 0.574
[22,    32] loss: 0.572
[23,    32] loss: 0.564
[24,    32] loss: 0.575
[25,    32] loss: 0.568
[26,    32] loss: 0.580
[27,    32] loss: 0.570
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 7.695606541410942e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3045941896,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.409457206044184}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.691
[3,    32] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026872136329118795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2206443563,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.969982310665731}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.618
[3,    32] loss: 0.610
[4,    32] loss: 0.595
[5,    32] loss: 0.584
[6,    32] loss: 0.594
[7,    32] loss: 0.588
[8,    32] loss: 0.589
[9,    32] loss: 0.587
[10,    32] loss: 0.579
[11,    32] loss: 0.587
[12,    32] loss: 0.584
[13,    32] loss: 0.591
[14,    32] loss: 0.589
[15,    32] loss: 0.585
[16,    32] loss: 0.591
[17,    32] loss: 0.590
[18,    32] loss: 0.586
[19,    32] loss: 0.595
[20,    32] loss: 0.583
[21,    32] loss: 0.584
[22,    32] loss: 0.582
[23,    32] loss: 0.591
[24,    32] loss: 0.583
[25,    32] loss: 0.596
[26,    32] loss: 0.601
[27,    32] loss: 0.587
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.757243893963023e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3230885080,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.504611704336668}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.691
[3,    32] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00018101836971862347,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 798378780,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.915453280729448}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.684
[3,    32] loss: 0.657
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022580850373665404,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2674918226,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.77387226433818}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.621
[3,    32] loss: 0.594
[4,    32] loss: 0.574
[5,    32] loss: 0.561
[6,    32] loss: 0.549
[7,    32] loss: 0.550
[8,    32] loss: 0.533
[9,    32] loss: 0.535
[10,    32] loss: 0.525
[11,    32] loss: 0.523
[12,    32] loss: 0.524
[13,    32] loss: 0.528
[14,    32] loss: 0.520
[15,    32] loss: 0.515
[16,    32] loss: 0.517
[17,    32] loss: 0.517
[18,    32] loss: 0.508
[19,    32] loss: 0.522
[20,    32] loss: 0.517
[21,    32] loss: 0.520
[22,    32] loss: 0.512
[23,    32] loss: 0.518
[24,    32] loss: 0.533
[25,    32] loss: 0.510
[26,    32] loss: 0.515
[27,    32] loss: 0.527
[28,    32] loss: 0.515
[29,    32] loss: 0.516
[30,    32] loss: 0.520
[31,    32] loss: 0.515
[32,    32] loss: 0.515
[33,    32] loss: 0.512
[34,    32] loss: 0.522
[35,    32] loss: 0.521
[36,    32] loss: 0.527
[37,    32] loss: 0.520
[38,    32] loss: 0.512
[39,    32] loss: 0.510
[40,    32] loss: 0.517
[41,    32] loss: 0.503
[42,    32] loss: 0.520
[43,    32] loss: 0.522
[44,    32] loss: 0.505
[45,    32] loss: 0.507
[46,    32] loss: 0.511
[47,    32] loss: 0.517
[48,    32] loss: 0.502
[49,    32] loss: 0.510
[50,    32] loss: 0.529
[51,    32] loss: 0.517
[52,    32] loss: 0.505
[53,    32] loss: 0.517
[54,    32] loss: 0.512
[55,    32] loss: 0.523
[56,    32] loss: 0.514
[57,    32] loss: 0.519
Early stopping applied (best metric=0.46486493945121765)
Finished Training
Total time taken: 136.65436697006226
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.619
[3,    32] loss: 0.592
[4,    32] loss: 0.578
[5,    32] loss: 0.566
[6,    32] loss: 0.550
[7,    32] loss: 0.545
[8,    32] loss: 0.535
[9,    32] loss: 0.529
[10,    32] loss: 0.527
[11,    32] loss: 0.535
[12,    32] loss: 0.521
[13,    32] loss: 0.514
[14,    32] loss: 0.524
[15,    32] loss: 0.516
[16,    32] loss: 0.511
[17,    32] loss: 0.517
[18,    32] loss: 0.513
[19,    32] loss: 0.515
[20,    32] loss: 0.515
[21,    32] loss: 0.506
[22,    32] loss: 0.509
[23,    32] loss: 0.508
[24,    32] loss: 0.510
[25,    32] loss: 0.516
[26,    32] loss: 0.518
[27,    32] loss: 0.505
[28,    32] loss: 0.510
[29,    32] loss: 0.507
[30,    32] loss: 0.506
[31,    32] loss: 0.513
[32,    32] loss: 0.510
Early stopping applied (best metric=0.46479377150535583)
Finished Training
Total time taken: 76.59993147850037
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.619
[3,    32] loss: 0.591
[4,    32] loss: 0.581
[5,    32] loss: 0.559
[6,    32] loss: 0.556
[7,    32] loss: 0.544
[8,    32] loss: 0.532
[9,    32] loss: 0.531
[10,    32] loss: 0.526
[11,    32] loss: 0.515
[12,    32] loss: 0.528
[13,    32] loss: 0.519
[14,    32] loss: 0.523
[15,    32] loss: 0.529
[16,    32] loss: 0.513
[17,    32] loss: 0.509
[18,    32] loss: 0.508
[19,    32] loss: 0.503
[20,    32] loss: 0.518
[21,    32] loss: 0.518
[22,    32] loss: 0.511
[23,    32] loss: 0.510
[24,    32] loss: 0.516
[25,    32] loss: 0.519
[26,    32] loss: 0.513
[27,    32] loss: 0.512
[28,    32] loss: 0.515
[29,    32] loss: 0.513
[30,    32] loss: 0.518
[31,    32] loss: 0.511
[32,    32] loss: 0.529
[33,    32] loss: 0.502
[34,    32] loss: 0.521
[35,    32] loss: 0.513
[36,    32] loss: 0.523
[37,    32] loss: 0.507
[38,    32] loss: 0.511
[39,    32] loss: 0.513
[40,    32] loss: 0.521
[41,    32] loss: 0.510
[42,    32] loss: 0.514
[43,    32] loss: 0.522
[44,    32] loss: 0.515
[45,    32] loss: 0.515
[46,    32] loss: 0.508
[47,    32] loss: 0.512
[48,    32] loss: 0.514
[49,    32] loss: 0.526
[50,    32] loss: 0.506
[51,    32] loss: 0.508
Early stopping applied (best metric=0.4644050896167755)
Finished Training
Total time taken: 122.05604600906372
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.619
[3,    32] loss: 0.586
[4,    32] loss: 0.567
[5,    32] loss: 0.557
[6,    32] loss: 0.536
[7,    32] loss: 0.542
[8,    32] loss: 0.523
[9,    32] loss: 0.529
[10,    32] loss: 0.520
[11,    32] loss: 0.510
[12,    32] loss: 0.523
[13,    32] loss: 0.511
[14,    32] loss: 0.519
[15,    32] loss: 0.516
[16,    32] loss: 0.516
[17,    32] loss: 0.514
[18,    32] loss: 0.508
[19,    32] loss: 0.511
[20,    32] loss: 0.516
[21,    32] loss: 0.510
[22,    32] loss: 0.522
[23,    32] loss: 0.523
[24,    32] loss: 0.499
[25,    32] loss: 0.523
[26,    32] loss: 0.515
[27,    32] loss: 0.499
[28,    32] loss: 0.514
[29,    32] loss: 0.505
[30,    32] loss: 0.507
[31,    32] loss: 0.506
[32,    32] loss: 0.518
[33,    32] loss: 0.510
[34,    32] loss: 0.514
[35,    32] loss: 0.518
[36,    32] loss: 0.511
[37,    32] loss: 0.516
[38,    32] loss: 0.509
[39,    32] loss: 0.519
[40,    32] loss: 0.518
[41,    32] loss: 0.499
[42,    32] loss: 0.503
[43,    32] loss: 0.522
[44,    32] loss: 0.500
[45,    32] loss: 0.521
[46,    32] loss: 0.513
[47,    32] loss: 0.512
Early stopping applied (best metric=0.47804561257362366)
Finished Training
Total time taken: 112.45617723464966
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.619
[3,    32] loss: 0.591
[4,    32] loss: 0.568
[5,    32] loss: 0.556
[6,    32] loss: 0.539
[7,    32] loss: 0.540
[8,    32] loss: 0.529
[9,    32] loss: 0.533
[10,    32] loss: 0.517
[11,    32] loss: 0.518
[12,    32] loss: 0.511
[13,    32] loss: 0.521
[14,    32] loss: 0.526
[15,    32] loss: 0.512
[16,    32] loss: 0.520
[17,    32] loss: 0.508
[18,    32] loss: 0.515
[19,    32] loss: 0.519
[20,    32] loss: 0.518
[21,    32] loss: 0.512
[22,    32] loss: 0.510
[23,    32] loss: 0.506
[24,    32] loss: 0.512
[25,    32] loss: 0.514
[26,    32] loss: 0.524
[27,    32] loss: 0.504
[28,    32] loss: 0.513
Early stopping applied (best metric=0.46602752804756165)
Finished Training
Total time taken: 66.98806095123291
{'S-palmitoylation-C Validation Accuracy': 0.7059817450865485, 'S-palmitoylation-C Validation Sensitivity': 0.6542574257425743, 'S-palmitoylation-C Validation Specificity': 0.7189454463913892, 'S-palmitoylation-C Validation Precision': 0.37049588404235645, 'S-palmitoylation-C AUC ROC': 0.7589191647949161, 'S-palmitoylation-C AUC PR': 0.46270918336659456, 'S-palmitoylation-C MCC': 0.3133342105288389, 'S-palmitoylation-C F1': 0.4719426504746809, 'Validation Loss (S-palmitoylation-C)': 0.46762738823890687, 'Validation Loss (total)': 0.46762738823890687, 'TimeToTrain': 102.95091652870178}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001499313840511922,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1468076048,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.241320581162734}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.618
[3,    32] loss: 0.603
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00266616068008559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3122646233,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.6710820247759894}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.610
[3,    32] loss: 0.583
[4,    32] loss: 0.560
[5,    32] loss: 0.556
[6,    32] loss: 0.541
[7,    32] loss: 0.526
[8,    32] loss: 0.529
[9,    32] loss: 0.516
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013201353771420702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 7184684,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.1953868167948345}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.613
[3,    32] loss: 0.597
[4,    32] loss: 0.577
[5,    32] loss: 0.567
[6,    32] loss: 0.541
[7,    32] loss: 0.542
[8,    32] loss: 0.517
[9,    32] loss: 0.529
[10,    32] loss: 0.510
[11,    32] loss: 0.504
[12,    32] loss: 0.497
[13,    32] loss: 0.493
[14,    32] loss: 0.492
[15,    32] loss: 0.490
[16,    32] loss: 0.483
[17,    32] loss: 0.491
[18,    32] loss: 0.475
[19,    32] loss: 0.473
[20,    32] loss: 0.476
[21,    32] loss: 0.477
[22,    32] loss: 0.480
[23,    32] loss: 0.475
[24,    32] loss: 0.485
[25,    32] loss: 0.470
[26,    32] loss: 0.473
[27,    32] loss: 0.473
[28,    32] loss: 0.466
[29,    32] loss: 0.474
[30,    32] loss: 0.459
[31,    32] loss: 0.471
[32,    32] loss: 0.474
[33,    32] loss: 0.478
[34,    32] loss: 0.472
Early stopping applied (best metric=0.4676748812198639)
Finished Training
Total time taken: 81.2933566570282
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.616
[3,    32] loss: 0.600
[4,    32] loss: 0.568
[5,    32] loss: 0.555
[6,    32] loss: 0.536
[7,    32] loss: 0.529
[8,    32] loss: 0.519
[9,    32] loss: 0.509
[10,    32] loss: 0.501
[11,    32] loss: 0.492
[12,    32] loss: 0.478
[13,    32] loss: 0.495
[14,    32] loss: 0.481
[15,    32] loss: 0.470
[16,    32] loss: 0.470
[17,    32] loss: 0.477
[18,    32] loss: 0.477
[19,    32] loss: 0.484
[20,    32] loss: 0.463
[21,    32] loss: 0.483
[22,    32] loss: 0.472
[23,    32] loss: 0.473
[24,    32] loss: 0.466
[25,    32] loss: 0.466
[26,    32] loss: 0.475
[27,    32] loss: 0.475
[28,    32] loss: 0.459
[29,    32] loss: 0.470
[30,    32] loss: 0.475
[31,    32] loss: 0.468
[32,    32] loss: 0.466
[33,    32] loss: 0.477
[34,    32] loss: 0.468
[35,    32] loss: 0.469
[36,    32] loss: 0.460
[37,    32] loss: 0.460
[38,    32] loss: 0.460
[39,    32] loss: 0.458
[40,    32] loss: 0.471
[41,    32] loss: 0.477
[42,    32] loss: 0.467
[43,    32] loss: 0.467
[44,    32] loss: 0.465
[45,    32] loss: 0.464
[46,    32] loss: 0.470
[47,    32] loss: 0.468
[48,    32] loss: 0.469
[49,    32] loss: 0.462
[50,    32] loss: 0.468
[51,    32] loss: 0.474
[52,    32] loss: 0.465
[53,    32] loss: 0.472
[54,    32] loss: 0.470
[55,    32] loss: 0.461
[56,    32] loss: 0.483
[57,    32] loss: 0.472
[58,    32] loss: 0.457
[59,    32] loss: 0.473
[60,    32] loss: 0.456
[61,    32] loss: 0.470
[62,    32] loss: 0.459
[63,    32] loss: 0.458
[64,    32] loss: 0.468
Early stopping applied (best metric=0.4693698585033417)
Finished Training
Total time taken: 152.89983654022217
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.617
[3,    32] loss: 0.591
[4,    32] loss: 0.563
[5,    32] loss: 0.560
[6,    32] loss: 0.535
[7,    32] loss: 0.520
[8,    32] loss: 0.515
[9,    32] loss: 0.500
[10,    32] loss: 0.508
[11,    32] loss: 0.483
[12,    32] loss: 0.488
[13,    32] loss: 0.475
[14,    32] loss: 0.475
[15,    32] loss: 0.472
[16,    32] loss: 0.476
[17,    32] loss: 0.466
[18,    32] loss: 0.468
[19,    32] loss: 0.462
[20,    32] loss: 0.464
[21,    32] loss: 0.465
[22,    32] loss: 0.462
[23,    32] loss: 0.461
[24,    32] loss: 0.463
[25,    32] loss: 0.467
[26,    32] loss: 0.453
[27,    32] loss: 0.460
[28,    32] loss: 0.458
[29,    32] loss: 0.461
[30,    32] loss: 0.450
[31,    32] loss: 0.462
[32,    32] loss: 0.465
[33,    32] loss: 0.465
[34,    32] loss: 0.450
[35,    32] loss: 0.459
[36,    32] loss: 0.462
[37,    32] loss: 0.470
[38,    32] loss: 0.454
Early stopping applied (best metric=0.4793919324874878)
Finished Training
Total time taken: 90.9282295703888
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.620
[3,    32] loss: 0.600
[4,    32] loss: 0.573
[5,    32] loss: 0.551
[6,    32] loss: 0.539
[7,    32] loss: 0.519
[8,    32] loss: 0.511
[9,    32] loss: 0.513
[10,    32] loss: 0.493
[11,    32] loss: 0.494
[12,    32] loss: 0.491
[13,    32] loss: 0.483
[14,    32] loss: 0.489
[15,    32] loss: 0.481
[16,    32] loss: 0.475
[17,    32] loss: 0.473
[18,    32] loss: 0.479
[19,    32] loss: 0.480
[20,    32] loss: 0.469
[21,    32] loss: 0.475
[22,    32] loss: 0.469
[23,    32] loss: 0.473
[24,    32] loss: 0.464
[25,    32] loss: 0.464
[26,    32] loss: 0.462
[27,    32] loss: 0.465
[28,    32] loss: 0.460
[29,    32] loss: 0.466
[30,    32] loss: 0.472
[31,    32] loss: 0.461
[32,    32] loss: 0.468
[33,    32] loss: 0.469
[34,    32] loss: 0.463
[35,    32] loss: 0.464
[36,    32] loss: 0.461
[37,    32] loss: 0.465
[38,    32] loss: 0.465
[39,    32] loss: 0.480
[40,    32] loss: 0.462
[41,    32] loss: 0.463
[42,    32] loss: 0.472
[43,    32] loss: 0.459
[44,    32] loss: 0.461
[45,    32] loss: 0.462
[46,    32] loss: 0.461
[47,    32] loss: 0.465
[48,    32] loss: 0.459
[49,    32] loss: 0.471
[50,    32] loss: 0.462
[51,    32] loss: 0.456
[52,    32] loss: 0.462
[53,    32] loss: 0.461
[54,    32] loss: 0.464
[55,    32] loss: 0.462
[56,    32] loss: 0.466
[57,    32] loss: 0.459
[58,    32] loss: 0.473
[59,    32] loss: 0.458
[60,    32] loss: 0.462
[61,    32] loss: 0.468
[62,    32] loss: 0.471
[63,    32] loss: 0.475
[64,    32] loss: 0.478
[65,    32] loss: 0.460
[66,    32] loss: 0.464
[67,    32] loss: 0.472
[68,    32] loss: 0.453
[69,    32] loss: 0.462
[70,    32] loss: 0.467
[71,    32] loss: 0.463
[72,    32] loss: 0.467
[73,    32] loss: 0.463
[74,    32] loss: 0.455
[75,    32] loss: 0.479
[76,    32] loss: 0.457
[77,    32] loss: 0.454
[78,    32] loss: 0.457
[79,    32] loss: 0.478
[80,    32] loss: 0.467
[81,    32] loss: 0.465
[82,    32] loss: 0.467
[83,    32] loss: 0.461
[84,    32] loss: 0.472
[85,    32] loss: 0.460
[86,    32] loss: 0.471
[87,    32] loss: 0.461
[88,    32] loss: 0.470
[89,    32] loss: 0.459
[90,    32] loss: 0.461
Early stopping applied (best metric=0.47618451714515686)
Finished Training
Total time taken: 215.1054663658142
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.620
[3,    32] loss: 0.596
[4,    32] loss: 0.576
[5,    32] loss: 0.573
[6,    32] loss: 0.540
[7,    32] loss: 0.529
[8,    32] loss: 0.528
[9,    32] loss: 0.508
[10,    32] loss: 0.513
[11,    32] loss: 0.514
[12,    32] loss: 0.493
[13,    32] loss: 0.490
[14,    32] loss: 0.488
[15,    32] loss: 0.477
[16,    32] loss: 0.483
[17,    32] loss: 0.476
[18,    32] loss: 0.471
[19,    32] loss: 0.477
[20,    32] loss: 0.479
[21,    32] loss: 0.473
[22,    32] loss: 0.474
[23,    32] loss: 0.470
[24,    32] loss: 0.471
[25,    32] loss: 0.466
[26,    32] loss: 0.478
[27,    32] loss: 0.470
[28,    32] loss: 0.475
[29,    32] loss: 0.469
[30,    32] loss: 0.469
[31,    32] loss: 0.476
[32,    32] loss: 0.474
[33,    32] loss: 0.474
[34,    32] loss: 0.483
[35,    32] loss: 0.468
[36,    32] loss: 0.463
Early stopping applied (best metric=0.4660966396331787)
Finished Training
Total time taken: 85.74376034736633
{'S-palmitoylation-C Validation Accuracy': 0.6812184225284662, 'S-palmitoylation-C Validation Sensitivity': 0.6883168316831684, 'S-palmitoylation-C Validation Specificity': 0.6794385209242498, 'S-palmitoylation-C Validation Precision': 0.35056379747717836, 'S-palmitoylation-C AUC ROC': 0.7536173999164291, 'S-palmitoylation-C AUC PR': 0.4547077127236889, 'S-palmitoylation-C MCC': 0.30171384173337135, 'S-palmitoylation-C F1': 0.4641376988580299, 'Validation Loss (S-palmitoylation-C)': 0.4717435657978058, 'Validation Loss (total)': 0.4717435657978058, 'TimeToTrain': 125.19412989616394}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017738366567673757,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1451873064,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.1528458352725037}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.611
[3,    32] loss: 0.587
[4,    32] loss: 0.566
[5,    32] loss: 0.542
[6,    32] loss: 0.536
[7,    32] loss: 0.524
[8,    32] loss: 0.501
[9,    32] loss: 0.490
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018308417906398949,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2356704080,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.916460452694829}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.609
[3,    32] loss: 0.592
[4,    32] loss: 0.571
[5,    32] loss: 0.562
[6,    32] loss: 0.547
[7,    32] loss: 0.544
[8,    32] loss: 0.535
[9,    32] loss: 0.528
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009854714832024953,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1680519834,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.343742657653071}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.626
[3,    32] loss: 0.615
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002073101312192774,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3659420653,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.998941202353147}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.620
[3,    32] loss: 0.601
[4,    32] loss: 0.580
[5,    32] loss: 0.565
[6,    32] loss: 0.554
[7,    32] loss: 0.547
[8,    32] loss: 0.549
[9,    32] loss: 0.552
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00041507003775115737,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1675321588,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.280734033351863}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.646
[3,    32] loss: 0.619
[4,    32] loss: 0.607
[5,    32] loss: 0.593
[6,    32] loss: 0.579
[7,    32] loss: 0.559
[8,    32] loss: 0.545
[9,    32] loss: 0.528
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009034899514006876,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4239632649,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.187789167819336}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.640
[3,    32] loss: 0.624
[4,    32] loss: 0.619
[5,    32] loss: 0.617
[6,    32] loss: 0.611
[7,    32] loss: 0.619
[8,    32] loss: 0.612
[9,    32] loss: 0.638
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020740201634535118,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 277578636,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.28880585889333}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.628
[3,    32] loss: 0.603
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009213798862865812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4130744595,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.01577246923591}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.629
[3,    32] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008039280878883122,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3332887279,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.4833076257488}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.688
[3,    32] loss: 0.679
[4,    32] loss: 0.687
[5,    32] loss: 0.657
[6,    32] loss: 0.655
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
[10,    32] loss: 0.693
[11,    32] loss: 0.693
[12,    32] loss: 0.693
[13,    32] loss: 0.693
[14,    32] loss: 0.693
[15,    32] loss: 0.693
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009934586770624006,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2736873824,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.085897578236693}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003044755077974264,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2483234641,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.935359644714372}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.609
[3,    32] loss: 0.597
[4,    32] loss: 0.575
[5,    32] loss: 0.574
[6,    32] loss: 0.563
[7,    32] loss: 0.556
[8,    32] loss: 0.549
[9,    32] loss: 0.554
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006047647126438493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 33407211,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.060976899588635}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.632
[3,    32] loss: 0.600
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007694286004878654,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 594697045,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.1719514638707804}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.701
[2,    32] loss: 0.636
[3,    32] loss: 0.604
[4,    32] loss: 0.603
[5,    32] loss: 0.585
[6,    32] loss: 0.585
[7,    32] loss: 0.568
[8,    32] loss: 0.573
[9,    32] loss: 0.577
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007261446926848705,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1284860554,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9582519425108682}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.626
[3,    32] loss: 0.606
[4,    32] loss: 0.603
[5,    32] loss: 0.588
[6,    32] loss: 0.583
[7,    32] loss: 0.569
[8,    32] loss: 0.563
[9,    32] loss: 0.565
[10,    32] loss: 0.546
[11,    32] loss: 0.556
[12,    32] loss: 0.552
[13,    32] loss: 0.550
[14,    32] loss: 0.540
[15,    32] loss: 0.538
[16,    32] loss: 0.535
[17,    32] loss: 0.537
[18,    32] loss: 0.527
[19,    32] loss: 0.527
[20,    32] loss: 0.521
[21,    32] loss: 0.524
[22,    32] loss: 0.525
[23,    32] loss: 0.511
[24,    32] loss: 0.518
[25,    32] loss: 0.513
[26,    32] loss: 0.514
[27,    32] loss: 0.506
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00010318196492274467,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3409679982,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.033201782068591}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.701
[2,    32] loss: 0.692
[3,    32] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00020751049519930381,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1668722792,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.243102907385817}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.674
[3,    32] loss: 0.641
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 2.576916868347086e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2839953515,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.497000367703006}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.693
[5,    32] loss: 0.692
[6,    32] loss: 0.691
[7,    32] loss: 0.689
[8,    32] loss: 0.685
[9,    32] loss: 0.679
[10,    32] loss: 0.671
[11,    32] loss: 0.663
[12,    32] loss: 0.655
[13,    32] loss: 0.649
[14,    32] loss: 0.644
[15,    32] loss: 0.641
[16,    32] loss: 0.633
[17,    32] loss: 0.633
[18,    32] loss: 0.627
[19,    32] loss: 0.626
[20,    32] loss: 0.623
[21,    32] loss: 0.615
[22,    32] loss: 0.617
[23,    32] loss: 0.621
[24,    32] loss: 0.611
[25,    32] loss: 0.611
[26,    32] loss: 0.605
[27,    32] loss: 0.610
[28,    32] loss: 0.605
[29,    32] loss: 0.607
[30,    32] loss: 0.604
[31,    32] loss: 0.605
[32,    32] loss: 0.604
[33,    32] loss: 0.594
[34,    32] loss: 0.601
[35,    32] loss: 0.594
[36,    32] loss: 0.599
[37,    32] loss: 0.597
[38,    32] loss: 0.592
[39,    32] loss: 0.591
[40,    32] loss: 0.593
[41,    32] loss: 0.590
[42,    32] loss: 0.591
[43,    32] loss: 0.591
[44,    32] loss: 0.587
[45,    32] loss: 0.588
[46,    32] loss: 0.588
[47,    32] loss: 0.588
[48,    32] loss: 0.579
[49,    32] loss: 0.578
[50,    32] loss: 0.579
[51,    32] loss: 0.577
[52,    32] loss: 0.580
[53,    32] loss: 0.576
[54,    32] loss: 0.574
[55,    32] loss: 0.576
[56,    32] loss: 0.572
[57,    32] loss: 0.574
[58,    32] loss: 0.564
[59,    32] loss: 0.564
[60,    32] loss: 0.566
[61,    32] loss: 0.556
[62,    32] loss: 0.558
[63,    32] loss: 0.551
[64,    32] loss: 0.556
[65,    32] loss: 0.552
[66,    32] loss: 0.550
[67,    32] loss: 0.550
[68,    32] loss: 0.554
[69,    32] loss: 0.542
[70,    32] loss: 0.539
[71,    32] loss: 0.539
[72,    32] loss: 0.535
[73,    32] loss: 0.531
[74,    32] loss: 0.532
[75,    32] loss: 0.528
[76,    32] loss: 0.528
[77,    32] loss: 0.526
[78,    32] loss: 0.517
[79,    32] loss: 0.525
[80,    32] loss: 0.513
[81,    32] loss: 0.514
[82,    32] loss: 0.514
[83,    32] loss: 0.508
[84,    32] loss: 0.508
[85,    32] loss: 0.499
[86,    32] loss: 0.501
[87,    32] loss: 0.490
[88,    32] loss: 0.490
[89,    32] loss: 0.490
[90,    32] loss: 0.488
[91,    32] loss: 0.479
[92,    32] loss: 0.480
[93,    32] loss: 0.479
[94,    32] loss: 0.478
[95,    32] loss: 0.470
[96,    32] loss: 0.474
[97,    32] loss: 0.468
[98,    32] loss: 0.465
[99,    32] loss: 0.462
[100,    32] loss: 0.460
[101,    32] loss: 0.454
[102,    32] loss: 0.454
[103,    32] loss: 0.449
[104,    32] loss: 0.441
Early stopping applied (best metric=0.47120246291160583)
Finished Training
Total time taken: 248.77703046798706
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.692
[5,    32] loss: 0.691
[6,    32] loss: 0.689
[7,    32] loss: 0.686
[8,    32] loss: 0.681
[9,    32] loss: 0.673
[10,    32] loss: 0.665
[11,    32] loss: 0.655
[12,    32] loss: 0.648
[13,    32] loss: 0.643
[14,    32] loss: 0.636
[15,    32] loss: 0.628
[16,    32] loss: 0.631
[17,    32] loss: 0.624
[18,    32] loss: 0.621
[19,    32] loss: 0.617
[20,    32] loss: 0.618
[21,    32] loss: 0.620
[22,    32] loss: 0.612
[23,    32] loss: 0.610
[24,    32] loss: 0.613
[25,    32] loss: 0.607
[26,    32] loss: 0.608
[27,    32] loss: 0.607
[28,    32] loss: 0.605
[29,    32] loss: 0.604
[30,    32] loss: 0.601
[31,    32] loss: 0.603
[32,    32] loss: 0.603
[33,    32] loss: 0.597
[34,    32] loss: 0.598
[35,    32] loss: 0.595
[36,    32] loss: 0.597
[37,    32] loss: 0.593
[38,    32] loss: 0.593
[39,    32] loss: 0.591
[40,    32] loss: 0.588
[41,    32] loss: 0.588
[42,    32] loss: 0.591
[43,    32] loss: 0.588
[44,    32] loss: 0.590
[45,    32] loss: 0.585
[46,    32] loss: 0.583
[47,    32] loss: 0.588
[48,    32] loss: 0.585
[49,    32] loss: 0.585
[50,    32] loss: 0.583
[51,    32] loss: 0.581
[52,    32] loss: 0.576
[53,    32] loss: 0.576
[54,    32] loss: 0.575
[55,    32] loss: 0.568
[56,    32] loss: 0.568
[57,    32] loss: 0.568
[58,    32] loss: 0.565
[59,    32] loss: 0.561
[60,    32] loss: 0.563
[61,    32] loss: 0.560
[62,    32] loss: 0.558
[63,    32] loss: 0.558
[64,    32] loss: 0.554
[65,    32] loss: 0.554
[66,    32] loss: 0.551
[67,    32] loss: 0.551
[68,    32] loss: 0.541
[69,    32] loss: 0.545
[70,    32] loss: 0.544
[71,    32] loss: 0.546
[72,    32] loss: 0.537
[73,    32] loss: 0.536
[74,    32] loss: 0.532
[75,    32] loss: 0.530
[76,    32] loss: 0.532
[77,    32] loss: 0.522
[78,    32] loss: 0.525
[79,    32] loss: 0.525
[80,    32] loss: 0.512
[81,    32] loss: 0.516
[82,    32] loss: 0.511
[83,    32] loss: 0.511
[84,    32] loss: 0.512
[85,    32] loss: 0.509
[86,    32] loss: 0.499
[87,    32] loss: 0.501
[88,    32] loss: 0.503
[89,    32] loss: 0.492
[90,    32] loss: 0.498
[91,    32] loss: 0.485
[92,    32] loss: 0.488
[93,    32] loss: 0.484
[94,    32] loss: 0.482
[95,    32] loss: 0.482
[96,    32] loss: 0.478
[97,    32] loss: 0.472
[98,    32] loss: 0.468
[99,    32] loss: 0.464
[100,    32] loss: 0.461
[101,    32] loss: 0.466
[102,    32] loss: 0.456
[103,    32] loss: 0.453
[104,    32] loss: 0.450
[105,    32] loss: 0.445
[106,    32] loss: 0.442
[107,    32] loss: 0.444
[108,    32] loss: 0.439
[109,    32] loss: 0.439
[110,    32] loss: 0.435
[111,    32] loss: 0.435
[112,    32] loss: 0.430
[113,    32] loss: 0.429
[114,    32] loss: 0.421
Early stopping applied (best metric=0.4728758931159973)
Finished Training
Total time taken: 271.89260625839233
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.694
[3,    32] loss: 0.693
[4,    32] loss: 0.692
[5,    32] loss: 0.691
[6,    32] loss: 0.689
[7,    32] loss: 0.686
[8,    32] loss: 0.682
[9,    32] loss: 0.676
[10,    32] loss: 0.667
[11,    32] loss: 0.658
[12,    32] loss: 0.651
[13,    32] loss: 0.643
[14,    32] loss: 0.635
[15,    32] loss: 0.633
[16,    32] loss: 0.630
[17,    32] loss: 0.624
[18,    32] loss: 0.615
[19,    32] loss: 0.617
[20,    32] loss: 0.617
[21,    32] loss: 0.615
[22,    32] loss: 0.610
[23,    32] loss: 0.611
[24,    32] loss: 0.610
[25,    32] loss: 0.608
[26,    32] loss: 0.602
[27,    32] loss: 0.606
[28,    32] loss: 0.605
[29,    32] loss: 0.606
[30,    32] loss: 0.601
[31,    32] loss: 0.601
[32,    32] loss: 0.596
[33,    32] loss: 0.599
[34,    32] loss: 0.597
[35,    32] loss: 0.602
[36,    32] loss: 0.597
[37,    32] loss: 0.596
[38,    32] loss: 0.600
[39,    32] loss: 0.597
[40,    32] loss: 0.597
[41,    32] loss: 0.594
[42,    32] loss: 0.585
[43,    32] loss: 0.589
[44,    32] loss: 0.583
[45,    32] loss: 0.588
[46,    32] loss: 0.583
[47,    32] loss: 0.586
[48,    32] loss: 0.583
[49,    32] loss: 0.584
[50,    32] loss: 0.581
[51,    32] loss: 0.587
[52,    32] loss: 0.579
[53,    32] loss: 0.578
[54,    32] loss: 0.574
[55,    32] loss: 0.579
[56,    32] loss: 0.574
[57,    32] loss: 0.566
[58,    32] loss: 0.569
[59,    32] loss: 0.570
[60,    32] loss: 0.563
[61,    32] loss: 0.562
[62,    32] loss: 0.562
[63,    32] loss: 0.565
[64,    32] loss: 0.564
[65,    32] loss: 0.560
[66,    32] loss: 0.557
[67,    32] loss: 0.554
[68,    32] loss: 0.547
[69,    32] loss: 0.549
[70,    32] loss: 0.543
[71,    32] loss: 0.545
[72,    32] loss: 0.543
[73,    32] loss: 0.542
[74,    32] loss: 0.539
[75,    32] loss: 0.527
[76,    32] loss: 0.536
[77,    32] loss: 0.528
[78,    32] loss: 0.527
[79,    32] loss: 0.527
[80,    32] loss: 0.523
[81,    32] loss: 0.516
[82,    32] loss: 0.514
[83,    32] loss: 0.518
[84,    32] loss: 0.515
[85,    32] loss: 0.512
[86,    32] loss: 0.508
[87,    32] loss: 0.506
[88,    32] loss: 0.508
[89,    32] loss: 0.504
[90,    32] loss: 0.500
[91,    32] loss: 0.494
[92,    32] loss: 0.492
[93,    32] loss: 0.488
[94,    32] loss: 0.494
[95,    32] loss: 0.485
[96,    32] loss: 0.482
[97,    32] loss: 0.484
[98,    32] loss: 0.469
[99,    32] loss: 0.474
[100,    32] loss: 0.471
[101,    32] loss: 0.461
[102,    32] loss: 0.464
[103,    32] loss: 0.461
[104,    32] loss: 0.462
[105,    32] loss: 0.461
[106,    32] loss: 0.455
[107,    32] loss: 0.446
[108,    32] loss: 0.450
[109,    32] loss: 0.445
[110,    32] loss: 0.442
[111,    32] loss: 0.441
[112,    32] loss: 0.438
[113,    32] loss: 0.433
[114,    32] loss: 0.423
[115,    32] loss: 0.429
[116,    32] loss: 0.420
[117,    32] loss: 0.425
Early stopping applied (best metric=0.46210604906082153)
Finished Training
Total time taken: 279.4172873497009
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.694
[3,    32] loss: 0.694
[4,    32] loss: 0.693
[5,    32] loss: 0.692
[6,    32] loss: 0.692
[7,    32] loss: 0.689
[8,    32] loss: 0.686
[9,    32] loss: 0.681
[10,    32] loss: 0.673
[11,    32] loss: 0.664
[12,    32] loss: 0.656
[13,    32] loss: 0.647
[14,    32] loss: 0.638
[15,    32] loss: 0.634
[16,    32] loss: 0.633
[17,    32] loss: 0.629
[18,    32] loss: 0.623
[19,    32] loss: 0.619
[20,    32] loss: 0.619
[21,    32] loss: 0.616
[22,    32] loss: 0.615
[23,    32] loss: 0.615
[24,    32] loss: 0.610
[25,    32] loss: 0.609
[26,    32] loss: 0.604
[27,    32] loss: 0.602
[28,    32] loss: 0.606
[29,    32] loss: 0.601
[30,    32] loss: 0.609
[31,    32] loss: 0.599
[32,    32] loss: 0.599
[33,    32] loss: 0.597
[34,    32] loss: 0.596
[35,    32] loss: 0.597
[36,    32] loss: 0.598
[37,    32] loss: 0.594
[38,    32] loss: 0.591
[39,    32] loss: 0.593
[40,    32] loss: 0.591
[41,    32] loss: 0.592
[42,    32] loss: 0.587
[43,    32] loss: 0.586
[44,    32] loss: 0.591
[45,    32] loss: 0.586
[46,    32] loss: 0.587
[47,    32] loss: 0.580
[48,    32] loss: 0.580
[49,    32] loss: 0.581
[50,    32] loss: 0.576
[51,    32] loss: 0.582
[52,    32] loss: 0.578
[53,    32] loss: 0.568
[54,    32] loss: 0.576
[55,    32] loss: 0.573
[56,    32] loss: 0.574
[57,    32] loss: 0.565
[58,    32] loss: 0.568
[59,    32] loss: 0.561
[60,    32] loss: 0.563
[61,    32] loss: 0.560
[62,    32] loss: 0.560
[63,    32] loss: 0.557
[64,    32] loss: 0.561
[65,    32] loss: 0.563
[66,    32] loss: 0.553
[67,    32] loss: 0.555
[68,    32] loss: 0.547
[69,    32] loss: 0.552
[70,    32] loss: 0.539
[71,    32] loss: 0.540
[72,    32] loss: 0.534
[73,    32] loss: 0.536
[74,    32] loss: 0.535
[75,    32] loss: 0.533
[76,    32] loss: 0.530
[77,    32] loss: 0.525
[78,    32] loss: 0.529
[79,    32] loss: 0.527
[80,    32] loss: 0.522
[81,    32] loss: 0.521
[82,    32] loss: 0.516
[83,    32] loss: 0.513
[84,    32] loss: 0.514
[85,    32] loss: 0.513
[86,    32] loss: 0.500
[87,    32] loss: 0.505
[88,    32] loss: 0.501
[89,    32] loss: 0.497
[90,    32] loss: 0.503
[91,    32] loss: 0.490
[92,    32] loss: 0.491
[93,    32] loss: 0.488
[94,    32] loss: 0.481
[95,    32] loss: 0.480
[96,    32] loss: 0.478
[97,    32] loss: 0.475
[98,    32] loss: 0.474
[99,    32] loss: 0.468
[100,    32] loss: 0.471
[101,    32] loss: 0.472
[102,    32] loss: 0.463
Early stopping applied (best metric=0.4782291054725647)
Finished Training
Total time taken: 244.04565930366516
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.693
[3,    32] loss: 0.692
[4,    32] loss: 0.692
[5,    32] loss: 0.691
[6,    32] loss: 0.689
[7,    32] loss: 0.685
[8,    32] loss: 0.680
[9,    32] loss: 0.673
[10,    32] loss: 0.663
[11,    32] loss: 0.655
[12,    32] loss: 0.648
[13,    32] loss: 0.642
[14,    32] loss: 0.632
[15,    32] loss: 0.630
[16,    32] loss: 0.627
[17,    32] loss: 0.621
[18,    32] loss: 0.618
[19,    32] loss: 0.617
[20,    32] loss: 0.617
[21,    32] loss: 0.613
[22,    32] loss: 0.612
[23,    32] loss: 0.608
[24,    32] loss: 0.607
[25,    32] loss: 0.603
[26,    32] loss: 0.602
[27,    32] loss: 0.603
[28,    32] loss: 0.602
[29,    32] loss: 0.600
[30,    32] loss: 0.602
[31,    32] loss: 0.606
[32,    32] loss: 0.595
[33,    32] loss: 0.596
[34,    32] loss: 0.598
[35,    32] loss: 0.593
[36,    32] loss: 0.597
[37,    32] loss: 0.588
[38,    32] loss: 0.588
[39,    32] loss: 0.592
[40,    32] loss: 0.591
[41,    32] loss: 0.587
[42,    32] loss: 0.585
[43,    32] loss: 0.590
[44,    32] loss: 0.591
[45,    32] loss: 0.583
[46,    32] loss: 0.590
[47,    32] loss: 0.579
[48,    32] loss: 0.584
[49,    32] loss: 0.577
[50,    32] loss: 0.578
[51,    32] loss: 0.579
[52,    32] loss: 0.576
[53,    32] loss: 0.575
[54,    32] loss: 0.574
[55,    32] loss: 0.570
[56,    32] loss: 0.569
[57,    32] loss: 0.571
[58,    32] loss: 0.567
[59,    32] loss: 0.558
[60,    32] loss: 0.561
[61,    32] loss: 0.566
[62,    32] loss: 0.562
[63,    32] loss: 0.551
[64,    32] loss: 0.555
[65,    32] loss: 0.550
[66,    32] loss: 0.547
[67,    32] loss: 0.552
[68,    32] loss: 0.541
[69,    32] loss: 0.536
[70,    32] loss: 0.542
[71,    32] loss: 0.539
[72,    32] loss: 0.534
[73,    32] loss: 0.531
[74,    32] loss: 0.528
[75,    32] loss: 0.526
[76,    32] loss: 0.530
[77,    32] loss: 0.528
[78,    32] loss: 0.520
[79,    32] loss: 0.512
[80,    32] loss: 0.514
[81,    32] loss: 0.507
[82,    32] loss: 0.499
[83,    32] loss: 0.507
[84,    32] loss: 0.501
[85,    32] loss: 0.504
[86,    32] loss: 0.493
[87,    32] loss: 0.492
[88,    32] loss: 0.493
[89,    32] loss: 0.486
[90,    32] loss: 0.488
[91,    32] loss: 0.480
[92,    32] loss: 0.481
[93,    32] loss: 0.477
[94,    32] loss: 0.477
[95,    32] loss: 0.466
[96,    32] loss: 0.464
[97,    32] loss: 0.469
[98,    32] loss: 0.454
[99,    32] loss: 0.460
[100,    32] loss: 0.456
[101,    32] loss: 0.455
[102,    32] loss: 0.441
[103,    32] loss: 0.447
[104,    32] loss: 0.439
Early stopping applied (best metric=0.48466137051582336)
Finished Training
Total time taken: 248.85052108764648
{'S-palmitoylation-C Validation Accuracy': 0.6887606886078502, 'S-palmitoylation-C Validation Sensitivity': 0.6693069306930693, 'S-palmitoylation-C Validation Specificity': 0.6936381310972078, 'S-palmitoylation-C Validation Precision': 0.3547095402456997, 'S-palmitoylation-C AUC ROC': 0.7526027449891884, 'S-palmitoylation-C AUC PR': 0.4392210433559648, 'S-palmitoylation-C MCC': 0.3000654837554996, 'S-palmitoylation-C F1': 0.46313020540949446, 'Validation Loss (S-palmitoylation-C)': 0.47381497621536256, 'Validation Loss (total)': 0.47381497621536256, 'TimeToTrain': 258.5966208934784}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001112383092508407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1254476563,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.43810421895493}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.635
[3,    32] loss: 0.619
[4,    32] loss: 0.605
[5,    32] loss: 0.594
[6,    32] loss: 0.594
[7,    32] loss: 0.569
[8,    32] loss: 0.574
[9,    32] loss: 0.569
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005836699720190509,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2173709967,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.4330528435922594}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.633
[3,    32] loss: 0.611
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00966990408117752,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2587354554,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.3067968492154263}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.641
[3,    32] loss: 0.625
[4,    32] loss: 0.606
[5,    32] loss: 0.598
[6,    32] loss: 0.595
[7,    32] loss: 0.591
[8,    32] loss: 0.587
[9,    32] loss: 0.582
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007015349221113087,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1164648439,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.73554861000108}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.623
[3,    32] loss: 0.619
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016970362490062612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 472053147,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.556062349571082}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.624
[3,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007747370042900739,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 825202687,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.19269409862442}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.644
[3,    32] loss: 0.620
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001594315897309325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3120803166,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.724501320894337}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.615
[3,    32] loss: 0.597
[4,    32] loss: 0.575
[5,    32] loss: 0.547
[6,    32] loss: 0.535
[7,    32] loss: 0.504
[8,    32] loss: 0.495
[9,    32] loss: 0.484
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017771825794924423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2331978125,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.2123640441352468}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.620
[3,    32] loss: 0.600
[4,    32] loss: 0.569
[5,    32] loss: 0.555
[6,    32] loss: 0.539
[7,    32] loss: 0.507
[8,    32] loss: 0.501
[9,    32] loss: 0.488
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009986055987556886,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1428327276,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.53430406841729}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.648
[3,    32] loss: 0.632
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000673756994032257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1618092174,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.030655199496735}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.636
[3,    32] loss: 0.616
[4,    32] loss: 0.602
[5,    32] loss: 0.605
[6,    32] loss: 0.599
[7,    32] loss: 0.593
[8,    32] loss: 0.586
[9,    32] loss: 0.587
[10,    32] loss: 0.586
[11,    32] loss: 0.580
[12,    32] loss: 0.578
[13,    32] loss: 0.578
[14,    32] loss: 0.582
[15,    32] loss: 0.573
[16,    32] loss: 0.580
[17,    32] loss: 0.579
[18,    32] loss: 0.587
[19,    32] loss: 0.574
[20,    32] loss: 0.576
[21,    32] loss: 0.578
[22,    32] loss: 0.576
[23,    32] loss: 0.577
[24,    32] loss: 0.579
[25,    32] loss: 0.582
[26,    32] loss: 0.575
[27,    32] loss: 0.583
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005615446206728125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3342757573,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.736132510120306}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.651
[3,    32] loss: 0.638
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00028916510958090205,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2033969235,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.447987211634242}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.662
[3,    32] loss: 0.630
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005641588664843484,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 655002925,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.858910241314753}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.625
[3,    32] loss: 0.618
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014751681769554245,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1209363448,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.166496058556037}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.614
[3,    32] loss: 0.593
[4,    32] loss: 0.570
[5,    32] loss: 0.555
[6,    32] loss: 0.537
[7,    32] loss: 0.524
[8,    32] loss: 0.521
[9,    32] loss: 0.502
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 2.7015778600412304e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4123515278,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.00670962998261}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004945597783700066,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2414080259,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.6558549645631}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.648
[3,    32] loss: 0.649
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008998640393710792,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3915054634,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.769898510799006}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009993488450815574,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3985186992,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.950895029696383}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009874928171533962,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2222216897,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.128718192149726}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0069728921030445335,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3579925489,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9002857193513036}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.619
[3,    32] loss: 0.603
[4,    32] loss: 0.597
[5,    32] loss: 0.585
[6,    32] loss: 0.576
[7,    32] loss: 0.573
[8,    32] loss: 0.557
[9,    32] loss: 0.565
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004166995139440419,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 835911531,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.7598047993758175}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.613
[3,    32] loss: 0.594
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00017349709750578025,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2902871762,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.401988613900727}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.679
[3,    32] loss: 0.641
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002577081277571558,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3420286576,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.773794761026377}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.630
[3,    32] loss: 0.617
[4,    32] loss: 0.615
[5,    32] loss: 0.618
[6,    32] loss: 0.613
[7,    32] loss: 0.620
[8,    32] loss: 0.624
[9,    32] loss: 0.617
[10,    32] loss: 0.614
[11,    32] loss: 0.614
[12,    32] loss: 0.615
[13,    32] loss: 0.626
[14,    32] loss: 0.616
[15,    32] loss: 0.621
[16,    32] loss: 0.627
[17,    32] loss: 0.618
[18,    32] loss: 0.617
[19,    32] loss: 0.619
[20,    32] loss: 0.611
[21,    32] loss: 0.608
[22,    32] loss: 0.621
[23,    32] loss: 0.618
[24,    32] loss: 0.616
[25,    32] loss: 0.618
[26,    32] loss: 0.618
[27,    32] loss: 0.607
[28,    32] loss: 0.609
[29,    32] loss: 0.621
[30,    32] loss: 0.620
[31,    32] loss: 0.618
[32,    32] loss: 0.617
[33,    32] loss: 0.618
[34,    32] loss: 0.611
[35,    32] loss: 0.615
[36,    32] loss: 0.617
[37,    32] loss: 0.613
[38,    32] loss: 0.619
[39,    32] loss: 0.613
[40,    32] loss: 0.618
[41,    32] loss: 0.619
[42,    32] loss: 0.617
[43,    32] loss: 0.619
[44,    32] loss: 0.613
[45,    32] loss: 0.608
[46,    32] loss: 0.614
Early stopping applied (best metric=0.48441943526268005)
Finished Training
Total time taken: 109.79729747772217
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.635
[3,    32] loss: 0.619
[4,    32] loss: 0.609
[5,    32] loss: 0.618
[6,    32] loss: 0.615
[7,    32] loss: 0.625
[8,    32] loss: 0.613
[9,    32] loss: 0.615
[10,    32] loss: 0.619
[11,    32] loss: 0.619
[12,    32] loss: 0.616
[13,    32] loss: 0.615
[14,    32] loss: 0.612
[15,    32] loss: 0.620
[16,    32] loss: 0.613
[17,    32] loss: 0.614
[18,    32] loss: 0.613
[19,    32] loss: 0.618
[20,    32] loss: 0.614
[21,    32] loss: 0.612
[22,    32] loss: 0.624
[23,    32] loss: 0.617
[24,    32] loss: 0.621
[25,    32] loss: 0.622
[26,    32] loss: 0.615
[27,    32] loss: 0.614
[28,    32] loss: 0.617
[29,    32] loss: 0.618
[30,    32] loss: 0.613
[31,    32] loss: 0.613
[32,    32] loss: 0.611
[33,    32] loss: 0.622
[34,    32] loss: 0.616
[35,    32] loss: 0.615
[36,    32] loss: 0.620
[37,    32] loss: 0.614
[38,    32] loss: 0.618
Early stopping applied (best metric=0.48685821890830994)
Finished Training
Total time taken: 90.92909908294678
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.626
[3,    32] loss: 0.618
[4,    32] loss: 0.618
[5,    32] loss: 0.611
[6,    32] loss: 0.626
[7,    32] loss: 0.614
[8,    32] loss: 0.614
[9,    32] loss: 0.615
[10,    32] loss: 0.609
[11,    32] loss: 0.612
[12,    32] loss: 0.612
[13,    32] loss: 0.629
[14,    32] loss: 0.616
[15,    32] loss: 0.611
[16,    32] loss: 0.613
[17,    32] loss: 0.619
[18,    32] loss: 0.613
[19,    32] loss: 0.613
[20,    32] loss: 0.613
[21,    32] loss: 0.622
[22,    32] loss: 0.610
[23,    32] loss: 0.611
[24,    32] loss: 0.618
[25,    32] loss: 0.615
[26,    32] loss: 0.613
[27,    32] loss: 0.613
[28,    32] loss: 0.615
[29,    32] loss: 0.612
[30,    32] loss: 0.617
[31,    32] loss: 0.616
[32,    32] loss: 0.614
[33,    32] loss: 0.614
[34,    32] loss: 0.606
[35,    32] loss: 0.619
[36,    32] loss: 0.613
[37,    32] loss: 0.616
[38,    32] loss: 0.613
[39,    32] loss: 0.613
[40,    32] loss: 0.615
[41,    32] loss: 0.611
[42,    32] loss: 0.612
[43,    32] loss: 0.603
[44,    32] loss: 0.615
[45,    32] loss: 0.617
[46,    32] loss: 0.613
[47,    32] loss: 0.613
[48,    32] loss: 0.610
[49,    32] loss: 0.611
[50,    32] loss: 0.612
[51,    32] loss: 0.617
[52,    32] loss: 0.611
[53,    32] loss: 0.623
[54,    32] loss: 0.612
[55,    32] loss: 0.613
Early stopping applied (best metric=0.4922802746295929)
Finished Training
Total time taken: 131.85359263420105
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.629
[3,    32] loss: 0.620
[4,    32] loss: 0.622
[5,    32] loss: 0.611
[6,    32] loss: 0.615
[7,    32] loss: 0.613
[8,    32] loss: 0.612
[9,    32] loss: 0.619
[10,    32] loss: 0.614
[11,    32] loss: 0.612
[12,    32] loss: 0.615
[13,    32] loss: 0.612
[14,    32] loss: 0.616
[15,    32] loss: 0.609
[16,    32] loss: 0.613
[17,    32] loss: 0.621
[18,    32] loss: 0.611
[19,    32] loss: 0.618
[20,    32] loss: 0.611
[21,    32] loss: 0.616
[22,    32] loss: 0.617
[23,    32] loss: 0.616
[24,    32] loss: 0.619
[25,    32] loss: 0.621
[26,    32] loss: 0.616
[27,    32] loss: 0.621
[28,    32] loss: 0.629
[29,    32] loss: 0.620
[30,    32] loss: 0.620
[31,    32] loss: 0.615
[32,    32] loss: 0.614
[33,    32] loss: 0.620
[34,    32] loss: 0.610
[35,    32] loss: 0.614
[36,    32] loss: 0.614
[37,    32] loss: 0.615
[38,    32] loss: 0.618
[39,    32] loss: 0.613
[40,    32] loss: 0.617
[41,    32] loss: 0.612
[42,    32] loss: 0.616
[43,    32] loss: 0.615
[44,    32] loss: 0.612
[45,    32] loss: 0.611
[46,    32] loss: 0.618
[47,    32] loss: 0.612
[48,    32] loss: 0.620
[49,    32] loss: 0.612
[50,    32] loss: 0.622
[51,    32] loss: 0.611
[52,    32] loss: 0.618
[53,    32] loss: 0.613
[54,    32] loss: 0.622
[55,    32] loss: 0.619
Early stopping applied (best metric=0.4858694076538086)
Finished Training
Total time taken: 131.7775948047638
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.618
[3,    32] loss: 0.611
[4,    32] loss: 0.610
[5,    32] loss: 0.616
[6,    32] loss: 0.605
[7,    32] loss: 0.624
[8,    32] loss: 0.615
[9,    32] loss: 0.609
[10,    32] loss: 0.606
[11,    32] loss: 0.610
[12,    32] loss: 0.616
[13,    32] loss: 0.607
[14,    32] loss: 0.610
[15,    32] loss: 0.609
[16,    32] loss: 0.612
[17,    32] loss: 0.617
[18,    32] loss: 0.620
[19,    32] loss: 0.616
[20,    32] loss: 0.618
[21,    32] loss: 0.604
[22,    32] loss: 0.611
[23,    32] loss: 0.613
[24,    32] loss: 0.610
[25,    32] loss: 0.612
[26,    32] loss: 0.617
[27,    32] loss: 0.606
[28,    32] loss: 0.607
[29,    32] loss: 0.612
[30,    32] loss: 0.615
[31,    32] loss: 0.609
[32,    32] loss: 0.613
[33,    32] loss: 0.607
[34,    32] loss: 0.610
[35,    32] loss: 0.613
[36,    32] loss: 0.611
[37,    32] loss: 0.611
[38,    32] loss: 0.609
[39,    32] loss: 0.614
[40,    32] loss: 0.629
[41,    32] loss: 0.616
[42,    32] loss: 0.615
[43,    32] loss: 0.617
[44,    32] loss: 0.614
[45,    32] loss: 0.608
[46,    32] loss: 0.622
[47,    32] loss: 0.607
[48,    32] loss: 0.604
[49,    32] loss: 0.614
[50,    32] loss: 0.609
[51,    32] loss: 0.613
[52,    32] loss: 0.614
[53,    32] loss: 0.615
[54,    32] loss: 0.609
[55,    32] loss: 0.623
Early stopping applied (best metric=0.4928661584854126)
Finished Training
Total time taken: 131.7275938987732
{'S-palmitoylation-C Validation Accuracy': 0.6969354178087802, 'S-palmitoylation-C Validation Sensitivity': 0.6522772277227723, 'S-palmitoylation-C Validation Specificity': 0.70812959408212, 'S-palmitoylation-C Validation Precision': 0.36006925570674253, 'S-palmitoylation-C AUC ROC': 0.7430718564135029, 'S-palmitoylation-C AUC PR': 0.4365570517678747, 'S-palmitoylation-C MCC': 0.3003826858349068, 'S-palmitoylation-C F1': 0.4636773714144974, 'Validation Loss (S-palmitoylation-C)': 0.4884586989879608, 'Validation Loss (total)': 0.4884586989879608, 'TimeToTrain': 119.2170355796814}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00743908837511666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789407571,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.29717524397132}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.666
[3,    32] loss: 0.646
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002354534650028535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3200588438,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.1319215082046545}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.669
[3,    32] loss: 0.628
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016153353115603971,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1797083276,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.3512735889522}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.613
[3,    32] loss: 0.592
[4,    32] loss: 0.588
[5,    32] loss: 0.575
[6,    32] loss: 0.563
[7,    32] loss: 0.565
[8,    32] loss: 0.558
[9,    32] loss: 0.554
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015435721216949358,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 104995789,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.057520312946806}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.622
[3,    32] loss: 0.611
[4,    32] loss: 0.600
[5,    32] loss: 0.593
[6,    32] loss: 0.590
[7,    32] loss: 0.597
[8,    32] loss: 0.592
[9,    32] loss: 0.589
[10,    32] loss: 0.592
[11,    32] loss: 0.595
[12,    32] loss: 0.582
[13,    32] loss: 0.589
[14,    32] loss: 0.591
[15,    32] loss: 0.586
[16,    32] loss: 0.585
[17,    32] loss: 0.586
[18,    32] loss: 0.587
[19,    32] loss: 0.596
[20,    32] loss: 0.587
[21,    32] loss: 0.590
[22,    32] loss: 0.591
[23,    32] loss: 0.582
[24,    32] loss: 0.580
[25,    32] loss: 0.589
[26,    32] loss: 0.582
[27,    32] loss: 0.591
[28,    32] loss: 0.593
[29,    32] loss: 0.595
[30,    32] loss: 0.591
[31,    32] loss: 0.596
[32,    32] loss: 0.587
[33,    32] loss: 0.585
[34,    32] loss: 0.586
[35,    32] loss: 0.598
[36,    32] loss: 0.594
[37,    32] loss: 0.587
[38,    32] loss: 0.589
[39,    32] loss: 0.588
[40,    32] loss: 0.582
[41,    32] loss: 0.579
[42,    32] loss: 0.586
[43,    32] loss: 0.583
[44,    32] loss: 0.590
[45,    32] loss: 0.583
[46,    32] loss: 0.596
[47,    32] loss: 0.607
Early stopping applied (best metric=0.47210460901260376)
Finished Training
Total time taken: 112.60236287117004
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.616
[3,    32] loss: 0.603
[4,    32] loss: 0.606
[5,    32] loss: 0.593
[6,    32] loss: 0.585
[7,    32] loss: 0.585
[8,    32] loss: 0.582
[9,    32] loss: 0.590
[10,    32] loss: 0.580
[11,    32] loss: 0.581
[12,    32] loss: 0.580
[13,    32] loss: 0.579
[14,    32] loss: 0.586
[15,    32] loss: 0.595
[16,    32] loss: 0.581
[17,    32] loss: 0.589
[18,    32] loss: 0.583
[19,    32] loss: 0.586
[20,    32] loss: 0.583
[21,    32] loss: 0.581
[22,    32] loss: 0.586
[23,    32] loss: 0.593
[24,    32] loss: 0.583
[25,    32] loss: 0.579
[26,    32] loss: 0.583
[27,    32] loss: 0.593
[28,    32] loss: 0.585
[29,    32] loss: 0.585
[30,    32] loss: 0.589
[31,    32] loss: 0.582
[32,    32] loss: 0.590
[33,    32] loss: 0.583
[34,    32] loss: 0.585
[35,    32] loss: 0.595
[36,    32] loss: 0.597
[37,    32] loss: 0.584
[38,    32] loss: 0.601
[39,    32] loss: 0.590
[40,    32] loss: 0.591
[41,    32] loss: 0.582
[42,    32] loss: 0.585
[43,    32] loss: 0.588
[44,    32] loss: 0.588
[45,    32] loss: 0.589
[46,    32] loss: 0.599
[47,    32] loss: 0.583
[48,    32] loss: 0.586
[49,    32] loss: 0.585
[50,    32] loss: 0.581
[51,    32] loss: 0.586
[52,    32] loss: 0.585
[53,    32] loss: 0.583
[54,    32] loss: 0.580
[55,    32] loss: 0.578
[56,    32] loss: 0.584
[57,    32] loss: 0.592
[58,    32] loss: 0.592
[59,    32] loss: 0.587
[60,    32] loss: 0.584
[61,    32] loss: 0.587
[62,    32] loss: 0.584
[63,    32] loss: 0.585
[64,    32] loss: 0.587
[65,    32] loss: 0.588
[66,    32] loss: 0.583
[67,    32] loss: 0.585
[68,    32] loss: 0.583
Early stopping applied (best metric=0.4811598062515259)
Finished Training
Total time taken: 162.9269721508026
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.623
[3,    32] loss: 0.613
[4,    32] loss: 0.590
[5,    32] loss: 0.592
[6,    32] loss: 0.593
[7,    32] loss: 0.591
[8,    32] loss: 0.587
[9,    32] loss: 0.591
[10,    32] loss: 0.578
[11,    32] loss: 0.582
[12,    32] loss: 0.583
[13,    32] loss: 0.599
[14,    32] loss: 0.583
[15,    32] loss: 0.581
[16,    32] loss: 0.588
[17,    32] loss: 0.581
[18,    32] loss: 0.583
[19,    32] loss: 0.583
[20,    32] loss: 0.583
[21,    32] loss: 0.598
[22,    32] loss: 0.590
[23,    32] loss: 0.580
[24,    32] loss: 0.587
[25,    32] loss: 0.587
[26,    32] loss: 0.593
[27,    32] loss: 0.584
[28,    32] loss: 0.584
[29,    32] loss: 0.592
[30,    32] loss: 0.590
Early stopping applied (best metric=0.48747485876083374)
Finished Training
Total time taken: 71.77086853981018
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.643
[3,    32] loss: 0.620
[4,    32] loss: 0.605
[5,    32] loss: 0.603
[6,    32] loss: 0.593
[7,    32] loss: 0.593
[8,    32] loss: 0.584
[9,    32] loss: 0.587
[10,    32] loss: 0.604
[11,    32] loss: 0.593
[12,    32] loss: 0.587
[13,    32] loss: 0.589
[14,    32] loss: 0.593
[15,    32] loss: 0.590
[16,    32] loss: 0.591
[17,    32] loss: 0.585
[18,    32] loss: 0.596
[19,    32] loss: 0.584
[20,    32] loss: 0.594
[21,    32] loss: 0.591
[22,    32] loss: 0.594
[23,    32] loss: 0.591
[24,    32] loss: 0.594
[25,    32] loss: 0.592
[26,    32] loss: 0.588
[27,    32] loss: 0.599
[28,    32] loss: 0.593
[29,    32] loss: 0.597
[30,    32] loss: 0.591
[31,    32] loss: 0.593
[32,    32] loss: 0.595
[33,    32] loss: 0.590
[34,    32] loss: 0.590
[35,    32] loss: 0.599
[36,    32] loss: 0.592
[37,    32] loss: 0.586
[38,    32] loss: 0.596
[39,    32] loss: 0.589
[40,    32] loss: 0.591
[41,    32] loss: 0.591
[42,    32] loss: 0.590
[43,    32] loss: 0.587
[44,    32] loss: 0.591
[45,    32] loss: 0.592
[46,    32] loss: 0.592
[47,    32] loss: 0.596
[48,    32] loss: 0.596
[49,    32] loss: 0.591
[50,    32] loss: 0.598
[51,    32] loss: 0.591
[52,    32] loss: 0.595
[53,    32] loss: 0.591
[54,    32] loss: 0.598
[55,    32] loss: 0.594
[56,    32] loss: 0.600
[57,    32] loss: 0.593
[58,    32] loss: 0.593
[59,    32] loss: 0.592
[60,    32] loss: 0.599
[61,    32] loss: 0.596
[62,    32] loss: 0.590
[63,    32] loss: 0.590
[64,    32] loss: 0.594
Early stopping applied (best metric=0.4866938591003418)
Finished Training
Total time taken: 152.99884939193726
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.622
[3,    32] loss: 0.612
[4,    32] loss: 0.597
[5,    32] loss: 0.604
[6,    32] loss: 0.585
[7,    32] loss: 0.587
[8,    32] loss: 0.581
[9,    32] loss: 0.587
[10,    32] loss: 0.583
[11,    32] loss: 0.589
[12,    32] loss: 0.590
[13,    32] loss: 0.586
[14,    32] loss: 0.581
[15,    32] loss: 0.590
[16,    32] loss: 0.587
[17,    32] loss: 0.583
[18,    32] loss: 0.586
[19,    32] loss: 0.584
[20,    32] loss: 0.585
[21,    32] loss: 0.589
[22,    32] loss: 0.588
[23,    32] loss: 0.595
[24,    32] loss: 0.591
[25,    32] loss: 0.592
[26,    32] loss: 0.585
[27,    32] loss: 0.581
[28,    32] loss: 0.589
[29,    32] loss: 0.595
[30,    32] loss: 0.586
[31,    32] loss: 0.579
[32,    32] loss: 0.582
[33,    32] loss: 0.583
[34,    32] loss: 0.584
[35,    32] loss: 0.583
[36,    32] loss: 0.591
[37,    32] loss: 0.593
[38,    32] loss: 0.586
[39,    32] loss: 0.587
[40,    32] loss: 0.583
[41,    32] loss: 0.590
[42,    32] loss: 0.598
[43,    32] loss: 0.587
[44,    32] loss: 0.584
[45,    32] loss: 0.589
[46,    32] loss: 0.592
[47,    32] loss: 0.589
[48,    32] loss: 0.588
[49,    32] loss: 0.585
[50,    32] loss: 0.590
[51,    32] loss: 0.584
[52,    32] loss: 0.583
[53,    32] loss: 0.584
[54,    32] loss: 0.590
[55,    32] loss: 0.580
[56,    32] loss: 0.585
[57,    32] loss: 0.592
[58,    32] loss: 0.591
[59,    32] loss: 0.587
[60,    32] loss: 0.582
[61,    32] loss: 0.590
[62,    32] loss: 0.585
[63,    32] loss: 0.594
[64,    32] loss: 0.582
[65,    32] loss: 0.583
Early stopping applied (best metric=0.48434728384017944)
Finished Training
Total time taken: 155.26187801361084
{'S-palmitoylation-C Validation Accuracy': 0.6736805673705237, 'S-palmitoylation-C Validation Sensitivity': 0.6855445544554456, 'S-palmitoylation-C Validation Specificity': 0.670708711476242, 'S-palmitoylation-C Validation Precision': 0.3435948807773543, 'S-palmitoylation-C AUC ROC': 0.7452013794990968, 'S-palmitoylation-C AUC PR': 0.43357890693725476, 'S-palmitoylation-C MCC': 0.2916395009893436, 'S-palmitoylation-C F1': 0.4571476045723853, 'Validation Loss (S-palmitoylation-C)': 0.48235608339309693, 'Validation Loss (total)': 0.48235608339309693, 'TimeToTrain': 131.1121861934662}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003436710079185435,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2227257992,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.689806249417742}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.635
[3,    32] loss: 0.631
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002989143641467873,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2040913627,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.6192405439932624}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.601
[3,    32] loss: 0.585
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002713666127360092,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3722774447,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.001843471046676}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.628
[3,    32] loss: 0.627
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011920411202798937,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3190275135,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.94798380185352}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.623
[3,    32] loss: 0.604
[4,    32] loss: 0.587
[5,    32] loss: 0.590
[6,    32] loss: 0.579
[7,    32] loss: 0.591
[8,    32] loss: 0.574
[9,    32] loss: 0.567
[10,    32] loss: 0.570
[11,    32] loss: 0.575
[12,    32] loss: 0.574
[13,    32] loss: 0.561
[14,    32] loss: 0.562
[15,    32] loss: 0.565
[16,    32] loss: 0.559
[17,    32] loss: 0.566
[18,    32] loss: 0.570
[19,    32] loss: 0.567
[20,    32] loss: 0.565
[21,    32] loss: 0.570
[22,    32] loss: 0.556
[23,    32] loss: 0.566
[24,    32] loss: 0.567
[25,    32] loss: 0.573
[26,    32] loss: 0.565
[27,    32] loss: 0.562
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004583151741625428,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4141117771,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.5486004111983185}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.609
[3,    32] loss: 0.594
[4,    32] loss: 0.591
[5,    32] loss: 0.582
[6,    32] loss: 0.589
[7,    32] loss: 0.576
[8,    32] loss: 0.575
[9,    32] loss: 0.581
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002550923447706877,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 176320263,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.024271411283678}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.627
[3,    32] loss: 0.619
[4,    32] loss: 0.627
[5,    32] loss: 0.610
[6,    32] loss: 0.611
[7,    32] loss: 0.614
[8,    32] loss: 0.605
[9,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007760293226260935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2692897903,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.146450853289432}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.637
[3,    32] loss: 0.614
[4,    32] loss: 0.598
[5,    32] loss: 0.591
[6,    32] loss: 0.577
[7,    32] loss: 0.574
[8,    32] loss: 0.567
[9,    32] loss: 0.560
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00016538345381815462,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2942026289,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.941082617581296}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.676
[3,    32] loss: 0.643
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002626422332418947,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 197670412,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.465134579171758}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.616
[3,    32] loss: 0.589
[4,    32] loss: 0.574
[5,    32] loss: 0.552
[6,    32] loss: 0.546
[7,    32] loss: 0.537
[8,    32] loss: 0.535
[9,    32] loss: 0.527
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023500401370058132,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 853171634,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.02404716941895}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.636
[3,    32] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004879663871188681,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2054326273,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.013018603501647}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.636
[3,    32] loss: 0.605
[4,    32] loss: 0.588
[5,    32] loss: 0.580
[6,    32] loss: 0.584
[7,    32] loss: 0.570
[8,    32] loss: 0.580
[9,    32] loss: 0.563
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064705256524717186,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 85016277,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.459968524222408}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.662
[3,    32] loss: 0.658
[4,    32] loss: 0.651
[5,    32] loss: 0.654
[6,    32] loss: 0.654
[7,    32] loss: 0.657
[8,    32] loss: 0.658
[9,    32] loss: 0.658
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067105707829737985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2161243679,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.93490033899604}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.661
[3,    32] loss: 0.646
[4,    32] loss: 0.648
[5,    32] loss: 0.659
[6,    32] loss: 0.651
[7,    32] loss: 0.654
[8,    32] loss: 0.652
[9,    32] loss: 0.655
[10,    32] loss: 0.655
[11,    32] loss: 0.652
[12,    32] loss: 0.661
[13,    32] loss: 0.675
[14,    32] loss: 0.662
[15,    32] loss: 0.683
[16,    32] loss: 0.693
[17,    32] loss: 0.693
[18,    32] loss: 0.693
[19,    32] loss: 0.693
[20,    32] loss: 0.693
[21,    32] loss: 0.693
[22,    32] loss: 0.693
[23,    32] loss: 0.693
[24,    32] loss: 0.693
[25,    32] loss: 0.693
[26,    32] loss: 0.693
[27,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009662747446342124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1974647353,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.603175764149057}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017343726279652639,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 777403891,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.044955554053473}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.619
[3,    32] loss: 0.598
[4,    32] loss: 0.591
[5,    32] loss: 0.589
[6,    32] loss: 0.577
[7,    32] loss: 0.568
[8,    32] loss: 0.576
[9,    32] loss: 0.570
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 4.7497446275165825e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1748353282,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.263439961938111}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.692
[3,    32] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030438173669329206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1753956714,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.6968402673120595}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.615
[3,    32] loss: 0.604
[4,    32] loss: 0.592
[5,    32] loss: 0.591
[6,    32] loss: 0.583
[7,    32] loss: 0.572
[8,    32] loss: 0.581
[9,    32] loss: 0.580
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017571155982531977,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3286828343,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.519662473687537}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.621
[3,    32] loss: 0.611
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00012772297490544178,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4170999510,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.4851248879802679}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.688
[3,    32] loss: 0.661
[4,    32] loss: 0.635
[5,    32] loss: 0.620
[6,    32] loss: 0.614
[7,    32] loss: 0.607
[8,    32] loss: 0.602
[9,    32] loss: 0.593
[10,    32] loss: 0.589
[11,    32] loss: 0.584
[12,    32] loss: 0.578
[13,    32] loss: 0.572
[14,    32] loss: 0.568
[15,    32] loss: 0.557
[16,    32] loss: 0.550
[17,    32] loss: 0.545
[18,    32] loss: 0.525
[19,    32] loss: 0.519
[20,    32] loss: 0.510
[21,    32] loss: 0.501
[22,    32] loss: 0.492
[23,    32] loss: 0.494
[24,    32] loss: 0.468
[25,    32] loss: 0.463
[26,    32] loss: 0.455
[27,    32] loss: 0.459
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004051491420905535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3299827497,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.185036540836941}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.685
[2,    32] loss: 0.611
[3,    32] loss: 0.605
[4,    32] loss: 0.570
[5,    32] loss: 0.568
[6,    32] loss: 0.554
[7,    32] loss: 0.551
[8,    32] loss: 0.552
[9,    32] loss: 0.542
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016887687710404231,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3739414360,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.1148971510699965}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.608
[3,    32] loss: 0.585
[4,    32] loss: 0.556
[5,    32] loss: 0.557
[6,    32] loss: 0.536
[7,    32] loss: 0.529
[8,    32] loss: 0.524
[9,    32] loss: 0.527
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026142414762760596,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1591821367,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.21129316892816385}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.628
[3,    32] loss: 0.608
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.698064774893924e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1790613541,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.738412135804413}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.691
[3,    32] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011782289332916405,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3795891361,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.713216614736321}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.629
[3,    32] loss: 0.614
[4,    32] loss: 0.598
[5,    32] loss: 0.592
[6,    32] loss: 0.584
[7,    32] loss: 0.586
[8,    32] loss: 0.570
[9,    32] loss: 0.576
[10,    32] loss: 0.575
[11,    32] loss: 0.576
[12,    32] loss: 0.579
[13,    32] loss: 0.577
[14,    32] loss: 0.573
[15,    32] loss: 0.565
[16,    32] loss: 0.571
[17,    32] loss: 0.571
[18,    32] loss: 0.564
[19,    32] loss: 0.577
[20,    32] loss: 0.572
[21,    32] loss: 0.575
[22,    32] loss: 0.581
[23,    32] loss: 0.580
[24,    32] loss: 0.567
[25,    32] loss: 0.572
[26,    32] loss: 0.576
[27,    32] loss: 0.583
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005738347265440909,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2944761066,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.28365181009099}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.636
[3,    32] loss: 0.615
[4,    32] loss: 0.600
[5,    32] loss: 0.583
[6,    32] loss: 0.579
[7,    32] loss: 0.573
[8,    32] loss: 0.565
[9,    32] loss: 0.542
[10,    32] loss: 0.532
[11,    32] loss: 0.533
[12,    32] loss: 0.525
[13,    32] loss: 0.518
[14,    32] loss: 0.520
[15,    32] loss: 0.519
[16,    32] loss: 0.518
[17,    32] loss: 0.514
[18,    32] loss: 0.511
[19,    32] loss: 0.516
[20,    32] loss: 0.520
[21,    32] loss: 0.506
[22,    32] loss: 0.505
[23,    32] loss: 0.512
[24,    32] loss: 0.521
[25,    32] loss: 0.511
[26,    32] loss: 0.500
[27,    32] loss: 0.518
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.510794313183268e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1707693989,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.929158420810808}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.702
[2,    32] loss: 0.692
[3,    32] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011002639669666393,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3341965829,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.776057479639107}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.615
[3,    32] loss: 0.601
[4,    32] loss: 0.585
[5,    32] loss: 0.587
[6,    32] loss: 0.567
[7,    32] loss: 0.567
[8,    32] loss: 0.555
[9,    32] loss: 0.549
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 2.097666974622595e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2955750109,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.624197193475247}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034813463762422966,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2050306093,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.203193184041059}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.609
[3,    32] loss: 0.590
[4,    32] loss: 0.581
[5,    32] loss: 0.573
[6,    32] loss: 0.567
[7,    32] loss: 0.560
[8,    32] loss: 0.557
[9,    32] loss: 0.550
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007389140475589474,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 507490692,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.1851434783628605}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.648
[3,    32] loss: 0.624
[4,    32] loss: 0.608
[5,    32] loss: 0.589
[6,    32] loss: 0.592
[7,    32] loss: 0.580
[8,    32] loss: 0.579
[9,    32] loss: 0.571
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 6.919815056864644e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1683061123,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.9555312764400634}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.691
[3,    32] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0051963857386054585,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1968046235,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.715041387564423}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.651
[3,    32] loss: 0.660
[4,    32] loss: 0.651
[5,    32] loss: 0.650
[6,    32] loss: 0.654
[7,    32] loss: 0.649
[8,    32] loss: 0.648
[9,    32] loss: 0.648
[10,    32] loss: 0.655
[11,    32] loss: 0.655
[12,    32] loss: 0.651
[13,    32] loss: 0.643
[14,    32] loss: 0.645
[15,    32] loss: 0.645
[16,    32] loss: 0.641
[17,    32] loss: 0.652
[18,    32] loss: 0.650
[19,    32] loss: 0.647
[20,    32] loss: 0.644
[21,    32] loss: 0.644
[22,    32] loss: 0.646
[23,    32] loss: 0.643
[24,    32] loss: 0.646
[25,    32] loss: 0.647
[26,    32] loss: 0.651
[27,    32] loss: 0.654
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011876462259684801,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1909587448,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.731207249525266}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.632
[3,    32] loss: 0.613
[4,    32] loss: 0.599
[5,    32] loss: 0.599
[6,    32] loss: 0.597
[7,    32] loss: 0.586
[8,    32] loss: 0.584
[9,    32] loss: 0.580
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003716489445504174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2562868569,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.31965064989094}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.615
[3,    32] loss: 0.609
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008466082531391845,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 29854362,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.134349127826143}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.635
[3,    32] loss: 0.625
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002069717108567577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 727585769,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.644208536513663}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.627
[3,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006083703688185482,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 985412422,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.337812314840733}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.631
[3,    32] loss: 0.626
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.361891242378861e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2260831035,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.32285302590806}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.691
[3,    32] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003397207289505515,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2909489040,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.465828543208819}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.619
[3,    32] loss: 0.606
[4,    32] loss: 0.590
[5,    32] loss: 0.601
[6,    32] loss: 0.584
[7,    32] loss: 0.593
[8,    32] loss: 0.597
[9,    32] loss: 0.598
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011541771118567835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 663736231,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.961876753661913}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.623
[3,    32] loss: 0.609
[4,    32] loss: 0.597
[5,    32] loss: 0.593
[6,    32] loss: 0.581
[7,    32] loss: 0.579
[8,    32] loss: 0.572
[9,    32] loss: 0.580
[10,    32] loss: 0.576
[11,    32] loss: 0.577
[12,    32] loss: 0.572
[13,    32] loss: 0.570
[14,    32] loss: 0.568
[15,    32] loss: 0.565
[16,    32] loss: 0.576
[17,    32] loss: 0.577
[18,    32] loss: 0.559
[19,    32] loss: 0.577
[20,    32] loss: 0.582
[21,    32] loss: 0.576
[22,    32] loss: 0.570
[23,    32] loss: 0.580
[24,    32] loss: 0.574
[25,    32] loss: 0.574
[26,    32] loss: 0.574
[27,    32] loss: 0.569
[28,    32] loss: 0.574
[29,    32] loss: 0.584
[30,    32] loss: 0.578
[31,    32] loss: 0.577
[32,    32] loss: 0.579
[33,    32] loss: 0.580
[34,    32] loss: 0.565
[35,    32] loss: 0.576
[36,    32] loss: 0.579
[37,    32] loss: 0.571
[38,    32] loss: 0.581
[39,    32] loss: 0.582
[40,    32] loss: 0.577
[41,    32] loss: 0.578
[42,    32] loss: 0.581
[43,    32] loss: 0.580
[44,    32] loss: 0.578
[45,    32] loss: 0.578
[46,    32] loss: 0.570
[47,    32] loss: 0.573
[48,    32] loss: 0.584
[49,    32] loss: 0.576
[50,    32] loss: 0.580
[51,    32] loss: 0.580
[52,    32] loss: 0.571
[53,    32] loss: 0.583
[54,    32] loss: 0.571
[55,    32] loss: 0.572
[56,    32] loss: 0.581
[57,    32] loss: 0.573
[58,    32] loss: 0.568
[59,    32] loss: 0.586
[60,    32] loss: 0.570
[61,    32] loss: 0.575
[62,    32] loss: 0.581
[63,    32] loss: 0.577
[64,    32] loss: 0.575
[65,    32] loss: 0.570
[66,    32] loss: 0.576
[67,    32] loss: 0.578
[68,    32] loss: 0.572
[69,    32] loss: 0.575
[70,    32] loss: 0.569
[71,    32] loss: 0.602
[72,    32] loss: 0.576
[73,    32] loss: 0.580
[74,    32] loss: 0.575
[75,    32] loss: 0.573
Early stopping applied (best metric=0.46562665700912476)
Finished Training
Total time taken: 179.46717190742493
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.626
[3,    32] loss: 0.608
[4,    32] loss: 0.594
[5,    32] loss: 0.588
[6,    32] loss: 0.590
[7,    32] loss: 0.582
[8,    32] loss: 0.573
[9,    32] loss: 0.579
[10,    32] loss: 0.572
[11,    32] loss: 0.586
[12,    32] loss: 0.574
[13,    32] loss: 0.567
[14,    32] loss: 0.568
[15,    32] loss: 0.574
[16,    32] loss: 0.579
[17,    32] loss: 0.573
[18,    32] loss: 0.581
[19,    32] loss: 0.574
[20,    32] loss: 0.576
[21,    32] loss: 0.569
[22,    32] loss: 0.574
[23,    32] loss: 0.569
[24,    32] loss: 0.569
[25,    32] loss: 0.582
[26,    32] loss: 0.572
[27,    32] loss: 0.584
[28,    32] loss: 0.584
[29,    32] loss: 0.574
[30,    32] loss: 0.578
[31,    32] loss: 0.573
[32,    32] loss: 0.570
[33,    32] loss: 0.578
[34,    32] loss: 0.581
[35,    32] loss: 0.571
[36,    32] loss: 0.569
[37,    32] loss: 0.572
[38,    32] loss: 0.576
[39,    32] loss: 0.579
[40,    32] loss: 0.581
[41,    32] loss: 0.582
[42,    32] loss: 0.577
[43,    32] loss: 0.574
[44,    32] loss: 0.575
[45,    32] loss: 0.581
[46,    32] loss: 0.593
[47,    32] loss: 0.578
[48,    32] loss: 0.577
[49,    32] loss: 0.576
[50,    32] loss: 0.577
[51,    32] loss: 0.587
[52,    32] loss: 0.582
[53,    32] loss: 0.580
[54,    32] loss: 0.572
[55,    32] loss: 0.582
[56,    32] loss: 0.582
[57,    32] loss: 0.575
[58,    32] loss: 0.585
[59,    32] loss: 0.577
[60,    32] loss: 0.579
[61,    32] loss: 0.577
[62,    32] loss: 0.581
[63,    32] loss: 0.578
[64,    32] loss: 0.571
Early stopping applied (best metric=0.47869375348091125)
Finished Training
Total time taken: 153.45385646820068
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.622
[3,    32] loss: 0.605
[4,    32] loss: 0.599
[5,    32] loss: 0.597
[6,    32] loss: 0.584
[7,    32] loss: 0.584
[8,    32] loss: 0.581
[9,    32] loss: 0.578
[10,    32] loss: 0.573
[11,    32] loss: 0.580
[12,    32] loss: 0.570
[13,    32] loss: 0.563
[14,    32] loss: 0.579
[15,    32] loss: 0.572
[16,    32] loss: 0.569
[17,    32] loss: 0.574
[18,    32] loss: 0.572
[19,    32] loss: 0.572
[20,    32] loss: 0.575
[21,    32] loss: 0.579
[22,    32] loss: 0.573
[23,    32] loss: 0.578
[24,    32] loss: 0.576
[25,    32] loss: 0.573
[26,    32] loss: 0.580
[27,    32] loss: 0.576
[28,    32] loss: 0.565
[29,    32] loss: 0.577
[30,    32] loss: 0.579
[31,    32] loss: 0.577
[32,    32] loss: 0.569
[33,    32] loss: 0.571
[34,    32] loss: 0.569
[35,    32] loss: 0.577
[36,    32] loss: 0.572
[37,    32] loss: 0.577
[38,    32] loss: 0.573
[39,    32] loss: 0.578
[40,    32] loss: 0.574
[41,    32] loss: 0.593
[42,    32] loss: 0.569
[43,    32] loss: 0.579
[44,    32] loss: 0.574
[45,    32] loss: 0.569
[46,    32] loss: 0.578
[47,    32] loss: 0.569
[48,    32] loss: 0.582
[49,    32] loss: 0.573
[50,    32] loss: 0.576
[51,    32] loss: 0.581
[52,    32] loss: 0.572
[53,    32] loss: 0.576
[54,    32] loss: 0.572
[55,    32] loss: 0.575
Early stopping applied (best metric=0.474710077047348)
Finished Training
Total time taken: 131.7325930595398
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.639
[3,    32] loss: 0.615
[4,    32] loss: 0.597
[5,    32] loss: 0.591
[6,    32] loss: 0.585
[7,    32] loss: 0.589
[8,    32] loss: 0.578
[9,    32] loss: 0.572
[10,    32] loss: 0.579
[11,    32] loss: 0.573
[12,    32] loss: 0.579
[13,    32] loss: 0.575
[14,    32] loss: 0.582
[15,    32] loss: 0.581
[16,    32] loss: 0.570
[17,    32] loss: 0.580
[18,    32] loss: 0.574
[19,    32] loss: 0.579
[20,    32] loss: 0.570
[21,    32] loss: 0.574
[22,    32] loss: 0.581
[23,    32] loss: 0.578
[24,    32] loss: 0.585
[25,    32] loss: 0.579
[26,    32] loss: 0.581
[27,    32] loss: 0.582
[28,    32] loss: 0.578
[29,    32] loss: 0.571
[30,    32] loss: 0.573
[31,    32] loss: 0.584
[32,    32] loss: 0.579
[33,    32] loss: 0.581
[34,    32] loss: 0.577
[35,    32] loss: 0.578
[36,    32] loss: 0.577
[37,    32] loss: 0.577
[38,    32] loss: 0.589
[39,    32] loss: 0.588
[40,    32] loss: 0.580
[41,    32] loss: 0.582
[42,    32] loss: 0.590
[43,    32] loss: 0.582
[44,    32] loss: 0.578
[45,    32] loss: 0.583
[46,    32] loss: 0.572
[47,    32] loss: 0.581
[48,    32] loss: 0.580
[49,    32] loss: 0.578
[50,    32] loss: 0.579
[51,    32] loss: 0.581
[52,    32] loss: 0.580
[53,    32] loss: 0.573
[54,    32] loss: 0.583
[55,    32] loss: 0.583
[56,    32] loss: 0.586
[57,    32] loss: 0.573
[58,    32] loss: 0.579
[59,    32] loss: 0.582
[60,    32] loss: 0.584
[61,    32] loss: 0.582
[62,    32] loss: 0.579
[63,    32] loss: 0.582
[64,    32] loss: 0.578
[65,    32] loss: 0.573
Early stopping applied (best metric=0.48516586422920227)
Finished Training
Total time taken: 155.62289595603943
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.632
[3,    32] loss: 0.608
[4,    32] loss: 0.601
[5,    32] loss: 0.592
[6,    32] loss: 0.583
[7,    32] loss: 0.590
[8,    32] loss: 0.590
[9,    32] loss: 0.573
[10,    32] loss: 0.572
[11,    32] loss: 0.575
[12,    32] loss: 0.579
[13,    32] loss: 0.581
[14,    32] loss: 0.587
[15,    32] loss: 0.572
[16,    32] loss: 0.582
[17,    32] loss: 0.579
[18,    32] loss: 0.581
[19,    32] loss: 0.575
[20,    32] loss: 0.582
[21,    32] loss: 0.586
[22,    32] loss: 0.579
[23,    32] loss: 0.576
[24,    32] loss: 0.578
[25,    32] loss: 0.568
[26,    32] loss: 0.574
[27,    32] loss: 0.578
[28,    32] loss: 0.579
[29,    32] loss: 0.579
[30,    32] loss: 0.575
[31,    32] loss: 0.578
[32,    32] loss: 0.578
[33,    32] loss: 0.582
Early stopping applied (best metric=0.46667060256004333)
Finished Training
Total time taken: 78.94895386695862
{'S-palmitoylation-C Validation Accuracy': 0.6850276942853363, 'S-palmitoylation-C Validation Sensitivity': 0.6986138613861386, 'S-palmitoylation-C Validation Specificity': 0.6816211580967964, 'S-palmitoylation-C Validation Precision': 0.3575032017120545, 'S-palmitoylation-C AUC ROC': 0.7588576521361012, 'S-palmitoylation-C AUC PR': 0.4540808317402491, 'S-palmitoylation-C MCC': 0.3132079315412922, 'S-palmitoylation-C F1': 0.4715767558940789, 'Validation Loss (S-palmitoylation-C)': 0.47417339086532595, 'Validation Loss (total)': 0.47417339086532595, 'TimeToTrain': 139.8450942516327}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020748411062907293,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2494789771,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.973223120835616}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.618
[3,    32] loss: 0.615
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005178898547016852,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1605043279,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.642647997378862}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.641
[3,    32] loss: 0.618
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001883458179970342,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1098794228,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.9221569869741435}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.620
[3,    32] loss: 0.598
[4,    32] loss: 0.576
[5,    32] loss: 0.567
[6,    32] loss: 0.564
[7,    32] loss: 0.558
[8,    32] loss: 0.552
[9,    32] loss: 0.546
[10,    32] loss: 0.541
[11,    32] loss: 0.549
[12,    32] loss: 0.545
[13,    32] loss: 0.541
[14,    32] loss: 0.536
[15,    32] loss: 0.531
[16,    32] loss: 0.535
[17,    32] loss: 0.541
[18,    32] loss: 0.531
[19,    32] loss: 0.524
[20,    32] loss: 0.522
[21,    32] loss: 0.520
[22,    32] loss: 0.539
[23,    32] loss: 0.541
[24,    32] loss: 0.529
[25,    32] loss: 0.534
[26,    32] loss: 0.530
[27,    32] loss: 0.536
[28,    32] loss: 0.546
[29,    32] loss: 0.526
[30,    32] loss: 0.533
[31,    32] loss: 0.534
[32,    32] loss: 0.534
[33,    32] loss: 0.532
[34,    32] loss: 0.530
[35,    32] loss: 0.535
[36,    32] loss: 0.534
[37,    32] loss: 0.530
[38,    32] loss: 0.541
[39,    32] loss: 0.524
[40,    32] loss: 0.537
[41,    32] loss: 0.525
[42,    32] loss: 0.532
[43,    32] loss: 0.543
[44,    32] loss: 0.533
[45,    32] loss: 0.533
[46,    32] loss: 0.530
[47,    32] loss: 0.532
[48,    32] loss: 0.537
[49,    32] loss: 0.531
[50,    32] loss: 0.537
[51,    32] loss: 0.537
[52,    32] loss: 0.536
[53,    32] loss: 0.532
[54,    32] loss: 0.543
[55,    32] loss: 0.539
[56,    32] loss: 0.534
[57,    32] loss: 0.538
[58,    32] loss: 0.532
[59,    32] loss: 0.530
[60,    32] loss: 0.536
[61,    32] loss: 0.529
[62,    32] loss: 0.536
[63,    32] loss: 0.541
[64,    32] loss: 0.529
[65,    32] loss: 0.539
Early stopping applied (best metric=0.4642407298088074)
Finished Training
Total time taken: 155.43988180160522
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.630
[3,    32] loss: 0.599
[4,    32] loss: 0.583
[5,    32] loss: 0.580
[6,    32] loss: 0.560
[7,    32] loss: 0.557
[8,    32] loss: 0.551
[9,    32] loss: 0.539
[10,    32] loss: 0.542
[11,    32] loss: 0.545
[12,    32] loss: 0.538
[13,    32] loss: 0.546
[14,    32] loss: 0.549
[15,    32] loss: 0.558
[16,    32] loss: 0.539
[17,    32] loss: 0.538
[18,    32] loss: 0.538
[19,    32] loss: 0.542
[20,    32] loss: 0.540
[21,    32] loss: 0.544
[22,    32] loss: 0.545
[23,    32] loss: 0.550
[24,    32] loss: 0.537
[25,    32] loss: 0.536
[26,    32] loss: 0.539
[27,    32] loss: 0.539
[28,    32] loss: 0.534
[29,    32] loss: 0.535
[30,    32] loss: 0.546
[31,    32] loss: 0.542
[32,    32] loss: 0.548
[33,    32] loss: 0.531
[34,    32] loss: 0.529
[35,    32] loss: 0.535
[36,    32] loss: 0.531
[37,    32] loss: 0.541
[38,    32] loss: 0.536
[39,    32] loss: 0.536
[40,    32] loss: 0.542
[41,    32] loss: 0.548
[42,    32] loss: 0.538
[43,    32] loss: 0.542
[44,    32] loss: 0.534
[45,    32] loss: 0.530
[46,    32] loss: 0.541
[47,    32] loss: 0.540
[48,    32] loss: 0.542
[49,    32] loss: 0.543
[50,    32] loss: 0.529
[51,    32] loss: 0.547
[52,    32] loss: 0.529
Early stopping applied (best metric=0.47188881039619446)
Finished Training
Total time taken: 124.52450585365295
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.618
[3,    32] loss: 0.594
[4,    32] loss: 0.579
[5,    32] loss: 0.562
[6,    32] loss: 0.557
[7,    32] loss: 0.548
[8,    32] loss: 0.543
[9,    32] loss: 0.542
[10,    32] loss: 0.537
[11,    32] loss: 0.531
[12,    32] loss: 0.526
[13,    32] loss: 0.523
[14,    32] loss: 0.536
[15,    32] loss: 0.530
[16,    32] loss: 0.529
[17,    32] loss: 0.523
[18,    32] loss: 0.535
[19,    32] loss: 0.526
[20,    32] loss: 0.527
[21,    32] loss: 0.533
[22,    32] loss: 0.527
[23,    32] loss: 0.533
[24,    32] loss: 0.529
[25,    32] loss: 0.539
[26,    32] loss: 0.529
[27,    32] loss: 0.521
[28,    32] loss: 0.531
[29,    32] loss: 0.528
[30,    32] loss: 0.524
[31,    32] loss: 0.530
[32,    32] loss: 0.530
[33,    32] loss: 0.527
[34,    32] loss: 0.520
[35,    32] loss: 0.533
[36,    32] loss: 0.533
[37,    32] loss: 0.537
[38,    32] loss: 0.528
Early stopping applied (best metric=0.47632089257240295)
Finished Training
Total time taken: 91.19710326194763
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.629
[3,    32] loss: 0.600
[4,    32] loss: 0.588
[5,    32] loss: 0.571
[6,    32] loss: 0.555
[7,    32] loss: 0.555
[8,    32] loss: 0.548
[9,    32] loss: 0.538
[10,    32] loss: 0.544
[11,    32] loss: 0.539
[12,    32] loss: 0.543
[13,    32] loss: 0.543
[14,    32] loss: 0.537
[15,    32] loss: 0.541
[16,    32] loss: 0.536
[17,    32] loss: 0.548
[18,    32] loss: 0.525
[19,    32] loss: 0.544
[20,    32] loss: 0.537
[21,    32] loss: 0.536
[22,    32] loss: 0.529
[23,    32] loss: 0.536
[24,    32] loss: 0.537
[25,    32] loss: 0.528
[26,    32] loss: 0.533
[27,    32] loss: 0.530
[28,    32] loss: 0.531
[29,    32] loss: 0.539
[30,    32] loss: 0.532
[31,    32] loss: 0.522
[32,    32] loss: 0.535
[33,    32] loss: 0.527
[34,    32] loss: 0.531
[35,    32] loss: 0.525
[36,    32] loss: 0.534
[37,    32] loss: 0.527
[38,    32] loss: 0.522
[39,    32] loss: 0.530
[40,    32] loss: 0.532
[41,    32] loss: 0.536
[42,    32] loss: 0.523
[43,    32] loss: 0.529
[44,    32] loss: 0.528
[45,    32] loss: 0.530
[46,    32] loss: 0.536
[47,    32] loss: 0.529
[48,    32] loss: 0.528
[49,    32] loss: 0.529
[50,    32] loss: 0.538
[51,    32] loss: 0.533
[52,    32] loss: 0.527
[53,    32] loss: 0.544
[54,    32] loss: 0.532
[55,    32] loss: 0.530
[56,    32] loss: 0.527
[57,    32] loss: 0.527
[58,    32] loss: 0.541
[59,    32] loss: 0.527
[60,    32] loss: 0.537
[61,    32] loss: 0.534
Early stopping applied (best metric=0.46586844325065613)
Finished Training
Total time taken: 145.8778088092804
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.620
[3,    32] loss: 0.587
[4,    32] loss: 0.569
[5,    32] loss: 0.574
[6,    32] loss: 0.559
[7,    32] loss: 0.553
[8,    32] loss: 0.549
[9,    32] loss: 0.546
[10,    32] loss: 0.544
[11,    32] loss: 0.536
[12,    32] loss: 0.533
[13,    32] loss: 0.538
[14,    32] loss: 0.534
[15,    32] loss: 0.530
[16,    32] loss: 0.531
[17,    32] loss: 0.531
[18,    32] loss: 0.536
[19,    32] loss: 0.529
[20,    32] loss: 0.540
[21,    32] loss: 0.545
[22,    32] loss: 0.522
[23,    32] loss: 0.535
[24,    32] loss: 0.528
[25,    32] loss: 0.538
[26,    32] loss: 0.535
[27,    32] loss: 0.536
[28,    32] loss: 0.519
[29,    32] loss: 0.532
[30,    32] loss: 0.531
[31,    32] loss: 0.519
[32,    32] loss: 0.528
[33,    32] loss: 0.533
[34,    32] loss: 0.535
[35,    32] loss: 0.532
[36,    32] loss: 0.529
[37,    32] loss: 0.525
[38,    32] loss: 0.538
[39,    32] loss: 0.536
[40,    32] loss: 0.536
[41,    32] loss: 0.528
[42,    32] loss: 0.530
[43,    32] loss: 0.532
[44,    32] loss: 0.535
[45,    32] loss: 0.534
[46,    32] loss: 0.530
[47,    32] loss: 0.530
[48,    32] loss: 0.524
[49,    32] loss: 0.526
[50,    32] loss: 0.538
[51,    32] loss: 0.528
[52,    32] loss: 0.539
[53,    32] loss: 0.545
[54,    32] loss: 0.535
[55,    32] loss: 0.535
[56,    32] loss: 0.527
[57,    32] loss: 0.541
[58,    32] loss: 0.534
[59,    32] loss: 0.526
[60,    32] loss: 0.533
[61,    32] loss: 0.540
Early stopping applied (best metric=0.47723183035850525)
Finished Training
Total time taken: 146.46976947784424
{'S-palmitoylation-C Validation Accuracy': 0.7123375048047538, 'S-palmitoylation-C Validation Sensitivity': 0.6455445544554456, 'S-palmitoylation-C Validation Specificity': 0.7290818858560794, 'S-palmitoylation-C Validation Precision': 0.37575826493886666, 'S-palmitoylation-C AUC ROC': 0.7569673888558535, 'S-palmitoylation-C AUC PR': 0.45867646657664024, 'S-palmitoylation-C MCC': 0.31636540793025164, 'S-palmitoylation-C F1': 0.4739900123924026, 'Validation Loss (S-palmitoylation-C)': 0.4711101412773132, 'Validation Loss (total)': 0.4711101412773132, 'TimeToTrain': 132.7018138408661}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002249209373496018,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3896165552,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.6307999507126896}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.612
[3,    32] loss: 0.594
[4,    32] loss: 0.561
[5,    32] loss: 0.559
[6,    32] loss: 0.535
[7,    32] loss: 0.535
[8,    32] loss: 0.510
[9,    32] loss: 0.508
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018505228308254545,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2624286352,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.261595778704448}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.611
[3,    32] loss: 0.593
[4,    32] loss: 0.582
[5,    32] loss: 0.568
[6,    32] loss: 0.564
[7,    32] loss: 0.551
[8,    32] loss: 0.560
[9,    32] loss: 0.552
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 2.9471438493309252e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3095180191,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.40876888752174}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.693
[3,    32] loss: 0.692
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002196248900981184,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2835369304,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.56058836105992}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.606
[3,    32] loss: 0.595
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008646864397231443,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1182281046,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.531975363423285}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.669
[3,    32] loss: 0.654
[4,    32] loss: 0.653
[5,    32] loss: 0.663
[6,    32] loss: 0.652
[7,    32] loss: 0.660
[8,    32] loss: 0.647
[9,    32] loss: 0.654
[10,    32] loss: 0.653
[11,    32] loss: 0.662
[12,    32] loss: 0.655
[13,    32] loss: 0.652
[14,    32] loss: 0.650
[15,    32] loss: 0.653
[16,    32] loss: 0.648
[17,    32] loss: 0.651
[18,    32] loss: 0.657
[19,    32] loss: 0.657
[20,    32] loss: 0.654
[21,    32] loss: 0.655
[22,    32] loss: 0.658
[23,    32] loss: 0.656
[24,    32] loss: 0.669
[25,    32] loss: 0.664
[26,    32] loss: 0.655
[27,    32] loss: 0.658
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015523871685730753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3060483912,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.401568090189874}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.618
[3,    32] loss: 0.592
[4,    32] loss: 0.564
[5,    32] loss: 0.550
[6,    32] loss: 0.547
[7,    32] loss: 0.537
[8,    32] loss: 0.522
[9,    32] loss: 0.517
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034627398612261527,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2882227725,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.416733441802915}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.703
[2,    32] loss: 0.615
[3,    32] loss: 0.600
[4,    32] loss: 0.593
[5,    32] loss: 0.585
[6,    32] loss: 0.580
[7,    32] loss: 0.587
[8,    32] loss: 0.585
[9,    32] loss: 0.578
[10,    32] loss: 0.577
[11,    32] loss: 0.575
[12,    32] loss: 0.583
[13,    32] loss: 0.578
[14,    32] loss: 0.576
[15,    32] loss: 0.569
[16,    32] loss: 0.576
[17,    32] loss: 0.578
[18,    32] loss: 0.575
[19,    32] loss: 0.581
[20,    32] loss: 0.580
[21,    32] loss: 0.579
[22,    32] loss: 0.573
[23,    32] loss: 0.581
[24,    32] loss: 0.582
[25,    32] loss: 0.580
[26,    32] loss: 0.581
[27,    32] loss: 0.579
[28,    32] loss: 0.574
[29,    32] loss: 0.580
[30,    32] loss: 0.582
[31,    32] loss: 0.577
[32,    32] loss: 0.582
[33,    32] loss: 0.578
Early stopping applied (best metric=0.47385767102241516)
Finished Training
Total time taken: 79.21921420097351
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.613
[3,    32] loss: 0.602
[4,    32] loss: 0.599
[5,    32] loss: 0.595
[6,    32] loss: 0.584
[7,    32] loss: 0.581
[8,    32] loss: 0.580
[9,    32] loss: 0.581
[10,    32] loss: 0.578
[11,    32] loss: 0.578
[12,    32] loss: 0.575
[13,    32] loss: 0.574
[14,    32] loss: 0.574
[15,    32] loss: 0.574
[16,    32] loss: 0.576
[17,    32] loss: 0.572
[18,    32] loss: 0.585
[19,    32] loss: 0.571
[20,    32] loss: 0.586
[21,    32] loss: 0.572
[22,    32] loss: 0.585
[23,    32] loss: 0.581
[24,    32] loss: 0.572
[25,    32] loss: 0.580
[26,    32] loss: 0.565
[27,    32] loss: 0.574
[28,    32] loss: 0.579
[29,    32] loss: 0.583
[30,    32] loss: 0.585
[31,    32] loss: 0.583
[32,    32] loss: 0.580
[33,    32] loss: 0.583
[34,    32] loss: 0.577
[35,    32] loss: 0.586
Early stopping applied (best metric=0.46569302678108215)
Finished Training
Total time taken: 83.83522725105286
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.622
[3,    32] loss: 0.610
[4,    32] loss: 0.589
[5,    32] loss: 0.587
[6,    32] loss: 0.580
[7,    32] loss: 0.587
[8,    32] loss: 0.579
[9,    32] loss: 0.574
[10,    32] loss: 0.572
[11,    32] loss: 0.576
[12,    32] loss: 0.576
[13,    32] loss: 0.572
[14,    32] loss: 0.578
[15,    32] loss: 0.572
[16,    32] loss: 0.570
[17,    32] loss: 0.575
[18,    32] loss: 0.579
[19,    32] loss: 0.584
[20,    32] loss: 0.575
[21,    32] loss: 0.568
[22,    32] loss: 0.574
[23,    32] loss: 0.577
[24,    32] loss: 0.567
[25,    32] loss: 0.572
[26,    32] loss: 0.573
[27,    32] loss: 0.591
[28,    32] loss: 0.575
[29,    32] loss: 0.578
[30,    32] loss: 0.573
[31,    32] loss: 0.586
[32,    32] loss: 0.572
[33,    32] loss: 0.575
[34,    32] loss: 0.573
[35,    32] loss: 0.573
[36,    32] loss: 0.570
[37,    32] loss: 0.579
[38,    32] loss: 0.582
[39,    32] loss: 0.571
[40,    32] loss: 0.576
[41,    32] loss: 0.572
[42,    32] loss: 0.581
[43,    32] loss: 0.575
[44,    32] loss: 0.579
[45,    32] loss: 0.577
[46,    32] loss: 0.577
[47,    32] loss: 0.578
[48,    32] loss: 0.581
[49,    32] loss: 0.579
[50,    32] loss: 0.581
[51,    32] loss: 0.581
[52,    32] loss: 0.586
[53,    32] loss: 0.578
[54,    32] loss: 0.571
[55,    32] loss: 0.573
[56,    32] loss: 0.575
[57,    32] loss: 0.571
[58,    32] loss: 0.583
[59,    32] loss: 0.571
[60,    32] loss: 0.575
[61,    32] loss: 0.572
[62,    32] loss: 0.577
[63,    32] loss: 0.572
[64,    32] loss: 0.578
[65,    32] loss: 0.575
[66,    32] loss: 0.580
[67,    32] loss: 0.572
[68,    32] loss: 0.576
[69,    32] loss: 0.583
[70,    32] loss: 0.572
[71,    32] loss: 0.576
[72,    32] loss: 0.584
[73,    32] loss: 0.574
[74,    32] loss: 0.581
[75,    32] loss: 0.574
[76,    32] loss: 0.570
[77,    32] loss: 0.573
[78,    32] loss: 0.585
[79,    32] loss: 0.579
Early stopping applied (best metric=0.47795552015304565)
Finished Training
Total time taken: 189.0235104560852
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.612
[3,    32] loss: 0.595
[4,    32] loss: 0.595
[5,    32] loss: 0.588
[6,    32] loss: 0.581
[7,    32] loss: 0.575
[8,    32] loss: 0.583
[9,    32] loss: 0.576
[10,    32] loss: 0.581
[11,    32] loss: 0.577
[12,    32] loss: 0.578
[13,    32] loss: 0.579
[14,    32] loss: 0.572
[15,    32] loss: 0.574
[16,    32] loss: 0.573
[17,    32] loss: 0.583
[18,    32] loss: 0.585
[19,    32] loss: 0.574
[20,    32] loss: 0.580
[21,    32] loss: 0.571
[22,    32] loss: 0.576
[23,    32] loss: 0.568
[24,    32] loss: 0.576
[25,    32] loss: 0.574
[26,    32] loss: 0.575
[27,    32] loss: 0.571
[28,    32] loss: 0.574
[29,    32] loss: 0.576
[30,    32] loss: 0.585
[31,    32] loss: 0.573
[32,    32] loss: 0.577
[33,    32] loss: 0.590
[34,    32] loss: 0.564
[35,    32] loss: 0.577
[36,    32] loss: 0.582
[37,    32] loss: 0.582
[38,    32] loss: 0.575
[39,    32] loss: 0.574
[40,    32] loss: 0.578
[41,    32] loss: 0.576
[42,    32] loss: 0.572
[43,    32] loss: 0.562
[44,    32] loss: 0.580
[45,    32] loss: 0.580
[46,    32] loss: 0.567
[47,    32] loss: 0.580
[48,    32] loss: 0.576
[49,    32] loss: 0.578
[50,    32] loss: 0.572
Early stopping applied (best metric=0.47561609745025635)
Finished Training
Total time taken: 119.97832560539246
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.617
[3,    32] loss: 0.602
[4,    32] loss: 0.591
[5,    32] loss: 0.593
[6,    32] loss: 0.577
[7,    32] loss: 0.579
[8,    32] loss: 0.581
[9,    32] loss: 0.568
[10,    32] loss: 0.580
[11,    32] loss: 0.571
[12,    32] loss: 0.579
[13,    32] loss: 0.587
[14,    32] loss: 0.578
[15,    32] loss: 0.575
[16,    32] loss: 0.579
[17,    32] loss: 0.578
[18,    32] loss: 0.580
[19,    32] loss: 0.574
[20,    32] loss: 0.583
[21,    32] loss: 0.578
[22,    32] loss: 0.583
[23,    32] loss: 0.580
[24,    32] loss: 0.575
[25,    32] loss: 0.579
[26,    32] loss: 0.586
[27,    32] loss: 0.581
[28,    32] loss: 0.590
[29,    32] loss: 0.578
[30,    32] loss: 0.582
[31,    32] loss: 0.578
[32,    32] loss: 0.577
[33,    32] loss: 0.576
[34,    32] loss: 0.578
[35,    32] loss: 0.581
[36,    32] loss: 0.584
[37,    32] loss: 0.578
[38,    32] loss: 0.581
[39,    32] loss: 0.579
[40,    32] loss: 0.584
[41,    32] loss: 0.585
[42,    32] loss: 0.583
[43,    32] loss: 0.581
[44,    32] loss: 0.583
[45,    32] loss: 0.580
[46,    32] loss: 0.574
[47,    32] loss: 0.580
[48,    32] loss: 0.588
[49,    32] loss: 0.579
[50,    32] loss: 0.595
[51,    32] loss: 0.577
[52,    32] loss: 0.582
[53,    32] loss: 0.579
[54,    32] loss: 0.580
[55,    32] loss: 0.583
[56,    32] loss: 0.583
Early stopping applied (best metric=0.47236889600753784)
Finished Training
Total time taken: 135.28336215019226
{'S-palmitoylation-C Validation Accuracy': 0.660264812819398, 'S-palmitoylation-C Validation Sensitivity': 0.7271287128712871, 'S-palmitoylation-C Validation Specificity': 0.6435054864090326, 'S-palmitoylation-C Validation Precision': 0.34661905117542235, 'S-palmitoylation-C AUC ROC': 0.7575578884325458, 'S-palmitoylation-C AUC PR': 0.4519451031440408, 'S-palmitoylation-C MCC': 0.30599313211876233, 'S-palmitoylation-C F1': 0.46381433308292624, 'Validation Loss (S-palmitoylation-C)': 0.47309824228286745, 'Validation Loss (total)': 0.47309824228286745, 'TimeToTrain': 121.46792793273926}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017109350924948488,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3742447908,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.001182203341768}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.621
[3,    32] loss: 0.608
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037123735837268283,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3477808848,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.671828560879117}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.628
[3,    32] loss: 0.600
[4,    32] loss: 0.592
[5,    32] loss: 0.585
[6,    32] loss: 0.590
[7,    32] loss: 0.580
[8,    32] loss: 0.581
[9,    32] loss: 0.577
[10,    32] loss: 0.575
[11,    32] loss: 0.576
[12,    32] loss: 0.565
[13,    32] loss: 0.565
[14,    32] loss: 0.576
[15,    32] loss: 0.566
[16,    32] loss: 0.571
[17,    32] loss: 0.574
[18,    32] loss: 0.575
[19,    32] loss: 0.582
[20,    32] loss: 0.573
[21,    32] loss: 0.566
[22,    32] loss: 0.569
[23,    32] loss: 0.563
[24,    32] loss: 0.571
[25,    32] loss: 0.566
[26,    32] loss: 0.570
[27,    32] loss: 0.566
[28,    32] loss: 0.567
[29,    32] loss: 0.568
[30,    32] loss: 0.574
[31,    32] loss: 0.563
[32,    32] loss: 0.567
[33,    32] loss: 0.571
[34,    32] loss: 0.571
[35,    32] loss: 0.576
[36,    32] loss: 0.577
[37,    32] loss: 0.567
[38,    32] loss: 0.580
[39,    32] loss: 0.567
[40,    32] loss: 0.569
[41,    32] loss: 0.572
[42,    32] loss: 0.575
[43,    32] loss: 0.564
[44,    32] loss: 0.568
[45,    32] loss: 0.574
[46,    32] loss: 0.577
[47,    32] loss: 0.567
[48,    32] loss: 0.568
[49,    32] loss: 0.565
[50,    32] loss: 0.570
[51,    32] loss: 0.566
[52,    32] loss: 0.564
Early stopping applied (best metric=0.4692161977291107)
Finished Training
Total time taken: 124.53833508491516
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.623
[3,    32] loss: 0.592
[4,    32] loss: 0.577
[5,    32] loss: 0.573
[6,    32] loss: 0.578
[7,    32] loss: 0.567
[8,    32] loss: 0.570
[9,    32] loss: 0.567
[10,    32] loss: 0.563
[11,    32] loss: 0.569
[12,    32] loss: 0.568
[13,    32] loss: 0.565
[14,    32] loss: 0.570
[15,    32] loss: 0.559
[16,    32] loss: 0.561
[17,    32] loss: 0.571
[18,    32] loss: 0.568
[19,    32] loss: 0.562
[20,    32] loss: 0.566
[21,    32] loss: 0.564
[22,    32] loss: 0.562
[23,    32] loss: 0.569
[24,    32] loss: 0.562
[25,    32] loss: 0.559
[26,    32] loss: 0.563
[27,    32] loss: 0.574
[28,    32] loss: 0.565
[29,    32] loss: 0.568
[30,    32] loss: 0.562
[31,    32] loss: 0.561
[32,    32] loss: 0.566
[33,    32] loss: 0.560
[34,    32] loss: 0.564
[35,    32] loss: 0.558
[36,    32] loss: 0.560
[37,    32] loss: 0.566
Early stopping applied (best metric=0.4666685163974762)
Finished Training
Total time taken: 88.62123918533325
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.615
[3,    32] loss: 0.590
[4,    32] loss: 0.589
[5,    32] loss: 0.576
[6,    32] loss: 0.565
[7,    32] loss: 0.565
[8,    32] loss: 0.571
[9,    32] loss: 0.568
[10,    32] loss: 0.555
[11,    32] loss: 0.550
[12,    32] loss: 0.560
[13,    32] loss: 0.567
[14,    32] loss: 0.552
[15,    32] loss: 0.563
[16,    32] loss: 0.560
[17,    32] loss: 0.561
[18,    32] loss: 0.559
[19,    32] loss: 0.559
[20,    32] loss: 0.572
[21,    32] loss: 0.566
[22,    32] loss: 0.557
[23,    32] loss: 0.558
[24,    32] loss: 0.558
[25,    32] loss: 0.560
[26,    32] loss: 0.561
[27,    32] loss: 0.561
[28,    32] loss: 0.561
[29,    32] loss: 0.561
[30,    32] loss: 0.566
[31,    32] loss: 0.565
[32,    32] loss: 0.568
[33,    32] loss: 0.559
[34,    32] loss: 0.562
[35,    32] loss: 0.557
[36,    32] loss: 0.563
[37,    32] loss: 0.561
[38,    32] loss: 0.564
[39,    32] loss: 0.564
[40,    32] loss: 0.557
[41,    32] loss: 0.560
[42,    32] loss: 0.563
[43,    32] loss: 0.558
[44,    32] loss: 0.564
[45,    32] loss: 0.569
[46,    32] loss: 0.561
[47,    32] loss: 0.562
[48,    32] loss: 0.565
[49,    32] loss: 0.564
[50,    32] loss: 0.549
Early stopping applied (best metric=0.4761071503162384)
Finished Training
Total time taken: 119.7663209438324
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.613
[3,    32] loss: 0.598
[4,    32] loss: 0.584
[5,    32] loss: 0.585
[6,    32] loss: 0.574
[7,    32] loss: 0.568
[8,    32] loss: 0.565
[9,    32] loss: 0.568
[10,    32] loss: 0.567
[11,    32] loss: 0.559
[12,    32] loss: 0.563
[13,    32] loss: 0.573
[14,    32] loss: 0.563
[15,    32] loss: 0.566
[16,    32] loss: 0.563
[17,    32] loss: 0.561
[18,    32] loss: 0.568
[19,    32] loss: 0.574
[20,    32] loss: 0.560
[21,    32] loss: 0.567
[22,    32] loss: 0.566
[23,    32] loss: 0.570
[24,    32] loss: 0.570
[25,    32] loss: 0.559
[26,    32] loss: 0.577
[27,    32] loss: 0.555
[28,    32] loss: 0.560
[29,    32] loss: 0.565
[30,    32] loss: 0.562
[31,    32] loss: 0.564
[32,    32] loss: 0.569
[33,    32] loss: 0.564
[34,    32] loss: 0.564
[35,    32] loss: 0.565
[36,    32] loss: 0.570
[37,    32] loss: 0.566
[38,    32] loss: 0.562
[39,    32] loss: 0.554
[40,    32] loss: 0.573
[41,    32] loss: 0.569
[42,    32] loss: 0.567
Early stopping applied (best metric=0.4625169038772583)
Finished Training
Total time taken: 100.41927218437195
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.623
[3,    32] loss: 0.600
[4,    32] loss: 0.588
[5,    32] loss: 0.576
[6,    32] loss: 0.575
[7,    32] loss: 0.571
[8,    32] loss: 0.568
[9,    32] loss: 0.570
[10,    32] loss: 0.569
[11,    32] loss: 0.568
[12,    32] loss: 0.565
[13,    32] loss: 0.563
[14,    32] loss: 0.569
[15,    32] loss: 0.563
[16,    32] loss: 0.556
[17,    32] loss: 0.562
[18,    32] loss: 0.569
[19,    32] loss: 0.574
[20,    32] loss: 0.559
[21,    32] loss: 0.555
[22,    32] loss: 0.564
[23,    32] loss: 0.560
[24,    32] loss: 0.560
[25,    32] loss: 0.567
[26,    32] loss: 0.560
[27,    32] loss: 0.566
[28,    32] loss: 0.563
[29,    32] loss: 0.572
[30,    32] loss: 0.560
[31,    32] loss: 0.566
[32,    32] loss: 0.567
[33,    32] loss: 0.567
[34,    32] loss: 0.571
[35,    32] loss: 0.562
[36,    32] loss: 0.559
[37,    32] loss: 0.562
[38,    32] loss: 0.557
[39,    32] loss: 0.560
[40,    32] loss: 0.563
[41,    32] loss: 0.561
[42,    32] loss: 0.573
[43,    32] loss: 0.562
[44,    32] loss: 0.563
[45,    32] loss: 0.572
[46,    32] loss: 0.556
[47,    32] loss: 0.554
[48,    32] loss: 0.574
[49,    32] loss: 0.557
[50,    32] loss: 0.563
[51,    32] loss: 0.564
[52,    32] loss: 0.575
[53,    32] loss: 0.564
[54,    32] loss: 0.561
[55,    32] loss: 0.563
[56,    32] loss: 0.557
[57,    32] loss: 0.563
[58,    32] loss: 0.574
[59,    32] loss: 0.557
[60,    32] loss: 0.571
[61,    32] loss: 0.555
Early stopping applied (best metric=0.4754171073436737)
Finished Training
Total time taken: 146.07739400863647
{'S-palmitoylation-C Validation Accuracy': 0.6851827066674229, 'S-palmitoylation-C Validation Sensitivity': 0.6724752475247525, 'S-palmitoylation-C Validation Specificity': 0.68836393385261, 'S-palmitoylation-C Validation Precision': 0.35463313158955095, 'S-palmitoylation-C AUC ROC': 0.7555220626584109, 'S-palmitoylation-C AUC PR': 0.45401634539522046, 'S-palmitoylation-C MCC': 0.29931500163181624, 'S-palmitoylation-C F1': 0.46238194191372056, 'Validation Loss (S-palmitoylation-C)': 0.46998517513275145, 'Validation Loss (total)': 0.46998517513275145, 'TimeToTrain': 115.88451228141784}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038568206777016964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 207277132,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.79687351032109}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.617
[3,    32] loss: 0.610
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028535945120018963,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2049640410,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.324779497984147}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.605
[3,    32] loss: 0.595
[4,    32] loss: 0.578
[5,    32] loss: 0.578
[6,    32] loss: 0.569
[7,    32] loss: 0.572
[8,    32] loss: 0.565
[9,    32] loss: 0.576
[10,    32] loss: 0.573
[11,    32] loss: 0.563
[12,    32] loss: 0.564
[13,    32] loss: 0.572
[14,    32] loss: 0.558
[15,    32] loss: 0.567
[16,    32] loss: 0.565
[17,    32] loss: 0.559
[18,    32] loss: 0.574
[19,    32] loss: 0.562
[20,    32] loss: 0.564
[21,    32] loss: 0.571
[22,    32] loss: 0.572
[23,    32] loss: 0.570
[24,    32] loss: 0.566
[25,    32] loss: 0.562
[26,    32] loss: 0.567
[27,    32] loss: 0.566
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008151861485062004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4214578844,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.244608638363935}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.666
[3,    32] loss: 0.655
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017508584759888415,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 176385265,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.53453332436598}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.620
[3,    32] loss: 0.595
[4,    32] loss: 0.586
[5,    32] loss: 0.577
[6,    32] loss: 0.577
[7,    32] loss: 0.579
[8,    32] loss: 0.572
[9,    32] loss: 0.574
[10,    32] loss: 0.581
[11,    32] loss: 0.576
[12,    32] loss: 0.570
[13,    32] loss: 0.571
[14,    32] loss: 0.568
[15,    32] loss: 0.576
[16,    32] loss: 0.573
[17,    32] loss: 0.576
[18,    32] loss: 0.572
[19,    32] loss: 0.576
[20,    32] loss: 0.565
[21,    32] loss: 0.579
[22,    32] loss: 0.585
[23,    32] loss: 0.572
[24,    32] loss: 0.580
[25,    32] loss: 0.566
[26,    32] loss: 0.570
[27,    32] loss: 0.568
[28,    32] loss: 0.576
[29,    32] loss: 0.578
[30,    32] loss: 0.577
[31,    32] loss: 0.577
[32,    32] loss: 0.583
[33,    32] loss: 0.563
[34,    32] loss: 0.572
[35,    32] loss: 0.571
[36,    32] loss: 0.574
[37,    32] loss: 0.573
[38,    32] loss: 0.572
[39,    32] loss: 0.574
[40,    32] loss: 0.575
[41,    32] loss: 0.571
Early stopping applied (best metric=0.4676796495914459)
Finished Training
Total time taken: 97.94826483726501
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.619
[3,    32] loss: 0.605
[4,    32] loss: 0.607
[5,    32] loss: 0.591
[6,    32] loss: 0.578
[7,    32] loss: 0.572
[8,    32] loss: 0.590
[9,    32] loss: 0.581
[10,    32] loss: 0.578
[11,    32] loss: 0.578
[12,    32] loss: 0.578
[13,    32] loss: 0.568
[14,    32] loss: 0.587
[15,    32] loss: 0.573
[16,    32] loss: 0.581
[17,    32] loss: 0.590
[18,    32] loss: 0.579
[19,    32] loss: 0.568
[20,    32] loss: 0.577
[21,    32] loss: 0.571
[22,    32] loss: 0.574
[23,    32] loss: 0.570
[24,    32] loss: 0.572
[25,    32] loss: 0.574
[26,    32] loss: 0.574
[27,    32] loss: 0.577
[28,    32] loss: 0.573
[29,    32] loss: 0.580
[30,    32] loss: 0.578
[31,    32] loss: 0.578
[32,    32] loss: 0.573
[33,    32] loss: 0.580
[34,    32] loss: 0.572
[35,    32] loss: 0.573
[36,    32] loss: 0.579
[37,    32] loss: 0.576
[38,    32] loss: 0.574
[39,    32] loss: 0.578
[40,    32] loss: 0.570
[41,    32] loss: 0.574
[42,    32] loss: 0.578
[43,    32] loss: 0.567
[44,    32] loss: 0.579
[45,    32] loss: 0.569
[46,    32] loss: 0.571
Early stopping applied (best metric=0.4798770546913147)
Finished Training
Total time taken: 110.1232979297638
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.624
[3,    32] loss: 0.606
[4,    32] loss: 0.600
[5,    32] loss: 0.586
[6,    32] loss: 0.587
[7,    32] loss: 0.574
[8,    32] loss: 0.588
[9,    32] loss: 0.579
[10,    32] loss: 0.576
[11,    32] loss: 0.580
[12,    32] loss: 0.574
[13,    32] loss: 0.572
[14,    32] loss: 0.576
[15,    32] loss: 0.578
[16,    32] loss: 0.582
[17,    32] loss: 0.578
[18,    32] loss: 0.582
[19,    32] loss: 0.581
[20,    32] loss: 0.579
[21,    32] loss: 0.574
[22,    32] loss: 0.572
[23,    32] loss: 0.576
[24,    32] loss: 0.574
[25,    32] loss: 0.573
[26,    32] loss: 0.578
[27,    32] loss: 0.575
[28,    32] loss: 0.578
[29,    32] loss: 0.572
[30,    32] loss: 0.583
[31,    32] loss: 0.568
[32,    32] loss: 0.579
[33,    32] loss: 0.577
[34,    32] loss: 0.582
[35,    32] loss: 0.585
[36,    32] loss: 0.578
[37,    32] loss: 0.573
[38,    32] loss: 0.572
[39,    32] loss: 0.577
[40,    32] loss: 0.576
Early stopping applied (best metric=0.46735405921936035)
Finished Training
Total time taken: 95.83825659751892
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.615
[3,    32] loss: 0.601
[4,    32] loss: 0.601
[5,    32] loss: 0.582
[6,    32] loss: 0.579
[7,    32] loss: 0.579
[8,    32] loss: 0.573
[9,    32] loss: 0.587
[10,    32] loss: 0.574
[11,    32] loss: 0.582
[12,    32] loss: 0.576
[13,    32] loss: 0.581
[14,    32] loss: 0.569
[15,    32] loss: 0.576
[16,    32] loss: 0.572
[17,    32] loss: 0.584
[18,    32] loss: 0.571
[19,    32] loss: 0.580
[20,    32] loss: 0.568
[21,    32] loss: 0.580
[22,    32] loss: 0.570
[23,    32] loss: 0.587
[24,    32] loss: 0.587
[25,    32] loss: 0.574
[26,    32] loss: 0.571
[27,    32] loss: 0.578
[28,    32] loss: 0.570
[29,    32] loss: 0.574
[30,    32] loss: 0.564
[31,    32] loss: 0.583
[32,    32] loss: 0.574
[33,    32] loss: 0.575
[34,    32] loss: 0.575
[35,    32] loss: 0.577
[36,    32] loss: 0.566
[37,    32] loss: 0.573
[38,    32] loss: 0.579
[39,    32] loss: 0.582
[40,    32] loss: 0.575
[41,    32] loss: 0.569
[42,    32] loss: 0.580
[43,    32] loss: 0.582
[44,    32] loss: 0.571
[45,    32] loss: 0.577
[46,    32] loss: 0.576
[47,    32] loss: 0.574
[48,    32] loss: 0.567
[49,    32] loss: 0.575
[50,    32] loss: 0.584
[51,    32] loss: 0.570
[52,    32] loss: 0.572
[53,    32] loss: 0.579
[54,    32] loss: 0.570
[55,    32] loss: 0.575
[56,    32] loss: 0.567
[57,    32] loss: 0.575
[58,    32] loss: 0.574
[59,    32] loss: 0.570
[60,    32] loss: 0.569
[61,    32] loss: 0.571
[62,    32] loss: 0.577
[63,    32] loss: 0.576
[64,    32] loss: 0.573
[65,    32] loss: 0.577
[66,    32] loss: 0.583
[67,    32] loss: 0.572
[68,    32] loss: 0.578
[69,    32] loss: 0.576
[70,    32] loss: 0.577
[71,    32] loss: 0.569
[72,    32] loss: 0.575
[73,    32] loss: 0.573
[74,    32] loss: 0.579
[75,    32] loss: 0.565
[76,    32] loss: 0.586
[77,    32] loss: 0.578
[78,    32] loss: 0.583
[79,    32] loss: 0.572
[80,    32] loss: 0.580
[81,    32] loss: 0.567
[82,    32] loss: 0.582
[83,    32] loss: 0.566
[84,    32] loss: 0.579
Early stopping applied (best metric=0.47108548879623413)
Finished Training
Total time taken: 200.89354181289673
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.617
[3,    32] loss: 0.601
[4,    32] loss: 0.594
[5,    32] loss: 0.585
[6,    32] loss: 0.577
[7,    32] loss: 0.584
[8,    32] loss: 0.574
[9,    32] loss: 0.565
[10,    32] loss: 0.578
[11,    32] loss: 0.567
[12,    32] loss: 0.587
[13,    32] loss: 0.575
[14,    32] loss: 0.571
[15,    32] loss: 0.572
[16,    32] loss: 0.576
[17,    32] loss: 0.569
[18,    32] loss: 0.574
[19,    32] loss: 0.578
[20,    32] loss: 0.574
[21,    32] loss: 0.564
[22,    32] loss: 0.576
[23,    32] loss: 0.579
[24,    32] loss: 0.569
[25,    32] loss: 0.566
[26,    32] loss: 0.579
[27,    32] loss: 0.576
[28,    32] loss: 0.576
[29,    32] loss: 0.581
[30,    32] loss: 0.571
[31,    32] loss: 0.567
[32,    32] loss: 0.573
[33,    32] loss: 0.577
[34,    32] loss: 0.577
[35,    32] loss: 0.582
[36,    32] loss: 0.578
[37,    32] loss: 0.576
[38,    32] loss: 0.576
[39,    32] loss: 0.576
[40,    32] loss: 0.578
Early stopping applied (best metric=0.4782051146030426)
Finished Training
Total time taken: 95.7462546825409
{'S-palmitoylation-C Validation Accuracy': 0.6997927812119952, 'S-palmitoylation-C Validation Sensitivity': 0.6665346534653465, 'S-palmitoylation-C Validation Specificity': 0.7081285591430705, 'S-palmitoylation-C Validation Precision': 0.3647482437776893, 'S-palmitoylation-C AUC ROC': 0.7559164685127602, 'S-palmitoylation-C AUC PR': 0.4503841300997983, 'S-palmitoylation-C MCC': 0.31174466223325314, 'S-palmitoylation-C F1': 0.4707174808582936, 'Validation Loss (S-palmitoylation-C)': 0.47284027338027956, 'Validation Loss (total)': 0.47284027338027956, 'TimeToTrain': 120.10992317199707}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002175454205615105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2778426154,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.859553721205229}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.672
[3,    32] loss: 0.637
[4,    32] loss: 0.619
[5,    32] loss: 0.609
[6,    32] loss: 0.603
[7,    32] loss: 0.594
[8,    32] loss: 0.581
[9,    32] loss: 0.574
[10,    32] loss: 0.563
[11,    32] loss: 0.550
[12,    32] loss: 0.538
[13,    32] loss: 0.528
[14,    32] loss: 0.516
[15,    32] loss: 0.508
[16,    32] loss: 0.492
[17,    32] loss: 0.483
[18,    32] loss: 0.486
[19,    32] loss: 0.471
[20,    32] loss: 0.461
[21,    32] loss: 0.457
[22,    32] loss: 0.444
[23,    32] loss: 0.443
[24,    32] loss: 0.434
[25,    32] loss: 0.427
[26,    32] loss: 0.434
[27,    32] loss: 0.412
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016686227530247509,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2487690854,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.827419736747568}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.610
[3,    32] loss: 0.598
[4,    32] loss: 0.572
[5,    32] loss: 0.566
[6,    32] loss: 0.556
[7,    32] loss: 0.557
[8,    32] loss: 0.552
[9,    32] loss: 0.546
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024427315022972425,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1004914900,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.13221469446895}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.631
[3,    32] loss: 0.616
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018318159240062533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1846171039,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.11866101957663}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.618
[3,    32] loss: 0.601
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 3.189708743457398e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3154211932,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.975538412407348}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.693
[3,    32] loss: 0.692
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004885084440292905,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 301023665,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.649691192776093}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.639
[3,    32] loss: 0.636
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023297023826288257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2387897402,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.418794044311806}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.611
[3,    32] loss: 0.592
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0042515686032169025,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1744751643,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.285377941476125}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.627
[3,    32] loss: 0.609
[4,    32] loss: 0.597
[5,    32] loss: 0.595
[6,    32] loss: 0.587
[7,    32] loss: 0.591
[8,    32] loss: 0.589
[9,    32] loss: 0.602
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00012501232244743957,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1457670841,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.12781169157690808}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.688
[3,    32] loss: 0.669
[4,    32] loss: 0.637
[5,    32] loss: 0.626
[6,    32] loss: 0.613
[7,    32] loss: 0.604
[8,    32] loss: 0.605
[9,    32] loss: 0.601
[10,    32] loss: 0.590
[11,    32] loss: 0.585
[12,    32] loss: 0.578
[13,    32] loss: 0.574
[14,    32] loss: 0.568
[15,    32] loss: 0.563
[16,    32] loss: 0.551
[17,    32] loss: 0.537
[18,    32] loss: 0.536
[19,    32] loss: 0.526
[20,    32] loss: 0.507
[21,    32] loss: 0.505
[22,    32] loss: 0.489
[23,    32] loss: 0.479
[24,    32] loss: 0.481
[25,    32] loss: 0.470
[26,    32] loss: 0.456
[27,    32] loss: 0.441
[28,    32] loss: 0.443
[29,    32] loss: 0.429
[30,    32] loss: 0.419
[31,    32] loss: 0.412
[32,    32] loss: 0.409
[33,    32] loss: 0.409
[34,    32] loss: 0.396
[35,    32] loss: 0.391
[36,    32] loss: 0.389
[37,    32] loss: 0.385
[38,    32] loss: 0.378
[39,    32] loss: 0.365
[40,    32] loss: 0.368
[41,    32] loss: 0.355
Early stopping applied (best metric=0.4668025076389313)
Finished Training
Total time taken: 98.02426147460938
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.685
[3,    32] loss: 0.661
[4,    32] loss: 0.636
[5,    32] loss: 0.623
[6,    32] loss: 0.611
[7,    32] loss: 0.614
[8,    32] loss: 0.599
[9,    32] loss: 0.594
[10,    32] loss: 0.590
[11,    32] loss: 0.585
[12,    32] loss: 0.571
[13,    32] loss: 0.570
[14,    32] loss: 0.556
[15,    32] loss: 0.559
[16,    32] loss: 0.543
[17,    32] loss: 0.538
[18,    32] loss: 0.529
[19,    32] loss: 0.520
[20,    32] loss: 0.512
[21,    32] loss: 0.500
[22,    32] loss: 0.493
[23,    32] loss: 0.478
[24,    32] loss: 0.482
[25,    32] loss: 0.469
[26,    32] loss: 0.451
[27,    32] loss: 0.446
[28,    32] loss: 0.442
[29,    32] loss: 0.429
[30,    32] loss: 0.428
[31,    32] loss: 0.419
[32,    32] loss: 0.413
[33,    32] loss: 0.406
[34,    32] loss: 0.393
[35,    32] loss: 0.384
[36,    32] loss: 0.387
[37,    32] loss: 0.378
[38,    32] loss: 0.377
[39,    32] loss: 0.370
[40,    32] loss: 0.358
[41,    32] loss: 0.364
[42,    32] loss: 0.352
Early stopping applied (best metric=0.46111229062080383)
Finished Training
Total time taken: 100.41827130317688
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.687
[3,    32] loss: 0.663
[4,    32] loss: 0.634
[5,    32] loss: 0.618
[6,    32] loss: 0.610
[7,    32] loss: 0.609
[8,    32] loss: 0.602
[9,    32] loss: 0.588
[10,    32] loss: 0.593
[11,    32] loss: 0.586
[12,    32] loss: 0.570
[13,    32] loss: 0.573
[14,    32] loss: 0.565
[15,    32] loss: 0.561
[16,    32] loss: 0.549
[17,    32] loss: 0.540
[18,    32] loss: 0.535
[19,    32] loss: 0.528
[20,    32] loss: 0.515
[21,    32] loss: 0.514
[22,    32] loss: 0.494
[23,    32] loss: 0.489
[24,    32] loss: 0.487
[25,    32] loss: 0.478
[26,    32] loss: 0.471
[27,    32] loss: 0.456
[28,    32] loss: 0.452
[29,    32] loss: 0.442
[30,    32] loss: 0.417
[31,    32] loss: 0.431
[32,    32] loss: 0.421
[33,    32] loss: 0.406
[34,    32] loss: 0.404
[35,    32] loss: 0.393
[36,    32] loss: 0.380
[37,    32] loss: 0.402
[38,    32] loss: 0.379
[39,    32] loss: 0.377
[40,    32] loss: 0.358
[41,    32] loss: 0.363
[42,    32] loss: 0.354
[43,    32] loss: 0.354
[44,    32] loss: 0.355
[45,    32] loss: 0.346
[46,    32] loss: 0.345
Early stopping applied (best metric=0.4800218343734741)
Finished Training
Total time taken: 110.12829518318176
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.687
[3,    32] loss: 0.665
[4,    32] loss: 0.638
[5,    32] loss: 0.617
[6,    32] loss: 0.614
[7,    32] loss: 0.600
[8,    32] loss: 0.602
[9,    32] loss: 0.597
[10,    32] loss: 0.586
[11,    32] loss: 0.584
[12,    32] loss: 0.574
[13,    32] loss: 0.569
[14,    32] loss: 0.557
[15,    32] loss: 0.547
[16,    32] loss: 0.550
[17,    32] loss: 0.543
[18,    32] loss: 0.534
[19,    32] loss: 0.519
[20,    32] loss: 0.522
[21,    32] loss: 0.506
[22,    32] loss: 0.494
[23,    32] loss: 0.492
[24,    32] loss: 0.481
[25,    32] loss: 0.473
[26,    32] loss: 0.451
[27,    32] loss: 0.440
[28,    32] loss: 0.443
[29,    32] loss: 0.451
[30,    32] loss: 0.428
[31,    32] loss: 0.421
[32,    32] loss: 0.423
[33,    32] loss: 0.403
[34,    32] loss: 0.393
[35,    32] loss: 0.394
[36,    32] loss: 0.378
[37,    32] loss: 0.390
[38,    32] loss: 0.383
[39,    32] loss: 0.368
[40,    32] loss: 0.364
[41,    32] loss: 0.366
Early stopping applied (best metric=0.478363573551178)
Finished Training
Total time taken: 97.90026354789734
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.686
[3,    32] loss: 0.663
[4,    32] loss: 0.636
[5,    32] loss: 0.620
[6,    32] loss: 0.617
[7,    32] loss: 0.608
[8,    32] loss: 0.607
[9,    32] loss: 0.596
[10,    32] loss: 0.593
[11,    32] loss: 0.585
[12,    32] loss: 0.583
[13,    32] loss: 0.584
[14,    32] loss: 0.576
[15,    32] loss: 0.568
[16,    32] loss: 0.559
[17,    32] loss: 0.545
[18,    32] loss: 0.542
[19,    32] loss: 0.532
[20,    32] loss: 0.526
[21,    32] loss: 0.514
[22,    32] loss: 0.508
[23,    32] loss: 0.498
[24,    32] loss: 0.489
[25,    32] loss: 0.480
[26,    32] loss: 0.471
[27,    32] loss: 0.465
[28,    32] loss: 0.451
[29,    32] loss: 0.444
[30,    32] loss: 0.436
[31,    32] loss: 0.422
[32,    32] loss: 0.419
[33,    32] loss: 0.418
[34,    32] loss: 0.404
[35,    32] loss: 0.403
[36,    32] loss: 0.396
[37,    32] loss: 0.387
[38,    32] loss: 0.378
[39,    32] loss: 0.378
[40,    32] loss: 0.366
[41,    32] loss: 0.365
[42,    32] loss: 0.361
[43,    32] loss: 0.351
Early stopping applied (best metric=0.47215694189071655)
Finished Training
Total time taken: 103.21027994155884
{'S-palmitoylation-C Validation Accuracy': 0.66732465011941, 'S-palmitoylation-C Validation Sensitivity': 0.7029702970297029, 'S-palmitoylation-C Validation Specificity': 0.6583867764359164, 'S-palmitoylation-C Validation Precision': 0.34527889260995476, 'S-palmitoylation-C AUC ROC': 0.7552752274505458, 'S-palmitoylation-C AUC PR': 0.4523936631430236, 'S-palmitoylation-C MCC': 0.29754935913049274, 'S-palmitoylation-C F1': 0.4590477424831559, 'Validation Loss (S-palmitoylation-C)': 0.47169142961502075, 'Validation Loss (total)': 0.47169142961502075, 'TimeToTrain': 101.93627429008484}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012650520996837764,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3639706782,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.277173274370828}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.618
[3,    32] loss: 0.596
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00039912635982591996,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 570122140,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9675857340639444}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.646
[3,    32] loss: 0.616
[4,    32] loss: 0.604
[5,    32] loss: 0.594
[6,    32] loss: 0.578
[7,    32] loss: 0.564
[8,    32] loss: 0.560
[9,    32] loss: 0.543
[10,    32] loss: 0.519
[11,    32] loss: 0.497
[12,    32] loss: 0.497
[13,    32] loss: 0.469
[14,    32] loss: 0.458
[15,    32] loss: 0.445
[16,    32] loss: 0.435
[17,    32] loss: 0.429
[18,    32] loss: 0.415
[19,    32] loss: 0.395
[20,    32] loss: 0.390
[21,    32] loss: 0.378
[22,    32] loss: 0.377
[23,    32] loss: 0.365
[24,    32] loss: 0.361
[25,    32] loss: 0.353
[26,    32] loss: 0.353
[27,    32] loss: 0.340
[28,    32] loss: 0.345
[29,    32] loss: 0.328
[30,    32] loss: 0.329
[31,    32] loss: 0.314
[32,    32] loss: 0.320
[33,    32] loss: 0.320
[34,    32] loss: 0.308
[35,    32] loss: 0.304
[36,    32] loss: 0.300
[37,    32] loss: 0.287
[38,    32] loss: 0.292
Early stopping applied (best metric=0.4615597128868103)
Finished Training
Total time taken: 90.68924355506897
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.638
[3,    32] loss: 0.618
[4,    32] loss: 0.603
[5,    32] loss: 0.586
[6,    32] loss: 0.573
[7,    32] loss: 0.553
[8,    32] loss: 0.539
[9,    32] loss: 0.525
[10,    32] loss: 0.511
[11,    32] loss: 0.497
[12,    32] loss: 0.482
[13,    32] loss: 0.461
[14,    32] loss: 0.451
[15,    32] loss: 0.442
[16,    32] loss: 0.428
[17,    32] loss: 0.420
[18,    32] loss: 0.405
[19,    32] loss: 0.392
[20,    32] loss: 0.388
[21,    32] loss: 0.375
[22,    32] loss: 0.376
[23,    32] loss: 0.364
[24,    32] loss: 0.348
[25,    32] loss: 0.351
[26,    32] loss: 0.350
[27,    32] loss: 0.328
[28,    32] loss: 0.325
[29,    32] loss: 0.333
[30,    32] loss: 0.331
[31,    32] loss: 0.316
[32,    32] loss: 0.308
[33,    32] loss: 0.310
Early stopping applied (best metric=0.4875566363334656)
Finished Training
Total time taken: 78.9832136631012
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.639
[3,    32] loss: 0.618
[4,    32] loss: 0.604
[5,    32] loss: 0.588
[6,    32] loss: 0.581
[7,    32] loss: 0.561
[8,    32] loss: 0.543
[9,    32] loss: 0.522
[10,    32] loss: 0.513
[11,    32] loss: 0.489
[12,    32] loss: 0.478
[13,    32] loss: 0.465
[14,    32] loss: 0.440
[15,    32] loss: 0.436
[16,    32] loss: 0.425
[17,    32] loss: 0.415
[18,    32] loss: 0.392
[19,    32] loss: 0.381
[20,    32] loss: 0.382
[21,    32] loss: 0.374
[22,    32] loss: 0.366
[23,    32] loss: 0.357
[24,    32] loss: 0.351
[25,    32] loss: 0.347
[26,    32] loss: 0.337
[27,    32] loss: 0.325
[28,    32] loss: 0.326
[29,    32] loss: 0.328
[30,    32] loss: 0.318
[31,    32] loss: 0.312
[32,    32] loss: 0.310
[33,    32] loss: 0.297
[34,    32] loss: 0.302
Early stopping applied (best metric=0.47957542538642883)
Finished Training
Total time taken: 81.39421892166138
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.646
[3,    32] loss: 0.618
[4,    32] loss: 0.603
[5,    32] loss: 0.589
[6,    32] loss: 0.570
[7,    32] loss: 0.558
[8,    32] loss: 0.532
[9,    32] loss: 0.523
[10,    32] loss: 0.500
[11,    32] loss: 0.492
[12,    32] loss: 0.470
[13,    32] loss: 0.459
[14,    32] loss: 0.441
[15,    32] loss: 0.425
[16,    32] loss: 0.413
[17,    32] loss: 0.397
[18,    32] loss: 0.393
[19,    32] loss: 0.382
[20,    32] loss: 0.381
[21,    32] loss: 0.368
[22,    32] loss: 0.361
[23,    32] loss: 0.347
[24,    32] loss: 0.344
[25,    32] loss: 0.345
[26,    32] loss: 0.335
[27,    32] loss: 0.330
[28,    32] loss: 0.321
[29,    32] loss: 0.317
[30,    32] loss: 0.313
[31,    32] loss: 0.307
[32,    32] loss: 0.310
Early stopping applied (best metric=0.47812291979789734)
Finished Training
Total time taken: 76.6252076625824
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.643
[3,    32] loss: 0.615
[4,    32] loss: 0.604
[5,    32] loss: 0.590
[6,    32] loss: 0.580
[7,    32] loss: 0.568
[8,    32] loss: 0.556
[9,    32] loss: 0.537
[10,    32] loss: 0.526
[11,    32] loss: 0.500
[12,    32] loss: 0.497
[13,    32] loss: 0.474
[14,    32] loss: 0.454
[15,    32] loss: 0.444
[16,    32] loss: 0.428
[17,    32] loss: 0.424
[18,    32] loss: 0.407
[19,    32] loss: 0.407
[20,    32] loss: 0.396
[21,    32] loss: 0.389
[22,    32] loss: 0.365
[23,    32] loss: 0.374
[24,    32] loss: 0.367
[25,    32] loss: 0.350
[26,    32] loss: 0.345
[27,    32] loss: 0.344
[28,    32] loss: 0.337
[29,    32] loss: 0.330
[30,    32] loss: 0.329
[31,    32] loss: 0.325
[32,    32] loss: 0.306
[33,    32] loss: 0.321
[34,    32] loss: 0.314
[35,    32] loss: 0.302
[36,    32] loss: 0.294
Early stopping applied (best metric=0.4515489935874939)
Finished Training
Total time taken: 86.08522987365723
{'S-palmitoylation-C Validation Accuracy': 0.7066208560968387, 'S-palmitoylation-C Validation Sensitivity': 0.6312871287128713, 'S-palmitoylation-C Validation Specificity': 0.7255042986932663, 'S-palmitoylation-C Validation Precision': 0.3656999261455497, 'S-palmitoylation-C AUC ROC': 0.7544529707666802, 'S-palmitoylation-C AUC PR': 0.4581286942918514, 'S-palmitoylation-C MCC': 0.30030350701891334, 'S-palmitoylation-C F1': 0.4629996209933898, 'Validation Loss (S-palmitoylation-C)': 0.4716727375984192, 'Validation Loss (total)': 0.4716727375984192, 'TimeToTrain': 82.75542273521424}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010587851358325622,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3716746014,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.078749168790576}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.632
[3,    32] loss: 0.610
[4,    32] loss: 0.599
[5,    32] loss: 0.597
[6,    32] loss: 0.585
[7,    32] loss: 0.585
[8,    32] loss: 0.585
[9,    32] loss: 0.583
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030486744316301807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1888455736,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.207695650867419}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.611
[3,    32] loss: 0.588
[4,    32] loss: 0.588
[5,    32] loss: 0.580
[6,    32] loss: 0.575
[7,    32] loss: 0.577
[8,    32] loss: 0.565
[9,    32] loss: 0.573
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043689896960947576,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2588137111,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.468748121632265}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.617
[3,    32] loss: 0.591
[4,    32] loss: 0.598
[5,    32] loss: 0.596
[6,    32] loss: 0.586
[7,    32] loss: 0.588
[8,    32] loss: 0.584
[9,    32] loss: 0.585
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002216887088219304,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1551755523,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.331524452250821}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.615
[3,    32] loss: 0.603
[4,    32] loss: 0.581
[5,    32] loss: 0.569
[6,    32] loss: 0.568
[7,    32] loss: 0.560
[8,    32] loss: 0.558
[9,    32] loss: 0.565
[10,    32] loss: 0.556
[11,    32] loss: 0.550
[12,    32] loss: 0.553
[13,    32] loss: 0.554
[14,    32] loss: 0.558
[15,    32] loss: 0.552
[16,    32] loss: 0.551
[17,    32] loss: 0.546
[18,    32] loss: 0.545
[19,    32] loss: 0.551
[20,    32] loss: 0.544
[21,    32] loss: 0.548
[22,    32] loss: 0.540
[23,    32] loss: 0.564
[24,    32] loss: 0.562
[25,    32] loss: 0.553
[26,    32] loss: 0.556
[27,    32] loss: 0.546
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007886438619348441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1844871008,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.81338887480581}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.646
[3,    32] loss: 0.649
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002877613377158931,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3392749445,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.45430072794798193}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.618
[3,    32] loss: 0.599
[4,    32] loss: 0.582
[5,    32] loss: 0.571
[6,    32] loss: 0.550
[7,    32] loss: 0.540
[8,    32] loss: 0.522
[9,    32] loss: 0.522
[10,    32] loss: 0.510
[11,    32] loss: 0.487
[12,    32] loss: 0.477
[13,    32] loss: 0.470
[14,    32] loss: 0.467
[15,    32] loss: 0.449
[16,    32] loss: 0.451
[17,    32] loss: 0.436
[18,    32] loss: 0.436
[19,    32] loss: 0.429
[20,    32] loss: 0.427
[21,    32] loss: 0.403
[22,    32] loss: 0.407
[23,    32] loss: 0.408
[24,    32] loss: 0.394
[25,    32] loss: 0.399
[26,    32] loss: 0.378
[27,    32] loss: 0.388
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004015244294102433,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2334294830,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.736157530842059}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.626
[3,    32] loss: 0.616
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022396851052602024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 492693062,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.618005061644578}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.622
[3,    32] loss: 0.614
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001043101478515803,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3640537717,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.4894913933529452}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.619
[3,    32] loss: 0.603
[4,    32] loss: 0.580
[5,    32] loss: 0.560
[6,    32] loss: 0.539
[7,    32] loss: 0.519
[8,    32] loss: 0.506
[9,    32] loss: 0.486
[10,    32] loss: 0.481
[11,    32] loss: 0.460
[12,    32] loss: 0.442
[13,    32] loss: 0.434
[14,    32] loss: 0.432
[15,    32] loss: 0.417
[16,    32] loss: 0.408
[17,    32] loss: 0.406
[18,    32] loss: 0.410
[19,    32] loss: 0.388
[20,    32] loss: 0.381
[21,    32] loss: 0.382
[22,    32] loss: 0.381
[23,    32] loss: 0.371
[24,    32] loss: 0.362
[25,    32] loss: 0.365
[26,    32] loss: 0.367
[27,    32] loss: 0.358
[28,    32] loss: 0.347
[29,    32] loss: 0.348
[30,    32] loss: 0.346
[31,    32] loss: 0.357
[32,    32] loss: 0.341
[33,    32] loss: 0.343
[34,    32] loss: 0.346
[35,    32] loss: 0.327
[36,    32] loss: 0.331
Early stopping applied (best metric=0.46708327531814575)
Finished Training
Total time taken: 86.2472333908081
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.688
[2,    32] loss: 0.625
[3,    32] loss: 0.601
[4,    32] loss: 0.578
[5,    32] loss: 0.562
[6,    32] loss: 0.535
[7,    32] loss: 0.512
[8,    32] loss: 0.493
[9,    32] loss: 0.475
[10,    32] loss: 0.467
[11,    32] loss: 0.457
[12,    32] loss: 0.441
[13,    32] loss: 0.430
[14,    32] loss: 0.426
[15,    32] loss: 0.400
[16,    32] loss: 0.413
[17,    32] loss: 0.404
[18,    32] loss: 0.393
[19,    32] loss: 0.386
[20,    32] loss: 0.378
[21,    32] loss: 0.378
[22,    32] loss: 0.370
[23,    32] loss: 0.361
[24,    32] loss: 0.368
[25,    32] loss: 0.356
[26,    32] loss: 0.362
[27,    32] loss: 0.348
[28,    32] loss: 0.349
[29,    32] loss: 0.345
[30,    32] loss: 0.339
[31,    32] loss: 0.343
[32,    32] loss: 0.335
Early stopping applied (best metric=0.4717015027999878)
Finished Training
Total time taken: 76.69320678710938
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.622
[3,    32] loss: 0.594
[4,    32] loss: 0.578
[5,    32] loss: 0.556
[6,    32] loss: 0.534
[7,    32] loss: 0.515
[8,    32] loss: 0.497
[9,    32] loss: 0.488
[10,    32] loss: 0.469
[11,    32] loss: 0.444
[12,    32] loss: 0.440
[13,    32] loss: 0.433
[14,    32] loss: 0.428
[15,    32] loss: 0.407
[16,    32] loss: 0.404
[17,    32] loss: 0.402
[18,    32] loss: 0.381
[19,    32] loss: 0.382
[20,    32] loss: 0.389
[21,    32] loss: 0.375
[22,    32] loss: 0.374
[23,    32] loss: 0.357
[24,    32] loss: 0.355
[25,    32] loss: 0.349
[26,    32] loss: 0.353
[27,    32] loss: 0.343
[28,    32] loss: 0.338
[29,    32] loss: 0.349
[30,    32] loss: 0.339
[31,    32] loss: 0.336
Early stopping applied (best metric=0.4739021360874176)
Finished Training
Total time taken: 74.30219984054565
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.620
[3,    32] loss: 0.592
[4,    32] loss: 0.579
[5,    32] loss: 0.556
[6,    32] loss: 0.538
[7,    32] loss: 0.519
[8,    32] loss: 0.492
[9,    32] loss: 0.474
[10,    32] loss: 0.467
[11,    32] loss: 0.459
[12,    32] loss: 0.437
[13,    32] loss: 0.429
[14,    32] loss: 0.419
[15,    32] loss: 0.409
[16,    32] loss: 0.411
[17,    32] loss: 0.399
[18,    32] loss: 0.391
[19,    32] loss: 0.392
[20,    32] loss: 0.383
[21,    32] loss: 0.382
[22,    32] loss: 0.380
[23,    32] loss: 0.365
[24,    32] loss: 0.357
[25,    32] loss: 0.351
[26,    32] loss: 0.354
[27,    32] loss: 0.359
[28,    32] loss: 0.342
[29,    32] loss: 0.347
Early stopping applied (best metric=0.46859726309776306)
Finished Training
Total time taken: 69.47913432121277
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.628
[3,    32] loss: 0.592
[4,    32] loss: 0.584
[5,    32] loss: 0.554
[6,    32] loss: 0.549
[7,    32] loss: 0.518
[8,    32] loss: 0.493
[9,    32] loss: 0.489
[10,    32] loss: 0.472
[11,    32] loss: 0.461
[12,    32] loss: 0.452
[13,    32] loss: 0.439
[14,    32] loss: 0.418
[15,    32] loss: 0.419
[16,    32] loss: 0.407
[17,    32] loss: 0.401
[18,    32] loss: 0.392
[19,    32] loss: 0.392
[20,    32] loss: 0.383
[21,    32] loss: 0.378
[22,    32] loss: 0.370
[23,    32] loss: 0.370
[24,    32] loss: 0.361
[25,    32] loss: 0.358
[26,    32] loss: 0.347
[27,    32] loss: 0.353
[28,    32] loss: 0.344
[29,    32] loss: 0.349
[30,    32] loss: 0.347
Early stopping applied (best metric=0.46660155057907104)
Finished Training
Total time taken: 71.65610718727112
{'S-palmitoylation-C Validation Accuracy': 0.709318733183362, 'S-palmitoylation-C Validation Sensitivity': 0.64, 'S-palmitoylation-C Validation Specificity': 0.7266934929439334, 'S-palmitoylation-C Validation Precision': 0.3717911301538362, 'S-palmitoylation-C AUC ROC': 0.758167321928819, 'S-palmitoylation-C AUC PR': 0.4646217195665995, 'S-palmitoylation-C MCC': 0.30954859299176424, 'S-palmitoylation-C F1': 0.4693525608083594, 'Validation Loss (S-palmitoylation-C)': 0.46957714557647706, 'Validation Loss (total)': 0.46957714557647706, 'TimeToTrain': 75.6755763053894}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00981808329744612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2703786672,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.113167722103633}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012967745094051015,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1244400761,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.599980564766968}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.629
[3,    32] loss: 0.609
[4,    32] loss: 0.602
[5,    32] loss: 0.602
[6,    32] loss: 0.586
[7,    32] loss: 0.587
[8,    32] loss: 0.585
[9,    32] loss: 0.589
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001950391546608162,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 865413694,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.162114370353827}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.619
[3,    32] loss: 0.610
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001617288670951795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1499061700,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.458629226192881}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.615
[3,    32] loss: 0.596
[4,    32] loss: 0.572
[5,    32] loss: 0.572
[6,    32] loss: 0.553
[7,    32] loss: 0.554
[8,    32] loss: 0.544
[9,    32] loss: 0.550
[10,    32] loss: 0.545
[11,    32] loss: 0.532
[12,    32] loss: 0.538
[13,    32] loss: 0.550
[14,    32] loss: 0.532
[15,    32] loss: 0.537
[16,    32] loss: 0.537
[17,    32] loss: 0.532
[18,    32] loss: 0.535
[19,    32] loss: 0.539
[20,    32] loss: 0.538
[21,    32] loss: 0.533
[22,    32] loss: 0.531
[23,    32] loss: 0.537
[24,    32] loss: 0.526
[25,    32] loss: 0.522
[26,    32] loss: 0.532
[27,    32] loss: 0.535
[28,    32] loss: 0.528
[29,    32] loss: 0.528
[30,    32] loss: 0.538
[31,    32] loss: 0.522
[32,    32] loss: 0.525
[33,    32] loss: 0.542
[34,    32] loss: 0.531
[35,    32] loss: 0.528
[36,    32] loss: 0.528
[37,    32] loss: 0.526
[38,    32] loss: 0.524
[39,    32] loss: 0.533
[40,    32] loss: 0.527
[41,    32] loss: 0.540
[42,    32] loss: 0.526
[43,    32] loss: 0.532
[44,    32] loss: 0.539
[45,    32] loss: 0.530
[46,    32] loss: 0.525
[47,    32] loss: 0.525
[48,    32] loss: 0.539
[49,    32] loss: 0.526
[50,    32] loss: 0.528
[51,    32] loss: 0.536
[52,    32] loss: 0.525
[53,    32] loss: 0.530
[54,    32] loss: 0.525
[55,    32] loss: 0.526
[56,    32] loss: 0.528
[57,    32] loss: 0.529
[58,    32] loss: 0.524
[59,    32] loss: 0.527
[60,    32] loss: 0.526
[61,    32] loss: 0.533
[62,    32] loss: 0.530
[63,    32] loss: 0.535
Early stopping applied (best metric=0.47398459911346436)
Finished Training
Total time taken: 150.50122570991516
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.622
[3,    32] loss: 0.592
[4,    32] loss: 0.588
[5,    32] loss: 0.567
[6,    32] loss: 0.562
[7,    32] loss: 0.546
[8,    32] loss: 0.533
[9,    32] loss: 0.539
[10,    32] loss: 0.544
[11,    32] loss: 0.532
[12,    32] loss: 0.534
[13,    32] loss: 0.521
[14,    32] loss: 0.529
[15,    32] loss: 0.529
[16,    32] loss: 0.525
[17,    32] loss: 0.525
[18,    32] loss: 0.536
[19,    32] loss: 0.528
[20,    32] loss: 0.521
[21,    32] loss: 0.518
[22,    32] loss: 0.526
[23,    32] loss: 0.528
[24,    32] loss: 0.526
[25,    32] loss: 0.529
[26,    32] loss: 0.530
[27,    32] loss: 0.528
[28,    32] loss: 0.522
[29,    32] loss: 0.521
[30,    32] loss: 0.522
[31,    32] loss: 0.532
[32,    32] loss: 0.528
[33,    32] loss: 0.533
[34,    32] loss: 0.523
[35,    32] loss: 0.528
[36,    32] loss: 0.519
[37,    32] loss: 0.521
[38,    32] loss: 0.530
[39,    32] loss: 0.515
[40,    32] loss: 0.531
[41,    32] loss: 0.522
[42,    32] loss: 0.525
[43,    32] loss: 0.525
[44,    32] loss: 0.519
[45,    32] loss: 0.529
[46,    32] loss: 0.537
[47,    32] loss: 0.523
[48,    32] loss: 0.521
[49,    32] loss: 0.524
[50,    32] loss: 0.531
[51,    32] loss: 0.518
[52,    32] loss: 0.533
[53,    32] loss: 0.525
[54,    32] loss: 0.519
[55,    32] loss: 0.525
[56,    32] loss: 0.531
[57,    32] loss: 0.525
[58,    32] loss: 0.528
[59,    32] loss: 0.525
[60,    32] loss: 0.520
[61,    32] loss: 0.531
[62,    32] loss: 0.535
[63,    32] loss: 0.520
[64,    32] loss: 0.518
[65,    32] loss: 0.524
[66,    32] loss: 0.523
[67,    32] loss: 0.530
[68,    32] loss: 0.522
[69,    32] loss: 0.524
[70,    32] loss: 0.529
[71,    32] loss: 0.517
[72,    32] loss: 0.524
Early stopping applied (best metric=0.4686107933521271)
Finished Training
Total time taken: 172.09626078605652
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.624
[3,    32] loss: 0.594
[4,    32] loss: 0.576
[5,    32] loss: 0.565
[6,    32] loss: 0.547
[7,    32] loss: 0.540
[8,    32] loss: 0.539
[9,    32] loss: 0.537
[10,    32] loss: 0.540
[11,    32] loss: 0.532
[12,    32] loss: 0.534
[13,    32] loss: 0.521
[14,    32] loss: 0.532
[15,    32] loss: 0.523
[16,    32] loss: 0.525
[17,    32] loss: 0.537
[18,    32] loss: 0.530
[19,    32] loss: 0.525
[20,    32] loss: 0.518
[21,    32] loss: 0.544
[22,    32] loss: 0.525
[23,    32] loss: 0.521
[24,    32] loss: 0.528
[25,    32] loss: 0.530
[26,    32] loss: 0.525
[27,    32] loss: 0.540
[28,    32] loss: 0.531
[29,    32] loss: 0.525
[30,    32] loss: 0.529
Early stopping applied (best metric=0.4741504490375519)
Finished Training
Total time taken: 71.91911244392395
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.607
[3,    32] loss: 0.594
[4,    32] loss: 0.593
[5,    32] loss: 0.566
[6,    32] loss: 0.559
[7,    32] loss: 0.545
[8,    32] loss: 0.547
[9,    32] loss: 0.550
[10,    32] loss: 0.555
[11,    32] loss: 0.544
[12,    32] loss: 0.539
[13,    32] loss: 0.537
[14,    32] loss: 0.535
[15,    32] loss: 0.536
[16,    32] loss: 0.536
[17,    32] loss: 0.535
[18,    32] loss: 0.534
[19,    32] loss: 0.539
[20,    32] loss: 0.537
[21,    32] loss: 0.520
[22,    32] loss: 0.532
[23,    32] loss: 0.525
[24,    32] loss: 0.536
[25,    32] loss: 0.529
[26,    32] loss: 0.539
[27,    32] loss: 0.542
[28,    32] loss: 0.532
[29,    32] loss: 0.537
[30,    32] loss: 0.534
[31,    32] loss: 0.526
[32,    32] loss: 0.525
[33,    32] loss: 0.530
[34,    32] loss: 0.536
[35,    32] loss: 0.535
[36,    32] loss: 0.531
[37,    32] loss: 0.531
[38,    32] loss: 0.532
[39,    32] loss: 0.530
[40,    32] loss: 0.524
[41,    32] loss: 0.536
Early stopping applied (best metric=0.46643829345703125)
Finished Training
Total time taken: 98.08715558052063
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.614
[3,    32] loss: 0.597
[4,    32] loss: 0.576
[5,    32] loss: 0.562
[6,    32] loss: 0.559
[7,    32] loss: 0.556
[8,    32] loss: 0.544
[9,    32] loss: 0.539
[10,    32] loss: 0.538
[11,    32] loss: 0.537
[12,    32] loss: 0.538
[13,    32] loss: 0.531
[14,    32] loss: 0.537
[15,    32] loss: 0.526
[16,    32] loss: 0.527
[17,    32] loss: 0.530
[18,    32] loss: 0.534
[19,    32] loss: 0.524
[20,    32] loss: 0.533
[21,    32] loss: 0.524
[22,    32] loss: 0.535
[23,    32] loss: 0.532
[24,    32] loss: 0.524
[25,    32] loss: 0.522
[26,    32] loss: 0.527
[27,    32] loss: 0.529
[28,    32] loss: 0.522
[29,    32] loss: 0.533
[30,    32] loss: 0.536
[31,    32] loss: 0.521
[32,    32] loss: 0.526
[33,    32] loss: 0.525
[34,    32] loss: 0.530
[35,    32] loss: 0.529
[36,    32] loss: 0.536
[37,    32] loss: 0.518
[38,    32] loss: 0.535
[39,    32] loss: 0.535
[40,    32] loss: 0.526
Early stopping applied (best metric=0.46352070569992065)
Finished Training
Total time taken: 95.77916717529297
{'S-palmitoylation-C Validation Accuracy': 0.6908260710662457, 'S-palmitoylation-C Validation Sensitivity': 0.6922772277227722, 'S-palmitoylation-C Validation Specificity': 0.6904629380933959, 'S-palmitoylation-C Validation Precision': 0.3596468164761086, 'S-palmitoylation-C AUC ROC': 0.7593344965215484, 'S-palmitoylation-C AUC PR': 0.4619934932275909, 'S-palmitoylation-C MCC': 0.31491493201345677, 'S-palmitoylation-C F1': 0.473237756278191, 'Validation Loss (S-palmitoylation-C)': 0.46934096813201903, 'Validation Loss (total)': 0.46934096813201903, 'TimeToTrain': 117.67658433914184}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008883345169587985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 787214550,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.808723729015313}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.632
[3,    32] loss: 0.630
[4,    32] loss: 0.626
[5,    32] loss: 0.623
[6,    32] loss: 0.620
[7,    32] loss: 0.624
[8,    32] loss: 0.618
[9,    32] loss: 0.620
[10,    32] loss: 0.616
[11,    32] loss: 0.629
[12,    32] loss: 0.630
[13,    32] loss: 0.629
[14,    32] loss: 0.624
[15,    32] loss: 0.625
[16,    32] loss: 0.622
[17,    32] loss: 0.645
[18,    32] loss: 0.625
[19,    32] loss: 0.629
[20,    32] loss: 0.630
[21,    32] loss: 0.616
[22,    32] loss: 0.615
[23,    32] loss: 0.625
[24,    32] loss: 0.631
[25,    32] loss: 0.640
[26,    32] loss: 0.624
[27,    32] loss: 0.624
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013000826363691207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3736031031,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.293370969693834}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.622
[3,    32] loss: 0.602
[4,    32] loss: 0.589
[5,    32] loss: 0.593
[6,    32] loss: 0.570
[7,    32] loss: 0.576
[8,    32] loss: 0.572
[9,    32] loss: 0.554
[10,    32] loss: 0.557
[11,    32] loss: 0.572
[12,    32] loss: 0.556
[13,    32] loss: 0.560
[14,    32] loss: 0.560
[15,    32] loss: 0.561
[16,    32] loss: 0.565
[17,    32] loss: 0.554
[18,    32] loss: 0.563
[19,    32] loss: 0.557
[20,    32] loss: 0.558
[21,    32] loss: 0.559
[22,    32] loss: 0.553
[23,    32] loss: 0.548
[24,    32] loss: 0.555
[25,    32] loss: 0.555
[26,    32] loss: 0.569
[27,    32] loss: 0.567
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020225143796055423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2752325579,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.937401385326332}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.603
[3,    32] loss: 0.585
[4,    32] loss: 0.574
[5,    32] loss: 0.550
[6,    32] loss: 0.545
[7,    32] loss: 0.540
[8,    32] loss: 0.529
[9,    32] loss: 0.528
[10,    32] loss: 0.522
[11,    32] loss: 0.515
[12,    32] loss: 0.518
[13,    32] loss: 0.508
[14,    32] loss: 0.514
[15,    32] loss: 0.505
[16,    32] loss: 0.506
[17,    32] loss: 0.506
[18,    32] loss: 0.510
[19,    32] loss: 0.516
[20,    32] loss: 0.511
[21,    32] loss: 0.502
[22,    32] loss: 0.505
[23,    32] loss: 0.506
[24,    32] loss: 0.496
[25,    32] loss: 0.497
[26,    32] loss: 0.515
[27,    32] loss: 0.495
[28,    32] loss: 0.503
[29,    32] loss: 0.500
[30,    32] loss: 0.492
[31,    32] loss: 0.494
[32,    32] loss: 0.508
[33,    32] loss: 0.506
Early stopping applied (best metric=0.4748842418193817)
Finished Training
Total time taken: 78.85612654685974
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.610
[3,    32] loss: 0.587
[4,    32] loss: 0.573
[5,    32] loss: 0.557
[6,    32] loss: 0.550
[7,    32] loss: 0.529
[8,    32] loss: 0.527
[9,    32] loss: 0.520
[10,    32] loss: 0.511
[11,    32] loss: 0.516
[12,    32] loss: 0.511
[13,    32] loss: 0.503
[14,    32] loss: 0.502
[15,    32] loss: 0.513
[16,    32] loss: 0.505
[17,    32] loss: 0.499
[18,    32] loss: 0.512
[19,    32] loss: 0.509
[20,    32] loss: 0.505
[21,    32] loss: 0.504
[22,    32] loss: 0.500
[23,    32] loss: 0.503
[24,    32] loss: 0.497
[25,    32] loss: 0.507
[26,    32] loss: 0.507
[27,    32] loss: 0.495
[28,    32] loss: 0.497
[29,    32] loss: 0.503
[30,    32] loss: 0.503
[31,    32] loss: 0.497
[32,    32] loss: 0.495
[33,    32] loss: 0.499
[34,    32] loss: 0.502
[35,    32] loss: 0.497
[36,    32] loss: 0.502
[37,    32] loss: 0.504
[38,    32] loss: 0.506
[39,    32] loss: 0.511
[40,    32] loss: 0.495
[41,    32] loss: 0.507
[42,    32] loss: 0.491
[43,    32] loss: 0.510
[44,    32] loss: 0.504
[45,    32] loss: 0.496
[46,    32] loss: 0.505
[47,    32] loss: 0.502
[48,    32] loss: 0.501
[49,    32] loss: 0.506
[50,    32] loss: 0.499
[51,    32] loss: 0.505
[52,    32] loss: 0.497
[53,    32] loss: 0.504
[54,    32] loss: 0.505
[55,    32] loss: 0.523
[56,    32] loss: 0.492
[57,    32] loss: 0.494
[58,    32] loss: 0.503
[59,    32] loss: 0.506
[60,    32] loss: 0.503
[61,    32] loss: 0.497
[62,    32] loss: 0.504
[63,    32] loss: 0.496
[64,    32] loss: 0.508
[65,    32] loss: 0.501
[66,    32] loss: 0.510
[67,    32] loss: 0.501
[68,    32] loss: 0.522
[69,    32] loss: 0.502
Early stopping applied (best metric=0.4891650676727295)
Finished Training
Total time taken: 165.00626373291016
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.616
[3,    32] loss: 0.591
[4,    32] loss: 0.578
[5,    32] loss: 0.553
[6,    32] loss: 0.548
[7,    32] loss: 0.536
[8,    32] loss: 0.534
[9,    32] loss: 0.527
[10,    32] loss: 0.525
[11,    32] loss: 0.530
[12,    32] loss: 0.514
[13,    32] loss: 0.510
[14,    32] loss: 0.521
[15,    32] loss: 0.512
[16,    32] loss: 0.501
[17,    32] loss: 0.508
[18,    32] loss: 0.502
[19,    32] loss: 0.505
[20,    32] loss: 0.507
[21,    32] loss: 0.511
[22,    32] loss: 0.521
[23,    32] loss: 0.511
[24,    32] loss: 0.496
[25,    32] loss: 0.503
[26,    32] loss: 0.506
[27,    32] loss: 0.508
[28,    32] loss: 0.505
[29,    32] loss: 0.502
[30,    32] loss: 0.513
[31,    32] loss: 0.507
[32,    32] loss: 0.499
[33,    32] loss: 0.504
Early stopping applied (best metric=0.4694286584854126)
Finished Training
Total time taken: 78.8521249294281
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.612
[3,    32] loss: 0.584
[4,    32] loss: 0.581
[5,    32] loss: 0.570
[6,    32] loss: 0.557
[7,    32] loss: 0.548
[8,    32] loss: 0.537
[9,    32] loss: 0.534
[10,    32] loss: 0.533
[11,    32] loss: 0.535
[12,    32] loss: 0.526
[13,    32] loss: 0.528
[14,    32] loss: 0.509
[15,    32] loss: 0.519
[16,    32] loss: 0.518
[17,    32] loss: 0.514
[18,    32] loss: 0.503
[19,    32] loss: 0.511
[20,    32] loss: 0.498
[21,    32] loss: 0.515
[22,    32] loss: 0.510
[23,    32] loss: 0.512
[24,    32] loss: 0.514
[25,    32] loss: 0.514
[26,    32] loss: 0.506
[27,    32] loss: 0.506
[28,    32] loss: 0.507
[29,    32] loss: 0.501
[30,    32] loss: 0.501
[31,    32] loss: 0.520
[32,    32] loss: 0.505
[33,    32] loss: 0.510
Early stopping applied (best metric=0.4727077782154083)
Finished Training
Total time taken: 78.91912627220154
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.608
[3,    32] loss: 0.586
[4,    32] loss: 0.569
[5,    32] loss: 0.550
[6,    32] loss: 0.538
[7,    32] loss: 0.533
[8,    32] loss: 0.530
[9,    32] loss: 0.525
[10,    32] loss: 0.517
[11,    32] loss: 0.507
[12,    32] loss: 0.514
[13,    32] loss: 0.513
[14,    32] loss: 0.508
[15,    32] loss: 0.494
[16,    32] loss: 0.506
[17,    32] loss: 0.501
[18,    32] loss: 0.517
[19,    32] loss: 0.499
[20,    32] loss: 0.503
[21,    32] loss: 0.506
[22,    32] loss: 0.503
[23,    32] loss: 0.490
[24,    32] loss: 0.509
[25,    32] loss: 0.504
[26,    32] loss: 0.508
[27,    32] loss: 0.503
[28,    32] loss: 0.505
[29,    32] loss: 0.494
Early stopping applied (best metric=0.48998504877090454)
Finished Training
Total time taken: 69.51711082458496
{'S-palmitoylation-C Validation Accuracy': 0.6812937232587888, 'S-palmitoylation-C Validation Sensitivity': 0.666930693069307, 'S-palmitoylation-C Validation Specificity': 0.684891072665042, 'S-palmitoylation-C Validation Precision': 0.34905281324352627, 'S-palmitoylation-C AUC ROC': 0.7429444820097121, 'S-palmitoylation-C AUC PR': 0.44569158083319826, 'S-palmitoylation-C MCC': 0.29076853873524183, 'S-palmitoylation-C F1': 0.45726144857656753, 'Validation Loss (S-palmitoylation-C)': 0.47923415899276733, 'Validation Loss (total)': 0.47923415899276733, 'TimeToTrain': 94.2301504611969}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011098628943094332,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1925206219,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.537743975057388}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.630
[3,    32] loss: 0.609
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006189177412580168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2646180298,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.9747979601801013}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.644
[3,    32] loss: 0.615
[4,    32] loss: 0.601
[5,    32] loss: 0.586
[6,    32] loss: 0.581
[7,    32] loss: 0.571
[8,    32] loss: 0.559
[9,    32] loss: 0.560
[10,    32] loss: 0.557
[11,    32] loss: 0.555
[12,    32] loss: 0.559
[13,    32] loss: 0.544
[14,    32] loss: 0.554
[15,    32] loss: 0.536
[16,    32] loss: 0.545
[17,    32] loss: 0.544
[18,    32] loss: 0.537
[19,    32] loss: 0.538
[20,    32] loss: 0.552
[21,    32] loss: 0.532
[22,    32] loss: 0.539
[23,    32] loss: 0.544
[24,    32] loss: 0.532
[25,    32] loss: 0.532
[26,    32] loss: 0.533
[27,    32] loss: 0.536
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020570319697706643,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3521714281,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.328591357533542}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.614
[3,    32] loss: 0.591
[4,    32] loss: 0.569
[5,    32] loss: 0.565
[6,    32] loss: 0.546
[7,    32] loss: 0.543
[8,    32] loss: 0.539
[9,    32] loss: 0.526
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 5.549984053025372e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2215989878,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.024310484051608}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.691
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003785045307868251,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 536160075,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.87799738106127}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.646
[3,    32] loss: 0.624
[4,    32] loss: 0.617
[5,    32] loss: 0.608
[6,    32] loss: 0.598
[7,    32] loss: 0.593
[8,    32] loss: 0.588
[9,    32] loss: 0.584
[10,    32] loss: 0.580
[11,    32] loss: 0.570
[12,    32] loss: 0.570
[13,    32] loss: 0.560
[14,    32] loss: 0.569
[15,    32] loss: 0.562
[16,    32] loss: 0.554
[17,    32] loss: 0.559
[18,    32] loss: 0.563
[19,    32] loss: 0.560
[20,    32] loss: 0.549
[21,    32] loss: 0.556
[22,    32] loss: 0.554
[23,    32] loss: 0.552
[24,    32] loss: 0.549
[25,    32] loss: 0.553
[26,    32] loss: 0.548
[27,    32] loss: 0.544
[28,    32] loss: 0.556
[29,    32] loss: 0.556
[30,    32] loss: 0.555
[31,    32] loss: 0.552
[32,    32] loss: 0.554
[33,    32] loss: 0.549
[34,    32] loss: 0.550
[35,    32] loss: 0.554
[36,    32] loss: 0.556
Early stopping applied (best metric=0.46965113282203674)
Finished Training
Total time taken: 86.26713824272156
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.650
[3,    32] loss: 0.624
[4,    32] loss: 0.607
[5,    32] loss: 0.608
[6,    32] loss: 0.598
[7,    32] loss: 0.592
[8,    32] loss: 0.590
[9,    32] loss: 0.576
[10,    32] loss: 0.580
[11,    32] loss: 0.572
[12,    32] loss: 0.565
[13,    32] loss: 0.573
[14,    32] loss: 0.554
[15,    32] loss: 0.560
[16,    32] loss: 0.553
[17,    32] loss: 0.553
[18,    32] loss: 0.546
[19,    32] loss: 0.555
[20,    32] loss: 0.552
[21,    32] loss: 0.548
[22,    32] loss: 0.546
[23,    32] loss: 0.552
[24,    32] loss: 0.548
[25,    32] loss: 0.547
[26,    32] loss: 0.554
[27,    32] loss: 0.548
[28,    32] loss: 0.542
[29,    32] loss: 0.540
[30,    32] loss: 0.550
[31,    32] loss: 0.547
[32,    32] loss: 0.549
[33,    32] loss: 0.540
[34,    32] loss: 0.546
[35,    32] loss: 0.551
[36,    32] loss: 0.553
[37,    32] loss: 0.549
[38,    32] loss: 0.546
[39,    32] loss: 0.545
[40,    32] loss: 0.555
[41,    32] loss: 0.553
[42,    32] loss: 0.556
[43,    32] loss: 0.558
[44,    32] loss: 0.562
[45,    32] loss: 0.560
[46,    32] loss: 0.560
[47,    32] loss: 0.555
[48,    32] loss: 0.556
Early stopping applied (best metric=0.4781613349914551)
Finished Training
Total time taken: 115.01018190383911
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.663
[3,    32] loss: 0.624
[4,    32] loss: 0.613
[5,    32] loss: 0.606
[6,    32] loss: 0.596
[7,    32] loss: 0.589
[8,    32] loss: 0.573
[9,    32] loss: 0.578
[10,    32] loss: 0.569
[11,    32] loss: 0.560
[12,    32] loss: 0.551
[13,    32] loss: 0.556
[14,    32] loss: 0.549
[15,    32] loss: 0.552
[16,    32] loss: 0.547
[17,    32] loss: 0.545
[18,    32] loss: 0.549
[19,    32] loss: 0.548
[20,    32] loss: 0.550
[21,    32] loss: 0.547
[22,    32] loss: 0.542
[23,    32] loss: 0.539
[24,    32] loss: 0.555
[25,    32] loss: 0.554
[26,    32] loss: 0.548
[27,    32] loss: 0.554
[28,    32] loss: 0.544
[29,    32] loss: 0.547
[30,    32] loss: 0.551
[31,    32] loss: 0.551
[32,    32] loss: 0.547
[33,    32] loss: 0.549
[34,    32] loss: 0.551
[35,    32] loss: 0.543
[36,    32] loss: 0.548
[37,    32] loss: 0.545
[38,    32] loss: 0.549
[39,    32] loss: 0.557
Early stopping applied (best metric=0.4851534366607666)
Finished Training
Total time taken: 92.90314936637878
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.648
[3,    32] loss: 0.623
[4,    32] loss: 0.620
[5,    32] loss: 0.609
[6,    32] loss: 0.607
[7,    32] loss: 0.600
[8,    32] loss: 0.596
[9,    32] loss: 0.587
[10,    32] loss: 0.579
[11,    32] loss: 0.574
[12,    32] loss: 0.567
[13,    32] loss: 0.570
[14,    32] loss: 0.559
[15,    32] loss: 0.562
[16,    32] loss: 0.548
[17,    32] loss: 0.551
[18,    32] loss: 0.556
[19,    32] loss: 0.550
[20,    32] loss: 0.538
[21,    32] loss: 0.547
[22,    32] loss: 0.546
[23,    32] loss: 0.548
[24,    32] loss: 0.553
[25,    32] loss: 0.559
[26,    32] loss: 0.559
[27,    32] loss: 0.544
[28,    32] loss: 0.551
[29,    32] loss: 0.556
[30,    32] loss: 0.552
[31,    32] loss: 0.556
[32,    32] loss: 0.545
[33,    32] loss: 0.558
[34,    32] loss: 0.546
[35,    32] loss: 0.545
[36,    32] loss: 0.543
[37,    32] loss: 0.544
[38,    32] loss: 0.549
[39,    32] loss: 0.548
[40,    32] loss: 0.549
[41,    32] loss: 0.549
[42,    32] loss: 0.550
[43,    32] loss: 0.549
[44,    32] loss: 0.541
[45,    32] loss: 0.547
[46,    32] loss: 0.551
[47,    32] loss: 0.559
[48,    32] loss: 0.545
[49,    32] loss: 0.555
[50,    32] loss: 0.559
[51,    32] loss: 0.545
[52,    32] loss: 0.555
[53,    32] loss: 0.557
[54,    32] loss: 0.554
[55,    32] loss: 0.554
[56,    32] loss: 0.556
[57,    32] loss: 0.557
[58,    32] loss: 0.558
[59,    32] loss: 0.557
[60,    32] loss: 0.559
[61,    32] loss: 0.556
[62,    32] loss: 0.553
[63,    32] loss: 0.560
[64,    32] loss: 0.564
[65,    32] loss: 0.564
Early stopping applied (best metric=0.480846643447876)
Finished Training
Total time taken: 155.52324771881104
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.670
[3,    32] loss: 0.634
[4,    32] loss: 0.620
[5,    32] loss: 0.599
[6,    32] loss: 0.602
[7,    32] loss: 0.591
[8,    32] loss: 0.585
[9,    32] loss: 0.586
[10,    32] loss: 0.578
[11,    32] loss: 0.564
[12,    32] loss: 0.565
[13,    32] loss: 0.558
[14,    32] loss: 0.559
[15,    32] loss: 0.552
[16,    32] loss: 0.554
[17,    32] loss: 0.543
[18,    32] loss: 0.551
[19,    32] loss: 0.546
[20,    32] loss: 0.552
[21,    32] loss: 0.545
[22,    32] loss: 0.552
[23,    32] loss: 0.550
[24,    32] loss: 0.549
[25,    32] loss: 0.542
[26,    32] loss: 0.547
[27,    32] loss: 0.540
[28,    32] loss: 0.554
[29,    32] loss: 0.547
[30,    32] loss: 0.546
[31,    32] loss: 0.552
[32,    32] loss: 0.547
Early stopping applied (best metric=0.4829568862915039)
Finished Training
Total time taken: 77.00012397766113
{'S-palmitoylation-C Validation Accuracy': 0.677962500866431, 'S-palmitoylation-C Validation Sensitivity': 0.6922772277227722, 'S-palmitoylation-C Validation Specificity': 0.6743726914082809, 'S-palmitoylation-C Validation Precision': 0.3487814811258172, 'S-palmitoylation-C AUC ROC': 0.7504353513319204, 'S-palmitoylation-C AUC PR': 0.44888785429104483, 'S-palmitoylation-C MCC': 0.30067067798134783, 'S-palmitoylation-C F1': 0.46289097839414356, 'Validation Loss (S-palmitoylation-C)': 0.47935388684272767, 'Validation Loss (total)': 0.47935388684272767, 'TimeToTrain': 105.34076824188233}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004340181622431455,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2821240328,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.04927282958112}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.648
[3,    32] loss: 0.649
[4,    32] loss: 0.645
[5,    32] loss: 0.640
[6,    32] loss: 0.641
[7,    32] loss: 0.639
[8,    32] loss: 0.643
[9,    32] loss: 0.647
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 7.906575702670924e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1698495260,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.796779152005364}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.691
[3,    32] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009740427378640469,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 650320611,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.533734124240818}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.643
[3,    32] loss: 0.659
[4,    32] loss: 0.642
[5,    32] loss: 0.650
[6,    32] loss: 0.641
[7,    32] loss: 0.644
[8,    32] loss: 0.644
[9,    32] loss: 0.654
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016059201801210273,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2371489750,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.255229958949945}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.626
[3,    32] loss: 0.607
[4,    32] loss: 0.598
[5,    32] loss: 0.590
[6,    32] loss: 0.589
[7,    32] loss: 0.583
[8,    32] loss: 0.592
[9,    32] loss: 0.583
[10,    32] loss: 0.586
[11,    32] loss: 0.579
[12,    32] loss: 0.587
[13,    32] loss: 0.577
[14,    32] loss: 0.588
[15,    32] loss: 0.586
[16,    32] loss: 0.579
[17,    32] loss: 0.582
[18,    32] loss: 0.588
[19,    32] loss: 0.572
[20,    32] loss: 0.582
[21,    32] loss: 0.582
[22,    32] loss: 0.585
[23,    32] loss: 0.576
[24,    32] loss: 0.594
[25,    32] loss: 0.585
[26,    32] loss: 0.587
[27,    32] loss: 0.586
[28,    32] loss: 0.588
[29,    32] loss: 0.587
[30,    32] loss: 0.581
[31,    32] loss: 0.587
[32,    32] loss: 0.573
[33,    32] loss: 0.578
[34,    32] loss: 0.588
[35,    32] loss: 0.586
[36,    32] loss: 0.588
[37,    32] loss: 0.583
[38,    32] loss: 0.586
[39,    32] loss: 0.586
[40,    32] loss: 0.591
[41,    32] loss: 0.583
[42,    32] loss: 0.582
[43,    32] loss: 0.586
[44,    32] loss: 0.580
[45,    32] loss: 0.579
[46,    32] loss: 0.583
[47,    32] loss: 0.581
[48,    32] loss: 0.579
[49,    32] loss: 0.585
Early stopping applied (best metric=0.46726685762405396)
Finished Training
Total time taken: 117.52318835258484
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.625
[3,    32] loss: 0.613
[4,    32] loss: 0.601
[5,    32] loss: 0.590
[6,    32] loss: 0.582
[7,    32] loss: 0.578
[8,    32] loss: 0.580
[9,    32] loss: 0.568
[10,    32] loss: 0.578
[11,    32] loss: 0.577
[12,    32] loss: 0.583
[13,    32] loss: 0.580
[14,    32] loss: 0.581
[15,    32] loss: 0.576
[16,    32] loss: 0.587
[17,    32] loss: 0.579
[18,    32] loss: 0.589
[19,    32] loss: 0.583
[20,    32] loss: 0.580
[21,    32] loss: 0.584
[22,    32] loss: 0.583
[23,    32] loss: 0.572
[24,    32] loss: 0.579
[25,    32] loss: 0.577
[26,    32] loss: 0.592
[27,    32] loss: 0.572
[28,    32] loss: 0.582
[29,    32] loss: 0.576
[30,    32] loss: 0.584
[31,    32] loss: 0.573
[32,    32] loss: 0.583
[33,    32] loss: 0.581
[34,    32] loss: 0.586
[35,    32] loss: 0.583
[36,    32] loss: 0.586
[37,    32] loss: 0.585
[38,    32] loss: 0.590
[39,    32] loss: 0.585
[40,    32] loss: 0.594
[41,    32] loss: 0.579
[42,    32] loss: 0.579
[43,    32] loss: 0.586
[44,    32] loss: 0.577
[45,    32] loss: 0.578
[46,    32] loss: 0.576
[47,    32] loss: 0.581
[48,    32] loss: 0.580
[49,    32] loss: 0.575
Early stopping applied (best metric=0.4750143885612488)
Finished Training
Total time taken: 117.25318741798401
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.631
[3,    32] loss: 0.611
[4,    32] loss: 0.603
[5,    32] loss: 0.585
[6,    32] loss: 0.588
[7,    32] loss: 0.585
[8,    32] loss: 0.593
[9,    32] loss: 0.582
[10,    32] loss: 0.586
[11,    32] loss: 0.580
[12,    32] loss: 0.577
[13,    32] loss: 0.586
[14,    32] loss: 0.576
[15,    32] loss: 0.587
[16,    32] loss: 0.579
[17,    32] loss: 0.583
[18,    32] loss: 0.580
[19,    32] loss: 0.578
[20,    32] loss: 0.580
[21,    32] loss: 0.586
[22,    32] loss: 0.592
[23,    32] loss: 0.582
[24,    32] loss: 0.586
[25,    32] loss: 0.578
[26,    32] loss: 0.581
[27,    32] loss: 0.578
[28,    32] loss: 0.580
[29,    32] loss: 0.578
[30,    32] loss: 0.583
[31,    32] loss: 0.581
[32,    32] loss: 0.587
[33,    32] loss: 0.585
[34,    32] loss: 0.583
[35,    32] loss: 0.578
[36,    32] loss: 0.586
[37,    32] loss: 0.587
[38,    32] loss: 0.581
[39,    32] loss: 0.587
[40,    32] loss: 0.577
[41,    32] loss: 0.575
[42,    32] loss: 0.574
[43,    32] loss: 0.579
[44,    32] loss: 0.586
[45,    32] loss: 0.589
[46,    32] loss: 0.584
[47,    32] loss: 0.583
[48,    32] loss: 0.590
[49,    32] loss: 0.582
[50,    32] loss: 0.586
[51,    32] loss: 0.581
Early stopping applied (best metric=0.47586873173713684)
Finished Training
Total time taken: 122.44219613075256
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.620
[3,    32] loss: 0.600
[4,    32] loss: 0.598
[5,    32] loss: 0.588
[6,    32] loss: 0.588
[7,    32] loss: 0.580
[8,    32] loss: 0.578
[9,    32] loss: 0.574
[10,    32] loss: 0.581
[11,    32] loss: 0.587
[12,    32] loss: 0.582
[13,    32] loss: 0.581
[14,    32] loss: 0.579
[15,    32] loss: 0.581
[16,    32] loss: 0.586
[17,    32] loss: 0.588
[18,    32] loss: 0.584
[19,    32] loss: 0.577
[20,    32] loss: 0.577
[21,    32] loss: 0.583
[22,    32] loss: 0.580
[23,    32] loss: 0.573
[24,    32] loss: 0.591
[25,    32] loss: 0.580
[26,    32] loss: 0.599
[27,    32] loss: 0.581
[28,    32] loss: 0.593
[29,    32] loss: 0.578
[30,    32] loss: 0.587
[31,    32] loss: 0.582
[32,    32] loss: 0.585
[33,    32] loss: 0.581
[34,    32] loss: 0.583
[35,    32] loss: 0.582
[36,    32] loss: 0.581
[37,    32] loss: 0.585
[38,    32] loss: 0.585
[39,    32] loss: 0.583
[40,    32] loss: 0.577
[41,    32] loss: 0.587
[42,    32] loss: 0.582
[43,    32] loss: 0.584
[44,    32] loss: 0.585
[45,    32] loss: 0.588
[46,    32] loss: 0.584
[47,    32] loss: 0.584
[48,    32] loss: 0.587
[49,    32] loss: 0.583
[50,    32] loss: 0.585
[51,    32] loss: 0.577
[52,    32] loss: 0.579
[53,    32] loss: 0.586
[54,    32] loss: 0.577
[55,    32] loss: 0.591
[56,    32] loss: 0.587
[57,    32] loss: 0.582
[58,    32] loss: 0.591
[59,    32] loss: 0.586
[60,    32] loss: 0.584
[61,    32] loss: 0.578
Early stopping applied (best metric=0.4738583564758301)
Finished Training
Total time taken: 146.42923402786255
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.616
[3,    32] loss: 0.606
[4,    32] loss: 0.593
[5,    32] loss: 0.590
[6,    32] loss: 0.587
[7,    32] loss: 0.600
[8,    32] loss: 0.586
[9,    32] loss: 0.581
[10,    32] loss: 0.575
[11,    32] loss: 0.580
[12,    32] loss: 0.583
[13,    32] loss: 0.581
[14,    32] loss: 0.578
[15,    32] loss: 0.583
[16,    32] loss: 0.590
[17,    32] loss: 0.581
[18,    32] loss: 0.579
[19,    32] loss: 0.601
[20,    32] loss: 0.584
[21,    32] loss: 0.593
[22,    32] loss: 0.581
[23,    32] loss: 0.589
[24,    32] loss: 0.584
[25,    32] loss: 0.582
[26,    32] loss: 0.589
[27,    32] loss: 0.579
[28,    32] loss: 0.587
[29,    32] loss: 0.587
[30,    32] loss: 0.579
Early stopping applied (best metric=0.4788444936275482)
Finished Training
Total time taken: 71.86211514472961
{'S-palmitoylation-C Validation Accuracy': 0.6937636817331141, 'S-palmitoylation-C Validation Sensitivity': 0.6827722772277228, 'S-palmitoylation-C Validation Specificity': 0.6965215698546896, 'S-palmitoylation-C Validation Precision': 0.3618393648557474, 'S-palmitoylation-C AUC ROC': 0.7576358705775782, 'S-palmitoylation-C AUC PR': 0.45781258943963393, 'S-palmitoylation-C MCC': 0.31386188082446675, 'S-palmitoylation-C F1': 0.47205298695480113, 'Validation Loss (S-palmitoylation-C)': 0.4741705656051636, 'Validation Loss (total)': 0.4741705656051636, 'TimeToTrain': 115.10198421478272}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004317093023994897,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1717816551,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.5869773764160255}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.613
[3,    32] loss: 0.599
[4,    32] loss: 0.590
[5,    32] loss: 0.584
[6,    32] loss: 0.580
[7,    32] loss: 0.583
[8,    32] loss: 0.584
[9,    32] loss: 0.595
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003950301116991988,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3822523348,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.80856086385692}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.653
[3,    32] loss: 0.629
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018099764848046023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2660877069,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.178688020403357}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.616
[3,    32] loss: 0.605
[4,    32] loss: 0.597
[5,    32] loss: 0.591
[6,    32] loss: 0.588
[7,    32] loss: 0.583
[8,    32] loss: 0.593
[9,    32] loss: 0.584
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018234903903121442,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1364907540,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.171364495799615}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.632
[3,    32] loss: 0.613
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010988320408427841,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3939641720,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.983618315834576}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.625
[3,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008973554921344582,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3615219015,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.045983698335974}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.693
[3,    32] loss: 0.693
[4,    32] loss: 0.693
[5,    32] loss: 0.693
[6,    32] loss: 0.693
[7,    32] loss: 0.693
[8,    32] loss: 0.693
[9,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009249962971510893,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2672576757,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.0784787607341}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.627
[3,    32] loss: 0.606
[4,    32] loss: 0.580
[5,    32] loss: 0.570
[6,    32] loss: 0.573
[7,    32] loss: 0.547
[8,    32] loss: 0.542
[9,    32] loss: 0.528
[10,    32] loss: 0.529
[11,    32] loss: 0.524
[12,    32] loss: 0.525
[13,    32] loss: 0.518
[14,    32] loss: 0.514
[15,    32] loss: 0.518
[16,    32] loss: 0.512
[17,    32] loss: 0.511
[18,    32] loss: 0.507
[19,    32] loss: 0.514
[20,    32] loss: 0.522
[21,    32] loss: 0.509
[22,    32] loss: 0.507
[23,    32] loss: 0.511
[24,    32] loss: 0.505
[25,    32] loss: 0.508
[26,    32] loss: 0.511
[27,    32] loss: 0.513
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031212842540037375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2909239980,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.370428457860303}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.624
[3,    32] loss: 0.626
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004424229347600535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3582997631,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.38019767743816}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.636
[3,    32] loss: 0.623
[4,    32] loss: 0.628
[5,    32] loss: 0.623
[6,    32] loss: 0.635
[7,    32] loss: 0.624
[8,    32] loss: 0.624
[9,    32] loss: 0.627
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002251562264477434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1289574310,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.64419908043886}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.613
[3,    32] loss: 0.602
[4,    32] loss: 0.576
[5,    32] loss: 0.572
[6,    32] loss: 0.561
[7,    32] loss: 0.566
[8,    32] loss: 0.557
[9,    32] loss: 0.553
[10,    32] loss: 0.556
[11,    32] loss: 0.546
[12,    32] loss: 0.559
[13,    32] loss: 0.549
[14,    32] loss: 0.557
[15,    32] loss: 0.569
[16,    32] loss: 0.552
[17,    32] loss: 0.556
[18,    32] loss: 0.558
[19,    32] loss: 0.556
[20,    32] loss: 0.548
[21,    32] loss: 0.556
[22,    32] loss: 0.548
[23,    32] loss: 0.560
[24,    32] loss: 0.562
[25,    32] loss: 0.552
[26,    32] loss: 0.544
[27,    32] loss: 0.550
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005390264468492117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 65332270,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.292355495004303}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.630
[3,    32] loss: 0.625
[4,    32] loss: 0.618
[5,    32] loss: 0.618
[6,    32] loss: 0.613
[7,    32] loss: 0.630
[8,    32] loss: 0.615
[9,    32] loss: 0.621
[10,    32] loss: 0.613
[11,    32] loss: 0.619
[12,    32] loss: 0.627
[13,    32] loss: 0.624
[14,    32] loss: 0.625
[15,    32] loss: 0.617
[16,    32] loss: 0.624
[17,    32] loss: 0.628
[18,    32] loss: 0.613
[19,    32] loss: 0.626
[20,    32] loss: 0.624
[21,    32] loss: 0.616
[22,    32] loss: 0.606
[23,    32] loss: 0.625
[24,    32] loss: 0.620
[25,    32] loss: 0.622
[26,    32] loss: 0.620
[27,    32] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008821996504413912,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2216139770,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.4787070490215}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.667
[3,    32] loss: 0.653
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001555211932310013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3885303131,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.6273974426842452}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.606
[3,    32] loss: 0.589
[4,    32] loss: 0.557
[5,    32] loss: 0.539
[6,    32] loss: 0.527
[7,    32] loss: 0.506
[8,    32] loss: 0.498
[9,    32] loss: 0.481
[10,    32] loss: 0.472
[11,    32] loss: 0.451
[12,    32] loss: 0.451
[13,    32] loss: 0.441
[14,    32] loss: 0.433
[15,    32] loss: 0.418
[16,    32] loss: 0.422
[17,    32] loss: 0.407
[18,    32] loss: 0.413
[19,    32] loss: 0.398
[20,    32] loss: 0.403
[21,    32] loss: 0.388
[22,    32] loss: 0.391
[23,    32] loss: 0.389
[24,    32] loss: 0.390
[25,    32] loss: 0.376
[26,    32] loss: 0.384
[27,    32] loss: 0.377
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011033996401966074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2820810336,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.279758165834004}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.631
[3,    32] loss: 0.610
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011432632779089704,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2963324588,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.023914128212061}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.632
[3,    32] loss: 0.610
[4,    32] loss: 0.600
[5,    32] loss: 0.590
[6,    32] loss: 0.582
[7,    32] loss: 0.582
[8,    32] loss: 0.568
[9,    32] loss: 0.566
[10,    32] loss: 0.573
[11,    32] loss: 0.560
[12,    32] loss: 0.567
[13,    32] loss: 0.565
[14,    32] loss: 0.562
[15,    32] loss: 0.559
[16,    32] loss: 0.560
[17,    32] loss: 0.568
[18,    32] loss: 0.560
[19,    32] loss: 0.565
[20,    32] loss: 0.562
[21,    32] loss: 0.565
[22,    32] loss: 0.564
[23,    32] loss: 0.565
[24,    32] loss: 0.561
[25,    32] loss: 0.562
[26,    32] loss: 0.565
[27,    32] loss: 0.569
[28,    32] loss: 0.561
[29,    32] loss: 0.569
[30,    32] loss: 0.560
[31,    32] loss: 0.564
[32,    32] loss: 0.569
[33,    32] loss: 0.561
Early stopping applied (best metric=0.4605746269226074)
Finished Training
Total time taken: 78.85812592506409
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.626
[3,    32] loss: 0.606
[4,    32] loss: 0.587
[5,    32] loss: 0.583
[6,    32] loss: 0.575
[7,    32] loss: 0.576
[8,    32] loss: 0.569
[9,    32] loss: 0.565
[10,    32] loss: 0.562
[11,    32] loss: 0.556
[12,    32] loss: 0.567
[13,    32] loss: 0.559
[14,    32] loss: 0.559
[15,    32] loss: 0.558
[16,    32] loss: 0.553
[17,    32] loss: 0.559
[18,    32] loss: 0.558
[19,    32] loss: 0.554
[20,    32] loss: 0.561
[21,    32] loss: 0.558
[22,    32] loss: 0.553
[23,    32] loss: 0.556
[24,    32] loss: 0.558
[25,    32] loss: 0.557
[26,    32] loss: 0.557
[27,    32] loss: 0.562
[28,    32] loss: 0.557
[29,    32] loss: 0.554
[30,    32] loss: 0.561
[31,    32] loss: 0.562
[32,    32] loss: 0.559
[33,    32] loss: 0.560
[34,    32] loss: 0.568
[35,    32] loss: 0.558
[36,    32] loss: 0.559
[37,    32] loss: 0.559
[38,    32] loss: 0.557
[39,    32] loss: 0.551
[40,    32] loss: 0.562
[41,    32] loss: 0.558
[42,    32] loss: 0.552
[43,    32] loss: 0.553
[44,    32] loss: 0.574
[45,    32] loss: 0.560
[46,    32] loss: 0.559
[47,    32] loss: 0.566
[48,    32] loss: 0.560
[49,    32] loss: 0.568
[50,    32] loss: 0.563
[51,    32] loss: 0.573
[52,    32] loss: 0.557
[53,    32] loss: 0.561
[54,    32] loss: 0.559
[55,    32] loss: 0.559
[56,    32] loss: 0.565
[57,    32] loss: 0.559
Early stopping applied (best metric=0.4732382595539093)
Finished Training
Total time taken: 136.42821550369263
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.622
[3,    32] loss: 0.605
[4,    32] loss: 0.592
[5,    32] loss: 0.581
[6,    32] loss: 0.574
[7,    32] loss: 0.567
[8,    32] loss: 0.568
[9,    32] loss: 0.557
[10,    32] loss: 0.563
[11,    32] loss: 0.559
[12,    32] loss: 0.562
[13,    32] loss: 0.564
[14,    32] loss: 0.554
[15,    32] loss: 0.555
[16,    32] loss: 0.563
[17,    32] loss: 0.562
[18,    32] loss: 0.554
[19,    32] loss: 0.560
[20,    32] loss: 0.554
[21,    32] loss: 0.547
[22,    32] loss: 0.556
[23,    32] loss: 0.547
[24,    32] loss: 0.552
[25,    32] loss: 0.562
[26,    32] loss: 0.561
[27,    32] loss: 0.563
[28,    32] loss: 0.544
[29,    32] loss: 0.554
[30,    32] loss: 0.552
[31,    32] loss: 0.554
[32,    32] loss: 0.562
[33,    32] loss: 0.550
[34,    32] loss: 0.559
[35,    32] loss: 0.561
[36,    32] loss: 0.555
[37,    32] loss: 0.557
[38,    32] loss: 0.564
[39,    32] loss: 0.553
[40,    32] loss: 0.561
[41,    32] loss: 0.565
[42,    32] loss: 0.553
[43,    32] loss: 0.566
Early stopping applied (best metric=0.4683127701282501)
Finished Training
Total time taken: 103.03016304969788
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.620
[3,    32] loss: 0.606
[4,    32] loss: 0.586
[5,    32] loss: 0.580
[6,    32] loss: 0.571
[7,    32] loss: 0.567
[8,    32] loss: 0.565
[9,    32] loss: 0.558
[10,    32] loss: 0.561
[11,    32] loss: 0.565
[12,    32] loss: 0.547
[13,    32] loss: 0.553
[14,    32] loss: 0.558
[15,    32] loss: 0.550
[16,    32] loss: 0.560
[17,    32] loss: 0.551
[18,    32] loss: 0.560
[19,    32] loss: 0.558
[20,    32] loss: 0.551
[21,    32] loss: 0.550
[22,    32] loss: 0.555
[23,    32] loss: 0.563
[24,    32] loss: 0.555
[25,    32] loss: 0.555
[26,    32] loss: 0.557
[27,    32] loss: 0.555
[28,    32] loss: 0.558
[29,    32] loss: 0.552
[30,    32] loss: 0.558
[31,    32] loss: 0.568
[32,    32] loss: 0.567
Early stopping applied (best metric=0.47907039523124695)
Finished Training
Total time taken: 76.7771224975586
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.628
[3,    32] loss: 0.607
[4,    32] loss: 0.597
[5,    32] loss: 0.586
[6,    32] loss: 0.583
[7,    32] loss: 0.566
[8,    32] loss: 0.570
[9,    32] loss: 0.566
[10,    32] loss: 0.568
[11,    32] loss: 0.565
[12,    32] loss: 0.553
[13,    32] loss: 0.562
[14,    32] loss: 0.558
[15,    32] loss: 0.564
[16,    32] loss: 0.552
[17,    32] loss: 0.554
[18,    32] loss: 0.554
[19,    32] loss: 0.568
[20,    32] loss: 0.558
[21,    32] loss: 0.561
[22,    32] loss: 0.558
[23,    32] loss: 0.557
[24,    32] loss: 0.563
[25,    32] loss: 0.559
[26,    32] loss: 0.568
[27,    32] loss: 0.558
[28,    32] loss: 0.559
[29,    32] loss: 0.560
[30,    32] loss: 0.559
[31,    32] loss: 0.571
[32,    32] loss: 0.565
[33,    32] loss: 0.567
[34,    32] loss: 0.564
[35,    32] loss: 0.561
[36,    32] loss: 0.577
[37,    32] loss: 0.579
[38,    32] loss: 0.560
[39,    32] loss: 0.570
[40,    32] loss: 0.561
[41,    32] loss: 0.562
[42,    32] loss: 0.564
[43,    32] loss: 0.558
[44,    32] loss: 0.581
[45,    32] loss: 0.565
[46,    32] loss: 0.572
[47,    32] loss: 0.567
[48,    32] loss: 0.562
[49,    32] loss: 0.561
[50,    32] loss: 0.574
[51,    32] loss: 0.568
[52,    32] loss: 0.564
[53,    32] loss: 0.564
[54,    32] loss: 0.568
[55,    32] loss: 0.565
[56,    32] loss: 0.574
[57,    32] loss: 0.579
[58,    32] loss: 0.553
[59,    32] loss: 0.562
[60,    32] loss: 0.569
[61,    32] loss: 0.579
[62,    32] loss: 0.568
[63,    32] loss: 0.571
[64,    32] loss: 0.566
[65,    32] loss: 0.561
[66,    32] loss: 0.558
[67,    32] loss: 0.573
Early stopping applied (best metric=0.46237483620643616)
Finished Training
Total time taken: 160.21181225776672
{'S-palmitoylation-C Validation Accuracy': 0.7088427947598254, 'S-palmitoylation-C Validation Sensitivity': 0.6681188118811882, 'S-palmitoylation-C Validation Specificity': 0.719051650850991, 'S-palmitoylation-C Validation Precision': 0.3736846249993355, 'S-palmitoylation-C AUC ROC': 0.7650936451253528, 'S-palmitoylation-C AUC PR': 0.4755535434921531, 'S-palmitoylation-C MCC': 0.32330099918965816, 'S-palmitoylation-C F1': 0.47922107302103534, 'Validation Loss (S-palmitoylation-C)': 0.46871417760849, 'Validation Loss (total)': 0.46871417760849, 'TimeToTrain': 111.06108784675598}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012940794750792876,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4044938435,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.766096400424487}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.624
[3,    32] loss: 0.600
[4,    32] loss: 0.583
[5,    32] loss: 0.565
[6,    32] loss: 0.548
[7,    32] loss: 0.542
[8,    32] loss: 0.532
[9,    32] loss: 0.521
[10,    32] loss: 0.523
[11,    32] loss: 0.521
[12,    32] loss: 0.507
[13,    32] loss: 0.508
[14,    32] loss: 0.512
[15,    32] loss: 0.509
[16,    32] loss: 0.517
[17,    32] loss: 0.501
[18,    32] loss: 0.504
[19,    32] loss: 0.494
[20,    32] loss: 0.505
[21,    32] loss: 0.499
[22,    32] loss: 0.498
[23,    32] loss: 0.504
[24,    32] loss: 0.498
[25,    32] loss: 0.504
[26,    32] loss: 0.487
[27,    32] loss: 0.492
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032112693090602822,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2208463571,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.55677659119874}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.621
[3,    32] loss: 0.607
[4,    32] loss: 0.595
[5,    32] loss: 0.593
[6,    32] loss: 0.589
[7,    32] loss: 0.588
[8,    32] loss: 0.594
[9,    32] loss: 0.585
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003408559427078825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1386251847,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.35173304794289}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.614
[3,    32] loss: 0.590
[4,    32] loss: 0.567
[5,    32] loss: 0.556
[6,    32] loss: 0.537
[7,    32] loss: 0.534
[8,    32] loss: 0.506
[9,    32] loss: 0.505
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 6.656733878074495e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3980372871,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.896086678974854}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.692
[3,    32] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0071120370048614965,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3451745619,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.19413499337204}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.703
[2,    32] loss: 0.636
[3,    32] loss: 0.615
[4,    32] loss: 0.627
[5,    32] loss: 0.620
[6,    32] loss: 0.608
[7,    32] loss: 0.609
[8,    32] loss: 0.610
[9,    32] loss: 0.615
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001244676258945079,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1056976914,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.755453246189457}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.625
[3,    32] loss: 0.600
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011171978076266258,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2162082543,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.252403321094352}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.618
[3,    32] loss: 0.603
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022357685566887216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4152581721,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.300155760873066}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.621
[3,    32] loss: 0.601
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001794659052684491,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3594028473,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.030010570809836}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.619
[3,    32] loss: 0.598
[4,    32] loss: 0.581
[5,    32] loss: 0.570
[6,    32] loss: 0.560
[7,    32] loss: 0.545
[8,    32] loss: 0.537
[9,    32] loss: 0.536
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003366938135277197,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1740826741,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.602063899205844}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.630
[3,    32] loss: 0.596
[4,    32] loss: 0.582
[5,    32] loss: 0.565
[6,    32] loss: 0.560
[7,    32] loss: 0.545
[8,    32] loss: 0.542
[9,    32] loss: 0.541
[10,    32] loss: 0.526
[11,    32] loss: 0.540
[12,    32] loss: 0.536
[13,    32] loss: 0.539
[14,    32] loss: 0.536
[15,    32] loss: 0.526
[16,    32] loss: 0.525
[17,    32] loss: 0.524
[18,    32] loss: 0.532
[19,    32] loss: 0.519
[20,    32] loss: 0.532
[21,    32] loss: 0.524
[22,    32] loss: 0.521
[23,    32] loss: 0.524
[24,    32] loss: 0.520
[25,    32] loss: 0.522
[26,    32] loss: 0.517
[27,    32] loss: 0.527
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008269089301210023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2626996080,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.281414149966137}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.629
[3,    32] loss: 0.606
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 1.0245030542457351e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 673944540,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.473433834784542}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.694
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013360547955757005,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 390453558,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.172234633891536}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.619
[3,    32] loss: 0.610
[4,    32] loss: 0.594
[5,    32] loss: 0.593
[6,    32] loss: 0.595
[7,    32] loss: 0.586
[8,    32] loss: 0.585
[9,    32] loss: 0.578
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006297081841073198,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3645164941,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.496144362287719}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.631
[3,    32] loss: 0.617
[4,    32] loss: 0.592
[5,    32] loss: 0.579
[6,    32] loss: 0.560
[7,    32] loss: 0.536
[8,    32] loss: 0.527
[9,    32] loss: 0.511
[10,    32] loss: 0.495
[11,    32] loss: 0.483
[12,    32] loss: 0.476
[13,    32] loss: 0.463
[14,    32] loss: 0.453
[15,    32] loss: 0.459
[16,    32] loss: 0.453
[17,    32] loss: 0.439
[18,    32] loss: 0.443
[19,    32] loss: 0.428
[20,    32] loss: 0.431
[21,    32] loss: 0.427
[22,    32] loss: 0.419
[23,    32] loss: 0.433
[24,    32] loss: 0.416
[25,    32] loss: 0.413
[26,    32] loss: 0.413
[27,    32] loss: 0.415
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011135797355930612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1340207699,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.18762664020224}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.619
[3,    32] loss: 0.601
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033437823630354684,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1820842679,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.2647414915401454}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.631
[3,    32] loss: 0.604
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002773331808321407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2545245217,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.70936466498682}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.614
[3,    32] loss: 0.603
[4,    32] loss: 0.599
[5,    32] loss: 0.588
[6,    32] loss: 0.585
[7,    32] loss: 0.582
[8,    32] loss: 0.584
[9,    32] loss: 0.573
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012366278545684482,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2541873337,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.607662314124935}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.622
[3,    32] loss: 0.599
[4,    32] loss: 0.596
[5,    32] loss: 0.576
[6,    32] loss: 0.575
[7,    32] loss: 0.565
[8,    32] loss: 0.562
[9,    32] loss: 0.564
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 8.815923586760147e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3818486745,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.658986245090434}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.691
[3,    32] loss: 0.683
[4,    32] loss: 0.660
[5,    32] loss: 0.633
[6,    32] loss: 0.622
[7,    32] loss: 0.615
[8,    32] loss: 0.608
[9,    32] loss: 0.610
[10,    32] loss: 0.605
[11,    32] loss: 0.594
[12,    32] loss: 0.586
[13,    32] loss: 0.590
[14,    32] loss: 0.587
[15,    32] loss: 0.581
[16,    32] loss: 0.583
[17,    32] loss: 0.568
[18,    32] loss: 0.571
[19,    32] loss: 0.560
[20,    32] loss: 0.552
[21,    32] loss: 0.543
[22,    32] loss: 0.538
[23,    32] loss: 0.529
[24,    32] loss: 0.526
[25,    32] loss: 0.517
[26,    32] loss: 0.511
[27,    32] loss: 0.493
[28,    32] loss: 0.488
[29,    32] loss: 0.488
[30,    32] loss: 0.469
[31,    32] loss: 0.472
[32,    32] loss: 0.457
[33,    32] loss: 0.438
[34,    32] loss: 0.449
[35,    32] loss: 0.437
[36,    32] loss: 0.434
[37,    32] loss: 0.417
[38,    32] loss: 0.404
[39,    32] loss: 0.406
[40,    32] loss: 0.398
[41,    32] loss: 0.398
[42,    32] loss: 0.387
[43,    32] loss: 0.386
[44,    32] loss: 0.376
[45,    32] loss: 0.378
[46,    32] loss: 0.369
[47,    32] loss: 0.368
[48,    32] loss: 0.359
[49,    32] loss: 0.355
[50,    32] loss: 0.353
Early stopping applied (best metric=0.4734644293785095)
Finished Training
Total time taken: 119.72117686271667
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.691
[3,    32] loss: 0.682
[4,    32] loss: 0.661
[5,    32] loss: 0.642
[6,    32] loss: 0.628
[7,    32] loss: 0.616
[8,    32] loss: 0.612
[9,    32] loss: 0.611
[10,    32] loss: 0.609
[11,    32] loss: 0.603
[12,    32] loss: 0.599
[13,    32] loss: 0.593
[14,    32] loss: 0.588
[15,    32] loss: 0.588
[16,    32] loss: 0.583
[17,    32] loss: 0.579
[18,    32] loss: 0.566
[19,    32] loss: 0.568
[20,    32] loss: 0.562
[21,    32] loss: 0.546
[22,    32] loss: 0.543
[23,    32] loss: 0.536
[24,    32] loss: 0.524
[25,    32] loss: 0.518
[26,    32] loss: 0.514
[27,    32] loss: 0.512
[28,    32] loss: 0.503
[29,    32] loss: 0.489
[30,    32] loss: 0.479
[31,    32] loss: 0.481
[32,    32] loss: 0.463
[33,    32] loss: 0.465
[34,    32] loss: 0.459
[35,    32] loss: 0.452
[36,    32] loss: 0.437
[37,    32] loss: 0.431
[38,    32] loss: 0.434
[39,    32] loss: 0.421
[40,    32] loss: 0.412
[41,    32] loss: 0.410
[42,    32] loss: 0.397
[43,    32] loss: 0.401
[44,    32] loss: 0.387
[45,    32] loss: 0.384
[46,    32] loss: 0.391
[47,    32] loss: 0.373
[48,    32] loss: 0.371
[49,    32] loss: 0.365
[50,    32] loss: 0.357
[51,    32] loss: 0.360
[52,    32] loss: 0.349
[53,    32] loss: 0.353
[54,    32] loss: 0.352
[55,    32] loss: 0.344
Early stopping applied (best metric=0.46038585901260376)
Finished Training
Total time taken: 131.74918365478516
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.680
[4,    32] loss: 0.659
[5,    32] loss: 0.638
[6,    32] loss: 0.629
[7,    32] loss: 0.620
[8,    32] loss: 0.611
[9,    32] loss: 0.605
[10,    32] loss: 0.602
[11,    32] loss: 0.602
[12,    32] loss: 0.597
[13,    32] loss: 0.586
[14,    32] loss: 0.587
[15,    32] loss: 0.588
[16,    32] loss: 0.584
[17,    32] loss: 0.578
[18,    32] loss: 0.570
[19,    32] loss: 0.560
[20,    32] loss: 0.560
[21,    32] loss: 0.554
[22,    32] loss: 0.548
[23,    32] loss: 0.538
[24,    32] loss: 0.530
[25,    32] loss: 0.519
[26,    32] loss: 0.512
[27,    32] loss: 0.511
[28,    32] loss: 0.504
[29,    32] loss: 0.484
[30,    32] loss: 0.493
[31,    32] loss: 0.478
[32,    32] loss: 0.464
[33,    32] loss: 0.463
[34,    32] loss: 0.456
[35,    32] loss: 0.444
[36,    32] loss: 0.429
[37,    32] loss: 0.425
[38,    32] loss: 0.420
[39,    32] loss: 0.416
[40,    32] loss: 0.409
[41,    32] loss: 0.408
[42,    32] loss: 0.392
[43,    32] loss: 0.390
[44,    32] loss: 0.393
[45,    32] loss: 0.387
[46,    32] loss: 0.381
[47,    32] loss: 0.369
[48,    32] loss: 0.366
[49,    32] loss: 0.368
[50,    32] loss: 0.350
Early stopping applied (best metric=0.4672708213329315)
Finished Training
Total time taken: 120.08916664123535
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.678
[4,    32] loss: 0.656
[5,    32] loss: 0.635
[6,    32] loss: 0.623
[7,    32] loss: 0.617
[8,    32] loss: 0.612
[9,    32] loss: 0.609
[10,    32] loss: 0.601
[11,    32] loss: 0.598
[12,    32] loss: 0.598
[13,    32] loss: 0.589
[14,    32] loss: 0.583
[15,    32] loss: 0.579
[16,    32] loss: 0.580
[17,    32] loss: 0.571
[18,    32] loss: 0.570
[19,    32] loss: 0.558
[20,    32] loss: 0.554
[21,    32] loss: 0.549
[22,    32] loss: 0.547
[23,    32] loss: 0.536
[24,    32] loss: 0.527
[25,    32] loss: 0.519
[26,    32] loss: 0.506
[27,    32] loss: 0.505
[28,    32] loss: 0.500
[29,    32] loss: 0.492
[30,    32] loss: 0.479
[31,    32] loss: 0.473
[32,    32] loss: 0.478
[33,    32] loss: 0.463
[34,    32] loss: 0.455
[35,    32] loss: 0.451
[36,    32] loss: 0.444
[37,    32] loss: 0.433
[38,    32] loss: 0.420
[39,    32] loss: 0.427
[40,    32] loss: 0.415
[41,    32] loss: 0.412
[42,    32] loss: 0.398
[43,    32] loss: 0.402
[44,    32] loss: 0.394
[45,    32] loss: 0.378
[46,    32] loss: 0.382
[47,    32] loss: 0.375
[48,    32] loss: 0.370
[49,    32] loss: 0.361
[50,    32] loss: 0.358
Early stopping applied (best metric=0.46506720781326294)
Finished Training
Total time taken: 119.52716755867004
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.689
[3,    32] loss: 0.676
[4,    32] loss: 0.651
[5,    32] loss: 0.636
[6,    32] loss: 0.623
[7,    32] loss: 0.618
[8,    32] loss: 0.608
[9,    32] loss: 0.601
[10,    32] loss: 0.600
[11,    32] loss: 0.597
[12,    32] loss: 0.598
[13,    32] loss: 0.590
[14,    32] loss: 0.583
[15,    32] loss: 0.580
[16,    32] loss: 0.578
[17,    32] loss: 0.573
[18,    32] loss: 0.566
[19,    32] loss: 0.563
[20,    32] loss: 0.555
[21,    32] loss: 0.553
[22,    32] loss: 0.542
[23,    32] loss: 0.530
[24,    32] loss: 0.526
[25,    32] loss: 0.512
[26,    32] loss: 0.505
[27,    32] loss: 0.503
[28,    32] loss: 0.491
[29,    32] loss: 0.479
[30,    32] loss: 0.470
[31,    32] loss: 0.466
[32,    32] loss: 0.459
[33,    32] loss: 0.448
[34,    32] loss: 0.443
[35,    32] loss: 0.439
[36,    32] loss: 0.433
[37,    32] loss: 0.425
[38,    32] loss: 0.415
[39,    32] loss: 0.408
[40,    32] loss: 0.409
[41,    32] loss: 0.399
[42,    32] loss: 0.398
[43,    32] loss: 0.389
[44,    32] loss: 0.377
[45,    32] loss: 0.378
[46,    32] loss: 0.370
[47,    32] loss: 0.369
[48,    32] loss: 0.363
[49,    32] loss: 0.365
[50,    32] loss: 0.365
[51,    32] loss: 0.346
[52,    32] loss: 0.334
[53,    32] loss: 0.351
[54,    32] loss: 0.346
[55,    32] loss: 0.334
[56,    32] loss: 0.333
[57,    32] loss: 0.332
[58,    32] loss: 0.327
[59,    32] loss: 0.326
[60,    32] loss: 0.318
[61,    32] loss: 0.311
[62,    32] loss: 0.320
[63,    32] loss: 0.313
[64,    32] loss: 0.314
[65,    32] loss: 0.307
Early stopping applied (best metric=0.46293357014656067)
Finished Training
Total time taken: 155.5472173690796
{'S-palmitoylation-C Validation Accuracy': 0.7132081576841403, 'S-palmitoylation-C Validation Sensitivity': 0.6685148514851486, 'S-palmitoylation-C Validation Specificity': 0.7244111566429534, 'S-palmitoylation-C Validation Precision': 0.37888305725813637, 'S-palmitoylation-C AUC ROC': 0.7627933363587776, 'S-palmitoylation-C AUC PR': 0.45821476795213545, 'S-palmitoylation-C MCC': 0.32933222298766973, 'S-palmitoylation-C F1': 0.48321043811854053, 'Validation Loss (S-palmitoylation-C)': 0.4658243775367737, 'Validation Loss (total)': 0.4658243775367737, 'TimeToTrain': 129.32678241729735}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003423227780562977,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 984466528,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.005788838144074049}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.617
[3,    32] loss: 0.601
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00575669533549355,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2119772122,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.732763747298517}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.630
[3,    32] loss: 0.614
[4,    32] loss: 0.615
[5,    32] loss: 0.618
[6,    32] loss: 0.615
[7,    32] loss: 0.618
[8,    32] loss: 0.621
[9,    32] loss: 0.612
[10,    32] loss: 0.612
[11,    32] loss: 0.615
[12,    32] loss: 0.606
[13,    32] loss: 0.614
[14,    32] loss: 0.607
[15,    32] loss: 0.607
[16,    32] loss: 0.614
[17,    32] loss: 0.615
[18,    32] loss: 0.623
[19,    32] loss: 0.616
[20,    32] loss: 0.612
[21,    32] loss: 0.611
[22,    32] loss: 0.609
[23,    32] loss: 0.610
[24,    32] loss: 0.611
[25,    32] loss: 0.618
[26,    32] loss: 0.614
[27,    32] loss: 0.614
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001767637923016711,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2910394998,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.402783837895707}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.619
[3,    32] loss: 0.590
[4,    32] loss: 0.569
[5,    32] loss: 0.556
[6,    32] loss: 0.526
[7,    32] loss: 0.522
[8,    32] loss: 0.511
[9,    32] loss: 0.494
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00881259920698043,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2241057976,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.803924443623227}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.659
[3,    32] loss: 0.643
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00034620611668503875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3423730119,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.1755564153176534}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.645
[3,    32] loss: 0.618
[4,    32] loss: 0.609
[5,    32] loss: 0.597
[6,    32] loss: 0.585
[7,    32] loss: 0.571
[8,    32] loss: 0.558
[9,    32] loss: 0.540
[10,    32] loss: 0.530
[11,    32] loss: 0.510
[12,    32] loss: 0.498
[13,    32] loss: 0.484
[14,    32] loss: 0.458
[15,    32] loss: 0.458
[16,    32] loss: 0.447
[17,    32] loss: 0.429
[18,    32] loss: 0.428
[19,    32] loss: 0.408
[20,    32] loss: 0.403
[21,    32] loss: 0.392
[22,    32] loss: 0.387
[23,    32] loss: 0.384
[24,    32] loss: 0.380
[25,    32] loss: 0.363
[26,    32] loss: 0.353
[27,    32] loss: 0.361
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003407647939559398,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3480401956,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.432614940772643}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.688
[2,    32] loss: 0.637
[3,    32] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003855168025608346,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4140826495,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.373804731143311}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.616
[3,    32] loss: 0.609
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007580351440574393,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1700443134,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.749498480407377}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.623
[3,    32] loss: 0.598
[4,    32] loss: 0.600
[5,    32] loss: 0.603
[6,    32] loss: 0.605
[7,    32] loss: 0.599
[8,    32] loss: 0.585
[9,    32] loss: 0.593
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029473596215194905,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3768027790,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.982562864709305}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.613
[3,    32] loss: 0.588
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002650500626263206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 612980220,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.719338553119304}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.690
[2,    32] loss: 0.617
[3,    32] loss: 0.601
[4,    32] loss: 0.586
[5,    32] loss: 0.584
[6,    32] loss: 0.580
[7,    32] loss: 0.574
[8,    32] loss: 0.576
[9,    32] loss: 0.591
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00021009061193394754,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1684045641,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.125948204369356}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.673
[3,    32] loss: 0.638
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006800003860139208,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 101312767,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.376997279147591}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.618
[3,    32] loss: 0.612
[4,    32] loss: 0.607
[5,    32] loss: 0.614
[6,    32] loss: 0.606
[7,    32] loss: 0.610
[8,    32] loss: 0.602
[9,    32] loss: 0.605
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002333233741467923,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3093487261,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.120459944931087}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.611
[3,    32] loss: 0.587
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 5.201900261696206e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3499745055,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.424342388408098}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.691
[4,    32] loss: 0.686
[5,    32] loss: 0.676
[6,    32] loss: 0.660
[7,    32] loss: 0.642
[8,    32] loss: 0.633
[9,    32] loss: 0.626
[10,    32] loss: 0.620
[11,    32] loss: 0.614
[12,    32] loss: 0.613
[13,    32] loss: 0.613
[14,    32] loss: 0.607
[15,    32] loss: 0.605
[16,    32] loss: 0.607
[17,    32] loss: 0.596
[18,    32] loss: 0.598
[19,    32] loss: 0.595
[20,    32] loss: 0.593
[21,    32] loss: 0.589
[22,    32] loss: 0.588
[23,    32] loss: 0.583
[24,    32] loss: 0.584
[25,    32] loss: 0.577
[26,    32] loss: 0.577
[27,    32] loss: 0.574
[28,    32] loss: 0.567
[29,    32] loss: 0.563
[30,    32] loss: 0.558
[31,    32] loss: 0.556
[32,    32] loss: 0.556
[33,    32] loss: 0.550
[34,    32] loss: 0.546
[35,    32] loss: 0.541
[36,    32] loss: 0.540
[37,    32] loss: 0.533
[38,    32] loss: 0.522
[39,    32] loss: 0.518
[40,    32] loss: 0.518
[41,    32] loss: 0.513
[42,    32] loss: 0.505
[43,    32] loss: 0.500
[44,    32] loss: 0.493
[45,    32] loss: 0.489
[46,    32] loss: 0.481
[47,    32] loss: 0.470
[48,    32] loss: 0.468
[49,    32] loss: 0.464
[50,    32] loss: 0.469
[51,    32] loss: 0.465
[52,    32] loss: 0.442
[53,    32] loss: 0.445
[54,    32] loss: 0.437
[55,    32] loss: 0.425
[56,    32] loss: 0.430
[57,    32] loss: 0.429
[58,    32] loss: 0.409
[59,    32] loss: 0.410
[60,    32] loss: 0.413
[61,    32] loss: 0.405
Early stopping applied (best metric=0.46186113357543945)
Finished Training
Total time taken: 146.2042043209076
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.693
[3,    32] loss: 0.691
[4,    32] loss: 0.686
[5,    32] loss: 0.676
[6,    32] loss: 0.660
[7,    32] loss: 0.646
[8,    32] loss: 0.640
[9,    32] loss: 0.622
[10,    32] loss: 0.623
[11,    32] loss: 0.619
[12,    32] loss: 0.610
[13,    32] loss: 0.611
[14,    32] loss: 0.604
[15,    32] loss: 0.602
[16,    32] loss: 0.600
[17,    32] loss: 0.600
[18,    32] loss: 0.598
[19,    32] loss: 0.600
[20,    32] loss: 0.594
[21,    32] loss: 0.587
[22,    32] loss: 0.591
[23,    32] loss: 0.586
[24,    32] loss: 0.578
[25,    32] loss: 0.576
[26,    32] loss: 0.573
[27,    32] loss: 0.579
[28,    32] loss: 0.567
[29,    32] loss: 0.570
[30,    32] loss: 0.560
[31,    32] loss: 0.550
[32,    32] loss: 0.551
[33,    32] loss: 0.551
[34,    32] loss: 0.545
[35,    32] loss: 0.546
[36,    32] loss: 0.536
[37,    32] loss: 0.532
[38,    32] loss: 0.530
[39,    32] loss: 0.523
[40,    32] loss: 0.517
[41,    32] loss: 0.520
[42,    32] loss: 0.505
[43,    32] loss: 0.500
[44,    32] loss: 0.501
[45,    32] loss: 0.486
[46,    32] loss: 0.489
[47,    32] loss: 0.485
[48,    32] loss: 0.470
[49,    32] loss: 0.476
[50,    32] loss: 0.466
[51,    32] loss: 0.462
[52,    32] loss: 0.461
[53,    32] loss: 0.449
[54,    32] loss: 0.453
[55,    32] loss: 0.450
[56,    32] loss: 0.431
[57,    32] loss: 0.430
[58,    32] loss: 0.432
[59,    32] loss: 0.430
[60,    32] loss: 0.422
[61,    32] loss: 0.415
[62,    32] loss: 0.410
[63,    32] loss: 0.411
[64,    32] loss: 0.401
[65,    32] loss: 0.399
[66,    32] loss: 0.397
[67,    32] loss: 0.388
[68,    32] loss: 0.380
[69,    32] loss: 0.382
[70,    32] loss: 0.375
Early stopping applied (best metric=0.46296560764312744)
Finished Training
Total time taken: 167.89923453330994
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.692
[3,    32] loss: 0.689
[4,    32] loss: 0.683
[5,    32] loss: 0.672
[6,    32] loss: 0.654
[7,    32] loss: 0.641
[8,    32] loss: 0.631
[9,    32] loss: 0.619
[10,    32] loss: 0.614
[11,    32] loss: 0.616
[12,    32] loss: 0.603
[13,    32] loss: 0.605
[14,    32] loss: 0.597
[15,    32] loss: 0.597
[16,    32] loss: 0.597
[17,    32] loss: 0.603
[18,    32] loss: 0.589
[19,    32] loss: 0.594
[20,    32] loss: 0.594
[21,    32] loss: 0.586
[22,    32] loss: 0.585
[23,    32] loss: 0.580
[24,    32] loss: 0.579
[25,    32] loss: 0.580
[26,    32] loss: 0.576
[27,    32] loss: 0.567
[28,    32] loss: 0.561
[29,    32] loss: 0.571
[30,    32] loss: 0.565
[31,    32] loss: 0.560
[32,    32] loss: 0.563
[33,    32] loss: 0.552
[34,    32] loss: 0.543
[35,    32] loss: 0.546
[36,    32] loss: 0.539
[37,    32] loss: 0.534
[38,    32] loss: 0.531
[39,    32] loss: 0.518
[40,    32] loss: 0.521
[41,    32] loss: 0.514
[42,    32] loss: 0.506
[43,    32] loss: 0.497
[44,    32] loss: 0.496
[45,    32] loss: 0.492
[46,    32] loss: 0.485
[47,    32] loss: 0.476
[48,    32] loss: 0.469
[49,    32] loss: 0.468
[50,    32] loss: 0.467
[51,    32] loss: 0.462
[52,    32] loss: 0.458
[53,    32] loss: 0.458
[54,    32] loss: 0.446
[55,    32] loss: 0.439
[56,    32] loss: 0.432
[57,    32] loss: 0.432
[58,    32] loss: 0.430
[59,    32] loss: 0.420
[60,    32] loss: 0.420
[61,    32] loss: 0.408
[62,    32] loss: 0.409
[63,    32] loss: 0.405
[64,    32] loss: 0.402
Early stopping applied (best metric=0.47052380442619324)
Finished Training
Total time taken: 153.2772126197815
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.691
[3,    32] loss: 0.688
[4,    32] loss: 0.682
[5,    32] loss: 0.670
[6,    32] loss: 0.654
[7,    32] loss: 0.642
[8,    32] loss: 0.635
[9,    32] loss: 0.627
[10,    32] loss: 0.625
[11,    32] loss: 0.615
[12,    32] loss: 0.611
[13,    32] loss: 0.616
[14,    32] loss: 0.607
[15,    32] loss: 0.606
[16,    32] loss: 0.604
[17,    32] loss: 0.598
[18,    32] loss: 0.602
[19,    32] loss: 0.589
[20,    32] loss: 0.591
[21,    32] loss: 0.593
[22,    32] loss: 0.579
[23,    32] loss: 0.582
[24,    32] loss: 0.582
[25,    32] loss: 0.574
[26,    32] loss: 0.577
[27,    32] loss: 0.572
[28,    32] loss: 0.576
[29,    32] loss: 0.567
[30,    32] loss: 0.559
[31,    32] loss: 0.558
[32,    32] loss: 0.542
[33,    32] loss: 0.542
[34,    32] loss: 0.534
[35,    32] loss: 0.533
[36,    32] loss: 0.529
[37,    32] loss: 0.521
[38,    32] loss: 0.521
[39,    32] loss: 0.511
[40,    32] loss: 0.505
[41,    32] loss: 0.510
[42,    32] loss: 0.495
[43,    32] loss: 0.499
[44,    32] loss: 0.493
[45,    32] loss: 0.477
[46,    32] loss: 0.479
[47,    32] loss: 0.466
[48,    32] loss: 0.465
[49,    32] loss: 0.452
[50,    32] loss: 0.450
[51,    32] loss: 0.442
[52,    32] loss: 0.443
[53,    32] loss: 0.426
[54,    32] loss: 0.434
[55,    32] loss: 0.431
[56,    32] loss: 0.423
[57,    32] loss: 0.417
[58,    32] loss: 0.416
[59,    32] loss: 0.405
[60,    32] loss: 0.406
[61,    32] loss: 0.400
Early stopping applied (best metric=0.4685782194137573)
Finished Training
Total time taken: 146.72420501708984
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.693
[3,    32] loss: 0.691
[4,    32] loss: 0.685
[5,    32] loss: 0.673
[6,    32] loss: 0.658
[7,    32] loss: 0.644
[8,    32] loss: 0.631
[9,    32] loss: 0.623
[10,    32] loss: 0.615
[11,    32] loss: 0.616
[12,    32] loss: 0.614
[13,    32] loss: 0.609
[14,    32] loss: 0.607
[15,    32] loss: 0.600
[16,    32] loss: 0.600
[17,    32] loss: 0.597
[18,    32] loss: 0.595
[19,    32] loss: 0.584
[20,    32] loss: 0.590
[21,    32] loss: 0.595
[22,    32] loss: 0.583
[23,    32] loss: 0.581
[24,    32] loss: 0.582
[25,    32] loss: 0.579
[26,    32] loss: 0.572
[27,    32] loss: 0.569
[28,    32] loss: 0.567
[29,    32] loss: 0.562
[30,    32] loss: 0.558
[31,    32] loss: 0.562
[32,    32] loss: 0.548
[33,    32] loss: 0.543
[34,    32] loss: 0.539
[35,    32] loss: 0.529
[36,    32] loss: 0.524
[37,    32] loss: 0.527
[38,    32] loss: 0.515
[39,    32] loss: 0.512
[40,    32] loss: 0.512
[41,    32] loss: 0.502
[42,    32] loss: 0.499
[43,    32] loss: 0.487
[44,    32] loss: 0.478
[45,    32] loss: 0.477
[46,    32] loss: 0.473
[47,    32] loss: 0.464
[48,    32] loss: 0.466
[49,    32] loss: 0.458
[50,    32] loss: 0.458
[51,    32] loss: 0.450
[52,    32] loss: 0.448
[53,    32] loss: 0.437
[54,    32] loss: 0.433
[55,    32] loss: 0.434
[56,    32] loss: 0.428
[57,    32] loss: 0.426
[58,    32] loss: 0.411
[59,    32] loss: 0.410
Early stopping applied (best metric=0.4655698835849762)
Finished Training
Total time taken: 141.25099444389343
{'S-palmitoylation-C Validation Accuracy': 0.6928084021752144, 'S-palmitoylation-C Validation Sensitivity': 0.6851485148514851, 'S-palmitoylation-C Validation Specificity': 0.6947281190475604, 'S-palmitoylation-C Validation Precision': 0.3631877794439688, 'S-palmitoylation-C AUC ROC': 0.7631336797004172, 'S-palmitoylation-C AUC PR': 0.46233914083359046, 'S-palmitoylation-C MCC': 0.31507916810543235, 'S-palmitoylation-C F1': 0.4728379603181295, 'Validation Loss (S-palmitoylation-C)': 0.46589972972869875, 'Validation Loss (total)': 0.46589972972869875, 'TimeToTrain': 151.07117018699645}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012483758613398776,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 66096766,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.17722711857351}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.614
[3,    32] loss: 0.591
[4,    32] loss: 0.578
[5,    32] loss: 0.560
[6,    32] loss: 0.550
[7,    32] loss: 0.551
[8,    32] loss: 0.527
[9,    32] loss: 0.536
[10,    32] loss: 0.521
[11,    32] loss: 0.525
[12,    32] loss: 0.523
[13,    32] loss: 0.526
[14,    32] loss: 0.521
[15,    32] loss: 0.514
[16,    32] loss: 0.523
[17,    32] loss: 0.529
[18,    32] loss: 0.522
[19,    32] loss: 0.512
[20,    32] loss: 0.521
[21,    32] loss: 0.527
[22,    32] loss: 0.516
[23,    32] loss: 0.511
[24,    32] loss: 0.523
[25,    32] loss: 0.520
[26,    32] loss: 0.508
[27,    32] loss: 0.519
[28,    32] loss: 0.522
[29,    32] loss: 0.529
[30,    32] loss: 0.516
[31,    32] loss: 0.527
[32,    32] loss: 0.517
[33,    32] loss: 0.519
[34,    32] loss: 0.514
[35,    32] loss: 0.524
[36,    32] loss: 0.520
[37,    32] loss: 0.518
[38,    32] loss: 0.515
[39,    32] loss: 0.516
[40,    32] loss: 0.524
[41,    32] loss: 0.513
[42,    32] loss: 0.517
[43,    32] loss: 0.510
[44,    32] loss: 0.520
[45,    32] loss: 0.521
[46,    32] loss: 0.514
[47,    32] loss: 0.526
[48,    32] loss: 0.521
[49,    32] loss: 0.519
[50,    32] loss: 0.522
[51,    32] loss: 0.523
[52,    32] loss: 0.519
[53,    32] loss: 0.517
Early stopping applied (best metric=0.46788907051086426)
Finished Training
Total time taken: 127.25317907333374
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.619
[3,    32] loss: 0.593
[4,    32] loss: 0.579
[5,    32] loss: 0.575
[6,    32] loss: 0.560
[7,    32] loss: 0.553
[8,    32] loss: 0.536
[9,    32] loss: 0.551
[10,    32] loss: 0.530
[11,    32] loss: 0.546
[12,    32] loss: 0.530
[13,    32] loss: 0.536
[14,    32] loss: 0.535
[15,    32] loss: 0.532
[16,    32] loss: 0.529
[17,    32] loss: 0.522
[18,    32] loss: 0.531
[19,    32] loss: 0.527
[20,    32] loss: 0.538
[21,    32] loss: 0.518
[22,    32] loss: 0.527
[23,    32] loss: 0.536
[24,    32] loss: 0.533
[25,    32] loss: 0.518
[26,    32] loss: 0.523
[27,    32] loss: 0.525
[28,    32] loss: 0.525
[29,    32] loss: 0.523
[30,    32] loss: 0.520
[31,    32] loss: 0.526
[32,    32] loss: 0.525
Early stopping applied (best metric=0.47386330366134644)
Finished Training
Total time taken: 76.607106924057
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.619
[3,    32] loss: 0.599
[4,    32] loss: 0.581
[5,    32] loss: 0.566
[6,    32] loss: 0.558
[7,    32] loss: 0.555
[8,    32] loss: 0.550
[9,    32] loss: 0.542
[10,    32] loss: 0.530
[11,    32] loss: 0.529
[12,    32] loss: 0.526
[13,    32] loss: 0.537
[14,    32] loss: 0.530
[15,    32] loss: 0.526
[16,    32] loss: 0.534
[17,    32] loss: 0.528
[18,    32] loss: 0.519
[19,    32] loss: 0.527
[20,    32] loss: 0.523
[21,    32] loss: 0.525
[22,    32] loss: 0.534
[23,    32] loss: 0.523
[24,    32] loss: 0.521
[25,    32] loss: 0.527
[26,    32] loss: 0.522
[27,    32] loss: 0.527
[28,    32] loss: 0.521
[29,    32] loss: 0.521
[30,    32] loss: 0.526
[31,    32] loss: 0.530
Early stopping applied (best metric=0.46376538276672363)
Finished Training
Total time taken: 74.34610342979431
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.615
[3,    32] loss: 0.590
[4,    32] loss: 0.576
[5,    32] loss: 0.575
[6,    32] loss: 0.563
[7,    32] loss: 0.543
[8,    32] loss: 0.544
[9,    32] loss: 0.541
[10,    32] loss: 0.531
[11,    32] loss: 0.534
[12,    32] loss: 0.529
[13,    32] loss: 0.522
[14,    32] loss: 0.518
[15,    32] loss: 0.523
[16,    32] loss: 0.516
[17,    32] loss: 0.518
[18,    32] loss: 0.526
[19,    32] loss: 0.518
[20,    32] loss: 0.525
[21,    32] loss: 0.517
[22,    32] loss: 0.527
[23,    32] loss: 0.530
[24,    32] loss: 0.508
[25,    32] loss: 0.513
[26,    32] loss: 0.525
[27,    32] loss: 0.519
[28,    32] loss: 0.519
[29,    32] loss: 0.525
[30,    32] loss: 0.525
[31,    32] loss: 0.514
Early stopping applied (best metric=0.4923422336578369)
Finished Training
Total time taken: 74.36610388755798
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.620
[3,    32] loss: 0.603
[4,    32] loss: 0.586
[5,    32] loss: 0.569
[6,    32] loss: 0.562
[7,    32] loss: 0.554
[8,    32] loss: 0.554
[9,    32] loss: 0.534
[10,    32] loss: 0.530
[11,    32] loss: 0.536
[12,    32] loss: 0.530
[13,    32] loss: 0.535
[14,    32] loss: 0.527
[15,    32] loss: 0.528
[16,    32] loss: 0.521
[17,    32] loss: 0.521
[18,    32] loss: 0.541
[19,    32] loss: 0.525
[20,    32] loss: 0.519
[21,    32] loss: 0.519
[22,    32] loss: 0.524
[23,    32] loss: 0.521
[24,    32] loss: 0.515
[25,    32] loss: 0.515
[26,    32] loss: 0.518
[27,    32] loss: 0.523
[28,    32] loss: 0.520
[29,    32] loss: 0.516
[30,    32] loss: 0.504
[31,    32] loss: 0.518
[32,    32] loss: 0.520
[33,    32] loss: 0.511
[34,    32] loss: 0.520
[35,    32] loss: 0.520
[36,    32] loss: 0.523
[37,    32] loss: 0.514
[38,    32] loss: 0.528
[39,    32] loss: 0.526
Early stopping applied (best metric=0.4653209447860718)
Finished Training
Total time taken: 93.5941309928894
{'S-palmitoylation-C Validation Accuracy': 0.698601611876721, 'S-palmitoylation-C Validation Sensitivity': 0.666930693069307, 'S-palmitoylation-C Validation Specificity': 0.7065409133583526, 'S-palmitoylation-C Validation Precision': 0.3642372312395063, 'S-palmitoylation-C AUC ROC': 0.7543784507635798, 'S-palmitoylation-C AUC PR': 0.4516546038106698, 'S-palmitoylation-C MCC': 0.3107964734111959, 'S-palmitoylation-C F1': 0.47037609134336533, 'Validation Loss (S-palmitoylation-C)': 0.4726361870765686, 'Validation Loss (total)': 0.4726361870765686, 'TimeToTrain': 89.23332486152648}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026845425142016568,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1377747511,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.480822590201518}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.606
[3,    32] loss: 0.596
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017945038767368372,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2899521334,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.892470498286697}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.626
[3,    32] loss: 0.592
[4,    32] loss: 0.580
[5,    32] loss: 0.563
[6,    32] loss: 0.566
[7,    32] loss: 0.552
[8,    32] loss: 0.546
[9,    32] loss: 0.538
[10,    32] loss: 0.551
[11,    32] loss: 0.541
[12,    32] loss: 0.528
[13,    32] loss: 0.535
[14,    32] loss: 0.536
[15,    32] loss: 0.527
[16,    32] loss: 0.531
[17,    32] loss: 0.528
[18,    32] loss: 0.526
[19,    32] loss: 0.529
[20,    32] loss: 0.526
[21,    32] loss: 0.520
[22,    32] loss: 0.523
[23,    32] loss: 0.518
[24,    32] loss: 0.530
[25,    32] loss: 0.531
[26,    32] loss: 0.517
[27,    32] loss: 0.519
[28,    32] loss: 0.530
[29,    32] loss: 0.524
[30,    32] loss: 0.522
[31,    32] loss: 0.536
[32,    32] loss: 0.524
[33,    32] loss: 0.528
[34,    32] loss: 0.530
[35,    32] loss: 0.535
[36,    32] loss: 0.530
[37,    32] loss: 0.535
[38,    32] loss: 0.527
[39,    32] loss: 0.527
[40,    32] loss: 0.537
[41,    32] loss: 0.524
[42,    32] loss: 0.521
[43,    32] loss: 0.529
[44,    32] loss: 0.529
[45,    32] loss: 0.520
[46,    32] loss: 0.528
[47,    32] loss: 0.528
[48,    32] loss: 0.529
[49,    32] loss: 0.529
[50,    32] loss: 0.529
[51,    32] loss: 0.530
Early stopping applied (best metric=0.46520018577575684)
Finished Training
Total time taken: 123.42617273330688
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.625
[3,    32] loss: 0.596
[4,    32] loss: 0.580
[5,    32] loss: 0.568
[6,    32] loss: 0.558
[7,    32] loss: 0.548
[8,    32] loss: 0.548
[9,    32] loss: 0.533
[10,    32] loss: 0.539
[11,    32] loss: 0.528
[12,    32] loss: 0.531
[13,    32] loss: 0.540
[14,    32] loss: 0.538
[15,    32] loss: 0.528
[16,    32] loss: 0.527
[17,    32] loss: 0.523
[18,    32] loss: 0.540
[19,    32] loss: 0.522
[20,    32] loss: 0.526
[21,    32] loss: 0.529
[22,    32] loss: 0.530
[23,    32] loss: 0.534
[24,    32] loss: 0.523
[25,    32] loss: 0.536
[26,    32] loss: 0.524
[27,    32] loss: 0.532
[28,    32] loss: 0.524
[29,    32] loss: 0.531
[30,    32] loss: 0.524
[31,    32] loss: 0.535
[32,    32] loss: 0.524
[33,    32] loss: 0.521
[34,    32] loss: 0.532
[35,    32] loss: 0.527
[36,    32] loss: 0.526
[37,    32] loss: 0.527
[38,    32] loss: 0.542
[39,    32] loss: 0.516
[40,    32] loss: 0.536
[41,    32] loss: 0.528
[42,    32] loss: 0.527
[43,    32] loss: 0.534
[44,    32] loss: 0.529
Early stopping applied (best metric=0.46290168166160583)
Finished Training
Total time taken: 105.21914768218994
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.613
[3,    32] loss: 0.588
[4,    32] loss: 0.577
[5,    32] loss: 0.556
[6,    32] loss: 0.549
[7,    32] loss: 0.545
[8,    32] loss: 0.545
[9,    32] loss: 0.540
[10,    32] loss: 0.539
[11,    32] loss: 0.542
[12,    32] loss: 0.535
[13,    32] loss: 0.534
[14,    32] loss: 0.538
[15,    32] loss: 0.537
[16,    32] loss: 0.536
[17,    32] loss: 0.523
[18,    32] loss: 0.532
[19,    32] loss: 0.528
[20,    32] loss: 0.533
[21,    32] loss: 0.534
[22,    32] loss: 0.526
[23,    32] loss: 0.527
[24,    32] loss: 0.533
[25,    32] loss: 0.533
[26,    32] loss: 0.527
[27,    32] loss: 0.527
[28,    32] loss: 0.528
[29,    32] loss: 0.518
[30,    32] loss: 0.532
[31,    32] loss: 0.531
[32,    32] loss: 0.535
[33,    32] loss: 0.522
[34,    32] loss: 0.533
[35,    32] loss: 0.534
[36,    32] loss: 0.533
[37,    32] loss: 0.525
[38,    32] loss: 0.530
[39,    32] loss: 0.522
[40,    32] loss: 0.533
[41,    32] loss: 0.533
[42,    32] loss: 0.535
[43,    32] loss: 0.526
[44,    32] loss: 0.529
[45,    32] loss: 0.531
[46,    32] loss: 0.532
[47,    32] loss: 0.534
Early stopping applied (best metric=0.48032450675964355)
Finished Training
Total time taken: 112.56515574455261
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.616
[3,    32] loss: 0.589
[4,    32] loss: 0.569
[5,    32] loss: 0.557
[6,    32] loss: 0.546
[7,    32] loss: 0.547
[8,    32] loss: 0.541
[9,    32] loss: 0.526
[10,    32] loss: 0.528
[11,    32] loss: 0.526
[12,    32] loss: 0.524
[13,    32] loss: 0.523
[14,    32] loss: 0.531
[15,    32] loss: 0.527
[16,    32] loss: 0.525
[17,    32] loss: 0.517
[18,    32] loss: 0.526
[19,    32] loss: 0.526
[20,    32] loss: 0.520
[21,    32] loss: 0.531
[22,    32] loss: 0.521
[23,    32] loss: 0.533
[24,    32] loss: 0.521
[25,    32] loss: 0.520
[26,    32] loss: 0.521
[27,    32] loss: 0.534
[28,    32] loss: 0.523
[29,    32] loss: 0.524
[30,    32] loss: 0.528
[31,    32] loss: 0.522
[32,    32] loss: 0.523
[33,    32] loss: 0.515
[34,    32] loss: 0.519
[35,    32] loss: 0.526
[36,    32] loss: 0.527
[37,    32] loss: 0.513
[38,    32] loss: 0.514
[39,    32] loss: 0.518
[40,    32] loss: 0.521
[41,    32] loss: 0.533
[42,    32] loss: 0.515
[43,    32] loss: 0.526
[44,    32] loss: 0.517
[45,    32] loss: 0.521
[46,    32] loss: 0.519
[47,    32] loss: 0.520
[48,    32] loss: 0.519
[49,    32] loss: 0.520
[50,    32] loss: 0.529
[51,    32] loss: 0.531
[52,    32] loss: 0.527
[53,    32] loss: 0.520
[54,    32] loss: 0.521
[55,    32] loss: 0.525
[56,    32] loss: 0.531
Early stopping applied (best metric=0.4812301695346832)
Finished Training
Total time taken: 134.17718410491943
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.619
[3,    32] loss: 0.591
[4,    32] loss: 0.577
[5,    32] loss: 0.567
[6,    32] loss: 0.546
[7,    32] loss: 0.548
[8,    32] loss: 0.538
[9,    32] loss: 0.537
[10,    32] loss: 0.540
[11,    32] loss: 0.534
[12,    32] loss: 0.533
[13,    32] loss: 0.534
[14,    32] loss: 0.524
[15,    32] loss: 0.530
[16,    32] loss: 0.532
[17,    32] loss: 0.521
[18,    32] loss: 0.531
[19,    32] loss: 0.520
[20,    32] loss: 0.523
[21,    32] loss: 0.533
[22,    32] loss: 0.520
[23,    32] loss: 0.530
[24,    32] loss: 0.530
[25,    32] loss: 0.521
[26,    32] loss: 0.528
[27,    32] loss: 0.529
[28,    32] loss: 0.528
[29,    32] loss: 0.520
[30,    32] loss: 0.541
[31,    32] loss: 0.522
[32,    32] loss: 0.523
[33,    32] loss: 0.538
[34,    32] loss: 0.523
[35,    32] loss: 0.525
[36,    32] loss: 0.522
[37,    32] loss: 0.528
[38,    32] loss: 0.533
[39,    32] loss: 0.532
[40,    32] loss: 0.520
[41,    32] loss: 0.527
[42,    32] loss: 0.528
Early stopping applied (best metric=0.4784028232097626)
Finished Training
Total time taken: 100.56414031982422
{'S-palmitoylation-C Validation Accuracy': 0.6666087260628746, 'S-palmitoylation-C Validation Sensitivity': 0.7124752475247524, 'S-palmitoylation-C Validation Specificity': 0.6551077938302847, 'S-palmitoylation-C Validation Precision': 0.3427428999624246, 'S-palmitoylation-C AUC ROC': 0.7513295967365605, 'S-palmitoylation-C AUC PR': 0.44909980260534577, 'S-palmitoylation-C MCC': 0.2995251287386079, 'S-palmitoylation-C F1': 0.46180280549521846, 'Validation Loss (S-palmitoylation-C)': 0.4736118733882904, 'Validation Loss (total)': 0.4736118733882904, 'TimeToTrain': 115.19036011695862}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009006954121826222,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4018103524,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.94681323894767}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.699
[2,    32] loss: 0.693
[3,    32] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005100725284274124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 715567923,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.793106580529551}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.639
[3,    32] loss: 0.621
[4,    32] loss: 0.607
[5,    32] loss: 0.595
[6,    32] loss: 0.580
[7,    32] loss: 0.556
[8,    32] loss: 0.545
[9,    32] loss: 0.541
[10,    32] loss: 0.530
[11,    32] loss: 0.514
[12,    32] loss: 0.508
[13,    32] loss: 0.500
[14,    32] loss: 0.494
[15,    32] loss: 0.487
[16,    32] loss: 0.491
[17,    32] loss: 0.479
[18,    32] loss: 0.467
[19,    32] loss: 0.470
[20,    32] loss: 0.468
[21,    32] loss: 0.476
[22,    32] loss: 0.474
[23,    32] loss: 0.458
[24,    32] loss: 0.469
[25,    32] loss: 0.453
[26,    32] loss: 0.458
[27,    32] loss: 0.458
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0059355783878175285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3698567109,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.030527955430036}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.687
[2,    32] loss: 0.617
[3,    32] loss: 0.609
[4,    32] loss: 0.610
[5,    32] loss: 0.602
[6,    32] loss: 0.598
[7,    32] loss: 0.603
[8,    32] loss: 0.597
[9,    32] loss: 0.604
[10,    32] loss: 0.597
[11,    32] loss: 0.599
[12,    32] loss: 0.602
[13,    32] loss: 0.599
[14,    32] loss: 0.601
[15,    32] loss: 0.595
[16,    32] loss: 0.599
[17,    32] loss: 0.599
[18,    32] loss: 0.604
[19,    32] loss: 0.607
[20,    32] loss: 0.601
[21,    32] loss: 0.607
[22,    32] loss: 0.604
[23,    32] loss: 0.602
[24,    32] loss: 0.600
[25,    32] loss: 0.601
[26,    32] loss: 0.602
[27,    32] loss: 0.609
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013446376791460084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2189896236,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.476036509292678}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.621
[3,    32] loss: 0.600
[4,    32] loss: 0.580
[5,    32] loss: 0.580
[6,    32] loss: 0.569
[7,    32] loss: 0.567
[8,    32] loss: 0.567
[9,    32] loss: 0.558
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024630108019503124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3611959557,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.565784399311172}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.613
[3,    32] loss: 0.610
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002155847991926937,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 991108731,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.156595395330768}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.611
[3,    32] loss: 0.583
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011415682381521381,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1335447743,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.064261817809605}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.619
[3,    32] loss: 0.602
[4,    32] loss: 0.587
[5,    32] loss: 0.573
[6,    32] loss: 0.570
[7,    32] loss: 0.558
[8,    32] loss: 0.563
[9,    32] loss: 0.559
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002690565909289201,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2152200369,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.070635714324649}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.627
[3,    32] loss: 0.608
[4,    32] loss: 0.595
[5,    32] loss: 0.603
[6,    32] loss: 0.595
[7,    32] loss: 0.595
[8,    32] loss: 0.589
[9,    32] loss: 0.599
[10,    32] loss: 0.595
[11,    32] loss: 0.594
[12,    32] loss: 0.597
[13,    32] loss: 0.597
[14,    32] loss: 0.591
[15,    32] loss: 0.585
[16,    32] loss: 0.592
[17,    32] loss: 0.592
[18,    32] loss: 0.596
[19,    32] loss: 0.593
[20,    32] loss: 0.593
[21,    32] loss: 0.590
[22,    32] loss: 0.590
[23,    32] loss: 0.596
[24,    32] loss: 0.593
[25,    32] loss: 0.594
[26,    32] loss: 0.598
[27,    32] loss: 0.595
[28,    32] loss: 0.595
[29,    32] loss: 0.589
[30,    32] loss: 0.603
[31,    32] loss: 0.587
[32,    32] loss: 0.594
[33,    32] loss: 0.596
[34,    32] loss: 0.592
Early stopping applied (best metric=0.47515255212783813)
Finished Training
Total time taken: 82.69811487197876
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.698
[2,    32] loss: 0.624
[3,    32] loss: 0.614
[4,    32] loss: 0.598
[5,    32] loss: 0.594
[6,    32] loss: 0.597
[7,    32] loss: 0.596
[8,    32] loss: 0.594
[9,    32] loss: 0.595
[10,    32] loss: 0.593
[11,    32] loss: 0.595
[12,    32] loss: 0.586
[13,    32] loss: 0.597
[14,    32] loss: 0.592
[15,    32] loss: 0.597
[16,    32] loss: 0.596
[17,    32] loss: 0.593
[18,    32] loss: 0.599
[19,    32] loss: 0.593
[20,    32] loss: 0.593
[21,    32] loss: 0.587
[22,    32] loss: 0.594
[23,    32] loss: 0.596
[24,    32] loss: 0.596
[25,    32] loss: 0.593
[26,    32] loss: 0.589
[27,    32] loss: 0.588
[28,    32] loss: 0.583
[29,    32] loss: 0.588
[30,    32] loss: 0.590
[31,    32] loss: 0.592
[32,    32] loss: 0.595
[33,    32] loss: 0.593
[34,    32] loss: 0.594
[35,    32] loss: 0.587
[36,    32] loss: 0.595
[37,    32] loss: 0.593
[38,    32] loss: 0.590
[39,    32] loss: 0.593
Early stopping applied (best metric=0.4825558662414551)
Finished Training
Total time taken: 93.5281310081482
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.618
[3,    32] loss: 0.604
[4,    32] loss: 0.599
[5,    32] loss: 0.601
[6,    32] loss: 0.600
[7,    32] loss: 0.592
[8,    32] loss: 0.589
[9,    32] loss: 0.596
[10,    32] loss: 0.607
[11,    32] loss: 0.593
[12,    32] loss: 0.596
[13,    32] loss: 0.586
[14,    32] loss: 0.591
[15,    32] loss: 0.591
[16,    32] loss: 0.599
[17,    32] loss: 0.586
[18,    32] loss: 0.597
[19,    32] loss: 0.589
[20,    32] loss: 0.589
[21,    32] loss: 0.592
[22,    32] loss: 0.592
[23,    32] loss: 0.592
[24,    32] loss: 0.604
[25,    32] loss: 0.595
[26,    32] loss: 0.595
[27,    32] loss: 0.589
[28,    32] loss: 0.595
[29,    32] loss: 0.587
[30,    32] loss: 0.587
[31,    32] loss: 0.590
[32,    32] loss: 0.586
[33,    32] loss: 0.590
[34,    32] loss: 0.595
[35,    32] loss: 0.598
[36,    32] loss: 0.599
[37,    32] loss: 0.592
[38,    32] loss: 0.596
[39,    32] loss: 0.591
[40,    32] loss: 0.597
[41,    32] loss: 0.598
[42,    32] loss: 0.590
[43,    32] loss: 0.596
[44,    32] loss: 0.590
[45,    32] loss: 0.585
[46,    32] loss: 0.588
[47,    32] loss: 0.593
[48,    32] loss: 0.596
[49,    32] loss: 0.594
[50,    32] loss: 0.589
[51,    32] loss: 0.591
[52,    32] loss: 0.597
[53,    32] loss: 0.594
[54,    32] loss: 0.592
[55,    32] loss: 0.589
[56,    32] loss: 0.592
[57,    32] loss: 0.595
[58,    32] loss: 0.595
[59,    32] loss: 0.592
[60,    32] loss: 0.588
Early stopping applied (best metric=0.49173638224601746)
Finished Training
Total time taken: 143.66120028495789
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.613
[3,    32] loss: 0.603
[4,    32] loss: 0.599
[5,    32] loss: 0.593
[6,    32] loss: 0.593
[7,    32] loss: 0.596
[8,    32] loss: 0.593
[9,    32] loss: 0.583
[10,    32] loss: 0.596
[11,    32] loss: 0.592
[12,    32] loss: 0.592
[13,    32] loss: 0.586
[14,    32] loss: 0.592
[15,    32] loss: 0.591
[16,    32] loss: 0.598
[17,    32] loss: 0.599
[18,    32] loss: 0.591
[19,    32] loss: 0.589
[20,    32] loss: 0.594
[21,    32] loss: 0.592
[22,    32] loss: 0.589
[23,    32] loss: 0.604
[24,    32] loss: 0.586
[25,    32] loss: 0.593
[26,    32] loss: 0.600
[27,    32] loss: 0.597
[28,    32] loss: 0.598
[29,    32] loss: 0.593
[30,    32] loss: 0.593
[31,    32] loss: 0.590
[32,    32] loss: 0.598
[33,    32] loss: 0.598
[34,    32] loss: 0.599
[35,    32] loss: 0.589
[36,    32] loss: 0.595
[37,    32] loss: 0.592
[38,    32] loss: 0.590
[39,    32] loss: 0.593
Early stopping applied (best metric=0.48386842012405396)
Finished Training
Total time taken: 93.27612948417664
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.700
[2,    32] loss: 0.624
[3,    32] loss: 0.604
[4,    32] loss: 0.601
[5,    32] loss: 0.596
[6,    32] loss: 0.590
[7,    32] loss: 0.594
[8,    32] loss: 0.595
[9,    32] loss: 0.595
[10,    32] loss: 0.583
[11,    32] loss: 0.593
[12,    32] loss: 0.591
[13,    32] loss: 0.592
[14,    32] loss: 0.604
[15,    32] loss: 0.596
[16,    32] loss: 0.609
[17,    32] loss: 0.595
[18,    32] loss: 0.594
[19,    32] loss: 0.602
[20,    32] loss: 0.589
[21,    32] loss: 0.595
[22,    32] loss: 0.593
[23,    32] loss: 0.597
[24,    32] loss: 0.593
[25,    32] loss: 0.600
[26,    32] loss: 0.595
[27,    32] loss: 0.590
[28,    32] loss: 0.592
[29,    32] loss: 0.602
[30,    32] loss: 0.597
[31,    32] loss: 0.592
[32,    32] loss: 0.592
[33,    32] loss: 0.596
[34,    32] loss: 0.594
[35,    32] loss: 0.595
[36,    32] loss: 0.595
[37,    32] loss: 0.597
[38,    32] loss: 0.602
[39,    32] loss: 0.595
[40,    32] loss: 0.596
[41,    32] loss: 0.600
[42,    32] loss: 0.591
[43,    32] loss: 0.597
[44,    32] loss: 0.588
[45,    32] loss: 0.601
[46,    32] loss: 0.594
[47,    32] loss: 0.600
[48,    32] loss: 0.587
[49,    32] loss: 0.602
[50,    32] loss: 0.593
[51,    32] loss: 0.593
Early stopping applied (best metric=0.48296239972114563)
Finished Training
Total time taken: 122.31517124176025
{'S-palmitoylation-C Validation Accuracy': 0.6583516071507338, 'S-palmitoylation-C Validation Sensitivity': 0.7021782178217822, 'S-palmitoylation-C Validation Specificity': 0.6473597965605525, 'S-palmitoylation-C Validation Precision': 0.3356252739417477, 'S-palmitoylation-C AUC ROC': 0.7402590470494954, 'S-palmitoylation-C AUC PR': 0.43323669378108876, 'S-palmitoylation-C MCC': 0.28555981882164794, 'S-palmitoylation-C F1': 0.4521784691810689, 'Validation Loss (S-palmitoylation-C)': 0.48325512409210203, 'Validation Loss (total)': 0.48325512409210203, 'TimeToTrain': 107.09574937820435}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001644584251064818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2879489156,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.075114130270848}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.634
[3,    32] loss: 0.618
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001202543086343666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 859467272,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.006466901039532}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.708
[2,    32] loss: 0.627
[3,    32] loss: 0.607
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011449428385024816,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 374271399,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.002724942982773}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.689
[2,    32] loss: 0.623
[3,    32] loss: 0.599
[4,    32] loss: 0.576
[5,    32] loss: 0.555
[6,    32] loss: 0.541
[7,    32] loss: 0.513
[8,    32] loss: 0.506
[9,    32] loss: 0.490
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007512101512278935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 773664137,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.85111481059414}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.642
[3,    32] loss: 0.622
[4,    32] loss: 0.614
[5,    32] loss: 0.618
[6,    32] loss: 0.614
[7,    32] loss: 0.610
[8,    32] loss: 0.608
[9,    32] loss: 0.607
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001268345868054535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1474231769,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.726537963527956}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.619
[3,    32] loss: 0.600
[4,    32] loss: 0.574
[5,    32] loss: 0.564
[6,    32] loss: 0.549
[7,    32] loss: 0.536
[8,    32] loss: 0.531
[9,    32] loss: 0.530
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['S-palmitoylation-C'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 8.815923586760147e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1241704368,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.658986245090434}
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.691
[3,    32] loss: 0.684
[4,    32] loss: 0.661
[5,    32] loss: 0.641
[6,    32] loss: 0.625
[7,    32] loss: 0.617
[8,    32] loss: 0.611
[9,    32] loss: 0.603
[10,    32] loss: 0.599
[11,    32] loss: 0.591
[12,    32] loss: 0.595
[13,    32] loss: 0.591
[14,    32] loss: 0.587
[15,    32] loss: 0.582
[16,    32] loss: 0.579
[17,    32] loss: 0.573
[18,    32] loss: 0.570
[19,    32] loss: 0.564
[20,    32] loss: 0.558
[21,    32] loss: 0.551
[22,    32] loss: 0.545
[23,    32] loss: 0.539
[24,    32] loss: 0.529
[25,    32] loss: 0.521
[26,    32] loss: 0.512
[27,    32] loss: 0.501
[28,    32] loss: 0.497
[29,    32] loss: 0.495
[30,    32] loss: 0.482
[31,    32] loss: 0.476
[32,    32] loss: 0.469
[33,    32] loss: 0.470
[34,    32] loss: 0.459
[35,    32] loss: 0.457
[36,    32] loss: 0.441
[37,    32] loss: 0.443
[38,    32] loss: 0.430
[39,    32] loss: 0.422
[40,    32] loss: 0.403
[41,    32] loss: 0.402
[42,    32] loss: 0.404
[43,    32] loss: 0.396
[44,    32] loss: 0.389
[45,    32] loss: 0.378
[46,    32] loss: 0.381
[47,    32] loss: 0.381
[48,    32] loss: 0.371
[49,    32] loss: 0.370
[50,    32] loss: 0.363
[51,    32] loss: 0.355
[52,    32] loss: 0.352
[53,    32] loss: 0.356
[54,    32] loss: 0.356
Early stopping applied (best metric=0.47181838750839233)
Finished Training
Total time taken: 129.06333231925964
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.681
[4,    32] loss: 0.657
[5,    32] loss: 0.637
[6,    32] loss: 0.627
[7,    32] loss: 0.617
[8,    32] loss: 0.608
[9,    32] loss: 0.610
[10,    32] loss: 0.599
[11,    32] loss: 0.600
[12,    32] loss: 0.591
[13,    32] loss: 0.590
[14,    32] loss: 0.584
[15,    32] loss: 0.581
[16,    32] loss: 0.576
[17,    32] loss: 0.571
[18,    32] loss: 0.567
[19,    32] loss: 0.561
[20,    32] loss: 0.554
[21,    32] loss: 0.549
[22,    32] loss: 0.543
[23,    32] loss: 0.536
[24,    32] loss: 0.528
[25,    32] loss: 0.516
[26,    32] loss: 0.515
[27,    32] loss: 0.495
[28,    32] loss: 0.487
[29,    32] loss: 0.476
[30,    32] loss: 0.475
[31,    32] loss: 0.455
[32,    32] loss: 0.452
[33,    32] loss: 0.451
[34,    32] loss: 0.446
[35,    32] loss: 0.437
[36,    32] loss: 0.430
[37,    32] loss: 0.424
[38,    32] loss: 0.413
[39,    32] loss: 0.417
[40,    32] loss: 0.403
[41,    32] loss: 0.400
[42,    32] loss: 0.399
[43,    32] loss: 0.385
[44,    32] loss: 0.382
[45,    32] loss: 0.377
[46,    32] loss: 0.371
[47,    32] loss: 0.372
[48,    32] loss: 0.366
[49,    32] loss: 0.364
[50,    32] loss: 0.367
[51,    32] loss: 0.359
[52,    32] loss: 0.357
Early stopping applied (best metric=0.47818875312805176)
Finished Training
Total time taken: 124.38208603858948
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.690
[3,    32] loss: 0.680
[4,    32] loss: 0.658
[5,    32] loss: 0.637
[6,    32] loss: 0.619
[7,    32] loss: 0.615
[8,    32] loss: 0.612
[9,    32] loss: 0.601
[10,    32] loss: 0.607
[11,    32] loss: 0.598
[12,    32] loss: 0.597
[13,    32] loss: 0.592
[14,    32] loss: 0.581
[15,    32] loss: 0.582
[16,    32] loss: 0.581
[17,    32] loss: 0.574
[18,    32] loss: 0.568
[19,    32] loss: 0.569
[20,    32] loss: 0.559
[21,    32] loss: 0.552
[22,    32] loss: 0.547
[23,    32] loss: 0.529
[24,    32] loss: 0.523
[25,    32] loss: 0.520
[26,    32] loss: 0.515
[27,    32] loss: 0.503
[28,    32] loss: 0.505
[29,    32] loss: 0.494
[30,    32] loss: 0.483
[31,    32] loss: 0.479
[32,    32] loss: 0.460
[33,    32] loss: 0.453
[34,    32] loss: 0.447
[35,    32] loss: 0.441
[36,    32] loss: 0.437
[37,    32] loss: 0.430
[38,    32] loss: 0.420
[39,    32] loss: 0.424
[40,    32] loss: 0.412
[41,    32] loss: 0.401
[42,    32] loss: 0.394
[43,    32] loss: 0.387
[44,    32] loss: 0.387
[45,    32] loss: 0.384
[46,    32] loss: 0.374
[47,    32] loss: 0.374
[48,    32] loss: 0.363
[49,    32] loss: 0.365
Early stopping applied (best metric=0.46720629930496216)
Finished Training
Total time taken: 117.28422594070435
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.690
[3,    32] loss: 0.677
[4,    32] loss: 0.651
[5,    32] loss: 0.628
[6,    32] loss: 0.620
[7,    32] loss: 0.613
[8,    32] loss: 0.612
[9,    32] loss: 0.607
[10,    32] loss: 0.605
[11,    32] loss: 0.596
[12,    32] loss: 0.587
[13,    32] loss: 0.586
[14,    32] loss: 0.582
[15,    32] loss: 0.581
[16,    32] loss: 0.579
[17,    32] loss: 0.571
[18,    32] loss: 0.566
[19,    32] loss: 0.560
[20,    32] loss: 0.556
[21,    32] loss: 0.550
[22,    32] loss: 0.538
[23,    32] loss: 0.530
[24,    32] loss: 0.529
[25,    32] loss: 0.518
[26,    32] loss: 0.510
[27,    32] loss: 0.505
[28,    32] loss: 0.496
[29,    32] loss: 0.489
[30,    32] loss: 0.480
[31,    32] loss: 0.472
[32,    32] loss: 0.467
[33,    32] loss: 0.458
[34,    32] loss: 0.453
[35,    32] loss: 0.447
[36,    32] loss: 0.442
[37,    32] loss: 0.430
[38,    32] loss: 0.405
[39,    32] loss: 0.410
[40,    32] loss: 0.408
[41,    32] loss: 0.395
[42,    32] loss: 0.390
[43,    32] loss: 0.385
[44,    32] loss: 0.378
[45,    32] loss: 0.381
[46,    32] loss: 0.378
[47,    32] loss: 0.364
[48,    32] loss: 0.366
[49,    32] loss: 0.362
[50,    32] loss: 0.351
[51,    32] loss: 0.354
[52,    32] loss: 0.345
[53,    32] loss: 0.340
[54,    32] loss: 0.339
Early stopping applied (best metric=0.4715667963027954)
Finished Training
Total time taken: 129.9814648628235
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.691
[3,    32] loss: 0.682
[4,    32] loss: 0.662
[5,    32] loss: 0.640
[6,    32] loss: 0.628
[7,    32] loss: 0.619
[8,    32] loss: 0.612
[9,    32] loss: 0.605
[10,    32] loss: 0.602
[11,    32] loss: 0.595
[12,    32] loss: 0.592
[13,    32] loss: 0.588
[14,    32] loss: 0.582
[15,    32] loss: 0.583
[16,    32] loss: 0.571
[17,    32] loss: 0.576
[18,    32] loss: 0.564
[19,    32] loss: 0.559
[20,    32] loss: 0.547
[21,    32] loss: 0.545
[22,    32] loss: 0.538
[23,    32] loss: 0.528
[24,    32] loss: 0.524
[25,    32] loss: 0.502
[26,    32] loss: 0.505
[27,    32] loss: 0.501
[28,    32] loss: 0.486
[29,    32] loss: 0.476
[30,    32] loss: 0.470
[31,    32] loss: 0.461
[32,    32] loss: 0.461
[33,    32] loss: 0.452
[34,    32] loss: 0.442
[35,    32] loss: 0.439
[36,    32] loss: 0.424
[37,    32] loss: 0.415
[38,    32] loss: 0.412
[39,    32] loss: 0.404
[40,    32] loss: 0.400
[41,    32] loss: 0.391
[42,    32] loss: 0.397
[43,    32] loss: 0.388
[44,    32] loss: 0.369
[45,    32] loss: 0.383
[46,    32] loss: 0.381
Early stopping applied (best metric=0.4812716543674469)
Finished Training
Total time taken: 110.49115324020386
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.691
[3,    32] loss: 0.683
[4,    32] loss: 0.666
[5,    32] loss: 0.648
[6,    32] loss: 0.632
[7,    32] loss: 0.623
[8,    32] loss: 0.619
[9,    32] loss: 0.612
[10,    32] loss: 0.612
[11,    32] loss: 0.606
[12,    32] loss: 0.601
[13,    32] loss: 0.598
[14,    32] loss: 0.589
[15,    32] loss: 0.590
[16,    32] loss: 0.584
[17,    32] loss: 0.580
[18,    32] loss: 0.568
[19,    32] loss: 0.560
[20,    32] loss: 0.556
[21,    32] loss: 0.551
[22,    32] loss: 0.543
[23,    32] loss: 0.537
[24,    32] loss: 0.526
[25,    32] loss: 0.521
[26,    32] loss: 0.509
[27,    32] loss: 0.509
[28,    32] loss: 0.494
[29,    32] loss: 0.487
[30,    32] loss: 0.481
[31,    32] loss: 0.476
[32,    32] loss: 0.462
[33,    32] loss: 0.455
[34,    32] loss: 0.442
[35,    32] loss: 0.448
[36,    32] loss: 0.436
[37,    32] loss: 0.426
[38,    32] loss: 0.422
[39,    32] loss: 0.421
[40,    32] loss: 0.408
[41,    32] loss: 0.409
[42,    32] loss: 0.400
[43,    32] loss: 0.394
[44,    32] loss: 0.386
[45,    32] loss: 0.378
[46,    32] loss: 0.382
[47,    32] loss: 0.368
[48,    32] loss: 0.371
Early stopping applied (best metric=0.4639039635658264)
Finished Training
Total time taken: 115.43978953361511
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.691
[3,    32] loss: 0.680
[4,    32] loss: 0.656
[5,    32] loss: 0.640
[6,    32] loss: 0.624
[7,    32] loss: 0.622
[8,    32] loss: 0.607
[9,    32] loss: 0.609
[10,    32] loss: 0.603
[11,    32] loss: 0.602
[12,    32] loss: 0.596
[13,    32] loss: 0.586
[14,    32] loss: 0.586
[15,    32] loss: 0.581
[16,    32] loss: 0.581
[17,    32] loss: 0.567
[18,    32] loss: 0.566
[19,    32] loss: 0.562
[20,    32] loss: 0.556
[21,    32] loss: 0.547
[22,    32] loss: 0.539
[23,    32] loss: 0.536
[24,    32] loss: 0.526
[25,    32] loss: 0.520
[26,    32] loss: 0.507
[27,    32] loss: 0.503
[28,    32] loss: 0.491
[29,    32] loss: 0.482
[30,    32] loss: 0.478
[31,    32] loss: 0.460
[32,    32] loss: 0.460
[33,    32] loss: 0.458
[34,    32] loss: 0.444
[35,    32] loss: 0.448
[36,    32] loss: 0.436
[37,    32] loss: 0.427
[38,    32] loss: 0.421
[39,    32] loss: 0.413
[40,    32] loss: 0.413
[41,    32] loss: 0.396
[42,    32] loss: 0.395
[43,    32] loss: 0.387
[44,    32] loss: 0.393
[45,    32] loss: 0.388
[46,    32] loss: 0.373
[47,    32] loss: 0.373
[48,    32] loss: 0.365
Early stopping applied (best metric=0.4730130732059479)
Finished Training
Total time taken: 115.14140725135803
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.691
[3,    32] loss: 0.681
[4,    32] loss: 0.662
[5,    32] loss: 0.644
[6,    32] loss: 0.633
[7,    32] loss: 0.624
[8,    32] loss: 0.615
[9,    32] loss: 0.612
[10,    32] loss: 0.606
[11,    32] loss: 0.602
[12,    32] loss: 0.596
[13,    32] loss: 0.595
[14,    32] loss: 0.590
[15,    32] loss: 0.592
[16,    32] loss: 0.585
[17,    32] loss: 0.576
[18,    32] loss: 0.575
[19,    32] loss: 0.564
[20,    32] loss: 0.566
[21,    32] loss: 0.553
[22,    32] loss: 0.555
[23,    32] loss: 0.544
[24,    32] loss: 0.541
[25,    32] loss: 0.532
[26,    32] loss: 0.528
[27,    32] loss: 0.511
[28,    32] loss: 0.499
[29,    32] loss: 0.502
[30,    32] loss: 0.487
[31,    32] loss: 0.482
[32,    32] loss: 0.476
[33,    32] loss: 0.469
[34,    32] loss: 0.453
[35,    32] loss: 0.454
[36,    32] loss: 0.450
[37,    32] loss: 0.441
[38,    32] loss: 0.437
[39,    32] loss: 0.435
[40,    32] loss: 0.420
[41,    32] loss: 0.413
[42,    32] loss: 0.404
[43,    32] loss: 0.400
[44,    32] loss: 0.402
[45,    32] loss: 0.394
[46,    32] loss: 0.385
[47,    32] loss: 0.381
[48,    32] loss: 0.372
[49,    32] loss: 0.365
[50,    32] loss: 0.363
[51,    32] loss: 0.356
[52,    32] loss: 0.363
Early stopping applied (best metric=0.46599581837654114)
Finished Training
Total time taken: 125.4836778640747
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.690
[3,    32] loss: 0.682
[4,    32] loss: 0.662
[5,    32] loss: 0.639
[6,    32] loss: 0.630
[7,    32] loss: 0.625
[8,    32] loss: 0.618
[9,    32] loss: 0.611
[10,    32] loss: 0.603
[11,    32] loss: 0.597
[12,    32] loss: 0.594
[13,    32] loss: 0.586
[14,    32] loss: 0.590
[15,    32] loss: 0.583
[16,    32] loss: 0.579
[17,    32] loss: 0.569
[18,    32] loss: 0.563
[19,    32] loss: 0.549
[20,    32] loss: 0.550
[21,    32] loss: 0.537
[22,    32] loss: 0.528
[23,    32] loss: 0.528
[24,    32] loss: 0.517
[25,    32] loss: 0.514
[26,    32] loss: 0.505
[27,    32] loss: 0.494
[28,    32] loss: 0.483
[29,    32] loss: 0.475
[30,    32] loss: 0.474
[31,    32] loss: 0.466
[32,    32] loss: 0.459
[33,    32] loss: 0.451
[34,    32] loss: 0.444
[35,    32] loss: 0.442
[36,    32] loss: 0.428
[37,    32] loss: 0.426
[38,    32] loss: 0.417
[39,    32] loss: 0.409
[40,    32] loss: 0.401
[41,    32] loss: 0.401
[42,    32] loss: 0.401
[43,    32] loss: 0.388
[44,    32] loss: 0.383
[45,    32] loss: 0.378
[46,    32] loss: 0.374
[47,    32] loss: 0.364
[48,    32] loss: 0.369
Early stopping applied (best metric=0.4751644432544708)
Finished Training
Total time taken: 115.86617422103882
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.691
[3,    32] loss: 0.682
[4,    32] loss: 0.665
[5,    32] loss: 0.638
[6,    32] loss: 0.627
[7,    32] loss: 0.615
[8,    32] loss: 0.610
[9,    32] loss: 0.607
[10,    32] loss: 0.597
[11,    32] loss: 0.596
[12,    32] loss: 0.593
[13,    32] loss: 0.585
[14,    32] loss: 0.584
[15,    32] loss: 0.576
[16,    32] loss: 0.574
[17,    32] loss: 0.565
[18,    32] loss: 0.558
[19,    32] loss: 0.560
[20,    32] loss: 0.552
[21,    32] loss: 0.544
[22,    32] loss: 0.538
[23,    32] loss: 0.527
[24,    32] loss: 0.525
[25,    32] loss: 0.513
[26,    32] loss: 0.507
[27,    32] loss: 0.500
[28,    32] loss: 0.493
[29,    32] loss: 0.490
[30,    32] loss: 0.476
[31,    32] loss: 0.466
[32,    32] loss: 0.467
[33,    32] loss: 0.457
[34,    32] loss: 0.453
[35,    32] loss: 0.442
[36,    32] loss: 0.432
[37,    32] loss: 0.423
[38,    32] loss: 0.418
[39,    32] loss: 0.414
[40,    32] loss: 0.407
[41,    32] loss: 0.398
[42,    32] loss: 0.399
[43,    32] loss: 0.390
[44,    32] loss: 0.389
[45,    32] loss: 0.384
[46,    32] loss: 0.377
[47,    32] loss: 0.373
[48,    32] loss: 0.372
[49,    32] loss: 0.371
[50,    32] loss: 0.354
[51,    32] loss: 0.356
Early stopping applied (best metric=0.4748542904853821)
Finished Training
Total time taken: 122.40461206436157
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.691
[3,    32] loss: 0.679
[4,    32] loss: 0.655
[5,    32] loss: 0.634
[6,    32] loss: 0.622
[7,    32] loss: 0.615
[8,    32] loss: 0.611
[9,    32] loss: 0.606
[10,    32] loss: 0.599
[11,    32] loss: 0.599
[12,    32] loss: 0.594
[13,    32] loss: 0.590
[14,    32] loss: 0.581
[15,    32] loss: 0.581
[16,    32] loss: 0.579
[17,    32] loss: 0.577
[18,    32] loss: 0.570
[19,    32] loss: 0.560
[20,    32] loss: 0.554
[21,    32] loss: 0.549
[22,    32] loss: 0.544
[23,    32] loss: 0.535
[24,    32] loss: 0.531
[25,    32] loss: 0.523
[26,    32] loss: 0.511
[27,    32] loss: 0.503
[28,    32] loss: 0.489
[29,    32] loss: 0.488
[30,    32] loss: 0.474
[31,    32] loss: 0.470
[32,    32] loss: 0.471
[33,    32] loss: 0.458
[34,    32] loss: 0.447
[35,    32] loss: 0.442
[36,    32] loss: 0.433
[37,    32] loss: 0.424
[38,    32] loss: 0.417
[39,    32] loss: 0.420
[40,    32] loss: 0.402
[41,    32] loss: 0.405
[42,    32] loss: 0.393
[43,    32] loss: 0.389
[44,    32] loss: 0.384
[45,    32] loss: 0.388
[46,    32] loss: 0.378
[47,    32] loss: 0.375
[48,    32] loss: 0.367
[49,    32] loss: 0.364
[50,    32] loss: 0.366
Early stopping applied (best metric=0.4834473133087158)
Finished Training
Total time taken: 120.72415924072266
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.691
[3,    32] loss: 0.683
[4,    32] loss: 0.658
[5,    32] loss: 0.635
[6,    32] loss: 0.627
[7,    32] loss: 0.615
[8,    32] loss: 0.611
[9,    32] loss: 0.602
[10,    32] loss: 0.602
[11,    32] loss: 0.594
[12,    32] loss: 0.593
[13,    32] loss: 0.585
[14,    32] loss: 0.584
[15,    32] loss: 0.578
[16,    32] loss: 0.577
[17,    32] loss: 0.569
[18,    32] loss: 0.562
[19,    32] loss: 0.559
[20,    32] loss: 0.563
[21,    32] loss: 0.548
[22,    32] loss: 0.540
[23,    32] loss: 0.533
[24,    32] loss: 0.525
[25,    32] loss: 0.519
[26,    32] loss: 0.505
[27,    32] loss: 0.500
[28,    32] loss: 0.490
[29,    32] loss: 0.487
[30,    32] loss: 0.484
[31,    32] loss: 0.473
[32,    32] loss: 0.465
[33,    32] loss: 0.459
[34,    32] loss: 0.451
[35,    32] loss: 0.445
[36,    32] loss: 0.431
[37,    32] loss: 0.423
[38,    32] loss: 0.418
[39,    32] loss: 0.414
[40,    32] loss: 0.412
[41,    32] loss: 0.395
[42,    32] loss: 0.397
[43,    32] loss: 0.395
[44,    32] loss: 0.382
[45,    32] loss: 0.383
[46,    32] loss: 0.367
[47,    32] loss: 0.368
[48,    32] loss: 0.364
[49,    32] loss: 0.359
Early stopping applied (best metric=0.4690844416618347)
Finished Training
Total time taken: 118.16928315162659
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.695
[2,    32] loss: 0.691
[3,    32] loss: 0.682
[4,    32] loss: 0.659
[5,    32] loss: 0.633
[6,    32] loss: 0.617
[7,    32] loss: 0.611
[8,    32] loss: 0.606
[9,    32] loss: 0.605
[10,    32] loss: 0.595
[11,    32] loss: 0.596
[12,    32] loss: 0.586
[13,    32] loss: 0.587
[14,    32] loss: 0.588
[15,    32] loss: 0.582
[16,    32] loss: 0.577
[17,    32] loss: 0.571
[18,    32] loss: 0.563
[19,    32] loss: 0.554
[20,    32] loss: 0.553
[21,    32] loss: 0.543
[22,    32] loss: 0.537
[23,    32] loss: 0.526
[24,    32] loss: 0.515
[25,    32] loss: 0.504
[26,    32] loss: 0.507
[27,    32] loss: 0.490
[28,    32] loss: 0.481
[29,    32] loss: 0.480
[30,    32] loss: 0.466
[31,    32] loss: 0.460
[32,    32] loss: 0.448
[33,    32] loss: 0.446
[34,    32] loss: 0.432
[35,    32] loss: 0.429
[36,    32] loss: 0.425
[37,    32] loss: 0.415
[38,    32] loss: 0.408
[39,    32] loss: 0.414
[40,    32] loss: 0.389
[41,    32] loss: 0.392
[42,    32] loss: 0.392
[43,    32] loss: 0.384
Early stopping applied (best metric=0.48695069551467896)
Finished Training
Total time taken: 104.53751635551453
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.693
[3,    32] loss: 0.686
[4,    32] loss: 0.669
[5,    32] loss: 0.643
[6,    32] loss: 0.631
[7,    32] loss: 0.618
[8,    32] loss: 0.609
[9,    32] loss: 0.606
[10,    32] loss: 0.605
[11,    32] loss: 0.599
[12,    32] loss: 0.595
[13,    32] loss: 0.594
[14,    32] loss: 0.587
[15,    32] loss: 0.589
[16,    32] loss: 0.582
[17,    32] loss: 0.579
[18,    32] loss: 0.564
[19,    32] loss: 0.562
[20,    32] loss: 0.560
[21,    32] loss: 0.554
[22,    32] loss: 0.540
[23,    32] loss: 0.537
[24,    32] loss: 0.529
[25,    32] loss: 0.523
[26,    32] loss: 0.516
[27,    32] loss: 0.500
[28,    32] loss: 0.495
[29,    32] loss: 0.486
[30,    32] loss: 0.483
[31,    32] loss: 0.470
[32,    32] loss: 0.463
[33,    32] loss: 0.455
[34,    32] loss: 0.442
[35,    32] loss: 0.437
[36,    32] loss: 0.435
[37,    32] loss: 0.420
[38,    32] loss: 0.421
[39,    32] loss: 0.412
[40,    32] loss: 0.408
[41,    32] loss: 0.403
[42,    32] loss: 0.391
[43,    32] loss: 0.397
[44,    32] loss: 0.385
[45,    32] loss: 0.383
[46,    32] loss: 0.379
[47,    32] loss: 0.361
[48,    32] loss: 0.359
[49,    32] loss: 0.364
[50,    32] loss: 0.358
[51,    32] loss: 0.350
[52,    32] loss: 0.348
Early stopping applied (best metric=0.4752209484577179)
Finished Training
Total time taken: 126.07387137413025
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.688
[3,    32] loss: 0.675
[4,    32] loss: 0.653
[5,    32] loss: 0.634
[6,    32] loss: 0.623
[7,    32] loss: 0.614
[8,    32] loss: 0.612
[9,    32] loss: 0.612
[10,    32] loss: 0.604
[11,    32] loss: 0.601
[12,    32] loss: 0.589
[13,    32] loss: 0.590
[14,    32] loss: 0.587
[15,    32] loss: 0.586
[16,    32] loss: 0.578
[17,    32] loss: 0.577
[18,    32] loss: 0.570
[19,    32] loss: 0.563
[20,    32] loss: 0.550
[21,    32] loss: 0.536
[22,    32] loss: 0.544
[23,    32] loss: 0.524
[24,    32] loss: 0.523
[25,    32] loss: 0.521
[26,    32] loss: 0.510
[27,    32] loss: 0.499
[28,    32] loss: 0.496
[29,    32] loss: 0.482
[30,    32] loss: 0.483
[31,    32] loss: 0.468
[32,    32] loss: 0.468
[33,    32] loss: 0.454
[34,    32] loss: 0.452
[35,    32] loss: 0.450
[36,    32] loss: 0.441
[37,    32] loss: 0.422
[38,    32] loss: 0.417
[39,    32] loss: 0.418
[40,    32] loss: 0.406
[41,    32] loss: 0.406
[42,    32] loss: 0.391
[43,    32] loss: 0.388
[44,    32] loss: 0.392
[45,    32] loss: 0.381
[46,    32] loss: 0.374
[47,    32] loss: 0.371
Early stopping applied (best metric=0.472525030374527)
Finished Training
Total time taken: 114.08667635917664
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.697
[2,    32] loss: 0.692
[3,    32] loss: 0.684
[4,    32] loss: 0.665
[5,    32] loss: 0.644
[6,    32] loss: 0.626
[7,    32] loss: 0.619
[8,    32] loss: 0.616
[9,    32] loss: 0.606
[10,    32] loss: 0.602
[11,    32] loss: 0.595
[12,    32] loss: 0.592
[13,    32] loss: 0.587
[14,    32] loss: 0.579
[15,    32] loss: 0.576
[16,    32] loss: 0.574
[17,    32] loss: 0.574
[18,    32] loss: 0.568
[19,    32] loss: 0.558
[20,    32] loss: 0.553
[21,    32] loss: 0.550
[22,    32] loss: 0.535
[23,    32] loss: 0.536
[24,    32] loss: 0.526
[25,    32] loss: 0.520
[26,    32] loss: 0.508
[27,    32] loss: 0.492
[28,    32] loss: 0.485
[29,    32] loss: 0.477
[30,    32] loss: 0.482
[31,    32] loss: 0.471
[32,    32] loss: 0.461
[33,    32] loss: 0.447
[34,    32] loss: 0.443
[35,    32] loss: 0.441
[36,    32] loss: 0.430
[37,    32] loss: 0.418
[38,    32] loss: 0.421
[39,    32] loss: 0.416
[40,    32] loss: 0.418
[41,    32] loss: 0.403
[42,    32] loss: 0.391
[43,    32] loss: 0.396
[44,    32] loss: 0.384
[45,    32] loss: 0.382
[46,    32] loss: 0.370
[47,    32] loss: 0.378
[48,    32] loss: 0.366
Early stopping applied (best metric=0.4796258509159088)
Finished Training
Total time taken: 116.50465536117554
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.680
[4,    32] loss: 0.661
[5,    32] loss: 0.644
[6,    32] loss: 0.630
[7,    32] loss: 0.621
[8,    32] loss: 0.614
[9,    32] loss: 0.612
[10,    32] loss: 0.601
[11,    32] loss: 0.602
[12,    32] loss: 0.604
[13,    32] loss: 0.586
[14,    32] loss: 0.588
[15,    32] loss: 0.584
[16,    32] loss: 0.585
[17,    32] loss: 0.576
[18,    32] loss: 0.573
[19,    32] loss: 0.562
[20,    32] loss: 0.565
[21,    32] loss: 0.550
[22,    32] loss: 0.546
[23,    32] loss: 0.536
[24,    32] loss: 0.518
[25,    32] loss: 0.521
[26,    32] loss: 0.512
[27,    32] loss: 0.509
[28,    32] loss: 0.487
[29,    32] loss: 0.482
[30,    32] loss: 0.473
[31,    32] loss: 0.475
[32,    32] loss: 0.465
[33,    32] loss: 0.454
[34,    32] loss: 0.452
[35,    32] loss: 0.437
[36,    32] loss: 0.432
[37,    32] loss: 0.424
[38,    32] loss: 0.418
[39,    32] loss: 0.417
[40,    32] loss: 0.402
[41,    32] loss: 0.399
[42,    32] loss: 0.400
[43,    32] loss: 0.388
[44,    32] loss: 0.385
[45,    32] loss: 0.393
[46,    32] loss: 0.380
[47,    32] loss: 0.381
[48,    32] loss: 0.368
[49,    32] loss: 0.364
[50,    32] loss: 0.357
Early stopping applied (best metric=0.4610421359539032)
Finished Training
Total time taken: 121.60598754882812
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.679
[4,    32] loss: 0.655
[5,    32] loss: 0.637
[6,    32] loss: 0.625
[7,    32] loss: 0.622
[8,    32] loss: 0.611
[9,    32] loss: 0.610
[10,    32] loss: 0.603
[11,    32] loss: 0.601
[12,    32] loss: 0.596
[13,    32] loss: 0.595
[14,    32] loss: 0.585
[15,    32] loss: 0.585
[16,    32] loss: 0.576
[17,    32] loss: 0.579
[18,    32] loss: 0.570
[19,    32] loss: 0.565
[20,    32] loss: 0.560
[21,    32] loss: 0.548
[22,    32] loss: 0.543
[23,    32] loss: 0.540
[24,    32] loss: 0.531
[25,    32] loss: 0.517
[26,    32] loss: 0.510
[27,    32] loss: 0.499
[28,    32] loss: 0.489
[29,    32] loss: 0.488
[30,    32] loss: 0.479
[31,    32] loss: 0.468
[32,    32] loss: 0.469
[33,    32] loss: 0.457
[34,    32] loss: 0.450
[35,    32] loss: 0.444
[36,    32] loss: 0.437
[37,    32] loss: 0.425
[38,    32] loss: 0.422
[39,    32] loss: 0.418
[40,    32] loss: 0.412
[41,    32] loss: 0.410
[42,    32] loss: 0.389
[43,    32] loss: 0.395
[44,    32] loss: 0.391
[45,    32] loss: 0.382
[46,    32] loss: 0.374
[47,    32] loss: 0.374
[48,    32] loss: 0.370
[49,    32] loss: 0.369
Early stopping applied (best metric=0.4630330502986908)
Finished Training
Total time taken: 118.5931646823883
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.694
[2,    32] loss: 0.690
[3,    32] loss: 0.681
[4,    32] loss: 0.656
[5,    32] loss: 0.637
[6,    32] loss: 0.622
[7,    32] loss: 0.622
[8,    32] loss: 0.609
[9,    32] loss: 0.606
[10,    32] loss: 0.607
[11,    32] loss: 0.603
[12,    32] loss: 0.593
[13,    32] loss: 0.588
[14,    32] loss: 0.588
[15,    32] loss: 0.579
[16,    32] loss: 0.577
[17,    32] loss: 0.580
[18,    32] loss: 0.572
[19,    32] loss: 0.564
[20,    32] loss: 0.556
[21,    32] loss: 0.553
[22,    32] loss: 0.545
[23,    32] loss: 0.530
[24,    32] loss: 0.527
[25,    32] loss: 0.525
[26,    32] loss: 0.510
[27,    32] loss: 0.500
[28,    32] loss: 0.499
[29,    32] loss: 0.495
[30,    32] loss: 0.479
[31,    32] loss: 0.473
[32,    32] loss: 0.463
[33,    32] loss: 0.454
[34,    32] loss: 0.454
[35,    32] loss: 0.441
[36,    32] loss: 0.434
[37,    32] loss: 0.427
[38,    32] loss: 0.432
[39,    32] loss: 0.423
[40,    32] loss: 0.416
[41,    32] loss: 0.406
[42,    32] loss: 0.396
[43,    32] loss: 0.394
[44,    32] loss: 0.390
[45,    32] loss: 0.386
[46,    32] loss: 0.386
[47,    32] loss: 0.369
[48,    32] loss: 0.370
Early stopping applied (best metric=0.4776861071586609)
Finished Training
Total time taken: 116.31920528411865
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.692
[2,    32] loss: 0.692
[3,    32] loss: 0.684
[4,    32] loss: 0.661
[5,    32] loss: 0.642
[6,    32] loss: 0.627
[7,    32] loss: 0.618
[8,    32] loss: 0.616
[9,    32] loss: 0.610
[10,    32] loss: 0.601
[11,    32] loss: 0.603
[12,    32] loss: 0.598
[13,    32] loss: 0.596
[14,    32] loss: 0.589
[15,    32] loss: 0.588
[16,    32] loss: 0.580
[17,    32] loss: 0.572
[18,    32] loss: 0.565
[19,    32] loss: 0.562
[20,    32] loss: 0.553
[21,    32] loss: 0.550
[22,    32] loss: 0.549
[23,    32] loss: 0.534
[24,    32] loss: 0.529
[25,    32] loss: 0.515
[26,    32] loss: 0.516
[27,    32] loss: 0.507
[28,    32] loss: 0.501
[29,    32] loss: 0.490
[30,    32] loss: 0.484
[31,    32] loss: 0.476
[32,    32] loss: 0.473
[33,    32] loss: 0.460
[34,    32] loss: 0.463
[35,    32] loss: 0.450
[36,    32] loss: 0.444
[37,    32] loss: 0.432
[38,    32] loss: 0.422
[39,    32] loss: 0.419
[40,    32] loss: 0.409
[41,    32] loss: 0.399
[42,    32] loss: 0.411
[43,    32] loss: 0.392
[44,    32] loss: 0.390
[45,    32] loss: 0.389
[46,    32] loss: 0.376
[47,    32] loss: 0.377
[48,    32] loss: 0.374
[49,    32] loss: 0.367
[50,    32] loss: 0.364
[51,    32] loss: 0.354
Early stopping applied (best metric=0.4645965099334717)
Finished Training
Total time taken: 123.67703485488892
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.690
[3,    32] loss: 0.677
[4,    32] loss: 0.650
[5,    32] loss: 0.628
[6,    32] loss: 0.628
[7,    32] loss: 0.617
[8,    32] loss: 0.609
[9,    32] loss: 0.605
[10,    32] loss: 0.601
[11,    32] loss: 0.595
[12,    32] loss: 0.587
[13,    32] loss: 0.584
[14,    32] loss: 0.583
[15,    32] loss: 0.582
[16,    32] loss: 0.574
[17,    32] loss: 0.577
[18,    32] loss: 0.567
[19,    32] loss: 0.561
[20,    32] loss: 0.556
[21,    32] loss: 0.545
[22,    32] loss: 0.545
[23,    32] loss: 0.536
[24,    32] loss: 0.518
[25,    32] loss: 0.512
[26,    32] loss: 0.510
[27,    32] loss: 0.497
[28,    32] loss: 0.496
[29,    32] loss: 0.489
[30,    32] loss: 0.480
[31,    32] loss: 0.475
[32,    32] loss: 0.460
[33,    32] loss: 0.466
[34,    32] loss: 0.448
[35,    32] loss: 0.438
[36,    32] loss: 0.438
[37,    32] loss: 0.441
[38,    32] loss: 0.427
[39,    32] loss: 0.417
[40,    32] loss: 0.411
[41,    32] loss: 0.398
[42,    32] loss: 0.406
[43,    32] loss: 0.394
[44,    32] loss: 0.392
[45,    32] loss: 0.387
[46,    32] loss: 0.377
[47,    32] loss: 0.380
[48,    32] loss: 0.376
[49,    32] loss: 0.360
[50,    32] loss: 0.361
[51,    32] loss: 0.356
Early stopping applied (best metric=0.4643910229206085)
Finished Training
Total time taken: 123.8674988746643
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.691
[2,    32] loss: 0.688
[3,    32] loss: 0.673
[4,    32] loss: 0.644
[5,    32] loss: 0.624
[6,    32] loss: 0.618
[7,    32] loss: 0.611
[8,    32] loss: 0.604
[9,    32] loss: 0.605
[10,    32] loss: 0.599
[11,    32] loss: 0.600
[12,    32] loss: 0.589
[13,    32] loss: 0.591
[14,    32] loss: 0.586
[15,    32] loss: 0.574
[16,    32] loss: 0.574
[17,    32] loss: 0.574
[18,    32] loss: 0.566
[19,    32] loss: 0.555
[20,    32] loss: 0.552
[21,    32] loss: 0.552
[22,    32] loss: 0.542
[23,    32] loss: 0.537
[24,    32] loss: 0.523
[25,    32] loss: 0.514
[26,    32] loss: 0.506
[27,    32] loss: 0.496
[28,    32] loss: 0.495
[29,    32] loss: 0.482
[30,    32] loss: 0.488
[31,    32] loss: 0.472
[32,    32] loss: 0.457
[33,    32] loss: 0.457
[34,    32] loss: 0.449
[35,    32] loss: 0.440
[36,    32] loss: 0.431
[37,    32] loss: 0.430
[38,    32] loss: 0.417
[39,    32] loss: 0.413
[40,    32] loss: 0.410
[41,    32] loss: 0.402
[42,    32] loss: 0.388
[43,    32] loss: 0.396
[44,    32] loss: 0.390
[45,    32] loss: 0.380
[46,    32] loss: 0.376
[47,    32] loss: 0.365
[48,    32] loss: 0.363
[49,    32] loss: 0.362
[50,    32] loss: 0.357
Early stopping applied (best metric=0.473285973072052)
Finished Training
Total time taken: 121.54466247558594
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.693
[2,    32] loss: 0.691
[3,    32] loss: 0.682
[4,    32] loss: 0.663
[5,    32] loss: 0.641
[6,    32] loss: 0.627
[7,    32] loss: 0.620
[8,    32] loss: 0.616
[9,    32] loss: 0.607
[10,    32] loss: 0.606
[11,    32] loss: 0.601
[12,    32] loss: 0.599
[13,    32] loss: 0.590
[14,    32] loss: 0.594
[15,    32] loss: 0.589
[16,    32] loss: 0.576
[17,    32] loss: 0.574
[18,    32] loss: 0.575
[19,    32] loss: 0.571
[20,    32] loss: 0.565
[21,    32] loss: 0.559
[22,    32] loss: 0.546
[23,    32] loss: 0.539
[24,    32] loss: 0.541
[25,    32] loss: 0.541
[26,    32] loss: 0.522
[27,    32] loss: 0.514
[28,    32] loss: 0.507
[29,    32] loss: 0.497
[30,    32] loss: 0.488
[31,    32] loss: 0.483
[32,    32] loss: 0.475
[33,    32] loss: 0.470
[34,    32] loss: 0.466
[35,    32] loss: 0.460
[36,    32] loss: 0.453
[37,    32] loss: 0.438
[38,    32] loss: 0.444
[39,    32] loss: 0.432
[40,    32] loss: 0.418
[41,    32] loss: 0.418
[42,    32] loss: 0.402
[43,    32] loss: 0.410
[44,    32] loss: 0.400
[45,    32] loss: 0.405
[46,    32] loss: 0.387
[47,    32] loss: 0.389
[48,    32] loss: 0.371
[49,    32] loss: 0.370
[50,    32] loss: 0.373
[51,    32] loss: 0.362
[52,    32] loss: 0.357
[53,    32] loss: 0.353
[54,    32] loss: 0.353
[55,    32] loss: 0.354
[56,    32] loss: 0.345
[57,    32] loss: 0.344
[58,    32] loss: 0.334
[59,    32] loss: 0.334
[60,    32] loss: 0.332
[61,    32] loss: 0.325
[62,    32] loss: 0.329
[63,    32] loss: 0.329
[64,    32] loss: 0.315
[65,    32] loss: 0.318
[66,    32] loss: 0.321
Early stopping applied (best metric=0.46503907442092896)
Finished Training
Total time taken: 160.6204478740692
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.692
[3,    32] loss: 0.684
[4,    32] loss: 0.663
[5,    32] loss: 0.639
[6,    32] loss: 0.628
[7,    32] loss: 0.618
[8,    32] loss: 0.614
[9,    32] loss: 0.603
[10,    32] loss: 0.607
[11,    32] loss: 0.601
[12,    32] loss: 0.601
[13,    32] loss: 0.597
[14,    32] loss: 0.590
[15,    32] loss: 0.589
[16,    32] loss: 0.581
[17,    32] loss: 0.575
[18,    32] loss: 0.569
[19,    32] loss: 0.562
[20,    32] loss: 0.554
[21,    32] loss: 0.554
[22,    32] loss: 0.543
[23,    32] loss: 0.545
[24,    32] loss: 0.525
[25,    32] loss: 0.517
[26,    32] loss: 0.509
[27,    32] loss: 0.493
[28,    32] loss: 0.501
[29,    32] loss: 0.490
[30,    32] loss: 0.472
[31,    32] loss: 0.466
[32,    32] loss: 0.462
[33,    32] loss: 0.451
[34,    32] loss: 0.449
[35,    32] loss: 0.439
[36,    32] loss: 0.425
[37,    32] loss: 0.432
[38,    32] loss: 0.416
[39,    32] loss: 0.411
[40,    32] loss: 0.417
[41,    32] loss: 0.403
[42,    32] loss: 0.396
[43,    32] loss: 0.391
[44,    32] loss: 0.381
[45,    32] loss: 0.377
[46,    32] loss: 0.370
[47,    32] loss: 0.371
[48,    32] loss: 0.362
[49,    32] loss: 0.360
[50,    32] loss: 0.354
[51,    32] loss: 0.353
[52,    32] loss: 0.339
[53,    32] loss: 0.340
[54,    32] loss: 0.349
[55,    32] loss: 0.335
Early stopping applied (best metric=0.4558637738227844)
Finished Training
Total time taken: 134.02680492401123
(2525, 33, 1024)
(10073, 33, 1024)
Loaded folder code/Thesis/dataset/train/S-palmitoylation-C/embeddings (12598 samples)
[1,     1] loss: 0.696
[2,    32] loss: 0.692
[3,    32] loss: 0.685
[4,    32] loss: 0.665
[5,    32] loss: 0.645
[6,    32] loss: 0.628
[7,    32] loss: 0.618
[8,    32] loss: 0.613
[9,    32] loss: 0.614
[10,    32] loss: 0.602
[11,    32] loss: 0.601
[12,    32] loss: 0.596
[13,    32] loss: 0.592
[14,    32] loss: 0.590
[15,    32] loss: 0.586
[16,    32] loss: 0.576
[17,    32] loss: 0.570
[18,    32] loss: 0.565
[19,    32] loss: 0.569
[20,    32] loss: 0.555
[21,    32] loss: 0.556
[22,    32] loss: 0.540
[23,    32] loss: 0.537
[24,    32] loss: 0.528
[25,    32] loss: 0.513
[26,    32] loss: 0.509
[27,    32] loss: 0.499
[28,    32] loss: 0.486
[29,    32] loss: 0.486
[30,    32] loss: 0.481
[31,    32] loss: 0.468
[32,    32] loss: 0.470
[33,    32] loss: 0.460
[34,    32] loss: 0.448
[35,    32] loss: 0.439
[36,    32] loss: 0.424
[37,    32] loss: 0.427
[38,    32] loss: 0.422
[39,    32] loss: 0.413
[40,    32] loss: 0.409
[41,    32] loss: 0.406
[42,    32] loss: 0.402
[43,    32] loss: 0.398
[44,    32] loss: 0.384
[45,    32] loss: 0.386
[46,    32] loss: 0.377
[47,    32] loss: 0.380
[48,    32] loss: 0.368
[49,    32] loss: 0.363
[50,    32] loss: 0.359
[51,    32] loss: 0.352
[52,    32] loss: 0.348
Early stopping applied (best metric=0.4642189145088196)
Finished Training
Total time taken: 126.55606341362
results!
{'gpu_mode': True, 'epochs': 200, 'batch_size': 512, 'learning_rate': 8.815923586760147e-05, 'test_data_ratio': 0.2, 'data_sample_mode': ['oversample'], 'crossValidation': True, 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'folds': 5, 'earlyStopping': True, 'ValidationMetric': 'Validation Loss (total)', 'earlyStoppingPatience': 25, 'CV_Repeats': 5, 'Experiment Name': 'Prottrans embeddings - local', 'weight_decay': 4.658986245090434, 'embeddingType': 'protBert', 'LSTM_layers': 1, 'LSTM_hidden_size': 32, 'LSTM_dropout': 0, 'UseUncertaintyBasedLoss': False, 'useLrWeight': False, 'CreateFigures': False, 'CNNType': 'Musite', 'FCType': 'Adapt', 'aminoAcid': ['S-palmitoylation-C'], 'random_state': 1241704393, 'current_CV_Repeat': 5, 'layerToSplitOn': 'FC', 'sample_weights': [1.0], 'currentFold': 4}
{'S-palmitoylation-C Validation Accuracy': 0.6934126669061167, 'S-palmitoylation-C Validation Sensitivity': 0.6727128712871288, 'S-palmitoylation-C Validation Specificity': 0.6986024577338285, 'S-palmitoylation-C Validation Precision': 0.3595524776177937, 'S-palmitoylation-C AUC ROC': 0.7545185140958967, 'S-palmitoylation-C AUC PR': 0.45283990930268114, 'S-palmitoylation-C MCC': 0.30748993089990717, 'S-palmitoylation-C F1': 0.4680030150417164, 'Validation Loss (S-palmitoylation-C)': 0.4711597728729248, 'Validation Loss (total)': 0.4711597728729248, 'TimeToTrain': 122.09779820442199}
{'S-palmitoylation-C Validation Accuracy': 0.01748391238999249, 'S-palmitoylation-C Validation Sensitivity': 0.03779066679244993, 'S-palmitoylation-C Validation Specificity': 0.02812426668993483, 'S-palmitoylation-C Validation Precision': 0.0150839338973424, 'S-palmitoylation-C AUC ROC': 0.010820422699758728, 'S-palmitoylation-C AUC PR': 0.018924747624376313, 'S-palmitoylation-C MCC': 0.020262209015658773, 'S-palmitoylation-C F1': 0.013499820497396139, 'Validation Loss (S-palmitoylation-C)': 0.0076349490238192316, 'Validation Loss (total)': 0.0076349490238192316, 'TimeToTrain': 10.311521965958324}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004787443135848901,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 452721191,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.84299149898137}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.623
[3,   134] loss: 0.625
[4,   134] loss: 0.625
[5,   134] loss: 0.628
[6,   134] loss: 0.629
[7,   134] loss: 0.630
[8,   134] loss: 0.626
[9,   134] loss: 0.627
[10,   134] loss: 0.627
[11,   134] loss: 0.625
[12,   134] loss: 0.630
[13,   134] loss: 0.625
[14,   134] loss: 0.623
[15,   134] loss: 0.627
[16,   134] loss: 0.623
[17,   134] loss: 0.625
[18,   134] loss: 0.625
[19,   134] loss: 0.626
[20,   134] loss: 0.625
[21,   134] loss: 0.626
[22,   134] loss: 0.625
[23,   134] loss: 0.624
[24,   134] loss: 0.624
[25,   134] loss: 0.629
[26,   134] loss: 0.625
[27,   134] loss: 0.625
[28,   134] loss: 0.629
[29,   134] loss: 0.626
[30,   134] loss: 0.626
[31,   134] loss: 0.627
[32,   134] loss: 0.623
[33,   134] loss: 0.625
[34,   134] loss: 0.621
[35,   134] loss: 0.627
[36,   134] loss: 0.630
[37,   134] loss: 0.625
[38,   134] loss: 0.627
[39,   134] loss: 0.630
[40,   134] loss: 0.621
Early stopping applied (best metric=0.36925816535949707)
Finished Training
Total time taken: 400.2790355682373
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.621
[3,   134] loss: 0.626
[4,   134] loss: 0.625
[5,   134] loss: 0.624
[6,   134] loss: 0.621
[7,   134] loss: 0.622
[8,   134] loss: 0.624
[9,   134] loss: 0.625
[10,   134] loss: 0.623
[11,   134] loss: 0.623
[12,   134] loss: 0.625
[13,   134] loss: 0.623
[14,   134] loss: 0.627
[15,   134] loss: 0.625
[16,   134] loss: 0.626
[17,   134] loss: 0.622
[18,   134] loss: 0.623
[19,   134] loss: 0.626
[20,   134] loss: 0.625
[21,   134] loss: 0.623
[22,   134] loss: 0.631
[23,   134] loss: 0.622
[24,   134] loss: 0.622
[25,   134] loss: 0.621
[26,   134] loss: 0.623
[27,   134] loss: 0.624
[28,   134] loss: 0.627
[29,   134] loss: 0.626
[30,   134] loss: 0.626
[31,   134] loss: 0.631
[32,   134] loss: 0.621
[33,   134] loss: 0.626
[34,   134] loss: 0.624
[35,   134] loss: 0.624
[36,   134] loss: 0.623
[37,   134] loss: 0.622
[38,   134] loss: 0.624
[39,   134] loss: 0.622
[40,   134] loss: 0.620
[41,   134] loss: 0.627
[42,   134] loss: 0.626
[43,   134] loss: 0.626
[44,   134] loss: 0.623
[45,   134] loss: 0.625
[46,   134] loss: 0.625
[47,   134] loss: 0.625
[48,   134] loss: 0.626
[49,   134] loss: 0.622
[50,   134] loss: 0.623
[51,   134] loss: 0.624
Early stopping applied (best metric=0.3629703223705292)
Finished Training
Total time taken: 509.0515367984772
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.627
[3,   134] loss: 0.625
[4,   134] loss: 0.626
[5,   134] loss: 0.629
[6,   134] loss: 0.628
[7,   134] loss: 0.627
[8,   134] loss: 0.623
[9,   134] loss: 0.622
[10,   134] loss: 0.629
[11,   134] loss: 0.626
[12,   134] loss: 0.629
[13,   134] loss: 0.627
[14,   134] loss: 0.628
[15,   134] loss: 0.631
[16,   134] loss: 0.626
[17,   134] loss: 0.624
[18,   134] loss: 0.625
[19,   134] loss: 0.628
[20,   134] loss: 0.624
[21,   134] loss: 0.625
[22,   134] loss: 0.631
[23,   134] loss: 0.628
[24,   134] loss: 0.626
[25,   134] loss: 0.627
[26,   134] loss: 0.625
[27,   134] loss: 0.626
[28,   134] loss: 0.625
[29,   134] loss: 0.627
[30,   134] loss: 0.623
[31,   134] loss: 0.621
[32,   134] loss: 0.624
[33,   134] loss: 0.624
[34,   134] loss: 0.625
[35,   134] loss: 0.622
[36,   134] loss: 0.624
[37,   134] loss: 0.625
[38,   134] loss: 0.626
Early stopping applied (best metric=0.3778535723686218)
Finished Training
Total time taken: 380.5193166732788
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.626
[3,   134] loss: 0.627
[4,   134] loss: 0.626
[5,   134] loss: 0.626
[6,   134] loss: 0.628
[7,   134] loss: 0.627
[8,   134] loss: 0.632
[9,   134] loss: 0.630
[10,   134] loss: 0.629
[11,   134] loss: 0.630
[12,   134] loss: 0.626
[13,   134] loss: 0.630
[14,   134] loss: 0.624
[15,   134] loss: 0.627
[16,   134] loss: 0.626
[17,   134] loss: 0.627
[18,   134] loss: 0.628
[19,   134] loss: 0.625
[20,   134] loss: 0.627
[21,   134] loss: 0.626
[22,   134] loss: 0.625
[23,   134] loss: 0.625
[24,   134] loss: 0.624
[25,   134] loss: 0.626
[26,   134] loss: 0.626
[27,   134] loss: 0.625
[28,   134] loss: 0.624
[29,   134] loss: 0.625
[30,   134] loss: 0.624
[31,   134] loss: 0.622
[32,   134] loss: 0.627
[33,   134] loss: 0.623
[34,   134] loss: 0.621
[35,   134] loss: 0.624
[36,   134] loss: 0.626
[37,   134] loss: 0.626
[38,   134] loss: 0.621
[39,   134] loss: 0.624
[40,   134] loss: 0.624
[41,   134] loss: 0.622
[42,   134] loss: 0.625
[43,   134] loss: 0.627
[44,   134] loss: 0.624
[45,   134] loss: 0.623
[46,   134] loss: 0.624
[47,   134] loss: 0.629
[48,   134] loss: 0.622
[49,   134] loss: 0.619
[50,   134] loss: 0.624
[51,   134] loss: 0.623
[52,   134] loss: 0.623
[53,   134] loss: 0.621
[54,   134] loss: 0.626
[55,   134] loss: 0.624
[56,   134] loss: 0.622
Early stopping applied (best metric=0.36559295654296875)
Finished Training
Total time taken: 557.6218676567078
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.617
[3,   134] loss: 0.621
[4,   134] loss: 0.621
[5,   134] loss: 0.622
[6,   134] loss: 0.620
[7,   134] loss: 0.626
[8,   134] loss: 0.620
[9,   134] loss: 0.619
[10,   134] loss: 0.619
[11,   134] loss: 0.621
[12,   134] loss: 0.620
[13,   134] loss: 0.624
[14,   134] loss: 0.621
[15,   134] loss: 0.622
[16,   134] loss: 0.623
[17,   134] loss: 0.622
[18,   134] loss: 0.619
[19,   134] loss: 0.620
[20,   134] loss: 0.622
[21,   134] loss: 0.619
[22,   134] loss: 0.624
[23,   134] loss: 0.621
[24,   134] loss: 0.621
[25,   134] loss: 0.621
[26,   134] loss: 0.620
[27,   134] loss: 0.620
[28,   134] loss: 0.621
[29,   134] loss: 0.619
[30,   134] loss: 0.621
[31,   134] loss: 0.621
[32,   134] loss: 0.618
[33,   134] loss: 0.623
[34,   134] loss: 0.624
[35,   134] loss: 0.622
[36,   134] loss: 0.621
[37,   134] loss: 0.627
Early stopping applied (best metric=0.3561228811740875)
Finished Training
Total time taken: 371.7221329212189
{'Methylation-K Validation Accuracy': 0.4803511322707086, 'Methylation-K Validation Sensitivity': 0.8750636648097279, 'Methylation-K Validation Specificity': 0.43754647858277584, 'Methylation-K Validation Precision': 0.1450600922429702, 'Methylation-K AUC ROC': 0.7463198080233825, 'Methylation-K AUC PR': 0.2575508995095238, 'Methylation-K MCC': 0.1899863952834386, 'Methylation-K F1': 0.2486180825114602, 'Validation Loss (Methylation-K)': 0.36635957956314086, 'Validation Loss (total)': 0.36635957956314086, 'TimeToTrain': 443.83877792358396}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005395847849855856,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2351754621,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.733341503137716}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.645
[3,   134] loss: 0.644
[4,   134] loss: 0.642
[5,   134] loss: 0.641
[6,   134] loss: 0.644
[7,   134] loss: 0.641
[8,   134] loss: 0.639
[9,   134] loss: 0.647
[10,   134] loss: 0.646
[11,   134] loss: 0.640
[12,   134] loss: 0.639
[13,   134] loss: 0.641
[14,   134] loss: 0.642
[15,   134] loss: 0.638
[16,   134] loss: 0.641
[17,   134] loss: 0.639
[18,   134] loss: 0.642
[19,   134] loss: 0.644
[20,   134] loss: 0.639
[21,   134] loss: 0.644
[22,   134] loss: 0.641
[23,   134] loss: 0.644
[24,   134] loss: 0.642
[25,   134] loss: 0.639
[26,   134] loss: 0.639
[27,   134] loss: 0.641
[28,   134] loss: 0.641
[29,   134] loss: 0.642
[30,   134] loss: 0.640
[31,   134] loss: 0.641
[32,   134] loss: 0.652
[33,   134] loss: 0.662
[34,   134] loss: 0.648
[35,   134] loss: 0.642
[36,   134] loss: 0.634
Early stopping applied (best metric=0.38155919313430786)
Finished Training
Total time taken: 357.83410477638245
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.643
[3,   134] loss: 0.648
[4,   134] loss: 0.647
[5,   134] loss: 0.648
[6,   134] loss: 0.650
[7,   134] loss: 0.650
[8,   134] loss: 0.646
[9,   134] loss: 0.644
[10,   134] loss: 0.648
[11,   134] loss: 0.644
[12,   134] loss: 0.645
[13,   134] loss: 0.643
[14,   134] loss: 0.649
[15,   134] loss: 0.646
[16,   134] loss: 0.652
[17,   134] loss: 0.640
[18,   134] loss: 0.643
[19,   134] loss: 0.651
[20,   134] loss: 0.643
[21,   134] loss: 0.644
[22,   134] loss: 0.642
[23,   134] loss: 0.646
[24,   134] loss: 0.643
[25,   134] loss: 0.642
[26,   134] loss: 0.643
[27,   134] loss: 0.642
[28,   134] loss: 0.645
[29,   134] loss: 0.645
[30,   134] loss: 0.643
[31,   134] loss: 0.637
[32,   134] loss: 0.649
[33,   134] loss: 0.654
[34,   134] loss: 0.643
[35,   134] loss: 0.640
[36,   134] loss: 0.637
[37,   134] loss: 0.643
[38,   134] loss: 0.639
[39,   134] loss: 0.639
[40,   134] loss: 0.639
[41,   134] loss: 0.636
[42,   134] loss: 0.640
[43,   134] loss: 0.641
Early stopping applied (best metric=0.386721134185791)
Finished Training
Total time taken: 428.9018199443817
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.693
[3,   134] loss: 0.693
[4,   134] loss: 0.693
[5,   134] loss: 0.693
[6,   134] loss: 0.693
[7,   134] loss: 0.693
[8,   134] loss: 0.693
[9,   134] loss: 0.693
[10,   134] loss: 0.693
[11,   134] loss: 0.693
[12,   134] loss: 0.693
[13,   134] loss: 0.693
[14,   134] loss: 0.693
[15,   134] loss: 0.693
[16,   134] loss: 0.693
[17,   134] loss: 0.693
[18,   134] loss: 0.693
[19,   134] loss: 0.693
[20,   134] loss: 0.693
[21,   134] loss: 0.693
[22,   134] loss: 0.693
[23,   134] loss: 0.693
[24,   134] loss: 0.693
[25,   134] loss: 0.693
[26,   134] loss: 0.693
[27,   134] loss: 0.693
[28,   134] loss: 0.693
[29,   134] loss: 0.693
[30,   134] loss: 0.693
[31,   134] loss: 0.693
[32,   134] loss: 0.693
[33,   134] loss: 0.693
[34,   134] loss: 0.693
[35,   134] loss: 0.693
[36,   134] loss: 0.693
[37,   134] loss: 0.693
[38,   134] loss: 0.693
[39,   134] loss: 0.693
[40,   134] loss: 0.693
[41,   134] loss: 0.693
[42,   134] loss: 0.693
[43,   134] loss: 0.693
[44,   134] loss: 0.693
[45,   134] loss: 0.693
[46,   134] loss: 0.693
[47,   134] loss: 0.693
[48,   134] loss: 0.693
[49,   134] loss: 0.693
[50,   134] loss: 0.693
[51,   134] loss: 0.693
[52,   134] loss: 0.693
[53,   134] loss: 0.693
[54,   134] loss: 0.693
[55,   134] loss: 0.693
[56,   134] loss: 0.693
[57,   134] loss: 0.693
[58,   134] loss: 0.693
[59,   134] loss: 0.693
[60,   134] loss: 0.693
[61,   134] loss: 0.693
[62,   134] loss: 0.693
[63,   134] loss: 0.693
[64,   134] loss: 0.693
[65,   134] loss: 0.693
[66,   134] loss: 0.693
[67,   134] loss: 0.693
[68,   134] loss: 0.693
[69,   134] loss: 0.693
[70,   134] loss: 0.693
[71,   134] loss: 0.693
[72,   134] loss: 0.693
[73,   134] loss: 0.693
[74,   134] loss: 0.693
[75,   134] loss: 0.693
[76,   134] loss: 0.693
[77,   134] loss: 0.693
[78,   134] loss: 0.693
[79,   134] loss: 0.693
[80,   134] loss: 0.693
Early stopping applied (best metric=0.4467017352581024)
Finished Training
Total time taken: 795.3443398475647
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.643
[3,   134] loss: 0.647
[4,   134] loss: 0.646
[5,   134] loss: 0.649
[6,   134] loss: 0.652
[7,   134] loss: 0.647
[8,   134] loss: 0.646
[9,   134] loss: 0.644
[10,   134] loss: 0.647
[11,   134] loss: 0.647
[12,   134] loss: 0.649
[13,   134] loss: 0.643
[14,   134] loss: 0.649
[15,   134] loss: 0.651
[16,   134] loss: 0.646
[17,   134] loss: 0.647
[18,   134] loss: 0.650
[19,   134] loss: 0.647
[20,   134] loss: 0.649
[21,   134] loss: 0.646
[22,   134] loss: 0.646
[23,   134] loss: 0.647
[24,   134] loss: 0.648
[25,   134] loss: 0.647
[26,   134] loss: 0.647
[27,   134] loss: 0.647
[28,   134] loss: 0.646
[29,   134] loss: 0.645
[30,   134] loss: 0.647
[31,   134] loss: 0.644
[32,   134] loss: 0.646
[33,   134] loss: 0.647
[34,   134] loss: 0.647
[35,   134] loss: 0.648
[36,   134] loss: 0.645
[37,   134] loss: 0.647
[38,   134] loss: 0.645
[39,   134] loss: 0.645
[40,   134] loss: 0.646
[41,   134] loss: 0.645
[42,   134] loss: 0.648
[43,   134] loss: 0.646
[44,   134] loss: 0.646
[45,   134] loss: 0.650
[46,   134] loss: 0.647
[47,   134] loss: 0.643
[48,   134] loss: 0.647
[49,   134] loss: 0.649
[50,   134] loss: 0.646
[51,   134] loss: 0.648
[52,   134] loss: 0.648
[53,   134] loss: 0.644
[54,   134] loss: 0.645
[55,   134] loss: 0.644
[56,   134] loss: 0.646
Early stopping applied (best metric=0.3809721767902374)
Finished Training
Total time taken: 560.2744798660278
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.646
[3,   134] loss: 0.644
[4,   134] loss: 0.648
[5,   134] loss: 0.648
[6,   134] loss: 0.649
[7,   134] loss: 0.646
[8,   134] loss: 0.648
[9,   134] loss: 0.646
[10,   134] loss: 0.646
[11,   134] loss: 0.646
[12,   134] loss: 0.644
[13,   134] loss: 0.642
[14,   134] loss: 0.643
[15,   134] loss: 0.643
[16,   134] loss: 0.644
[17,   134] loss: 0.646
[18,   134] loss: 0.649
[19,   134] loss: 0.641
[20,   134] loss: 0.644
[21,   134] loss: 0.645
[22,   134] loss: 0.643
[23,   134] loss: 0.642
[24,   134] loss: 0.641
[25,   134] loss: 0.648
[26,   134] loss: 0.662
[27,   134] loss: 0.647
[28,   134] loss: 0.647
[29,   134] loss: 0.642
[30,   134] loss: 0.640
[31,   134] loss: 0.636
[32,   134] loss: 0.642
[33,   134] loss: 0.640
[34,   134] loss: 0.637
[35,   134] loss: 0.637
[36,   134] loss: 0.637
[37,   134] loss: 0.638
[38,   134] loss: 0.639
[39,   134] loss: 0.637
[40,   134] loss: 0.638
[41,   134] loss: 0.638
[42,   134] loss: 0.639
[43,   134] loss: 0.638
[44,   134] loss: 0.645
[45,   134] loss: 0.637
[46,   134] loss: 0.635
[47,   134] loss: 0.635
[48,   134] loss: 0.638
[49,   134] loss: 0.643
[50,   134] loss: 0.638
[51,   134] loss: 0.634
[52,   134] loss: 0.642
[53,   134] loss: 0.639
[54,   134] loss: 0.638
[55,   134] loss: 0.638
[56,   134] loss: 0.636
[57,   134] loss: 0.640
[58,   134] loss: 0.635
[59,   134] loss: 0.644
[60,   134] loss: 0.650
[61,   134] loss: 0.665
[62,   134] loss: 0.666
[63,   134] loss: 0.661
Early stopping applied (best metric=0.38372576236724854)
Finished Training
Total time taken: 629.6314878463745
{'Methylation-K Validation Accuracy': 0.3609578877145491, 'Methylation-K Validation Sensitivity': 0.900949904590157, 'Methylation-K Validation Specificity': 0.3023947830876226, 'Methylation-K Validation Precision': 0.1257249191700965, 'Methylation-K AUC ROC': 0.6769585702708502, 'Methylation-K AUC PR': 0.293394446348159, 'Methylation-K MCC': 0.12665650324355415, 'Methylation-K F1': 0.21998190462858003, 'Validation Loss (Methylation-K)': 0.39593600034713744, 'Validation Loss (total)': 0.39593600034713744, 'TimeToTrain': 554.3972464561463}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004372838908607941,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3259609772,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.33318366452241}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.627
[3,   134] loss: 0.632
[4,   134] loss: 0.631
[5,   134] loss: 0.629
[6,   134] loss: 0.633
[7,   134] loss: 0.630
[8,   134] loss: 0.631
[9,   134] loss: 0.632
[10,   134] loss: 0.631
[11,   134] loss: 0.628
[12,   134] loss: 0.631
[13,   134] loss: 0.631
[14,   134] loss: 0.630
[15,   134] loss: 0.633
[16,   134] loss: 0.633
[17,   134] loss: 0.631
[18,   134] loss: 0.632
[19,   134] loss: 0.631
[20,   134] loss: 0.632
[21,   134] loss: 0.628
[22,   134] loss: 0.628
[23,   134] loss: 0.629
[24,   134] loss: 0.633
[25,   134] loss: 0.631
[26,   134] loss: 0.630
[27,   134] loss: 0.630
Early stopping applied (best metric=0.3766721487045288)
Finished Training
Total time taken: 267.6379134654999
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.631
[3,   134] loss: 0.632
[4,   134] loss: 0.629
[5,   134] loss: 0.628
[6,   134] loss: 0.630
[7,   134] loss: 0.627
[8,   134] loss: 0.628
[9,   134] loss: 0.630
[10,   134] loss: 0.629
[11,   134] loss: 0.627
[12,   134] loss: 0.630
[13,   134] loss: 0.630
[14,   134] loss: 0.631
[15,   134] loss: 0.630
[16,   134] loss: 0.628
[17,   134] loss: 0.630
[18,   134] loss: 0.634
[19,   134] loss: 0.631
[20,   134] loss: 0.633
[21,   134] loss: 0.629
[22,   134] loss: 0.626
[23,   134] loss: 0.626
[24,   134] loss: 0.629
[25,   134] loss: 0.630
[26,   134] loss: 0.631
[27,   134] loss: 0.631
[28,   134] loss: 0.633
[29,   134] loss: 0.628
[30,   134] loss: 0.628
[31,   134] loss: 0.629
[32,   134] loss: 0.626
[33,   134] loss: 0.628
[34,   134] loss: 0.631
[35,   134] loss: 0.629
[36,   134] loss: 0.628
[37,   134] loss: 0.630
Early stopping applied (best metric=0.3764404058456421)
Finished Training
Total time taken: 370.5348389148712
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.688
[2,   134] loss: 0.632
[3,   134] loss: 0.632
[4,   134] loss: 0.632
[5,   134] loss: 0.626
[6,   134] loss: 0.631
[7,   134] loss: 0.627
[8,   134] loss: 0.641
[9,   134] loss: 0.636
[10,   134] loss: 0.631
[11,   134] loss: 0.630
[12,   134] loss: 0.632
[13,   134] loss: 0.629
[14,   134] loss: 0.632
[15,   134] loss: 0.627
[16,   134] loss: 0.630
[17,   134] loss: 0.631
[18,   134] loss: 0.631
[19,   134] loss: 0.629
[20,   134] loss: 0.631
[21,   134] loss: 0.630
[22,   134] loss: 0.631
[23,   134] loss: 0.628
[24,   134] loss: 0.629
[25,   134] loss: 0.630
[26,   134] loss: 0.631
[27,   134] loss: 0.631
[28,   134] loss: 0.633
[29,   134] loss: 0.633
[30,   134] loss: 0.630
[31,   134] loss: 0.630
[32,   134] loss: 0.631
[33,   134] loss: 0.628
[34,   134] loss: 0.629
[35,   134] loss: 0.629
[36,   134] loss: 0.626
[37,   134] loss: 0.628
[38,   134] loss: 0.637
[39,   134] loss: 0.629
[40,   134] loss: 0.629
[41,   134] loss: 0.630
[42,   134] loss: 0.632
[43,   134] loss: 0.628
[44,   134] loss: 0.631
[45,   134] loss: 0.625
[46,   134] loss: 0.630
[47,   134] loss: 0.629
[48,   134] loss: 0.628
[49,   134] loss: 0.627
[50,   134] loss: 0.632
[51,   134] loss: 0.627
[52,   134] loss: 0.630
[53,   134] loss: 0.629
[54,   134] loss: 0.629
[55,   134] loss: 0.631
[56,   134] loss: 0.627
[57,   134] loss: 0.629
[58,   134] loss: 0.627
[59,   134] loss: 0.633
[60,   134] loss: 0.634
[61,   134] loss: 0.631
Early stopping applied (best metric=0.36572250723838806)
Finished Training
Total time taken: 609.986713886261
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.634
[3,   134] loss: 0.635
[4,   134] loss: 0.635
[5,   134] loss: 0.632
[6,   134] loss: 0.637
[7,   134] loss: 0.634
[8,   134] loss: 0.637
[9,   134] loss: 0.635
[10,   134] loss: 0.637
[11,   134] loss: 0.637
[12,   134] loss: 0.634
[13,   134] loss: 0.635
[14,   134] loss: 0.635
[15,   134] loss: 0.632
[16,   134] loss: 0.637
[17,   134] loss: 0.637
[18,   134] loss: 0.633
[19,   134] loss: 0.634
[20,   134] loss: 0.634
[21,   134] loss: 0.631
[22,   134] loss: 0.633
[23,   134] loss: 0.634
[24,   134] loss: 0.633
[25,   134] loss: 0.633
[26,   134] loss: 0.634
Early stopping applied (best metric=0.37443089485168457)
Finished Training
Total time taken: 262.7094008922577
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.640
[3,   134] loss: 0.637
[4,   134] loss: 0.634
[5,   134] loss: 0.636
[6,   134] loss: 0.641
[7,   134] loss: 0.637
[8,   134] loss: 0.640
[9,   134] loss: 0.635
[10,   134] loss: 0.640
[11,   134] loss: 0.635
[12,   134] loss: 0.638
[13,   134] loss: 0.637
[14,   134] loss: 0.638
[15,   134] loss: 0.635
[16,   134] loss: 0.634
[17,   134] loss: 0.636
[18,   134] loss: 0.638
[19,   134] loss: 0.638
[20,   134] loss: 0.639
[21,   134] loss: 0.639
[22,   134] loss: 0.638
[23,   134] loss: 0.637
[24,   134] loss: 0.635
[25,   134] loss: 0.645
[26,   134] loss: 0.637
[27,   134] loss: 0.639
[28,   134] loss: 0.636
[29,   134] loss: 0.639
[30,   134] loss: 0.633
[31,   134] loss: 0.637
[32,   134] loss: 0.635
[33,   134] loss: 0.637
[34,   134] loss: 0.640
[35,   134] loss: 0.637
[36,   134] loss: 0.637
[37,   134] loss: 0.633
[38,   134] loss: 0.640
[39,   134] loss: 0.638
[40,   134] loss: 0.638
[41,   134] loss: 0.635
[42,   134] loss: 0.637
[43,   134] loss: 0.635
[44,   134] loss: 0.636
[45,   134] loss: 0.636
[46,   134] loss: 0.634
[47,   134] loss: 0.637
[48,   134] loss: 0.636
[49,   134] loss: 0.635
[50,   134] loss: 0.638
[51,   134] loss: 0.638
[52,   134] loss: 0.636
[53,   134] loss: 0.639
[54,   134] loss: 0.638
[55,   134] loss: 0.637
[56,   134] loss: 0.634
Early stopping applied (best metric=0.3796132504940033)
Finished Training
Total time taken: 559.7478172779083
{'Methylation-K Validation Accuracy': 0.46207004613515545, 'Methylation-K Validation Sensitivity': 0.8724655813942651, 'Methylation-K Validation Specificity': 0.4175621361120495, 'Methylation-K Validation Precision': 0.14036586995975897, 'Methylation-K AUC ROC': 0.7319433266145774, 'Methylation-K AUC PR': 0.23768289965852052, 'Methylation-K MCC': 0.17785302999310956, 'Methylation-K F1': 0.241589777885633, 'Validation Loss (Methylation-K)': 0.37457584142684935, 'Validation Loss (total)': 0.37457584142684935, 'TimeToTrain': 414.12333688735964}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00641722712123865,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3564157511,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.577077339269062}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.699
[2,   134] loss: 0.633
[3,   134] loss: 0.633
[4,   134] loss: 0.635
[5,   134] loss: 0.632
[6,   134] loss: 0.633
[7,   134] loss: 0.634
[8,   134] loss: 0.631
[9,   134] loss: 0.634
[10,   134] loss: 0.629
[11,   134] loss: 0.637
[12,   134] loss: 0.631
[13,   134] loss: 0.633
[14,   134] loss: 0.627
[15,   134] loss: 0.628
[16,   134] loss: 0.632
[17,   134] loss: 0.631
[18,   134] loss: 0.631
[19,   134] loss: 0.627
[20,   134] loss: 0.629
[21,   134] loss: 0.628
[22,   134] loss: 0.638
[23,   134] loss: 0.637
[24,   134] loss: 0.658
[25,   134] loss: 0.649
[26,   134] loss: 0.650
[27,   134] loss: 0.643
[28,   134] loss: 0.634
[29,   134] loss: 0.629
[30,   134] loss: 0.629
[31,   134] loss: 0.629
[32,   134] loss: 0.627
[33,   134] loss: 0.629
[34,   134] loss: 0.630
[35,   134] loss: 0.630
[36,   134] loss: 0.633
[37,   134] loss: 0.632
[38,   134] loss: 0.634
[39,   134] loss: 0.630
[40,   134] loss: 0.630
[41,   134] loss: 0.628
[42,   134] loss: 0.627
[43,   134] loss: 0.624
[44,   134] loss: 0.625
[45,   134] loss: 0.632
[46,   134] loss: 0.626
[47,   134] loss: 0.631
[48,   134] loss: 0.628
[49,   134] loss: 0.625
[50,   134] loss: 0.623
[51,   134] loss: 0.632
[52,   134] loss: 0.628
[53,   134] loss: 0.628
[54,   134] loss: 0.630
[55,   134] loss: 0.633
[56,   134] loss: 0.625
[57,   134] loss: 0.628
[58,   134] loss: 0.632
[59,   134] loss: 0.631
[60,   134] loss: 0.627
[61,   134] loss: 0.628
[62,   134] loss: 0.632
Early stopping applied (best metric=0.36270013451576233)
Finished Training
Total time taken: 614.8754913806915
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.624
[3,   134] loss: 0.626
[4,   134] loss: 0.631
[5,   134] loss: 0.628
[6,   134] loss: 0.629
[7,   134] loss: 0.629
[8,   134] loss: 0.629
[9,   134] loss: 0.633
[10,   134] loss: 0.632
[11,   134] loss: 0.633
[12,   134] loss: 0.631
[13,   134] loss: 0.628
[14,   134] loss: 0.630
[15,   134] loss: 0.627
[16,   134] loss: 0.627
[17,   134] loss: 0.636
[18,   134] loss: 0.628
[19,   134] loss: 0.628
[20,   134] loss: 0.627
[21,   134] loss: 0.631
[22,   134] loss: 0.631
[23,   134] loss: 0.627
[24,   134] loss: 0.626
[25,   134] loss: 0.631
[26,   134] loss: 0.627
[27,   134] loss: 0.627
[28,   134] loss: 0.624
[29,   134] loss: 0.628
[30,   134] loss: 0.625
[31,   134] loss: 0.628
[32,   134] loss: 0.631
[33,   134] loss: 0.625
[34,   134] loss: 0.629
[35,   134] loss: 0.631
[36,   134] loss: 0.635
[37,   134] loss: 0.628
[38,   134] loss: 0.630
[39,   134] loss: 0.628
[40,   134] loss: 0.630
[41,   134] loss: 0.624
[42,   134] loss: 0.630
[43,   134] loss: 0.628
[44,   134] loss: 0.629
[45,   134] loss: 0.632
[46,   134] loss: 0.629
[47,   134] loss: 0.631
[48,   134] loss: 0.625
[49,   134] loss: 0.630
[50,   134] loss: 0.628
[51,   134] loss: 0.629
Early stopping applied (best metric=0.3653450310230255)
Finished Training
Total time taken: 509.7580177783966
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.637
[3,   134] loss: 0.631
[4,   134] loss: 0.632
[5,   134] loss: 0.630
[6,   134] loss: 0.631
[7,   134] loss: 0.642
[8,   134] loss: 0.636
[9,   134] loss: 0.632
[10,   134] loss: 0.633
[11,   134] loss: 0.630
[12,   134] loss: 0.635
[13,   134] loss: 0.629
[14,   134] loss: 0.634
[15,   134] loss: 0.632
[16,   134] loss: 0.632
[17,   134] loss: 0.629
[18,   134] loss: 0.633
[19,   134] loss: 0.634
[20,   134] loss: 0.631
[21,   134] loss: 0.637
[22,   134] loss: 0.632
[23,   134] loss: 0.630
[24,   134] loss: 0.632
[25,   134] loss: 0.632
[26,   134] loss: 0.634
[27,   134] loss: 0.632
[28,   134] loss: 0.636
[29,   134] loss: 0.633
[30,   134] loss: 0.631
[31,   134] loss: 0.633
[32,   134] loss: 0.632
[33,   134] loss: 0.632
[34,   134] loss: 0.633
[35,   134] loss: 0.633
[36,   134] loss: 0.632
[37,   134] loss: 0.637
[38,   134] loss: 0.635
[39,   134] loss: 0.640
[40,   134] loss: 0.631
Early stopping applied (best metric=0.36313962936401367)
Finished Training
Total time taken: 401.424551486969
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.627
[3,   134] loss: 0.627
[4,   134] loss: 0.630
[5,   134] loss: 0.631
[6,   134] loss: 0.634
[7,   134] loss: 0.633
[8,   134] loss: 0.629
[9,   134] loss: 0.632
[10,   134] loss: 0.631
[11,   134] loss: 0.630
[12,   134] loss: 0.627
[13,   134] loss: 0.629
[14,   134] loss: 0.634
[15,   134] loss: 0.629
[16,   134] loss: 0.632
[17,   134] loss: 0.628
[18,   134] loss: 0.627
[19,   134] loss: 0.629
[20,   134] loss: 0.641
[21,   134] loss: 0.633
[22,   134] loss: 0.625
[23,   134] loss: 0.623
[24,   134] loss: 0.624
[25,   134] loss: 0.622
[26,   134] loss: 0.622
[27,   134] loss: 0.624
[28,   134] loss: 0.621
[29,   134] loss: 0.624
[30,   134] loss: 0.628
[31,   134] loss: 0.624
[32,   134] loss: 0.622
[33,   134] loss: 0.624
[34,   134] loss: 0.628
[35,   134] loss: 0.625
[36,   134] loss: 0.631
[37,   134] loss: 0.625
[38,   134] loss: 0.627
[39,   134] loss: 0.623
[40,   134] loss: 0.626
[41,   134] loss: 0.622
[42,   134] loss: 0.629
[43,   134] loss: 0.620
[44,   134] loss: 0.625
[45,   134] loss: 0.624
[46,   134] loss: 0.624
[47,   134] loss: 0.624
[48,   134] loss: 0.625
[49,   134] loss: 0.625
[50,   134] loss: 0.626
[51,   134] loss: 0.626
[52,   134] loss: 0.621
[53,   134] loss: 0.623
[54,   134] loss: 0.628
[55,   134] loss: 0.623
[56,   134] loss: 0.627
[57,   134] loss: 0.622
[58,   134] loss: 0.629
[59,   134] loss: 0.624
[60,   134] loss: 0.624
[61,   134] loss: 0.626
[62,   134] loss: 0.626
[63,   134] loss: 0.625
[64,   134] loss: 0.627
[65,   134] loss: 0.626
[66,   134] loss: 0.623
[67,   134] loss: 0.625
[68,   134] loss: 0.621
[69,   134] loss: 0.624
[70,   134] loss: 0.625
[71,   134] loss: 0.622
[72,   134] loss: 0.627
[73,   134] loss: 0.626
[74,   134] loss: 0.625
[75,   134] loss: 0.625
[76,   134] loss: 0.621
[77,   134] loss: 0.620
Early stopping applied (best metric=0.37432587146759033)
Finished Training
Total time taken: 770.1031656265259
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.628
[3,   134] loss: 0.629
[4,   134] loss: 0.630
[5,   134] loss: 0.627
[6,   134] loss: 0.632
[7,   134] loss: 0.633
[8,   134] loss: 0.634
[9,   134] loss: 0.635
[10,   134] loss: 0.631
[11,   134] loss: 0.630
[12,   134] loss: 0.658
[13,   134] loss: 0.659
[14,   134] loss: 0.658
[15,   134] loss: 0.659
[16,   134] loss: 0.657
[17,   134] loss: 0.658
[18,   134] loss: 0.655
[19,   134] loss: 0.629
[20,   134] loss: 0.634
[21,   134] loss: 0.627
[22,   134] loss: 0.629
[23,   134] loss: 0.631
[24,   134] loss: 0.627
[25,   134] loss: 0.624
[26,   134] loss: 0.633
[27,   134] loss: 0.629
Early stopping applied (best metric=0.3688061833381653)
Finished Training
Total time taken: 271.4721579551697
{'Methylation-K Validation Accuracy': 0.4927271539115728, 'Methylation-K Validation Sensitivity': 0.847648770622622, 'Methylation-K Validation Specificity': 0.45423528772182126, 'Methylation-K Validation Precision': 0.144704068834092, 'Methylation-K AUC ROC': 0.7328952608796225, 'Methylation-K AUC PR': 0.2464663614984039, 'Methylation-K MCC': 0.18218538160510955, 'Methylation-K F1': 0.24699933572413268, 'Validation Loss (Methylation-K)': 0.3668633699417114, 'Validation Loss (total)': 0.3668633699417114, 'TimeToTrain': 513.5266768455506}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006931300095013361,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3606284396,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.819050477780387}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.638
[3,   134] loss: 0.640
[4,   134] loss: 0.635
[5,   134] loss: 0.637
[6,   134] loss: 0.640
[7,   134] loss: 0.638
[8,   134] loss: 0.638
[9,   134] loss: 0.635
[10,   134] loss: 0.637
[11,   134] loss: 0.635
[12,   134] loss: 0.640
[13,   134] loss: 0.638
[14,   134] loss: 0.641
[15,   134] loss: 0.636
[16,   134] loss: 0.634
[17,   134] loss: 0.639
[18,   134] loss: 0.635
[19,   134] loss: 0.634
[20,   134] loss: 0.634
[21,   134] loss: 0.633
[22,   134] loss: 0.634
[23,   134] loss: 0.633
[24,   134] loss: 0.632
[25,   134] loss: 0.637
[26,   134] loss: 0.632
[27,   134] loss: 0.634
[28,   134] loss: 0.634
[29,   134] loss: 0.641
[30,   134] loss: 0.633
[31,   134] loss: 0.635
[32,   134] loss: 0.634
[33,   134] loss: 0.635
[34,   134] loss: 0.634
[35,   134] loss: 0.636
[36,   134] loss: 0.635
[37,   134] loss: 0.636
[38,   134] loss: 0.640
[39,   134] loss: 0.639
[40,   134] loss: 0.638
[41,   134] loss: 0.634
[42,   134] loss: 0.634
[43,   134] loss: 0.639
[44,   134] loss: 0.635
[45,   134] loss: 0.643
Early stopping applied (best metric=0.37358298897743225)
Finished Training
Total time taken: 446.94857811927795
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.640
[3,   134] loss: 0.637
[4,   134] loss: 0.635
[5,   134] loss: 0.636
[6,   134] loss: 0.635
[7,   134] loss: 0.656
[8,   134] loss: 0.635
[9,   134] loss: 0.638
[10,   134] loss: 0.637
[11,   134] loss: 0.637
[12,   134] loss: 0.632
[13,   134] loss: 0.634
[14,   134] loss: 0.630
[15,   134] loss: 0.631
[16,   134] loss: 0.633
[17,   134] loss: 0.630
[18,   134] loss: 0.630
[19,   134] loss: 0.634
[20,   134] loss: 0.640
[21,   134] loss: 0.635
[22,   134] loss: 0.635
[23,   134] loss: 0.631
[24,   134] loss: 0.630
[25,   134] loss: 0.634
[26,   134] loss: 0.633
[27,   134] loss: 0.632
[28,   134] loss: 0.631
[29,   134] loss: 0.635
[30,   134] loss: 0.632
[31,   134] loss: 0.633
[32,   134] loss: 0.628
[33,   134] loss: 0.636
[34,   134] loss: 0.630
[35,   134] loss: 0.629
[36,   134] loss: 0.632
[37,   134] loss: 0.631
[38,   134] loss: 0.630
[39,   134] loss: 0.633
[40,   134] loss: 0.628
[41,   134] loss: 0.627
[42,   134] loss: 0.633
[43,   134] loss: 0.634
[44,   134] loss: 0.627
[45,   134] loss: 0.630
[46,   134] loss: 0.630
[47,   134] loss: 0.631
[48,   134] loss: 0.631
Early stopping applied (best metric=0.3691929578781128)
Finished Training
Total time taken: 477.46145701408386
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.641
[3,   134] loss: 0.640
[4,   134] loss: 0.640
[5,   134] loss: 0.642
[6,   134] loss: 0.641
[7,   134] loss: 0.642
[8,   134] loss: 0.641
[9,   134] loss: 0.640
[10,   134] loss: 0.640
[11,   134] loss: 0.641
[12,   134] loss: 0.639
[13,   134] loss: 0.637
[14,   134] loss: 0.641
[15,   134] loss: 0.636
[16,   134] loss: 0.641
[17,   134] loss: 0.642
[18,   134] loss: 0.639
[19,   134] loss: 0.636
[20,   134] loss: 0.642
[21,   134] loss: 0.640
[22,   134] loss: 0.642
[23,   134] loss: 0.641
[24,   134] loss: 0.642
[25,   134] loss: 0.642
[26,   134] loss: 0.639
[27,   134] loss: 0.646
[28,   134] loss: 0.637
[29,   134] loss: 0.638
[30,   134] loss: 0.636
[31,   134] loss: 0.639
[32,   134] loss: 0.637
[33,   134] loss: 0.643
[34,   134] loss: 0.640
[35,   134] loss: 0.636
[36,   134] loss: 0.640
Early stopping applied (best metric=0.37143248319625854)
Finished Training
Total time taken: 361.93571853637695
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.641
[3,   134] loss: 0.630
[4,   134] loss: 0.625
[5,   134] loss: 0.630
[6,   134] loss: 0.631
[7,   134] loss: 0.633
[8,   134] loss: 0.625
[9,   134] loss: 0.629
[10,   134] loss: 0.632
[11,   134] loss: 0.628
[12,   134] loss: 0.627
[13,   134] loss: 0.628
[14,   134] loss: 0.630
[15,   134] loss: 0.630
[16,   134] loss: 0.629
[17,   134] loss: 0.626
[18,   134] loss: 0.629
[19,   134] loss: 0.626
[20,   134] loss: 0.629
[21,   134] loss: 0.632
[22,   134] loss: 0.633
[23,   134] loss: 0.635
[24,   134] loss: 0.634
[25,   134] loss: 0.628
[26,   134] loss: 0.630
[27,   134] loss: 0.627
[28,   134] loss: 0.629
[29,   134] loss: 0.631
[30,   134] loss: 0.632
[31,   134] loss: 0.634
[32,   134] loss: 0.642
[33,   134] loss: 0.658
[34,   134] loss: 0.658
[35,   134] loss: 0.660
[36,   134] loss: 0.658
Early stopping applied (best metric=0.37888985872268677)
Finished Training
Total time taken: 360.99056935310364
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.638
[3,   134] loss: 0.633
[4,   134] loss: 0.633
[5,   134] loss: 0.646
[6,   134] loss: 0.680
[7,   134] loss: 0.693
[8,   134] loss: 0.693
[9,   134] loss: 0.693
[10,   134] loss: 0.693
[11,   134] loss: 0.693
[12,   134] loss: 0.693
[13,   134] loss: 0.693
[14,   134] loss: 0.693
[15,   134] loss: 0.693
[16,   134] loss: 0.693
[17,   134] loss: 0.693
[18,   134] loss: 0.693
[19,   134] loss: 0.693
[20,   134] loss: 0.693
[21,   134] loss: 0.693
[22,   134] loss: 0.693
[23,   134] loss: 0.693
[24,   134] loss: 0.693
[25,   134] loss: 0.693
[26,   134] loss: 0.693
[27,   134] loss: 0.693
[28,   134] loss: 0.693
[29,   134] loss: 0.693
Early stopping applied (best metric=0.3777335286140442)
Finished Training
Total time taken: 291.1287808418274
{'Methylation-K Validation Accuracy': 0.43620796327564987, 'Methylation-K Validation Sensitivity': 0.8759089564097008, 'Methylation-K Validation Specificity': 0.38851996088493385, 'Methylation-K Validation Precision': 0.1354915232602989, 'Methylation-K AUC ROC': 0.7264266875068173, 'Methylation-K AUC PR': 0.23765294146020746, 'Methylation-K MCC': 0.1650104161635241, 'Methylation-K F1': 0.23432478974941964, 'Validation Loss (Methylation-K)': 0.37416636347770693, 'Validation Loss (total)': 0.37416636347770693, 'TimeToTrain': 387.69302077293395}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019461172351190935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789348525,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.9387280239896527}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.565
[3,   134] loss: 0.543
[4,   134] loss: 0.531
[5,   134] loss: 0.520
[6,   134] loss: 0.516
[7,   134] loss: 0.512
[8,   134] loss: 0.508
[9,   134] loss: 0.508
[10,   134] loss: 0.503
[11,   134] loss: 0.504
[12,   134] loss: 0.501
[13,   134] loss: 0.503
[14,   134] loss: 0.503
[15,   134] loss: 0.502
[16,   134] loss: 0.505
[17,   134] loss: 0.500
[18,   134] loss: 0.497
[19,   134] loss: 0.500
[20,   134] loss: 0.498
[21,   134] loss: 0.498
[22,   134] loss: 0.499
[23,   134] loss: 0.504
[24,   134] loss: 0.502
[25,   134] loss: 0.500
[26,   134] loss: 0.499
[27,   134] loss: 0.502
[28,   134] loss: 0.498
[29,   134] loss: 0.497
[30,   134] loss: 0.499
[31,   134] loss: 0.498
Early stopping applied (best metric=0.3606805205345154)
Finished Training
Total time taken: 307.7025468349457
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.702
[2,   134] loss: 0.570
[3,   134] loss: 0.547
[4,   134] loss: 0.531
[5,   134] loss: 0.525
[6,   134] loss: 0.517
[7,   134] loss: 0.515
[8,   134] loss: 0.508
[9,   134] loss: 0.508
[10,   134] loss: 0.507
[11,   134] loss: 0.506
[12,   134] loss: 0.504
[13,   134] loss: 0.502
[14,   134] loss: 0.503
[15,   134] loss: 0.502
[16,   134] loss: 0.500
[17,   134] loss: 0.502
[18,   134] loss: 0.502
[19,   134] loss: 0.501
[20,   134] loss: 0.500
[21,   134] loss: 0.498
[22,   134] loss: 0.499
[23,   134] loss: 0.499
[24,   134] loss: 0.500
[25,   134] loss: 0.499
[26,   134] loss: 0.498
[27,   134] loss: 0.501
[28,   134] loss: 0.497
[29,   134] loss: 0.492
[30,   134] loss: 0.496
[31,   134] loss: 0.495
[32,   134] loss: 0.495
[33,   134] loss: 0.499
[34,   134] loss: 0.495
[35,   134] loss: 0.498
[36,   134] loss: 0.496
[37,   134] loss: 0.492
Early stopping applied (best metric=0.375547856092453)
Finished Training
Total time taken: 370.4707102775574
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.565
[3,   134] loss: 0.546
[4,   134] loss: 0.530
[5,   134] loss: 0.525
[6,   134] loss: 0.515
[7,   134] loss: 0.515
[8,   134] loss: 0.513
[9,   134] loss: 0.503
[10,   134] loss: 0.505
[11,   134] loss: 0.503
[12,   134] loss: 0.502
[13,   134] loss: 0.503
[14,   134] loss: 0.502
[15,   134] loss: 0.504
[16,   134] loss: 0.503
[17,   134] loss: 0.503
[18,   134] loss: 0.499
[19,   134] loss: 0.500
[20,   134] loss: 0.499
[21,   134] loss: 0.501
[22,   134] loss: 0.501
[23,   134] loss: 0.499
[24,   134] loss: 0.495
[25,   134] loss: 0.502
[26,   134] loss: 0.496
Early stopping applied (best metric=0.3513053059577942)
Finished Training
Total time taken: 261.56581234931946
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.559
[3,   134] loss: 0.539
[4,   134] loss: 0.527
[5,   134] loss: 0.519
[6,   134] loss: 0.515
[7,   134] loss: 0.509
[8,   134] loss: 0.505
[9,   134] loss: 0.504
[10,   134] loss: 0.502
[11,   134] loss: 0.503
[12,   134] loss: 0.502
[13,   134] loss: 0.498
[14,   134] loss: 0.499
[15,   134] loss: 0.500
[16,   134] loss: 0.495
[17,   134] loss: 0.499
[18,   134] loss: 0.498
[19,   134] loss: 0.497
[20,   134] loss: 0.496
[21,   134] loss: 0.496
[22,   134] loss: 0.496
[23,   134] loss: 0.496
[24,   134] loss: 0.496
[25,   134] loss: 0.495
[26,   134] loss: 0.492
[27,   134] loss: 0.496
Early stopping applied (best metric=0.3599090278148651)
Finished Training
Total time taken: 271.1010091304779
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.561
[3,   134] loss: 0.543
[4,   134] loss: 0.525
[5,   134] loss: 0.517
[6,   134] loss: 0.513
[7,   134] loss: 0.512
[8,   134] loss: 0.505
[9,   134] loss: 0.499
[10,   134] loss: 0.498
[11,   134] loss: 0.503
[12,   134] loss: 0.499
[13,   134] loss: 0.500
[14,   134] loss: 0.503
[15,   134] loss: 0.497
[16,   134] loss: 0.499
[17,   134] loss: 0.496
[18,   134] loss: 0.497
[19,   134] loss: 0.498
[20,   134] loss: 0.502
[21,   134] loss: 0.499
[22,   134] loss: 0.495
[23,   134] loss: 0.495
[24,   134] loss: 0.495
[25,   134] loss: 0.500
[26,   134] loss: 0.493
Early stopping applied (best metric=0.3608121871948242)
Finished Training
Total time taken: 261.95975708961487
{'Methylation-K Validation Accuracy': 0.6603042308320209, 'Methylation-K Validation Sensitivity': 0.7580897994179883, 'Methylation-K Validation Specificity': 0.6496991588014992, 'Methylation-K Validation Precision': 0.1928170976449066, 'Methylation-K AUC ROC': 0.7796051935056555, 'Methylation-K AUC PR': 0.28876589845511147, 'Methylation-K MCC': 0.25072127213149126, 'Methylation-K F1': 0.3062494742168332, 'Validation Loss (Methylation-K)': 0.36165097951889036, 'Validation Loss (total)': 0.36165097951889036, 'TimeToTrain': 294.5599671363831}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006242557457712341,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3731797201,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.345149679734533}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.704
[2,   134] loss: 0.649
[3,   134] loss: 0.649
[4,   134] loss: 0.664
[5,   134] loss: 0.693
[6,   134] loss: 0.693
[7,   134] loss: 0.693
[8,   134] loss: 0.693
[9,   134] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012337775577263255,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 736995133,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.90345446124104}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.607
[3,   134] loss: 0.606
[4,   134] loss: 0.604
[5,   134] loss: 0.601
[6,   134] loss: 0.599
[7,   134] loss: 0.604
[8,   134] loss: 0.607
[9,   134] loss: 0.608
[10,   134] loss: 0.607
[11,   134] loss: 0.609
[12,   134] loss: 0.605
[13,   134] loss: 0.604
[14,   134] loss: 0.605
[15,   134] loss: 0.607
[16,   134] loss: 0.607
[17,   134] loss: 0.606
[18,   134] loss: 0.604
[19,   134] loss: 0.604
[20,   134] loss: 0.603
[21,   134] loss: 0.603
[22,   134] loss: 0.603
[23,   134] loss: 0.603
[24,   134] loss: 0.602
[25,   134] loss: 0.601
[26,   134] loss: 0.610
[27,   134] loss: 0.604
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007367791053760874,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4076599254,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.749354731493728}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.618
[3,   134] loss: 0.614
[4,   134] loss: 0.614
[5,   134] loss: 0.610
[6,   134] loss: 0.616
[7,   134] loss: 0.613
[8,   134] loss: 0.614
[9,   134] loss: 0.611
[10,   134] loss: 0.621
[11,   134] loss: 0.613
[12,   134] loss: 0.611
[13,   134] loss: 0.618
[14,   134] loss: 0.615
[15,   134] loss: 0.613
[16,   134] loss: 0.614
[17,   134] loss: 0.612
[18,   134] loss: 0.612
[19,   134] loss: 0.613
[20,   134] loss: 0.613
[21,   134] loss: 0.614
[22,   134] loss: 0.612
[23,   134] loss: 0.613
[24,   134] loss: 0.610
[25,   134] loss: 0.613
[26,   134] loss: 0.613
[27,   134] loss: 0.612
[28,   134] loss: 0.615
[29,   134] loss: 0.616
[30,   134] loss: 0.612
Early stopping applied (best metric=0.3485395610332489)
Finished Training
Total time taken: 298.5203936100006
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.614
[3,   134] loss: 0.612
[4,   134] loss: 0.616
[5,   134] loss: 0.618
[6,   134] loss: 0.613
[7,   134] loss: 0.616
[8,   134] loss: 0.616
[9,   134] loss: 0.615
[10,   134] loss: 0.613
[11,   134] loss: 0.616
[12,   134] loss: 0.616
[13,   134] loss: 0.617
[14,   134] loss: 0.619
[15,   134] loss: 0.616
[16,   134] loss: 0.613
[17,   134] loss: 0.614
[18,   134] loss: 0.613
[19,   134] loss: 0.612
[20,   134] loss: 0.613
[21,   134] loss: 0.617
[22,   134] loss: 0.616
[23,   134] loss: 0.618
[24,   134] loss: 0.617
[25,   134] loss: 0.615
[26,   134] loss: 0.616
[27,   134] loss: 0.615
[28,   134] loss: 0.619
[29,   134] loss: 0.612
[30,   134] loss: 0.614
[31,   134] loss: 0.611
[32,   134] loss: 0.614
[33,   134] loss: 0.617
[34,   134] loss: 0.613
[35,   134] loss: 0.616
[36,   134] loss: 0.612
[37,   134] loss: 0.617
[38,   134] loss: 0.614
[39,   134] loss: 0.617
[40,   134] loss: 0.612
[41,   134] loss: 0.610
[42,   134] loss: 0.612
[43,   134] loss: 0.611
[44,   134] loss: 0.614
[45,   134] loss: 0.615
[46,   134] loss: 0.613
[47,   134] loss: 0.616
[48,   134] loss: 0.620
[49,   134] loss: 0.615
[50,   134] loss: 0.614
[51,   134] loss: 0.613
[52,   134] loss: 0.611
[53,   134] loss: 0.615
[54,   134] loss: 0.612
[55,   134] loss: 0.612
[56,   134] loss: 0.614
[57,   134] loss: 0.616
[58,   134] loss: 0.614
[59,   134] loss: 0.612
[60,   134] loss: 0.614
[61,   134] loss: 0.612
[62,   134] loss: 0.614
[63,   134] loss: 0.613
[64,   134] loss: 0.615
[65,   134] loss: 0.614
[66,   134] loss: 0.615
[67,   134] loss: 0.618
[68,   134] loss: 0.613
[69,   134] loss: 0.615
[70,   134] loss: 0.613
[71,   134] loss: 0.614
[72,   134] loss: 0.613
[73,   134] loss: 0.614
[74,   134] loss: 0.616
[75,   134] loss: 0.613
[76,   134] loss: 0.615
[77,   134] loss: 0.613
[78,   134] loss: 0.616
[79,   134] loss: 0.617
[80,   134] loss: 0.615
[81,   134] loss: 0.617
[82,   134] loss: 0.619
[83,   134] loss: 0.614
[84,   134] loss: 0.617
[85,   134] loss: 0.612
[86,   134] loss: 0.616
[87,   134] loss: 0.610
[88,   134] loss: 0.617
[89,   134] loss: 0.616
[90,   134] loss: 0.612
[91,   134] loss: 0.611
[92,   134] loss: 0.611
[93,   134] loss: 0.612
[94,   134] loss: 0.616
[95,   134] loss: 0.616
[96,   134] loss: 0.616
[97,   134] loss: 0.611
Early stopping applied (best metric=0.34793829917907715)
Finished Training
Total time taken: 967.0934190750122
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.617
[3,   134] loss: 0.614
[4,   134] loss: 0.615
[5,   134] loss: 0.616
[6,   134] loss: 0.614
[7,   134] loss: 0.612
[8,   134] loss: 0.614
[9,   134] loss: 0.617
[10,   134] loss: 0.614
[11,   134] loss: 0.615
[12,   134] loss: 0.613
[13,   134] loss: 0.615
[14,   134] loss: 0.613
[15,   134] loss: 0.614
[16,   134] loss: 0.616
[17,   134] loss: 0.612
[18,   134] loss: 0.612
[19,   134] loss: 0.612
[20,   134] loss: 0.614
[21,   134] loss: 0.613
[22,   134] loss: 0.612
[23,   134] loss: 0.614
[24,   134] loss: 0.614
[25,   134] loss: 0.618
[26,   134] loss: 0.612
[27,   134] loss: 0.616
[28,   134] loss: 0.615
[29,   134] loss: 0.614
[30,   134] loss: 0.615
[31,   134] loss: 0.616
[32,   134] loss: 0.611
[33,   134] loss: 0.615
[34,   134] loss: 0.614
[35,   134] loss: 0.616
[36,   134] loss: 0.616
[37,   134] loss: 0.615
[38,   134] loss: 0.615
[39,   134] loss: 0.616
[40,   134] loss: 0.617
[41,   134] loss: 0.618
[42,   134] loss: 0.615
[43,   134] loss: 0.622
[44,   134] loss: 0.616
Early stopping applied (best metric=0.35114866495132446)
Finished Training
Total time taken: 434.4905409812927
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.613
[3,   134] loss: 0.614
[4,   134] loss: 0.615
[5,   134] loss: 0.614
[6,   134] loss: 0.614
[7,   134] loss: 0.618
[8,   134] loss: 0.617
[9,   134] loss: 0.612
[10,   134] loss: 0.613
[11,   134] loss: 0.617
[12,   134] loss: 0.613
[13,   134] loss: 0.617
[14,   134] loss: 0.615
[15,   134] loss: 0.616
[16,   134] loss: 0.614
[17,   134] loss: 0.616
[18,   134] loss: 0.617
[19,   134] loss: 0.612
[20,   134] loss: 0.612
[21,   134] loss: 0.615
[22,   134] loss: 0.613
[23,   134] loss: 0.617
[24,   134] loss: 0.616
[25,   134] loss: 0.613
[26,   134] loss: 0.614
[27,   134] loss: 0.616
[28,   134] loss: 0.613
[29,   134] loss: 0.613
[30,   134] loss: 0.614
[31,   134] loss: 0.615
Early stopping applied (best metric=0.34910672903060913)
Finished Training
Total time taken: 306.2560667991638
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.611
[3,   134] loss: 0.606
[4,   134] loss: 0.610
[5,   134] loss: 0.611
[6,   134] loss: 0.615
[7,   134] loss: 0.613
[8,   134] loss: 0.615
[9,   134] loss: 0.613
[10,   134] loss: 0.612
[11,   134] loss: 0.612
[12,   134] loss: 0.617
[13,   134] loss: 0.611
[14,   134] loss: 0.610
[15,   134] loss: 0.612
[16,   134] loss: 0.609
[17,   134] loss: 0.612
[18,   134] loss: 0.615
[19,   134] loss: 0.615
[20,   134] loss: 0.609
[21,   134] loss: 0.611
[22,   134] loss: 0.612
[23,   134] loss: 0.612
[24,   134] loss: 0.612
[25,   134] loss: 0.617
[26,   134] loss: 0.611
[27,   134] loss: 0.612
[28,   134] loss: 0.612
[29,   134] loss: 0.613
[30,   134] loss: 0.613
[31,   134] loss: 0.609
[32,   134] loss: 0.616
[33,   134] loss: 0.613
[34,   134] loss: 0.613
Early stopping applied (best metric=0.36856383085250854)
Finished Training
Total time taken: 335.9919512271881
{'Methylation-K Validation Accuracy': 0.47503370838188924, 'Methylation-K Validation Sensitivity': 0.8826025568439961, 'Methylation-K Validation Specificity': 0.4308318497205842, 'Methylation-K Validation Precision': 0.14410478517410463, 'Methylation-K AUC ROC': 0.7496165928140313, 'Methylation-K AUC PR': 0.25982839044716277, 'Methylation-K MCC': 0.1902824389465963, 'Methylation-K F1': 0.2477066052003715, 'Validation Loss (Methylation-K)': 0.3530594170093536, 'Validation Loss (total)': 0.3530594170093536, 'TimeToTrain': 468.4704743385315}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00012282179026360733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3201320564,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.762735735558985}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.611
[3,   134] loss: 0.596
[4,   134] loss: 0.585
[5,   134] loss: 0.566
[6,   134] loss: 0.551
[7,   134] loss: 0.532
[8,   134] loss: 0.519
[9,   134] loss: 0.504
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037345677426246825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3391979183,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.439195098572581}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.610
[3,   134] loss: 0.612
[4,   134] loss: 0.606
[5,   134] loss: 0.609
[6,   134] loss: 0.608
[7,   134] loss: 0.607
[8,   134] loss: 0.608
[9,   134] loss: 0.610
[10,   134] loss: 0.605
[11,   134] loss: 0.608
[12,   134] loss: 0.611
[13,   134] loss: 0.613
[14,   134] loss: 0.608
[15,   134] loss: 0.613
[16,   134] loss: 0.610
[17,   134] loss: 0.608
[18,   134] loss: 0.611
[19,   134] loss: 0.610
[20,   134] loss: 0.609
[21,   134] loss: 0.611
[22,   134] loss: 0.608
[23,   134] loss: 0.608
[24,   134] loss: 0.610
[25,   134] loss: 0.612
[26,   134] loss: 0.611
[27,   134] loss: 0.612
[28,   134] loss: 0.611
[29,   134] loss: 0.607
[30,   134] loss: 0.613
[31,   134] loss: 0.609
[32,   134] loss: 0.611
[33,   134] loss: 0.611
[34,   134] loss: 0.608
[35,   134] loss: 0.607
[36,   134] loss: 0.609
[37,   134] loss: 0.607
[38,   134] loss: 0.606
[39,   134] loss: 0.605
[40,   134] loss: 0.606
[41,   134] loss: 0.609
[42,   134] loss: 0.607
[43,   134] loss: 0.607
[44,   134] loss: 0.608
[45,   134] loss: 0.606
[46,   134] loss: 0.609
[47,   134] loss: 0.605
[48,   134] loss: 0.606
[49,   134] loss: 0.610
[50,   134] loss: 0.609
[51,   134] loss: 0.609
[52,   134] loss: 0.611
[53,   134] loss: 0.610
[54,   134] loss: 0.605
[55,   134] loss: 0.606
[56,   134] loss: 0.606
[57,   134] loss: 0.611
[58,   134] loss: 0.607
[59,   134] loss: 0.605
[60,   134] loss: 0.605
[61,   134] loss: 0.607
[62,   134] loss: 0.607
[63,   134] loss: 0.607
[64,   134] loss: 0.607
[65,   134] loss: 0.612
[66,   134] loss: 0.609
[67,   134] loss: 0.609
[68,   134] loss: 0.608
[69,   134] loss: 0.610
[70,   134] loss: 0.613
[71,   134] loss: 0.606
[72,   134] loss: 0.610
[73,   134] loss: 0.611
Early stopping applied (best metric=0.35926318168640137)
Finished Training
Total time taken: 716.1396992206573
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.609
[3,   134] loss: 0.609
[4,   134] loss: 0.606
[5,   134] loss: 0.612
[6,   134] loss: 0.608
[7,   134] loss: 0.609
[8,   134] loss: 0.609
[9,   134] loss: 0.612
[10,   134] loss: 0.615
[11,   134] loss: 0.612
[12,   134] loss: 0.608
[13,   134] loss: 0.611
[14,   134] loss: 0.614
[15,   134] loss: 0.613
[16,   134] loss: 0.608
[17,   134] loss: 0.610
[18,   134] loss: 0.609
[19,   134] loss: 0.606
[20,   134] loss: 0.607
[21,   134] loss: 0.608
[22,   134] loss: 0.610
[23,   134] loss: 0.608
[24,   134] loss: 0.611
[25,   134] loss: 0.610
[26,   134] loss: 0.606
Early stopping applied (best metric=0.3591576814651489)
Finished Training
Total time taken: 245.5108139514923
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.608
[3,   134] loss: 0.611
[4,   134] loss: 0.611
[5,   134] loss: 0.610
[6,   134] loss: 0.608
[7,   134] loss: 0.609
[8,   134] loss: 0.607
[9,   134] loss: 0.606
[10,   134] loss: 0.608
[11,   134] loss: 0.606
[12,   134] loss: 0.612
[13,   134] loss: 0.611
[14,   134] loss: 0.610
[15,   134] loss: 0.606
[16,   134] loss: 0.610
[17,   134] loss: 0.606
[18,   134] loss: 0.610
[19,   134] loss: 0.610
[20,   134] loss: 0.609
[21,   134] loss: 0.611
[22,   134] loss: 0.608
[23,   134] loss: 0.609
[24,   134] loss: 0.609
[25,   134] loss: 0.609
[26,   134] loss: 0.610
[27,   134] loss: 0.613
[28,   134] loss: 0.613
[29,   134] loss: 0.609
[30,   134] loss: 0.604
[31,   134] loss: 0.611
[32,   134] loss: 0.609
[33,   134] loss: 0.608
[34,   134] loss: 0.605
[35,   134] loss: 0.611
[36,   134] loss: 0.609
[37,   134] loss: 0.607
[38,   134] loss: 0.606
[39,   134] loss: 0.609
[40,   134] loss: 0.610
[41,   134] loss: 0.613
[42,   134] loss: 0.608
[43,   134] loss: 0.609
[44,   134] loss: 0.608
[45,   134] loss: 0.607
[46,   134] loss: 0.605
[47,   134] loss: 0.611
[48,   134] loss: 0.611
[49,   134] loss: 0.608
[50,   134] loss: 0.606
[51,   134] loss: 0.608
[52,   134] loss: 0.610
[53,   134] loss: 0.610
[54,   134] loss: 0.610
[55,   134] loss: 0.609
[56,   134] loss: 0.608
Early stopping applied (best metric=0.3552926778793335)
Finished Training
Total time taken: 542.9502904415131
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.616
[3,   134] loss: 0.613
[4,   134] loss: 0.612
[5,   134] loss: 0.611
[6,   134] loss: 0.610
[7,   134] loss: 0.614
[8,   134] loss: 0.615
[9,   134] loss: 0.612
[10,   134] loss: 0.610
[11,   134] loss: 0.614
[12,   134] loss: 0.614
[13,   134] loss: 0.612
[14,   134] loss: 0.617
[15,   134] loss: 0.612
[16,   134] loss: 0.612
[17,   134] loss: 0.613
[18,   134] loss: 0.613
[19,   134] loss: 0.608
[20,   134] loss: 0.612
[21,   134] loss: 0.615
[22,   134] loss: 0.608
[23,   134] loss: 0.612
[24,   134] loss: 0.614
[25,   134] loss: 0.616
[26,   134] loss: 0.609
[27,   134] loss: 0.613
[28,   134] loss: 0.615
[29,   134] loss: 0.617
[30,   134] loss: 0.618
[31,   134] loss: 0.614
[32,   134] loss: 0.614
[33,   134] loss: 0.613
[34,   134] loss: 0.615
[35,   134] loss: 0.614
[36,   134] loss: 0.613
[37,   134] loss: 0.612
[38,   134] loss: 0.614
Early stopping applied (best metric=0.3604018986225128)
Finished Training
Total time taken: 370.04052686691284
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.605
[3,   134] loss: 0.603
[4,   134] loss: 0.603
[5,   134] loss: 0.607
[6,   134] loss: 0.607
[7,   134] loss: 0.607
[8,   134] loss: 0.607
[9,   134] loss: 0.605
[10,   134] loss: 0.606
[11,   134] loss: 0.607
[12,   134] loss: 0.606
[13,   134] loss: 0.605
[14,   134] loss: 0.608
[15,   134] loss: 0.608
[16,   134] loss: 0.608
[17,   134] loss: 0.610
[18,   134] loss: 0.610
[19,   134] loss: 0.608
[20,   134] loss: 0.607
[21,   134] loss: 0.606
[22,   134] loss: 0.608
[23,   134] loss: 0.607
[24,   134] loss: 0.604
[25,   134] loss: 0.609
[26,   134] loss: 0.607
[27,   134] loss: 0.605
[28,   134] loss: 0.605
[29,   134] loss: 0.609
[30,   134] loss: 0.607
[31,   134] loss: 0.607
[32,   134] loss: 0.607
[33,   134] loss: 0.605
[34,   134] loss: 0.606
[35,   134] loss: 0.603
[36,   134] loss: 0.608
[37,   134] loss: 0.608
[38,   134] loss: 0.606
[39,   134] loss: 0.608
[40,   134] loss: 0.609
Early stopping applied (best metric=0.34968996047973633)
Finished Training
Total time taken: 395.6085515022278
{'Methylation-K Validation Accuracy': 0.4536650859385821, 'Methylation-K Validation Sensitivity': 0.8901649809762792, 'Methylation-K Validation Specificity': 0.40632740833108183, 'Methylation-K Validation Precision': 0.14023324684809835, 'Methylation-K AUC ROC': 0.7532055087971986, 'Methylation-K AUC PR': 0.2633448936690837, 'Methylation-K MCC': 0.1823030292428736, 'Methylation-K F1': 0.24216993730280925, 'Validation Loss (Methylation-K)': 0.3567610800266266, 'Validation Loss (total)': 0.3567610800266266, 'TimeToTrain': 454.0499763965607}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007091795818547314,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3285626402,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.197577387583402}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.596
[3,   134] loss: 0.586
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007655312252684848,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2973823884,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.114247009686709}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.701
[2,   134] loss: 0.632
[3,   134] loss: 0.634
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005591948724906546,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1764238424,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.718387417102702}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.619
[3,   134] loss: 0.616
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007937304044838164,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 245276518,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.5864875162823875}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.611
[3,   134] loss: 0.610
[4,   134] loss: 0.608
[5,   134] loss: 0.608
[6,   134] loss: 0.613
[7,   134] loss: 0.610
[8,   134] loss: 0.608
[9,   134] loss: 0.608
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006807586076256742,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2002869232,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.064228749430402}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.594
[3,   134] loss: 0.590
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016450361874821007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1420726727,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.2301486978202325}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.577
[3,   134] loss: 0.574
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035285735577969975,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4204577424,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.66813944015571}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.594
[3,   134] loss: 0.590
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009892054307535546,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2037160083,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.15806421197653}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.666
[3,   134] loss: 0.644
[4,   134] loss: 0.648
[5,   134] loss: 0.645
[6,   134] loss: 0.644
[7,   134] loss: 0.644
[8,   134] loss: 0.654
[9,   134] loss: 0.665
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005527858816445528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3020583209,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.81555817958071}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.628
[3,   134] loss: 0.627
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004409332224761264,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2572176726,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.48320756958999}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.617
[3,   134] loss: 0.613
[4,   134] loss: 0.614
[5,   134] loss: 0.613
[6,   134] loss: 0.620
[7,   134] loss: 0.616
[8,   134] loss: 0.615
[9,   134] loss: 0.615
[10,   134] loss: 0.616
[11,   134] loss: 0.615
[12,   134] loss: 0.616
[13,   134] loss: 0.616
[14,   134] loss: 0.613
[15,   134] loss: 0.615
[16,   134] loss: 0.612
[17,   134] loss: 0.614
[18,   134] loss: 0.615
[19,   134] loss: 0.614
[20,   134] loss: 0.614
[21,   134] loss: 0.612
[22,   134] loss: 0.611
[23,   134] loss: 0.614
[24,   134] loss: 0.611
[25,   134] loss: 0.613
[26,   134] loss: 0.615
[27,   134] loss: 0.611
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035129061522779065,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 144735030,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.022104935809049}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.606
[3,   134] loss: 0.607
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002012699672731159,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3528702424,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.605347517164233}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.605
[3,   134] loss: 0.612
[4,   134] loss: 0.603
[5,   134] loss: 0.605
[6,   134] loss: 0.605
[7,   134] loss: 0.606
[8,   134] loss: 0.609
[9,   134] loss: 0.604
[10,   134] loss: 0.607
[11,   134] loss: 0.608
[12,   134] loss: 0.611
[13,   134] loss: 0.608
[14,   134] loss: 0.606
[15,   134] loss: 0.608
[16,   134] loss: 0.607
[17,   134] loss: 0.607
[18,   134] loss: 0.605
[19,   134] loss: 0.607
[20,   134] loss: 0.608
[21,   134] loss: 0.606
[22,   134] loss: 0.605
[23,   134] loss: 0.607
[24,   134] loss: 0.610
[25,   134] loss: 0.606
[26,   134] loss: 0.607
[27,   134] loss: 0.608
[28,   134] loss: 0.607
[29,   134] loss: 0.605
[30,   134] loss: 0.609
[31,   134] loss: 0.606
[32,   134] loss: 0.608
[33,   134] loss: 0.611
[34,   134] loss: 0.609
[35,   134] loss: 0.610
[36,   134] loss: 0.610
[37,   134] loss: 0.611
[38,   134] loss: 0.607
[39,   134] loss: 0.609
[40,   134] loss: 0.606
[41,   134] loss: 0.606
[42,   134] loss: 0.606
[43,   134] loss: 0.608
[44,   134] loss: 0.606
[45,   134] loss: 0.609
[46,   134] loss: 0.605
[47,   134] loss: 0.608
[48,   134] loss: 0.608
[49,   134] loss: 0.608
[50,   134] loss: 0.608
[51,   134] loss: 0.611
[52,   134] loss: 0.610
[53,   134] loss: 0.605
Early stopping applied (best metric=0.3538704216480255)
Finished Training
Total time taken: 517.4835572242737
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.611
[3,   134] loss: 0.607
[4,   134] loss: 0.609
[5,   134] loss: 0.609
[6,   134] loss: 0.609
[7,   134] loss: 0.608
[8,   134] loss: 0.604
[9,   134] loss: 0.608
[10,   134] loss: 0.611
[11,   134] loss: 0.608
[12,   134] loss: 0.608
[13,   134] loss: 0.607
[14,   134] loss: 0.605
[15,   134] loss: 0.606
[16,   134] loss: 0.604
[17,   134] loss: 0.606
[18,   134] loss: 0.610
[19,   134] loss: 0.609
[20,   134] loss: 0.606
[21,   134] loss: 0.605
[22,   134] loss: 0.605
[23,   134] loss: 0.610
[24,   134] loss: 0.606
[25,   134] loss: 0.605
[26,   134] loss: 0.610
[27,   134] loss: 0.607
[28,   134] loss: 0.606
[29,   134] loss: 0.607
[30,   134] loss: 0.605
[31,   134] loss: 0.605
[32,   134] loss: 0.608
[33,   134] loss: 0.609
[34,   134] loss: 0.607
[35,   134] loss: 0.608
Early stopping applied (best metric=0.3594113290309906)
Finished Training
Total time taken: 339.8263292312622
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.699
[2,   134] loss: 0.607
[3,   134] loss: 0.603
[4,   134] loss: 0.606
[5,   134] loss: 0.606
[6,   134] loss: 0.609
[7,   134] loss: 0.609
[8,   134] loss: 0.609
[9,   134] loss: 0.608
[10,   134] loss: 0.610
[11,   134] loss: 0.606
[12,   134] loss: 0.604
[13,   134] loss: 0.607
[14,   134] loss: 0.610
[15,   134] loss: 0.605
[16,   134] loss: 0.609
[17,   134] loss: 0.612
[18,   134] loss: 0.611
[19,   134] loss: 0.612
[20,   134] loss: 0.609
[21,   134] loss: 0.610
[22,   134] loss: 0.609
[23,   134] loss: 0.609
[24,   134] loss: 0.609
[25,   134] loss: 0.607
[26,   134] loss: 0.607
[27,   134] loss: 0.609
[28,   134] loss: 0.607
[29,   134] loss: 0.613
[30,   134] loss: 0.611
Early stopping applied (best metric=0.36578914523124695)
Finished Training
Total time taken: 293.35333347320557
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.605
[3,   134] loss: 0.606
[4,   134] loss: 0.605
[5,   134] loss: 0.604
[6,   134] loss: 0.607
[7,   134] loss: 0.606
[8,   134] loss: 0.606
[9,   134] loss: 0.603
[10,   134] loss: 0.605
[11,   134] loss: 0.605
[12,   134] loss: 0.604
[13,   134] loss: 0.604
[14,   134] loss: 0.608
[15,   134] loss: 0.602
[16,   134] loss: 0.607
[17,   134] loss: 0.606
[18,   134] loss: 0.602
[19,   134] loss: 0.602
[20,   134] loss: 0.603
[21,   134] loss: 0.606
[22,   134] loss: 0.606
[23,   134] loss: 0.603
[24,   134] loss: 0.603
[25,   134] loss: 0.605
[26,   134] loss: 0.606
[27,   134] loss: 0.604
[28,   134] loss: 0.603
[29,   134] loss: 0.609
[30,   134] loss: 0.605
[31,   134] loss: 0.608
[32,   134] loss: 0.605
[33,   134] loss: 0.606
[34,   134] loss: 0.607
[35,   134] loss: 0.608
[36,   134] loss: 0.606
[37,   134] loss: 0.608
[38,   134] loss: 0.606
[39,   134] loss: 0.604
[40,   134] loss: 0.606
Early stopping applied (best metric=0.36858099699020386)
Finished Training
Total time taken: 394.50134110450745
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.613
[3,   134] loss: 0.607
[4,   134] loss: 0.610
[5,   134] loss: 0.606
[6,   134] loss: 0.605
[7,   134] loss: 0.606
[8,   134] loss: 0.608
[9,   134] loss: 0.610
[10,   134] loss: 0.605
[11,   134] loss: 0.612
[12,   134] loss: 0.607
[13,   134] loss: 0.611
[14,   134] loss: 0.610
[15,   134] loss: 0.609
[16,   134] loss: 0.609
[17,   134] loss: 0.607
[18,   134] loss: 0.606
[19,   134] loss: 0.608
[20,   134] loss: 0.607
[21,   134] loss: 0.608
[22,   134] loss: 0.612
[23,   134] loss: 0.611
[24,   134] loss: 0.609
[25,   134] loss: 0.606
[26,   134] loss: 0.607
[27,   134] loss: 0.610
[28,   134] loss: 0.606
[29,   134] loss: 0.604
[30,   134] loss: 0.606
[31,   134] loss: 0.608
[32,   134] loss: 0.614
[33,   134] loss: 0.614
[34,   134] loss: 0.608
[35,   134] loss: 0.608
[36,   134] loss: 0.607
[37,   134] loss: 0.606
[38,   134] loss: 0.604
[39,   134] loss: 0.607
[40,   134] loss: 0.609
Early stopping applied (best metric=0.36109939217567444)
Finished Training
Total time taken: 390.09722661972046
{'Methylation-K Validation Accuracy': 0.49050878008454485, 'Methylation-K Validation Sensitivity': 0.8675017066595837, 'Methylation-K Validation Specificity': 0.4496239467901449, 'Methylation-K Validation Precision': 0.14629347656709715, 'Methylation-K AUC ROC': 0.7463781945625828, 'Methylation-K AUC PR': 0.25528340534297966, 'Methylation-K MCC': 0.19155785688656823, 'Methylation-K F1': 0.2502151878884873, 'Validation Loss (Methylation-K)': 0.3617502570152283, 'Validation Loss (total)': 0.3617502570152283, 'TimeToTrain': 387.0523575305939}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006776628361137174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4091816400,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.77437103542455}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.639
[3,   134] loss: 0.635
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004254869579857526,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927649494,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.748681296088117}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.583
[3,   134] loss: 0.572
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035586389049880075,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 564958008,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.548500406885706}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.619
[3,   134] loss: 0.623
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038175496386651255,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4277592220,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.726237207492627}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.631
[3,   134] loss: 0.635
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025923417861239157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 611473948,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.465177902394583}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.632
[3,   134] loss: 0.629
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009336374586580796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3826206975,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.811509490026886}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.693
[3,   134] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009423248262306776,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2101611541,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.369486516398128}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.609
[3,   134] loss: 0.612
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00762584069369862,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 262634799,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.837932283742447}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.667
[3,   134] loss: 0.640
[4,   134] loss: 0.644
[5,   134] loss: 0.642
[6,   134] loss: 0.638
[7,   134] loss: 0.636
[8,   134] loss: 0.639
[9,   134] loss: 0.653
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004205320866559516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4039850054,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.366110422565594}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.650
[3,   134] loss: 0.649
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012392025739260742,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1631302674,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.7257750934429636}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.562
[3,   134] loss: 0.533
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005810282205701079,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 941471603,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.343188109527063}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.600
[3,   134] loss: 0.599
[4,   134] loss: 0.596
[5,   134] loss: 0.593
[6,   134] loss: 0.593
[7,   134] loss: 0.594
[8,   134] loss: 0.594
[9,   134] loss: 0.597
[10,   134] loss: 0.593
[11,   134] loss: 0.599
[12,   134] loss: 0.597
[13,   134] loss: 0.596
[14,   134] loss: 0.598
[15,   134] loss: 0.593
[16,   134] loss: 0.597
[17,   134] loss: 0.599
[18,   134] loss: 0.599
[19,   134] loss: 0.600
[20,   134] loss: 0.598
[21,   134] loss: 0.594
[22,   134] loss: 0.603
[23,   134] loss: 0.600
[24,   134] loss: 0.602
[25,   134] loss: 0.600
[26,   134] loss: 0.597
[27,   134] loss: 0.598
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005454549185052599,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1921853519,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.720740815191123}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.633
[3,   134] loss: 0.629
[4,   134] loss: 0.640
[5,   134] loss: 0.635
[6,   134] loss: 0.634
[7,   134] loss: 0.633
[8,   134] loss: 0.638
[9,   134] loss: 0.637
[10,   134] loss: 0.634
[11,   134] loss: 0.634
[12,   134] loss: 0.637
[13,   134] loss: 0.635
[14,   134] loss: 0.631
[15,   134] loss: 0.636
[16,   134] loss: 0.634
[17,   134] loss: 0.635
[18,   134] loss: 0.633
[19,   134] loss: 0.636
[20,   134] loss: 0.638
[21,   134] loss: 0.638
[22,   134] loss: 0.632
[23,   134] loss: 0.633
[24,   134] loss: 0.634
[25,   134] loss: 0.637
[26,   134] loss: 0.638
[27,   134] loss: 0.635
[28,   134] loss: 0.633
[29,   134] loss: 0.634
[30,   134] loss: 0.634
[31,   134] loss: 0.630
[32,   134] loss: 0.639
[33,   134] loss: 0.685
[34,   134] loss: 0.693
[35,   134] loss: 0.693
[36,   134] loss: 0.693
[37,   134] loss: 0.693
[38,   134] loss: 0.693
[39,   134] loss: 0.693
[40,   134] loss: 0.693
[41,   134] loss: 0.693
[42,   134] loss: 0.693
[43,   134] loss: 0.693
[44,   134] loss: 0.693
[45,   134] loss: 0.693
[46,   134] loss: 0.693
[47,   134] loss: 0.693
[48,   134] loss: 0.693
[49,   134] loss: 0.693
[50,   134] loss: 0.693
[51,   134] loss: 0.693
[52,   134] loss: 0.693
[53,   134] loss: 0.693
[54,   134] loss: 0.693
Early stopping applied (best metric=0.37164121866226196)
Finished Training
Total time taken: 520.702045917511
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.633
[3,   134] loss: 0.632
[4,   134] loss: 0.635
[5,   134] loss: 0.631
[6,   134] loss: 0.632
[7,   134] loss: 0.633
[8,   134] loss: 0.633
[9,   134] loss: 0.631
[10,   134] loss: 0.636
[11,   134] loss: 0.630
[12,   134] loss: 0.633
[13,   134] loss: 0.634
[14,   134] loss: 0.636
[15,   134] loss: 0.632
[16,   134] loss: 0.634
[17,   134] loss: 0.631
[18,   134] loss: 0.632
[19,   134] loss: 0.634
[20,   134] loss: 0.635
[21,   134] loss: 0.632
[22,   134] loss: 0.636
[23,   134] loss: 0.633
[24,   134] loss: 0.632
[25,   134] loss: 0.634
[26,   134] loss: 0.636
[27,   134] loss: 0.639
[28,   134] loss: 0.635
[29,   134] loss: 0.635
[30,   134] loss: 0.633
Early stopping applied (best metric=0.36460018157958984)
Finished Training
Total time taken: 282.5913362503052
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.636
[3,   134] loss: 0.633
[4,   134] loss: 0.634
[5,   134] loss: 0.632
[6,   134] loss: 0.634
[7,   134] loss: 0.632
[8,   134] loss: 0.632
[9,   134] loss: 0.632
[10,   134] loss: 0.631
[11,   134] loss: 0.633
[12,   134] loss: 0.635
[13,   134] loss: 0.633
[14,   134] loss: 0.637
[15,   134] loss: 0.635
[16,   134] loss: 0.633
[17,   134] loss: 0.636
[18,   134] loss: 0.635
[19,   134] loss: 0.635
[20,   134] loss: 0.632
[21,   134] loss: 0.633
[22,   134] loss: 0.633
[23,   134] loss: 0.638
[24,   134] loss: 0.635
[25,   134] loss: 0.634
[26,   134] loss: 0.636
[27,   134] loss: 0.634
[28,   134] loss: 0.633
[29,   134] loss: 0.633
[30,   134] loss: 0.632
[31,   134] loss: 0.632
[32,   134] loss: 0.634
[33,   134] loss: 0.631
[34,   134] loss: 0.632
[35,   134] loss: 0.629
[36,   134] loss: 0.633
[37,   134] loss: 0.636
[38,   134] loss: 0.635
[39,   134] loss: 0.638
[40,   134] loss: 0.634
[41,   134] loss: 0.632
[42,   134] loss: 0.631
[43,   134] loss: 0.635
[44,   134] loss: 0.633
[45,   134] loss: 0.634
[46,   134] loss: 0.633
[47,   134] loss: 0.633
[48,   134] loss: 0.634
[49,   134] loss: 0.633
[50,   134] loss: 0.635
[51,   134] loss: 0.634
[52,   134] loss: 0.632
[53,   134] loss: 0.631
[54,   134] loss: 0.633
[55,   134] loss: 0.633
[56,   134] loss: 0.632
[57,   134] loss: 0.635
[58,   134] loss: 0.632
[59,   134] loss: 0.631
[60,   134] loss: 0.633
[61,   134] loss: 0.634
[62,   134] loss: 0.632
[63,   134] loss: 0.635
[64,   134] loss: 0.634
[65,   134] loss: 0.632
[66,   134] loss: 0.630
[67,   134] loss: 0.634
[68,   134] loss: 0.630
[69,   134] loss: 0.632
[70,   134] loss: 0.633
[71,   134] loss: 0.632
[72,   134] loss: 0.636
[73,   134] loss: 0.639
[74,   134] loss: 0.628
[75,   134] loss: 0.633
[76,   134] loss: 0.635
[77,   134] loss: 0.631
[78,   134] loss: 0.632
[79,   134] loss: 0.630
[80,   134] loss: 0.630
[81,   134] loss: 0.630
[82,   134] loss: 0.632
[83,   134] loss: 0.629
[84,   134] loss: 0.631
[85,   134] loss: 0.632
[86,   134] loss: 0.630
[87,   134] loss: 0.631
[88,   134] loss: 0.634
[89,   134] loss: 0.631
[90,   134] loss: 0.629
[91,   134] loss: 0.630
[92,   134] loss: 0.631
[93,   134] loss: 0.631
[94,   134] loss: 0.631
[95,   134] loss: 0.629
[96,   134] loss: 0.627
[97,   134] loss: 0.631
[98,   134] loss: 0.628
[99,   134] loss: 0.628
[100,   134] loss: 0.631
[101,   134] loss: 0.631
[102,   134] loss: 0.629
[103,   134] loss: 0.631
[104,   134] loss: 0.633
[105,   134] loss: 0.631
[106,   134] loss: 0.633
Early stopping applied (best metric=0.36713117361068726)
Finished Training
Total time taken: 1026.4219982624054
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.630
[3,   134] loss: 0.629
[4,   134] loss: 0.622
[5,   134] loss: 0.627
[6,   134] loss: 0.623
[7,   134] loss: 0.624
[8,   134] loss: 0.626
[9,   134] loss: 0.625
[10,   134] loss: 0.628
[11,   134] loss: 0.624
[12,   134] loss: 0.627
[13,   134] loss: 0.630
[14,   134] loss: 0.627
[15,   134] loss: 0.625
[16,   134] loss: 0.625
[17,   134] loss: 0.624
[18,   134] loss: 0.629
[19,   134] loss: 0.630
[20,   134] loss: 0.627
[21,   134] loss: 0.624
[22,   134] loss: 0.627
[23,   134] loss: 0.627
[24,   134] loss: 0.627
[25,   134] loss: 0.623
[26,   134] loss: 0.626
[27,   134] loss: 0.629
[28,   134] loss: 0.630
[29,   134] loss: 0.625
[30,   134] loss: 0.628
[31,   134] loss: 0.627
[32,   134] loss: 0.623
[33,   134] loss: 0.625
[34,   134] loss: 0.630
[35,   134] loss: 0.626
[36,   134] loss: 0.625
[37,   134] loss: 0.622
[38,   134] loss: 0.624
[39,   134] loss: 0.624
[40,   134] loss: 0.625
[41,   134] loss: 0.624
[42,   134] loss: 0.625
[43,   134] loss: 0.631
[44,   134] loss: 0.626
[45,   134] loss: 0.627
[46,   134] loss: 0.627
[47,   134] loss: 0.624
[48,   134] loss: 0.625
[49,   134] loss: 0.629
[50,   134] loss: 0.626
[51,   134] loss: 0.626
[52,   134] loss: 0.631
[53,   134] loss: 0.628
[54,   134] loss: 0.628
[55,   134] loss: 0.627
[56,   134] loss: 0.623
[57,   134] loss: 0.629
[58,   134] loss: 0.625
[59,   134] loss: 0.625
[60,   134] loss: 0.627
[61,   134] loss: 0.627
[62,   134] loss: 0.627
[63,   134] loss: 0.624
[64,   134] loss: 0.623
[65,   134] loss: 0.627
[66,   134] loss: 0.623
[67,   134] loss: 0.628
[68,   134] loss: 0.626
[69,   134] loss: 0.628
[70,   134] loss: 0.628
[71,   134] loss: 0.628
[72,   134] loss: 0.629
[73,   134] loss: 0.625
[74,   134] loss: 0.627
[75,   134] loss: 0.627
[76,   134] loss: 0.624
[77,   134] loss: 0.627
[78,   134] loss: 0.627
[79,   134] loss: 0.628
[80,   134] loss: 0.626
[81,   134] loss: 0.625
[82,   134] loss: 0.627
[83,   134] loss: 0.629
[84,   134] loss: 0.625
Early stopping applied (best metric=0.36275210976600647)
Finished Training
Total time taken: 820.4195199012756
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.627
[3,   134] loss: 0.629
[4,   134] loss: 0.630
[5,   134] loss: 0.629
[6,   134] loss: 0.628
[7,   134] loss: 0.628
[8,   134] loss: 0.632
[9,   134] loss: 0.632
[10,   134] loss: 0.632
[11,   134] loss: 0.627
[12,   134] loss: 0.630
[13,   134] loss: 0.628
[14,   134] loss: 0.629
[15,   134] loss: 0.628
[16,   134] loss: 0.631
[17,   134] loss: 0.628
[18,   134] loss: 0.627
[19,   134] loss: 0.625
[20,   134] loss: 0.627
[21,   134] loss: 0.626
[22,   134] loss: 0.625
[23,   134] loss: 0.629
[24,   134] loss: 0.631
[25,   134] loss: 0.625
[26,   134] loss: 0.629
[27,   134] loss: 0.625
[28,   134] loss: 0.626
[29,   134] loss: 0.628
[30,   134] loss: 0.627
[31,   134] loss: 0.641
[32,   134] loss: 0.630
[33,   134] loss: 0.629
[34,   134] loss: 0.626
[35,   134] loss: 0.624
[36,   134] loss: 0.630
[37,   134] loss: 0.628
Early stopping applied (best metric=0.3748016357421875)
Finished Training
Total time taken: 362.72575402259827
{'Methylation-K Validation Accuracy': 0.42495228451772465, 'Methylation-K Validation Sensitivity': 0.8931761575578808, 'Methylation-K Validation Specificity': 0.3741723590791554, 'Methylation-K Validation Precision': 0.1351501568661603, 'Methylation-K AUC ROC': 0.7334769730844821, 'Methylation-K AUC PR': 0.23706763637005898, 'Methylation-K MCC': 0.16821507714452802, 'Methylation-K F1': 0.23440155811335672, 'Validation Loss (Methylation-K)': 0.3681852638721466, 'Validation Loss (total)': 0.3681852638721466, 'TimeToTrain': 602.5721308708191}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005897892244172192,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3272248610,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.249738865498955}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.640
[3,   134] loss: 0.640
[4,   134] loss: 0.643
[5,   134] loss: 0.642
[6,   134] loss: 0.638
[7,   134] loss: 0.638
[8,   134] loss: 0.640
[9,   134] loss: 0.638
[10,   134] loss: 0.642
[11,   134] loss: 0.641
[12,   134] loss: 0.640
[13,   134] loss: 0.641
[14,   134] loss: 0.643
[15,   134] loss: 0.640
[16,   134] loss: 0.640
[17,   134] loss: 0.639
[18,   134] loss: 0.638
[19,   134] loss: 0.641
[20,   134] loss: 0.648
[21,   134] loss: 0.637
[22,   134] loss: 0.641
[23,   134] loss: 0.642
[24,   134] loss: 0.644
[25,   134] loss: 0.640
[26,   134] loss: 0.643
[27,   134] loss: 0.636
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005309766861508828,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1755005608,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.29864405426998}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.611
[3,   134] loss: 0.608
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003882447682530283,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1918737026,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.214480120215327}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.589
[3,   134] loss: 0.567
[4,   134] loss: 0.555
[5,   134] loss: 0.546
[6,   134] loss: 0.539
[7,   134] loss: 0.537
[8,   134] loss: 0.542
[9,   134] loss: 0.538
[10,   134] loss: 0.537
[11,   134] loss: 0.536
[12,   134] loss: 0.540
[13,   134] loss: 0.540
[14,   134] loss: 0.537
[15,   134] loss: 0.541
[16,   134] loss: 0.540
[17,   134] loss: 0.543
[18,   134] loss: 0.538
[19,   134] loss: 0.537
[20,   134] loss: 0.546
[21,   134] loss: 0.535
[22,   134] loss: 0.541
[23,   134] loss: 0.541
[24,   134] loss: 0.541
[25,   134] loss: 0.544
[26,   134] loss: 0.542
[27,   134] loss: 0.542
[28,   134] loss: 0.543
[29,   134] loss: 0.544
[30,   134] loss: 0.544
[31,   134] loss: 0.544
[32,   134] loss: 0.547
[33,   134] loss: 0.549
[34,   134] loss: 0.541
[35,   134] loss: 0.543
[36,   134] loss: 0.546
[37,   134] loss: 0.542
Early stopping applied (best metric=0.3566821217536926)
Finished Training
Total time taken: 357.98965287208557
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.592
[3,   134] loss: 0.569
[4,   134] loss: 0.553
[5,   134] loss: 0.548
[6,   134] loss: 0.540
[7,   134] loss: 0.539
[8,   134] loss: 0.535
[9,   134] loss: 0.537
[10,   134] loss: 0.537
[11,   134] loss: 0.538
[12,   134] loss: 0.534
[13,   134] loss: 0.535
[14,   134] loss: 0.536
[15,   134] loss: 0.535
[16,   134] loss: 0.541
[17,   134] loss: 0.532
[18,   134] loss: 0.538
[19,   134] loss: 0.534
[20,   134] loss: 0.536
[21,   134] loss: 0.541
[22,   134] loss: 0.537
[23,   134] loss: 0.534
[24,   134] loss: 0.536
[25,   134] loss: 0.540
[26,   134] loss: 0.536
[27,   134] loss: 0.537
[28,   134] loss: 0.540
[29,   134] loss: 0.538
[30,   134] loss: 0.538
[31,   134] loss: 0.543
[32,   134] loss: 0.538
[33,   134] loss: 0.536
[34,   134] loss: 0.540
[35,   134] loss: 0.538
[36,   134] loss: 0.539
[37,   134] loss: 0.538
[38,   134] loss: 0.538
[39,   134] loss: 0.539
[40,   134] loss: 0.536
[41,   134] loss: 0.541
Early stopping applied (best metric=0.34764301776885986)
Finished Training
Total time taken: 381.7297809123993
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.589
[3,   134] loss: 0.567
[4,   134] loss: 0.554
[5,   134] loss: 0.543
[6,   134] loss: 0.542
[7,   134] loss: 0.535
[8,   134] loss: 0.532
[9,   134] loss: 0.534
[10,   134] loss: 0.527
[11,   134] loss: 0.534
[12,   134] loss: 0.533
[13,   134] loss: 0.528
[14,   134] loss: 0.530
[15,   134] loss: 0.530
[16,   134] loss: 0.533
[17,   134] loss: 0.534
[18,   134] loss: 0.536
[19,   134] loss: 0.533
[20,   134] loss: 0.530
[21,   134] loss: 0.534
[22,   134] loss: 0.530
[23,   134] loss: 0.534
[24,   134] loss: 0.531
[25,   134] loss: 0.532
[26,   134] loss: 0.529
[27,   134] loss: 0.534
[28,   134] loss: 0.530
[29,   134] loss: 0.531
[30,   134] loss: 0.532
[31,   134] loss: 0.530
[32,   134] loss: 0.534
[33,   134] loss: 0.535
[34,   134] loss: 0.532
[35,   134] loss: 0.532
[36,   134] loss: 0.534
[37,   134] loss: 0.528
[38,   134] loss: 0.532
[39,   134] loss: 0.532
[40,   134] loss: 0.533
[41,   134] loss: 0.532
[42,   134] loss: 0.531
[43,   134] loss: 0.535
Early stopping applied (best metric=0.34752535820007324)
Finished Training
Total time taken: 420.30161213874817
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.590
[3,   134] loss: 0.569
[4,   134] loss: 0.555
[5,   134] loss: 0.544
[6,   134] loss: 0.542
[7,   134] loss: 0.536
[8,   134] loss: 0.540
[9,   134] loss: 0.534
[10,   134] loss: 0.538
[11,   134] loss: 0.542
[12,   134] loss: 0.533
[13,   134] loss: 0.534
[14,   134] loss: 0.536
[15,   134] loss: 0.535
[16,   134] loss: 0.537
[17,   134] loss: 0.536
[18,   134] loss: 0.534
[19,   134] loss: 0.535
[20,   134] loss: 0.537
[21,   134] loss: 0.536
[22,   134] loss: 0.541
[23,   134] loss: 0.536
[24,   134] loss: 0.535
[25,   134] loss: 0.537
[26,   134] loss: 0.537
[27,   134] loss: 0.537
[28,   134] loss: 0.536
[29,   134] loss: 0.536
[30,   134] loss: 0.538
[31,   134] loss: 0.535
[32,   134] loss: 0.532
[33,   134] loss: 0.535
[34,   134] loss: 0.536
[35,   134] loss: 0.538
[36,   134] loss: 0.540
[37,   134] loss: 0.538
[38,   134] loss: 0.536
[39,   134] loss: 0.535
[40,   134] loss: 0.538
[41,   134] loss: 0.539
Early stopping applied (best metric=0.34291237592697144)
Finished Training
Total time taken: 401.1795995235443
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.586
[3,   134] loss: 0.563
[4,   134] loss: 0.550
[5,   134] loss: 0.541
[6,   134] loss: 0.537
[7,   134] loss: 0.536
[8,   134] loss: 0.531
[9,   134] loss: 0.529
[10,   134] loss: 0.526
[11,   134] loss: 0.532
[12,   134] loss: 0.529
[13,   134] loss: 0.528
[14,   134] loss: 0.525
[15,   134] loss: 0.525
[16,   134] loss: 0.523
[17,   134] loss: 0.527
[18,   134] loss: 0.526
[19,   134] loss: 0.525
[20,   134] loss: 0.526
[21,   134] loss: 0.526
[22,   134] loss: 0.526
[23,   134] loss: 0.526
[24,   134] loss: 0.524
[25,   134] loss: 0.521
[26,   134] loss: 0.524
[27,   134] loss: 0.525
[28,   134] loss: 0.527
[29,   134] loss: 0.529
[30,   134] loss: 0.526
[31,   134] loss: 0.526
[32,   134] loss: 0.526
[33,   134] loss: 0.525
[34,   134] loss: 0.526
[35,   134] loss: 0.525
[36,   134] loss: 0.525
[37,   134] loss: 0.528
[38,   134] loss: 0.523
[39,   134] loss: 0.526
[40,   134] loss: 0.528
[41,   134] loss: 0.524
[42,   134] loss: 0.528
[43,   134] loss: 0.524
[44,   134] loss: 0.525
[45,   134] loss: 0.527
[46,   134] loss: 0.527
[47,   134] loss: 0.528
[48,   134] loss: 0.525
[49,   134] loss: 0.524
[50,   134] loss: 0.527
[51,   134] loss: 0.528
[52,   134] loss: 0.525
[53,   134] loss: 0.528
[54,   134] loss: 0.526
[55,   134] loss: 0.526
[56,   134] loss: 0.523
[57,   134] loss: 0.526
[58,   134] loss: 0.528
[59,   134] loss: 0.526
[60,   134] loss: 0.529
[61,   134] loss: 0.530
Early stopping applied (best metric=0.35706794261932373)
Finished Training
Total time taken: 595.5106198787689
{'Methylation-K Validation Accuracy': 0.6469188642471889, 'Methylation-K Validation Sensitivity': 0.7988737211702676, 'Methylation-K Validation Specificity': 0.6304386285516156, 'Methylation-K Validation Precision': 0.19020962450205633, 'Methylation-K AUC ROC': 0.7883870155395077, 'Methylation-K AUC PR': 0.30735474148599684, 'Methylation-K MCC': 0.2594808677592989, 'Methylation-K F1': 0.3071297595360631, 'Validation Loss (Methylation-K)': 0.35036616325378417, 'Validation Loss (total)': 0.35036616325378417, 'TimeToTrain': 431.3422530651093}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00672474561444272,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2705033651,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.06872748747183}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.646
[3,   134] loss: 0.642
[4,   134] loss: 0.640
[5,   134] loss: 0.639
[6,   134] loss: 0.642
[7,   134] loss: 0.642
[8,   134] loss: 0.644
[9,   134] loss: 0.639
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074815410159905235,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1754315466,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.849173681183274}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.660
[3,   134] loss: 0.664
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00044371406958572737,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3815541684,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.927845277873878}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.592
[3,   134] loss: 0.561
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011096469653770155,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3856833114,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.625143128007849}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.697
[2,   134] loss: 0.570
[3,   134] loss: 0.558
[4,   134] loss: 0.548
[5,   134] loss: 0.540
[6,   134] loss: 0.547
[7,   134] loss: 0.536
[8,   134] loss: 0.539
[9,   134] loss: 0.541
[10,   134] loss: 0.539
[11,   134] loss: 0.538
[12,   134] loss: 0.541
[13,   134] loss: 0.540
[14,   134] loss: 0.541
[15,   134] loss: 0.542
[16,   134] loss: 0.541
[17,   134] loss: 0.544
[18,   134] loss: 0.536
[19,   134] loss: 0.537
[20,   134] loss: 0.538
[21,   134] loss: 0.539
[22,   134] loss: 0.539
[23,   134] loss: 0.539
[24,   134] loss: 0.539
[25,   134] loss: 0.539
[26,   134] loss: 0.542
[27,   134] loss: 0.536
[28,   134] loss: 0.541
[29,   134] loss: 0.537
[30,   134] loss: 0.536
[31,   134] loss: 0.537
[32,   134] loss: 0.536
[33,   134] loss: 0.539
[34,   134] loss: 0.540
[35,   134] loss: 0.538
[36,   134] loss: 0.537
[37,   134] loss: 0.537
[38,   134] loss: 0.537
[39,   134] loss: 0.540
[40,   134] loss: 0.539
[41,   134] loss: 0.538
[42,   134] loss: 0.539
[43,   134] loss: 0.541
[44,   134] loss: 0.540
[45,   134] loss: 0.539
[46,   134] loss: 0.537
Early stopping applied (best metric=0.3478595018386841)
Finished Training
Total time taken: 444.45044565200806
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.567
[3,   134] loss: 0.560
[4,   134] loss: 0.544
[5,   134] loss: 0.544
[6,   134] loss: 0.542
[7,   134] loss: 0.541
[8,   134] loss: 0.541
[9,   134] loss: 0.541
[10,   134] loss: 0.540
[11,   134] loss: 0.542
[12,   134] loss: 0.542
[13,   134] loss: 0.542
[14,   134] loss: 0.543
[15,   134] loss: 0.541
[16,   134] loss: 0.541
[17,   134] loss: 0.540
[18,   134] loss: 0.539
[19,   134] loss: 0.536
[20,   134] loss: 0.537
[21,   134] loss: 0.542
[22,   134] loss: 0.542
[23,   134] loss: 0.542
[24,   134] loss: 0.533
[25,   134] loss: 0.542
[26,   134] loss: 0.541
[27,   134] loss: 0.539
[28,   134] loss: 0.542
Early stopping applied (best metric=0.3602859377861023)
Finished Training
Total time taken: 266.64554476737976
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.571
[3,   134] loss: 0.552
[4,   134] loss: 0.549
[5,   134] loss: 0.540
[6,   134] loss: 0.541
[7,   134] loss: 0.541
[8,   134] loss: 0.541
[9,   134] loss: 0.542
[10,   134] loss: 0.538
[11,   134] loss: 0.543
[12,   134] loss: 0.542
[13,   134] loss: 0.541
[14,   134] loss: 0.541
[15,   134] loss: 0.542
[16,   134] loss: 0.542
[17,   134] loss: 0.535
[18,   134] loss: 0.539
[19,   134] loss: 0.541
[20,   134] loss: 0.538
[21,   134] loss: 0.536
[22,   134] loss: 0.543
[23,   134] loss: 0.539
[24,   134] loss: 0.539
[25,   134] loss: 0.541
[26,   134] loss: 0.541
[27,   134] loss: 0.534
[28,   134] loss: 0.538
[29,   134] loss: 0.535
[30,   134] loss: 0.538
[31,   134] loss: 0.537
[32,   134] loss: 0.541
[33,   134] loss: 0.537
[34,   134] loss: 0.539
[35,   134] loss: 0.540
[36,   134] loss: 0.541
[37,   134] loss: 0.540
[38,   134] loss: 0.539
[39,   134] loss: 0.542
[40,   134] loss: 0.540
Early stopping applied (best metric=0.35522109270095825)
Finished Training
Total time taken: 392.33903217315674
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.578
[3,   134] loss: 0.563
[4,   134] loss: 0.552
[5,   134] loss: 0.548
[6,   134] loss: 0.542
[7,   134] loss: 0.543
[8,   134] loss: 0.547
[9,   134] loss: 0.541
[10,   134] loss: 0.544
[11,   134] loss: 0.545
[12,   134] loss: 0.541
[13,   134] loss: 0.541
[14,   134] loss: 0.547
[15,   134] loss: 0.545
[16,   134] loss: 0.544
[17,   134] loss: 0.543
[18,   134] loss: 0.540
[19,   134] loss: 0.544
[20,   134] loss: 0.547
[21,   134] loss: 0.543
[22,   134] loss: 0.547
[23,   134] loss: 0.545
[24,   134] loss: 0.544
[25,   134] loss: 0.542
[26,   134] loss: 0.545
[27,   134] loss: 0.542
[28,   134] loss: 0.542
[29,   134] loss: 0.547
[30,   134] loss: 0.545
[31,   134] loss: 0.545
[32,   134] loss: 0.546
[33,   134] loss: 0.548
[34,   134] loss: 0.542
[35,   134] loss: 0.542
[36,   134] loss: 0.543
[37,   134] loss: 0.548
[38,   134] loss: 0.545
[39,   134] loss: 0.550
[40,   134] loss: 0.547
[41,   134] loss: 0.545
[42,   134] loss: 0.546
[43,   134] loss: 0.547
[44,   134] loss: 0.549
[45,   134] loss: 0.547
[46,   134] loss: 0.547
[47,   134] loss: 0.548
[48,   134] loss: 0.548
Early stopping applied (best metric=0.3466941714286804)
Finished Training
Total time taken: 471.25155544281006
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.689
[2,   134] loss: 0.571
[3,   134] loss: 0.554
[4,   134] loss: 0.546
[5,   134] loss: 0.545
[6,   134] loss: 0.545
[7,   134] loss: 0.538
[8,   134] loss: 0.544
[9,   134] loss: 0.540
[10,   134] loss: 0.540
[11,   134] loss: 0.541
[12,   134] loss: 0.547
[13,   134] loss: 0.540
[14,   134] loss: 0.542
[15,   134] loss: 0.544
[16,   134] loss: 0.542
[17,   134] loss: 0.543
[18,   134] loss: 0.539
[19,   134] loss: 0.542
[20,   134] loss: 0.542
[21,   134] loss: 0.541
[22,   134] loss: 0.540
[23,   134] loss: 0.541
[24,   134] loss: 0.542
[25,   134] loss: 0.540
[26,   134] loss: 0.537
[27,   134] loss: 0.540
Early stopping applied (best metric=0.3546401560306549)
Finished Training
Total time taken: 264.8685460090637
{'Methylation-K Validation Accuracy': 0.5853315798989382, 'Methylation-K Validation Sensitivity': 0.8325353389204592, 'Methylation-K Validation Specificity': 0.5585208277045972, 'Methylation-K Validation Precision': 0.17002206427012836, 'Methylation-K AUC ROC': 0.7800197496735447, 'Methylation-K AUC PR': 0.300481807464783, 'Methylation-K MCC': 0.23283804197683736, 'Methylation-K F1': 0.282273602432866, 'Validation Loss (Methylation-K)': 0.352940171957016, 'Validation Loss (total)': 0.352940171957016, 'TimeToTrain': 367.91102480888367}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004569984819476376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3007442482,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.709136478248418}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.636
[3,   134] loss: 0.636
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0059890300208032664,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 926687490,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.387097031871042}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.618
[3,   134] loss: 0.618
[4,   134] loss: 0.616
[5,   134] loss: 0.618
[6,   134] loss: 0.616
[7,   134] loss: 0.617
[8,   134] loss: 0.617
[9,   134] loss: 0.617
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005844948364352027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3400542303,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.435269273405066}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.653
[3,   134] loss: 0.656
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036506147243539093,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3895417572,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.793463344684454}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.699
[2,   134] loss: 0.642
[3,   134] loss: 0.645
[4,   134] loss: 0.642
[5,   134] loss: 0.644
[6,   134] loss: 0.638
[7,   134] loss: 0.643
[8,   134] loss: 0.646
[9,   134] loss: 0.643
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005237590580639979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4178387165,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.724918711303385}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.645
[3,   134] loss: 0.651
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006591849573039305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2724823288,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.211221792008006}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.616
[3,   134] loss: 0.619
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005345101472888548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1820424280,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.467170075845127}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.634
[3,   134] loss: 0.635
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007335271441058966,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3702284680,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.679305797777698}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.651
[3,   134] loss: 0.650
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025780981482833266,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3851136714,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.478575721958109}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.617
[3,   134] loss: 0.621
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008914049112652058,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2229455265,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.384563242736245}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.567
[3,   134] loss: 0.548
[4,   134] loss: 0.541
[5,   134] loss: 0.531
[6,   134] loss: 0.521
[7,   134] loss: 0.521
[8,   134] loss: 0.522
[9,   134] loss: 0.517
[10,   134] loss: 0.517
[11,   134] loss: 0.516
[12,   134] loss: 0.520
[13,   134] loss: 0.517
[14,   134] loss: 0.516
[15,   134] loss: 0.514
[16,   134] loss: 0.515
[17,   134] loss: 0.514
[18,   134] loss: 0.514
[19,   134] loss: 0.514
[20,   134] loss: 0.512
[21,   134] loss: 0.516
[22,   134] loss: 0.515
[23,   134] loss: 0.515
[24,   134] loss: 0.517
[25,   134] loss: 0.512
[26,   134] loss: 0.515
[27,   134] loss: 0.513
[28,   134] loss: 0.519
[29,   134] loss: 0.519
[30,   134] loss: 0.514
[31,   134] loss: 0.511
[32,   134] loss: 0.519
[33,   134] loss: 0.514
[34,   134] loss: 0.517
[35,   134] loss: 0.516
[36,   134] loss: 0.518
[37,   134] loss: 0.519
[38,   134] loss: 0.522
[39,   134] loss: 0.518
[40,   134] loss: 0.513
[41,   134] loss: 0.516
[42,   134] loss: 0.516
[43,   134] loss: 0.518
[44,   134] loss: 0.517
[45,   134] loss: 0.518
[46,   134] loss: 0.516
Early stopping applied (best metric=0.34425535798072815)
Finished Training
Total time taken: 445.1141927242279
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.692
[2,   134] loss: 0.570
[3,   134] loss: 0.552
[4,   134] loss: 0.537
[5,   134] loss: 0.524
[6,   134] loss: 0.524
[7,   134] loss: 0.523
[8,   134] loss: 0.519
[9,   134] loss: 0.521
[10,   134] loss: 0.514
[11,   134] loss: 0.519
[12,   134] loss: 0.517
[13,   134] loss: 0.513
[14,   134] loss: 0.516
[15,   134] loss: 0.517
[16,   134] loss: 0.520
[17,   134] loss: 0.514
[18,   134] loss: 0.519
[19,   134] loss: 0.515
[20,   134] loss: 0.515
[21,   134] loss: 0.518
[22,   134] loss: 0.518
[23,   134] loss: 0.514
[24,   134] loss: 0.513
[25,   134] loss: 0.514
[26,   134] loss: 0.512
[27,   134] loss: 0.513
[28,   134] loss: 0.520
[29,   134] loss: 0.518
[30,   134] loss: 0.517
[31,   134] loss: 0.517
[32,   134] loss: 0.515
[33,   134] loss: 0.515
[34,   134] loss: 0.518
[35,   134] loss: 0.515
[36,   134] loss: 0.516
[37,   134] loss: 0.516
[38,   134] loss: 0.513
[39,   134] loss: 0.520
[40,   134] loss: 0.516
[41,   134] loss: 0.518
[42,   134] loss: 0.515
[43,   134] loss: 0.516
[44,   134] loss: 0.516
[45,   134] loss: 0.519
[46,   134] loss: 0.515
[47,   134] loss: 0.515
[48,   134] loss: 0.514
[49,   134] loss: 0.516
[50,   134] loss: 0.516
[51,   134] loss: 0.511
[52,   134] loss: 0.516
[53,   134] loss: 0.516
[54,   134] loss: 0.517
[55,   134] loss: 0.515
[56,   134] loss: 0.512
[57,   134] loss: 0.518
[58,   134] loss: 0.514
[59,   134] loss: 0.512
[60,   134] loss: 0.513
[61,   134] loss: 0.518
Early stopping applied (best metric=0.3354995548725128)
Finished Training
Total time taken: 585.0330898761749
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.566
[3,   134] loss: 0.542
[4,   134] loss: 0.531
[5,   134] loss: 0.517
[6,   134] loss: 0.519
[7,   134] loss: 0.514
[8,   134] loss: 0.509
[9,   134] loss: 0.512
[10,   134] loss: 0.504
[11,   134] loss: 0.511
[12,   134] loss: 0.505
[13,   134] loss: 0.505
[14,   134] loss: 0.507
[15,   134] loss: 0.509
[16,   134] loss: 0.508
[17,   134] loss: 0.512
[18,   134] loss: 0.508
[19,   134] loss: 0.510
[20,   134] loss: 0.508
[21,   134] loss: 0.506
[22,   134] loss: 0.511
[23,   134] loss: 0.508
[24,   134] loss: 0.508
[25,   134] loss: 0.505
[26,   134] loss: 0.508
[27,   134] loss: 0.508
[28,   134] loss: 0.508
[29,   134] loss: 0.504
[30,   134] loss: 0.507
[31,   134] loss: 0.507
[32,   134] loss: 0.509
[33,   134] loss: 0.508
[34,   134] loss: 0.508
[35,   134] loss: 0.505
[36,   134] loss: 0.504
[37,   134] loss: 0.505
[38,   134] loss: 0.510
[39,   134] loss: 0.509
[40,   134] loss: 0.508
[41,   134] loss: 0.505
[42,   134] loss: 0.504
[43,   134] loss: 0.507
[44,   134] loss: 0.511
[45,   134] loss: 0.506
[46,   134] loss: 0.508
[47,   134] loss: 0.510
[48,   134] loss: 0.510
[49,   134] loss: 0.511
[50,   134] loss: 0.511
[51,   134] loss: 0.512
[52,   134] loss: 0.508
[53,   134] loss: 0.509
[54,   134] loss: 0.510
[55,   134] loss: 0.510
[56,   134] loss: 0.508
[57,   134] loss: 0.512
[58,   134] loss: 0.508
[59,   134] loss: 0.512
[60,   134] loss: 0.507
[61,   134] loss: 0.509
[62,   134] loss: 0.512
[63,   134] loss: 0.513
[64,   134] loss: 0.510
[65,   134] loss: 0.510
[66,   134] loss: 0.513
[67,   134] loss: 0.508
[68,   134] loss: 0.506
[69,   134] loss: 0.505
[70,   134] loss: 0.505
[71,   134] loss: 0.508
[72,   134] loss: 0.510
[73,   134] loss: 0.509
[74,   134] loss: 0.508
[75,   134] loss: 0.507
[76,   134] loss: 0.510
[77,   134] loss: 0.508
[78,   134] loss: 0.512
[79,   134] loss: 0.511
[80,   134] loss: 0.507
[81,   134] loss: 0.506
[82,   134] loss: 0.510
[83,   134] loss: 0.509
[84,   134] loss: 0.509
[85,   134] loss: 0.510
[86,   134] loss: 0.511
[87,   134] loss: 0.509
[88,   134] loss: 0.514
[89,   134] loss: 0.511
[90,   134] loss: 0.513
[91,   134] loss: 0.510
[92,   134] loss: 0.511
[93,   134] loss: 0.511
[94,   134] loss: 0.513
[95,   134] loss: 0.511
[96,   134] loss: 0.509
[97,   134] loss: 0.513
[98,   134] loss: 0.509
[99,   134] loss: 0.508
[100,   134] loss: 0.508
[101,   134] loss: 0.513
[102,   134] loss: 0.511
[103,   134] loss: 0.508
Early stopping applied (best metric=0.3523576855659485)
Finished Training
Total time taken: 1003.461749792099
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.696
[2,   134] loss: 0.563
[3,   134] loss: 0.539
[4,   134] loss: 0.529
[5,   134] loss: 0.524
[6,   134] loss: 0.518
[7,   134] loss: 0.514
[8,   134] loss: 0.515
[9,   134] loss: 0.514
[10,   134] loss: 0.509
[11,   134] loss: 0.512
[12,   134] loss: 0.512
[13,   134] loss: 0.511
[14,   134] loss: 0.510
[15,   134] loss: 0.507
[16,   134] loss: 0.510
[17,   134] loss: 0.514
[18,   134] loss: 0.510
[19,   134] loss: 0.508
[20,   134] loss: 0.515
[21,   134] loss: 0.511
[22,   134] loss: 0.513
[23,   134] loss: 0.508
[24,   134] loss: 0.515
[25,   134] loss: 0.509
[26,   134] loss: 0.510
[27,   134] loss: 0.512
[28,   134] loss: 0.512
[29,   134] loss: 0.511
[30,   134] loss: 0.512
[31,   134] loss: 0.508
[32,   134] loss: 0.513
[33,   134] loss: 0.512
[34,   134] loss: 0.513
[35,   134] loss: 0.512
[36,   134] loss: 0.511
[37,   134] loss: 0.510
[38,   134] loss: 0.512
[39,   134] loss: 0.510
[40,   134] loss: 0.512
[41,   134] loss: 0.513
[42,   134] loss: 0.511
[43,   134] loss: 0.511
[44,   134] loss: 0.513
[45,   134] loss: 0.510
[46,   134] loss: 0.512
[47,   134] loss: 0.510
[48,   134] loss: 0.513
[49,   134] loss: 0.509
[50,   134] loss: 0.511
[51,   134] loss: 0.509
[52,   134] loss: 0.506
[53,   134] loss: 0.508
[54,   134] loss: 0.511
[55,   134] loss: 0.509
Early stopping applied (best metric=0.36155447363853455)
Finished Training
Total time taken: 537.0045032501221
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.567
[3,   134] loss: 0.543
[4,   134] loss: 0.530
[5,   134] loss: 0.524
[6,   134] loss: 0.523
[7,   134] loss: 0.513
[8,   134] loss: 0.514
[9,   134] loss: 0.515
[10,   134] loss: 0.515
[11,   134] loss: 0.517
[12,   134] loss: 0.515
[13,   134] loss: 0.516
[14,   134] loss: 0.511
[15,   134] loss: 0.515
[16,   134] loss: 0.512
[17,   134] loss: 0.514
[18,   134] loss: 0.510
[19,   134] loss: 0.511
[20,   134] loss: 0.512
[21,   134] loss: 0.514
[22,   134] loss: 0.512
[23,   134] loss: 0.510
[24,   134] loss: 0.513
[25,   134] loss: 0.510
[26,   134] loss: 0.513
[27,   134] loss: 0.513
Early stopping applied (best metric=0.3594920337200165)
Finished Training
Total time taken: 265.75110030174255
{'Methylation-K Validation Accuracy': 0.6266066582735523, 'Methylation-K Validation Sensitivity': 0.8066418764168769, 'Methylation-K Validation Specificity': 0.6070815222667896, 'Methylation-K Validation Precision': 0.18357139529496014, 'Methylation-K AUC ROC': 0.7868304094507704, 'Methylation-K AUC PR': 0.3081095239770918, 'Methylation-K MCC': 0.24937696282054103, 'Methylation-K F1': 0.29854689317343874, 'Validation Loss (Methylation-K)': 0.3506318211555481, 'Validation Loss (total)': 0.3506318211555481, 'TimeToTrain': 567.2729271888733}
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009659051444141906,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4016491396,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.832100995434446}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.691
[2,   134] loss: 0.629
[3,   134] loss: 0.634
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006911195537496337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2228173490,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.01109139021799}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.694
[2,   134] loss: 0.649
[3,   134] loss: 0.660
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 1.631555285451817e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1468384550,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.927972792914435}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.690
[2,   134] loss: 0.692
[3,   134] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 1,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Methylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 25,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005617825712418441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4142239726,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.936177177473626}
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.583
[3,   134] loss: 0.552
[4,   134] loss: 0.534
[5,   134] loss: 0.522
[6,   134] loss: 0.515
[7,   134] loss: 0.511
[8,   134] loss: 0.505
[9,   134] loss: 0.503
[10,   134] loss: 0.500
[11,   134] loss: 0.502
[12,   134] loss: 0.499
[13,   134] loss: 0.498
[14,   134] loss: 0.500
[15,   134] loss: 0.498
[16,   134] loss: 0.495
[17,   134] loss: 0.499
[18,   134] loss: 0.497
[19,   134] loss: 0.496
[20,   134] loss: 0.495
[21,   134] loss: 0.495
[22,   134] loss: 0.496
[23,   134] loss: 0.500
[24,   134] loss: 0.491
[25,   134] loss: 0.497
[26,   134] loss: 0.496
[27,   134] loss: 0.494
[28,   134] loss: 0.490
[29,   134] loss: 0.494
[30,   134] loss: 0.498
[31,   134] loss: 0.495
[32,   134] loss: 0.497
[33,   134] loss: 0.494
[34,   134] loss: 0.497
[35,   134] loss: 0.494
[36,   134] loss: 0.497
[37,   134] loss: 0.496
[38,   134] loss: 0.498
[39,   134] loss: 0.492
[40,   134] loss: 0.495
[41,   134] loss: 0.497
[42,   134] loss: 0.499
[43,   134] loss: 0.494
[44,   134] loss: 0.497
[45,   134] loss: 0.497
[46,   134] loss: 0.492
Early stopping applied (best metric=0.3499133288860321)
Finished Training
Total time taken: 456.45188307762146
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.580
[3,   134] loss: 0.555
[4,   134] loss: 0.532
[5,   134] loss: 0.527
[6,   134] loss: 0.513
[7,   134] loss: 0.516
[8,   134] loss: 0.508
[9,   134] loss: 0.508
[10,   134] loss: 0.507
[11,   134] loss: 0.506
[12,   134] loss: 0.504
[13,   134] loss: 0.501
[14,   134] loss: 0.501
[15,   134] loss: 0.501
[16,   134] loss: 0.499
[17,   134] loss: 0.502
[18,   134] loss: 0.502
[19,   134] loss: 0.502
[20,   134] loss: 0.500
[21,   134] loss: 0.502
[22,   134] loss: 0.496
[23,   134] loss: 0.497
[24,   134] loss: 0.497
[25,   134] loss: 0.501
[26,   134] loss: 0.498
[27,   134] loss: 0.496
[28,   134] loss: 0.504
[29,   134] loss: 0.496
[30,   134] loss: 0.500
[31,   134] loss: 0.498
[32,   134] loss: 0.500
[33,   134] loss: 0.503
[34,   134] loss: 0.497
[35,   134] loss: 0.501
[36,   134] loss: 0.502
[37,   134] loss: 0.499
[38,   134] loss: 0.501
[39,   134] loss: 0.498
[40,   134] loss: 0.496
[41,   134] loss: 0.495
[42,   134] loss: 0.498
[43,   134] loss: 0.505
[44,   134] loss: 0.502
[45,   134] loss: 0.495
[46,   134] loss: 0.498
[47,   134] loss: 0.498
[48,   134] loss: 0.499
[49,   134] loss: 0.500
[50,   134] loss: 0.501
[51,   134] loss: 0.500
[52,   134] loss: 0.502
[53,   134] loss: 0.497
[54,   134] loss: 0.497
[55,   134] loss: 0.497
[56,   134] loss: 0.499
[57,   134] loss: 0.499
[58,   134] loss: 0.500
[59,   134] loss: 0.498
[60,   134] loss: 0.499
[61,   134] loss: 0.494
Early stopping applied (best metric=0.33931469917297363)
Finished Training
Total time taken: 596.9936273097992
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.698
[2,   134] loss: 0.573
[3,   134] loss: 0.544
[4,   134] loss: 0.530
[5,   134] loss: 0.517
[6,   134] loss: 0.505
[7,   134] loss: 0.501
[8,   134] loss: 0.500
[9,   134] loss: 0.492
[10,   134] loss: 0.498
[11,   134] loss: 0.497
[12,   134] loss: 0.495
[13,   134] loss: 0.495
[14,   134] loss: 0.492
[15,   134] loss: 0.491
[16,   134] loss: 0.498
[17,   134] loss: 0.493
[18,   134] loss: 0.490
[19,   134] loss: 0.497
[20,   134] loss: 0.492
[21,   134] loss: 0.492
[22,   134] loss: 0.496
[23,   134] loss: 0.492
[24,   134] loss: 0.491
[25,   134] loss: 0.493
[26,   134] loss: 0.497
[27,   134] loss: 0.491
[28,   134] loss: 0.493
[29,   134] loss: 0.489
[30,   134] loss: 0.489
[31,   134] loss: 0.490
[32,   134] loss: 0.492
[33,   134] loss: 0.491
[34,   134] loss: 0.491
[35,   134] loss: 0.489
[36,   134] loss: 0.493
[37,   134] loss: 0.494
[38,   134] loss: 0.492
[39,   134] loss: 0.492
[40,   134] loss: 0.492
[41,   134] loss: 0.489
[42,   134] loss: 0.494
[43,   134] loss: 0.488
[44,   134] loss: 0.494
[45,   134] loss: 0.493
[46,   134] loss: 0.493
[47,   134] loss: 0.492
[48,   134] loss: 0.493
[49,   134] loss: 0.488
[50,   134] loss: 0.492
[51,   134] loss: 0.493
[52,   134] loss: 0.493
[53,   134] loss: 0.486
Early stopping applied (best metric=0.3574163019657135)
Finished Training
Total time taken: 518.4533593654633
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.695
[2,   134] loss: 0.578
[3,   134] loss: 0.551
[4,   134] loss: 0.530
[5,   134] loss: 0.519
[6,   134] loss: 0.514
[7,   134] loss: 0.507
[8,   134] loss: 0.501
[9,   134] loss: 0.501
[10,   134] loss: 0.506
[11,   134] loss: 0.499
[12,   134] loss: 0.499
[13,   134] loss: 0.502
[14,   134] loss: 0.497
[15,   134] loss: 0.498
[16,   134] loss: 0.500
[17,   134] loss: 0.496
[18,   134] loss: 0.497
[19,   134] loss: 0.499
[20,   134] loss: 0.501
[21,   134] loss: 0.497
[22,   134] loss: 0.497
[23,   134] loss: 0.493
[24,   134] loss: 0.496
[25,   134] loss: 0.497
[26,   134] loss: 0.496
[27,   134] loss: 0.496
[28,   134] loss: 0.498
[29,   134] loss: 0.502
[30,   134] loss: 0.498
[31,   134] loss: 0.497
[32,   134] loss: 0.501
[33,   134] loss: 0.497
[34,   134] loss: 0.499
[35,   134] loss: 0.498
[36,   134] loss: 0.503
[37,   134] loss: 0.499
[38,   134] loss: 0.498
[39,   134] loss: 0.501
[40,   134] loss: 0.500
Early stopping applied (best metric=0.3599186837673187)
Finished Training
Total time taken: 392.4042272567749
(4634, 33, 1024)
(42729, 33, 1024)
Loaded folder code/Thesis/dataset/train/Methylation-K/embeddings (47363 samples)
[1,     1] loss: 0.693
[2,   134] loss: 0.573
[3,   134] loss: 0.548
[4,   134] loss: 0.528
[5,   134] loss: 0.517
[6,   134] loss: 0.512
[7,   134] loss: 0.509
[8,   134] loss: 0.505
[9,   134] loss: 0.505
[10,   134] loss: 0.501
[11,   134] loss: 0.501
[12,   134] loss: 0.499
[13,   134] loss: 0.498
[14,   134] loss: 0.496
[15,   134] loss: 0.499
[16,   134] loss: 0.497
[17,   134] loss: 0.494
[18,   134] loss: 0.495
[19,   134] loss: 0.496
[20,   134] loss: 0.494
[21,   134] loss: 0.495
[22,   134] loss: 0.494
[23,   134] loss: 0.495
[24,   134] loss: 0.494
[25,   134] loss: 0.493
[26,   134] loss: 0.496
[27,   134] loss: 0.495
[28,   134] loss: 0.491
[29,   134] loss: 0.492
[30,   134] loss: 0.493
[31,   134] loss: 0.496
[32,   134] loss: 0.496
[33,   134] loss: 0.492
[34,   134] loss: 0.490
[35,   134] loss: 0.495
[36,   134] loss: 0.497
[37,   134] loss: 0.493
[38,   134] loss: 0.496
[39,   134] loss: 0.494
[40,   134] loss: 0.499
[41,   134] loss: 0.495
[42,   134] loss: 0.491
[43,   134] loss: 0.492
[44,   134] loss: 0.493
[45,   134] loss: 0.498
[46,   134] loss: 0.499
[47,   134] loss: 0.496
Early stopping applied (best metric=0.3643709421157837)
Finished Training
Total time taken: 459.84162616729736
{'Methylation-K Validation Accuracy': 0.6596933866122549, 'Methylation-K Validation Sensitivity': 0.7857064638712398, 'Methylation-K Validation Specificity': 0.6460257906922192, 'Methylation-K Validation Precision': 0.19479179055618256, 'Methylation-K AUC ROC': 0.7940418874520131, 'Methylation-K AUC PR': 0.31672476797632476, 'Methylation-K MCC': 0.26293789186289884, 'Methylation-K F1': 0.31188549898926365, 'Validation Loss (Methylation-K)': 0.3541867911815643, 'Validation Loss (total)': 0.3541867911815643, 'TimeToTrain': 484.8289446353912}